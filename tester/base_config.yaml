# base_config.yaml

# paddle errors which will be ignored and considered as pass
paddle_error_dismiss:
  # API: "error_message"
  # API:
  #   - "error_msg1"
  #   - "error_msg2"
  paddle.nn.functional.conv1d: "(PreconditionNotMet) The element size of "
  paddle.nn.functional.conv1d_transpose: "(PreconditionNotMet) The element size of "
  paddle.nn.functional.conv2d: "(PreconditionNotMet) The element size of "
  paddle.nn.functional.conv2d_transpose: "(PreconditionNotMet) The element size of "
  paddle.nn.functional.conv3d: "(PreconditionNotMet) The element size of "
  paddle.nn.functional.conv3d_transpose: "(PreconditionNotMet) The element size of "
  paddle.vision.ops.distribute_fpn_proposals:
    - "(PreconditionNotMet) The number of proposals in FPN "
    - "(PreconditionNotMet) The number of images "
  paddle.incubate.nn.functional.variable_length_memory_efficient_attention: "(InvalidArgument) the kernel should not be launched"
  paddle.Tensor.inverse: "(Unimplemented) cublasMatInv does not support batch_size > 65536."
  paddle.incubate.nn.functional.fused_rms_norm : "(Fatal) Failed to launch RMSNorm kernel"
  paddle.nn.functional.ctc_loss: "(InvalidArgument) The total number of elements "
  paddle.unbind: "(PreconditionNotMet) The number of proposals in unbind"
  paddle.linalg.lstsq:
    - "(PreconditionNotMet) The element size of x should be <= INT_MAX(2147483647)"
    - "(PreconditionNotMet) The element size of y should be <= INT_MAX(2147483647)"
  paddle.Tensor.lu: "(PreconditionNotMet) Matrix size too large for LU decomposition."
  paddle.linalg.solve: "(PreconditionNotMet) Input tensor 'A' or 'B' has too many elements (> INT_MAX)."
  paddle.nn.functional.group_norm: "(Unimplemented) GroupNorm kernel launch failed"
  paddle.einsum: "(Unimplemented) cublas GEMM does not support N >"
  paddle.nn.functional.cross_entropy: "(PreconditionNotMet) The num of the classes should be <= INT_MAX(2147483647)"
  paddle.nn.functional.class_center_sample:
    - "(InvalidArgument) The total number of elements for 'label' should be less than"
    - "(InvalidArgument) Illegal memory allocation, total allocated space must be greater than 0"
  paddle.linalg.matrix_rank: "(PreconditionNotMet) The element size of x should be <= INT_MAX(2147483647)"
  paddle.argsort: "(PreconditionNotMet) The dimension being sorted should be less than 2^31"

# some accuracy error can be considered tolerable
special_accuracy_atol_rtol:
  # API: [atol, rtol]
  paddle.cumsum: [2.0, 0.01]
  paddle.Tensor.cumsum: [2.0, 0.01]
  paddle.logcumsumexp: [2.0, 0.01]
  paddle.Tensor.logcumsumexp: [2.0, 0.01]
  paddle.tensordot: [1.0, 1.0]
  paddle.incubate.nn.functional.fused_bias_act: [1, 0.01]
  paddle.incubate.nn.functional.fused_rms_norm : [3, 0.5]
  paddle.incubate.nn.functional.fused_layer_norm: [1, 0.01]
  paddle.lerp : [5, 0.05]
  paddle.nn.functional.layer_norm : [0.2, 0.02]
  paddle.nn.functional.upsample: [0.5, 1.5]
  paddle.nn.functional.interpolate: [0.5, 1.5]
  paddle.incubate.nn.functional.fused_bias_dropout_residual_layer_norm: [1.0, 1.0]
  paddle.nn.functional.softmax: [1, 0.01]
  paddle.matmul: [2.0, 0.05]
  paddle.nn.functional.bilinear: [2.0, 0.08]
  paddle.nn.functional.interpolate: [1.5, 0.01]
  paddle.nn.functional.normalize: [0.2, 0.01]

# All configs that report dtype diff when not in not_check_dtype list should be
# copied to tester/api_config/5_accuracy/accuracy_gpu_error_dtype_diff.txt
not_check_dtype:
  - paddle.Tensor.__add__
  - paddle.Tensor.cumsum
  - paddle.Tensor.frexp
  - paddle.add
  - paddle.add_n
  - paddle.atan2
  - paddle.clip
  - paddle.copysign
  - paddle.cummax
  - paddle.cummin
  - paddle.cumprod
  - paddle.cumsum
  - paddle.floor
  - paddle.frexp
  - paddle.histogram
  - paddle.incubate.nn.functional.fused_layer_norm
  - paddle.ldexp
  - paddle.ldexp_
  - paddle.linalg.lstsq
  - paddle.nn.functional.adaptive_max_pool1d
  - paddle.nn.functional.adaptive_max_pool2d
  - paddle.nn.functional.adaptive_max_pool3d
  - paddle.nn.functional.conv2d_transpose
  - paddle.nn.functional.linear
  - paddle.nn.functional.max_pool1d
  - paddle.nn.functional.max_pool2d
  - paddle.nn.functional.max_pool3d
  - paddle.nn.functional.one_hot
  - paddle.nn.functional.smooth_l1_loss
  - paddle.vision.ops.roi_align
  - paddle.where

# keep rand_apis and stochastic_behavior_apis lists for future reference when checking if new api configs have random behavior @Cutelemon6
# rand_apis:
#   - paddle.Tensor.__dir__
#   - paddle.Tensor.bernoulli_
#   - paddle.Tensor.cauchy_
#   - paddle.Tensor.exponential_
#   - paddle.Tensor.geometric_
#   - paddle.Tensor.log_normal_
#   - paddle.Tensor.multinomial
#   - paddle.Tensor.normal_
#   - paddle.Tensor.uniform_
#   - paddle.bernoulli_
#   - paddle.binomial
#   - paddle.cauchy_
#   - paddle.empty
#   - paddle.empty_like
#   - paddle.geometric_
#   - paddle.log_normal
#   - paddle.log_normal_
#   - paddle.multinomial
#   - paddle.normal
#   - paddle.normal_
#   - paddle.poisson
#   - paddle.rand
#   - paddle.randint
#   - paddle.randint_like
#   - paddle.randn
#   - paddle.randperm
#   - paddle.standard_gamma
#   - paddle.standard_normal
#   - paddle.uniform

# stochastic_behavior_apis:
#   - paddle.Tensor.top_p_sampling
#   - paddle.incubate.nn.functional.fused_bias_dropout_residual_layer_norm
#   - paddle.incubate.nn.functional.fused_dropout_add
#   - paddle.incubate.nn.functional.fused_multi_head_attention # If parameter "dropout_rate=0.5, attn_dropout_rate=0.5 (default value)" is not equal to 0.0 or 1.0, the result involves random calculation.
#   - paddle.incubate.nn.functional.moe_dispatch
#   - paddle.nn.functional.alpha_dropout
#   - paddle.nn.functional.dropout
#   - paddle.nn.functional.dropout2d
#   - paddle.nn.functional.dropout3d
#   - paddle.nn.functional.feature_alpha_dropout
#   - paddle.nn.functional.fused_feedforward
#   - paddle.nn.functional.rrelu # If parameter "training=True" is set, the result involves random calculation.
#   - paddle.nn.functional.scaled_dot_product_attention # If parameter "dropout_p=0.0" is not equal to 0.0 or 1.0, the result involves random calculation.
#   - paddle.scatter # If overwrite is set to True and index contain duplicate values, the result involves random calculation.
#   - paddle.nn.functional.gumbel_softmax

single_op_no_signature_apis:
  - __add__
  - __div__
  - __eq__
  - __floordiv__
  - __ge__
  - __gt__
  - __le__
  - __lt__
  - __matmul__
  - __mod__
  - __mul__
  - __ne__
  - __pow__
  - __radd__
  - __rmatmul__
  - __rmod__
  - __rmul__
  - __rpow__
  - __rsub__
  - __rtruediv__
  - __sub__
  - __truediv__

handle_axes_api:
  - paddle.max
  - paddle.mean
  - paddle.min
  - paddle.prod
  - paddle.sum

forward_only_apis:
  - __and__
  - __eq__
  - __floordiv__
  - __ge__
  - __gt__
  - __invert__
  - __le__
  - __lshift__
  - __lt__
  - __ne__
  - __or__
  - __rand__
  - __rand__
  - __rfloordiv__
  - __rlshift__
  - __ror__
  - __ror__
  - __rrshift__
  - __rshift__
  - __rxor__
  - __rxor__
  - __xor__
  - accuracy
  - accuracy_check
  - adadelta_
  - adagrad_
  - adam_
  - adamax_
  - adamw_
  - add_act_xpu
  - add_act_xpu
  - add_group_norm_silu
  - add_group_norm_silu
  - add_layernorm_xpu
  - add_layernorm_xpu
  - add_n
  - addcmul_xpu
  - addcmul_xpu
  - all
  - all_reduce
  - allclose
  - any
  - apply_per_channel_scale
  - arange
  - argmax
  - argmin
  - asgd_
  - assign_pos
  - assign_value
  - assign_value_
  - atleast_1d
  - atleast_2d
  - atleast_3d
  - auc
  - average_accumulates_
  - barrier
  - batch_fc
  - bernoulli
  - bincount
  - binomial
  - bipartite_match
  - bitwise_and
  - bitwise_invert
  - bitwise_left_shift
  - bitwise_not
  - bitwise_or
  - bitwise_right_shift
  - bitwise_xor
  - blha_get_max_len
  - blha_get_max_len
  - block_multihead_attention
  - block_multihead_attention_
  - block_multihead_attention_
  - block_multihead_attention_xpu
  - block_multihead_attention_xpu
  - bn_act_xpu
  - bn_act_xpu
  - box_clip
  - box_coder
  - bucketize
  - c_allgather
  - c_allreduce_avg
  - c_allreduce_max
  - c_allreduce_min
  - c_allreduce_prod
  - c_allreduce_sum
  - c_broadcast
  - c_concat
  - c_identity
  - c_reduce_avg
  - c_reduce_max
  - c_reduce_min
  - c_reduce_prod
  - c_reduce_sum
  - c_reducescatter
  - c_scatter
  - c_split
  - c_sync_calc_stream
  - c_sync_comm_stream
  - check_finite_and_unscale_
  - check_numerics
  - chunk_eval
  - class_center_sample
  - clip_by_norm
  - coalesce
  - coalesce_tensor
  - coalesce_tensor_
  - conv1d_xpu
  - conv1d_xpu
  - conv2d_transpose_bias
  - conv2d_transpose_xpu
  - conv2d_transpose_xpu
  - conv2d_xpu
  - conv2d_xpu
  - conv3d_implicit_gemm
  - copy_to
  - crf_decoding
  - cross_attention_xpu
  - cross_attention_xpu
  - ctc_align
  - data
  - decayed_adagrad
  - decode_jpeg
  - depend
  - dequantize_abs_max
  - dequantize_linear
  - dequantize_log
  - dequantize_xpu
  - dequantize_xpu
  - detection_map
  - dgc
  - dgc_momentum
  - diag_embed
  - diff
  - dirichlet
  - distribute_fpn_proposals
  - distributed_fused_lamb
  - distributed_fused_lamb_init
  - distributed_fused_lamb_init
  - distributed_lookup_table
  - distributed_push_sparse
  - dpsgd
  - edit_distance
  - eigh
  - eigvals
  - eigvalsh
  - embedding_grad_dense
  - embedding_with_eltwise_add_xpu
  - embedding_with_eltwise_add_xpu
  - empty
  - empty_like
  - equal
  - equal_all
  - eye
  - fake_channel_wise_dequantize_max_abs
  - fake_channel_wise_quantize_abs_max
  - fake_dequantize_max_abs
  - fake_quantize_abs_max
  - fake_quantize_moving_average_abs_max
  - fake_quantize_range_abs_max
  - fast_layernorm_xpu
  - fast_layernorm_xpu
  - fast_where_xpu
  - fast_where_xpu
  - fc
  - fc
  - fc_xpu
  - fc_xpu
  - feed
  - fetch
  - floor_divide
  - fp8_fp8_half_gemm_fused
  - ftrl
  - full
  - full_
  - full_batch_size_like
  - full_int_array
  - full_like
  - full_with_tensor
  - fused_adam_
  - fused_bias_act
  - fused_bias_act
  - fused_bias_residual_layernorm
  - fused_bias_residual_layernorm
  - fused_conv2d_add_act
  - fused_conv2d_add_act
  - fused_dconv_drelu_dbn
  - fused_dconv_drelu_dbn
  - fused_elementwise_add
  - fused_elementwise_add
  - fused_elementwise_div
  - fused_elementwise_div
  - fused_elementwise_mul
  - fused_elementwise_mul
  - fused_elementwise_sub
  - fused_elementwise_sub
  - fused_embedding_eltwise_layernorm
  - fused_embedding_eltwise_layernorm
  - fused_embedding_fc_lstm
  - fused_fc_elementwise_layernorm
  - fused_fc_elementwise_layernorm
  - fused_layer_norm
  - fused_linear_param_grad_add
  - fused_linear_param_grad_add
  - fused_moe
  - fused_multi_transformer
  - fused_multi_transformer_
  - fused_multi_transformer_int8_xpu
  - fused_multi_transformer_int8_xpu
  - fused_multi_transformer_xpu
  - fused_multi_transformer_xpu
  - fused_scale_bias_add_relu
  - fused_scale_bias_add_relu
  - fused_scale_bias_relu_conv_bn
  - fused_scale_bias_relu_conv_bn
  - fused_token_prune
  - fused_token_prune
  - fusion_group
  - fusion_group
  - fusion_gru
  - fusion_gru
  - fusion_lstm
  - fusion_lstm
  - fusion_repeated_fc_relu
  - fusion_repeated_fc_relu
  - fusion_seqconv_eltadd_relu
  - fusion_seqconv_eltadd_relu
  - fusion_seqexpand_concat_fc
  - fusion_seqpool_concat
  - fusion_seqpool_cvm_concat
  - fusion_seqpool_cvm_concat
  - fusion_squared_mat_sub
  - fusion_squared_mat_sub
  - fusion_transpose_flatten_concat
  - fusion_transpose_flatten_concat
  - gather_tree
  - gaussian
  - gemm_epilogue
  - gemm_epilogue
  - generate_proposals
  - generate_sequence_xpu
  - generate_sequence_xpu
  - get_tensor_from_selected_rows
  - graph_khop_sampler
  - graph_sample_neighbors
  - greater_equal
  - greater_than
  - group_norm_silu_xpu
  - group_norm_silu_xpu
  - histogram
  - histogram_bin_edges
  - histogramdd
  - increment
  - indices
  - is_empty
  - isclose
  - isfinite
  - isin
  - isinf
  - isnan
  - isneginf
  - isposinf
  - isreal
  - lamb_
  - lars_momentum
  - layer_norm_act_xpu
  - layer_norm_act_xpu
  - layer_norm_relu_xpu
  - less
  - less_equal
  - less_than
  - limit_by_capacity
  - linspace
  - llm_int8_linear
  - load_combine
  - lod_array_length
  - logical_and
  - logical_not
  - logical_or
  - logical_xor
  - logspace
  - lower
  - lstsq
  - mask_adaptive_xpu
  - mask_adaptive_xpu
  - masked_multihead_attention
  - masked_multihead_attention_
  - matrix_nms
  - matrix_rank
  - matrix_rank_tol
  - memcpy
  - memcpy_d2h
  - memcpy_h2d
  - merge_selected_rows
  - merged_adam_
  - merged_momentum_
  - moe
  - momentum_
  - moving_average_abs_max_scale
  - multi_encoder_xpu
  - multi_encoder_xpu
  - multiclass_nms3
  - multihead_matmul
  - multihead_matmul
  - multinomial
  - nadam_
  - nextafter
  - nms
  - nonzero
  - nop
  - not_equal
  - npu_identity
  - number_count
  - numel
  - one_hot
  - onednn_to_paddle_layout
  - ones
  - ones_like
  - pad2d_xpu
  - pad2d_xpu
  - partial_allgather
  - partial_recv
  - partial_send
  - pinv
  - print
  - prior_box
  - prune_gate_by_capacity
  - pull_box_sparse
  - push_dense
  - push_sparse_v2
  - qkv_attention_xpu
  - qkv_attention_xpu
  - qkv_unpack_mha
  - qkv_unpack_mha
  - qr
  - quantize_linear
  - quantize_xpu
  - quantize_xpu
  - radam_
  - randint
  - random_routing
  - randperm
  - read_file
  - recv_v2
  - reindex_graph
  - remainder
  - rmsprop_
  - roformer_relative_embedding_xpu
  - roformer_relative_embedding_xpu
  - row_conv
  - rprop_
  - sample_neighbors
  - save_combine
  - searchsorted
  - seed
  - self_dp_attention
  - self_dp_attention
  - send_and_recv
  - send_v2
  - sequence_mask
  - sequence_unpad_xpu
  - sequence_unpad_xpu
  - sgd_
  - shadow_feed
  - shadow_feed_tensors
  - shape
  - shard_index
  - share_data_
  - sine_pos_xpu
  - sine_pos_xpu
  - skip_layernorm
  - skip_layernorm
  - sparse_momentum
  - spatial_transformer_resblock_xpu
  - spatial_transformer_resblock_xpu
  - squeeze_excitation_block
  - squeeze_excitation_block
  - standard_gamma
  - standard_normal
  - svd_lowrank
  - tdm_child
  - tdm_sampler
  - to_sparse_csr
  - top_p_sampling
  - tril_indices
  - triu_indices
  - truncated_gaussian_random
  - uniform
  - uniform_random_batch_size_like
  - unique
  - unique_consecutive
  - update_loss_scaling_
  - upper
  - vander
  - variable_length_memory_efficient_attention
  - variable_length_memory_efficient_attention
  - viterbi_decode
  - weight_dequantize
  - weight_only_linear_xpu
  - weight_only_linear_xpu
  - weight_quantize
  - weighted_sample_neighbors
  - write_to_array
  - yolo_box
  - yolo_box_head
  - yolo_box_post
  - yolo_box_xpu
  - yolo_box_xpu
  - zeros
  - zeros_like
  - median
