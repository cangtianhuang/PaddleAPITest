torch.Tensor.to(Tensor([2, 2], "float32"), device="cuda")
torch.Tensor.add(Tensor([2, 2], "float32"), Tensor([2, 2], "float32"))
torch.relu(Tensor([2, 2], "float32"))
torch.Tensor.add(Tensor([2, 2], "float32"), 10)
torch.nn.functional.softmax(Tensor([2, 2], "float32"), dim=1, _stacklevel=3, dtype=None)
torch.cat(tuple(Tensor([2, 2], "float32"), Tensor([2, 2], "float32")), dim=0)
torch.Tensor.argmax(Tensor([4, 2], "float32"), dim=0)
torch.Tensor.clamp(Tensor([4], "float32"), min=0)
torch.ones_like(Tensor([4], "float32"))
torch.Tensor.backward(Tensor([4], "float32"), gradient=Tensor([4], "float32"), retain_graph=None, create_graph=False, inputs=None)
custom_ops.custom_leaky_relu.default(Tensor([4], "float32"), 0.2)
torch.ones_like(Tensor([4], "float32"))
torch.autograd.grad(tuple(Tensor([4], "float32")), tuple(Tensor([4], "float32")), grad_outputs=Tensor([4], "float32"), retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False, is_grads_batched=False, materialize_grads=False)
