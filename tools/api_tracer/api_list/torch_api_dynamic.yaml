- torch.Tensor.T.__get__
- torch.Tensor.__and__
- torch.Tensor.__array__
- torch.Tensor.__bool__
- torch.Tensor.__deepcopy__
- torch.Tensor.__eq__
- torch.Tensor.__floordiv__
- torch.Tensor.__format__
- torch.Tensor.__getitem__
- torch.Tensor.__index__
- torch.Tensor.__int__
- torch.Tensor.__invert__
- torch.Tensor.__ior__
- torch.Tensor.__len__
- torch.Tensor.__or__
- torch.Tensor.__pow__
- torch.Tensor.__reduce_ex__
- torch.Tensor.__rpow__
- torch.Tensor.__rsub__
- torch.Tensor.__rtruediv__
- torch.Tensor.__setitem__
- torch.Tensor._backward_hooks.__set__
- torch.Tensor._clear_non_serializable_cached_data
- torch.Tensor._make_subclass
- torch.Tensor._reduce_ex_internal
- torch.Tensor._typed_storage
- torch.Tensor.add
- torch.Tensor.add_
- torch.Tensor.addcmul
- torch.Tensor.all
- torch.Tensor.amax
- torch.Tensor.any
- torch.Tensor.argmax
- torch.Tensor.argsort
- torch.Tensor.backward
- torch.Tensor.bfloat16
- torch.Tensor.bool
- torch.Tensor.chunk
- torch.Tensor.clamp
- torch.Tensor.clamp_
- torch.Tensor.clip
- torch.Tensor.clone
- torch.Tensor.contiguous
- torch.Tensor.copy_
- torch.Tensor.cos
- torch.Tensor.cpu
- torch.Tensor.cuda
- torch.Tensor.cumsum
- torch.Tensor.data.__get__
- torch.Tensor.data.__set__
- torch.Tensor.data_ptr
- torch.Tensor.detach
- torch.Tensor.dim
- torch.Tensor.div
- torch.Tensor.div_
- torch.Tensor.double
- torch.Tensor.element_size
- torch.Tensor.eq
- torch.Tensor.erfinv_
- torch.Tensor.expand
- torch.Tensor.expand_as
- torch.Tensor.fill_
- torch.Tensor.flatten
- torch.Tensor.float
- torch.Tensor.floor_
- torch.Tensor.gather
- torch.Tensor.ge
- torch.Tensor.get_device
- torch.Tensor.grad.__get__
- torch.Tensor.grad.__set__
- torch.Tensor.grad_fn.__get__
- torch.Tensor.gt
- torch.Tensor.half
- torch.Tensor.index_add_
- torch.Tensor.int
- torch.Tensor.is_complex
- torch.Tensor.is_contiguous
- torch.Tensor.is_cpu.__get__
- torch.Tensor.is_floating_point
- torch.Tensor.is_leaf.__get__
- torch.Tensor.is_meta.__get__
- torch.Tensor.is_mps.__get__
- torch.Tensor.is_quantized.__get__
- torch.Tensor.is_sparse.__get__
- torch.Tensor.item
- torch.Tensor.layout.__get__
- torch.Tensor.le
- torch.Tensor.logsumexp
- torch.Tensor.long
- torch.Tensor.lt
- torch.Tensor.masked_fill
- torch.Tensor.masked_fill_
- torch.Tensor.masked_scatter
- torch.Tensor.matmul
- torch.Tensor.max
- torch.Tensor.mean
- torch.Tensor.min
- torch.Tensor.mul
- torch.Tensor.mul_
- torch.Tensor.ndim.__get__
- torch.Tensor.ne
- torch.Tensor.neg
- torch.Tensor.nonzero
- torch.Tensor.norm
- torch.Tensor.normal_
- torch.Tensor.numel
- torch.Tensor.numpy
- torch.Tensor.permute
- torch.Tensor.pin_memory
- torch.Tensor.pow
- torch.Tensor.prod
- torch.Tensor.random_
- torch.Tensor.reciprocal
- torch.Tensor.remainder
- torch.Tensor.repeat
- torch.Tensor.repeat_interleave
- torch.Tensor.requires_grad.__get__
- torch.Tensor.requires_grad.__set__
- torch.Tensor.requires_grad_
- torch.Tensor.reshape
- torch.Tensor.round
- torch.Tensor.scatter
- torch.Tensor.scatter_
- torch.Tensor.scatter_add_
- torch.Tensor.sigmoid
- torch.Tensor.sin
- torch.Tensor.size
- torch.Tensor.softmax
- torch.Tensor.sort
- torch.Tensor.split
- torch.Tensor.split_with_sizes
- torch.Tensor.squeeze
- torch.Tensor.storage_offset
- torch.Tensor.sub
- torch.Tensor.sub_
- torch.Tensor.sum
- torch.Tensor.t
- torch.Tensor.tile
- torch.Tensor.to
- torch.Tensor.tolist
- torch.Tensor.topk
- torch.Tensor.transpose
- torch.Tensor.type
- torch.Tensor.type_as
- torch.Tensor.unbind
- torch.Tensor.unflatten
- torch.Tensor.uniform_
- torch.Tensor.unique
- torch.Tensor.unsqueeze
- torch.Tensor.untyped_storage
- torch.Tensor.view
- torch.Tensor.view_as
- torch.Tensor.zero_
- torch.TypedStorage
- torch.UntypedStorage
- torch._C._linalg.linalg_solve
- torch._C._linalg.linalg_vector_norm
- torch._C._nn._parse_to
- torch._C._nn.gelu
- torch._C._nn.linear
- torch._C._nn.one_hot
- torch._C._nn.scaled_dot_product_attention
- torch._C._set_grad_enabled
- torch._C._special.special_logit
- torch.__future__.get_overwrite_module_params_on_conversion
- torch.__future__.get_swap_module_params_on_conversion
- torch._amp_foreach_non_finite_check_and_unscale_
- torch._amp_update_scale_
- torch._assert
- torch._foreach_add_
- torch._foreach_addcdiv_
- torch._foreach_addcmul_
- torch._foreach_div_
- torch._foreach_lerp_
- torch._foreach_mul_
- torch._foreach_norm
- torch._foreach_sqrt
- torch._has_compatible_shallow_copy_type
- torch._library.custom_ops.CustomOpDef
- torch._tensor.__floordiv__
- torch._tensor.__rdiv__
- torch._tensor.__rpow__
- torch._tensor.__rsub__
- torch._tensor.pow
- torch.abs
- torch.accelerator.current_accelerator
- torch.accelerator.is_available
- torch.adaptive_avg_pool1d
- torch.addcmul
- torch.all
- torch.amp.autocast_mode.is_autocast_available
- torch.amp.custom_bwd
- torch.amp.custom_fwd
- torch.arange
- torch.are_deterministic_algorithms_enabled
- torch.argmax
- torch.argsort
- torch.argwhere
- torch.as_tensor
- torch.asarray
- torch.autograd.backward
- torch.autograd.forward_ad.unpack_dual
- torch.autograd.function.BackwardCFunction
- torch.autograd.function.once_differentiable
- torch.autograd.grad_mode.inference_mode.clone
- torch.autograd.graph.increment_version
- torch.backends.cuda.cudnn_sdp_enabled
- torch.backends.cuda.enable_cudnn_sdp
- torch.backends.cuda.enable_flash_sdp
- torch.backends.cuda.enable_math_sdp
- torch.backends.cuda.enable_mem_efficient_sdp
- torch.backends.cuda.flash_sdp_enabled
- torch.backends.cuda.is_built
- torch.backends.cuda.math_sdp_enabled
- torch.backends.cuda.mem_efficient_sdp_enabled
- torch.bitwise_or
- torch.bmm
- torch.broadcast_to
- torch.cat
- torch.ceil
- torch.chunk
- torch.clamp
- torch.clip
- torch.compile
- torch.concat
- torch.conv2d
- torch.conv3d
- torch.conv_transpose2d
- torch.cos
- torch.cuda.CUDAGraph
- torch.cuda.Event
- torch.cuda.Stream
- torch.cuda.cudart
- torch.cuda.current_device
- torch.cuda.current_stream
- torch.cuda.device_count
- torch.cuda.empty_cache
- torch.cuda.get_device_capability
- torch.cuda.get_device_name
- torch.cuda.get_device_properties
- torch.cuda.get_rng_state
- torch.cuda.graph_pool_handle
- torch.cuda.ipc_collect
- torch.cuda.is_available
- torch.cuda.is_current_stream_capturing
- torch.cuda.is_initialized
- torch.cuda.jiterator._create_jit_fn
- torch.cuda.jiterator._create_multi_output_jit_fn
- torch.cuda.manual_seed
- torch.cuda.manual_seed_all
- torch.cuda.mem_get_info
- torch.cuda.memory_allocated
- torch.cuda.memory_reserved
- torch.cuda.set_device
- torch.cuda.set_rng_state
- torch.cuda.set_stream
- torch.cuda.stream
- torch.cuda.synchronize
- torch.cumprod
- torch.cumsum
- torch.deg2rad
- torch.diff
- torch.distributed.is_available
- torch.distributed.is_initialized
- torch.distributions.distribution.Distribution
- torch.div
- torch.einsum
- torch.empty
- torch.empty_like
- torch.empty_strided
- torch.exp
- torch.expm1
- torch.eye
- torch.flatten
- torch.floor_divide
- torch.from_numpy
- torch.full
- torch.full_like
- torch.functional.einsum
- torch.functional.meshgrid
- torch.functional.norm
- torch.functional.split
- torch.functional.unique_consecutive
- torch.fx.Graph
- torch.fx.Graph.owning_module.fget
- torch.fx.GraphModule
- torch.fx.GraphModule.graph.fget
- torch.fx.Node
- torch.fx.Node.args.fget
- torch.fx.Node.kwargs.fget
- torch.fx.Node.stack_trace.fget
- torch.fx.Tracer
- torch.fx.experimental.symbolic_shapes.DimConstraints
- torch.fx.experimental.symbolic_shapes.ShapeEnv
- torch.fx.experimental.symbolic_shapes.ShapeEnvSettings
- torch.fx.experimental.symbolic_shapes.StatefulSymbolicContext
- torch.fx.experimental.symbolic_shapes.StatelessSymbolicContext
- torch.fx.experimental.symbolic_shapes.compute_unbacked_bindings
- torch.fx.experimental.symbolic_shapes.guard_size_oblivious
- torch.fx.experimental.symbolic_shapes.is_accessor_node
- torch.fx.experimental.symbolic_shapes.resolve_unbacked_bindings
- torch.fx.experimental.symbolic_shapes.sym_eq
- torch.gather
- torch.ge
- torch.get_default_device
- torch.get_default_dtype
- torch.get_device_module
- torch.get_rng_state
- torch.greater
- torch.index_select
- torch.inverse
- torch.is_complex
- torch.is_deterministic_algorithms_warn_only_enabled
- torch.is_floating_point
- torch.is_grad_enabled
- torch.is_inference_mode_enabled
- torch.is_storage
- torch.is_tensor
- torch.isclose
- torch.isfinite
- torch.isin
- torch.isinf
- torch.isnan
- torch.jit.annotate
- torch.jit.ignore
- torch.jit.interface
- torch.jit.script
- torch.jit.unused
- torch.le
- torch.lerp
- torch.library.Library
- torch.library.custom_op
- torch.library.infer_schema
- torch.library.register_fake
- torch.linalg.solve
- torch.linalg.vector_norm
- torch.linspace
- torch.load
- torch.log
- torch.logical_and
- torch.manual_seed
- torch.masked_select
- torch.matmul
- torch.max
- torch.mean
- torch.meshgrid
- torch.min
- torch.minimum
- torch.mm
- torch.mps.manual_seed
- torch.multinomial
- torch.ne
- torch.nn.AdaptiveAvgPool1d
- torch.nn.ConstantPad2d
- torch.nn.Conv2d
- torch.nn.Conv3d
- torch.nn.ConvTranspose2d
- torch.nn.CrossEntropyLoss
- torch.nn.Dropout
- torch.nn.Embedding
- torch.nn.GELU
- torch.nn.GroupNorm
- torch.nn.Identity
- torch.nn.LayerNorm
- torch.nn.LeakyReLU
- torch.nn.Linear
- torch.nn.Module
- torch.nn.ModuleDict
- torch.nn.ModuleList
- torch.nn.MultiheadAttention
- torch.nn.ParameterDict
- torch.nn.ReLU
- torch.nn.Sequential
- torch.nn.SiLU
- torch.nn.Sigmoid
- torch.nn.Softmax
- torch.nn.Tanh
- torch.nn.Upsample
- torch.nn.attention.sdpa_kernel
- torch.nn.functional.adaptive_avg_pool1d
- torch.nn.functional.conv2d
- torch.nn.functional.conv3d
- torch.nn.functional.conv_transpose2d
- torch.nn.functional.cross_entropy
- torch.nn.functional.dropout
- torch.nn.functional.embedding
- torch.nn.functional.gelu
- torch.nn.functional.grid_sample
- torch.nn.functional.group_norm
- torch.nn.functional.interpolate
- torch.nn.functional.layer_norm
- torch.nn.functional.leaky_relu
- torch.nn.functional.linear
- torch.nn.functional.normalize
- torch.nn.functional.one_hot
- torch.nn.functional.pad
- torch.nn.functional.relu
- torch.nn.functional.scaled_dot_product_attention
- torch.nn.functional.sigmoid
- torch.nn.functional.silu
- torch.nn.functional.softmax
- torch.nn.init.calculate_gain
- torch.nn.init.constant_
- torch.nn.init.kaiming_normal_
- torch.nn.init.kaiming_uniform_
- torch.nn.init.normal_
- torch.nn.init.ones_
- torch.nn.init.uniform_
- torch.nn.init.xavier_uniform_
- torch.nn.init.zeros_
- torch.nn.parallel.DistributedDataParallel
- torch.nn.parameter.Parameter
- torch.nn.utils.clip_grad_norm_
- torch.nn.utils.rnn.pad_sequence
- torch.no_grad.clone
- torch.nonzero
- torch.norm
- torch.normal
- torch.ones
- torch.ones_like
- torch.ops.higher_order.custom_function_call
- torch.optim.Adam
- torch.optim.Optimizer
- torch.optim.lr_scheduler.LRScheduler
- torch.optim.lr_scheduler.LambdaLR
- torch.outer
- torch.polar
- torch.pow
- torch.rand
- torch.randn
- torch.randn_like
- torch.random.fork_rng
- torch.random.get_rng_state
- torch.random.initial_seed
- torch.random.set_rng_state
- torch.randperm
- torch.repeat_interleave
- torch.roll
- torch.round
- torch.rsqrt
- torch.save
- torch.seed
- torch.set_default_dtype
- torch.set_rng_state
- torch.sigmoid
- torch.sin
- torch.sort
- torch.special.logit
- torch.split
- torch.spmm
- torch.sqrt
- torch.stack
- torch.sum
- torch.tan
- torch.tanh
- torch.tensor
- torch.testing.assert_close
- torch.topk
- torch.triu
- torch.unbind
- torch.unique
- torch.unique_consecutive
- torch.use_deterministic_algorithms
- torch.utils.checkpoint.checkpoint
- torch.utils.data.BatchSampler
- torch.utils.data.DataLoader
- torch.utils.data.DataLoader.multiprocessing_context.fget
- torch.utils.data.RandomSampler
- torch.view_as_complex
- torch.view_as_real
- torch.vmap
- torch.where
- torch.xpu.is_available
- torch.xpu.is_initialized
- torch.xpu.manual_seed_all
- torch.zeros
- torch.zeros_like
