- torch.BoolTensor
- torch.FloatTensor
- torch.Generator
- torch.HalfTensor
- torch.IntTensor
- torch.LongTensor
- torch.Size
- torch.Tensor
- torch.Tensor.T
- torch.Tensor.abs
- torch.Tensor.add
- torch.Tensor.add_
- torch.Tensor.all
- torch.Tensor.allclose
- torch.Tensor.amax
- torch.Tensor.aminmax
- torch.Tensor.any
- torch.Tensor.argmax
- torch.Tensor.argsort
- torch.Tensor.backward
- torch.Tensor.bfloat16
- torch.Tensor.bitwise_or_
- torch.Tensor.bool
- torch.Tensor.byte
- torch.Tensor.chunk
- torch.Tensor.clamp
- torch.Tensor.clamp_
- torch.Tensor.clip
- torch.Tensor.clone
- torch.Tensor.contiguous
- torch.Tensor.copy_
- torch.Tensor.cos
- torch.Tensor.cpu
- torch.Tensor.cuda
- torch.Tensor.cumsum
- torch.Tensor.data
- torch.Tensor.detach
- torch.Tensor.device
- torch.Tensor.diag
- torch.Tensor.dim
- torch.Tensor.div
- torch.Tensor.div_
- torch.Tensor.dtype
- torch.Tensor.eq
- torch.Tensor.erfinv_
- torch.Tensor.exp
- torch.Tensor.exp_
- torch.Tensor.expand
- torch.Tensor.expand_as
- torch.Tensor.exponential_
- torch.Tensor.fill_
- torch.Tensor.fill_diagonal_
- torch.Tensor.flatten
- torch.Tensor.flip
- torch.Tensor.float
- torch.Tensor.floor
- torch.Tensor.floor_
- torch.Tensor.gather
- torch.Tensor.get_device
- torch.Tensor.grad
- torch.Tensor.gt
- torch.Tensor.half
- torch.Tensor.histc
- torch.Tensor.index_add_
- torch.Tensor.index_select
- torch.Tensor.int
- torch.Tensor.is_contiguous
- torch.Tensor.is_cuda
- torch.Tensor.is_floating_point
- torch.Tensor.isnan
- torch.Tensor.item
- torch.Tensor.log
- torch.Tensor.long
- torch.Tensor.masked_fill
- torch.Tensor.masked_fill_
- torch.Tensor.masked_scatter
- torch.Tensor.max
- torch.Tensor.mean
- torch.Tensor.median
- torch.Tensor.min
- torch.Tensor.mul_
- torch.Tensor.nanmean
- torch.Tensor.nansum
- torch.Tensor.ndim
- torch.Tensor.ndimension
- torch.Tensor.ne
- torch.Tensor.nelement
- torch.Tensor.new_empty
- torch.Tensor.new_full
- torch.Tensor.new_ones
- torch.Tensor.new_tensor
- torch.Tensor.new_zeros
- torch.Tensor.nonzero
- torch.Tensor.norm
- torch.Tensor.normal_
- torch.Tensor.numel
- torch.Tensor.numpy
- torch.Tensor.permute
- torch.Tensor.pin_memory
- torch.Tensor.pow
- torch.Tensor.prod
- torch.Tensor.reciprocal
- torch.Tensor.record_stream
- torch.Tensor.register_hook
- torch.Tensor.repeat
- torch.Tensor.repeat_interleave
- torch.Tensor.requires_grad
- torch.Tensor.requires_grad_
- torch.Tensor.reshape
- torch.Tensor.roll
- torch.Tensor.round
- torch.Tensor.scatter_
- torch.Tensor.scatter_add
- torch.Tensor.scatter_add_
- torch.Tensor.shape
- torch.Tensor.sigmoid
- torch.Tensor.sin
- torch.Tensor.size
- torch.Tensor.softmax
- torch.Tensor.sort
- torch.Tensor.split
- torch.Tensor.squeeze
- torch.Tensor.std
- torch.Tensor.sub_
- torch.Tensor.sum
- torch.Tensor.t
- torch.Tensor.tile
- torch.Tensor.to
- torch.Tensor.tolist
- torch.Tensor.topk
- torch.Tensor.transpose
- torch.Tensor.tril
- torch.Tensor.type
- torch.Tensor.type_as
- torch.Tensor.unbind
- torch.Tensor.unflatten
- torch.Tensor.uniform_
- torch.Tensor.unique
- torch.Tensor.unsqueeze
- torch.Tensor.values
- torch.Tensor.var
- torch.Tensor.view
- torch.Tensor.view_as
- torch.Tensor.zero_
- torch._C._is_tracing
- torch._dynamo.config.accumulated_cache_size_limit
- torch._dynamo.config.cache_size_limit
- torch._foreach_add_
- torch._foreach_mul_
- torch._scaled_mm
- torch._utils._flatten_dense_tensors
- torch._utils._take_tensors
- torch._utils._unflatten_dense_tensors
- torch.abs
- torch.allclose
- torch.amp.autocast
- torch.any
- torch.arange
- torch.argmax
- torch.argmin
- torch.argsort
- torch.argwhere
- torch.as_tensor
- torch.autocast
- torch.autograd.Function
- torch.autograd.function.FunctionCtx.mark_non_differentiable
- torch.autograd.function.FunctionCtx.save_for_backward
- torch.autograd.function.FunctionCtx.saved_tensors
- torch.autograd.function.once_differentiable
- torch.autograd.profiler.profile.key_averages
- torch.autograd.profiler.profile.table
- torch.backends.cuda.matmul.allow_tf32
- torch.backends.cudnn.benchmark
- torch.backends.cudnn.deterministic
- torch.bfloat16
- torch.bmm
- torch.bool
- torch.bucketize
- torch.cat
- torch.channels_last
- torch.chunk
- torch.clamp
- torch.compile
- torch.complex64
- torch.contiguous_format
- torch.cos
- torch.cuda
- torch.cuda.FloatTensor
- torch.cuda.HalfTensor
- torch.cuda.IntTensor
- torch.cuda.LongTensor
- torch.cuda.Stream
- torch.cuda.amp.GradScaler
- torch.cuda.amp.autocast
- torch.cuda.current_device
- torch.cuda.current_stream
- torch.cuda.device
- torch.cuda.device_count
- torch.cuda.empty_cache
- torch.cuda.get_device_capability
- torch.cuda.get_device_name
- torch.cuda.get_device_properties
- torch.cuda.is_available
- torch.cuda.is_bf16_supported
- torch.cuda.manual_seed
- torch.cuda.manual_seed_all
- torch.cuda.max_memory_allocated
- torch.cuda.max_memory_reserved
- torch.cuda.mem_get_info
- torch.cuda.set_device
- torch.cuda.set_stream
- torch.cuda.stream
- torch.cuda.synchronize
- torch.cummax
- torch.cumsum
- torch.device
- torch.diag
- torch.diagonal
- torch.diff
- torch.distributed.ReduceOp.MAX
- torch.distributed.ReduceOp.SUM
- torch.distributed._initialized
- torch.distributed._tensor.experimental.implicit_replication
- torch.distributed.algorithms._checkpoint.checkpoint_wrapper.CheckpointImpl.NO_REENTRANT
- torch.distributed.algorithms._checkpoint.checkpoint_wrapper.apply_activation_checkpointing
- torch.distributed.all_gather
- torch.distributed.all_gather_object
- torch.distributed.all_reduce
- torch.distributed.all_to_all
- torch.distributed.all_to_all_single
- torch.distributed.barrier
- torch.distributed.broadcast
- torch.distributed.broadcast_object_list
- torch.distributed.destroy_process_group
- torch.distributed.device_mesh.init_device_mesh
- torch.distributed.fsdp.BackwardPrefetch
- torch.distributed.fsdp.CPUOffload
- torch.distributed.fsdp.FullStateDictConfig
- torch.distributed.fsdp.FullyShardedDataParallel
- torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type
- torch.distributed.fsdp.MixedPrecision
- torch.distributed.fsdp.ShardingStrategy
- torch.distributed.fsdp.ShardingStrategy.FULL_SHARD
- torch.distributed.fsdp.StateDictType.FULL_STATE_DICT
- torch.distributed.fsdp.StateDictType.LOCAL_STATE_DICT
- torch.distributed.fsdp._traversal_utils._get_fsdp_handles
- torch.distributed.gather_object
- torch.distributed.get_rank
- torch.distributed.get_world_size
- torch.distributed.group.WORLD
- torch.distributed.init_process_group
- torch.distributed.is_available
- torch.distributed.is_initialized
- torch.distributed.reduce
- torch.distributed.utils._free_storage
- torch.div
- torch.dot
- torch.dtype
- torch.einsum
- torch.empty
- torch.empty_like
- torch.eq
- torch.equal
- torch.exp
- torch.eye
- torch.finfo
- torch.flatten
- torch.flip
- torch.float16
- torch.float32
- torch.float64
- torch.float8_e4m3fn
- torch.floor
- torch.floor_divide
- torch.fmod
- torch.from_numpy
- torch.full
- torch.full_like
- torch.fx.wrap
- torch.gather
- torch.get_autocast_dtype
- torch.get_autocast_gpu_dtype
- torch.get_default_dtype
- torch.get_rng_state
- torch.gt
- torch.histc
- torch.hub.load
- torch.hub.load_state_dict_from_url
- torch.hub.urlparse
- torch.index_select
- torch.inf
- torch.int32
- torch.int64
- torch.int8
- torch.is_autocast_enabled
- torch.is_complex
- torch.is_floating_point
- torch.is_grad_enabled
- torch.is_tensor
- torch.isinf
- torch.isnan
- torch.jit.freeze
- torch.jit.ignore
- torch.jit.is_tracing
- torch.jit.trace
- torch.linalg.norm
- torch.linalg.solve
- torch.linalg.vector_norm
- torch.linspace
- torch.load
- torch.log
- torch.logical_and
- torch.long
- torch.manual_seed
- torch.masked_fill
- torch.matmul
- torch.max
- torch.maximum
- torch.mean
- torch.min
- torch.minimum
- torch.mul
- torch.multinomial
- torch.multiprocessing.Manager
- torch.multiprocessing.get_start_method
- torch.multiprocessing.set_start_method
- torch.multiprocessing.spawn
- torch.musa
- torch.musa.empty_cache
- torch.musa.get_rng_state
- torch.musa.get_rng_state_all
- torch.ne
- torch.nn.AdaptiveAvgPool1d
- torch.nn.AdaptiveAvgPool2d
- torch.nn.AdaptiveAvgPool3d
- torch.nn.AdaptiveMaxPool1d
- torch.nn.AdaptiveMaxPool2d
- torch.nn.AdaptiveMaxPool3d
- torch.nn.AvgPool1d
- torch.nn.AvgPool2d
- torch.nn.AvgPool3d
- torch.nn.BCEWithLogitsLoss
- torch.nn.BatchNorm1d
- torch.nn.BatchNorm2d
- torch.nn.BatchNorm3d
- torch.nn.Conv1d
- torch.nn.Conv2d
- torch.nn.Conv3d
- torch.nn.ConvTranspose2d
- torch.nn.ConvTranspose3d
- torch.nn.CrossEntropyLoss
- torch.nn.DataParallel
- torch.nn.Dropout
- torch.nn.Dropout2d
- torch.nn.ELU
- torch.nn.Embedding
- torch.nn.GELU
- torch.nn.GroupNorm
- torch.nn.Identity
- torch.nn.InstanceNorm1d
- torch.nn.InstanceNorm2d
- torch.nn.InstanceNorm3d
- torch.nn.LayerNorm
- torch.nn.LeakyReLU
- torch.nn.Linear
- torch.nn.MSELoss
- torch.nn.MaxPool1d
- torch.nn.MaxPool2d
- torch.nn.MaxPool3d
- torch.nn.Module
- torch.nn.Module.add_module
- torch.nn.Module.buffers
- torch.nn.Module.children
- torch.nn.Module.eval
- torch.nn.Module.get_submodule
- torch.nn.Module.load_state_dict
- torch.nn.Module.modules
- torch.nn.Module.named_children
- torch.nn.Module.named_modules
- torch.nn.Module.named_parameters
- torch.nn.Module.parameters
- torch.nn.Module.register_buffer
- torch.nn.Module.register_forward_hook
- torch.nn.Module.register_parameter
- torch.nn.Module.state_dict
- torch.nn.Module.to
- torch.nn.Module.train
- torch.nn.Module.zero_grad
- torch.nn.ModuleList
- torch.nn.MultiheadAttention
- torch.nn.PReLU
- torch.nn.RReLU
- torch.nn.ReLU
- torch.nn.ReLU6
- torch.nn.ReflectionPad2d
- torch.nn.ReplicationPad2d
- torch.nn.Sequential
- torch.nn.SiLU
- torch.nn.Sigmoid
- torch.nn.Softmax
- torch.nn.SyncBatchNorm
- torch.nn.SyncBatchNorm2d
- torch.nn.Tanh
- torch.nn.Unflatten
- torch.nn.Unfold
- torch.nn.Upsample
- torch.nn.ZeroPad2d
- torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION
- torch.nn.attention.flex_attention.and_masks
- torch.nn.attention.flex_attention.create_block_mask
- torch.nn.attention.flex_attention.flex_attention
- torch.nn.attention.flex_attention.or_masks
- torch.nn.attention.sdpa_kernel
- torch.nn.functional
- torch.nn.functional._Reduction.get_enum
- torch.nn.functional.adaptive_avg_pool2d
- torch.nn.functional.affine_grid
- torch.nn.functional.avg_pool2d
- torch.nn.functional.batch_norm
- torch.nn.functional.binary_cross_entropy_with_logits
- torch.nn.functional.conv2d
- torch.nn.functional.cross_entropy
- torch.nn.functional.dropout
- torch.nn.functional.gelu
- torch.nn.functional.grid_sample
- torch.nn.functional.interpolate
- torch.nn.functional.leaky_relu
- torch.nn.functional.linear
- torch.nn.functional.log_softmax
- torch.nn.functional.logsigmoid
- torch.nn.functional.max_pool2d
- torch.nn.functional.normalize
- torch.nn.functional.one_hot
- torch.nn.functional.pad
- torch.nn.functional.pixel_shuffle
- torch.nn.functional.relu
- torch.nn.functional.relu_
- torch.nn.functional.scaled_dot_product_attention
- torch.nn.functional.sigmoid
- torch.nn.functional.silu
- torch.nn.functional.softmax
- torch.nn.functional.softplus
- torch.nn.functional.tanh
- torch.nn.init._calculate_fan_in_and_fan_out
- torch.nn.init.constant_
- torch.nn.init.kaiming_normal_
- torch.nn.init.kaiming_uniform_
- torch.nn.init.normal_
- torch.nn.init.ones_
- torch.nn.init.orthogonal_
- torch.nn.init.trunc_normal_
- torch.nn.init.uniform_
- torch.nn.init.xavier_normal_
- torch.nn.init.xavier_uniform_
- torch.nn.init.zeros_
- torch.nn.modules.batchnorm._BatchNorm
- torch.nn.modules.module._IncompatibleKeys
- torch.nn.modules.utils._pair
- torch.nn.modules.utils._triple
- torch.nn.nn.LayerNorm
- torch.nn.parallel.DistributedDataParallel
- torch.nn.parallel._functions._get_stream
- torch.nn.parallel.distributed._find_tensors
- torch.nn.parameter.Parameter
- torch.nn.quantized.FloatFunctional
- torch.nn.utils.clip_grad.clip_grad_norm_
- torch.nn.utils.clip_grad_norm_
- torch.nn.utils.rnn.pad_sequence
- torch.nn.utils.spectral_norm
- torch.no_grad
- torch.nonzero
- torch.norm
- torch.ones
- torch.ones_like
- torch.optim.Adagrad
- torch.optim.AdamW
- torch.optim.Optimizer
- torch.optim.Optimizer.step
- torch.optim.SGD
- torch.optim.lr_scheduler.LambdaLR
- torch.optim.lr_scheduler.ReduceLROnPlateau
- torch.outer
- torch.polar
- torch.pow
- torch.prod
- torch.profiler.ProfilerActivity.CPU
- torch.profiler.ProfilerActivity.CUDA
- torch.profiler.profile
- torch.profiler.schedule
- torch.profiler.tensorboard_trace_handler
- torch.quantile
- torch.quantization.fuse_modules
- torch.rand
- torch.rand_like
- torch.randint
- torch.randn
- torch.randn_like
- torch.randperm
- torch.relu
- torch.repeat_interleave
- torch.roll
- torch.round
- torch.rsqrt
- torch.save
- torch.serialization.safe_globals
- torch.set_rng_state
- torch.sin
- torch.softmax
- torch.sort
- torch.special.expm1
- torch.special.logsumexp
- torch.split
- torch.sqrt
- torch.square
- torch.squeeze
- torch.stack
- torch.std
- torch.sum
- torch.tensor
- torch.topk
- torch.torch.int32
- torch.tril
- torch.triu
- torch.uint8
- torch.unbind
- torch.unique
- torch.unique_consecutive
- torch.utils.checkpoint.checkpoint
- torch.utils.cpp_extension.ROCM_HOME
- torch.utils.cpp_extension.load
- torch.utils.data.ConcatDataset
- torch.utils.data.DataLoader
- torch.utils.data.Dataset
- torch.utils.data.IterableDataset
- torch.utils.data.RandomSampler
- torch.utils.data.Sampler
- torch.utils.data.SequentialSampler
- torch.utils.data.dataset.ChainDataset
- torch.utils.data.dataset.IterableDataset
- torch.utils.data.default_collate
- torch.utils.data.distributed.DistributedSampler
- torch.utils.data.get_worker_info
- torch.var
- torch.version.hip
- torch.view_as_complex
- torch.view_as_real
- torch.where
- torch.zeros
- torch.zeros_like
