2025-07-30 10:36:06.141096 test begin: paddle.Tensor.__getitem__(Tensor([10, 7576, 12800],"bfloat16"), slice(None,-3,None), )
W0730 10:36:28.259945 131152 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0730 10:36:39.378450 131152 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 2715238400, memory's size is 1939456000.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):2715238400 > memory_size():1939456000.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

[Prof] paddle.Tensor.__getitem__ 	 paddle.Tensor.__getitem__(Tensor([10, 7576, 12800],"bfloat16"), slice(None,-3,None), ) 	 969728000 	 1000 	 0.005513429641723633 	 0.005868673324584961 	 3.0040740966796875e-05 	 3.886222839355469e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:36:49.264970 test begin: paddle.Tensor.__getitem__(Tensor([10, 7576, 16770],"bfloat16"), slice(None,-3,None), )
W0730 10:37:22.800813 132233 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 3557386560, memory's size is 2540990464.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):3557386560 > memory_size():2540990464.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

[Prof] paddle.Tensor.__getitem__ 	 paddle.Tensor.__getitem__(Tensor([10, 7576, 16770],"bfloat16"), slice(None,-3,None), ) 	 1270495200 	 1000 	 0.005478858947753906 	 0.005299568176269531 	 1.8596649169921875e-05 	 3.0279159545898438e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:37:35.476646 test begin: paddle.Tensor.__getitem__(Tensor([10, 7712, 12800],"bfloat16"), slice(None,-2,None), )
W0730 10:38:11.537091 132788 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 3158835200, memory's size is 1974272000.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):3158835200 > memory_size():1974272000.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

[Prof] paddle.Tensor.__getitem__ 	 paddle.Tensor.__getitem__(Tensor([10, 7712, 12800],"bfloat16"), slice(None,-2,None), ) 	 987136000 	 1000 	 0.010125398635864258 	 0.005468606948852539 	 2.6941299438476562e-05 	 3.123283386230469e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:38:22.214823 test begin: paddle.Tensor.__getitem__(Tensor([10, 7712, 16470],"bfloat16"), slice(None,-2,None), )
W0730 10:38:57.652706 132932 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 4064532480, memory's size is 2540332800.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):4064532480 > memory_size():2540332800.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

[Prof] paddle.Tensor.__getitem__ 	 paddle.Tensor.__getitem__(Tensor([10, 7712, 16470],"bfloat16"), slice(None,-2,None), ) 	 1270166400 	 1000 	 0.0054318904876708984 	 0.005346536636352539 	 2.002716064453125e-05 	 2.47955322265625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:39:10.055888 test begin: paddle.Tensor.__getitem__(Tensor([10, 8168, 12800],"bfloat16"), slice(None,-6,None), )
W0730 10:39:34.895010 133509 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.Tensor.__getitem__ 	 paddle.Tensor.__getitem__(Tensor([10, 8168, 12800],"bfloat16"), slice(None,-6,None), ) 	 1045504000 	 1000 	 0.00545191764831543 	 0.00525665283203125 	 1.8596649169921875e-05 	 2.2411346435546875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:39:44.526994 test begin: paddle.Tensor.__getitem__(Tensor([10, 8168, 15550],"bfloat16"), slice(None,-6,None), )
W0730 10:40:12.035447 133637 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.Tensor.__getitem__ 	 paddle.Tensor.__getitem__(Tensor([10, 8168, 15550],"bfloat16"), slice(None,-6,None), ) 	 1270124000 	 1000 	 0.005526542663574219 	 0.005271434783935547 	 3.361701965332031e-05 	 2.1219253540039062e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:40:19.742499 test begin: paddle.Tensor.__getitem__(Tensor([10, 9923, 12800],"bfloat16"), slice(None,-2,None), )
W0730 10:40:59.990855 133749 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 4064460800, memory's size is 2540288000.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):4064460800 > memory_size():2540288000.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

[Prof] paddle.Tensor.__getitem__ 	 paddle.Tensor.__getitem__(Tensor([10, 9923, 12800],"bfloat16"), slice(None,-2,None), ) 	 1270144000 	 1000 	 0.012268781661987305 	 0.005247831344604492 	 5.745887756347656e-05 	 3.719329833984375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:41:16.825920 test begin: paddle.Tensor.__getitem__(Tensor([10, 9923, 12800],"bfloat16"), slice(None,-3,None), )
W0730 10:41:52.899925 134699 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 3556403200, memory's size is 2540288000.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):3556403200 > memory_size():2540288000.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

[Prof] paddle.Tensor.__getitem__ 	 paddle.Tensor.__getitem__(Tensor([10, 9923, 12800],"bfloat16"), slice(None,-3,None), ) 	 1270144000 	 1000 	 0.00560760498046875 	 0.00984954833984375 	 9.059906005859375e-06 	 3.647804260253906e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:42:05.376308 test begin: paddle.Tensor.__getitem__(Tensor([10, 9923, 12800],"bfloat16"), slice(None,-6,None), )
W0730 10:42:31.702056 135611 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.Tensor.__getitem__ 	 paddle.Tensor.__getitem__(Tensor([10, 9923, 12800],"bfloat16"), slice(None,-6,None), ) 	 1270144000 	 1000 	 0.005565643310546875 	 0.009770631790161133 	 1.7881393432617188e-05 	 2.2172927856445312e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:42:40.158253 test begin: paddle.Tensor.__len__(Tensor([1000, 1352, 376],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([1000, 1352, 376],"float32"), ) 	 508352000 	 1000 	 0.0047833919525146484 	 0.005105733871459961 	 6.4373016357421875e-06 	 2.09808349609375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:42:47.977548 test begin: paddle.Tensor.__len__(Tensor([1000, 376, 1352],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([1000, 376, 1352],"float32"), ) 	 508352000 	 1000 	 0.004750728607177734 	 0.004967927932739258 	 7.62939453125e-06 	 2.1696090698242188e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:42:57.005467 test begin: paddle.Tensor.__len__(Tensor([1000000, 509],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([1000000, 509],"float32"), ) 	 509000000 	 1000 	 0.004739046096801758 	 0.005051136016845703 	 7.152557373046875e-06 	 1.8596649169921875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:43:05.122980 test begin: paddle.Tensor.__len__(Tensor([230, 1501, 1501],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([230, 1501, 1501],"float32"), ) 	 518190230 	 1000 	 0.004731655120849609 	 0.004927635192871094 	 6.4373016357421875e-06 	 2.0265579223632812e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:43:13.285595 test begin: paddle.Tensor.__len__(Tensor([3600, 376, 376],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([3600, 376, 376],"float32"), ) 	 508953600 	 1000 	 0.004741668701171875 	 0.00864100456237793 	 6.67572021484375e-06 	 6.580352783203125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:43:21.234725 test begin: paddle.Tensor.__len__(Tensor([500, 1501, 677],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([500, 1501, 677],"float32"), ) 	 508088500 	 1000 	 0.004859447479248047 	 0.004901885986328125 	 6.67572021484375e-06 	 1.811981201171875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:43:29.121483 test begin: paddle.Tensor.__len__(Tensor([500, 677, 1501],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([500, 677, 1501],"float32"), ) 	 508088500 	 1000 	 0.004796028137207031 	 0.005982160568237305 	 6.9141387939453125e-06 	 5.7220458984375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:43:37.516277 test begin: paddle.Tensor.__len__(Tensor([5080330, 100],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([5080330, 100],"float32"), ) 	 508033000 	 1000 	 0.004645347595214844 	 0.0048618316650390625 	 6.4373016357421875e-06 	 1.811981201171875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:43:45.451746 test begin: paddle.Tensor.all(Tensor([10, 1, 2048, 24807],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([10, 1, 2048, 24807],"bool"), ) 	 508047360 	 1000 	 0.464008092880249 	 0.5068209171295166 	 0.23705053329467773 	 0.2589430809020996 	 None 	 None 	 None 	 None 	 
2025-07-30 10:43:53.557888 test begin: paddle.Tensor.all(Tensor([10, 1, 24807, 2048],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([10, 1, 24807, 2048],"bool"), ) 	 508047360 	 1000 	 0.46394944190979004 	 0.5068392753601074 	 0.23706293106079102 	 0.2589836120605469 	 None 	 None 	 None 	 None 	 
2025-07-30 10:44:02.038892 test begin: paddle.Tensor.all(Tensor([10, 13, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([10, 13, 2048, 2048],"bool"), ) 	 545259520 	 1000 	 0.49897170066833496 	 0.5443041324615479 	 0.25495338439941406 	 0.278134822845459 	 None 	 None 	 None 	 None 	 
2025-07-30 10:44:11.435274 test begin: paddle.Tensor.all(Tensor([130, 1, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([130, 1, 2048, 2048],"bool"), ) 	 545259520 	 1000 	 0.49893808364868164 	 0.5443787574768066 	 0.2549457550048828 	 0.27818727493286133 	 None 	 None 	 None 	 None 	 
2025-07-30 10:44:19.902271 test begin: paddle.Tensor.all(Tensor([1590, 10, 32000],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([1590, 10, 32000],"bool"), ) 	 508800000 	 1000 	 0.46822214126586914 	 0.5092377662658691 	 0.23981738090515137 	 0.2601962089538574 	 None 	 None 	 None 	 None 	 
2025-07-30 10:44:28.108298 test begin: paddle.Tensor.all(Tensor([20, 10, 2540161],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([20, 10, 2540161],"bool"), ) 	 508032200 	 1000 	 0.46770334243774414 	 0.5076315402984619 	 0.23901653289794922 	 0.25945258140563965 	 None 	 None 	 None 	 None 	 
2025-07-30 10:44:37.164205 test begin: paddle.Tensor.all(Tensor([20, 100, 256000],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([20, 100, 256000],"bool"), ) 	 512000000 	 1000 	 0.468250036239624 	 0.5182623863220215 	 0.23925328254699707 	 0.26482272148132324 	 None 	 None 	 None 	 None 	 
2025-07-30 10:44:45.795833 test begin: paddle.Tensor.all(Tensor([20, 794, 32000],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([20, 794, 32000],"bool"), ) 	 508160000 	 1000 	 0.46607422828674316 	 0.5086464881896973 	 0.23814964294433594 	 0.25934267044067383 	 None 	 None 	 None 	 None 	 
2025-07-30 10:44:53.906592 test begin: paddle.Tensor.all(Tensor([200, 10, 256000],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([200, 10, 256000],"bool"), ) 	 512000000 	 1000 	 0.4682478904724121 	 0.5181310176849365 	 0.23927617073059082 	 0.264725923538208 	 None 	 None 	 None 	 None 	 
2025-07-30 10:45:01.775866 test begin: paddle.Tensor.any(Tensor([10, 1379, 192, 192],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 1379, 192, 192],"bool"), axis=list[2,3,], ) 	 508354560 	 1000 	 0.49581074714660645 	 0.5528535842895508 	 0.25335216522216797 	 0.5382153987884521 	 None 	 None 	 None 	 None 	 
2025-07-30 10:45:09.820907 test begin: paddle.Tensor.any(Tensor([10, 1501, 184, 184],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 1501, 184, 184],"bool"), axis=list[2,3,], ) 	 508178560 	 1000 	 0.5065038204193115 	 0.5686812400817871 	 0.2587907314300537 	 0.5539538860321045 	 None 	 None 	 None 	 None 	 
2025-07-30 10:45:18.099152 test begin: paddle.Tensor.any(Tensor([10, 300, 184, 921],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 300, 184, 921],"bool"), axis=list[2,3,], ) 	 508392000 	 1000 	 0.6389646530151367 	 0.5274615287780762 	 0.32647204399108887 	 0.5124473571777344 	 None 	 None 	 None 	 None 	 
2025-07-30 10:45:26.378781 test begin: paddle.Tensor.any(Tensor([10, 300, 192, 883],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 300, 192, 883],"bool"), axis=list[2,3,], ) 	 508608000 	 1000 	 0.54221510887146 	 0.5249738693237305 	 0.27706050872802734 	 0.5101191997528076 	 None 	 None 	 None 	 None 	 
2025-07-30 10:45:34.541637 test begin: paddle.Tensor.any(Tensor([10, 300, 883, 192],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 300, 883, 192],"bool"), axis=list[2,3,], ) 	 508608000 	 1000 	 0.542172908782959 	 0.5249567031860352 	 0.27704763412475586 	 0.510120153427124 	 None 	 None 	 None 	 None 	 
2025-07-30 10:45:42.629997 test begin: paddle.Tensor.any(Tensor([10, 300, 921, 184],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 300, 921, 184],"bool"), axis=list[2,3,], ) 	 508392000 	 1000 	 0.638967752456665 	 0.5274572372436523 	 0.32648372650146484 	 0.512474775314331 	 None 	 None 	 None 	 None 	 
2025-07-30 10:45:50.906504 test begin: paddle.Tensor.any(Tensor([100, 300, 136, 136],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([100, 300, 136, 136],"bool"), axis=list[2,3,], ) 	 554880000 	 1000 	 0.5394537448883057 	 0.7132153511047363 	 0.5270800590515137 	 0.698352575302124 	 None 	 None 	 None 	 None 	 
2025-07-30 10:45:59.773540 test begin: paddle.Tensor.any(Tensor([20, 1374, 136, 136],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([20, 1374, 136, 136],"bool"), axis=list[2,3,], ) 	 508270080 	 1000 	 0.49483728408813477 	 0.6555702686309814 	 0.4811375141143799 	 0.6408467292785645 	 None 	 None 	 None 	 None 	 
2025-07-30 10:46:08.022137 test begin: paddle.Tensor.any(Tensor([20, 300, 136, 623],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([20, 300, 136, 623],"bool"), axis=list[2,3,], ) 	 508368000 	 1000 	 0.620936393737793 	 0.5353412628173828 	 0.3172900676727295 	 0.5206704139709473 	 None 	 None 	 None 	 None 	 
2025-07-30 10:46:16.239397 test begin: paddle.Tensor.any(Tensor([20, 300, 623, 136],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([20, 300, 623, 136],"bool"), axis=list[2,3,], ) 	 508368000 	 1000 	 0.6209428310394287 	 0.535362720489502 	 0.31728458404541016 	 0.5205545425415039 	 None 	 None 	 None 	 None 	 
2025-07-30 10:46:24.496242 test begin: paddle.Tensor.any(Tensor([50, 300, 192, 192],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([50, 300, 192, 192],"bool"), axis=list[2,3,], ) 	 552960000 	 1000 	 0.5367298126220703 	 0.5987229347229004 	 0.27424120903015137 	 0.5839898586273193 	 None 	 None 	 None 	 None 	 
2025-07-30 10:46:33.211690 test begin: paddle.Tensor.any(Tensor([60, 300, 184, 184],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([60, 300, 184, 184],"bool"), axis=list[2,3,], ) 	 609408000 	 1000 	 0.6038703918457031 	 0.679638147354126 	 0.30857038497924805 	 0.6631438732147217 	 None 	 None 	 None 	 None 	 
2025-07-30 10:46:42.940625 test begin: paddle.Tensor.astype(Tensor([10, 32, 388, 4096],"float32"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([10, 32, 388, 4096],"float32"), "float32", ) 	 508559360 	 1000 	 0.004170417785644531 	 0.002295970916748047 	 7.271766662597656e-05 	 1.71661376953125e-05 	 0.03145575523376465 	 0.04492497444152832 	 3.528594970703125e-05 	 7.510185241699219e-05 	 
2025-07-30 10:46:58.831052 test begin: paddle.Tensor.astype(Tensor([10, 32, 4096, 388],"float32"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([10, 32, 4096, 388],"float32"), "float32", ) 	 508559360 	 1000 	 0.003238201141357422 	 0.0035021305084228516 	 1.6450881958007812e-05 	 2.5033950805664062e-05 	 0.029354572296142578 	 0.06622982025146484 	 3.457069396972656e-05 	 7.486343383789062e-05 	 
2025-07-30 10:47:19.471822 test begin: paddle.Tensor.astype(Tensor([10, 4, 4096, 4096],"float32"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([10, 4, 4096, 4096],"float32"), "float32", ) 	 671088640 	 1000 	 0.004781961441040039 	 0.002339601516723633 	 1.3589859008789062e-05 	 1.7404556274414062e-05 	 0.030463218688964844 	 0.046120405197143555 	 8.940696716308594e-05 	 9.202957153320312e-05 	 
2025-07-30 10:47:40.656408 test begin: paddle.Tensor.detach(Tensor([1003520, 1013],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([1003520, 1013],"bfloat16"), ) 	 1016565760 	 1000 	 0.0008492469787597656 	 0.005112171173095703 	 3.814697265625e-05 	 2.288818359375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:48:23.790109 test begin: paddle.Tensor.detach(Tensor([10130, 100352],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([10130, 100352],"bfloat16"), ) 	 1016565760 	 1000 	 0.001491546630859375 	 0.0029807090759277344 	 5.0067901611328125e-05 	 2.2172927856445312e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:48:56.614661 test begin: paddle.Tensor.detach(Tensor([124040, 8192],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([124040, 8192],"bfloat16"), ) 	 1016135680 	 1000 	 0.0008959770202636719 	 0.005117177963256836 	 1.4781951904296875e-05 	 2.8133392333984375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:49:31.454862 test begin: paddle.Tensor.detach(Tensor([17720, 57344],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([17720, 57344],"bfloat16"), ) 	 1016135680 	 1000 	 0.0007593631744384766 	 0.0052547454833984375 	 1.049041748046875e-05 	 0.0001049041748046875 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:50:04.007781 test begin: paddle.Tensor.detach(Tensor([81920, 12404],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([81920, 12404],"bfloat16"), ) 	 1016135680 	 1000 	 0.0007555484771728516 	 0.0031518936157226562 	 7.62939453125e-06 	 3.647804260253906e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:50:37.115936 test begin: paddle.Tensor.dim(Tensor([1116160, 911],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([1116160, 911],"bfloat16"), ) 	 1016821760 	 1000 	 0.0007145404815673828 	 0.0016491413116455078 	 7.867813110351562e-06 	 1.621246337890625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:50:53.230242 test begin: paddle.Tensor.dim(Tensor([124040, 8192],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([124040, 8192],"bfloat16"), ) 	 1016135680 	 1000 	 0.0007205009460449219 	 0.002234220504760742 	 9.5367431640625e-06 	 2.1219253540039062e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:51:10.417918 test begin: paddle.Tensor.dim(Tensor([141760, 7168],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([141760, 7168],"bfloat16"), ) 	 1016135680 	 1000 	 0.0007052421569824219 	 0.0016493797302246094 	 2.5033950805664062e-05 	 2.5272369384765625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:51:26.437819 test begin: paddle.Tensor.dim(Tensor([71680, 14176],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([71680, 14176],"bfloat16"), ) 	 1016135680 	 1000 	 0.0006968975067138672 	 0.002529621124267578 	 1.0967254638671875e-05 	 7.271766662597656e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:51:42.018184 test begin: paddle.Tensor.dim(Tensor([9110, 111616],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([9110, 111616],"bfloat16"), ) 	 1016821760 	 1000 	 0.0007073879241943359 	 0.0016224384307861328 	 3.123283386230469e-05 	 1.6927719116210938e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:51:57.823493 test begin: paddle.Tensor.dim(Tensor([958720, 1060],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([958720, 1060],"bfloat16"), ) 	 1016243200 	 1000 	 0.0008580684661865234 	 0.002192974090576172 	 1.3589859008789062e-05 	 2.1219253540039062e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:52:13.704426 test begin: paddle.Tensor.equal_all(Tensor([2540160101],"int64"), Tensor([8],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([2540160101],"int64"), Tensor([8],"int64"), ) 	 2540160109 	 1000 	 0.01785755157470703 	 0.002791166305541992 	 1.3113021850585938e-05 	 1.9311904907226562e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:52:50.611182 test begin: paddle.Tensor.equal_all(Tensor([25401601],"int64"), Tensor([801],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([25401601],"int64"), Tensor([801],"int64"), ) 	 25402402 	 1000 	 0.01803112030029297 	 0.002743244171142578 	 1.2874603271484375e-05 	 1.6450881958007812e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:52:51.209755 test begin: paddle.Tensor.equal_all(Tensor([801, 3175201],"int64"), Tensor([801, 3],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([801, 3175201],"int64"), Tensor([801, 3],"int64"), ) 	 2543338404 	 1000 	 0.01783585548400879 	 0.002666950225830078 	 1.8835067749023438e-05 	 1.7642974853515625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:53:28.461851 test begin: paddle.Tensor.equal_all(Tensor([801, 3],"int64"), Tensor([801, 3175201],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([801, 3],"int64"), Tensor([801, 3175201],"int64"), ) 	 2543338404 	 1000 	 0.024094820022583008 	 0.00479435920715332 	 1.811981201171875e-05 	 1.9550323486328125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:54:07.570612 test begin: paddle.Tensor.equal_all(Tensor([801, 3],"int64"), Tensor([8467201, 3],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([801, 3],"int64"), Tensor([8467201, 3],"int64"), ) 	 25404006 	 1000 	 0.017832279205322266 	 0.0030317306518554688 	 1.8835067749023438e-05 	 4.458427429199219e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:54:08.197970 test begin: paddle.Tensor.equal_all(Tensor([801],"int64"), Tensor([25401601],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([801],"int64"), Tensor([25401601],"int64"), ) 	 25402402 	 1000 	 0.017905235290527344 	 0.004460334777832031 	 1.4781951904296875e-05 	 7.367134094238281e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:54:08.622905 test begin: paddle.Tensor.equal_all(Tensor([8467201, 3],"int64"), Tensor([801, 3],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8467201, 3],"int64"), Tensor([801, 3],"int64"), ) 	 25404006 	 1000 	 0.0176999568939209 	 0.0026786327362060547 	 1.6689300537109375e-05 	 1.52587890625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:54:09.121869 test begin: paddle.Tensor.equal_all(Tensor([846720101, 3],"int64"), Tensor([8, 3],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([846720101, 3],"int64"), Tensor([8, 3],"int64"), ) 	 2540160327 	 1000 	 0.017910480499267578 	 0.0027709007263183594 	 2.6941299438476562e-05 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:54:45.361960 test begin: paddle.Tensor.equal_all(Tensor([8],"int64"), Tensor([2540160101],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8],"int64"), Tensor([2540160101],"int64"), ) 	 2540160109 	 1000 	 0.0183413028717041 	 0.0027313232421875 	 3.886222839355469e-05 	 1.71661376953125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:55:43.123658 test begin: paddle.Tensor.fill_diagonal_(Tensor([1280, 396901],"float32"), 0, wrap=False, )
[Prof] paddle.Tensor.fill_diagonal_ 	 paddle.Tensor.fill_diagonal_(Tensor([1280, 396901],"float32"), 0, wrap=False, ) 	 508033280 	 1000 	 0.023996829986572266 	 0.011197328567504883 	 2.4080276489257812e-05 	 3.266334533691406e-05 	 0.0323030948638916 	 0.042748212814331055 	 2.86102294921875e-05 	 6.008148193359375e-05 	 combined
2025-07-30 10:55:59.744936 test begin: paddle.Tensor.fill_diagonal_(Tensor([3969010, 128],"float32"), 0, wrap=False, )
[Prof] paddle.Tensor.fill_diagonal_ 	 paddle.Tensor.fill_diagonal_(Tensor([3969010, 128],"float32"), 0, wrap=False, ) 	 508033280 	 1000 	 0.02451491355895996 	 0.01109004020690918 	 3.4809112548828125e-05 	 3.3855438232421875e-05 	 0.03224062919616699 	 0.045454978942871094 	 3.0994415283203125e-05 	 6.604194641113281e-05 	 combined
2025-07-30 10:56:17.379929 test begin: paddle.Tensor.flatten(Tensor([10, 64, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([10, 64, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, ) 	 1684480000 	 1000 	 0.006358146667480469 	 0.004571199417114258 	 1.5020370483398438e-05 	 2.2411346435546875e-05 	 0.04282641410827637 	 0.052240848541259766 	 4.76837158203125e-05 	 5.53131103515625e-05 	 
2025-07-30 10:57:10.135102 test begin: paddle.Tensor.flatten(Tensor([1280, 127, 56, 56],"float32"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 127, 56, 56],"float32"), 2, ) 	 509788160 	 1000 	 0.005589723587036133 	 0.0044825077056884766 	 9.059906005859375e-06 	 1.9550323486328125e-05 	 0.04318046569824219 	 0.05242586135864258 	 3.814697265625e-05 	 6.961822509765625e-05 	 
2025-07-30 10:57:26.293061 test begin: paddle.Tensor.flatten(Tensor([1280, 254, 56, 56],"float16"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 254, 56, 56],"float16"), 2, ) 	 1019576320 	 1000 	 0.005808115005493164 	 0.004471778869628906 	 1.811981201171875e-05 	 2.002716064453125e-05 	 0.04312539100646973 	 0.051720380783081055 	 5.412101745605469e-05 	 5.507469177246094e-05 	 
2025-07-30 10:58:04.215469 test begin: paddle.Tensor.flatten(Tensor([1280, 512, 14, 56],"float32"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 512, 14, 56],"float32"), 2, ) 	 513802240 	 1000 	 0.005635976791381836 	 0.004391670227050781 	 1.6689300537109375e-05 	 2.09808349609375e-05 	 0.04304671287536621 	 0.05474042892456055 	 6.961822509765625e-05 	 5.888938903808594e-05 	 
2025-07-30 10:58:22.294321 test begin: paddle.Tensor.flatten(Tensor([1280, 512, 28, 56],"float16"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 512, 28, 56],"float16"), 2, ) 	 1027604480 	 1000 	 0.0056972503662109375 	 0.004492759704589844 	 1.4543533325195312e-05 	 2.1219253540039062e-05 	 0.043442726135253906 	 0.05194354057312012 	 5.7697296142578125e-05 	 8.082389831542969e-05 	 
2025-07-30 10:59:00.401176 test begin: paddle.Tensor.flatten(Tensor([1280, 512, 56, 14],"float32"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 512, 56, 14],"float32"), 2, ) 	 513802240 	 1000 	 0.005772590637207031 	 0.0043828487396240234 	 2.0742416381835938e-05 	 1.9788742065429688e-05 	 0.04297041893005371 	 0.052227020263671875 	 3.743171691894531e-05 	 5.316734313964844e-05 	 
2025-07-30 10:59:16.997373 test begin: paddle.Tensor.flatten(Tensor([1280, 512, 56, 28],"float16"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 512, 56, 28],"float16"), 2, ) 	 1027604480 	 1000 	 0.0056324005126953125 	 0.00809931755065918 	 1.239776611328125e-05 	 6.413459777832031e-05 	 0.04304242134094238 	 0.05941152572631836 	 5.4836273193359375e-05 	 6.222724914550781e-05 	 
2025-07-30 10:59:55.404171 test begin: paddle.Tensor.flatten(Tensor([320, 512, 56, 56],"float32"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([320, 512, 56, 56],"float32"), 2, ) 	 513802240 	 1000 	 0.005503416061401367 	 0.00445556640625 	 8.58306884765625e-06 	 2.574920654296875e-05 	 0.042897701263427734 	 0.0600736141204834 	 4.6253204345703125e-05 	 7.867813110351562e-05 	 
2025-07-30 11:00:13.726144 test begin: paddle.Tensor.flatten(Tensor([40, 5, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([40, 5, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, ) 	 526400000 	 1000 	 0.006083965301513672 	 0.004732608795166016 	 1.0967254638671875e-05 	 4.267692565917969e-05 	 0.04294013977050781 	 0.06239914894104004 	 4.100799560546875e-05 	 8.606910705566406e-05 	 
2025-07-30 11:00:30.611180 test begin: paddle.Tensor.flatten(Tensor([40, 64, 2, 376, 280],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([40, 64, 2, 376, 280],"float32"), start_axis=1, stop_axis=2, ) 	 539033600 	 1000 	 0.00607609748840332 	 0.006297111511230469 	 9.298324584960938e-06 	 4.9591064453125e-05 	 0.04309821128845215 	 0.05196022987365723 	 3.4332275390625e-05 	 4.124641418457031e-05 	 
2025-07-30 11:00:47.410309 test begin: paddle.Tensor.flatten(Tensor([40, 64, 25, 29, 280],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([40, 64, 25, 29, 280],"float32"), start_axis=1, stop_axis=2, ) 	 519680000 	 1000 	 0.006055116653442383 	 0.004624605178833008 	 1.239776611328125e-05 	 2.1219253540039062e-05 	 0.04290413856506348 	 0.05249190330505371 	 4.2438507080078125e-05 	 6.4849853515625e-05 	 
2025-07-30 11:01:04.081391 test begin: paddle.Tensor.flatten(Tensor([40, 64, 25, 376, 22],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([40, 64, 25, 376, 22],"float32"), start_axis=1, stop_axis=2, ) 	 529408000 	 1000 	 0.006155729293823242 	 0.004650115966796875 	 1.3113021850585938e-05 	 2.2649765014648438e-05 	 0.04288792610168457 	 0.06083536148071289 	 3.719329833984375e-05 	 7.605552673339844e-05 	 
2025-07-30 11:01:22.060310 test begin: paddle.Tensor.flatten(Tensor([640, 512, 56, 56],"float16"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([640, 512, 56, 56],"float16"), 2, ) 	 1027604480 	 1000 	 0.005653858184814453 	 0.004484653472900391 	 1.049041748046875e-05 	 2.3365020751953125e-05 	 0.043267011642456055 	 0.05134463310241699 	 3.933906555175781e-05 	 4.839897155761719e-05 	 
2025-07-30 11:02:00.218997 test begin: paddle.Tensor.gather(Tensor([40, 12700801],"float32"), Tensor([40, 1],"int64"), 1, )
[Prof] paddle.Tensor.gather 	 paddle.Tensor.gather(Tensor([40, 12700801],"float32"), Tensor([40, 1],"int64"), 1, ) 	 508032080 	 1000 	 0.017693519592285156 	 1.902761697769165 	 2.09808349609375e-05 	 7.605552673339844e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 11:02:11.797281 test begin: paddle.Tensor.gather(Tensor([400, 1270080],"float32"), Tensor([400, 1],"int64"), 1, )
[Prof] paddle.Tensor.gather 	 paddle.Tensor.gather(Tensor([400, 1270080],"float32"), Tensor([400, 1],"int64"), 1, ) 	 508032400 	 1000 	 0.01781940460205078 	 13.668656349182129 	 2.9325485229492188e-05 	 0.0002124309539794922 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 11:02:34.907230 test begin: paddle.Tensor.gather(Tensor([4000, 127008],"float32"), Tensor([4000, 1],"int64"), 1, )
[Prof] paddle.Tensor.gather 	 paddle.Tensor.gather(Tensor([4000, 127008],"float32"), Tensor([4000, 1],"int64"), 1, ) 	 508036000 	 1000 	 0.17172503471374512 	 152.6723039150238 	 0.16144680976867676 	 0.00026345252990722656 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 11:05:18.042284 test begin: paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([13001],"int64"), )
[Prof] paddle.Tensor.index_select 	 paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([13001],"int64"), ) 	 50816225 	 1000 	 0.009988546371459961 	 0.013584375381469727 	 1.3828277587890625e-05 	 4.9591064453125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 11:05:19.448606 test begin: paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([18201],"int64"), )
[Prof] paddle.Tensor.index_select 	 paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([18201],"int64"), ) 	 50821425 	 1000 	 0.009813785552978516 	 0.013348102569580078 	 1.6450881958007812e-05 	 3.838539123535156e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 11:05:20.418345 test begin: paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([9101],"int64"), )
[Prof] paddle.Tensor.index_select 	 paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([9101],"int64"), ) 	 50812325 	 1000 	 0.01000213623046875 	 0.013690948486328125 	 1.33514404296875e-05 	 4.696846008300781e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 11:05:21.353238 test begin: paddle.Tensor.index_select(Tensor([4004, 12689],"float32"), axis=0, index=Tensor([18201],"int64"), )
[Prof] paddle.Tensor.index_select 	 paddle.Tensor.index_select(Tensor([4004, 12689],"float32"), axis=0, index=Tensor([18201],"int64"), ) 	 50824957 	 1000 	 3.142441749572754 	 2.7540571689605713 	 3.1325418949127197 	 2.7400460243225098 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 11:05:33.882773 test begin: paddle.Tensor.is_complex(Tensor([201, 3, 100, 42337],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3, 100, 42337],"float64"), ) 	 2552921100 	 1000 	 0.003673553466796875 	 0.0017085075378417969 	 1.9550323486328125e-05 	 1.7881393432617188e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:06:22.848270 test begin: paddle.Tensor.is_complex(Tensor([201, 3, 105841, 40],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3, 105841, 40],"float64"), ) 	 2552884920 	 1000 	 0.003509044647216797 	 0.001684427261352539 	 1.2874603271484375e-05 	 1.6927719116210938e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:07:11.925301 test begin: paddle.Tensor.is_complex(Tensor([201, 3, 40, 105841],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3, 40, 105841],"float64"), ) 	 2552884920 	 1000 	 0.006745100021362305 	 0.001668691635131836 	 3.7670135498046875e-05 	 1.7642974853515625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:08:01.193070 test begin: paddle.Tensor.is_complex(Tensor([201, 3, 42337, 100],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3, 42337, 100],"float64"), ) 	 2552921100 	 1000 	 0.0035905838012695312 	 0.004756450653076172 	 9.298324584960938e-06 	 2.574920654296875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:08:50.295307 test begin: paddle.Tensor.is_complex(Tensor([201, 3176, 100, 40],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3176, 100, 40],"float64"), ) 	 2553504000 	 1000 	 0.003514528274536133 	 0.0016798973083496094 	 1.0251998901367188e-05 	 1.7881393432617188e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:09:41.256335 test begin: paddle.Tensor.is_complex(Tensor([201, 3176, 40, 100],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3176, 40, 100],"float64"), ) 	 2553504000 	 1000 	 0.0035457611083984375 	 0.0016868114471435547 	 7.867813110351562e-06 	 1.71661376953125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:10:30.002602 test begin: paddle.Tensor.is_complex(Tensor([211701, 3, 100, 40],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([211701, 3, 100, 40],"float64"), ) 	 2540412000 	 1000 	 0.0036115646362304688 	 0.001680612564086914 	 7.867813110351562e-06 	 1.6689300537109375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:11:28.468515 test begin: paddle.Tensor.is_complex(Tensor([211701, 3, 40, 100],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([211701, 3, 40, 100],"float64"), ) 	 2540412000 	 1000 	 0.003538370132446289 	 0.0030617713928222656 	 1.049041748046875e-05 	 6.031990051269531e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:12:17.237084 test begin: paddle.Tensor.is_complex(Tensor([301, 100, 84673],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([301, 100, 84673],"float64"), ) 	 2548657300 	 1000 	 0.0034873485565185547 	 0.0016491413116455078 	 7.3909759521484375e-06 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:13:06.621017 test begin: paddle.Tensor.is_complex(Tensor([301, 211681, 40],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([301, 211681, 40],"float64"), ) 	 2548639240 	 1000 	 0.003525257110595703 	 0.0016155242919921875 	 1.2636184692382812e-05 	 1.621246337890625e-05 	 None 	 None 	 None 	 None 	 
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 9, in <module>
    from tester import (APIConfig, APITestAccuracy, APITestCINNVSDygraph,
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/__init__.py", line 74, in __getattr__
    from .api_config import APIConfig
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/__init__.py", line 22, in __getattr__
    from .config_analyzer import APIConfig
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py", line 8, in <module>
    import paddle
  File "/usr/local/lib/python3.10/dist-packages/paddle/__init__.py", line 38, in <module>
    from .base import core  # noqa: F401
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/__init__.py", line 205, in <module>
    __bootstrap__()
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/__init__.py", line 197, in __bootstrap__
    core.init_devices()
KeyboardInterrupt
2025-07-30 10:36:08.669075 test begin: paddle.Tensor.is_complex(Tensor([635101, 100, 40],"float64"), )
W0730 10:37:07.501561 131264 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([635101, 100, 40],"float64"), ) 	 2540404000 	 1000 	 0.0037755966186523438 	 0.0020377635955810547 	 1.239776611328125e-05 	 3.218650817871094e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:37:13.957092 test begin: paddle.Tensor.item(Tensor([201, 1, 12700801],"int64"), 0, )
[Prof] paddle.Tensor.item 	 paddle.Tensor.item(Tensor([201, 1, 12700801],"int64"), 0, ) 	 2552861001 	 1000 	 0.020547151565551758 	 0.02856612205505371 	 1.5020370483398438e-05 	 6.580352783203125e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:37:52.163623 test begin: paddle.Tensor.item(Tensor([201, 12700801, 1],"int64"), 0, )
[Prof] paddle.Tensor.item 	 paddle.Tensor.item(Tensor([201, 12700801, 1],"int64"), 0, ) 	 2552861001 	 1000 	 0.02036261558532715 	 0.028751611709594727 	 2.5510787963867188e-05 	 6.842613220214844e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:38:53.214706 test begin: paddle.Tensor.item(Tensor([2540160101, 1, 1],"int64"), 0, )
[Prof] paddle.Tensor.item 	 paddle.Tensor.item(Tensor([2540160101, 1, 1],"int64"), 0, ) 	 2540160101 	 1000 	 0.019735097885131836 	 0.02837347984313965 	 1.2874603271484375e-05 	 6.508827209472656e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:40:12.213844 test begin: paddle.Tensor.logical_not(Tensor([508032010],"bool"), )
[Prof] paddle.Tensor.logical_not 	 paddle.Tensor.logical_not(Tensor([508032010],"bool"), ) 	 508032010 	 1000 	 0.7855064868927002 	 0.7482457160949707 	 0.7766177654266357 	 0.7338624000549316 	 None 	 None 	 None 	 None 	 
2025-07-30 10:40:21.070621 test begin: paddle.Tensor.lu(Tensor([1693, 300],"float32"), )
/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:924: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
LU, pivots = torch.lu(A, compute_pivots)
should be replaced with
LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
and
LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
should be replaced with
LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2055.)
  LU, pivots, infos = torch._lu_with_info(
[Prof] paddle.Tensor.lu 	 paddle.Tensor.lu(Tensor([1693, 300],"float32"), ) 	 507900 	 1000 	 2.9299604892730713 	 15.549713850021362 	 8.988380432128906e-05 	 0.0005061626434326172 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:40:40.872888 test begin: paddle.Tensor.lu(Tensor([301, 1193],"float32"), )
[Prof] paddle.Tensor.lu 	 paddle.Tensor.lu(Tensor([301, 1193],"float32"), ) 	 359093 	 1000 	 1.3767964839935303 	 5.952592849731445 	 5.5789947509765625e-05 	 0.00023674964904785156 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:40:48.795099 test begin: paddle.Tensor.lu(Tensor([301, 422, 3],"float64"), )
[Prof] paddle.Tensor.lu 	 paddle.Tensor.lu(Tensor([301, 422, 3],"float64"), ) 	 381066 	 1000 	 8.006834745407104 	 0.12302660942077637 	 0.00011515617370605469 	 9.465217590332031e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:41:00.235117 test begin: paddle.Tensor.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=0, destination=2, ) 	 254018100 	 1000 	 0.007591962814331055 	 0.0046999454498291016 	 1.2874603271484375e-05 	 2.4080276489257812e-05 	 0.0389864444732666 	 0.06543159484863281 	 3.0040740966796875e-05 	 6.794929504394531e-05 	 
2025-07-30 10:41:10.823512 test begin: paddle.Tensor.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254018100 	 1000 	 0.007649421691894531 	 0.010231971740722656 	 7.867813110351562e-06 	 2.47955322265625e-05 	 0.04658865928649902 	 0.09026288986206055 	 5.1975250244140625e-05 	 7.987022399902344e-05 	 
2025-07-30 10:41:21.775596 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 1058401],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 1058401],"float64"), source=0, destination=2, ) 	 254016240 	 1000 	 0.006748199462890625 	 0.004658937454223633 	 9.298324584960938e-06 	 2.6226043701171875e-05 	 0.039075374603271484 	 0.052611589431762695 	 4.0531158447265625e-05 	 7.009506225585938e-05 	 
2025-07-30 10:41:31.497888 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=0, destination=2, ) 	 254017680 	 1000 	 0.012115478515625 	 0.004614830017089844 	 1.8835067749023438e-05 	 2.0742416381835938e-05 	 0.039095163345336914 	 0.05591225624084473 	 3.8623809814453125e-05 	 8.535385131835938e-05 	 
2025-07-30 10:41:44.057976 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254017680 	 1000 	 0.007428646087646484 	 0.005694150924682617 	 1.2874603271484375e-05 	 2.1696090698242188e-05 	 0.0389707088470459 	 0.05281710624694824 	 2.384185791015625e-05 	 5.507469177246094e-05 	 
2025-07-30 10:41:55.303383 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=0, destination=2, ) 	 254017200 	 1000 	 0.007045745849609375 	 0.0045948028564453125 	 7.62939453125e-06 	 1.9550323486328125e-05 	 0.039084672927856445 	 0.05347561836242676 	 4.744529724121094e-05 	 7.772445678710938e-05 	 
2025-07-30 10:42:04.852116 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254017200 	 1000 	 0.007488727569580078 	 0.008588314056396484 	 9.059906005859375e-06 	 9.393692016601562e-05 	 0.03967118263244629 	 0.07012438774108887 	 6.866455078125e-05 	 0.00010061264038085938 	 
2025-07-30 10:42:16.213385 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 635041, 5],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 635041, 5],"float64"), source=0, destination=2, ) 	 254016400 	 1000 	 0.009204626083374023 	 0.0045130252838134766 	 4.5299530029296875e-05 	 2.0265579223632812e-05 	 0.03902244567871094 	 0.06450438499450684 	 3.4809112548828125e-05 	 8.130073547363281e-05 	 
2025-07-30 10:42:26.250262 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=0, destination=2, ) 	 254018800 	 1000 	 0.007089853286743164 	 0.008496999740600586 	 1.1920928955078125e-05 	 2.47955322265625e-05 	 0.0389246940612793 	 0.05347037315368652 	 4.2438507080078125e-05 	 6.461143493652344e-05 	 
2025-07-30 10:42:38.800313 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254018800 	 1000 	 0.012640714645385742 	 0.005644559860229492 	 1.4543533325195312e-05 	 2.2649765014648438e-05 	 0.03886246681213379 	 0.05298972129821777 	 4.124641418457031e-05 	 5.245208740234375e-05 	 
2025-07-30 10:42:51.229200 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 423361, 3, 5],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 423361, 3, 5],"float64"), source=0, destination=2, ) 	 254016600 	 1000 	 0.006780862808227539 	 0.004468202590942383 	 8.344650268554688e-06 	 2.4318695068359375e-05 	 0.03895425796508789 	 0.07078862190246582 	 4.649162292480469e-05 	 8.940696716308594e-05 	 
2025-07-30 10:43:02.730558 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=0, destination=2, ) 	 254020200 	 1000 	 0.01211404800415039 	 0.008495807647705078 	 1.0728836059570312e-05 	 2.2411346435546875e-05 	 0.04629921913146973 	 0.062296390533447266 	 3.790855407714844e-05 	 7.653236389160156e-05 	 
2025-07-30 10:43:15.424461 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254020200 	 1000 	 0.012489080429077148 	 0.010379552841186523 	 1.2636184692382812e-05 	 2.4080276489257812e-05 	 0.046143293380737305 	 0.06060194969177246 	 4.291534423828125e-05 	 6.079673767089844e-05 	 
2025-07-30 10:43:26.459795 test begin: paddle.Tensor.moveaxis(x=Tensor([8467210, 2, 3, 5],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([8467210, 2, 3, 5],"float64"), source=0, destination=2, ) 	 254016300 	 1000 	 0.011101007461547852 	 0.006946563720703125 	 1.0251998901367188e-05 	 2.5987625122070312e-05 	 0.042096853256225586 	 0.05360293388366699 	 3.743171691894531e-05 	 6.198883056640625e-05 	 
2025-07-30 10:43:38.719875 test begin: paddle.Tensor.rank(Tensor([2560, 1536, 3, 44],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 1536, 3, 44],"float32"), ) 	 519045120 	 1000 	 0.041806936264038086 	 0.029587268829345703 	 3.409385681152344e-05 	 6.67572021484375e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:43:46.830132 test begin: paddle.Tensor.rank(Tensor([2560, 1536, 44, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 1536, 44, 3],"float32"), ) 	 519045120 	 1000 	 0.04001736640930176 	 0.029630661010742188 	 3.457069396972656e-05 	 5.602836608886719e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:43:55.158743 test begin: paddle.Tensor.rank(Tensor([2560, 2048, 3, 33],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 2048, 3, 33],"float32"), ) 	 519045120 	 1000 	 0.04096412658691406 	 0.03129243850708008 	 2.9802322387695312e-05 	 7.796287536621094e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:44:04.045536 test begin: paddle.Tensor.rank(Tensor([2560, 2048, 33, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 2048, 33, 3],"float32"), ) 	 519045120 	 1000 	 0.04045224189758301 	 0.029482364654541016 	 3.314018249511719e-05 	 6.103515625e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:44:12.389552 test begin: paddle.Tensor.rank(Tensor([2560, 22051, 3, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 22051, 3, 3],"float32"), ) 	 508055040 	 1000 	 0.05266594886779785 	 0.037293195724487305 	 4.9591064453125e-05 	 9.489059448242188e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:44:21.045400 test begin: paddle.Tensor.rank(Tensor([2560, 768, 3, 87],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 768, 3, 87],"float32"), ) 	 513146880 	 1000 	 0.04049324989318848 	 0.02928638458251953 	 3.361701965332031e-05 	 8.034706115722656e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:44:28.983251 test begin: paddle.Tensor.rank(Tensor([2560, 768, 87, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 768, 87, 3],"float32"), ) 	 513146880 	 1000 	 0.04055476188659668 	 0.039260149002075195 	 3.123283386230469e-05 	 8.153915405273438e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:44:37.343149 test begin: paddle.Tensor.rank(Tensor([27570, 2048, 3, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([27570, 2048, 3, 3],"float32"), ) 	 508170240 	 1000 	 0.04013943672180176 	 0.030049562454223633 	 2.6464462280273438e-05 	 6.198883056640625e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:44:45.261896 test begin: paddle.Tensor.rank(Tensor([36760, 1536, 3, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([36760, 1536, 3, 3],"float32"), ) 	 508170240 	 1000 	 0.054531097412109375 	 0.04078793525695801 	 3.719329833984375e-05 	 7.390975952148438e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:44:54.570807 test begin: paddle.Tensor.rank(Tensor([73510, 768, 3, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([73510, 768, 3, 3],"float32"), ) 	 508101120 	 1000 	 0.05637693405151367 	 0.037976741790771484 	 4.100799560546875e-05 	 7.2479248046875e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:45:08.408369 test begin: paddle.Tensor.reshape(Tensor([124040, 8192],"bfloat16"), list[-1,8192,], )
[Prof] paddle.Tensor.reshape 	 paddle.Tensor.reshape(Tensor([124040, 8192],"bfloat16"), list[-1,8192,], ) 	 1016135680 	 1000 	 0.005288839340209961 	 0.004072904586791992 	 1.52587890625e-05 	 3.337860107421875e-05 	 0.04404902458190918 	 4.498018741607666 	 2.1219253540039062e-05 	 2.2993013858795166 	 
2025-07-30 10:45:45.383919 test begin: paddle.Tensor.set_(Tensor([2001],"bool"), Tensor([15, 3386881],"bool"), list[20,], list[2,], 0, )
[Prof] paddle.Tensor.set_ 	 paddle.Tensor.set_(Tensor([2001],"bool"), Tensor([15, 3386881],"bool"), list[20,], list[2,], 0, ) 	 50805216 	 1000 	 0.09440183639526367 	 0.0023026466369628906 	 3.504753112792969e-05 	 1.9311904907226562e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:45:46.264064 test begin: paddle.Tensor.set_(Tensor([2001],"bool"), Tensor([16934401, 3],"bool"), list[20,], list[2,], 0, )
[Prof] paddle.Tensor.set_ 	 paddle.Tensor.set_(Tensor([2001],"bool"), Tensor([16934401, 3],"bool"), list[20,], list[2,], 0, ) 	 50805204 	 1000 	 0.03563094139099121 	 0.0022819042205810547 	 2.6941299438476562e-05 	 1.5735626220703125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:45:47.034673 test begin: paddle.Tensor.set_(Tensor([50803201],"bool"), Tensor([1501, 3],"bool"), list[20,], list[2,], 0, )
[Prof] paddle.Tensor.set_ 	 paddle.Tensor.set_(Tensor([50803201],"bool"), Tensor([1501, 3],"bool"), list[20,], list[2,], 0, ) 	 50807704 	 1000 	 0.03549313545227051 	 0.0022962093353271484 	 2.5272369384765625e-05 	 1.6927719116210938e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:45:47.794094 test begin: paddle.Tensor.slice(Tensor([127008010, 4],"float32"), list[1,], list[0,], list[1,], )
[Prof] paddle.Tensor.slice 	 paddle.Tensor.slice(Tensor([127008010, 4],"float32"), list[1,], list[0,], list[1,], ) 	 508032040 	 1000 	 0.00733637809753418 	 0.013711929321289062 	 2.2172927856445312e-05 	 5.91278076171875e-05 	 4.860079050064087 	 4.627986669540405 	 2.4825336933135986 	 2.3641433715820312 	 combined
2025-07-30 10:46:07.330288 test begin: paddle.Tensor.slice(Tensor([40, 12700801],"float32"), list[1,], list[0,], list[1,], )
[Prof] paddle.Tensor.slice 	 paddle.Tensor.slice(Tensor([40, 12700801],"float32"), list[1,], list[0,], list[1,], ) 	 508032040 	 1000 	 0.00708460807800293 	 0.01329660415649414 	 1.049041748046875e-05 	 3.0040740966796875e-05 	 1.516669511795044 	 1.3186681270599365 	 0.7754054069519043 	 0.6745071411132812 	 combined
2025-07-30 10:46:18.224833 test begin: paddle.Tensor.slice_scatter(Tensor([80, 3175201],"float64"), Tensor([80, 3],"float64"), list[1,], list[0,], list[6,], list[2,], )
[Prof] paddle.Tensor.slice_scatter 	 paddle.Tensor.slice_scatter(Tensor([80, 3175201],"float64"), Tensor([80, 3],"float64"), list[1,], list[0,], list[6,], list[2,], ) 	 254016320 	 1000 	 0.014624834060668945 	 3.066767692565918 	 1.1205673217773438e-05 	 1.0420591831207275 	 3.098527669906616 	 3.06984543800354 	 0.5267410278320312 	 0.782811164855957 	 
2025-07-30 10:46:39.274249 test begin: paddle.Tensor.squeeze(Tensor([10, 2, 3840, 10240],"float32"), 0, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([10, 2, 3840, 10240],"float32"), 0, ) 	 786432000 	 1000 	 0.004374265670776367 	 0.004000186920166016 	 1.2159347534179688e-05 	 2.3365020751953125e-05 	 0.040288686752319336 	 0.05213737487792969 	 4.267692565917969e-05 	 6.008148193359375e-05 	 
2025-07-30 10:47:03.805687 test begin: paddle.Tensor.squeeze(Tensor([10, 3, 1654, 10240],"float32"), 0, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([10, 3, 1654, 10240],"float32"), 0, ) 	 508108800 	 1000 	 0.005827665328979492 	 0.006356000900268555 	 5.7220458984375e-05 	 3.552436828613281e-05 	 0.040474891662597656 	 0.05393576622009277 	 4.029273986816406e-05 	 5.316734313964844e-05 	 
2025-07-30 10:47:21.724044 test begin: paddle.Tensor.squeeze(Tensor([10, 3, 3840, 10240],"float32"), 0, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([10, 3, 3840, 10240],"float32"), 0, ) 	 1179648000 	 1000 	 0.004577159881591797 	 0.003954172134399414 	 2.574920654296875e-05 	 2.002716064453125e-05 	 0.04075741767883301 	 0.04850506782531738 	 4.506111145019531e-05 	 6.031990051269531e-05 	 
2025-07-30 10:47:58.543531 test begin: paddle.Tensor.squeeze(Tensor([10, 3, 3840, 4411],"float32"), 0, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([10, 3, 3840, 4411],"float32"), 0, ) 	 508147200 	 1000 	 0.008941411972045898 	 0.007621288299560547 	 1.1920928955078125e-05 	 2.4080276489257812e-05 	 0.04808807373046875 	 0.055175065994262695 	 4.029273986816406e-05 	 7.367134094238281e-05 	 
2025-07-30 10:48:16.087903 test begin: paddle.Tensor.squeeze(Tensor([160, 1, 125, 25500],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([160, 1, 125, 25500],"float32"), 1, ) 	 510000000 	 1000 	 0.009057998657226562 	 0.0077953338623046875 	 1.239776611328125e-05 	 2.4318695068359375e-05 	 0.048431396484375 	 0.05925250053405762 	 5.650520324707031e-05 	 5.91278076171875e-05 	 
2025-07-30 10:48:38.733337 test begin: paddle.Tensor.squeeze(Tensor([160, 1, 80, 39691],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([160, 1, 80, 39691],"float32"), 1, ) 	 508044800 	 1000 	 0.004416704177856445 	 0.0042493343353271484 	 8.344650268554688e-06 	 2.5033950805664062e-05 	 0.04077792167663574 	 0.05175280570983887 	 3.361701965332031e-05 	 5.626678466796875e-05 	 
2025-07-30 10:48:54.631591 test begin: paddle.Tensor.squeeze(Tensor([160, 2, 80, 25500],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([160, 2, 80, 25500],"float32"), 1, ) 	 652800000 	 1000 	 0.004344463348388672 	 0.003979921340942383 	 7.152557373046875e-06 	 2.3603439331054688e-05 	 0.04053449630737305 	 0.04963254928588867 	 3.910064697265625e-05 	 5.364418029785156e-05 	 
2025-07-30 10:49:15.320485 test begin: paddle.Tensor.squeeze(Tensor([2000, 1, 127009, 2],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([2000, 1, 127009, 2],"float32"), 1, ) 	 508036000 	 1000 	 0.004497528076171875 	 0.004148721694946289 	 8.344650268554688e-06 	 2.2649765014648438e-05 	 0.040476083755493164 	 0.05196809768676758 	 3.552436828613281e-05 	 6.604194641113281e-05 	 
2025-07-30 10:49:31.048751 test begin: paddle.Tensor.squeeze(Tensor([2000, 1, 37632, 7],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([2000, 1, 37632, 7],"float32"), 1, ) 	 526848000 	 1000 	 0.0044252872467041016 	 0.004169940948486328 	 6.4373016357421875e-06 	 2.3126602172851562e-05 	 0.0407557487487793 	 0.051788330078125 	 3.933906555175781e-05 	 6.4849853515625e-05 	 
2025-07-30 10:49:47.883401 test begin: paddle.Tensor.squeeze(Tensor([2000, 4, 37632, 2],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([2000, 4, 37632, 2],"float32"), 1, ) 	 602112000 	 1000 	 0.008964061737060547 	 0.007700443267822266 	 1.1444091796875e-05 	 2.193450927734375e-05 	 0.04823112487792969 	 0.05465126037597656 	 3.361701965332031e-05 	 5.650520324707031e-05 	 
2025-07-30 10:50:09.352072 test begin: paddle.Tensor.squeeze(Tensor([250, 1, 80, 25500],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([250, 1, 80, 25500],"float32"), 1, ) 	 510000000 	 1000 	 0.004494905471801758 	 0.004141569137573242 	 1.1920928955078125e-05 	 2.7418136596679688e-05 	 0.04100775718688965 	 0.05198359489440918 	 4.220008850097656e-05 	 7.605552673339844e-05 	 
2025-07-30 10:50:25.511176 test begin: paddle.Tensor.squeeze(Tensor([6760, 1, 37632, 2],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([6760, 1, 37632, 2],"float32"), 1, ) 	 508784640 	 1000 	 0.0044879913330078125 	 0.0041120052337646484 	 7.62939453125e-06 	 2.3603439331054688e-05 	 0.04064512252807617 	 0.07020974159240723 	 3.790855407714844e-05 	 8.988380432128906e-05 	 
2025-07-30 10:50:42.889576 test begin: paddle.Tensor.transpose(Tensor([1064960, 955],"bfloat16"), list[1,0,], )
[Prof] paddle.Tensor.transpose 	 paddle.Tensor.transpose(Tensor([1064960, 955],"bfloat16"), list[1,0,], ) 	 1017036800 	 1000 	 0.003286123275756836 	 0.004546403884887695 	 1.1682510375976562e-05 	 2.3126602172851562e-05 	 0.04210996627807617 	 4.502275466918945 	 4.220008850097656e-05 	 2.301224708557129 	 
2025-07-30 10:51:18.979806 test begin: paddle.Tensor.transpose(Tensor([1085440, 937],"bfloat16"), list[1,0,], )
[Prof] paddle.Tensor.transpose 	 paddle.Tensor.transpose(Tensor([1085440, 937],"bfloat16"), list[1,0,], ) 	 1017057280 	 1000 	 0.0033273696899414062 	 0.004528999328613281 	 8.58306884765625e-06 	 2.3603439331054688e-05 	 0.0421602725982666 	 4.502394914627075 	 2.8371810913085938e-05 	 2.301185369491577 	 
2025-07-30 10:51:55.276306 test begin: paddle.Tensor.transpose(Tensor([1116160, 911],"bfloat16"), list[1,0,], )
[Prof] paddle.Tensor.transpose 	 paddle.Tensor.transpose(Tensor([1116160, 911],"bfloat16"), list[1,0,], ) 	 1016821760 	 1000 	 0.0033082962036132812 	 0.004534482955932617 	 8.106231689453125e-06 	 2.193450927734375e-05 	 0.043408870697021484 	 4.571601152420044 	 8.058547973632812e-05 	 2.371049642562866 	 
2025-07-30 10:52:37.716885 test begin: paddle.Tensor.transpose(Tensor([141760, 7168],"bfloat16"), list[1,0,], )
[Prof] paddle.Tensor.transpose 	 paddle.Tensor.transpose(Tensor([141760, 7168],"bfloat16"), list[1,0,], ) 	 1016135680 	 1000 	 0.003266572952270508 	 0.004561185836791992 	 8.106231689453125e-06 	 4.220008850097656e-05 	 0.04509711265563965 	 4.497903347015381 	 3.719329833984375e-05 	 2.299121379852295 	 
2025-07-30 10:53:14.543522 test begin: paddle.Tensor.trunc(Tensor([18144010, 28],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([18144010, 28],"float32"), ) 	 508032280 	 1000 	 0.008464336395263672 	 2.928093194961548 	 1.5497207641601562e-05 	 2.9165618419647217 	 0.04831814765930176 	 1.3114697933197021 	 3.457069396972656e-05 	 1.2494292259216309 	 
2025-07-30 10:53:37.366022 test begin: paddle.Tensor.trunc(Tensor([20, 3175201, 8],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([20, 3175201, 8],"float32"), ) 	 508032160 	 1000 	 0.008266925811767578 	 2.928006887435913 	 1.5497207641601562e-05 	 2.916666030883789 	 0.0508420467376709 	 1.3118901252746582 	 4.887580871582031e-05 	 1.2505240440368652 	 
2025-07-30 10:54:02.327637 test begin: paddle.Tensor.trunc(Tensor([20, 8, 3175201],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([20, 8, 3175201],"float32"), ) 	 508032160 	 1000 	 0.008501291275024414 	 2.9294369220733643 	 2.2649765014648438e-05 	 2.9180400371551514 	 0.04824018478393555 	 1.3114182949066162 	 3.361701965332031e-05 	 1.2510862350463867 	 
2025-07-30 10:54:25.246388 test begin: paddle.Tensor.trunc(Tensor([280, 1814401],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([280, 1814401],"float32"), ) 	 508032280 	 1000 	 0.008369922637939453 	 2.930877923965454 	 1.1682510375976562e-05 	 2.9169349670410156 	 0.04847383499145508 	 1.3115065097808838 	 3.170967102050781e-05 	 1.2515285015106201 	 
2025-07-30 10:54:47.973696 test begin: paddle.Tensor.trunc(Tensor([63504010, 8],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([63504010, 8],"float32"), ) 	 508032080 	 1000 	 0.008234977722167969 	 2.9280147552490234 	 1.5497207641601562e-05 	 2.9166202545166016 	 0.04794883728027344 	 1.3137903213500977 	 1.9550323486328125e-05 	 1.2458107471466064 	 
2025-07-30 10:55:15.231614 test begin: paddle.Tensor.trunc(Tensor([7938010, 8, 8],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([7938010, 8, 8],"float32"), ) 	 508032640 	 1000 	 0.008423805236816406 	 2.927988290786743 	 1.6450881958007812e-05 	 2.916566848754883 	 0.0485682487487793 	 1.3116140365600586 	 3.62396240234375e-05 	 1.2502353191375732 	 
2025-07-30 10:55:39.860148 test begin: paddle.Tensor.trunc(Tensor([80, 6350401],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([80, 6350401],"float32"), ) 	 508032080 	 1000 	 0.008472204208374023 	 3.6168062686920166 	 2.574920654296875e-05 	 2.9168694019317627 	 0.04826045036315918 	 1.3113596439361572 	 2.3126602172851562e-05 	 1.2506330013275146 	 
2025-07-30 10:56:03.646595 test begin: paddle.Tensor.unbind(Tensor([30, 115, 2304, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 115, 2304, 64],"float32"), 0, ) 	 508723200 	 1000 	 0.04488539695739746 	 0.03971505165100098 	 4.4345855712890625e-05 	 3.2901763916015625e-05 	 3.528947353363037 	 2.968188524246216 	 3.421760320663452 	 2.6730105876922607 	 
2025-07-30 10:56:26.510634 test begin: paddle.Tensor.unbind(Tensor([30, 1351, 196, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 1351, 196, 64],"float32"), 0, ) 	 508408320 	 1000 	 0.03937530517578125 	 0.03653550148010254 	 2.956390380859375e-05 	 7.963180541992188e-05 	 3.52596116065979 	 2.9652750492095947 	 3.428438663482666 	 2.683600425720215 	 
2025-07-30 10:56:49.032076 test begin: paddle.Tensor.unbind(Tensor([30, 60, 2304, 123],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 60, 2304, 123],"float32"), 0, ) 	 510105600 	 1000 	 0.044701337814331055 	 0.03976750373840332 	 1.9311904907226562e-05 	 3.886222839355469e-05 	 3.5415828227996826 	 2.976811647415161 	 3.4346766471862793 	 2.671725273132324 	 
2025-07-30 10:57:16.337582 test begin: paddle.Tensor.unbind(Tensor([30, 60, 4411, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 60, 4411, 64],"float32"), 0, ) 	 508147200 	 1000 	 0.04319596290588379 	 0.0321652889251709 	 3.7670135498046875e-05 	 3.409385681152344e-05 	 3.525557279586792 	 2.9653892517089844 	 3.4261770248413086 	 2.6632235050201416 	 
2025-07-30 10:57:42.208906 test begin: paddle.Tensor.unbind(Tensor([30, 864, 196, 101],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 864, 196, 101],"float32"), 0, ) 	 513112320 	 1000 	 0.03909015655517578 	 0.032120466232299805 	 1.7642974853515625e-05 	 3.8623809814453125e-05 	 3.558337688446045 	 2.9950783252716064 	 3.460789680480957 	 2.7129569053649902 	 
2025-07-30 10:58:06.761840 test begin: paddle.Tensor.unbind(Tensor([30, 864, 307, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 864, 307, 64],"float32"), 0, ) 	 509276160 	 1000 	 0.039131879806518555 	 0.031810760498046875 	 1.811981201171875e-05 	 3.170967102050781e-05 	 3.531069040298462 	 2.9717133045196533 	 3.4335968494415283 	 2.656975269317627 	 
2025-07-30 10:58:29.376152 test begin: paddle.Tensor.unbind(Tensor([30, 960, 196, 91],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 960, 196, 91],"float32"), 0, ) 	 513676800 	 1000 	 0.0447535514831543 	 0.03291606903076172 	 1.9073486328125e-05 	 7.605552673339844e-05 	 3.562706708908081 	 2.9983320236206055 	 3.4656121730804443 	 2.7156660556793213 	 
2025-07-30 10:58:55.433786 test begin: paddle.Tensor.unbind(Tensor([30, 960, 276, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 960, 276, 64],"float32"), 0, ) 	 508723200 	 1000 	 0.03902912139892578 	 0.033704280853271484 	 2.09808349609375e-05 	 8.487701416015625e-05 	 3.526329278945923 	 2.9651691913604736 	 3.428471088409424 	 2.660900354385376 	 
2025-07-30 10:59:17.963581 test begin: paddle.Tensor.unbind(Tensor([50, 864, 196, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([50, 864, 196, 64],"float32"), 0, ) 	 541900800 	 1000 	 0.06185030937194824 	 0.05140566825866699 	 2.4318695068359375e-05 	 3.838539123535156e-05 	 3.7552897930145264 	 3.1508779525756836 	 3.6248843669891357 	 2.7056233882904053 	 
2025-07-30 10:59:43.054081 test begin: paddle.Tensor.unbind(Tensor([50, 960, 196, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([50, 960, 196, 64],"float32"), 0, ) 	 602112000 	 1000 	 0.06253170967102051 	 0.05233502388000488 	 1.9788742065429688e-05 	 0.00018143653869628906 	 4.182720899581909 	 3.499577522277832 	 4.053353548049927 	 3.068671226501465 	 
2025-07-30 11:00:12.356173 test begin: paddle.Tensor.unbind(Tensor([60, 60, 2304, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([60, 60, 2304, 64],"float32"), 0, ) 	 530841600 	 1000 	 0.08103704452514648 	 0.064727783203125 	 5.745887756347656e-05 	 7.677078247070312e-05 	 3.6821210384368896 	 3.082767963409424 	 3.5235252380371094 	 2.5255227088928223 	 
2025-07-30 11:00:43.487299 test begin: paddle.Tensor.unsqueeze(Tensor([1720, 544, 544],"float32"), 0, )
Warning: The core code of paddle.Tensor.unsqueeze is too complex.
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([1720, 544, 544],"float32"), 0, ) 	 509009920 	 1000 	 0.004110097885131836 	 0.003840923309326172 	 7.62939453125e-06 	 2.1696090698242188e-05 	 0.04074287414550781 	 0.05235934257507324 	 3.314018249511719e-05 	 6.0558319091796875e-05 	 
2025-07-30 11:00:59.226380 test begin: paddle.Tensor.unsqueeze(Tensor([1720, 544, 544],"float32"), 1, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([1720, 544, 544],"float32"), 1, ) 	 509009920 	 1000 	 0.008630037307739258 	 0.007177114486694336 	 1.4066696166992188e-05 	 2.3126602172851562e-05 	 0.05074119567871094 	 0.05925130844116211 	 6.079673767089844e-05 	 5.841255187988281e-05 	 
2025-07-30 11:01:18.235769 test begin: paddle.Tensor.unsqueeze(Tensor([20, 3840, 10240],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([20, 3840, 10240],"float32"), 0, ) 	 786432000 	 1000 	 0.004037380218505859 	 0.0037217140197753906 	 8.344650268554688e-06 	 2.1457672119140625e-05 	 0.04150080680847168 	 0.05421137809753418 	 4.982948303222656e-05 	 8.058547973632812e-05 	 
2025-07-30 11:01:46.095230 test begin: paddle.Tensor.unsqueeze(Tensor([2000, 467, 544],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([2000, 467, 544],"float32"), 0, ) 	 508096000 	 1000 	 0.004102230072021484 	 0.003765106201171875 	 8.106231689453125e-06 	 2.2172927856445312e-05 	 0.040917158126831055 	 0.053056955337524414 	 3.9577484130859375e-05 	 8.177757263183594e-05 	 
2025-07-30 11:02:02.365946 test begin: paddle.Tensor.unsqueeze(Tensor([2000, 467, 544],"float32"), 1, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([2000, 467, 544],"float32"), 1, ) 	 508096000 	 1000 	 0.004071950912475586 	 0.0037407875061035156 	 7.62939453125e-06 	 2.09808349609375e-05 	 0.04453468322753906 	 0.0717010498046875 	 6.771087646484375e-05 	 7.796287536621094e-05 	 
2025-07-30 11:02:20.601970 test begin: paddle.Tensor.unsqueeze(Tensor([2000, 544, 467],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([2000, 544, 467],"float32"), 0, ) 	 508096000 	 1000 	 0.004052162170410156 	 0.003777027130126953 	 6.9141387939453125e-06 	 2.2172927856445312e-05 	 0.04097318649291992 	 0.053437232971191406 	 3.814697265625e-05 	 6.341934204101562e-05 	 
2025-07-30 11:02:38.058776 test begin: paddle.Tensor.unsqueeze(Tensor([2000, 544, 467],"float32"), 1, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([2000, 544, 467],"float32"), 1, ) 	 508096000 	 1000 	 0.004125356674194336 	 0.0036957263946533203 	 8.344650268554688e-06 	 1.9788742065429688e-05 	 0.04095125198364258 	 0.052570343017578125 	 1.9788742065429688e-05 	 6.389617919921875e-05 	 
2025-07-30 11:02:54.138785 test begin: paddle.Tensor.unsqueeze(Tensor([30, 1654, 10240],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([30, 1654, 10240],"float32"), 0, ) 	 508108800 	 1000 	 0.004049062728881836 	 0.004353523254394531 	 6.9141387939453125e-06 	 5.626678466796875e-05 	 0.041144371032714844 	 0.05193495750427246 	 4.315376281738281e-05 	 5.650520324707031e-05 	 
2025-07-30 11:03:14.037276 test begin: paddle.Tensor.unsqueeze(Tensor([30, 3840, 4411],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([30, 3840, 4411],"float32"), 0, ) 	 508147200 	 1000 	 0.004056692123413086 	 0.003704071044921875 	 8.344650268554688e-06 	 2.0265579223632812e-05 	 0.041939735412597656 	 0.05269956588745117 	 5.555152893066406e-05 	 7.128715515136719e-05 	 
2025-07-30 11:03:32.676652 test begin: paddle.all(Tensor([50, 1016065, 10],"bool"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([50, 1016065, 10],"bool"), None, False, None, ) 	 508032500 	 1000 	 0.4687225818634033 	 0.5093889236450195 	 0.2394700050354004 	 0.2595250606536865 	 None 	 None 	 None 	 None 	 
2025-07-30 11:03:40.849912 test begin: paddle.all(Tensor([50, 6, 1693441],"bool"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([50, 6, 1693441],"bool"), None, False, None, ) 	 508032300 	 1000 	 0.46857762336730957 	 0.5094234943389893 	 0.2394411563873291 	 0.26097702980041504 	 None 	 None 	 None 	 None 	 
2025-07-30 11:03:48.731778 test begin: paddle.all(Tensor([508032010],"bool"), )
[Prof] paddle.all 	 paddle.all(Tensor([508032010],"bool"), ) 	 508032010 	 1000 	 0.47092652320861816 	 0.5079622268676758 	 0.2406144142150879 	 0.25954174995422363 	 None 	 None 	 None 	 None 	 
2025-07-30 11:03:56.906036 test begin: paddle.all(Tensor([8467210, 6, 10],"bool"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([8467210, 6, 10],"bool"), None, False, None, ) 	 508032600 	 1000 	 0.46640586853027344 	 0.5079481601715088 	 0.23833012580871582 	 0.2595326900482178 	 None 	 None 	 None 	 None 	 
2025-07-30 11:04:04.940077 test begin: paddle.any(Tensor([10, 12404, 4096],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([10, 12404, 4096],"bool"), ) 	 508067840 	 1000 	 0.46627092361450195 	 0.5299029350280762 	 0.23823976516723633 	 0.27003908157348633 	 None 	 None 	 None 	 None 	 
2025-07-30 11:04:13.282391 test begin: paddle.any(Tensor([10, 300, 169345],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([10, 300, 169345],"bool"), ) 	 508035000 	 1000 	 0.4696788787841797 	 0.5296480655670166 	 0.24165940284729004 	 0.27066540718078613 	 None 	 None 	 None 	 None 	 
2025-07-30 11:04:21.721208 test begin: paddle.any(Tensor([11240, 45199],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([11240, 45199],"bool"), ) 	 508036760 	 1000 	 0.4676377773284912 	 0.5284633636474609 	 0.2382187843322754 	 0.2700214385986328 	 None 	 None 	 None 	 None 	 
2025-07-30 11:04:29.778795 test begin: paddle.any(Tensor([15876010, 32],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([15876010, 32],"bool"), ) 	 508032320 	 1000 	 0.463939905166626 	 0.5295073986053467 	 0.2370448112487793 	 0.2705512046813965 	 None 	 None 	 None 	 None 	 
2025-07-30 11:04:41.872317 test begin: paddle.any(Tensor([420, 300, 4096],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([420, 300, 4096],"bool"), ) 	 516096000 	 1000 	 0.4733901023864746 	 0.5393483638763428 	 0.24187350273132324 	 0.27561330795288086 	 None 	 None 	 None 	 None 	 
2025-07-30 11:04:49.956762 test begin: paddle.any(Tensor([5120, 99226],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([5120, 99226],"bool"), ) 	 508037120 	 1000 	 0.4658646583557129 	 0.5284476280212402 	 0.23802995681762695 	 0.27004575729370117 	 None 	 None 	 None 	 None 	 
2025-07-30 11:04:58.086582 test begin: paddle.as_complex(Tensor([320, 15, 207, 8, 32, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([320, 15, 207, 8, 32, 2],"float32"), ) 	 508723200 	 1000 	 0.0031232833862304688 	 0.004416465759277344 	 1.049041748046875e-05 	 1.9788742065429688e-05 	 0.037715911865234375 	 0.05392909049987793 	 2.7179718017578125e-05 	 7.152557373046875e-05 	 
Error: Can not import paddle core while this file exists: /usr/local/lib/python3.10/dist-packages/paddle/base/libpaddle.so
KeyboardInterrupt

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 9, in <module>
    from tester import (APIConfig, APITestAccuracy, APITestCINNVSDygraph,
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/__init__.py", line 74, in __getattr__
    from .api_config import APIConfig
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/__init__.py", line 22, in __getattr__
    from .config_analyzer import APIConfig
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py", line 8, in <module>
    import paddle
  File "/usr/local/lib/python3.10/dist-packages/paddle/__init__.py", line 38, in <module>
    from .base import core  # noqa: F401
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/__init__.py", line 38, in <module>
    from . import (  # noqa: F401
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/backward.py", line 28, in <module>
    from . import core, framework, log_helper, unique_name
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/core.py", line 388, in <module>
    raise e
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/core.py", line 267, in <module>
    from . import libpaddle
ImportError: initialization failed
2025-07-30 10:36:11.494983 test begin: paddle.as_complex(Tensor([320, 15, 8, 207, 32, 2],"float32"), )
W0730 10:36:20.582957 131380 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([320, 15, 8, 207, 32, 2],"float32"), ) 	 508723200 	 1000 	 0.00331878662109375 	 0.00867605209350586 	 1.33514404296875e-05 	 5.9604644775390625e-05 	 0.040711402893066406 	 0.08459091186523438 	 5.936622619628906e-05 	 7.200241088867188e-05 	 
2025-07-30 10:36:29.431551 test begin: paddle.as_complex(Tensor([320, 15, 8, 8, 827, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([320, 15, 8, 8, 827, 2],"float32"), ) 	 508108800 	 1000 	 0.0033292770385742188 	 0.004380226135253906 	 2.0265579223632812e-05 	 2.1219253540039062e-05 	 0.03825068473815918 	 0.07851600646972656 	 3.838539123535156e-05 	 6.508827209472656e-05 	 
2025-07-30 10:36:42.977820 test begin: paddle.as_complex(Tensor([320, 388, 8, 8, 32, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([320, 388, 8, 8, 32, 2],"float32"), ) 	 508559360 	 1000 	 0.0032410621643066406 	 0.004351615905761719 	 8.106231689453125e-06 	 1.9311904907226562e-05 	 0.03787851333618164 	 0.06124567985534668 	 5.221366882324219e-05 	 6.270408630371094e-05 	 
2025-07-30 10:36:56.460524 test begin: paddle.as_complex(Tensor([8270, 15, 8, 8, 32, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([8270, 15, 8, 8, 32, 2],"float32"), ) 	 508108800 	 1000 	 0.007521867752075195 	 0.00814509391784668 	 4.76837158203125e-05 	 3.24249267578125e-05 	 0.04715609550476074 	 0.08874320983886719 	 0.00016427040100097656 	 8.940696716308594e-05 	 
2025-07-30 10:37:11.759459 test begin: paddle.as_strided(Tensor([15876010, 32],"float32"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([15876010, 32],"float32"), shape=tuple(3,), stride=tuple(1,), ) 	 508032320 	 1000 	 0.017581701278686523 	 0.004489898681640625 	 2.384185791015625e-05 	 4.0531158447265625e-05 	 1.5009081363677979 	 1.3141355514526367 	 0.7663440704345703 	 0.6710715293884277 	 
2025-07-30 10:37:24.925187 test begin: paddle.as_strided(Tensor([31752010, 32],"float16"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([31752010, 32],"float16"), shape=tuple(3,), stride=tuple(1,), ) 	 1016064320 	 1000 	 0.017354488372802734 	 0.00463104248046875 	 2.002716064453125e-05 	 3.528594970703125e-05 	 1.5155904293060303 	 1.3146116733551025 	 0.7743768692016602 	 0.6714434623718262 	 
2025-07-30 10:37:52.063728 test begin: paddle.as_strided(Tensor([31752010, 32],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([31752010, 32],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), ) 	 1016064320 	 1000 	 0.017653465270996094 	 0.004628419876098633 	 1.7881393432617188e-05 	 2.47955322265625e-05 	 1.5128087997436523 	 1.315506935119629 	 0.7732148170471191 	 0.6719000339508057 	 
2025-07-30 10:38:19.801159 test begin: paddle.as_strided(Tensor([320, 1587601],"float32"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([320, 1587601],"float32"), shape=tuple(3,), stride=tuple(1,), ) 	 508032320 	 1000 	 0.017566680908203125 	 0.004482746124267578 	 2.193450927734375e-05 	 2.6702880859375e-05 	 1.5001001358032227 	 1.3140907287597656 	 0.7661275863647461 	 0.6711668968200684 	 
2025-07-30 10:38:30.880064 test begin: paddle.as_strided(Tensor([320, 3175201],"float16"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([320, 3175201],"float16"), shape=tuple(3,), stride=tuple(1,), ) 	 1016064320 	 1000 	 0.022893190383911133 	 0.004451751708984375 	 2.86102294921875e-05 	 2.4080276489257812e-05 	 1.5145783424377441 	 1.314629316329956 	 0.7734665870666504 	 0.671442985534668 	 
2025-07-30 10:38:56.112823 test begin: paddle.as_strided(Tensor([320, 3175201],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([320, 3175201],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), ) 	 1016064320 	 1000 	 0.01754283905029297 	 0.0046918392181396484 	 1.71661376953125e-05 	 2.2649765014648438e-05 	 1.5141870975494385 	 1.315445899963379 	 0.7745957374572754 	 0.6718504428863525 	 
2025-07-30 10:39:21.367220 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Error] CUDA out of memory. Tried to allocate 18.93 GiB. GPU 0 has a total capacity of 39.39 GiB of which 18.08 GiB is free. Process 151300 has 21.30 GiB memory in use. Of the allocated memory 1024 bytes is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-07-30 10:40:24.871663 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), )
W0730 10:41:10.160928 133845 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), ) 	 2540160856 	 1000 	 0.0028793811798095703 	 0.00932931900024414 	 2.5510787963867188e-05 	 4.6253204345703125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:41:24.455943 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 1000 	 0.002722501754760742 	 0.007591962814331055 	 1.4066696166992188e-05 	 3.838539123535156e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:42:13.968157 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 1058401],"float64"), ) 	 2548629608 	 1000 	 0.0030171871185302734 	 0.008344888687133789 	 1.7881393432617188e-05 	 2.2649765014648438e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:43:16.395591 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), )
[Error] CUDA out of memory. Tried to allocate 18.99 GiB. GPU 0 has a total capacity of 39.39 GiB of which 1022.31 MiB is free. Process 56224 has 38.39 GiB memory in use. Of the allocated memory 189.00 KiB is allocated by PyTorch, and 1.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-07-30 10:44:32.706622 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), )
W0730 10:45:16.097625 138751 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), ) 	 2548657300 	 1000 	 0.002981424331665039 	 0.01166987419128418 	 1.1920928955078125e-05 	 5.435943603515625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:45:27.136507 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), ) 	 2548654290 	 1000 	 0.0029091835021972656 	 0.007858514785766602 	 1.6927719116210938e-05 	 5.14984130859375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:46:21.187007 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), ) 	 25425720 	 1000 	 0.002869844436645508 	 0.007614850997924805 	 1.0251998901367188e-05 	 2.2172927856445312e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:46:21.887744 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 1000 	 0.0028820037841796875 	 0.007622718811035156 	 9.5367431640625e-06 	 5.984306335449219e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:47:23.310120 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 1000 	 0.0028769969940185547 	 0.007459163665771484 	 8.821487426757812e-06 	 2.288818359375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:48:17.583315 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 1000 	 0.0028753280639648438 	 0.007425069808959961 	 7.152557373046875e-06 	 2.09808349609375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:48:19.498989 test begin: paddle.atleast_1d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 1000 	 0.0028867721557617188 	 0.007309436798095703 	 1.2636184692382812e-05 	 2.002716064453125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:49:13.386684 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 1000 	 0.002841949462890625 	 0.007285118103027344 	 1.0728836059570312e-05 	 2.3365020751953125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:50:07.494882 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), ) 	 2548633220 	 1000 	 0.0030982494354248047 	 0.010257482528686523 	 2.5033950805664062e-05 	 2.3603439331054688e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:51:01.851634 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), ) 	 2548632618 	 1000 	 0.002863168716430664 	 0.007582902908325195 	 1.049041748046875e-05 	 3.1948089599609375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:51:56.126571 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), ) 	 25406424 	 1000 	 0.002958059310913086 	 0.0076062679290771484 	 1.7881393432617188e-05 	 5.173683166503906e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:51:56.840273 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 1000 	 0.002879619598388672 	 0.0075092315673828125 	 1.1205673217773438e-05 	 2.193450927734375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:52:56.267171 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 1000 	 0.0028619766235351562 	 0.0074465274810791016 	 9.059906005859375e-06 	 2.193450927734375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:52:56.970121 test begin: paddle.atleast_1d(Tensor([301, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 423361, 5],"float64"), ) 	 2548633220 	 1000 	 0.0016791820526123047 	 0.006357908248901367 	 8.58306884765625e-06 	 2.1219253540039062e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:53:50.416673 test begin: paddle.atleast_1d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 1000 	 0.002913951873779297 	 0.00758051872253418 	 9.5367431640625e-06 	 2.3126602172851562e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:54:45.162399 test begin: paddle.atleast_1d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 1000 	 0.002883434295654297 	 0.007456064224243164 	 7.867813110351562e-06 	 2.3603439331054688e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:55:50.196167 test begin: paddle.atleast_1d(Tensor([301, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 846721, 2, 5],"float64"), ) 	 2548630210 	 1000 	 0.001722574234008789 	 0.00855708122253418 	 1.0728836059570312e-05 	 2.5510787963867188e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:57:04.237931 test begin: paddle.atleast_1d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 1000 	 0.002917051315307617 	 0.007502317428588867 	 2.6226043701171875e-05 	 2.4080276489257812e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:58:16.031400 test begin: paddle.atleast_1d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 1000 	 0.0028531551361083984 	 0.007533073425292969 	 7.3909759521484375e-06 	 2.2172927856445312e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:58:16.696048 test begin: paddle.atleast_1d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 1000 	 0.0028142929077148438 	 0.007269859313964844 	 1.0013580322265625e-05 	 2.2172927856445312e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:59:15.830449 test begin: paddle.atleast_1d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 1000 	 0.003239870071411133 	 0.007417440414428711 	 2.09808349609375e-05 	 2.47955322265625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:59:16.481150 test begin: paddle.atleast_1d(Tensor([63504101, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([63504101, 4, 2, 5],"float64"), ) 	 2540164040 	 1000 	 0.001622915267944336 	 0.006330013275146484 	 1.0251998901367188e-05 	 2.2649765014648438e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:00:09.815546 test begin: paddle.atleast_1d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 2540164280 	 1000 	 0.0028345584869384766 	 0.007512569427490234 	 8.821487426757812e-06 	 4.57763671875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:01:11.074982 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), ) 	 2540164280 	 1000 	 0.003765106201171875 	 0.007469654083251953 	 1.0728836059570312e-05 	 2.2411346435546875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:02:04.541756 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 2540164280 	 1000 	 0.003759145736694336 	 0.00802469253540039 	 1.0013580322265625e-05 	 5.030632019042969e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:03:11.627295 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), ) 	 2540160856 	 1000 	 0.003729104995727539 	 0.007425069808959961 	 1.0251998901367188e-05 	 2.1457672119140625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:04:04.886057 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 1000 	 0.0036802291870117188 	 0.0074231624603271484 	 1.0251998901367188e-05 	 2.9802322387695312e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:04:58.103798 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 1058401],"float64"), ) 	 2548629608 	 1000 	 0.0019314289093017578 	 0.006154298782348633 	 1.2159347534179688e-05 	 2.1219253540039062e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:05:51.439839 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548653688 	 1000 	 0.0037975311279296875 	 0.010420560836791992 	 1.2159347534179688e-05 	 2.5987625122070312e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:06:51.397092 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548653688 	 1000 	 0.0038263797760009766 	 0.0074918270111083984 	 1.0251998901367188e-05 	 2.002716064453125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:07:45.804629 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), ) 	 2548653688 	 1000 	 0.003804445266723633 	 0.0074198246002197266 	 1.1682510375976562e-05 	 2.6464462280273438e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:08:54.541279 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), ) 	 2548657300 	 1000 	 0.0037953853607177734 	 0.007427215576171875 	 9.298324584960938e-06 	 2.1219253540039062e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:10:02.788841 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), ) 	 2548654290 	 1000 	 0.0037920475006103516 	 0.00754237174987793 	 6.67572021484375e-06 	 2.47955322265625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:11:15.711319 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), ) 	 25425720 	 1000 	 0.0038008689880371094 	 0.010375499725341797 	 1.4543533325195312e-05 	 2.3126602172851562e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:11:16.553311 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 1000 	 0.003785371780395508 	 0.0074443817138671875 	 1.0728836059570312e-05 	 2.2172927856445312e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:12:09.878163 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 1000 	 0.0037772655487060547 	 0.007603645324707031 	 6.9141387939453125e-06 	 2.8371810913085938e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:13:09.319191 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 1000 	 0.003782987594604492 	 0.007644176483154297 	 8.344650268554688e-06 	 3.314018249511719e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:13:10.052693 test begin: paddle.atleast_2d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 1000 	 0.003837108612060547 	 0.007542610168457031 	 7.62939453125e-06 	 2.8371810913085938e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:14:14.974088 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 1000 	 0.0038518905639648438 	 0.012194395065307617 	 8.821487426757812e-06 	 5.7220458984375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:15:18.316640 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), ) 	 2548633220 	 1000 	 0.003768444061279297 	 0.01482248306274414 	 7.62939453125e-06 	 8.678436279296875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:16:19.816230 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), ) 	 2548632618 	 1000 	 0.003770112991333008 	 0.007344961166381836 	 8.106231689453125e-06 	 2.3126602172851562e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:17:29.967423 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), ) 	 25406424 	 1000 	 0.003732919692993164 	 0.007912397384643555 	 7.152557373046875e-06 	 5.0067901611328125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:17:30.821334 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 1000 	 0.0037872791290283203 	 0.007673740386962891 	 1.1920928955078125e-05 	 5.078315734863281e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:18:24.741917 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 1000 	 0.0037708282470703125 	 0.007374286651611328 	 9.5367431640625e-06 	 2.2172927856445312e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:18:25.499580 test begin: paddle.atleast_2d(Tensor([301, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 423361, 5],"float64"), ) 	 2548633220 	 1000 	 0.0019085407257080078 	 0.006236076354980469 	 6.4373016357421875e-06 	 1.9788742065429688e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:19:36.240305 test begin: paddle.atleast_2d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 1000 	 0.003835439682006836 	 0.007464408874511719 	 1.1682510375976562e-05 	 2.1457672119140625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:20:40.902178 test begin: paddle.atleast_2d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 1000 	 0.003879070281982422 	 0.009145498275756836 	 1.4781951904296875e-05 	 5.221366882324219e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:21:42.524073 test begin: paddle.atleast_2d(Tensor([301, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 846721, 2, 5],"float64"), ) 	 2548630210 	 1000 	 0.001924276351928711 	 0.0062634944915771484 	 9.298324584960938e-06 	 2.1696090698242188e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:22:45.283533 test begin: paddle.atleast_2d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 1000 	 0.0038483142852783203 	 0.01028299331665039 	 7.867813110351562e-06 	 2.1696090698242188e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:23:54.644394 test begin: paddle.atleast_2d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 1000 	 0.0037326812744140625 	 0.007452249526977539 	 1.2159347534179688e-05 	 2.384185791015625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:23:55.484661 test begin: paddle.atleast_2d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 1000 	 0.0037310123443603516 	 0.007372379302978516 	 8.58306884765625e-06 	 2.1696090698242188e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:24:59.969733 test begin: paddle.atleast_2d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 1000 	 0.003796815872192383 	 0.007451772689819336 	 7.3909759521484375e-06 	 3.0279159545898438e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:25:00.627634 test begin: paddle.atleast_2d(Tensor([63504101, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([63504101, 4, 2, 5],"float64"), ) 	 2540164040 	 1000 	 0.0036106109619140625 	 0.006302356719970703 	 1.1682510375976562e-05 	 2.09808349609375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:25:54.308799 test begin: paddle.atleast_2d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 2540164280 	 1000 	 0.0037407875061035156 	 0.007502079010009766 	 1.0967254638671875e-05 	 2.6226043701171875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:27:06.236151 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), ) 	 2540164280 	 1000 	 0.004708528518676758 	 0.012939691543579102 	 1.049041748046875e-05 	 6.699562072753906e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:28:07.115896 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 2540164280 	 1000 	 0.004651784896850586 	 0.007776021957397461 	 1.0013580322265625e-05 	 6.866455078125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:29:13.836402 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), ) 	 2540160856 	 1000 	 0.004643917083740234 	 0.00741267204284668 	 1.049041748046875e-05 	 2.1457672119140625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:30:14.395712 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 1000 	 0.0045626163482666016 	 0.01751542091369629 	 1.0013580322265625e-05 	 7.295608520507812e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:31:16.391887 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 1058401],"float64"), ) 	 2548629608 	 1000 	 0.0028083324432373047 	 0.0063855648040771484 	 3.7670135498046875e-05 	 2.002716064453125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:32:29.558315 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548653688 	 1000 	 0.004827976226806641 	 0.00724029541015625 	 1.1920928955078125e-05 	 2.0265579223632812e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:33:26.749484 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548653688 	 1000 	 0.004929304122924805 	 0.00745701789855957 	 1.0251998901367188e-05 	 2.3365020751953125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:34:32.519483 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), ) 	 2548653688 	 1000 	 0.004810333251953125 	 0.007414817810058594 	 9.5367431640625e-06 	 2.1219253540039062e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:35:40.374836 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), ) 	 2548657300 	 1000 	 0.004823446273803711 	 0.007427215576171875 	 9.5367431640625e-06 	 2.288818359375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:36:34.434768 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), ) 	 2548654290 	 1000 	 0.004793882369995117 	 0.009505987167358398 	 7.62939453125e-06 	 2.765655517578125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:37:29.488223 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), ) 	 25425720 	 1000 	 0.005184650421142578 	 0.00741267204284668 	 3.218650817871094e-05 	 2.1457672119140625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:37:30.775968 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 1000 	 0.004822731018066406 	 0.007906913757324219 	 9.298324584960938e-06 	 7.2479248046875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:38:45.066260 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 1000 	 0.004796504974365234 	 0.007366180419921875 	 1.33514404296875e-05 	 2.288818359375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:39:54.086355 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 1000 	 0.004735231399536133 	 0.0074236392974853516 	 1.1444091796875e-05 	 2.1219253540039062e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:39:54.805380 test begin: paddle.atleast_3d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 1000 	 0.004736185073852539 	 0.0074427127838134766 	 7.3909759521484375e-06 	 2.2411346435546875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:40:56.142947 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 1000 	 0.0047686100006103516 	 0.007257223129272461 	 7.152557373046875e-06 	 2.09808349609375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:41:50.648718 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), ) 	 2548633220 	 1000 	 0.0047719478607177734 	 0.008842229843139648 	 9.775161743164062e-06 	 5.340576171875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:42:50.985105 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), ) 	 2548632618 	 1000 	 0.00474238395690918 	 0.007489919662475586 	 1.1682510375976562e-05 	 2.1457672119140625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:43:45.667372 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), ) 	 25406424 	 1000 	 0.004637718200683594 	 0.007259368896484375 	 1.1205673217773438e-05 	 2.0742416381835938e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:43:46.473369 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 1000 	 0.00473332405090332 	 0.010254621505737305 	 1.0967254638671875e-05 	 2.1696090698242188e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:44:46.934353 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 1000 	 0.0046918392181396484 	 0.0071926116943359375 	 8.58306884765625e-06 	 2.2411346435546875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:44:47.641984 test begin: paddle.atleast_3d(Tensor([301, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 423361, 5],"float64"), ) 	 2548633220 	 1000 	 0.0022852420806884766 	 0.006418466567993164 	 2.09808349609375e-05 	 2.0742416381835938e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:45:53.470835 test begin: paddle.atleast_3d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 1000 	 0.004808902740478516 	 0.0073740482330322266 	 1.1444091796875e-05 	 5.054473876953125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:46:53.930774 test begin: paddle.atleast_3d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 1000 	 0.004813194274902344 	 0.007405519485473633 	 1.1920928955078125e-05 	 2.2411346435546875e-05 	 None 	 None 	 None 	 None 	 
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 9, in <module>
    from tester import (APIConfig, APITestAccuracy, APITestCINNVSDygraph,
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/__init__.py", line 74, in __getattr__
    from .api_config import APIConfig
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/__init__.py", line 22, in __getattr__
    from .config_analyzer import APIConfig
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py", line 9, in <module>
    import torch
  File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 2611, in <module>
    from torch import _meta_registrations
  File "/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py", line 12, in <module>
    from torch._decomp import (
  File "/usr/local/lib/python3.10/dist-packages/torch/_decomp/__init__.py", line 277, in <module>
    import torch._refs
  File "/usr/local/lib/python3.10/dist-packages/torch/_refs/__init__.py", line 2538, in <module>
    @register_decomposition(aten.std_mean)
  File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 1265, in __getattr__
    op, overload_names = _get_packet(qualified_op_name, module_name)
  File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 1290, in _get_packet
    op, overload_names = torch._C._jit_get_operation(qualname)
KeyboardInterrupt
2025-07-30 10:36:14.369192 test begin: paddle.atleast_3d(Tensor([301, 846721, 2, 5],"float64"), )
W0730 10:37:10.116874 131510 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 846721, 2, 5],"float64"), ) 	 2548630210 	 1000 	 0.002415180206298828 	 0.008132457733154297 	 2.09808349609375e-05 	 6.079673767089844e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:37:16.772856 test begin: paddle.atleast_3d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 1000 	 0.0049593448638916016 	 0.007735252380371094 	 1.0251998901367188e-05 	 3.981590270996094e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:38:14.358952 test begin: paddle.atleast_3d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 1000 	 0.005527973175048828 	 0.007668972015380859 	 4.124641418457031e-05 	 3.0040740966796875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:38:14.989959 test begin: paddle.atleast_3d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 1000 	 0.004718780517578125 	 0.007368803024291992 	 2.574920654296875e-05 	 2.5510787963867188e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:39:19.007607 test begin: paddle.atleast_3d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 1000 	 0.005151033401489258 	 0.010303258895874023 	 1.430511474609375e-05 	 2.9325485229492188e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:39:19.643324 test begin: paddle.atleast_3d(Tensor([63504101, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([63504101, 4, 2, 5],"float64"), ) 	 2540164040 	 1000 	 0.002298116683959961 	 0.00639653205871582 	 8.344650268554688e-06 	 2.002716064453125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:40:43.524328 test begin: paddle.atleast_3d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 2540164280 	 1000 	 0.005011081695556641 	 0.009654521942138672 	 1.5497207641601562e-05 	 8.797645568847656e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:41:43.970620 test begin: paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 1, 117601, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 1, 117601, 2],"bool"), ) 	 508036320 	 1000 	 0.8583352565765381 	 0.7476236820220947 	 0.8495779037475586 	 0.7341010570526123 	 None 	 None 	 None 	 None 	 
2025-07-30 10:41:55.200119 test begin: paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 1, 5, 47041],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 1, 5, 47041],"bool"), ) 	 508042800 	 1000 	 0.8487989902496338 	 0.7477278709411621 	 0.8384301662445068 	 0.7293648719787598 	 None 	 None 	 None 	 None 	 
2025-07-30 10:42:06.337513 test begin: paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 508053600 	 1000 	 0.8583943843841553 	 0.7470440864562988 	 0.8496708869934082 	 0.7348537445068359 	 None 	 None 	 None 	 None 	 
2025-07-30 10:42:15.430787 test begin: paddle.bitwise_not(Tensor([20, 3, 3, 3, 94081, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 3, 3, 94081, 1, 5, 2],"bool"), ) 	 508037400 	 1000 	 0.8601803779602051 	 0.7470896244049072 	 0.8513967990875244 	 0.7348761558532715 	 None 	 None 	 None 	 None 	 
2025-07-30 10:42:24.153442 test begin: paddle.bitwise_not(Tensor([20, 3, 3, 70561, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 3, 70561, 4, 1, 5, 2],"bool"), ) 	 508039200 	 1000 	 0.8597218990325928 	 0.7471640110015869 	 0.8444807529449463 	 0.7286155223846436 	 None 	 None 	 None 	 None 	 
2025-07-30 10:42:33.451388 test begin: paddle.bitwise_not(Tensor([20, 3, 70561, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 70561, 3, 4, 1, 5, 2],"bool"), ) 	 508039200 	 1000 	 0.8585553169250488 	 0.7470254898071289 	 0.8499250411987305 	 0.7350709438323975 	 None 	 None 	 None 	 None 	 
2025-07-30 10:42:42.154631 test begin: paddle.bitwise_not(Tensor([20, 70561, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 70561, 3, 3, 4, 1, 5, 2],"bool"), ) 	 508039200 	 1000 	 0.8598546981811523 	 0.747136116027832 	 0.8512759208679199 	 0.735008716583252 	 None 	 None 	 None 	 None 	 
2025-07-30 10:42:51.298962 test begin: paddle.bitwise_not(Tensor([470410, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([470410, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 508042800 	 1000 	 0.8476476669311523 	 0.7623355388641357 	 0.8391022682189941 	 0.7365026473999023 	 None 	 None 	 None 	 None 	 
2025-07-30 10:43:00.691941 test begin: paddle.bucketize(Tensor([2, 4],"float64"), Tensor([2540160101],"float64"), )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 4],"float64"), Tensor([2540160101],"float64"), ) 	 2540160109 	 1000 	 0.011434555053710938 	 0.010954618453979492 	 2.2172927856445312e-05 	 3.600120544433594e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:43:49.440631 test begin: paddle.bucketize(Tensor([2, 4],"float64"), Tensor([2540160101],"float64"), right=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 4],"float64"), Tensor([2540160101],"float64"), right=True, ) 	 2540160109 	 1000 	 0.011726856231689453 	 0.011128664016723633 	 1.621246337890625e-05 	 3.2901763916015625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:44:42.389774 test begin: paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), ) 	 25402405 	 1000 	 0.011248111724853516 	 0.010814428329467773 	 1.4543533325195312e-05 	 2.8848648071289062e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:44:42.895415 test begin: paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), out_int32=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), out_int32=True, ) 	 25402405 	 1000 	 0.011684656143188477 	 0.01123189926147461 	 1.5497207641601562e-05 	 3.62396240234375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:44:43.394518 test begin: paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), right=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), right=True, ) 	 25402405 	 1000 	 0.011609315872192383 	 0.010943174362182617 	 2.09808349609375e-05 	 2.956390380859375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:44:43.891760 test begin: paddle.combinations(Tensor([2540160101],"int64"), 0, True, )
[Prof] paddle.combinations 	 paddle.combinations(Tensor([2540160101],"int64"), 0, True, ) 	 2540160101 	 1000 	 0.012592077255249023 	 0.003987550735473633 	 1.0728836059570312e-05 	 2.2649765014648438e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:45:21.530923 test begin: paddle.cummax(Tensor([10001, 2080],"float32"), axis=-2, )
[Prof] paddle.cummax 	 paddle.cummax(Tensor([10001, 2080],"float32"), axis=-2, ) 	 20802080 	 1000 	 5.704549789428711 	 5.705260753631592 	 5.691443204879761 	 5.689308404922485 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:45:34.751723 test begin: paddle.cummax(Tensor([208001, 100],"float32"), axis=-1, )
[Prof] paddle.cummax 	 paddle.cummax(Tensor([208001, 100],"float32"), axis=-1, ) 	 20800100 	 1000 	 0.5542964935302734 	 3.5094854831695557 	 0.5336616039276123 	 3.484386444091797 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:45:40.815734 test begin: paddle.diag(Tensor([20000, 25402],"float32"), )
[Prof] paddle.diag 	 paddle.diag(Tensor([20000, 25402],"float32"), ) 	 508040000 	 1000 	 0.009394407272338867 	 0.017651796340942383 	 1.6450881958007812e-05 	 6.67572021484375e-05 	 1.522043228149414 	 1.3267388343811035 	 0.7781708240509033 	 0.6778419017791748 	 
2025-07-30 10:45:51.608065 test begin: paddle.diag(Tensor([20000, 25402],"float32"), offset=-1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([20000, 25402],"float32"), offset=-1, ) 	 508040000 	 1000 	 0.009654045104980469 	 0.017786502838134766 	 1.3828277587890625e-05 	 3.719329833984375e-05 	 1.5211737155914307 	 1.3265917301177979 	 0.7773404121398926 	 0.6777760982513428 	 
2025-07-30 10:46:02.513846 test begin: paddle.diag(Tensor([20000, 25402],"float32"), offset=1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([20000, 25402],"float32"), offset=1, ) 	 508040000 	 1000 	 0.00960850715637207 	 0.017449140548706055 	 1.3113021850585938e-05 	 5.245208740234375e-05 	 1.5215709209442139 	 1.3291282653808594 	 0.7777135372161865 	 0.6786091327667236 	 
2025-07-30 10:46:14.508234 test begin: paddle.diag(Tensor([254020, 2000],"float32"), )
[Prof] paddle.diag 	 paddle.diag(Tensor([254020, 2000],"float32"), ) 	 508040000 	 1000 	 0.009263753890991211 	 0.01748943328857422 	 1.2636184692382812e-05 	 4.673004150390625e-05 	 1.5136117935180664 	 1.3181793689727783 	 0.7720956802368164 	 0.673513650894165 	 
2025-07-30 10:46:25.279149 test begin: paddle.diag(Tensor([254020, 2000],"float32"), offset=-1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([254020, 2000],"float32"), offset=-1, ) 	 508040000 	 1000 	 0.009531974792480469 	 0.01753401756286621 	 1.0728836059570312e-05 	 3.2901763916015625e-05 	 1.5112745761871338 	 1.3182339668273926 	 0.7724964618682861 	 0.6735410690307617 	 
2025-07-30 10:46:37.264090 test begin: paddle.diag(Tensor([254020, 2000],"float32"), offset=1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([254020, 2000],"float32"), offset=1, ) 	 508040000 	 1000 	 0.009510278701782227 	 0.01734638214111328 	 1.811981201171875e-05 	 5.054473876953125e-05 	 1.5125339031219482 	 1.3182604312896729 	 0.7724061012268066 	 0.6734554767608643 	 
2025-07-30 10:46:48.739383 test begin: paddle.diagonal_scatter(Tensor([100, 5080321],"bool"), Tensor([100],"bool"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([100, 5080321],"bool"), Tensor([100],"bool"), offset=0, axis1=0, axis2=1, ) 	 508032200 	 1000 	 0.7656095027923584 	 0.7754931449890137 	 0.19522619247436523 	 0.26357078552246094 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:47:05.417754 test begin: paddle.diagonal_scatter(Tensor([50803210, 10],"bool"), Tensor([10],"bool"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([50803210, 10],"bool"), Tensor([10],"bool"), offset=0, axis1=0, axis2=1, ) 	 508032110 	 1000 	 0.7659482955932617 	 0.7768304347991943 	 0.19532322883605957 	 0.26360344886779785 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:47:23.302386 test begin: paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[-1,1,3,], )
W0730 10:47:33.228518 142602 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[-1,1,3,], ) 	 254016180 	 1000 	 0.03412055969238281 	 0.009707927703857422 	 5.459785461425781e-05 	 3.838539123535156e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:47:35.132070 test begin: paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[-1,], )
W0730 10:47:41.756414 142847 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[-1,], ) 	 254016180 	 1000 	 0.0194547176361084 	 0.007135868072509766 	 5.888938903808594e-05 	 2.5987625122070312e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:47:42.739623 test begin: paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[2,4,], )
W0730 10:47:49.295619 142939 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[2,4,], ) 	 254016180 	 1000 	 0.02372121810913086 	 0.008518457412719727 	 1.621246337890625e-05 	 3.0279159545898438e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:47:50.002053 test begin: paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[-1,1,3,], )
W0730 10:48:01.822810 143043 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[-1,1,3,], ) 	 254016240 	 1000 	 0.031407833099365234 	 0.00972294807434082 	 5.0067901611328125e-05 	 2.47955322265625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:48:04.382122 test begin: paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[-1,], )
W0730 10:48:11.238627 143298 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[-1,], ) 	 254016240 	 1000 	 0.01685333251953125 	 0.0071125030517578125 	 1.621246337890625e-05 	 2.6702880859375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:48:12.124154 test begin: paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[2,4,], )
W0730 10:48:18.831751 143475 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[2,4,], ) 	 254016240 	 1000 	 0.02352166175842285 	 0.008443117141723633 	 2.0503997802734375e-05 	 2.4318695068359375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:48:21.862822 test begin: paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[-1,1,3,], )
W0730 10:48:31.971976 143647 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[-1,1,3,], ) 	 254016120 	 1000 	 0.03458237648010254 	 0.009674072265625 	 4.315376281738281e-05 	 2.5033950805664062e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:48:33.041688 test begin: paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[-1,], )
W0730 10:48:39.845448 143839 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[-1,], ) 	 254016120 	 1000 	 0.016963958740234375 	 0.0072174072265625 	 1.33514404296875e-05 	 2.1457672119140625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:48:40.614574 test begin: paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[2,4,], )
W0730 10:48:47.449054 143930 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[2,4,], ) 	 254016120 	 1000 	 0.023662567138671875 	 0.013565540313720703 	 2.5272369384765625e-05 	 9.655952453613281e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:48:48.341833 test begin: paddle.empty_like(Tensor([1016064010],"uint8"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([1016064010],"uint8"), ) 	 1016064010 	 1000 	 0.01950359344482422 	 0.010520696640014648 	 2.0265579223632812e-05 	 6.079673767089844e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:48:59.124729 test begin: paddle.empty_like(Tensor([40960, 12404],"bool"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([40960, 12404],"bool"), ) 	 508067840 	 1000 	 0.019463062286376953 	 0.010067224502563477 	 1.9550323486328125e-05 	 4.1961669921875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:49:10.020600 test begin: paddle.empty_like(Tensor([40960, 12404],"float32"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([40960, 12404],"float32"), ) 	 508067840 	 1000 	 0.011761665344238281 	 0.008711576461791992 	 1.7642974853515625e-05 	 4.4345855712890625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:49:19.660102 test begin: paddle.empty_like(Tensor([7938010, 64],"bool"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([7938010, 64],"bool"), ) 	 508032640 	 1000 	 0.011748075485229492 	 0.00835728645324707 	 1.4066696166992188e-05 	 6.270408630371094e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:49:28.074278 test begin: paddle.empty_like(Tensor([7938010, 64],"float32"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([7938010, 64],"float32"), ) 	 508032640 	 1000 	 0.011762857437133789 	 0.01489400863647461 	 1.0728836059570312e-05 	 5.888938903808594e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:49:39.296121 test begin: paddle.equal_all(Tensor([101, 2, 10, 16],"bool"), Tensor([158761, 2, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([101, 2, 10, 16],"bool"), Tensor([158761, 2, 10, 16],"bool"), ) 	 50835840 	 1000 	 0.02452826499938965 	 0.004214286804199219 	 2.4318695068359375e-05 	 1.71661376953125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:49:40.087136 test begin: paddle.equal_all(Tensor([12801],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([12801],"float32"), Tensor([50803201],"float32"), ) 	 50816002 	 1000 	 0.023738861083984375 	 0.006133079528808594 	 1.8358230590820312e-05 	 8.606910705566406e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:49:40.939669 test begin: paddle.equal_all(Tensor([158761, 2, 10, 16],"bool"), Tensor([101, 2, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([158761, 2, 10, 16],"bool"), Tensor([101, 2, 10, 16],"bool"), ) 	 50835840 	 1000 	 0.02591109275817871 	 0.004237174987792969 	 2.8133392333984375e-05 	 1.8596649169921875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:49:41.694749 test begin: paddle.equal_all(Tensor([16, 3175201],"float32"), Tensor([1601, 16],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([16, 3175201],"float32"), Tensor([1601, 16],"float32"), ) 	 50828832 	 1000 	 0.02143120765686035 	 0.0042035579681396484 	 2.5272369384765625e-05 	 2.2172927856445312e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:49:42.498144 test begin: paddle.equal_all(Tensor([1601, 16],"float32"), Tensor([16, 3175201],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1601, 16],"float32"), Tensor([16, 3175201],"float32"), ) 	 50828832 	 1000 	 0.02449631690979004 	 0.004225492477416992 	 2.8133392333984375e-05 	 1.9311904907226562e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:49:43.403641 test begin: paddle.equal_all(Tensor([1601, 16],"float32"), Tensor([3175201, 16],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1601, 16],"float32"), Tensor([3175201, 16],"float32"), ) 	 50828832 	 1000 	 0.024547815322875977 	 0.004264354705810547 	 4.1484832763671875e-05 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:49:44.228679 test begin: paddle.equal_all(Tensor([3175201, 16],"float32"), Tensor([1601, 16],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([3175201, 16],"float32"), Tensor([1601, 16],"float32"), ) 	 50828832 	 1000 	 0.01766800880432129 	 0.0025293827056884766 	 1.3589859008789062e-05 	 1.7642974853515625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:49:45.015818 test begin: paddle.equal_all(Tensor([50803201],"float32"), Tensor([12801],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([50803201],"float32"), Tensor([12801],"float32"), ) 	 50816002 	 1000 	 0.02439260482788086 	 0.004247903823852539 	 1.1444091796875e-05 	 1.6927719116210938e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:49:45.852304 test begin: paddle.flatten(Tensor([40510, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([40510, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, ) 	 508157440 	 1000 	 0.010766983032226562 	 0.004373073577880859 	 1.6450881958007812e-05 	 2.3126602172851562e-05 	 0.042989253997802734 	 0.05152130126953125 	 5.054473876953125e-05 	 6.651878356933594e-05 	 
2025-07-30 10:50:01.729266 test begin: paddle.flatten(Tensor([40960, 254, 7, 7],"float32"), start_axis=1, stop_axis=-1, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([40960, 254, 7, 7],"float32"), start_axis=1, stop_axis=-1, ) 	 509788160 	 1000 	 0.005694389343261719 	 0.007782459259033203 	 1.0013580322265625e-05 	 2.6464462280273438e-05 	 0.05245804786682129 	 0.054277658462524414 	 7.987022399902344e-05 	 7.963180541992188e-05 	 
2025-07-30 10:50:20.416510 test begin: paddle.flatten(Tensor([40960, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([40960, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, ) 	 513802240 	 1000 	 0.005828380584716797 	 0.004285573959350586 	 8.58306884765625e-06 	 2.0503997802734375e-05 	 0.04302072525024414 	 0.051477909088134766 	 5.936622619628906e-05 	 6.461143493652344e-05 	 
2025-07-30 10:50:38.330051 test begin: paddle.flatten(Tensor([4160, 50, 10, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([4160, 50, 10, 256],"float32"), start_axis=2, ) 	 532480000 	 1000 	 0.0057604312896728516 	 0.004207611083984375 	 8.58306884765625e-06 	 1.9788742065429688e-05 	 0.04627227783203125 	 0.0574641227722168 	 3.504753112792969e-05 	 9.012222290039062e-05 	 
2025-07-30 10:50:54.911985 test begin: paddle.flatten(Tensor([4160, 50, 7, 349],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([4160, 50, 7, 349],"float32"), start_axis=2, ) 	 508144000 	 1000 	 0.00572657585144043 	 0.004166364669799805 	 1.430511474609375e-05 	 2.09808349609375e-05 	 0.04336404800415039 	 0.053545475006103516 	 4.0531158447265625e-05 	 8.463859558105469e-05 	 
2025-07-30 10:51:11.394860 test begin: paddle.flatten(Tensor([4160, 69, 7, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([4160, 69, 7, 256],"float32"), start_axis=2, ) 	 514375680 	 1000 	 0.00566864013671875 	 0.00420379638671875 	 1.3113021850585938e-05 	 1.9073486328125e-05 	 0.042836666107177734 	 0.05241107940673828 	 3.600120544433594e-05 	 7.677078247070312e-05 	 
2025-07-30 10:51:27.277188 test begin: paddle.flatten(Tensor([5120, 50, 7, 284],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([5120, 50, 7, 284],"float32"), start_axis=2, ) 	 508928000 	 1000 	 0.005728721618652344 	 0.004149436950683594 	 9.298324584960938e-06 	 1.9073486328125e-05 	 0.04308152198791504 	 0.05179262161254883 	 3.2901763916015625e-05 	 5.793571472167969e-05 	 
2025-07-30 10:51:43.302774 test begin: paddle.flatten(Tensor([5120, 50, 8, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([5120, 50, 8, 256],"float32"), start_axis=2, ) 	 524288000 	 1000 	 0.005616664886474609 	 0.004135847091674805 	 8.344650268554688e-06 	 1.8596649169921875e-05 	 0.04275321960449219 	 0.05233025550842285 	 3.528594970703125e-05 	 6.222724914550781e-05 	 
2025-07-30 10:51:59.860690 test begin: paddle.flatten(Tensor([5120, 56, 7, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([5120, 56, 7, 256],"float32"), start_axis=2, ) 	 513802240 	 1000 	 0.010604143142700195 	 0.007585763931274414 	 1.52587890625e-05 	 2.09808349609375e-05 	 0.052350521087646484 	 0.05970263481140137 	 7.557868957519531e-05 	 6.079673767089844e-05 	 
2025-07-30 10:52:18.872249 test begin: paddle.flatten(Tensor([5680, 50, 7, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([5680, 50, 7, 256],"float32"), start_axis=2, ) 	 508928000 	 1000 	 0.0056879520416259766 	 0.004100799560546875 	 1.2159347534179688e-05 	 1.8835067749023438e-05 	 0.04319143295288086 	 0.05312824249267578 	 3.814697265625e-05 	 9.34600830078125e-05 	 
2025-07-30 10:52:39.601723 test begin: paddle.full_like(Tensor([10, 1, 2048, 24807],"bool"), -65504.0, dtype=Dtype(float16), )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([10, 1, 2048, 24807],"bool"), -65504.0, dtype=Dtype(float16), ) 	 508047360 	 1000 	 0.6573586463928223 	 0.6579082012176514 	 0.6363787651062012 	 0.6375672817230225 	 None 	 None 	 None 	 None 	 
2025-07-30 10:52:48.883527 test begin: paddle.full_like(Tensor([10, 1, 24807, 2048],"bool"), -65504.0, dtype=Dtype(float16), )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([10, 1, 24807, 2048],"bool"), -65504.0, dtype=Dtype(float16), ) 	 508047360 	 1000 	 0.657111406326294 	 0.6577365398406982 	 0.6463375091552734 	 0.6442060470581055 	 None 	 None 	 None 	 None 	 
2025-07-30 10:52:57.396776 test begin: paddle.full_like(Tensor([10, 13, 2048, 2048],"bool"), -65504.0, dtype=Dtype(float16), )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([10, 13, 2048, 2048],"bool"), -65504.0, dtype=Dtype(float16), ) 	 545259520 	 1000 	 0.705237865447998 	 0.7054927349090576 	 0.6944818496704102 	 0.6919350624084473 	 None 	 None 	 None 	 None 	 
2025-07-30 10:53:06.262444 test begin: paddle.gather_nd(Tensor([1001, 413, 128],"float32"), index=Tensor([20, 50, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([1001, 413, 128],"float32"), index=Tensor([20, 50, 2],"int64"), ) 	 52918864 	 1000 	 0.011567831039428711 	 105.64383578300476 	 1.2874603271484375e-05 	 0.0002396106719970703 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:54.169600 test begin: paddle.gather_nd(Tensor([1001, 413, 128],"float32"), index=Tensor([5, 50, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([1001, 413, 128],"float32"), index=Tensor([5, 50, 2],"int64"), ) 	 52917364 	 1000 	 0.011486053466796875 	 20.186482191085815 	 1.3113021850585938e-05 	 0.0002200603485107422 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:55:15.410860 test begin: paddle.gather_nd(Tensor([101, 1417, 716],"bfloat16"), Tensor([778, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([101, 1417, 716],"bfloat16"), Tensor([778, 2],"int64"), ) 	 102473328 	 1000 	 0.011144876480102539 	 62.473283767700195 	 2.2411346435546875e-05 	 0.00023651123046875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:56:19.883162 test begin: paddle.gather_nd(Tensor([101, 1417, 716],"bfloat16"), Tensor([816, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([101, 1417, 716],"bfloat16"), Tensor([816, 2],"int64"), ) 	 102473404 	 1000 	 0.011076688766479492 	 65.50204157829285 	 1.5020370483398438e-05 	 0.00024437904357910156 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:57:27.314437 test begin: paddle.gather_nd(Tensor([101, 819, 1240],"bfloat16"), Tensor([778, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([101, 819, 1240],"bfloat16"), Tensor([778, 2],"int64"), ) 	 102573116 	 1000 	 0.012136697769165039 	 62.41209006309509 	 0.0012540817260742188 	 0.0002334117889404297 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:58:31.747737 test begin: paddle.gather_nd(Tensor([101, 8192, 124],"bfloat16"), Tensor([816, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([101, 8192, 124],"bfloat16"), Tensor([816, 2],"int64"), ) 	 102598240 	 1000 	 0.011074066162109375 	 65.65632605552673 	 1.4066696166992188e-05 	 0.00023674964904785156 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:59:39.355118 test begin: paddle.geometric.send_uv(Tensor([1270081, 20],"float64"), Tensor([1270081, 1],"float64"), Tensor([1501],"int64"), Tensor([1501],"int64"), "add", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([1270081, 20],"float64"), Tensor([1270081, 1],"float64"), Tensor([1501],"int64"), Tensor([1501],"int64"), "add", ) 	 26674703 	 1000 	 0.8390204906463623 	 0.02209758758544922 	 5.7220458984375e-05 	 3.743171691894531e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:59:41.932287 test begin: paddle.geometric.send_uv(Tensor([25401601, 1],"float64"), Tensor([25401601, 20],"float64"), Tensor([1501],"int64"), Tensor([1501],"int64"), "mul", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([25401601, 1],"float64"), Tensor([25401601, 20],"float64"), Tensor([1501],"int64"), Tensor([1501],"int64"), "mul", ) 	 533436623 	 1000 	 0.31145572662353516 	 0.011916399002075195 	 3.981590270996094e-05 	 3.457069396972656e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 11:00:05.097614 test begin: paddle.geometric.send_uv(Tensor([25401601, 20],"float64"), Tensor([25401601, 1],"float64"), Tensor([1501],"int64"), Tensor([1501],"int64"), "add", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([25401601, 20],"float64"), Tensor([25401601, 1],"float64"), Tensor([1501],"int64"), Tensor([1501],"int64"), "add", ) 	 533436623 	 1000 	 3.1414365768432617 	 0.014585733413696289 	 5.2928924560546875e-05 	 6.461143493652344e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 11:00:34.123183 test begin: paddle.histogram_bin_edges(Tensor([50, 10160641],"float32"), bins=10, min=0, max=1, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([50, 10160641],"float32"), bins=10, min=0, max=1, ) 	 508032050 	 1000 	 0.10531449317932129 	 0.016005992889404297 	 3.3855438232421875e-05 	 4.267692565917969e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 11:00:46.656359 test begin: paddle.histogram_bin_edges(Tensor([50, 10160641],"float32"), bins=10, min=1, max=1, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([50, 10160641],"float32"), bins=10, min=1, max=1, ) 	 508032050 	 1000 	 0.11846089363098145 	 0.01610088348388672 	 4.792213439941406e-05 	 5.8650970458984375e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 11:00:55.593491 test begin: paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[-1,1,3,], )
W0730 11:01:04.477070  1865 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[-1,1,3,], ) 	 254016180 	 1000 	 0.0309293270111084 	 0.00962066650390625 	 2.1457672119140625e-05 	 2.6702880859375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:01:05.482256 test begin: paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[-1,], )
W0730 11:01:11.998853  2654 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[-1,], ) 	 254016180 	 1000 	 0.0171663761138916 	 0.0071544647216796875 	 1.7404556274414062e-05 	 2.9087066650390625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:01:12.685075 test begin: paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[2,4,], )
W0730 11:01:19.361882  2822 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[2,4,], ) 	 254016180 	 1000 	 0.02392578125 	 0.008462667465209961 	 1.4781951904296875e-05 	 2.6702880859375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:01:20.976082 test begin: paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[-1,1,3,], )
W0730 11:01:33.176916  3145 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[-1,1,3,], ) 	 254016120 	 1000 	 0.053173065185546875 	 0.015118837356567383 	 2.3126602172851562e-05 	 2.5510787963867188e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:01:37.058828 test begin: paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[-1,], )
W0730 11:01:43.606878  3642 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[-1,], ) 	 254016120 	 1000 	 0.028868913650512695 	 0.014637231826782227 	 1.71661376953125e-05 	 7.2479248046875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:01:44.480077 test begin: paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[2,4,], )
W0730 11:01:51.036444  3975 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[2,4,], ) 	 254016120 	 1000 	 0.024041414260864258 	 0.008422613143920898 	 1.33514404296875e-05 	 2.1696090698242188e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:01:51.724958 test begin: paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[-1,1,3,], )
W0730 11:02:01.939409  4159 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[-1,1,3,], ) 	 254016240 	 1000 	 0.031023263931274414 	 0.009768962860107422 	 2.2411346435546875e-05 	 3.5762786865234375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:02:03.842589 test begin: paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[-1,], )
W0730 11:02:11.804827  4675 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[-1,], ) 	 254016240 	 1000 	 0.01712489128112793 	 0.007252693176269531 	 1.3113021850585938e-05 	 2.6226043701171875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:02:12.909434 test begin: paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[2,4,], )
W0730 11:02:19.649665  5059 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[2,4,], ) 	 254016240 	 1000 	 0.040715932846069336 	 0.013567209243774414 	 1.8835067749023438e-05 	 2.6702880859375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:02:21.092042 test begin: paddle.is_complex(Tensor([1003520, 507],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([1003520, 507],"float32"), ) 	 508784640 	 1000 	 0.0034894943237304688 	 0.0017704963684082031 	 7.867813110351562e-06 	 1.5735626220703125e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:02:29.231748 test begin: paddle.is_complex(Tensor([5070, 100352],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([5070, 100352],"float32"), ) 	 508784640 	 1000 	 0.003457784652709961 	 0.0017924308776855469 	 1.1682510375976562e-05 	 1.6450881958007812e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:02:39.240055 test begin: paddle.is_complex(Tensor([62020, 8192],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([62020, 8192],"float32"), ) 	 508067840 	 1000 	 0.004400730133056641 	 0.0021991729736328125 	 7.62939453125e-06 	 1.8596649169921875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:02:47.501558 test begin: paddle.is_complex(Tensor([81920, 6202],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([81920, 6202],"float32"), ) 	 508067840 	 1000 	 0.003448963165283203 	 0.001783609390258789 	 6.198883056640625e-06 	 1.621246337890625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:02:55.216683 test begin: paddle.is_complex(Tensor([8860, 57344],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([8860, 57344],"float32"), ) 	 508067840 	 1000 	 0.003583192825317383 	 0.0018057823181152344 	 8.58306884765625e-06 	 1.6689300537109375e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 11:03:02.964309 test begin: paddle.is_empty(Tensor([101606410, 5],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(Tensor([101606410, 5],"float32"), ) 	 508032050 	 1000 	 0.007446765899658203 	 0.002351522445678711 	 1.0967254638671875e-05 	 1.7881393432617188e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 11:03:11.837426 test begin: paddle.is_empty(Tensor([169344010, 3],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(Tensor([169344010, 3],"float32"), ) 	 508032030 	 1000 	 0.007439136505126953 	 0.0023491382598876953 	 1.9550323486328125e-05 	 1.811981201171875e-05 	 None 	 None 	 None 	 None 	 combined
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 9, in <module>
    from tester import (APIConfig, APITestAccuracy, APITestCINNVSDygraph,
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/__init__.py", line 74, in __getattr__
    from .api_config import APIConfig
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/__init__.py", line 22, in __getattr__
    from .config_analyzer import APIConfig
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py", line 9, in <module>
    import torch
  File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 409, in <module>
    from torch._C import *  # noqa: F403
  File "<frozen importlib._bootstrap>", line 216, in _lock_unlock_module
KeyboardInterrupt
2025-07-30 10:36:16.755305 test begin: paddle.is_empty(Tensor([20, 25401601],"float32"), )
W0730 10:36:28.250250 131614 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.is_empty 	 paddle.is_empty(Tensor([20, 25401601],"float32"), ) 	 508032020 	 1000 	 0.0037107467651367188 	 0.0018913745880126953 	 1.1444091796875e-05 	 1.8596649169921875e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:36:29.352315 test begin: paddle.is_empty(Tensor([30, 16934401],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(Tensor([30, 16934401],"float32"), ) 	 508032030 	 1000 	 0.0036649703979492188 	 0.001676797866821289 	 1.2636184692382812e-05 	 1.9073486328125e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:36:38.293243 test begin: paddle.is_empty(x=Tensor([40, 32, 396901],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(x=Tensor([40, 32, 396901],"float32"), ) 	 508033280 	 1000 	 0.0038971900939941406 	 0.0016469955444335938 	 1.1205673217773438e-05 	 1.52587890625e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:36:46.734024 test begin: paddle.is_empty(x=Tensor([40, 396901, 32],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(x=Tensor([40, 396901, 32],"float32"), ) 	 508033280 	 1000 	 0.0038254261016845703 	 0.0016765594482421875 	 6.9141387939453125e-06 	 1.430511474609375e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:36:55.082648 test begin: paddle.is_empty(x=Tensor([496130, 32, 32],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(x=Tensor([496130, 32, 32],"float32"), ) 	 508037120 	 1000 	 0.003912448883056641 	 0.0016374588012695312 	 8.821487426757812e-06 	 1.4543533325195312e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:37:03.369407 test begin: paddle.isreal(Tensor([15876010, 32],"bool"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([15876010, 32],"bool"), ) 	 508032320 	 1000 	 0.384476900100708 	 0.33185315132141113 	 0.36791372299194336 	 0.3186788558959961 	 None 	 None 	 None 	 None 	 
2025-07-30 10:37:11.145265 test begin: paddle.isreal(Tensor([31752010, 32],"bfloat16"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([31752010, 32],"bfloat16"), ) 	 1016064320 	 1000 	 0.7654469013214111 	 1.3324673175811768 	 0.7480638027191162 	 0.6429905891418457 	 None 	 None 	 None 	 None 	 
2025-07-30 10:37:33.355956 test begin: paddle.isreal(Tensor([31752010, 32],"float16"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([31752010, 32],"float16"), ) 	 1016064320 	 1000 	 0.7644858360290527 	 0.6606643199920654 	 0.7473483085632324 	 0.6461505889892578 	 None 	 None 	 None 	 None 	 
2025-07-30 10:37:54.731633 test begin: paddle.isreal(Tensor([640, 1587601],"bfloat16"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([640, 1587601],"bfloat16"), ) 	 1016064640 	 1000 	 0.765427827835083 	 0.6578702926635742 	 0.7480068206787109 	 0.645439624786377 	 None 	 None 	 None 	 None 	 
2025-07-30 10:38:12.855164 test begin: paddle.isreal(Tensor([640, 1587601],"float16"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([640, 1587601],"float16"), ) 	 1016064640 	 1000 	 0.764376163482666 	 0.6579570770263672 	 0.7481265068054199 	 0.6455316543579102 	 None 	 None 	 None 	 None 	 
2025-07-30 10:38:35.385438 test begin: paddle.isreal(Tensor([640, 793801],"bool"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([640, 793801],"bool"), ) 	 508032640 	 1000 	 0.3841667175292969 	 0.33845949172973633 	 0.368009090423584 	 0.3170278072357178 	 None 	 None 	 None 	 None 	 
2025-07-30 10:38:43.237902 test begin: paddle.linalg.matrix_transpose(Tensor([20, 3, 8467201],"float32"), )
[Prof] paddle.linalg.matrix_transpose 	 paddle.linalg.matrix_transpose(Tensor([20, 3, 8467201],"float32"), ) 	 508032060 	 1000 	 0.0045964717864990234 	 0.003915548324584961 	 1.1682510375976562e-05 	 2.1457672119140625e-05 	 0.040825605392456055 	 0.05778908729553223 	 4.00543212890625e-05 	 6.723403930664062e-05 	 combined
2025-07-30 10:39:00.924333 test begin: paddle.linalg.matrix_transpose(Tensor([20, 6350401, 4],"float32"), )
[Prof] paddle.linalg.matrix_transpose 	 paddle.linalg.matrix_transpose(Tensor([20, 6350401, 4],"float32"), ) 	 508032080 	 1000 	 0.00449824333190918 	 0.003827810287475586 	 1.4543533325195312e-05 	 2.1219253540039062e-05 	 0.038684844970703125 	 0.05138039588928223 	 3.4332275390625e-05 	 4.649162292480469e-05 	 combined
2025-07-30 10:39:18.003855 test begin: paddle.linalg.matrix_transpose(Tensor([42336010, 3, 4],"float32"), )
[Prof] paddle.linalg.matrix_transpose 	 paddle.linalg.matrix_transpose(Tensor([42336010, 3, 4],"float32"), ) 	 508032120 	 1000 	 0.0044667720794677734 	 0.003875255584716797 	 1.5735626220703125e-05 	 1.9073486328125e-05 	 0.0386655330657959 	 0.051778554916381836 	 2.4318695068359375e-05 	 5.7220458984375e-05 	 combined
2025-07-30 10:39:36.782921 test begin: paddle.logical_not(Tensor([2150400, 237],"bool"), )
[Prof] paddle.logical_not 	 paddle.logical_not(Tensor([2150400, 237],"bool"), ) 	 509644800 	 1000 	 0.7811739444732666 	 1.632422685623169 	 0.7659275531768799 	 0.7293069362640381 	 None 	 None 	 None 	 None 	 
2025-07-30 10:39:47.252841 test begin: paddle.logical_not(Tensor([2204160, 231],"bool"), )
[Prof] paddle.logical_not 	 paddle.logical_not(Tensor([2204160, 231],"bool"), ) 	 509160960 	 1000 	 0.7844297885894775 	 0.7490503787994385 	 0.7761855125427246 	 0.736534595489502 	 None 	 None 	 None 	 None 	 
2025-07-30 10:39:56.254159 test begin: paddle.logical_not(Tensor([2257920, 226],"bool"), )
[Prof] paddle.logical_not 	 paddle.logical_not(Tensor([2257920, 226],"bool"), ) 	 510289920 	 1000 	 0.7821171283721924 	 0.7503490447998047 	 0.7700848579406738 	 0.7373371124267578 	 None 	 None 	 None 	 None 	 
2025-07-30 10:40:05.081781 test begin: paddle.logical_not(Tensor([6350410, 80],"bool"), )
[Prof] paddle.logical_not 	 paddle.logical_not(Tensor([6350410, 80],"bool"), ) 	 508032800 	 1000 	 0.7782113552093506 	 0.7504429817199707 	 0.7699170112609863 	 0.7353405952453613 	 None 	 None 	 None 	 None 	 
2025-07-30 10:40:15.809007 test begin: paddle.matrix_transpose(Tensor([20, 12700801, 4],"float16"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 12700801, 4],"float16"), ) 	 1016064080 	 1000 	 0.004632711410522461 	 0.003858804702758789 	 2.4080276489257812e-05 	 1.9550323486328125e-05 	 0.04866933822631836 	 0.051351070404052734 	 2.9087066650390625e-05 	 6.604194641113281e-05 	 combined
2025-07-30 10:40:54.547332 test begin: paddle.matrix_transpose(Tensor([20, 3, 16934401],"float16"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 3, 16934401],"float16"), ) 	 1016064060 	 1000 	 0.006131887435913086 	 0.007102012634277344 	 3.218650817871094e-05 	 1.9788742065429688e-05 	 0.04580974578857422 	 0.058217763900756836 	 4.696846008300781e-05 	 6.985664367675781e-05 	 combined
2025-07-30 10:41:33.877478 test begin: paddle.matrix_transpose(Tensor([20, 3, 4233601],"float64"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 3, 4233601],"float64"), ) 	 254016060 	 1000 	 0.004542112350463867 	 0.003835439682006836 	 1.5497207641601562e-05 	 1.8835067749023438e-05 	 0.040439605712890625 	 0.05234956741333008 	 5.173683166503906e-05 	 5.2928924560546875e-05 	 combined
2025-07-30 10:41:44.390689 test begin: paddle.matrix_transpose(Tensor([20, 3, 8467201],"float32"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 3, 8467201],"float32"), ) 	 508032060 	 1000 	 0.0044689178466796875 	 0.0038690567016601562 	 1.430511474609375e-05 	 1.9788742065429688e-05 	 0.03832364082336426 	 0.05144238471984863 	 3.170967102050781e-05 	 4.1961669921875e-05 	 combined
2025-07-30 10:42:01.753705 test begin: paddle.matrix_transpose(Tensor([20, 3175201, 4],"float64"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 3175201, 4],"float64"), ) 	 254016080 	 1000 	 0.004529714584350586 	 0.003835916519165039 	 1.3113021850585938e-05 	 1.9311904907226562e-05 	 0.03919696807861328 	 0.05587649345397949 	 4.029273986816406e-05 	 7.033348083496094e-05 	 combined
2025-07-30 10:42:12.621865 test begin: paddle.matrix_transpose(Tensor([20, 6350401, 4],"float32"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 6350401, 4],"float32"), ) 	 508032080 	 1000 	 0.004598140716552734 	 0.003861665725708008 	 1.1444091796875e-05 	 1.8835067749023438e-05 	 0.038039445877075195 	 0.05228114128112793 	 3.24249267578125e-05 	 6.556510925292969e-05 	 combined
2025-07-30 10:42:29.525844 test begin: paddle.matrix_transpose(Tensor([21168010, 3, 4],"float64"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([21168010, 3, 4],"float64"), ) 	 254016120 	 1000 	 0.00451350212097168 	 0.007122039794921875 	 1.9311904907226562e-05 	 2.1457672119140625e-05 	 0.03818821907043457 	 0.059099435806274414 	 3.0517578125e-05 	 7.581710815429688e-05 	 combined
2025-07-30 10:42:41.428913 test begin: paddle.matrix_transpose(Tensor([42336010, 3, 4],"float32"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([42336010, 3, 4],"float32"), ) 	 508032120 	 1000 	 0.004498481750488281 	 0.00735783576965332 	 1.71661376953125e-05 	 4.6253204345703125e-05 	 0.04548048973083496 	 0.05796456336975098 	 4.863739013671875e-05 	 5.1021575927734375e-05 	 combined
2025-07-30 10:43:00.157483 test begin: paddle.matrix_transpose(Tensor([84672010, 3, 4],"float16"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([84672010, 3, 4],"float16"), ) 	 1016064120 	 1000 	 0.00449371337890625 	 0.0040013790130615234 	 1.2636184692382812e-05 	 1.9073486328125e-05 	 0.060424089431762695 	 0.051496028900146484 	 5.507469177246094e-05 	 5.936622619628906e-05 	 combined
2025-07-30 10:43:40.401716 test begin: paddle.moveaxis(Tensor([20, 3, 120961, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([20, 3, 120961, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 254018100 	 1000 	 0.008880615234375 	 0.0060520172119140625 	 6.9141387939453125e-06 	 2.0503997802734375e-05 	 0.038329362869262695 	 0.054213762283325195 	 2.8133392333984375e-05 	 5.888938903808594e-05 	 
2025-07-30 10:43:51.249292 test begin: paddle.moveaxis(Tensor([20, 3, 4, 151201, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([20, 3, 4, 151201, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 254017680 	 1000 	 0.00798487663269043 	 0.006029605865478516 	 1.5735626220703125e-05 	 2.1219253540039062e-05 	 0.038442134857177734 	 0.05598640441894531 	 2.5272369384765625e-05 	 6.365776062011719e-05 	 
2025-07-30 10:44:03.994428 test begin: paddle.moveaxis(Tensor([20, 3, 4, 5, 211681],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([20, 3, 4, 5, 211681],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 254017200 	 1000 	 0.008099555969238281 	 0.005924224853515625 	 1.1205673217773438e-05 	 1.9311904907226562e-05 	 0.03844141960144043 	 0.053362131118774414 	 2.8133392333984375e-05 	 6.866455078125e-05 	 
2025-07-30 10:44:14.842205 test begin: paddle.moveaxis(Tensor([20, 90721, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([20, 90721, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 254018800 	 1000 	 0.008156299591064453 	 0.006042003631591797 	 1.6689300537109375e-05 	 2.193450927734375e-05 	 0.05209922790527344 	 0.05283999443054199 	 4.935264587402344e-05 	 5.459785461425781e-05 	 
2025-07-30 10:44:25.517325 test begin: paddle.moveaxis(Tensor([604810, 3, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([604810, 3, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 254020200 	 1000 	 0.008113384246826172 	 0.0059680938720703125 	 1.239776611328125e-05 	 1.9073486328125e-05 	 0.03850197792053223 	 0.05312943458557129 	 3.647804260253906e-05 	 4.38690185546875e-05 	 
2025-07-30 10:44:37.243216 test begin: paddle.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=0, destination=2, ) 	 254018100 	 1000 	 0.006997585296630859 	 0.0047838687896728516 	 7.867813110351562e-06 	 1.8358230590820312e-05 	 0.0386199951171875 	 0.05454611778259277 	 4.38690185546875e-05 	 6.031990051269531e-05 	 
2025-07-30 10:44:48.097483 test begin: paddle.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254018100 	 1000 	 0.007633686065673828 	 0.005913496017456055 	 1.1682510375976562e-05 	 2.002716064453125e-05 	 0.03909778594970703 	 0.058847665786743164 	 4.76837158203125e-05 	 5.9604644775390625e-05 	 
2025-07-30 10:44:59.119160 test begin: paddle.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=0, destination=2, ) 	 254017680 	 1000 	 0.010116100311279297 	 0.0046770572662353516 	 3.838539123535156e-05 	 1.8358230590820312e-05 	 0.04440045356750488 	 0.05472922325134277 	 3.5762786865234375e-05 	 6.67572021484375e-05 	 
2025-07-30 10:45:09.881060 test begin: paddle.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254017680 	 1000 	 0.007594585418701172 	 0.0064544677734375 	 9.298324584960938e-06 	 6.341934204101562e-05 	 0.04035496711730957 	 0.05334973335266113 	 3.719329833984375e-05 	 6.175041198730469e-05 	 
2025-07-30 10:45:20.973235 test begin: paddle.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=0, destination=2, ) 	 254017200 	 1000 	 0.007056236267089844 	 0.0047070980072021484 	 7.62939453125e-06 	 2.0742416381835938e-05 	 0.03896331787109375 	 0.05306410789489746 	 4.172325134277344e-05 	 4.291534423828125e-05 	 
2025-07-30 10:45:31.682164 test begin: paddle.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254017200 	 1000 	 0.0074918270111083984 	 0.005997419357299805 	 8.106231689453125e-06 	 2.0742416381835938e-05 	 0.04040050506591797 	 0.052999019622802734 	 4.291534423828125e-05 	 6.508827209472656e-05 	 
2025-07-30 10:45:42.478837 test begin: paddle.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=0, destination=2, ) 	 254018800 	 1000 	 0.0071566104888916016 	 0.007368803024291992 	 8.106231689453125e-06 	 5.7220458984375e-05 	 0.038687705993652344 	 0.05444812774658203 	 3.790855407714844e-05 	 5.507469177246094e-05 	 
2025-07-30 10:45:53.273346 test begin: paddle.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254018800 	 1000 	 0.009499311447143555 	 0.006458759307861328 	 3.457069396972656e-05 	 7.081031799316406e-05 	 0.03847837448120117 	 0.06130099296569824 	 1.8835067749023438e-05 	 7.462501525878906e-05 	 
2025-07-30 10:46:04.309700 test begin: paddle.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=0, destination=2, ) 	 254020200 	 1000 	 0.007025718688964844 	 0.004741668701171875 	 8.344650268554688e-06 	 1.9788742065429688e-05 	 0.04640841484069824 	 0.06879782676696777 	 4.553794860839844e-05 	 5.626678466796875e-05 	 
2025-07-30 10:46:15.210435 test begin: paddle.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254020200 	 1000 	 0.007644176483154297 	 0.005990028381347656 	 1.1205673217773438e-05 	 1.8596649169921875e-05 	 0.03824138641357422 	 0.05364704132080078 	 2.5510787963867188e-05 	 6.103515625e-05 	 
2025-07-30 10:46:25.963088 test begin: paddle.multiplex(inputs=list[Tensor([12700801, 4],"float32"),Tensor([12700801, 4],"float32"),], index=Tensor([601, 1],"int32"), )
[Prof] paddle.multiplex 	 paddle.multiplex(inputs=list[Tensor([12700801, 4],"float32"),Tensor([12700801, 4],"float32"),], index=Tensor([601, 1],"int32"), ) 	 101607009 	 1000 	 1.9593925476074219 	 15.66197395324707 	 8.940696716308594e-05 	 0.0002377033233642578 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:46:47.390505 test begin: paddle.nn.functional.dropout(Tensor([75760, 13412],"bfloat16"), 0.0, training=True, mode="upscale_in_train", )
[Prof] paddle.nn.functional.dropout 	 paddle.nn.functional.dropout(Tensor([75760, 13412],"bfloat16"), 0.0, training=True, mode="upscale_in_train", ) 	 1016093120 	 1000 	 0.0011050701141357422 	 0.007178306579589844 	 1.2159347534179688e-05 	 2.2649765014648438e-05 	 0.03795289993286133 	 4.484832286834717 	 4.172325134277344e-05 	 2.290818452835083 	 combined
2025-07-30 10:47:26.169314 test begin: paddle.nn.functional.dropout(Tensor([77120, 13176],"bfloat16"), 0.0, training=True, mode="upscale_in_train", )
[Prof] paddle.nn.functional.dropout 	 paddle.nn.functional.dropout(Tensor([77120, 13176],"bfloat16"), 0.0, training=True, mode="upscale_in_train", ) 	 1016133120 	 1000 	 0.0010023117065429688 	 0.007180452346801758 	 8.106231689453125e-06 	 2.0265579223632812e-05 	 0.050759315490722656 	 4.487989187240601 	 4.649162292480469e-05 	 2.293977737426758 	 combined
2025-07-30 10:48:04.601998 test begin: paddle.nn.functional.dropout(Tensor([793810, 1280],"bfloat16"), 0.0, training=True, mode="upscale_in_train", )
[Prof] paddle.nn.functional.dropout 	 paddle.nn.functional.dropout(Tensor([793810, 1280],"bfloat16"), 0.0, training=True, mode="upscale_in_train", ) 	 1016076800 	 1000 	 0.0009970664978027344 	 0.009775638580322266 	 1.1444091796875e-05 	 2.1457672119140625e-05 	 0.030753135681152344 	 4.487363815307617 	 1.7404556274414062e-05 	 2.293672800064087 	 combined
2025-07-30 10:48:45.676675 test begin: paddle.nn.functional.dropout(Tensor([81680, 12440],"bfloat16"), 0.0, training=True, mode="upscale_in_train", )
[Prof] paddle.nn.functional.dropout 	 paddle.nn.functional.dropout(Tensor([81680, 12440],"bfloat16"), 0.0, training=True, mode="upscale_in_train", ) 	 1016099200 	 1000 	 0.0010018348693847656 	 0.007214069366455078 	 1.2636184692382812e-05 	 3.457069396972656e-05 	 0.030942440032958984 	 4.486952304840088 	 2.0742416381835938e-05 	 2.293598175048828 	 combined
2025-07-30 10:49:24.646135 test begin: paddle.nn.functional.embedding(Tensor([101, 1024],"int64"), weight=Tensor([151936, 669],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, )
W0730 10:49:27.973949 145200 backward.cc:462] While running Node (EmbeddingGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.nn.functional.embedding 	 paddle.nn.functional.embedding(Tensor([101, 1024],"int64"), weight=Tensor([151936, 669],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, ) 	 101748608 	 1000 	 0.373049259185791 	 0.9201798439025879 	 0.3614795207977295 	 0.8990006446838379 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:49:29.484362 test begin: paddle.nn.functional.embedding(Tensor([101, 1024],"int64"), weight=Tensor([24807, 4096],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, )
W0730 10:49:39.531550 145222 backward.cc:462] While running Node (EmbeddingGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.nn.functional.embedding 	 paddle.nn.functional.embedding(Tensor([101, 1024],"int64"), weight=Tensor([24807, 4096],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, ) 	 101712896 	 1000 	 1.769852876663208 	 5.5099406242370605 	 1.7572531700134277 	 5.48528265953064 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:49:45.988825 test begin: paddle.nn.functional.embedding(Tensor([101, 4097],"int64"), weight=Tensor([100352, 1013],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, )
W0730 10:49:55.949704 145493 backward.cc:462] While running Node (EmbeddingGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.nn.functional.embedding 	 paddle.nn.functional.embedding(Tensor([101, 4097],"int64"), weight=Tensor([100352, 1013],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, ) 	 102070373 	 1000 	 1.8845765590667725 	 5.5381810665130615 	 1.871079921722412 	 5.500774145126343 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:50:02.268685 test begin: paddle.nn.functional.embedding(Tensor([801, 1024],"int64"), weight=Tensor([50304, 2020],"float16"), padding_idx=None, sparse=False, name=None, )
[Prof] paddle.nn.functional.embedding 	 paddle.nn.functional.embedding(Tensor([801, 1024],"int64"), weight=Tensor([50304, 2020],"float16"), padding_idx=None, sparse=False, name=None, ) 	 102434304 	 1000 	 7.184825897216797 	 22.2706036567688 	 7.173820972442627 	 22.239949941635132 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:51:15.127517 test begin: paddle.nn.functional.grid_sample(Tensor([1000, 1, 662, 768],"float32"), Tensor([1000, 1, 12544, 2],"float32"), align_corners=False, )
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(Tensor([1000, 1, 662, 768],"float32"), Tensor([1000, 1, 12544, 2],"float32"), align_corners=False, ) 	 533504000 	 1000 	 0.5549123287200928 	 0.5504555702209473 	 0.5429651737213135 	 0.5325195789337158 	 3.288309097290039 	 3.0775632858276367 	 1.6811168193817139 	 1.5732405185699463 	 
2025-07-30 10:51:33.195105 test begin: paddle.nn.functional.grid_sample(Tensor([1000, 1, 662, 768],"float32"), Tensor([1000, 1, 662, 2],"float32"), align_corners=False, )
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(Tensor([1000, 1, 662, 768],"float32"), Tensor([1000, 1, 662, 2],"float32"), align_corners=False, ) 	 509740000 	 1000 	 0.1029059886932373 	 0.09855985641479492 	 0.09148669242858887 	 0.08076286315917969 	 1.7910666465759277 	 1.593221664428711 	 0.914926290512085 	 0.8147814273834229 	 
2025-07-30 10:51:45.760479 test begin: paddle.nn.functional.grid_sample(Tensor([1000, 1, 768, 662],"float32"), Tensor([1000, 1, 12544, 2],"float32"), align_corners=False, )
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(Tensor([1000, 1, 768, 662],"float32"), Tensor([1000, 1, 12544, 2],"float32"), align_corners=False, ) 	 533504000 	 1000 	 0.5693738460540771 	 0.5681567192077637 	 0.5501277446746826 	 0.5402288436889648 	 3.3531906604766846 	 3.1631650924682617 	 1.71340012550354 	 1.6162240505218506 	 
2025-07-30 10:52:02.704792 test begin: paddle.nn.functional.grid_sample(Tensor([1000, 1, 768, 768],"float32"), Tensor([1000, 1, 12544, 2],"float32"), align_corners=False, )
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(Tensor([1000, 1, 768, 768],"float32"), Tensor([1000, 1, 12544, 2],"float32"), align_corners=False, ) 	 614912000 	 1000 	 0.5984208583831787 	 0.5878279209136963 	 0.5860650539398193 	 0.5633645057678223 	 3.646144151687622 	 3.3468503952026367 	 1.86226224899292 	 1.1404430866241455 	 
2025-07-30 10:52:21.514689 test begin: paddle.nn.functional.grid_sample(Tensor([1720, 1, 544, 544],"float32"), Tensor([1720, 1, 12544, 2],"float32"), align_corners=False, )
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(Tensor([1720, 1, 544, 544],"float32"), Tensor([1720, 1, 12544, 2],"float32"), align_corners=False, ) 	 552161280 	 1000 	 0.7179880142211914 	 0.7473647594451904 	 0.7061359882354736 	 0.7200839519500732 	 4.077713966369629 	 3.842921018600464 	 2.1099777221679688 	 1.9639379978179932 	 
2025-07-30 10:52:44.801314 test begin: paddle.nn.functional.grid_sample(Tensor([2000, 1, 467, 544],"float32"), Tensor([2000, 1, 12544, 2],"float32"), align_corners=False, )
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(Tensor([2000, 1, 467, 544],"float32"), Tensor([2000, 1, 12544, 2],"float32"), align_corners=False, ) 	 558272000 	 1000 	 0.7874033451080322 	 0.8145487308502197 	 0.7757511138916016 	 0.7966318130493164 	 4.290253639221191 	 4.116842269897461 	 2.191377639770508 	 2.104339122772217 	 
2025-07-30 10:53:04.866705 test begin: paddle.nn.functional.grid_sample(Tensor([2000, 1, 467, 544],"float32"), Tensor([2000, 1, 467, 2],"float32"), align_corners=False, )
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(Tensor([2000, 1, 467, 544],"float32"), Tensor([2000, 1, 467, 2],"float32"), align_corners=False, ) 	 509964000 	 1000 	 0.13485050201416016 	 0.1288454532623291 	 0.12342619895935059 	 0.11110854148864746 	 1.8949198722839355 	 1.6806538105010986 	 0.968883752822876 	 0.8586611747741699 	 
2025-07-30 10:53:17.052154 test begin: paddle.nn.functional.grid_sample(Tensor([2000, 1, 544, 467],"float32"), Tensor([2000, 1, 12544, 2],"float32"), align_corners=False, )
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(Tensor([2000, 1, 544, 467],"float32"), Tensor([2000, 1, 12544, 2],"float32"), align_corners=False, ) 	 558272000 	 1000 	 0.7886333465576172 	 0.8164777755737305 	 0.776458740234375 	 0.7955112457275391 	 4.33992862701416 	 4.209951400756836 	 2.2169854640960693 	 2.174497127532959 	 
2025-07-30 10:53:37.519274 test begin: paddle.nn.functional.grid_sample(Tensor([2000, 1, 544, 544],"float32"), Tensor([2000, 1, 12544, 2],"float32"), align_corners=False, )
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(Tensor([2000, 1, 544, 544],"float32"), Tensor([2000, 1, 12544, 2],"float32"), align_corners=False, ) 	 642048000 	 1000 	 0.8342876434326172 	 0.8580551147460938 	 0.8218209743499756 	 0.840214729309082 	 4.680996656417847 	 4.463902950286865 	 2.3913497924804688 	 1.5207808017730713 	 
2025-07-30 10:54:01.927288 test begin: paddle.nn.functional.grid_sample(Tensor([870, 1, 768, 768],"float32"), Tensor([870, 1, 12544, 2],"float32"), align_corners=False, )
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(Tensor([870, 1, 768, 768],"float32"), Tensor([870, 1, 12544, 2],"float32"), align_corners=False, ) 	 534973440 	 1000 	 0.5195839405059814 	 0.5103917121887207 	 0.5080976486206055 	 0.49239325523376465 	 3.1586966514587402 	 2.91886830329895 	 1.614583969116211 	 1.4919941425323486 	 
2025-07-30 10:54:22.523130 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 2540162448 	 1000 	 0.06148505210876465 	 0.0527036190032959 	 1.71661376953125e-05 	 4.220008850097656e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:22.745753 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([211680101, 3, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([211680101, 3, 8],"int32"), kernel_size=2, stride=2, ) 	 5080322448 	 1000 	 0.05615401268005371 	 0.05194544792175293 	 2.1696090698242188e-05 	 4.1484832763671875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:22.982746 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([211680101, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([211680101, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 5080322448 	 1000 	 0.05632424354553223 	 0.06831502914428711 	 1.621246337890625e-05 	 5.14984130859375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:23.209562 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 16934401],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 16934401],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 10262247006 	 1000 	 0.06690001487731934 	 0.06987309455871582 	 1.7881393432617188e-05 	 7.462501525878906e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:23.442429 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 7696685406 	 1000 	 0.061197757720947266 	 0.05269742012023926 	 3.743171691894531e-05 	 5.650520324707031e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:23.651281 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5131125927 	 1000 	 0.06443047523498535 	 0.05271148681640625 	 3.647804260253906e-05 	 5.459785461425781e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:23.864059 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8467201],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8467201],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5131123806 	 1000 	 0.05122661590576172 	 0.04057145118713379 	 3.24249267578125e-05 	 5.221366882324219e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:24.041503 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 2565564327 	 1000 	 0.042115211486816406 	 0.040055036544799805 	 2.2649765014648438e-05 	 6.67572021484375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:24.203374 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 2565564832 	 1000 	 0.040370941162109375 	 0.0394434928894043 	 4.220008850097656e-05 	 5.841255187988281e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:24.362142 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 25404048 	 1000 	 0.040372371673583984 	 0.039936065673828125 	 2.7418136596679688e-05 	 5.2928924560546875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:24.520815 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 3, 16934401],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 3, 16934401],"int32"), kernel_size=2, stride=2, ) 	 5131125927 	 1000 	 0.03433990478515625 	 0.03827714920043945 	 1.5735626220703125e-05 	 6.0558319091796875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:24.672214 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 3, 16934401],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 3, 16934401],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 5131125927 	 1000 	 0.03523683547973633 	 0.03815889358520508 	 2.5987625122070312e-05 	 5.316734313964844e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:24.824005 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 6350401, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 6350401, 8],"int32"), kernel_size=2, stride=2, ) 	 5131126432 	 1000 	 0.0345156192779541 	 0.03790473937988281 	 2.4557113647460938e-05 	 5.53131103515625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:24.975557 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 6350401, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 6350401, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 5131126432 	 1000 	 0.03540682792663574 	 0.037741661071777344 	 1.5735626220703125e-05 	 6.079673767089844e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:25.129843 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, ) 	 50805648 	 1000 	 0.03437376022338867 	 0.0382843017578125 	 2.6464462280273438e-05 	 5.054473876953125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:25.282552 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 50805648 	 1000 	 0.035643577575683594 	 0.03843331336975098 	 3.266334533691406e-05 	 5.602836608886719e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:25.433887 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3175201, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3175201, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5131124816 	 1000 	 0.040256500244140625 	 0.039717674255371094 	 1.6689300537109375e-05 	 3.7670135498046875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:25.591310 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5131126432 	 1000 	 0.04035353660583496 	 0.03972887992858887 	 1.2874603271484375e-05 	 3.218650817871094e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:25.748638 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 7696686416 	 1000 	 0.04023265838623047 	 0.03982281684875488 	 1.2159347534179688e-05 	 6.127357482910156e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:25.906795 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 6350401, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 6350401, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 10262248016 	 1000 	 0.04021930694580078 	 0.039350032806396484 	 2.3603439331054688e-05 	 5.435943603515625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:26.066756 test begin: paddle.nn.functional.max_unpool1d(Tensor([105840101, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([105840101, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5080324848 	 1000 	 0.04067873954772949 	 0.04209542274475098 	 2.0503997802734375e-05 	 6.270408630371094e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:26.228529 test begin: paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 50805648 	 1000 	 0.041346073150634766 	 0.043520450592041016 	 2.8371810913085938e-05 	 7.82012939453125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:26.391599 test begin: paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 2590965648 	 1000 	 0.04024934768676758 	 0.039530277252197266 	 2.2172927856445312e-05 	 4.673004150390625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:26.549347 test begin: paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([1, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([1, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5080322448 	 1000 	 0.041468143463134766 	 0.039562225341796875 	 1.811981201171875e-05 	 5.030632019042969e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:26.821956 test begin: paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5105724048 	 1000 	 0.04038524627685547 	 0.04445505142211914 	 3.075599670410156e-05 	 6.413459777832031e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:26.985426 test begin: paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([211680101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([211680101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 10160644848 	 1000 	 0.04029369354248047 	 0.03946328163146973 	 4.6253204345703125e-05 	 5.7220458984375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:27.143749 test begin: paddle.nn.functional.max_unpool2d(Tensor([1894, 8, 86, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([1894, 8, 86, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 222571440 	 1000 	 0.06065082550048828 	 0.08592367172241211 	 0.030972957611083984 	 0.043762922286987305 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:27.583156 test begin: paddle.nn.functional.max_unpool2d(Tensor([189401, 8, 86, 39],"float32"), Tensor([189401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([189401, 8, 86, 39],"float32"), Tensor([189401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 10164015264 	 1000 	 0.060649871826171875 	 0.08591842651367188 	 0.03096151351928711 	 0.043758392333984375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:28.024145 test begin: paddle.nn.functional.max_unpool2d(Tensor([189401, 8, 86, 39],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([189401, 8, 86, 39],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5083724880 	 1000 	 0.0606236457824707 	 0.0859227180480957 	 0.030956029891967773 	 0.04374504089355469 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:28.440033 test begin: paddle.nn.functional.max_unpool2d(Tensor([3887, 16, 43, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([3887, 16, 43, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 134484736 	 1000 	 0.022980928421020508 	 0.029274940490722656 	 0.0011744499206542969 	 5.078315734863281e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:28.633916 test begin: paddle.nn.functional.max_unpool2d(Tensor([388701, 16, 43, 19],"float32"), Tensor([388701, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([388701, 16, 43, 19],"float32"), Tensor([388701, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 10162198944 	 1000 	 0.023325443267822266 	 0.029276371002197266 	 0.0009121894836425781 	 4.9591064453125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:54:28.825053 test begin: paddle.nn.functional.max_unpool2d(Tensor([388701, 16, 43, 19],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([388701, 16, 43, 19],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5081936080 	 1000 	 0.023288488388061523 	 0.02914571762084961 	 0.0004608631134033203 	 5.698204040527344e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 9, in <module>
    from tester import (APIConfig, APITestAccuracy, APITestCINNVSDygraph,
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/__init__.py", line 74, in __getattr__
    from .api_config import APIConfig
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/__init__.py", line 22, in __getattr__
    from .config_analyzer import APIConfig
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py", line 8, in <module>
    import paddle
  File "/usr/local/lib/python3.10/dist-packages/paddle/__init__.py", line 90, in <module>
    import paddle.distributed.fleet
  File "/usr/local/lib/python3.10/dist-packages/paddle/distributed/__init__.py", line 21, in <module>
    from paddle.distributed.fleet.base.topology import ParallelMode
  File "/usr/local/lib/python3.10/dist-packages/paddle/distributed/fleet/__init__.py", line 17, in <module>
    from .base.distributed_strategy import DistributedStrategy
  File "/usr/local/lib/python3.10/dist-packages/paddle/distributed/fleet/base/distributed_strategy.py", line 28, in <module>
    from paddle.distributed.fleet.utils.log_util import logger
  File "/usr/local/lib/python3.10/dist-packages/paddle/distributed/fleet/utils/__init__.py", line 20, in <module>
    from . import (  # noqa: F401
  File "/usr/local/lib/python3.10/dist-packages/paddle/distributed/fleet/utils/sequence_parallel_utils.py", line 24, in <module>
    from paddle.distributed.fleet.meta_parallel import get_rng_state_tracker
  File "/usr/local/lib/python3.10/dist-packages/paddle/distributed/fleet/meta_parallel/__init__.py", line 29, in <module>
    from .pipeline_parallel import (  # noqa: F401
  File "/usr/local/lib/python3.10/dist-packages/paddle/distributed/fleet/meta_parallel/pipeline_parallel.py", line 29, in <module>
    from ..meta_optimizers.dygraph_optimizer import HybridParallelOptimizer
  File "/usr/local/lib/python3.10/dist-packages/paddle/distributed/fleet/meta_optimizers/__init__.py", line 35, in <module>
    from .ps_optimizer import ParameterServerOptimizer  # noqa: F401
  File "/usr/local/lib/python3.10/dist-packages/paddle/distributed/fleet/meta_optimizers/ps_optimizer.py", line 20, in <module>
    import paddle.distributed.passes
  File "/usr/local/lib/python3.10/dist-packages/paddle/distributed/passes/__init__.py", line 118, in <module>
    from .ps_server_pass import (  # noqa: F401
  File "/usr/local/lib/python3.10/dist-packages/paddle/distributed/passes/ps_server_pass.py", line 29, in <module>
    from ..ps.utils.public import (
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1012, in get_code
  File "<frozen importlib._bootstrap_external>", line 672, in _compile_bytecode
KeyboardInterrupt
2025-07-30 10:36:19.735908 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([388701, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
W0730 10:36:20.163267 131731 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([388701, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5081936080 	 1000 	 0.022037506103515625 	 0.0334019660949707 	 4.482269287109375e-05 	 6.389617919921875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:20.901107 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([840101, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([840101, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5081317920 	 1000 	 0.022601842880249023 	 0.04047107696533203 	 0.00015282630920410156 	 6.67572021484375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:21.084790 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([189401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([189401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5083724880 	 1000 	 0.0592195987701416 	 0.0857093334197998 	 0.02892589569091797 	 0.04359912872314453 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:21.534639 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 2612, 19],"float32"), Tensor([6401, 16, 2612, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 2612, 19],"float32"), Tensor([6401, 16, 2612, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 10165402496 	 1000 	 0.021581411361694336 	 0.030917644500732422 	 0.0001773834228515625 	 5.793571472167969e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:21.738495 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 2612, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 2612, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5166375120 	 1000 	 0.021992921829223633 	 0.03254389762878418 	 0.0003256797790527344 	 6.103515625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:21.943161 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 1154],"float32"), Tensor([6401, 16, 43, 1154],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 1154],"float32"), Tensor([6401, 16, 43, 1154],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 10164173504 	 1000 	 0.022336721420288086 	 0.030956506729125977 	 8.606910705566406e-05 	 5.507469177246094e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:22.159075 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 1154],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 1154],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5165760624 	 1000 	 0.021987438201904297 	 0.03082418441772461 	 0.0005676746368408203 	 5.7220458984375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:22.358162 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([3887, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([3887, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 134484736 	 1000 	 0.02199554443359375 	 0.030957937240600586 	 0.00039577484130859375 	 5.888938903808594e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:22.556949 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 16, 2612, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 16, 2612, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5166375120 	 1000 	 0.022037029266357422 	 0.030816316604614258 	 0.0003056526184082031 	 6.222724914550781e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:22.757584 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 16, 43, 1154],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 16, 43, 1154],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5165760624 	 1000 	 0.021991968154907227 	 0.03058314323425293 	 0.0004940032958984375 	 5.435943603515625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:22.957222 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 972, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 972, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5166861596 	 1000 	 0.023714303970336914 	 0.03216290473937988 	 2.47955322265625e-05 	 5.555152893066406e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:23.253642 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 237, 86, 39],"float32"), Tensor([6401, 237, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 237, 86, 39],"float32"), Tensor([6401, 237, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 10176284196 	 1000 	 0.05901622772216797 	 0.08538246154785156 	 0.030038118362426758 	 0.04345703125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:23.669628 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 237, 86, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 237, 86, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5259893730 	 1000 	 0.058847904205322266 	 0.10341715812683105 	 0.029979944229125977 	 0.043442487716674805 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:25.885290 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 1182],"float32"), Tensor([6401, 32, 21, 1182],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 1182],"float32"), Tensor([6401, 32, 21, 1182],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 10168679808 	 1000 	 0.024309635162353516 	 0.032952308654785156 	 1.7642974853515625e-05 	 5.14984130859375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:26.066862 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 1182],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 1182],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5123053152 	 1000 	 0.02377772331237793 	 0.03774905204772949 	 1.8835067749023438e-05 	 5.412101745605469e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:26.285983 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 32, 21, 1182],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 32, 21, 1182],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5123053152 	 1000 	 0.023777008056640625 	 0.03170442581176758 	 2.5033950805664062e-05 	 4.2438507080078125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:28.255185 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 32, 2757, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 32, 2757, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5121209664 	 1000 	 0.022974729537963867 	 0.03269791603088379 	 2.0742416381835938e-05 	 5.7220458984375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:28.429317 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 4201, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 4201, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5121036837 	 1000 	 0.021691083908081055 	 0.03122687339782715 	 1.6450881958007812e-05 	 4.887580871582031e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:28.591906 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([8401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([8401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 89522496 	 1000 	 0.022554636001586914 	 0.03138399124145508 	 2.0742416381835938e-05 	 5.054473876953125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:28.752334 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 2757, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 2757, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5121209664 	 1000 	 0.021646499633789062 	 0.03359723091125488 	 1.6927719116210938e-05 	 5.602836608886719e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:28.918824 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 2757, 9],"float32"), Tensor([6401, 32, 2757, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 2757, 9],"float32"), Tensor([6401, 32, 2757, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 10164992832 	 1000 	 0.021756649017333984 	 0.03142976760864258 	 1.7881393432617188e-05 	 5.435943603515625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:29.076999 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 4201, 21, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 4201, 21, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5121036837 	 1000 	 0.021628618240356445 	 0.031259775161743164 	 3.409385681152344e-05 	 4.839897155761719e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:29.235685 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 4201, 21, 9],"float32"), Tensor([6401, 4201, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 4201, 21, 9],"float32"), Tensor([6401, 4201, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 10164647178 	 1000 	 0.021814584732055664 	 0.031157732009887695 	 1.8596649169921875e-05 	 6.580352783203125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:29.393606 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 2545, 39],"float32"), Tensor([6401, 8, 2545, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 2545, 39],"float32"), Tensor([6401, 8, 2545, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 10165300080 	 1000 	 0.058983802795410156 	 0.08593010902404785 	 0.03003215789794922 	 0.04372072219848633 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:29.822809 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 2545, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 2545, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5254401672 	 1000 	 0.05915403366088867 	 0.08597469329833984 	 0.030121326446533203 	 0.04375743865966797 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:30.288908 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 1154],"float32"), Tensor([6401, 8, 86, 1154],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 1154],"float32"), Tensor([6401, 8, 86, 1154],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 10164173504 	 1000 	 0.06106066703796387 	 0.08573269844055176 	 0.03018498420715332 	 0.0436396598815918 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:30.719067 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 1154],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 1154],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5253838384 	 1000 	 0.059181928634643555 	 0.08892154693603516 	 0.03014373779296875 	 0.043648719787597656 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:31.161471 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([1894, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([1894, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 222571440 	 1000 	 0.05922341346740723 	 0.08564209938049316 	 0.030178308486938477 	 0.04360318183898926 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:31.603450 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 237, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 237, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5259893730 	 1000 	 0.05914592742919922 	 0.08568811416625977 	 0.030115842819213867 	 0.04359889030456543 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:32.026153 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 8, 2545, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 8, 2545, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5254401672 	 1000 	 0.05930685997009277 	 0.08570098876953125 	 0.03018951416015625 	 0.0436251163482666 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:32.460126 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 8, 86, 1154],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 8, 86, 1154],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5253838384 	 1000 	 0.059137821197509766 	 0.08568215370178223 	 0.03011345863342285 	 0.04360842704772949 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:32.902624 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 972, 43, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 972, 43, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5166861596 	 1000 	 0.025859355926513672 	 0.031763315200805664 	 2.288818359375e-05 	 8.749961853027344e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:33.142388 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 972, 43, 19],"float32"), Tensor([6401, 972, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 972, 43, 19],"float32"), Tensor([6401, 972, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 10166375448 	 1000 	 0.022605419158935547 	 0.03170490264892578 	 2.3365020751953125e-05 	 4.601478576660156e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:33.348419 test begin: paddle.nn.functional.max_unpool2d(Tensor([8401, 32, 21, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([8401, 32, 21, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 89522496 	 1000 	 0.0225374698638916 	 0.032346487045288086 	 1.621246337890625e-05 	 4.57763671875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:33.514195 test begin: paddle.nn.functional.max_unpool2d(Tensor([840101, 32, 21, 9],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([840101, 32, 21, 9],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5081317920 	 1000 	 0.0224301815032959 	 0.03262209892272949 	 1.5974044799804688e-05 	 5.698204040527344e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:33.684791 test begin: paddle.nn.functional.max_unpool2d(Tensor([840101, 32, 21, 9],"float32"), Tensor([840101, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([840101, 32, 21, 9],"float32"), Tensor([840101, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 10161861696 	 1000 	 0.030678987503051758 	 0.042595863342285156 	 3.838539123535156e-05 	 6.29425048828125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:33.893206 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([14112101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([14112101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5080356720 	 1000 	 0.0390467643737793 	 0.04389047622680664 	 2.8848648071289062e-05 	 5.459785461425781e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:34.062234 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([14112101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([14112101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5080356720 	 1000 	 0.05157017707824707 	 0.03559160232543945 	 2.5987625122070312e-05 	 6.651878356933594e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:34.244630 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([7056101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([7056101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2540196720 	 1000 	 0.04576516151428223 	 0.04522275924682617 	 3.600120544433594e-05 	 8.630752563476562e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:34.416195 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131141380 	 1000 	 0.02944040298461914 	 0.044152021408081055 	 1.4543533325195312e-05 	 6.127357482910156e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:34.562969 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131141380 	 1000 	 0.029193639755249023 	 0.04555368423461914 	 1.5735626220703125e-05 	 9.512901306152344e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:34.717520 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131141380 	 1000 	 0.030086517333984375 	 0.04519391059875488 	 2.002716064453125e-05 	 6.389617919921875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:34.879729 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565607050 	 1000 	 0.030558347702026367 	 0.044736623764038086 	 2.193450927734375e-05 	 7.033348083496094e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:35.037681 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 2565607050 	 1000 	 0.03263068199157715 	 0.034824371337890625 	 2.8133392333984375e-05 	 6.008148193359375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:36.867747 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565607050 	 1000 	 0.030384540557861328 	 0.03530097007751465 	 2.288818359375e-05 	 7.987022399902344e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:38.748370 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 7696702980 	 1000 	 0.031553030014038086 	 0.03474617004394531 	 2.9087066650390625e-05 	 5.0067901611328125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:38.918447 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 7696702980 	 1000 	 0.03258061408996582 	 0.036095619201660156 	 1.8358230590820312e-05 	 5.221366882324219e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:39.082196 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131137744 	 1000 	 0.0364530086517334 	 0.04188036918640137 	 3.075599670410156e-05 	 7.128715515136719e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:39.265803 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131137744 	 1000 	 0.031071901321411133 	 0.0348665714263916 	 2.9087066650390625e-05 	 6.222724914550781e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:39.418669 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131137744 	 1000 	 0.03349018096923828 	 0.04451394081115723 	 0.00018262863159179688 	 7.605552673339844e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:39.593894 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565605232 	 1000 	 0.02909684181213379 	 0.03514695167541504 	 1.6450881958007812e-05 	 5.245208740234375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:39.736595 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 2565605232 	 1000 	 0.029534101486206055 	 0.035181283950805664 	 1.4781951904296875e-05 	 6.580352783203125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:39.877526 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565605232 	 1000 	 0.029819488525390625 	 0.03450965881347656 	 1.811981201171875e-05 	 4.315376281738281e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:40.016057 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 7696699344 	 1000 	 0.0509181022644043 	 0.04283571243286133 	 4.0531158447265625e-05 	 9.131431579589844e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:40.223614 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 7696699344 	 1000 	 0.028822660446166992 	 0.03490328788757324 	 2.002716064453125e-05 	 5.078315734863281e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:40.382645 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131135320 	 1000 	 0.045883893966674805 	 0.03459978103637695 	 3.4332275390625e-05 	 5.602836608886719e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:40.536387 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131135320 	 1000 	 0.029690980911254883 	 0.0364532470703125 	 3.981590270996094e-05 	 6.151199340820312e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:40.678096 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131135320 	 1000 	 0.03156709671020508 	 0.03517937660217285 	 1.8835067749023438e-05 	 4.3392181396484375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:40.818840 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565604020 	 1000 	 0.029799699783325195 	 0.05185866355895996 	 1.6689300537109375e-05 	 0.0001804828643798828 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:40.978589 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 2565604020 	 1000 	 0.03587532043457031 	 0.03634834289550781 	 1.9311904907226562e-05 	 7.62939453125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:41.128193 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565604020 	 1000 	 0.03016805648803711 	 0.044440269470214844 	 0.000171661376953125 	 8.678436279296875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:41.285518 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 7696696920 	 1000 	 0.02975296974182129 	 0.0449061393737793 	 0.00012683868408203125 	 6.198883056640625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:41.448395 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 7696696920 	 1000 	 0.03008270263671875 	 0.044644832611083984 	 1.4543533325195312e-05 	 5.8650970458984375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:41.598699 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 211681, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 211681, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565610080 	 1000 	 0.029643774032592773 	 0.044246673583984375 	 1.430511474609375e-05 	 7.414817810058594e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:41.747245 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565607050 	 1000 	 0.029421091079711914 	 0.04099559783935547 	 1.52587890625e-05 	 6.198883056640625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:41.891870 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565605232 	 1000 	 0.030013561248779297 	 0.03490591049194336 	 2.2649765014648438e-05 	 4.673004150390625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:42.034386 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 423361],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 423361],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565604020 	 1000 	 0.029340267181396484 	 0.03470277786254883 	 1.811981201171875e-05 	 4.3392181396484375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:42.169989 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131165620 	 1000 	 0.03838920593261719 	 0.044591665267944336 	 4.291534423828125e-05 	 7.319450378417969e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:42.335042 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131165620 	 1000 	 0.02964043617248535 	 0.03533744812011719 	 1.3828277587890625e-05 	 3.170967102050781e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:42.499342 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131166832 	 1000 	 0.030596017837524414 	 0.04424095153808594 	 0.00012445449829101562 	 6.437301635742188e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:42.663180 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131166832 	 1000 	 0.02939009666442871 	 0.04433178901672363 	 1.430511474609375e-05 	 5.650520324707031e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:42.820799 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131168650 	 1000 	 0.02947378158569336 	 0.04423022270202637 	 1.71661376953125e-05 	 6.771087646484375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:42.968883 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131168650 	 1000 	 0.02973794937133789 	 0.04423928260803223 	 1.33514404296875e-05 	 5.245208740234375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:43.125787 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131171680 	 1000 	 0.031267642974853516 	 0.03740835189819336 	 2.4557113647460938e-05 	 7.724761962890625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:43.266207 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131171680 	 1000 	 0.030083894729614258 	 0.0370488166809082 	 1.621246337890625e-05 	 3.600120544433594e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:43.406739 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50839920 	 1000 	 0.02980661392211914 	 0.03524041175842285 	 1.4781951904296875e-05 	 2.8371810913085938e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:43.560165 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 50839920 	 1000 	 0.03423810005187988 	 0.04405546188354492 	 1.430511474609375e-05 	 5.364418029785156e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:43.728741 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25438320 	 1000 	 0.02930140495300293 	 0.04428887367248535 	 1.3589859008789062e-05 	 7.128715515136719e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:43.887294 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 846721],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 846721],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 10262258520 	 1000 	 0.03207206726074219 	 0.03627133369445801 	 1.6689300537109375e-05 	 3.7670135498046875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:44.035648 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 846721],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 846721],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 10262258520 	 1000 	 0.044542789459228516 	 0.03587150573730469 	 1.8596649169921875e-05 	 4.291534423828125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:44.218458 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 705601, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 705601, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 10262260944 	 1000 	 0.029613256454467773 	 0.0351412296295166 	 1.430511474609375e-05 	 4.1961669921875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:44.355063 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 705601, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 705601, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 10262260944 	 1000 	 0.03608989715576172 	 0.03558230400085449 	 2.1457672119140625e-05 	 4.57763671875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:44.513202 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 564481, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 564481, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 10262264580 	 1000 	 0.02966141700744629 	 0.035736799240112305 	 1.430511474609375e-05 	 3.5762786865234375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:44.649377 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 564481, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 564481, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 10262264580 	 1000 	 0.032328128814697266 	 0.035460710525512695 	 1.621246337890625e-05 	 4.1961669921875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:36:44.797259 test begin: paddle.numel(Tensor([508032010],"float32"), )
[Prof] paddle.numel 	 paddle.numel(Tensor([508032010],"float32"), ) 	 508032010 	 1000 	 0.009017705917358398 	 0.030286073684692383 	 2.6941299438476562e-05 	 4.172325134277344e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:36:52.681242 test begin: paddle.positive(Tensor([100, 5080321],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([100, 5080321],"float32"), ) 	 508032100 	 1000 	 0.0019164085388183594 	 0.000202178955078125 	 6.9141387939453125e-06 	 1.4543533325195312e-05 	 0.032509565353393555 	 0.06165671348571777 	 4.124641418457031e-05 	 6.890296936035156e-05 	 combined
2025-07-30 10:37:11.329110 test begin: paddle.positive(Tensor([16934410, 3, 4, 5],"float16"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([16934410, 3, 4, 5],"float16"), ) 	 1016064600 	 1000 	 0.0018956661224365234 	 0.00020551681518554688 	 6.9141387939453125e-06 	 1.430511474609375e-05 	 0.0291748046875 	 0.06005716323852539 	 2.7894973754882812e-05 	 5.5789947509765625e-05 	 combined
2025-07-30 10:37:50.781006 test begin: paddle.positive(Tensor([20, 1270081, 4, 5],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 1270081, 4, 5],"float32"), ) 	 508032400 	 1000 	 0.0018916130065917969 	 0.00020074844360351562 	 6.4373016357421875e-06 	 1.4066696166992188e-05 	 0.029186010360717773 	 0.05051851272583008 	 2.8371810913085938e-05 	 7.367134094238281e-05 	 combined
2025-07-30 10:38:09.329290 test begin: paddle.positive(Tensor([20, 2540161, 4, 5],"float16"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 2540161, 4, 5],"float16"), ) 	 1016064400 	 1000 	 0.0019083023071289062 	 0.00020241737365722656 	 6.9141387939453125e-06 	 1.4066696166992188e-05 	 0.028888940811157227 	 0.04937386512756348 	 2.4557113647460938e-05 	 6.890296936035156e-05 	 combined
2025-07-30 10:38:47.646112 test begin: paddle.positive(Tensor([20, 3, 1693441, 5],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 3, 1693441, 5],"float32"), ) 	 508032300 	 1000 	 0.0018417835235595703 	 0.00020122528076171875 	 7.3909759521484375e-06 	 1.430511474609375e-05 	 0.029007434844970703 	 0.045300960540771484 	 2.2649765014648438e-05 	 5.245208740234375e-05 	 combined
2025-07-30 10:39:04.489509 test begin: paddle.positive(Tensor([20, 3, 3386881, 5],"float16"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 3, 3386881, 5],"float16"), ) 	 1016064300 	 1000 	 0.0019159317016601562 	 0.0002048015594482422 	 7.152557373046875e-06 	 1.5020370483398438e-05 	 0.0290219783782959 	 0.06445193290710449 	 2.86102294921875e-05 	 9.250640869140625e-05 	 combined
2025-07-30 10:39:56.877915 test begin: paddle.positive(Tensor([20, 3, 4, 2116801],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 3, 4, 2116801],"float32"), ) 	 508032240 	 1000 	 0.001958608627319336 	 0.00020003318786621094 	 8.58306884765625e-06 	 1.3828277587890625e-05 	 0.028771638870239258 	 0.04541587829589844 	 2.6941299438476562e-05 	 6.723403930664062e-05 	 combined
2025-07-30 10:40:13.751886 test begin: paddle.positive(Tensor([20, 3, 4, 4233601],"float16"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 3, 4, 4233601],"float16"), ) 	 1016064240 	 1000 	 0.002978086471557617 	 0.00020360946655273438 	 8.344650268554688e-06 	 1.5735626220703125e-05 	 0.03356003761291504 	 0.05130505561828613 	 1.7404556274414062e-05 	 4.744529724121094e-05 	 combined
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 9, in <module>
    from tester import (APIConfig, APITestAccuracy, APITestCINNVSDygraph,
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/__init__.py", line 74, in __getattr__
    from .api_config import APIConfig
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/__init__.py", line 22, in __getattr__
    from .config_analyzer import APIConfig
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py", line 8, in <module>
    import paddle
  File "/usr/local/lib/python3.10/dist-packages/paddle/__init__.py", line 38, in <module>
    from .base import core  # noqa: F401
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/__init__.py", line 38, in <module>
    from . import (  # noqa: F401
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/backward.py", line 28, in <module>
    from . import core, framework, log_helper, unique_name
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/core.py", line 269, in <module>
    if avx_supported() and not libpaddle.is_compiled_with_avx():
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/core.py", line 76, in avx_supported
    has_avx = pipe.read() != ''
KeyboardInterrupt
2025-07-30 10:36:22.973740 test begin: paddle.positive(Tensor([496130, 1024],"float32"), )
W0730 10:36:31.610728 131890 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.positive 	 paddle.positive(Tensor([496130, 1024],"float32"), ) 	 508037120 	 1000 	 0.0026633739471435547 	 0.00022292137145996094 	 2.0503997802734375e-05 	 2.47955322265625e-05 	 0.031588077545166016 	 0.05434679985046387 	 3.6716461181640625e-05 	 8.797645568847656e-05 	 combined
2025-07-30 10:36:41.154550 test begin: paddle.positive(Tensor([8467210, 3, 4, 5],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([8467210, 3, 4, 5],"float32"), ) 	 508032600 	 1000 	 0.0018458366394042969 	 0.00023174285888671875 	 7.3909759521484375e-06 	 1.6689300537109375e-05 	 0.030674457550048828 	 0.05984950065612793 	 4.4345855712890625e-05 	 7.987022399902344e-05 	 combined
2025-07-30 10:37:03.317694 test begin: paddle.rank(input=Tensor([1270080101, 2],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([1270080101, 2],"float64"), ) 	 2540160202 	 1000 	 0.044309139251708984 	 0.030796051025390625 	 2.4557113647460938e-05 	 5.817413330078125e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:37:53.486683 test begin: paddle.rank(input=Tensor([201, 12700801],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([201, 12700801],"float64"), ) 	 2552861001 	 1000 	 0.040007829666137695 	 0.02921319007873535 	 2.86102294921875e-05 	 6.246566772460938e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:39:03.674341 test begin: paddle.rank(input=Tensor([301, 2, 2, 2116801],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([301, 2, 2, 2116801],"float64"), ) 	 2548628404 	 1000 	 0.04552507400512695 	 0.030803203582763672 	 3.0279159545898438e-05 	 5.817413330078125e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:40:13.339701 test begin: paddle.rank(input=Tensor([301, 2, 2116801, 2],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([301, 2, 2116801, 2],"float64"), ) 	 2548628404 	 1000 	 0.040610313415527344 	 0.02930760383605957 	 2.574920654296875e-05 	 5.9604644775390625e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:41:13.748433 test begin: paddle.rank(input=Tensor([301, 2116801, 2, 2],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([301, 2116801, 2, 2],"float64"), ) 	 2548628404 	 1000 	 0.04072904586791992 	 0.029378175735473633 	 3.4332275390625e-05 	 6.437301635742188e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:42:36.789731 test begin: paddle.rank(input=Tensor([317520101, 2, 2, 2],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([317520101, 2, 2, 2],"float64"), ) 	 2540160808 	 1000 	 0.04353904724121094 	 0.029231786727905273 	 3.24249267578125e-05 	 5.7220458984375e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:43:32.966051 test begin: paddle.reshape(Tensor([141760, 7168],"bfloat16"), list[-1,7168,], )
[Prof] paddle.reshape 	 paddle.reshape(Tensor([141760, 7168],"bfloat16"), list[-1,7168,], ) 	 1016135680 	 1000 	 0.005477190017700195 	 0.004031658172607422 	 9.298324584960938e-06 	 2.0265579223632812e-05 	 0.04626107215881348 	 4.498212575912476 	 4.0531158447265625e-05 	 2.299166202545166 	 
2025-07-30 10:44:10.689701 test begin: paddle.searchsorted(Tensor([2540160101],"float64"), Tensor([512],"float64"), )
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([2540160101],"float64"), Tensor([512],"float64"), ) 	 2540160613 	 1000 	 0.009802103042602539 	 0.011140823364257812 	 0.0018467903137207031 	 3.3855438232421875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:44:59.685501 test begin: paddle.searchsorted(Tensor([25401601],"float64"), Tensor([51201],"float64"), )
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([25401601],"float64"), Tensor([51201],"float64"), ) 	 25452802 	 1000 	 0.010216951370239258 	 0.01088404655456543 	 0.0022242069244384766 	 3.814697265625e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:45:00.308122 test begin: paddle.searchsorted(Tensor([50803201],"float32"), Tensor([51201],"float32"), )
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([50803201],"float32"), Tensor([51201],"float32"), ) 	 50854402 	 1000 	 0.015881776809692383 	 0.01690220832824707 	 3.600120544433594e-05 	 3.3855438232421875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:45:01.168286 test begin: paddle.searchsorted(Tensor([50803201],"int32"), Tensor([51201],"int32"), )
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([50803201],"int32"), Tensor([51201],"int32"), ) 	 50854402 	 1000 	 0.014632701873779297 	 0.016951560974121094 	 1.4781951904296875e-05 	 3.0040740966796875e-05 	 None 	 None 	 None 	 None 	 
2025-07-30 10:45:01.740112 test begin: paddle.select_scatter(Tensor([20, 3, 282241, 5, 6],"int32"), Tensor([20, 3, 5, 6],"int32"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([20, 3, 282241, 5, 6],"int32"), Tensor([20, 3, 5, 6],"int32"), 2, 1, ) 	 508035600 	 1000 	 0.020789384841918945 	 3.0695338249206543 	 2.765655517578125e-05 	 1.0425455570220947 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:45:22.014053 test begin: paddle.select_scatter(Tensor([20, 3, 4, 1058401],"float64"), Tensor([20, 3, 1058401],"float64"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([20, 3, 4, 1058401],"float64"), Tensor([20, 3, 1058401],"float64"), 2, 1, ) 	 317520300 	 1000 	 0.7580897808074951 	 3.8102622032165527 	 0.7383942604064941 	 1.2955875396728516 	 6.8907506465911865 	 4.140932321548462 	 0.8791804313659668 	 1.0562503337860107 	 
2025-07-30 10:45:49.100189 test begin: paddle.select_scatter(Tensor([20, 3, 846721, 5],"float64"), Tensor([20, 3, 5],"float64"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([20, 3, 846721, 5],"float64"), Tensor([20, 3, 5],"float64"), 2, 1, ) 	 254016600 	 1000 	 0.019617319107055664 	 3.0673177242279053 	 1.5020370483398438e-05 	 1.0422406196594238 	 3.109776020050049 	 3.070939064025879 	 0.39475417137145996 	 0.7841091156005859 	 
2025-07-30 10:46:08.193185 test begin: paddle.select_scatter(Tensor([20, 635040, 4],"float32"), Tensor([20, 4],"float32"), 1, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([20, 635040, 4],"float32"), Tensor([20, 4],"float32"), 1, 1, ) 	 50803280 	 1000 	 0.01935863494873047 	 0.32411885261535645 	 1.8358230590820312e-05 	 0.10741066932678223 	 0.33049726486206055 	 0.3180661201477051 	 0.04196047782897949 	 0.08106327056884766 	 
2025-07-30 10:46:13.073484 test begin: paddle.shape(Tensor([10, 1600, 376, 280],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([10, 1600, 376, 280],"float32"), ) 	 1684480000 	 1000 	 0.009557723999023438 	 0.03965020179748535 	 2.0265579223632812e-05 	 6.365776062011719e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:46:41.118247 test begin: paddle.shape(Tensor([130, 128, 256, 256],"float16"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([130, 128, 256, 256],"float16"), ) 	 1090519040 	 1000 	 0.004568815231323242 	 0.03161144256591797 	 1.2636184692382812e-05 	 6.532669067382812e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:47:01.392256 test begin: paddle.shape(Tensor([40, 121, 376, 280],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 121, 376, 280],"float32"), ) 	 509555200 	 1000 	 0.009535074234008789 	 0.03990769386291504 	 1.8835067749023438e-05 	 6.866455078125e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:47:10.976216 test begin: paddle.shape(Tensor([40, 128, 256, 388],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 128, 256, 388],"float32"), ) 	 508559360 	 1000 	 0.009669303894042969 	 0.040273427963256836 	 1.811981201171875e-05 	 9.584426879882812e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:47:19.596941 test begin: paddle.shape(Tensor([40, 128, 256, 776],"float16"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 128, 256, 776],"float16"), ) 	 1017118720 	 1000 	 0.0045664310455322266 	 0.03657937049865723 	 1.3589859008789062e-05 	 6.198883056640625e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:47:40.156587 test begin: paddle.shape(Tensor([40, 128, 388, 256],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 128, 388, 256],"float32"), ) 	 508559360 	 1000 	 0.00450444221496582 	 0.03253602981567383 	 1.3589859008789062e-05 	 7.05718994140625e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:47:51.324616 test begin: paddle.shape(Tensor([40, 128, 776, 256],"float16"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 128, 776, 256],"float16"), ) 	 1017118720 	 1000 	 0.004500865936279297 	 0.03235316276550293 	 1.1682510375976562e-05 	 7.605552673339844e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:48:10.041746 test begin: paddle.shape(Tensor([40, 1600, 29, 280],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 1600, 29, 280],"float32"), ) 	 519680000 	 1000 	 0.004537820816040039 	 0.03162431716918945 	 8.58306884765625e-06 	 5.984306335449219e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:48:19.423227 test begin: paddle.shape(Tensor([40, 1600, 376, 22],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 1600, 376, 22],"float32"), ) 	 529408000 	 1000 	 0.009572267532348633 	 0.039679765701293945 	 1.049041748046875e-05 	 7.486343383789062e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:48:28.591469 test begin: paddle.shape(Tensor([40, 194, 256, 256],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 194, 256, 256],"float32"), ) 	 508559360 	 1000 	 0.004542350769042969 	 0.03157997131347656 	 8.106231689453125e-06 	 7.390975952148438e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:48:38.408685 test begin: paddle.shape(Tensor([40, 388, 256, 256],"float16"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 388, 256, 256],"float16"), ) 	 1017118720 	 1000 	 0.0045468807220458984 	 0.03171372413635254 	 1.2159347534179688e-05 	 6.508827209472656e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:48:57.124882 test begin: paddle.shape(Tensor([70, 128, 256, 256],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([70, 128, 256, 256],"float32"), ) 	 587202560 	 1000 	 0.004535198211669922 	 0.03580808639526367 	 1.2159347534179688e-05 	 5.91278076171875e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:49:07.318487 test begin: paddle.slice(Tensor([653440, 1555],"bfloat16"), axes=list[0,], starts=list[0,], ends=list[8168,], )
W0730 10:49:28.404481 144864 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.slice 	 paddle.slice(Tensor([653440, 1555],"bfloat16"), axes=list[0,], starts=list[0,], ends=list[8168,], ) 	 1016099200 	 1000 	 0.00808405876159668 	 0.013478994369506836 	 3.4809112548828125e-05 	 3.266334533691406e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:49:30.953609 test begin: paddle.slice(Tensor([653440, 1555],"bfloat16"), axes=list[0,], starts=list[16336,], ends=list[24504,], )
W0730 10:49:46.618793 145290 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.slice 	 paddle.slice(Tensor([653440, 1555],"bfloat16"), axes=list[0,], starts=list[16336,], ends=list[24504,], ) 	 1016099200 	 1000 	 0.0076258182525634766 	 0.013393640518188477 	 1.9311904907226562e-05 	 4.029273986816406e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:49:49.271917 test begin: paddle.slice(Tensor([653440, 1555],"bfloat16"), axes=list[0,], starts=list[24504,], ends=list[32672,], )
W0730 10:50:04.597946 145580 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.slice 	 paddle.slice(Tensor([653440, 1555],"bfloat16"), axes=list[0,], starts=list[24504,], ends=list[32672,], ) 	 1016099200 	 1000 	 0.007750272750854492 	 0.01333165168762207 	 3.814697265625e-05 	 3.743171691894531e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:50:06.724067 test begin: paddle.slice(Tensor([793810, 1280],"bfloat16"), axes=list[0,], starts=list[0,], ends=list[8168,], )
W0730 10:50:24.132094 145922 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.slice 	 paddle.slice(Tensor([793810, 1280],"bfloat16"), axes=list[0,], starts=list[0,], ends=list[8168,], ) 	 1016076800 	 1000 	 0.007695198059082031 	 0.013711929321289062 	 1.2159347534179688e-05 	 7.486343383789062e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:50:28.873326 test begin: paddle.slice(Tensor([793810, 1280],"bfloat16"), axes=list[0,], starts=list[16336,], ends=list[24504,], )
W0730 10:50:44.399665 146327 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.slice 	 paddle.slice(Tensor([793810, 1280],"bfloat16"), axes=list[0,], starts=list[16336,], ends=list[24504,], ) 	 1016076800 	 1000 	 0.013321161270141602 	 0.019797563552856445 	 1.7881393432617188e-05 	 5.4836273193359375e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:50:46.548783 test begin: paddle.slice(Tensor([793810, 1280],"bfloat16"), axes=list[0,], starts=list[24504,], ends=list[32672,], )
W0730 10:51:01.888878 146669 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.slice 	 paddle.slice(Tensor([793810, 1280],"bfloat16"), axes=list[0,], starts=list[24504,], ends=list[32672,], ) 	 1016076800 	 1000 	 0.007706403732299805 	 0.013426780700683594 	 1.430511474609375e-05 	 3.790855407714844e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-30 10:51:03.996592 test begin: paddle.slice_scatter(Tensor([80, 423361, 3, 5],"float32"), Tensor([80, 2, 3, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([80, 423361, 3, 5],"float32"), Tensor([80, 2, 3, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 508035600 	 1000 	 0.015152692794799805 	 3.0680859088897705 	 1.1920928955078125e-05 	 1.0425207614898682 	 3.104126214981079 	 3.0756826400756836 	 0.5274293422698975 	 0.6271636486053467 	 combined
2025-07-30 10:51:33.282376 test begin: paddle.slice_scatter(Tensor([80, 6, 3, 176401],"float64"), Tensor([80, 6, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([80, 6, 3, 176401],"float64"), Tensor([80, 6, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 254020320 	 1000 	 0.014918327331542969 	 3.0669288635253906 	 1.52587890625e-05 	 1.0426051616668701 	 3.0955774784088135 	 3.0761144161224365 	 0.5262811183929443 	 0.6272015571594238 	 combined
2025-07-30 10:51:52.600211 test begin: paddle.slice_scatter(Tensor([80, 6, 3, 352801],"float32"), Tensor([80, 6, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([80, 6, 3, 352801],"float32"), Tensor([80, 6, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 508036320 	 1000 	 0.015152215957641602 	 3.068603992462158 	 1.9788742065429688e-05 	 1.0427212715148926 	 3.0907089710235596 	 3.0744564533233643 	 0.524486780166626 	 0.6271741390228271 	 combined
2025-07-30 10:52:17.882945 test begin: paddle.squeeze(Tensor([100, 512, 1, 100, 100],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([100, 512, 1, 100, 100],"float32"), axis=list[2,], ) 	 512000000 	 1000 	 0.015480518341064453 	 0.0049762725830078125 	 1.1682510375976562e-05 	 2.0503997802734375e-05 	 0.042437076568603516 	 0.0527036190032959 	 2.5272369384765625e-05 	 4.76837158203125e-05 	 
2025-07-30 10:52:33.933410 test begin: paddle.squeeze(Tensor([1053440, 483],"float32"), )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([1053440, 483],"float32"), ) 	 508811520 	 1000 	 0.003862142562866211 	 0.003876209259033203 	 1.1205673217773438e-05 	 2.193450927734375e-05 	 0.042382240295410156 	 0.04785513877868652 	 5.793571472167969e-05 	 3.981590270996094e-05 	 
2025-07-30 10:52:49.995597 test begin: paddle.squeeze(Tensor([3969010, 128],"float32"), )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([3969010, 128],"float32"), ) 	 508033280 	 1000 	 0.0039539337158203125 	 0.0038399696350097656 	 2.1219253540039062e-05 	 1.8596649169921875e-05 	 0.04460573196411133 	 0.058431386947631836 	 5.078315734863281e-05 	 7.271766662597656e-05 	 
2025-07-30 10:53:08.384473 test begin: paddle.squeeze(Tensor([4211200, 25, 5],"float32"), axis=-1, )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([4211200, 25, 5],"float32"), axis=-1, ) 	 526400000 	 1000 	 0.004582643508911133 	 0.004023551940917969 	 1.1920928955078125e-05 	 1.9073486328125e-05 	 0.04210686683654785 	 0.04854297637939453 	 3.504753112792969e-05 	 4.744529724121094e-05 	 
2025-07-30 10:53:25.462662 test begin: paddle.squeeze(Tensor([4211200, 31, 4],"float32"), axis=-1, )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([4211200, 31, 4],"float32"), axis=-1, ) 	 522188800 	 1000 	 0.004636287689208984 	 0.0040149688720703125 	 1.1205673217773438e-05 	 1.9550323486328125e-05 	 0.04265475273132324 	 0.04863595962524414 	 4.887580871582031e-05 	 5.364418029785156e-05 	 
2025-07-30 10:53:44.462191 test begin: paddle.squeeze(Tensor([5080330, 25, 4],"float32"), axis=-1, )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([5080330, 25, 4],"float32"), axis=-1, ) 	 508033000 	 1000 	 0.0045566558837890625 	 0.004002571105957031 	 1.0013580322265625e-05 	 1.9550323486328125e-05 	 0.04216885566711426 	 0.04829001426696777 	 3.147125244140625e-05 	 4.0531158447265625e-05 	 
2025-07-30 10:54:01.903955 test begin: paddle.squeeze(Tensor([80, 512, 1, 100, 125],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([80, 512, 1, 100, 125],"float32"), axis=list[2,], ) 	 512000000 	 1000 	 0.0047266483306884766 	 0.005322456359863281 	 1.1444091796875e-05 	 5.173683166503906e-05 	 0.044039249420166016 	 0.05338644981384277 	 3.4809112548828125e-05 	 4.220008850097656e-05 	 
2025-07-30 10:54:23.819643 test begin: paddle.squeeze(Tensor([80, 512, 1, 125, 100],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([80, 512, 1, 125, 100],"float32"), axis=list[2,], ) 	 512000000 	 1000 	 0.009464263916015625 	 0.00879049301147461 	 1.049041748046875e-05 	 2.6702880859375e-05 	 0.05284738540649414 	 0.08027124404907227 	 6.079673767089844e-05 	 0.000118255615234375 	 
2025-07-30 10:54:42.691658 test begin: paddle.squeeze(Tensor([80, 512, 2, 100, 100],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([80, 512, 2, 100, 100],"float32"), axis=list[2,], ) 	 819200000 	 1000 	 0.00946950912475586 	 0.005011320114135742 	 1.5020370483398438e-05 	 2.4080276489257812e-05 	 0.042450904846191406 	 0.052008867263793945 	 4.839897155761719e-05 	 6.103515625e-05 	 
2025-07-30 10:55:21.222928 test begin: paddle.squeeze(Tensor([80, 636, 1, 100, 100],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([80, 636, 1, 100, 100],"float32"), axis=list[2,], ) 	 508800000 	 1000 	 0.00943136215209961 	 0.004998922348022461 	 1.52587890625e-05 	 2.09808349609375e-05 	 0.0496981143951416 	 0.05327343940734863 	 2.6226043701171875e-05 	 6.008148193359375e-05 	 
2025-07-30 10:55:39.808549 test begin: paddle.t(Tensor([100, 5080321],"float32"), )
[Prof] paddle.t 	 paddle.t(Tensor([100, 5080321],"float32"), ) 	 508032100 	 1000 	 0.008671760559082031 	 0.0038084983825683594 	 1.4066696166992188e-05 	 2.5510787963867188e-05 	 0.04066061973571777 	 0.06912803649902344 	 2.8371810913085938e-05 	 0.00010251998901367188 	 
2025-07-30 10:56:00.156736 test begin: paddle.t(Tensor([200, 2540161],"float32"), )
[Prof] paddle.t 	 paddle.t(Tensor([200, 2540161],"float32"), ) 	 508032200 	 1000 	 0.004169464111328125 	 0.003754854202270508 	 1.2159347534179688e-05 	 2.3603439331054688e-05 	 0.04081249237060547 	 0.05619049072265625 	 3.409385681152344e-05 	 0.00010657310485839844 	 
2025-07-30 10:56:16.070556 test begin: paddle.t(Tensor([25401610, 20],"float32"), )
[Prof] paddle.t 	 paddle.t(Tensor([25401610, 20],"float32"), ) 	 508032200 	 1000 	 0.004240751266479492 	 0.003725767135620117 	 1.1682510375976562e-05 	 2.384185791015625e-05 	 0.04082441329956055 	 0.0692439079284668 	 4.6253204345703125e-05 	 9.870529174804688e-05 	 
2025-07-30 10:56:34.934054 test begin: paddle.t(Tensor([496130, 512],"int64"), )
[Prof] paddle.t 	 paddle.t(Tensor([496130, 512],"int64"), ) 	 254018560 	 1000 	 0.00422215461730957 	 0.003841400146484375 	 1.5497207641601562e-05 	 2.3365020751953125e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:56:42.404773 test begin: paddle.t(Tensor([50803210, 10],"float32"), )
[Prof] paddle.t 	 paddle.t(Tensor([50803210, 10],"float32"), ) 	 508032100 	 1000 	 0.004148006439208984 	 0.003751993179321289 	 9.059906005859375e-06 	 2.5272369384765625e-05 	 0.04069113731384277 	 0.0590360164642334 	 3.838539123535156e-05 	 7.796287536621094e-05 	 
2025-07-30 10:56:57.909985 test begin: paddle.t(Tensor([5120, 49613],"int64"), )
[Prof] paddle.t 	 paddle.t(Tensor([5120, 49613],"int64"), ) 	 254018560 	 1000 	 0.004117727279663086 	 0.003798961639404297 	 1.0251998901367188e-05 	 1.8835067749023438e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:57:06.713796 test begin: paddle.take(Tensor([12700801, 4],"float32"), Tensor([201, 3],"int64"), mode="raise", )
[Prof] paddle.take 	 paddle.take(Tensor([12700801, 4],"float32"), Tensor([201, 3],"int64"), mode="raise", ) 	 50803807 	 1000 	 0.08711099624633789 	 0.12409710884094238 	 1.9073486328125e-05 	 6.270408630371094e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:57:08.404229 test begin: paddle.take(Tensor([3, 16934401],"float32"), Tensor([201, 3],"int64"), mode="raise", )
[Prof] paddle.take 	 paddle.take(Tensor([3, 16934401],"float32"), Tensor([201, 3],"int64"), mode="raise", ) 	 50803806 	 1000 	 0.08795762062072754 	 0.12199664115905762 	 1.8358230590820312e-05 	 6.723403930664062e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:57:09.584006 test begin: paddle.take(Tensor([3, 8467201],"float64"), Tensor([501, 8],"int64"), mode="clip", )
[Prof] paddle.take 	 paddle.take(Tensor([3, 8467201],"float64"), Tensor([501, 8],"int64"), mode="clip", ) 	 25405611 	 1000 	 0.05610537528991699 	 0.04322075843811035 	 1.8835067749023438e-05 	 4.7206878662109375e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:57:10.316258 test begin: paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), list[2,3,], axis=3, )
W0730 10:57:17.592584 157588 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), list[2,3,], axis=3, ) 	 254017120 	 1000 	 0.024167537689208984 	 0.007957696914672852 	 1.1205673217773438e-05 	 2.5987625122070312e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:57:19.539480 test begin: paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), list[2,4,6,], axis=3, )
W0730 10:57:26.072638 157913 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), list[2,4,6,], axis=3, ) 	 254017120 	 1000 	 0.030829906463623047 	 0.009231090545654297 	 2.0742416381835938e-05 	 2.3365020751953125e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:57:26.770504 test begin: paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), tuple(2,6,), axis=3, )
W0730 10:57:33.709069 158080 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), tuple(2,6,), axis=3, ) 	 254017120 	 1000 	 0.02446579933166504 	 0.008666515350341797 	 4.506111145019531e-05 	 0.00010561943054199219 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:57:37.142372 test begin: paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), list[2,3,], axis=3, )
W0730 10:57:44.705264 158555 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), list[2,3,], axis=3, ) 	 254017120 	 1000 	 0.042539358139038086 	 0.012898683547973633 	 6.890296936035156e-05 	 3.075599670410156e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:57:46.456010 test begin: paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), list[2,4,6,], axis=3, )
W0730 10:57:53.728477 158748 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), list[2,4,6,], axis=3, ) 	 254017120 	 1000 	 0.05238223075866699 	 0.014573335647583008 	 3.981590270996094e-05 	 2.6941299438476562e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:57:55.070127 test begin: paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), tuple(2,6,), axis=3, )
W0730 10:58:01.673312 159066 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), tuple(2,6,), axis=3, ) 	 254017120 	 1000 	 0.023819923400878906 	 0.007886648178100586 	 1.430511474609375e-05 	 1.9550323486328125e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:58:02.652749 test begin: paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), list[2,3,], axis=3, )
W0730 10:58:09.530120 159379 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), list[2,3,], axis=3, ) 	 254017120 	 1000 	 0.04013633728027344 	 0.012942790985107422 	 1.4543533325195312e-05 	 2.6941299438476562e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:58:11.219611 test begin: paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), list[2,4,6,], axis=3, )
W0730 10:58:19.005458 159555 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), list[2,4,6,], axis=3, ) 	 254017120 	 1000 	 0.03092050552368164 	 0.009236812591552734 	 3.695487976074219e-05 	 2.6464462280273438e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:58:20.252770 test begin: paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), tuple(2,6,), axis=3, )
W0730 10:58:28.014011 159878 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), tuple(2,6,), axis=3, ) 	 254017120 	 1000 	 0.023722410202026367 	 0.007977724075317383 	 1.4066696166992188e-05 	 3.62396240234375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:58:29.605073 test begin: paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), list[2,3,], axis=3, )
W0730 10:58:36.613214 160202 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), list[2,3,], axis=3, ) 	 254016640 	 1000 	 0.04008913040161133 	 0.007995843887329102 	 2.3603439331054688e-05 	 2.4557113647460938e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:58:40.943358 test begin: paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), list[2,4,6,], axis=3, )
W0730 10:58:47.955281 160533 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), list[2,4,6,], axis=3, ) 	 254016640 	 1000 	 0.052973270416259766 	 0.009190797805786133 	 2.574920654296875e-05 	 2.7179718017578125e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:58:50.988860 test begin: paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), tuple(2,6,), axis=3, )
W0730 10:58:57.635097 160852 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), tuple(2,6,), axis=3, ) 	 254016640 	 1000 	 0.04024147987365723 	 0.007943391799926758 	 1.6450881958007812e-05 	 2.0503997802734375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:58:58.511759 test begin: paddle.transpose(Tensor([20, 150, 512, 512],"float32"), list[0,2,3,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([20, 150, 512, 512],"float32"), list[0,2,3,1,], ) 	 786432000 	 1000 	 0.003450155258178711 	 0.0046539306640625 	 1.3828277587890625e-05 	 2.8371810913085938e-05 	 0.041109561920166016 	 0.07370305061340332 	 5.817413330078125e-05 	 0.00010347366333007812 	 
2025-07-30 10:59:25.561911 test begin: paddle.transpose(Tensor([20, 7168, 7168],"bfloat16"), list[0,2,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([20, 7168, 7168],"bfloat16"), list[0,2,1,], ) 	 1027604480 	 1000 	 0.003382444381713867 	 0.004658699035644531 	 1.1205673217773438e-05 	 3.123283386230469e-05 	 0.046729087829589844 	 4.55017876625061 	 0.0001323223114013672 	 2.3264107704162598 	 
2025-07-30 11:00:05.121476 test begin: paddle.transpose(Tensor([40, 150, 166, 512],"float32"), list[0,2,3,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([40, 150, 166, 512],"float32"), list[0,2,3,1,], ) 	 509952000 	 1000 	 0.003431081771850586 	 0.008274316787719727 	 1.0251998901367188e-05 	 2.5033950805664062e-05 	 0.050594329833984375 	 0.0818183422088623 	 4.863739013671875e-05 	 0.0005407333374023438 	 
2025-07-30 11:00:27.189592 test begin: paddle.transpose(Tensor([40, 150, 512, 166],"float32"), list[0,2,3,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([40, 150, 512, 166],"float32"), list[0,2,3,1,], ) 	 509952000 	 1000 	 0.0034525394439697266 	 0.0056459903717041016 	 1.0251998901367188e-05 	 7.867813110351562e-05 	 0.04052567481994629 	 0.05529952049255371 	 5.030632019042969e-05 	 8.821487426757812e-05 	 
2025-07-30 11:00:43.074565 test begin: paddle.transpose(Tensor([40, 3584, 7168],"bfloat16"), list[0,2,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([40, 3584, 7168],"bfloat16"), list[0,2,1,], ) 	 1027604480 	 1000 	 0.0033676624298095703 	 0.004539012908935547 	 1.4066696166992188e-05 	 2.47955322265625e-05 	 0.04453873634338379 	 4.549787759780884 	 4.8160552978515625e-05 	 2.323699712753296 	 
2025-07-30 11:01:20.284022 test begin: paddle.transpose(Tensor([40, 49, 512, 512],"float32"), list[0,2,3,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([40, 49, 512, 512],"float32"), list[0,2,3,1,], ) 	 513802240 	 1000 	 0.003372669219970703 	 0.0046269893646240234 	 7.867813110351562e-06 	 2.0503997802734375e-05 	 0.04080080986022949 	 0.05354166030883789 	 3.24249267578125e-05 	 6.246566772460938e-05 	 
2025-07-30 11:01:39.054431 test begin: paddle.transpose(Tensor([60, 2363, 7168],"bfloat16"), list[0,2,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([60, 2363, 7168],"bfloat16"), list[0,2,1,], ) 	 1016279040 	 1000 	 0.003364086151123047 	 0.0045244693756103516 	 8.106231689453125e-06 	 2.2649765014648438e-05 	 0.04810643196105957 	 4.500250339508057 	 4.2438507080078125e-05 	 2.2980880737304688 	 
2025-07-30 11:02:20.913369 test begin: paddle.transpose(Tensor([60, 3584, 4726],"bfloat16"), list[0,2,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([60, 3584, 4726],"bfloat16"), list[0,2,1,], ) 	 1016279040 	 1000 	 0.007531404495239258 	 0.0046405792236328125 	 1.239776611328125e-05 	 3.552436828613281e-05 	 0.044698476791381836 	 4.500327825546265 	 4.1484832763671875e-05 	 2.3008148670196533 	 
2025-07-30 11:03:04.497286 test begin: paddle.transpose(Tensor([60, 7168, 2363],"bfloat16"), list[0,2,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([60, 7168, 2363],"bfloat16"), list[0,2,1,], ) 	 1016279040 	 1000 	 0.003391265869140625 	 0.004449367523193359 	 2.7179718017578125e-05 	 2.3126602172851562e-05 	 0.044733524322509766 	 4.498655080795288 	 4.8160552978515625e-05 	 2.2994065284729004 	 
2025-07-30 11:03:50.956378 test begin: paddle.trunc(Tensor([200, 2540161],"float32"), )
[Prof] paddle.trunc 	 paddle.trunc(Tensor([200, 2540161],"float32"), ) 	 508032200 	 1000 	 0.008173465728759766 	 2.931788682937622 	 9.059906005859375e-06 	 2.9160594940185547 	 0.05103254318237305 	 1.3112876415252686 	 4.124641418457031e-05 	 1.2318589687347412 	 
2025-07-30 11:04:14.552686 test begin: paddle.trunc(Tensor([25401610, 20],"float32"), )
[Prof] paddle.trunc 	 paddle.trunc(Tensor([25401610, 20],"float32"), ) 	 508032200 	 1000 	 0.008248567581176758 	 3.3869450092315674 	 1.5735626220703125e-05 	 2.9129891395568848 	 0.050188302993774414 	 1.3127667903900146 	 2.9802322387695312e-05 	 1.2515692710876465 	 
2025-07-30 11:04:42.013284 test begin: paddle.trunc(input=Tensor([1176010, 6, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([1176010, 6, 6, 6],"float64"), ) 	 254018160 	 1000 	 0.008388042449951172 	 2.9173810482025146 	 1.33514404296875e-05 	 2.905189275741577 	 0.0513911247253418 	 1.3117845058441162 	 3.218650817871094e-05 	 1.2450029850006104 	 
2025-07-30 11:05:00.179795 test begin: paddle.trunc(input=Tensor([196010, 6, 6, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([196010, 6, 6, 6, 6],"float64"), ) 	 254028960 	 1000 	 0.008379220962524414 	 2.9143283367156982 	 1.7881393432617188e-05 	 2.902223587036133 	 0.050260305404663086 	 1.3176782131195068 	 4.482269287109375e-05 	 1.2456550598144531 	 
2025-07-30 11:05:15.417699 test begin: paddle.trunc(input=Tensor([30, 39201, 6, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([30, 39201, 6, 6, 6],"float64"), ) 	 254022480 	 1000 	 0.008449077606201172 	 2.9173338413238525 	 1.9550323486328125e-05 	 2.905207872390747 	 0.05047106742858887 	 1.311758041381836 	 3.123283386230469e-05 	 1.246208667755127 	 
2025-07-30 11:05:30.759958 test begin: paddle.trunc(input=Tensor([30, 6, 39201, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([30, 6, 39201, 6, 6],"float64"), ) 	 254022480 	 1000 	 0.008369207382202148 	 2.942091464996338 	 1.1682510375976562e-05 	 2.9013335704803467 	 0.050435543060302734 	 1.3139116764068604 	 3.314018249511719e-05 	 1.2504994869232178 	 
2025-07-30 11:05:46.436224 test begin: paddle.trunc(input=Tensor([30, 6, 6, 39201, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([30, 6, 6, 39201, 6],"float64"), ) 	 254022480 	 1000 	 0.015576839447021484 	 2.9175262451171875 	 1.3828277587890625e-05 	 2.8992786407470703 	 0.058640480041503906 	 1.3118526935577393 	 3.838539123535156e-05 	 1.2282030582427979 	 
2025-07-30 11:06:05.455994 test begin: paddle.trunc(input=Tensor([30, 6, 6, 6, 39201],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([30, 6, 6, 6, 39201],"float64"), ) 	 254022480 	 1000 	 0.00826883316040039 	 2.914303779602051 	 1.7881393432617188e-05 	 2.9023821353912354 	 0.050534963607788086 	 1.3147382736206055 	 4.792213439941406e-05 	 1.2235910892486572 	 
2025-07-30 11:06:21.811171 test begin: paddle.trunc(input=Tensor([60, 117601, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([60, 117601, 6, 6],"float64"), ) 	 254018160 	 1000 	 0.008254289627075195 	 2.91815185546875 	 1.1682510375976562e-05 	 2.904093027114868 	 0.050889015197753906 	 1.3158409595489502 	 3.266334533691406e-05 	 1.2529363632202148 	 
2025-07-30 11:06:39.421852 test begin: paddle.trunc(input=Tensor([60, 6, 117601, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([60, 6, 117601, 6],"float64"), ) 	 254018160 	 1000 	 0.008711099624633789 	 2.930917739868164 	 3.337860107421875e-05 	 2.9032375812530518 	 0.050238847732543945 	 1.315035104751587 	 2.1457672119140625e-05 	 1.2319672107696533 	 
2025-07-30 11:06:56.727733 test begin: paddle.trunc(input=Tensor([60, 6, 6, 117601],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([60, 6, 6, 117601],"float64"), ) 	 254018160 	 1000 	 0.00836491584777832 	 2.915738344192505 	 1.5020370483398438e-05 	 2.90368914604187 	 0.05054163932800293 	 1.3135294914245605 	 3.981590270996094e-05 	 1.2278070449829102 	 
2025-07-30 11:07:12.285468 test begin: paddle.unbind(Tensor([20, 3, 1058401, 8],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([20, 3, 1058401, 8],"float32"), axis=0, ) 	 508032480 	 1000 	 0.02828240394592285 	 0.02291417121887207 	 2.9802322387695312e-05 	 2.7179718017578125e-05 	 3.6326444149017334 	 3.0434188842773438 	 3.5470261573791504 	 2.8312928676605225 	 
2025-07-30 11:07:37.418982 test begin: paddle.unbind(Tensor([20, 3, 8, 1058401],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([20, 3, 8, 1058401],"float32"), axis=0, ) 	 508032480 	 1000 	 0.02872443199157715 	 0.022960662841796875 	 2.574920654296875e-05 	 3.719329833984375e-05 	 3.6268041133880615 	 3.042114019393921 	 3.5414178371429443 	 2.810746431350708 	 
2025-07-30 11:08:00.420089 test begin: paddle.unbind(Tensor([20, 396901, 8, 8],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([20, 396901, 8, 8],"float32"), axis=0, ) 	 508033280 	 1000 	 0.03585314750671387 	 0.022829294204711914 	 4.506111145019531e-05 	 4.363059997558594e-05 	 3.5204055309295654 	 2.9833667278289795 	 3.432356357574463 	 2.746217727661133 	 
2025-07-30 11:08:24.743936 test begin: paddle.unbind(Tensor([30, 3386881, 5],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([30, 3386881, 5],"float32"), axis=0, ) 	 508032150 	 1000 	 0.04029393196105957 	 0.039637088775634766 	 6.389617919921875e-05 	 0.00010895729064941406 	 3.6787667274475098 	 3.070493221282959 	 3.5721797943115234 	 2.8049397468566895 	 
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 9, in <module>
    from tester import (APIConfig, APITestAccuracy, APITestCINNVSDygraph,
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/__init__.py", line 74, in __getattr__
    from .api_config import APIConfig
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/__init__.py", line 22, in __getattr__
    from .config_analyzer import APIConfig
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py", line 7, in <module>
    import numpy
  File "/usr/local/lib/python3.10/dist-packages/numpy/__init__.py", line 149, in <module>
    from . import lib
  File "/usr/local/lib/python3.10/dist-packages/numpy/lib/__init__.py", line 23, in <module>
    from . import index_tricks
  File "/usr/local/lib/python3.10/dist-packages/numpy/lib/index_tricks.py", line 12, in <module>
    import numpy.matrixlib as matrixlib
  File "/usr/local/lib/python3.10/dist-packages/numpy/matrixlib/__init__.py", line 4, in <module>
    from . import defmatrix
  File "/usr/local/lib/python3.10/dist-packages/numpy/matrixlib/defmatrix.py", line 12, in <module>
    from numpy.linalg import matrix_power
  File "/usr/local/lib/python3.10/dist-packages/numpy/linalg/__init__.py", line 73, in <module>
    from . import linalg
  File "/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py", line 37, in <module>
    from numpy._typing import NDArray
  File "/usr/local/lib/python3.10/dist-packages/numpy/_typing/__init__.py", line 182, in <module>
    from ._array_like import (
  File "/usr/local/lib/python3.10/dist-packages/numpy/_typing/_array_like.py", line 122, in <module>
    dtype[Union[bool_, number[Any]]],
  File "/usr/lib/python3.10/typing.py", line 309, in inner
    return cached(*args, **kwds)
  File "/usr/lib/python3.10/typing.py", line 403, in __getitem__
    return self._getitem(self, parameters)
  File "/usr/lib/python3.10/typing.py", line 516, in Union
    parameters = _remove_dups_flatten(parameters)
  File "/usr/lib/python3.10/typing.py", line 274, in _remove_dups_flatten
    if isinstance(p, (_UnionGenericAlias, types.UnionType)):
KeyboardInterrupt
2025-07-30 10:36:25.692012 test begin: paddle.Tensor.diagonal(Tensor([301, 84672],"float64"), axis1=-2, axis2=-1, )
W0730 10:36:28.247304 131926 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.diagonal 	 paddle.Tensor.diagonal(Tensor([301, 84672],"float64"), axis1=-2, axis2=-1, ) 	 25486272 	 1000 	 0.003843069076538086 	 0.0049304962158203125 	 1.5020370483398438e-05 	 2.8371810913085938e-05 	 0.15063881874084473 	 0.14028143882751465 	 0.07682323455810547 	 0.06237292289733887 	 
2025-07-30 10:36:29.066781 test begin: paddle.Tensor.diagonal(Tensor([8467201, 3],"float64"), axis1=-2, axis2=-1, )
[Prof] paddle.Tensor.diagonal 	 paddle.Tensor.diagonal(Tensor([8467201, 3],"float64"), axis1=-2, axis2=-1, ) 	 25401603 	 1000 	 0.0037691593170166016 	 0.004511833190917969 	 1.1205673217773438e-05 	 1.9311904907226562e-05 	 0.14687180519104004 	 0.13852858543395996 	 0.07495737075805664 	 0.06480550765991211 	 
2025-07-30 10:36:29.900841 test begin: paddle.Tensor.gather_nd(Tensor([11, 53, 8],"float32"), Tensor([40, 50, 2],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([11, 53, 8],"float32"), Tensor([40, 50, 2],"int64"), ) 	 8664 	 1000 	 0.011026382446289062 	 159.18136835098267 	 1.1205673217773438e-05 	 0.00026726722717285156 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:39:09.354093 test begin: paddle.Tensor.gather_nd(Tensor([16, 3, 15, 80, 8],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([16, 3, 15, 80, 8],"float32"), Tensor([516, 4],"int64"), ) 	 462864 	 1000 	 0.010753393173217773 	 78.38994789123535 	 1.1682510375976562e-05 	 0.00022411346435546875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:40:27.915736 test begin: paddle.Tensor.gather_nd(Tensor([16, 3, 80, 156, 85],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([16, 3, 80, 156, 85],"float32"), Tensor([516, 4],"int64"), ) 	 50920464 	 1000 	 0.011019229888916016 	 77.94292306900024 	 1.7404556274414062e-05 	 0.00022673606872558594 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:41:46.932345 test begin: paddle.Tensor.gather_nd(Tensor([16, 3, 80, 80, 166],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([16, 3, 80, 80, 166],"float32"), Tensor([516, 4],"int64"), ) 	 50997264 	 1000 	 0.010591268539428711 	 78.13264989852905 	 1.239776611328125e-05 	 0.00010943412780761719 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:43:06.169706 test begin: paddle.Tensor.gather_nd(Tensor([16, 6, 80, 80, 85],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([16, 6, 80, 80, 85],"float32"), Tensor([516, 4],"int64"), ) 	 52226064 	 1000 	 0.010651588439941406 	 78.31317234039307 	 1.1205673217773438e-05 	 0.00022745132446289062 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:44:25.611757 test begin: paddle.Tensor.gather_nd(Tensor([32, 3, 80, 80, 85],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([32, 3, 80, 80, 85],"float32"), Tensor([385, 4],"int64"), ) 	 52225540 	 1000 	 0.010483741760253906 	 57.982876777648926 	 1.239776611328125e-05 	 0.00022220611572265625 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:45:24.713873 test begin: paddle.Tensor.gather_nd(Tensor([32, 3, 80, 80, 85],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([32, 3, 80, 80, 85],"float32"), Tensor([516, 4],"int64"), ) 	 52226064 	 1000 	 0.01035451889038086 	 77.81084251403809 	 1.1444091796875e-05 	 0.00023174285888671875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:46:43.632169 test begin: paddle.Tensor.gather_nd(Tensor([8, 12, 80, 80, 85],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([8, 12, 80, 80, 85],"float32"), Tensor([385, 4],"int64"), ) 	 52225540 	 1000 	 0.010451793670654297 	 58.12041711807251 	 1.0967254638671875e-05 	 0.000164031982421875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:47:42.918355 test begin: paddle.Tensor.gather_nd(Tensor([8, 3, 312, 80, 85],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([8, 3, 312, 80, 85],"float32"), Tensor([385, 4],"int64"), ) 	 50919940 	 1000 	 0.010671854019165039 	 58.040913581848145 	 1.1682510375976562e-05 	 0.00021314620971679688 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:48:42.060419 test begin: paddle.Tensor.gather_nd(Tensor([8, 3, 80, 312, 85],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([8, 3, 80, 312, 85],"float32"), Tensor([385, 4],"int64"), ) 	 50919940 	 1000 	 0.010808229446411133 	 59.39085531234741 	 1.1920928955078125e-05 	 0.0002162456512451172 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:49:42.566044 test begin: paddle.Tensor.gather_nd(Tensor([8, 3, 80, 80, 331],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([8, 3, 80, 80, 331],"float32"), Tensor([385, 4],"int64"), ) 	 50843140 	 1000 	 0.010965347290039062 	 58.54001522064209 	 1.2874603271484375e-05 	 0.00022077560424804688 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:50:42.167452 test begin: paddle.cartesian_prod(list[Tensor([20],"complex128"),Tensor([50],"complex128"),Tensor([5080],"complex128"),], )
[Prof] paddle.cartesian_prod 	 paddle.cartesian_prod(list[Tensor([20],"complex128"),Tensor([50],"complex128"),Tensor([5080],"complex128"),], ) 	 5150 	 1000 	 0.5876197814941406 	 1.1209321022033691 	 0.08579897880554199 	 0.2853250503540039 	 110.83751511573792 	 0.5247957706451416 	 28.274062156677246 	 0.10718441009521484 	 
2025-07-30 10:52:37.581989 test begin: paddle.cartesian_prod(list[Tensor([30],"complex128"),Tensor([30],"complex128"),Tensor([5080],"complex128"),], )
[Prof] paddle.cartesian_prod 	 paddle.cartesian_prod(list[Tensor([30],"complex128"),Tensor([30],"complex128"),Tensor([5080],"complex128"),], ) 	 5140 	 1000 	 0.536412239074707 	 1.0076322555541992 	 0.07767105102539062 	 0.25717711448669434 	 78.31415319442749 	 0.47588562965393066 	 19.984753608703613 	 0.09720087051391602 	 
2025-07-30 10:53:58.457677 test begin: paddle.cartesian_prod(list[Tensor([40],"int32"),Tensor([30],"int32"),Tensor([5080],"int32"),], )
[Prof] paddle.cartesian_prod 	 paddle.cartesian_prod(list[Tensor([40],"int32"),Tensor([30],"int32"),Tensor([5080],"int32"),], ) 	 5150 	 1000 	 0.3628866672515869 	 0.5290176868438721 	 0.05281639099121094 	 0.1352839469909668 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:55:00.040923 test begin: paddle.cartesian_prod(list[Tensor([40],"int32"),Tensor([400],"int32"),Tensor([508],"int32"),], )
[Prof] paddle.cartesian_prod 	 paddle.cartesian_prod(list[Tensor([40],"int32"),Tensor([400],"int32"),Tensor([508],"int32"),], ) 	 948 	 1000 	 0.4745166301727295 	 0.7049579620361328 	 0.06879949569702148 	 0.18035531044006348 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 10:56:18.520920 test begin: paddle.crop(x=Tensor([201, 14112, 3, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([201, 14112, 3, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 	 25528608 	 1000 	 0.02333664894104004 	 0.019623994827270508 	 2.5510787963867188e-05 	 5.4836273193359375e-05 	 0.14781475067138672 	 0.16239118576049805 	 0.09110331535339355 	 0.01259469985961914 	 combined
2025-07-30 10:56:19.477642 test begin: paddle.crop(x=Tensor([201, 3, 14112, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([201, 3, 14112, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 	 25528608 	 1000 	 0.018056154251098633 	 0.019426345825195312 	 2.6226043701171875e-05 	 4.673004150390625e-05 	 0.1506054401397705 	 0.17000842094421387 	 0.10066103935241699 	 0.019144296646118164 	 combined
2025-07-30 10:56:20.373233 test begin: paddle.crop(x=Tensor([201, 3, 3, 14112],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([201, 3, 3, 14112],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 	 25528608 	 1000 	 0.017893314361572266 	 0.019776582717895508 	 1.33514404296875e-05 	 5.1975250244140625e-05 	 0.14639520645141602 	 0.1672370433807373 	 0.09655046463012695 	 0.018989086151123047 	 combined
2025-07-30 10:56:21.248861 test begin: paddle.crop(x=Tensor([301, 84672],"float64"), shape=list[2,2,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([301, 84672],"float64"), shape=list[2,2,], ) 	 25486272 	 1000 	 0.018011808395385742 	 0.016153335571289062 	 1.2159347534179688e-05 	 6.461143493652344e-05 	 0.14613032341003418 	 0.1511240005493164 	 0.09634947776794434 	 0.03077411651611328 	 combined
2025-07-30 10:56:22.100381 test begin: paddle.crop(x=Tensor([8467201, 3],"float64"), shape=list[2,2,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([8467201, 3],"float64"), shape=list[2,2,], ) 	 25401603 	 1000 	 0.017856597900390625 	 0.013422727584838867 	 1.1920928955078125e-05 	 3.4332275390625e-05 	 0.14642071723937988 	 0.14373302459716797 	 0.09668779373168945 	 0.026803970336914062 	 combined
2025-07-30 10:56:22.937498 test begin: paddle.diagonal(x=Tensor([117601, 6, 6, 6],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([117601, 6, 6, 6],"float64"), ) 	 25401816 	 1000 	 0.003586292266845703 	 0.004268169403076172 	 7.152557373046875e-06 	 1.7642974853515625e-05 	 0.1467573642730713 	 0.1387636661529541 	 0.07489514350891113 	 0.06745100021362305 	 
2025-07-30 10:56:23.849223 test begin: paddle.diagonal(x=Tensor([176401, 6, 6, 2, 2],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([176401, 6, 6, 2, 2],"float64"), ) 	 25401744 	 1000 	 0.003587484359741211 	 0.00435328483581543 	 7.3909759521484375e-06 	 1.7642974853515625e-05 	 0.14856839179992676 	 0.13888072967529297 	 0.07517695426940918 	 0.04760599136352539 	 
2025-07-30 10:56:24.688997 test begin: paddle.diagonal(x=Tensor([601, 1176, 6, 6],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 1176, 6, 6],"float64"), ) 	 25443936 	 1000 	 0.0036077499389648438 	 0.004320383071899414 	 6.4373016357421875e-06 	 1.7881393432617188e-05 	 0.14837121963500977 	 0.13967466354370117 	 0.07500362396240234 	 0.06589388847351074 	 
2025-07-30 10:56:25.519198 test begin: paddle.diagonal(x=Tensor([601, 1764, 6, 2, 2],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 1764, 6, 2, 2],"float64"), ) 	 25443936 	 1000 	 0.003641843795776367 	 0.00432896614074707 	 1.1205673217773438e-05 	 1.7404556274414062e-05 	 0.14717459678649902 	 0.13959121704101562 	 0.07512521743774414 	 0.06678175926208496 	 
2025-07-30 10:56:26.337838 test begin: paddle.diagonal(x=Tensor([601, 6, 1176, 6],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 6, 1176, 6],"float64"), ) 	 25443936 	 1000 	 0.003632783889770508 	 0.004312753677368164 	 8.58306884765625e-06 	 1.6689300537109375e-05 	 0.1470048427581787 	 0.1396329402923584 	 0.07513761520385742 	 0.06762957572937012 	 
2025-07-30 10:56:27.153381 test begin: paddle.diagonal(x=Tensor([601, 6, 1764, 2, 2],"float64"), axis1=-1, axis2=2, )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 6, 1764, 2, 2],"float64"), axis1=-1, axis2=2, ) 	 25443936 	 1000 	 0.0037903785705566406 	 0.004532575607299805 	 1.6689300537109375e-05 	 1.71661376953125e-05 	 0.14990520477294922 	 0.14302730560302734 	 0.07655143737792969 	 0.06993770599365234 	 
2025-07-30 10:56:27.969565 test begin: paddle.diagonal(x=Tensor([601, 6, 6, 1176],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 6, 6, 1176],"float64"), ) 	 25443936 	 1000 	 0.0036118030548095703 	 0.004329204559326172 	 6.9141387939453125e-06 	 1.7404556274414062e-05 	 0.14706039428710938 	 0.13959884643554688 	 0.07505011558532715 	 0.06709027290344238 	 
2025-07-30 10:56:28.789350 test begin: paddle.diagonal(x=Tensor([601, 6, 6, 2, 588],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 6, 6, 2, 588],"float64"), ) 	 25443936 	 1000 	 0.0035147666931152344 	 0.007032155990600586 	 1.3589859008789062e-05 	 6.937980651855469e-05 	 0.1470165252685547 	 0.13968348503112793 	 0.07507157325744629 	 0.06730127334594727 	 
2025-07-30 10:56:29.618022 test begin: paddle.diagonal(x=Tensor([601, 6, 6, 2, 588],"float64"), axis1=-1, axis2=2, )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 6, 6, 2, 588],"float64"), axis1=-1, axis2=2, ) 	 25443936 	 1000 	 0.0036935806274414062 	 0.004910945892333984 	 6.67572021484375e-06 	 4.57763671875e-05 	 0.16439104080200195 	 0.1545884609222412 	 0.08397507667541504 	 0.07892084121704102 	 
2025-07-30 10:56:30.481296 test begin: paddle.diagonal(x=Tensor([601, 6, 6, 588, 2],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 6, 6, 588, 2],"float64"), ) 	 25443936 	 1000 	 0.003609895706176758 	 0.0062427520751953125 	 5.9604644775390625e-06 	 4.696846008300781e-05 	 0.14714765548706055 	 0.13960671424865723 	 0.07510590553283691 	 0.06573319435119629 	 
2025-07-30 10:56:31.304336 test begin: paddle.fft.ihfft(x=Tensor([201, 14112, 3, 3],"float64"), n=2, axis=1, )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([201, 14112, 3, 3],"float64"), n=2, axis=1, ) 	 25528608 	 1000 	 0.06763887405395508 	 0.0556340217590332 	 2.0742416381835938e-05 	 7.796287536621094e-05 	 0.17511725425720215 	 0.17109155654907227 	 0.022382020950317383 	 0.000102996826171875 	 
2025-07-30 10:56:32.331108 test begin: paddle.fft.ihfft(x=Tensor([201, 4, 3, 10584],"float64"), n=2, )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([201, 4, 3, 10584],"float64"), n=2, ) 	 25528608 	 1000 	 0.07285284996032715 	 0.04001045227050781 	 2.4318695068359375e-05 	 7.2479248046875e-05 	 0.17562437057495117 	 0.16020989418029785 	 0.02246689796447754 	 0.000125885009765625 	 
2025-07-30 10:56:33.311018 test begin: paddle.fft.ihfft2(x=Tensor([401, 21168, 3],"float64"), s=tuple(1,2,), )
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([401, 21168, 3],"float64"), s=tuple(1,2,), ) 	 25465104 	 1000 	 0.09441089630126953 	 0.06566643714904785 	 3.170967102050781e-05 	 6.914138793945312e-05 	 0.17141485214233398 	 0.20059418678283691 	 0.021813154220581055 	 0.00010204315185546875 	 
2025-07-30 10:56:34.444161 test begin: paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([482, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([482, 1],"int64"), ) 	 58720738 	 1000 	 0.5422275066375732 	 15.576245784759521 	 0.07766246795654297 	 0.00022339820861816406 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:56:54.529172 test begin: paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([496, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([496, 1],"int64"), ) 	 58720752 	 1000 	 0.09008908271789551 	 16.182488203048706 	 0.08023381233215332 	 0.0002243518829345703 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:57:12.449145 test begin: paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([512, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([512, 1],"int64"), ) 	 58720768 	 1000 	 0.0930166244506836 	 16.71258854866028 	 0.08317303657531738 	 0.0002224445343017578 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:57:30.918374 test begin: paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([482, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([482, 1],"int64"), ) 	 58720738 	 1000 	 0.08734512329101562 	 15.823960781097412 	 0.07756805419921875 	 0.00018787384033203125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:57:48.517494 test begin: paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([496, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([496, 1],"int64"), ) 	 58720752 	 1000 	 0.08977985382080078 	 16.042875051498413 	 0.07991647720336914 	 0.0002181529998779297 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:58:06.636080 test begin: paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([512, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([512, 1],"int64"), ) 	 58720768 	 1000 	 0.7609338760375977 	 16.685784101486206 	 0.08253669738769531 	 0.0002219676971435547 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:58:26.406888 test begin: paddle.incubate.segment_mean(Tensor([301, 16934],"float32"), Tensor([301],"int32"), )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_mean" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_mean" instead.
    Reason: paddle.incubate.segment_mean will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_mean" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_mean" instead.
    Reason: paddle.incubate.segment_mean will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.incubate.segment_mean 	 paddle.incubate.segment_mean(Tensor([301, 16934],"float32"), Tensor([301],"int32"), ) 	 5097435 	 1000 	 0.07015752792358398 	 0.26906609535217285 	 4.076957702636719e-05 	 7.915496826171875e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 10:58:31.654004 test begin: paddle.nn.functional.gather_tree(Tensor([100, 4, 8],"int64"), Tensor([100, 4, 8],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([100, 4, 8],"int64"), Tensor([100, 4, 8],"int64"), ) 	 6400 	 1000 	 0.01125788688659668 	 209.64300775527954 	 1.7881393432617188e-05 	 0.00022649765014648438 	 None 	 None 	 None 	 None 	 combined
2025-07-30 11:02:01.563240 test begin: paddle.nn.functional.gather_tree(Tensor([100, 8, 4],"int64"), Tensor([100, 8, 4],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([100, 8, 4],"int64"), Tensor([100, 8, 4],"int64"), ) 	 6400 	 1000 	 0.010762691497802734 	 208.6503565311432 	 9.775161743164062e-06 	 0.00024008750915527344 	 None 	 None 	 None 	 None 	 combined
2025-07-30 11:05:30.479812 test begin: paddle.nn.functional.gather_tree(Tensor([20, 28, 8],"int64"), Tensor([20, 28, 8],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([20, 28, 8],"int64"), Tensor([20, 28, 8],"int64"), ) 	 8960 	 1000 	 0.010846138000488281 	 287.10151386260986 	 1.3589859008789062e-05 	 0.0002238750457763672 	 None 	 None 	 None 	 None 	 combined
2025-07-30 11:10:17.897799 test begin: paddle.nn.functional.gather_tree(Tensor([20, 30, 4],"int64"), Tensor([20, 30, 4],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([20, 30, 4],"int64"), Tensor([20, 30, 4],"int64"), ) 	 4800 	 1000 	 0.01104426383972168 	 154.05935525894165 	 1.33514404296875e-05 	 0.00022172927856445312 	 None 	 None 	 None 	 None 	 combined
2025-07-30 11:12:52.130387 test begin: paddle.nn.functional.gather_tree(Tensor([20, 4, 57],"int64"), Tensor([20, 4, 57],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([20, 4, 57],"int64"), Tensor([20, 4, 57],"int64"), ) 	 9120 	 1000 	 0.010866641998291016 	 291.4902093410492 	 1.0251998901367188e-05 	 0.0002288818359375 	 None 	 None 	 None 	 None 	 combined
2025-07-30 11:17:45.563799 test begin: paddle.nn.functional.gather_tree(Tensor([20, 57, 4],"int64"), Tensor([20, 57, 4],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([20, 57, 4],"int64"), Tensor([20, 57, 4],"int64"), ) 	 9120 	 1000 	 0.010883808135986328 	 291.70635867118835 	 1.52587890625e-05 	 0.0002338886260986328 	 None 	 None 	 None 	 None 	 combined
2025-07-30 11:22:39.081577 test begin: paddle.nn.functional.gather_tree(Tensor([20, 8, 15],"int64"), Tensor([20, 8, 15],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([20, 8, 15],"int64"), Tensor([20, 8, 15],"int64"), ) 	 4800 	 1000 	 0.015215635299682617 	 153.38845920562744 	 2.3365020751953125e-05 	 0.0002415180206298828 	 None 	 None 	 None 	 None 	 combined
2025-07-30 11:25:12.751176 test begin: paddle.nn.functional.gather_tree(Tensor([200, 4, 4],"int64"), Tensor([200, 4, 4],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([200, 4, 4],"int64"), Tensor([200, 4, 4],"int64"), ) 	 6400 	 1000 	 0.01093912124633789 	 209.3712661266327 	 1.4543533325195312e-05 	 0.00023174285888671875 	 None 	 None 	 None 	 None 	 combined
2025-07-30 11:28:43.154423 test begin: paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="mean", name=None, )
[Prof] paddle.nn.functional.nll_loss 	 paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="mean", name=None, ) 	 25443143 	 1000 	 0.02794480323791504 	 0.0384974479675293 	 2.1457672119140625e-05 	 4.673004150390625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 11:28:43.905038 test begin: paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="none", name=None, )
[Prof] paddle.nn.functional.nll_loss 	 paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="none", name=None, ) 	 25443143 	 1000 	 0.028707027435302734 	 0.02390742301940918 	 2.288818359375e-05 	 5.054473876953125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 11:28:44.632716 test begin: paddle.scatter_nd(Tensor([1280, 2],"int64"), Tensor([1280, 9, 10],"float32"), list[3,5,9,10,], )
[Prof] paddle.scatter_nd 	 paddle.scatter_nd(Tensor([1280, 2],"int64"), Tensor([1280, 9, 10],"float32"), list[3,5,9,10,], ) 	 117760 	 1000 	 0.03645944595336914 	 159.060462474823 	 1.33514404296875e-05 	 0.00020551681518554688 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-30 11:31:24.120544 test begin: paddle.strided_slice(x=Tensor([301, 4, 3528, 6],"float64"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
[Prof] paddle.strided_slice 	 paddle.strided_slice(x=Tensor([301, 4, 3528, 6],"float64"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], ) 	 25486272 	 1000 	 0.0059092044830322266 	 0.21388697624206543 	 9.059906005859375e-06 	 6.270408630371094e-05 	 0.15346765518188477 	 0.25237536430358887 	 0.07768654823303223 	 0.008614063262939453 	 combined
2025-07-30 11:31:25.919229 test begin: paddle.trace(x=Tensor([20, 3, 42336],"float64"), offset=1, axis1=0, axis2=2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([20, 3, 42336],"float64"), offset=1, axis1=0, axis2=2, ) 	 2540160 	 1000 	 0.06568551063537598 	 0.020643949508666992 	 2.7418136596679688e-05 	 3.8623809814453125e-05 	 0.16761088371276855 	 0.08569025993347168 	 3.147125244140625e-05 	 3.361701965332031e-05 	 combined
2025-07-30 11:31:26.318519 test begin: paddle.trace(x=Tensor([30, 84672],"float64"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([30, 84672],"float64"), offset=0, axis1=0, axis2=1, ) 	 2540160 	 1000 	 0.07085585594177246 	 0.020215749740600586 	 2.86102294921875e-05 	 3.337860107421875e-05 	 0.13285136222839355 	 0.08497071266174316 	 2.384185791015625e-05 	 3.981590270996094e-05 	 combined
2025-07-30 11:31:26.671412 test begin: paddle.trace(x=Tensor([4233601, 3, 2],"float64"), offset=0, axis1=-3, axis2=-2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([4233601, 3, 2],"float64"), offset=0, axis1=-3, axis2=-2, ) 	 25401606 	 1000 	 0.06557965278625488 	 0.020911455154418945 	 4.291534423828125e-05 	 6.031990051269531e-05 	 0.7626347541809082 	 0.13834738731384277 	 3.790855407714844e-05 	 0.04947924613952637 	 combined
2025-07-30 11:31:28.197058 test begin: paddle.trace(x=Tensor([4233601, 3, 2],"float64"), offset=1, axis1=0, axis2=2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([4233601, 3, 2],"float64"), offset=1, axis1=0, axis2=2, ) 	 25401606 	 1000 	 0.06594514846801758 	 0.020734786987304688 	 2.86102294921875e-05 	 4.839897155761719e-05 	 0.7545061111450195 	 0.1397075653076172 	 2.6464462280273438e-05 	 0.05178976058959961 	 combined
2025-07-30 11:31:29.852923 test begin: paddle.trace(x=Tensor([6350401, 4],"float64"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([6350401, 4],"float64"), offset=0, axis1=0, axis2=1, ) 	 25401604 	 1000 	 0.06685376167297363 	 0.020638227462768555 	 2.9325485229492188e-05 	 5.7220458984375e-05 	 0.603938102722168 	 0.13818693161010742 	 2.6702880859375e-05 	 0.05336880683898926 	 combined
2025-07-30 11:31:31.219700 test begin: paddle.unbind(Tensor([30, 9, 1881601],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([30, 9, 1881601],"float32"), axis=0, ) 	 508032270 	 1000 	 0.048470497131347656 	 0.03409600257873535 	 5.245208740234375e-05 	 6.222724914550781e-05 	 3.6856119632720947 	 3.0749142169952393 	 3.554877519607544 	 2.8049206733703613 	 
2025-07-30 11:31:57.723156 test begin: paddle.unbind(Tensor([40, 2116801, 6],"float32"), )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([40, 2116801, 6],"float32"), ) 	 508032240 	 1000 	 0.04775190353393555 	 0.04388594627380371 	 3.1948089599609375e-05 	 5.936622619628906e-05 	 3.6707472801208496 	 3.0533981323242188 	 3.5418620109558105 	 2.680394411087036 	 
2025-07-30 11:32:21.493104 test begin: paddle.unbind(Tensor([40, 5, 2540161],"float32"), )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([40, 5, 2540161],"float32"), ) 	 508032200 	 1000 	 0.04844784736633301 	 0.04417538642883301 	 5.125999450683594e-05 	 0.0001087188720703125 	 3.6761820316314697 	 3.0552756786346436 	 3.562333106994629 	 2.72111177444458 	 
2025-07-30 11:32:48.594402 test begin: paddle.unflatten(x=Tensor([40, 6, 2116801],"float32"), axis=0, shape=Tensor([2],"int64"), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([40, 6, 2116801],"float32"), axis=0, shape=Tensor([2],"int64"), ) 	 508032242 	 1000 	 0.0945577621459961 	 0.005028486251831055 	 3.5762786865234375e-05 	 2.002716064453125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 11:33:06.080777 test begin: paddle.unflatten(x=Tensor([40, 793801, 16],"float32"), axis=0, shape=Tensor([2],"int64"), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([40, 793801, 16],"float32"), axis=0, shape=Tensor([2],"int64"), ) 	 508032642 	 1000 	 0.0908823013305664 	 0.005012989044189453 	 3.409385681152344e-05 	 2.0742416381835938e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 11:33:22.808103 test begin: paddle.unflatten(x=Tensor([5292010, 6, 16],"float32"), axis=0, shape=Tensor([2],"int64"), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([5292010, 6, 16],"float32"), axis=0, shape=Tensor([2],"int64"), ) 	 508032962 	 1000 	 0.08982419967651367 	 0.005223512649536133 	 3.0040740966796875e-05 	 4.100799560546875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-30 11:33:41.281831 test begin: paddle.unfold(Tensor([50, 20321281],"float16"), 0, 5, 1, )
[Prof] paddle.unfold 	 paddle.unfold(Tensor([50, 20321281],"float16"), 0, 5, 1, ) 	 1016064050 	 1000 	 0.016086816787719727 	 0.004323005676269531 	 1.049041748046875e-05 	 1.9550323486328125e-05 	 41.1663384437561 	 40.9913227558136 	 41.1125168800354 	 13.980783939361572 	 
2025-07-30 11:36:54.092093 test begin: paddle.unsqueeze(Tensor([250, 1024, 1024],"int64"), 1, )
Warning: The core code of paddle.unsqueeze is too complex.
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([250, 1024, 1024],"int64"), 1, ) 	 262144000 	 1000 	 0.004061698913574219 	 0.005930662155151367 	 1.0967254638671875e-05 	 2.384185791015625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:37:02.619214 test begin: paddle.unsqueeze(Tensor([39700, 50, 256],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([39700, 50, 256],"float32"), axis=2, ) 	 508160000 	 1000 	 0.00413060188293457 	 0.0037086009979248047 	 8.106231689453125e-06 	 1.9788742065429688e-05 	 0.04012894630432129 	 0.07462382316589355 	 5.054473876953125e-05 	 0.00010371208190917969 	 
2025-07-30 11:37:19.685602 test begin: paddle.unsqueeze(Tensor([40, 1024, 6202],"int64"), 1, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([40, 1024, 6202],"int64"), 1, ) 	 254033920 	 1000 	 0.003930807113647461 	 0.003713369369506836 	 1.049041748046875e-05 	 1.8596649169921875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:37:28.017534 test begin: paddle.unsqueeze(Tensor([40, 6202, 1024],"int64"), 1, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([40, 6202, 1024],"int64"), 1, ) 	 254033920 	 1000 	 0.003901243209838867 	 0.003743410110473633 	 9.298324584960938e-06 	 1.8596649169921875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:37:39.704488 test begin: paddle.unsqueeze(Tensor([4160, 478, 256],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([4160, 478, 256],"float32"), axis=2, ) 	 509050880 	 1000 	 0.0041658878326416016 	 0.0036563873291015625 	 1.1205673217773438e-05 	 1.9550323486328125e-05 	 0.04102492332458496 	 0.0602567195892334 	 9.846687316894531e-05 	 6.341934204101562e-05 	 
2025-07-30 11:37:59.465911 test begin: paddle.unsqueeze(Tensor([4160, 50, 2443],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([4160, 50, 2443],"float32"), axis=2, ) 	 508144000 	 1000 	 0.004196882247924805 	 0.0036334991455078125 	 1.0013580322265625e-05 	 1.8358230590820312e-05 	 0.040059804916381836 	 0.05865311622619629 	 2.765655517578125e-05 	 7.2479248046875e-05 	 
2025-07-30 11:38:16.282501 test begin: paddle.unsqueeze(Tensor([5120, 388, 256],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([5120, 388, 256],"float32"), axis=2, ) 	 508559360 	 1000 	 0.004103899002075195 	 0.0038177967071533203 	 1.2874603271484375e-05 	 2.384185791015625e-05 	 0.04030752182006836 	 0.06503772735595703 	 2.574920654296875e-05 	 6.723403930664062e-05 	 
2025-07-30 11:38:33.455108 test begin: paddle.unsqueeze(Tensor([5120, 50, 1985],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([5120, 50, 1985],"float32"), axis=2, ) 	 508160000 	 1000 	 0.004125356674194336 	 0.0037856101989746094 	 1.0967254638671875e-05 	 2.0742416381835938e-05 	 0.042307138442993164 	 0.06792426109313965 	 5.2928924560546875e-05 	 6.318092346191406e-05 	 
2025-07-30 11:38:50.409875 test begin: paddle.view(Tensor([100, 10, 10, 50804],"float32"), list[-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 10, 10, 50804],"float32"), list[-1,], ) 	 508040000 	 1000 	 0.013463973999023438 	 0.003906965255737305 	 1.52587890625e-05 	 1.9550323486328125e-05 	 0.03998708724975586 	 0.05764293670654297 	 4.458427429199219e-05 	 6.67572021484375e-05 	 
2025-07-30 11:39:10.682359 test begin: paddle.view(Tensor([100, 10, 10, 50804],"float32"), list[10,100,-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 10, 10, 50804],"float32"), list[10,100,-1,], ) 	 508040000 	 1000 	 0.013516426086425781 	 0.0054395198822021484 	 1.5497207641601562e-05 	 5.9604644775390625e-05 	 0.050301551818847656 	 0.07151651382446289 	 3.62396240234375e-05 	 7.772445678710938e-05 	 
2025-07-30 11:39:27.899219 test begin: paddle.view(Tensor([100, 10, 25402, 20],"float32"), list[-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 10, 25402, 20],"float32"), list[-1,], ) 	 508040000 	 1000 	 0.013386726379394531 	 0.004845619201660156 	 1.4543533325195312e-05 	 6.580352783203125e-05 	 0.0395357608795166 	 0.07583904266357422 	 2.0503997802734375e-05 	 8.487701416015625e-05 	 
2025-07-30 11:39:44.822024 test begin: paddle.view(Tensor([100, 10, 25402, 20],"float32"), list[10,100,-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 10, 25402, 20],"float32"), list[10,100,-1,], ) 	 508040000 	 1000 	 0.013650178909301758 	 0.004006862640380859 	 1.9073486328125e-05 	 2.002716064453125e-05 	 0.03975081443786621 	 0.05759310722351074 	 4.029273986816406e-05 	 6.198883056640625e-05 	 
2025-07-30 11:40:04.440080 test begin: paddle.view(Tensor([100, 25402, 10, 20],"float32"), list[-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 25402, 10, 20],"float32"), list[-1,], ) 	 508040000 	 1000 	 0.02314138412475586 	 0.003919839859008789 	 3.933906555175781e-05 	 1.7881393432617188e-05 	 0.03961539268493652 	 0.0720679759979248 	 4.1484832763671875e-05 	 5.078315734863281e-05 	 
2025-07-30 11:40:21.183212 test begin: paddle.view(Tensor([100, 25402, 10, 20],"float32"), list[10,100,-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 25402, 10, 20],"float32"), list[10,100,-1,], ) 	 508040000 	 1000 	 0.013538360595703125 	 0.0039408206939697266 	 1.0967254638671875e-05 	 1.9550323486328125e-05 	 0.03955388069152832 	 0.05728936195373535 	 4.291534423828125e-05 	 5.9604644775390625e-05 	 
2025-07-30 11:40:41.036279 test begin: paddle.view(Tensor([254020, 10, 10, 20],"float32"), list[-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([254020, 10, 10, 20],"float32"), list[-1,], ) 	 508040000 	 1000 	 0.013205766677856445 	 0.00394129753112793 	 1.621246337890625e-05 	 2.09808349609375e-05 	 0.0396726131439209 	 0.05782008171081543 	 2.4318695068359375e-05 	 7.295608520507812e-05 	 
2025-07-30 11:40:57.588727 test begin: paddle.view(Tensor([254020, 10, 10, 20],"float32"), list[10,100,-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([254020, 10, 10, 20],"float32"), list[10,100,-1,], ) 	 508040000 	 1000 	 0.013565540313720703 	 0.006161689758300781 	 9.775161743164062e-06 	 2.4080276489257812e-05 	 0.03986930847167969 	 0.06298112869262695 	 2.574920654296875e-05 	 8.0108642578125e-05 	 
2025-07-30 11:41:14.467846 test begin: paddle.view_as(Tensor([10, 10, 10, 50804],"float32"), Tensor([10, 100, 50804],"float32"), )
[Prof] paddle.view_as 	 paddle.view_as(Tensor([10, 10, 10, 50804],"float32"), Tensor([10, 100, 50804],"float32"), ) 	 101608000 	 1000 	 0.014073848724365234 	 0.0034363269805908203 	 8.106231689453125e-06 	 1.7881393432617188e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-30 11:41:17.065756 test begin: paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[-1,1,3,], )
W0730 11:41:27.684370 44708 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[-1,1,3,], ) 	 254016120 	 1000 	 0.030926942825317383 	 0.011052608489990234 	 1.9311904907226562e-05 	 6.890296936035156e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:41:29.400805 test begin: paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[-1,], )
W0730 11:41:38.886750 45934 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[-1,], ) 	 254016120 	 1000 	 0.016782760620117188 	 0.006846904754638672 	 1.2874603271484375e-05 	 2.5272369384765625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:41:40.283616 test begin: paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[2,4,], )
W0730 11:41:47.835673 46846 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[2,4,], ) 	 254016120 	 1000 	 0.023525476455688477 	 0.00787806510925293 	 1.8358230590820312e-05 	 2.1457672119140625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:41:49.284125 test begin: paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[-1,1,3,], )
W0730 11:41:59.645287 47377 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[-1,1,3,], ) 	 254016180 	 1000 	 0.0304563045501709 	 0.009246826171875 	 1.4781951904296875e-05 	 4.458427429199219e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:42:01.513627 test begin: paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[-1,], )
W0730 11:42:08.718657 48471 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[-1,], ) 	 254016180 	 1000 	 0.020529747009277344 	 0.00678253173828125 	 2.956390380859375e-05 	 2.1696090698242188e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:42:09.822442 test begin: paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[2,4,], )
W0730 11:42:16.901365 49268 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[2,4,], ) 	 254016180 	 1000 	 0.023578643798828125 	 0.007961034774780273 	 2.4318695068359375e-05 	 2.2649765014648438e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:42:18.992287 test begin: paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[-1,1,3,], )
W0730 11:42:29.441498 50199 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[-1,1,3,], ) 	 254016240 	 1000 	 0.05341601371765137 	 0.00917363166809082 	 4.4345855712890625e-05 	 2.3603439331054688e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:42:31.325106 test begin: paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[-1,], )
W0730 11:42:38.888161 51487 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[-1,], ) 	 254016240 	 1000 	 0.016759872436523438 	 0.006784915924072266 	 4.124641418457031e-05 	 2.1219253540039062e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-30 11:42:41.312356 test begin: paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[2,4,], )
W0730 11:42:50.079522 52249 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[2,4,], ) 	 254016240 	 1000 	 0.023488521575927734 	 0.010287046432495117 	 1.239776611328125e-05 	 0.0001709461212158203 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 9, in <module>
    from tester import (APIConfig, APITestAccuracy, APITestCINNVSDygraph,
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/__init__.py", line 74, in __getattr__
    from .api_config import APIConfig
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/__init__.py", line 22, in __getattr__
    from .config_analyzer import APIConfig
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py", line 8, in <module>
    import paddle
  File "/usr/local/lib/python3.10/dist-packages/paddle/__init__.py", line 38, in <module>
    from .base import core  # noqa: F401
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/__init__.py", line 205, in <module>
    __bootstrap__()
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/__init__.py", line 197, in __bootstrap__
    core.init_devices()
KeyboardInterrupt
