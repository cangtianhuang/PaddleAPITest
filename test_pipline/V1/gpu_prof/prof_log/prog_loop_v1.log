2025-08-04 11:30:37.653992 test begin: paddle.abs(Tensor([13, 64, 256, 256],"float32"), )
W0804 11:30:39.682348 84179 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.abs 	 paddle.abs(Tensor([13, 64, 256, 256],"float32"), ) 	 54525952 	 33746 	 10.735576152801514 	 10.77926230430603 	 0.3255176544189453 	 0.3260047435760498 	 16.297959327697754 	 26.889688730239868 	 0.4934394359588623 	 0.4071817398071289 	 
2025-08-04 11:31:47.767631 test begin: paddle.abs(Tensor([16, 128, 128, 194],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 128, 128, 194],"float32"), ) 	 50855936 	 33746 	 10.003274917602539 	 10.057707786560059 	 0.3028831481933594 	 0.3045194149017334 	 15.207105875015259 	 25.092073678970337 	 0.4604370594024658 	 0.3799006938934326 	 
2025-08-04 11:32:50.546774 test begin: paddle.abs(Tensor([16, 128, 194, 128],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 128, 194, 128],"float32"), ) 	 50855936 	 33746 	 10.010719299316406 	 10.054262399673462 	 0.30286741256713867 	 0.3044705390930176 	 15.206023216247559 	 25.092692852020264 	 0.4605100154876709 	 0.3799276351928711 	 
2025-08-04 11:33:54.868768 test begin: paddle.abs(Tensor([16, 194, 128, 128],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 194, 128, 128],"float32"), ) 	 50855936 	 33746 	 10.012916088104248 	 10.054121494293213 	 0.3029050827026367 	 0.30450987815856934 	 15.206646919250488 	 25.092597484588623 	 0.46041226387023926 	 0.37993335723876953 	 
2025-08-04 11:34:57.118250 test begin: paddle.abs(Tensor([16, 256, 194, 64],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 256, 194, 64],"float32"), ) 	 50855936 	 33746 	 10.001985311508179 	 10.05429744720459 	 0.30294179916381836 	 0.30449748039245605 	 15.20771312713623 	 25.093130111694336 	 0.4605894088745117 	 0.3799779415130615 	 
2025-08-04 11:36:00.047686 test begin: paddle.abs(Tensor([16, 256, 64, 194],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 256, 64, 194],"float32"), ) 	 50855936 	 33746 	 10.00081753730774 	 10.05411171913147 	 0.30288267135620117 	 0.3044114112854004 	 15.206597566604614 	 25.09307289123535 	 0.46064114570617676 	 0.37997865676879883 	 
2025-08-04 11:37:03.258753 test begin: paddle.abs(Tensor([16, 49, 256, 256],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 49, 256, 256],"float32"), ) 	 51380224 	 33746 	 10.100699186325073 	 11.848993062973022 	 0.30589747428894043 	 0.3074829578399658 	 15.361481428146362 	 25.35187292098999 	 0.4652426242828369 	 0.38384485244750977 	 
2025-08-04 11:38:09.117947 test begin: paddle.abs(Tensor([16, 64, 194, 256],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 64, 194, 256],"float32"), ) 	 50855936 	 33746 	 10.000399351119995 	 10.606385707855225 	 0.30287814140319824 	 0.3044769763946533 	 15.20666217803955 	 25.092394590377808 	 0.4605698585510254 	 0.37996721267700195 	 
2025-08-04 11:39:13.005695 test begin: paddle.abs(Tensor([16, 64, 256, 194],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 64, 256, 194],"float32"), ) 	 50855936 	 33746 	 10.004122495651245 	 11.570393800735474 	 0.30287790298461914 	 0.3045156002044678 	 15.205944061279297 	 25.09421968460083 	 0.4605245590209961 	 0.37996482849121094 	 
2025-08-04 11:40:18.999374 test begin: paddle.abs(Tensor([16, 776, 64, 64],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 776, 64, 64],"float32"), ) 	 50855936 	 33746 	 10.000803709030151 	 10.062169313430786 	 0.30287957191467285 	 0.3044924736022949 	 15.209123849868774 	 25.09441113471985 	 0.46190857887268066 	 0.380047082901001 	 
2025-08-04 11:41:21.172491 test begin: paddle.abs(Tensor([25, 128, 128, 128],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([25, 128, 128, 128],"float32"), ) 	 52428800 	 33746 	 10.314676523208618 	 10.360645294189453 	 0.3121500015258789 	 0.3137803077697754 	 15.66753625869751 	 25.866894960403442 	 0.4743075370788574 	 0.39171600341796875 	 
2025-08-04 11:42:25.190913 test begin: paddle.abs(Tensor([49, 256, 64, 64],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([49, 256, 64, 64],"float32"), ) 	 51380224 	 33746 	 10.102842330932617 	 10.154197216033936 	 0.3059225082397461 	 0.30752134323120117 	 15.358829975128174 	 25.351253032684326 	 0.4649534225463867 	 0.383847713470459 	 
2025-08-04 11:43:28.284282 test begin: paddle.acos(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.acos 	 paddle.acos(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33801 	 9.992156028747559 	 10.057329177856445 	 0.3020801544189453 	 0.3040759563446045 	 15.216862201690674 	 70.33370208740234 	 0.46015405654907227 	 0.3544955253601074 	 
2025-08-04 11:45:15.798150 test begin: paddle.acos(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.acos 	 paddle.acos(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33801 	 9.997857093811035 	 10.080507040023804 	 0.3021824359893799 	 0.3040752410888672 	 15.216067552566528 	 70.33465719223022 	 0.45995450019836426 	 0.3544762134552002 	 
2025-08-04 11:47:04.761539 test begin: paddle.acos(Tensor([10, 5080321],"float32"), )
[Prof] paddle.acos 	 paddle.acos(Tensor([10, 5080321],"float32"), ) 	 50803210 	 33801 	 9.994253158569336 	 10.068382024765015 	 0.30218076705932617 	 0.3040633201599121 	 15.21687626838684 	 70.33535599708557 	 0.4601001739501953 	 0.35444045066833496 	 
2025-08-04 11:48:54.143009 test begin: paddle.acos(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.acos 	 paddle.acos(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33801 	 9.993921279907227 	 10.805599689483643 	 0.3021814823150635 	 0.3040652275085449 	 15.216723203659058 	 70.33479022979736 	 0.46009016036987305 	 0.3545083999633789 	 
2025-08-04 11:50:43.734911 test begin: paddle.acos(Tensor([5080321, 10],"float32"), )
[Prof] paddle.acos 	 paddle.acos(Tensor([5080321, 10],"float32"), ) 	 50803210 	 33801 	 10.013972997665405 	 11.192710638046265 	 0.30217838287353516 	 0.304124116897583 	 15.216200828552246 	 70.33368682861328 	 0.4600491523742676 	 0.35444092750549316 	 
2025-08-04 11:52:34.272079 test begin: paddle.acos(x=Tensor([3, 3, 5644801],"float32"), )
[Prof] paddle.acos 	 paddle.acos(x=Tensor([3, 3, 5644801],"float32"), ) 	 50803209 	 33801 	 9.99427580833435 	 10.057450771331787 	 0.3021984100341797 	 0.3040900230407715 	 15.215391635894775 	 70.50250101089478 	 0.46005845069885254 	 0.35441040992736816 	 
2025-08-04 11:54:21.829096 test begin: paddle.acos(x=Tensor([3, 5644801, 3],"float32"), )
[Prof] paddle.acos 	 paddle.acos(x=Tensor([3, 5644801, 3],"float32"), ) 	 50803209 	 33801 	 9.992674827575684 	 10.057442903518677 	 0.30211853981018066 	 0.304093599319458 	 15.217540740966797 	 70.3340847492218 	 0.4600992202758789 	 0.35456156730651855 	 
2025-08-04 11:56:09.278801 test begin: paddle.acos(x=Tensor([5644801, 3, 3],"float32"), )
[Prof] paddle.acos 	 paddle.acos(x=Tensor([5644801, 3, 3],"float32"), ) 	 50803209 	 33801 	 9.995652198791504 	 10.072230815887451 	 0.3022491931915283 	 0.30405688285827637 	 15.216331720352173 	 70.33420395851135 	 0.460085391998291 	 0.3544425964355469 	 
2025-08-04 11:57:59.346583 test begin: paddle.acosh(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.acosh 	 paddle.acosh(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33859 	 9.994768857955933 	 10.11755633354187 	 0.30167627334594727 	 0.30536842346191406 	 15.29837679862976 	 45.309306621551514 	 0.46178174018859863 	 0.3419463634490967 	 
2025-08-04 11:59:21.873564 test begin: paddle.acosh(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.acosh 	 paddle.acosh(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33859 	 9.99617075920105 	 10.126352071762085 	 0.3017396926879883 	 0.30536937713623047 	 15.295136213302612 	 45.30808210372925 	 0.46165966987609863 	 0.34198999404907227 	 
2025-08-04 12:00:45.421440 test begin: paddle.acosh(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.acosh 	 paddle.acosh(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33859 	 9.996200323104858 	 10.124100923538208 	 0.30171632766723633 	 0.305417537689209 	 15.293535709381104 	 45.3077495098114 	 0.4614098072052002 	 0.3419153690338135 	 
2025-08-04 12:02:11.163359 test begin: paddle.add(x=Tensor([2, 256, 320, 352],"float32"), y=Tensor([2, 256, 320, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([2, 256, 320, 352],"float32"), y=Tensor([2, 256, 320, 352],"float32"), ) 	 115343360 	 22147 	 11.308561563491821 	 11.225056409835815 	 0.5218620300292969 	 0.517383337020874 	 12.131494283676147 	 1.3339855670928955 	 0.5598165988922119 	 0.0001952648162841797 	 
2025-08-04 12:02:51.194338 test begin: paddle.add(x=Tensor([2, 256, 336, 336],"float32"), y=Tensor([2, 256, 336, 336],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([2, 256, 336, 336],"float32"), y=Tensor([2, 256, 336, 336],"float32"), ) 	 115605504 	 22147 	 11.334696769714355 	 11.245041131973267 	 0.5230042934417725 	 0.5185799598693848 	 12.15491247177124 	 1.5811996459960938 	 0.5608222484588623 	 0.00011301040649414062 	 
2025-08-04 12:03:31.311768 test begin: paddle.add(x=Tensor([2, 256, 352, 352],"float32"), y=Tensor([2, 256, 352, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([2, 256, 352, 352],"float32"), y=Tensor([2, 256, 352, 352],"float32"), ) 	 126877696 	 22147 	 12.425584554672241 	 12.323143243789673 	 0.5734374523162842 	 0.5686602592468262 	 13.337849140167236 	 1.3785765171051025 	 0.6154742240905762 	 0.00010061264038085938 	 
2025-08-04 12:04:14.054086 test begin: paddle.add(x=Tensor([8, 256, 320, 78],"float32"), y=Tensor([8, 256, 320, 78],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 320, 78],"float32"), y=Tensor([8, 256, 320, 78],"float32"), ) 	 102236160 	 22147 	 10.034303903579712 	 9.951010704040527 	 0.4631011486053467 	 0.4590139389038086 	 10.753782033920288 	 1.4633679389953613 	 0.49633073806762695 	 0.00028967857360839844 	 
2025-08-04 12:04:49.589827 test begin: paddle.add(x=Tensor([8, 256, 336, 74],"float32"), y=Tensor([8, 256, 336, 74],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 336, 74],"float32"), y=Tensor([8, 256, 336, 74],"float32"), ) 	 101842944 	 22147 	 9.99555492401123 	 9.911291122436523 	 0.4612584114074707 	 0.4573061466217041 	 10.713130474090576 	 1.2514793872833252 	 0.4943268299102783 	 0.000186920166015625 	 
2025-08-04 12:05:24.137524 test begin: paddle.add(x=Tensor([8, 256, 352, 71],"float32"), y=Tensor([8, 256, 352, 71],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 352, 71],"float32"), y=Tensor([8, 256, 352, 71],"float32"), ) 	 102367232 	 22147 	 10.046480655670166 	 9.96093463897705 	 0.4636542797088623 	 0.4596707820892334 	 10.770819425582886 	 1.2435665130615234 	 0.49694371223449707 	 0.0001227855682373047 	 
2025-08-04 12:05:58.850627 test begin: paddle.add(x=Tensor([8, 256, 71, 352],"float32"), y=Tensor([8, 256, 71, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 71, 352],"float32"), y=Tensor([8, 256, 71, 352],"float32"), ) 	 102367232 	 22147 	 10.04569411277771 	 9.960898637771606 	 0.4636528491973877 	 0.45969676971435547 	 10.768434524536133 	 1.2446963787078857 	 0.49695754051208496 	 7.939338684082031e-05 	 
2025-08-04 12:06:33.535514 test begin: paddle.add(x=Tensor([8, 256, 74, 336],"float32"), y=Tensor([8, 256, 74, 336],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 74, 336],"float32"), y=Tensor([8, 256, 74, 336],"float32"), ) 	 101842944 	 22147 	 9.99907898902893 	 9.910265684127808 	 0.4612700939178467 	 0.4573836326599121 	 10.713867664337158 	 1.2225689888000488 	 0.4943656921386719 	 0.00018024444580078125 	 
2025-08-04 12:07:09.458908 test begin: paddle.add(x=Tensor([8, 52, 352, 352],"float32"), y=Tensor([8, 52, 352, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 52, 352, 352],"float32"), y=Tensor([8, 52, 352, 352],"float32"), ) 	 103088128 	 22147 	 10.122229814529419 	 10.030279397964478 	 0.4669785499572754 	 0.46283984184265137 	 10.845114707946777 	 1.267482042312622 	 0.5004620552062988 	 0.00019407272338867188 	 
2025-08-04 12:07:44.505672 test begin: paddle.add(x=Tensor([8, 57, 320, 352],"float32"), y=Tensor([8, 57, 320, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 57, 320, 352],"float32"), y=Tensor([8, 57, 320, 352],"float32"), ) 	 102727680 	 22147 	 10.08675503730774 	 9.996072053909302 	 0.4653773307800293 	 0.4613006114959717 	 10.805535078048706 	 1.239537239074707 	 0.498685359954834 	 8.130073547363281e-05 	 
2025-08-04 12:08:21.231635 test begin: paddle.add(x=Tensor([8, 57, 336, 336],"float32"), y=Tensor([8, 57, 336, 336],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 57, 336, 336],"float32"), y=Tensor([8, 57, 336, 336],"float32"), ) 	 102961152 	 22147 	 10.104718685150146 	 10.036760807037354 	 0.46623945236206055 	 0.4623897075653076 	 10.834116458892822 	 1.2356624603271484 	 0.49996066093444824 	 8.177757263183594e-05 	 
2025-08-04 12:08:56.133302 test begin: paddle.add_n(list[Tensor([194, 128, 64, 64],"float16"),Tensor([194, 128, 64, 64],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([194, 128, 64, 64],"float16"),Tensor([194, 128, 64, 64],"float16"),], ) 	 203423744 	 21148 	 12.103009939193726 	 32.10394597053528 	 0.584768533706665 	 0.775684118270874 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:09:44.305391 test begin: paddle.add_n(list[Tensor([388, 256, 32, 32],"float16"),Tensor([388, 256, 32, 32],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([388, 256, 32, 32],"float16"),Tensor([388, 256, 32, 32],"float16"),], ) 	 203423744 	 21148 	 12.08081841468811 	 32.117640256881714 	 0.5837564468383789 	 0.7760453224182129 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:10:32.502093 test begin: paddle.add_n(list[Tensor([64, 128, 194, 64],"float16"),Tensor([64, 128, 194, 64],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 128, 194, 64],"float16"),Tensor([64, 128, 194, 64],"float16"),], ) 	 203423744 	 21148 	 12.09017539024353 	 32.12597632408142 	 0.5838503837585449 	 0.7757730484008789 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:11:26.071952 test begin: paddle.add_n(list[Tensor([64, 128, 64, 194],"float16"),Tensor([64, 128, 64, 194],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 128, 64, 194],"float16"),Tensor([64, 128, 64, 194],"float16"),], ) 	 203423744 	 21148 	 12.094644784927368 	 32.12373495101929 	 0.5838422775268555 	 0.7760112285614014 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:12:15.345500 test begin: paddle.add_n(list[Tensor([64, 128, 64, 97],"float32"),Tensor([64, 128, 64, 97],"float32"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 128, 64, 97],"float32"),Tensor([64, 128, 64, 97],"float32"),], ) 	 101711872 	 21148 	 10.011915922164917 	 22.393555164337158 	 0.483776330947876 	 0.5410294532775879 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:12:49.527564 test begin: paddle.add_n(list[Tensor([64, 128, 97, 64],"float32"),Tensor([64, 128, 97, 64],"float32"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 128, 97, 64],"float32"),Tensor([64, 128, 97, 64],"float32"),], ) 	 101711872 	 21148 	 10.003319263458252 	 22.39313054084778 	 0.4833414554595947 	 0.5412256717681885 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:13:23.748200 test begin: paddle.add_n(list[Tensor([64, 1551, 32, 32],"float16"),Tensor([64, 1551, 32, 32],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 1551, 32, 32],"float16"),Tensor([64, 1551, 32, 32],"float16"),], ) 	 203292672 	 21148 	 12.115783214569092 	 32.12851643562317 	 0.5854835510253906 	 0.7755370140075684 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:14:14.679909 test begin: paddle.add_n(list[Tensor([64, 194, 64, 64],"float32"),Tensor([64, 194, 64, 64],"float32"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 194, 64, 64],"float32"),Tensor([64, 194, 64, 64],"float32"),], ) 	 101711872 	 21148 	 10.00364375114441 	 22.393959045410156 	 0.48342323303222656 	 0.5412461757659912 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:14:48.859908 test begin: paddle.add_n(list[Tensor([64, 256, 194, 32],"float16"),Tensor([64, 256, 194, 32],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 256, 194, 32],"float16"),Tensor([64, 256, 194, 32],"float16"),], ) 	 203423744 	 21148 	 12.107897281646729 	 32.14021825790405 	 0.5846688747406006 	 0.7764508724212646 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:15:39.807321 test begin: paddle.add_n(list[Tensor([64, 256, 32, 194],"float16"),Tensor([64, 256, 32, 194],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 256, 32, 194],"float16"),Tensor([64, 256, 32, 194],"float16"),], ) 	 203423744 	 21148 	 12.089237928390503 	 32.115556955337524 	 0.583827018737793 	 0.7760517597198486 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:16:28.749326 test begin: paddle.add_n(list[Tensor([64, 388, 64, 64],"float16"),Tensor([64, 388, 64, 64],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 388, 64, 64],"float16"),Tensor([64, 388, 64, 64],"float16"),], ) 	 203423744 	 21148 	 12.101186513900757 	 32.1259651184082 	 0.5848605632781982 	 0.7763168811798096 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:17:20.569467 test begin: paddle.add_n(list[Tensor([97, 128, 64, 64],"float32"),Tensor([97, 128, 64, 64],"float32"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([97, 128, 64, 64],"float32"),Tensor([97, 128, 64, 64],"float32"),], ) 	 101711872 	 21148 	 10.003297328948975 	 22.394533395767212 	 0.4834713935852051 	 0.5412437915802002 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:17:54.742088 test begin: paddle.addmm(Tensor([1016065, 50],"float32"), Tensor([1016065, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([1016065, 50],"float32"), Tensor([1016065, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, ) 	 132092450 	 31761 	 32.50281572341919 	 31.591243028640747 	 0.5230846405029297 	 0.33837342262268066 	 85.0213532447815 	 60.370424032211304 	 0.3908665180206299 	 0.4849123954772949 	 
2025-08-04 12:21:27.678382 test begin: paddle.addmm(Tensor([30, 1693441],"float32"), Tensor([30, 80],"float32"), Tensor([80, 1693441],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([30, 1693441],"float32"), Tensor([30, 80],"float32"), Tensor([80, 1693441],"float32"), alpha=1.0, beta=2.0, ) 	 186280910 	 31761 	 37.26361298561096 	 36.53408718109131 	 0.39968037605285645 	 0.2941727638244629 	 102.30671906471252 	 67.23366022109985 	 0.41163158416748047 	 0.4312319755554199 	 
2025-08-04 12:25:35.114784 test begin: paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 1016065],"float32"), Tensor([1016065, 50],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 1016065],"float32"), Tensor([1016065, 50],"float32"), alpha=1.0, beta=2.0, ) 	 81286700 	 31761 	 10.127842664718628 	 9.970971822738647 	 0.10869693756103516 	 0.10788083076477051 	 34.737713098526 	 20.191377878189087 	 0.1865074634552002 	 0.2159280776977539 	 
2025-08-04 12:26:53.213755 test begin: paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 1693441],"float32"), Tensor([1693441, 50],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 1693441],"float32"), Tensor([1693441, 50],"float32"), alpha=1.0, beta=2.0, ) 	 135476780 	 31761 	 16.10228967666626 	 16.078197240829468 	 0.17322659492492676 	 0.17219305038452148 	 58.1020131111145 	 33.8300518989563 	 0.23406100273132324 	 0.21771478652954102 	 
2025-08-04 12:29:00.815504 test begin: paddle.addmm(Tensor([30, 635041],"float32"), Tensor([30, 80],"float32"), Tensor([80, 635041],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([30, 635041],"float32"), Tensor([30, 80],"float32"), Tensor([80, 635041],"float32"), alpha=1.0, beta=2.0, ) 	 69856910 	 31761 	 13.468268394470215 	 13.291224479675293 	 0.2166731357574463 	 0.14259099960327148 	 38.750988245010376 	 25.25774908065796 	 0.17816972732543945 	 0.20353412628173828 	 
2025-08-04 12:30:33.830394 test begin: paddle.addmm(Tensor([635041, 50],"float32"), Tensor([635041, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([635041, 50],"float32"), Tensor([635041, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, ) 	 82559330 	 31761 	 20.57563042640686 	 20.040045022964478 	 0.330963134765625 	 0.21472501754760742 	 53.735002756118774 	 37.78885817527771 	 0.24700641632080078 	 0.3044450283050537 	 
2025-08-04 12:32:47.962773 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 5080321],"float64"), y=Tensor([5080321, 5],"float64"), )
[Prof] paddle.addmm 	 paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 5080321],"float64"), y=Tensor([5080321, 5],"float64"), ) 	 50803235 	 31761 	 19.74253559112549 	 19.74019145965576 	 0.21145915985107422 	 0.21198463439941406 	 95.48226237297058 	 80.44405102729797 	 0.21946930885314941 	 0.25887179374694824 	 
2025-08-04 12:36:27.234238 test begin: paddle.all(Tensor([423361, 6, 10],"float64"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([423361, 6, 10],"float64"), None, False, None, ) 	 25401660 	 49878 	 10.043522596359253 	 7.501866579055786 	 0.06848740577697754 	 0.07685685157775879 	 None 	 None 	 None 	 None 	 
2025-08-04 12:36:47.231662 test begin: paddle.all(Tensor([5, 508033, 10],"float64"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([5, 508033, 10],"float64"), None, False, None, ) 	 25401650 	 49878 	 10.043512105941772 	 7.495678663253784 	 0.06846904754638672 	 0.0767815113067627 	 None 	 None 	 None 	 None 	 
2025-08-04 12:37:06.152692 test begin: paddle.all(Tensor([5, 6, 846721],"float64"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([5, 6, 846721],"float64"), None, False, None, ) 	 25401630 	 49878 	 10.118643999099731 	 7.495879411697388 	 0.0690145492553711 	 0.07680273056030273 	 None 	 None 	 None 	 None 	 
2025-08-04 12:37:24.713000 test begin: paddle.all(Tensor([50, 1016065, 10],"bool"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([50, 1016065, 10],"bool"), None, False, None, ) 	 508032500 	 49878 	 23.177510023117065 	 25.305711269378662 	 0.23745226860046387 	 0.2593417167663574 	 None 	 None 	 None 	 None 	 
2025-08-04 12:38:20.365479 test begin: paddle.all(Tensor([50, 6, 1693441],"bool"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([50, 6, 1693441],"bool"), None, False, None, ) 	 508032300 	 49878 	 23.176961421966553 	 25.311229944229126 	 0.23745942115783691 	 0.25931763648986816 	 None 	 None 	 None 	 None 	 
2025-08-04 12:39:17.667575 test begin: paddle.all(Tensor([508032010],"bool"), )
[Prof] paddle.all 	 paddle.all(Tensor([508032010],"bool"), ) 	 508032010 	 49878 	 23.32104730606079 	 25.30336046218872 	 0.238877534866333 	 0.25920772552490234 	 None 	 None 	 None 	 None 	 
2025-08-04 12:40:13.359064 test begin: paddle.all(Tensor([8467210, 6, 10],"bool"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([8467210, 6, 10],"bool"), None, False, None, ) 	 508032600 	 49878 	 23.163378477096558 	 25.32195019721985 	 0.237274169921875 	 0.2594730854034424 	 None 	 None 	 None 	 None 	 
2025-08-04 12:41:09.083258 test begin: paddle.allclose(Tensor([1124, 45199],"float32"), Tensor([1124, 45199],"float32"), )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([1124, 45199],"float32"), Tensor([1124, 45199],"float32"), ) 	 101607352 	 9683 	 10.019856214523315 	 33.091521978378296 	 1.0576117038726807 	 0.00027251243591308594 	 None 	 None 	 None 	 None 	 
2025-08-04 12:41:54.101853 test begin: paddle.allclose(Tensor([13, 32, 122124],"float32"), Tensor([13, 32, 122124],"float32"), rtol=0.0001, atol=0.0001, )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([13, 32, 122124],"float32"), Tensor([13, 32, 122124],"float32"), rtol=0.0001, atol=0.0001, ) 	 101607168 	 9683 	 10.154321670532227 	 33.03619408607483 	 1.0719377994537354 	 0.0002536773681640625 	 None 	 None 	 None 	 None 	 
2025-08-04 12:42:40.259845 test begin: paddle.allclose(Tensor([13, 61062, 64],"float32"), Tensor([13, 61062, 64],"float32"), rtol=0.0001, atol=0.0001, )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([13, 61062, 64],"float32"), Tensor([13, 61062, 64],"float32"), rtol=0.0001, atol=0.0001, ) 	 101607168 	 9683 	 10.152381420135498 	 33.04216694831848 	 1.071686029434204 	 0.00026607513427734375 	 None 	 None 	 None 	 None 	 
2025-08-04 12:43:25.345640 test begin: paddle.allclose(Tensor([1587601, 32],"float32"), Tensor([1587601, 32],"float32"), )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([1587601, 32],"float32"), Tensor([1587601, 32],"float32"), ) 	 101606464 	 9683 	 10.25626015663147 	 33.01786422729492 	 1.2265009880065918 	 0.00025844573974609375 	 None 	 None 	 None 	 None 	 
2025-08-04 12:44:16.804226 test begin: paddle.allclose(Tensor([24807, 32, 64],"float32"), Tensor([24807, 32, 64],"float32"), rtol=0.0001, atol=0.0001, )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([24807, 32, 64],"float32"), Tensor([24807, 32, 64],"float32"), rtol=0.0001, atol=0.0001, ) 	 101609472 	 9683 	 10.208371877670288 	 33.01479744911194 	 1.0778436660766602 	 0.0005950927734375 	 None 	 None 	 None 	 None 	 
2025-08-04 12:45:01.912242 test begin: paddle.allclose(Tensor([30522, 1665],"float32"), Tensor([30522, 1665],"float32"), )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([30522, 1665],"float32"), Tensor([30522, 1665],"float32"), ) 	 101638260 	 9683 	 10.018036603927612 	 33.04117441177368 	 1.0575602054595947 	 0.0002562999725341797 	 None 	 None 	 None 	 None 	 
2025-08-04 12:45:46.916700 test begin: paddle.allclose(Tensor([6350401, 8],"float32"), Tensor([6350401, 8],"float32"), )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([6350401, 8],"float32"), Tensor([6350401, 8],"float32"), ) 	 101606416 	 9683 	 10.081504106521606 	 33.07222056388855 	 1.0645344257354736 	 0.00025391578674316406 	 None 	 None 	 None 	 None 	 
2025-08-04 12:46:34.551676 test begin: paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[-1,-2,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 50766 	 11.39082145690918 	 9.035737752914429 	 0.11467218399047852 	 0.09093356132507324 	 56.38850402832031 	 66.57311081886292 	 0.2271561622619629 	 0.22338008880615234 	 
2025-08-04 12:49:01.400577 test begin: paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 50766 	 12.113669157028198 	 10.181726694107056 	 0.12194490432739258 	 0.10247635841369629 	 57.08912467956543 	 68.92255330085754 	 0.23006653785705566 	 0.23118090629577637 	 
2025-08-04 12:51:32.969236 test begin: paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[0,1,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[0,1,], keepdim=False, ) 	 50803300 	 50766 	 10.06646180152893 	 8.125429391860962 	 0.2027132511138916 	 0.1632070541381836 	 55.43346929550171 	 65.46924352645874 	 0.2790689468383789 	 0.26339292526245117 	 
2025-08-04 12:53:53.443679 test begin: paddle.amax(Tensor([10, 508033, 10],"float32"), axis=list[-1,-2,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 508033, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 50766 	 11.385793924331665 	 9.033486604690552 	 0.11460065841674805 	 0.09101581573486328 	 56.38998341560364 	 66.57393789291382 	 0.22715497016906738 	 0.2232511043548584 	 
2025-08-04 12:56:17.768291 test begin: paddle.amax(Tensor([10, 508033, 10],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 508033, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 50766 	 28.988076210021973 	 22.99664306640625 	 0.583587646484375 	 0.46279215812683105 	 66.56892395019531 	 74.09105849266052 	 0.3349885940551758 	 0.29810166358947754 	 
2025-08-04 12:59:34.011734 test begin: paddle.amax(Tensor([10, 508033, 10],"float32"), axis=list[0,1,], keepdim=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f00f2121c00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 13:09:46.067205 test begin: paddle.amax(Tensor([508033, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, )
W0804 13:09:47.063359 121508 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.amax 	 paddle.amax(Tensor([508033, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 50766 	 22.318391799926758 	 14.614517450332642 	 0.44959521293640137 	 0.29421257972717285 	 59.75505781173706 	 66.60303211212158 	 0.30071401596069336 	 0.26797008514404297 	 
2025-08-04 13:12:32.048794 test begin: paddle.amax(Tensor([508033, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([508033, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 50766 	 13.208165884017944 	 10.943939924240112 	 0.13293004035949707 	 0.11011838912963867 	 57.968116760253906 	 72.38013362884521 	 0.23354244232177734 	 0.24259424209594727 	 
2025-08-04 13:15:07.484003 test begin: paddle.amax(Tensor([508033, 10, 10],"float32"), axis=list[0,1,], keepdim=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f10a9002920>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 13:25:14.391919 test begin: paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[-1,-2,], keepdim=False, )
W0804 13:25:15.348266 127012 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 50777 	 11.183739423751831 	 9.030813217163086 	 0.11255908012390137 	 0.0908653736114502 	 55.763822078704834 	 66.60119247436523 	 0.22460293769836426 	 0.2233898639678955 	 
2025-08-04 13:27:38.641231 test begin: paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 50777 	 12.476560115814209 	 10.160765171051025 	 0.12555623054504395 	 0.10224604606628418 	 56.98107385635376 	 68.84580898284912 	 0.22950530052185059 	 0.23081731796264648 	 
2025-08-04 13:30:08.458689 test begin: paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[0,1,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[0,1,], keepdim=False, ) 	 50803300 	 50777 	 10.06412386894226 	 8.105376958847046 	 0.20265674591064453 	 0.1631467342376709 	 55.45574998855591 	 65.47100138664246 	 0.27903246879577637 	 0.2633688449859619 	 
2025-08-04 13:32:28.546564 test begin: paddle.amin(Tensor([10, 508033, 10],"float32"), axis=list[-1,-2,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 508033, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 50777 	 11.760271787643433 	 9.03771424293518 	 0.11835908889770508 	 0.09089446067810059 	 56.3165488243103 	 66.62757015228271 	 0.22679805755615234 	 0.223344087600708 	 
2025-08-04 13:34:54.832190 test begin: paddle.amin(Tensor([10, 508033, 10],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 508033, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 50777 	 28.986849546432495 	 22.98068881034851 	 0.5834202766418457 	 0.4625546932220459 	 66.54807806015015 	 74.08978724479675 	 0.3347630500793457 	 0.2979874610900879 	 
2025-08-04 13:38:08.316282 test begin: paddle.amin(Tensor([10, 508033, 10],"float32"), axis=list[0,1,], keepdim=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6833d68790>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 13:48:18.978751 test begin: paddle.amin(Tensor([508033, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, )
W0804 13:48:19.973361 134145 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.amin 	 paddle.amin(Tensor([508033, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 50777 	 22.331143140792847 	 14.610856533050537 	 0.45091867446899414 	 0.2940552234649658 	 59.74547338485718 	 66.61896896362305 	 0.30058908462524414 	 0.2679765224456787 	 
2025-08-04 13:51:04.794927 test begin: paddle.amin(Tensor([508033, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([508033, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 50777 	 13.199748039245605 	 10.939736127853394 	 0.1328573226928711 	 0.11013412475585938 	 57.45194625854492 	 72.36421585083008 	 0.2314455509185791 	 0.24258089065551758 	 
2025-08-04 13:53:41.647054 test begin: paddle.amin(Tensor([508033, 10, 10],"float32"), axis=list[0,1,], keepdim=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc639857100>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 14:03:47.628550 test begin: paddle.any(Tensor([10, 12404, 4096],"bool"), )
W0804 14:03:54.940712 135468 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.any 	 paddle.any(Tensor([10, 12404, 4096],"bool"), ) 	 508067840 	 21580 	 10.03666377067566 	 11.409694194793701 	 0.23764872550964355 	 0.2701568603515625 	 None 	 None 	 None 	 None 	 
2025-08-04 14:04:16.802245 test begin: paddle.any(Tensor([10, 300, 169345],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([10, 300, 169345],"bool"), ) 	 508035000 	 21580 	 10.03077220916748 	 11.419368982315063 	 0.23754668235778809 	 0.2703831195831299 	 None 	 None 	 None 	 None 	 
2025-08-04 14:04:45.496387 test begin: paddle.any(Tensor([11240, 45199],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([11240, 45199],"bool"), ) 	 508036760 	 21580 	 9.992370367050171 	 11.41309905052185 	 0.23659586906433105 	 0.2702453136444092 	 None 	 None 	 None 	 None 	 
2025-08-04 14:05:14.090710 test begin: paddle.any(Tensor([15876010, 32],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([15876010, 32],"bool"), ) 	 508032320 	 21580 	 10.067177057266235 	 11.420371770858765 	 0.23838257789611816 	 0.2704024314880371 	 None 	 None 	 None 	 None 	 
2025-08-04 14:05:43.750906 test begin: paddle.any(Tensor([420, 300, 4096],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([420, 300, 4096],"bool"), ) 	 516096000 	 21580 	 10.187228918075562 	 11.641247510910034 	 0.24126386642456055 	 0.2756614685058594 	 None 	 None 	 None 	 None 	 
2025-08-04 14:06:13.075972 test begin: paddle.any(Tensor([5120, 99226],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([5120, 99226],"bool"), ) 	 508037120 	 21580 	 10.068766117095947 	 11.389577150344849 	 0.23841452598571777 	 0.26975083351135254 	 None 	 None 	 None 	 None 	 
2025-08-04 14:06:42.827095 test begin: paddle.argmax(Tensor([15877, 100, 32],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([15877, 100, 32],"float32"), axis=1, ) 	 50806400 	 15617 	 11.88297152519226 	 2.8564014434814453 	 0.7776803970336914 	 0.18694758415222168 	 None 	 None 	 None 	 None 	 
2025-08-04 14:06:58.437202 test begin: paddle.argmax(Tensor([29151, 100, 18],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([29151, 100, 18],"float32"), axis=1, ) 	 52471800 	 15617 	 10.005370378494263 	 2.720486879348755 	 0.6547937393188477 	 0.17804765701293945 	 None 	 None 	 None 	 None 	 
2025-08-04 14:07:12.048858 test begin: paddle.argmax(Tensor([29151, 28, 64],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([29151, 28, 64],"float32"), axis=1, ) 	 52238592 	 15617 	 23.464694261550903 	 3.848698616027832 	 1.535552740097046 	 0.18024873733520508 	 None 	 None 	 None 	 None 	 
2025-08-04 14:07:42.515622 test begin: paddle.argmax(Tensor([29151, 55, 32],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([29151, 55, 32],"float32"), axis=1, ) 	 51305760 	 15617 	 11.777733087539673 	 2.6416258811950684 	 0.7707419395446777 	 0.17243599891662598 	 None 	 None 	 None 	 None 	 
2025-08-04 14:08:02.568873 test begin: paddle.argmax(Tensor([39691, 20, 64],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([39691, 20, 64],"float32"), axis=1, ) 	 50804480 	 15617 	 31.92210841178894 	 4.283356666564941 	 2.089022397994995 	 0.17995429039001465 	 None 	 None 	 None 	 None 	 
2025-08-04 14:08:43.627442 test begin: paddle.argmax(Tensor([7939, 100, 64],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([7939, 100, 64],"float32"), axis=1, ) 	 50809600 	 15617 	 11.857992887496948 	 2.817035436630249 	 0.7759873867034912 	 0.18439054489135742 	 None 	 None 	 None 	 None 	 
2025-08-04 14:08:59.208649 test begin: paddle.argmax(Tensor([80239, 10, 64],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([80239, 10, 64],"float32"), axis=1, ) 	 51352960 	 15617 	 64.47417330741882 	 3.0614895820617676 	 4.219426393508911 	 0.2003309726715088 	 None 	 None 	 None 	 None 	 
2025-08-04 14:10:07.690123 test begin: paddle.argmax(Tensor([80239, 20, 32],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([80239, 20, 32],"float32"), axis=1, ) 	 51352960 	 15617 	 32.268484592437744 	 2.7847580909729004 	 2.111722946166992 	 0.18133282661437988 	 None 	 None 	 None 	 None 	 
2025-08-04 14:10:45.078999 test begin: paddle.argmin(Tensor([104534, 3, 3, 3, 3, 3],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([104534, 3, 3, 3, 3, 3],"float64"), axis=0, ) 	 25401762 	 22702 	 10.830355644226074 	 3.6466801166534424 	 0.48743462562561035 	 0.08205342292785645 	 None 	 None 	 None 	 None 	 
2025-08-04 14:11:00.251961 test begin: paddle.argmin(Tensor([203213, 5, 5, 5],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([203213, 5, 5, 5],"float64"), axis=0, ) 	 25401625 	 22702 	 11.035510063171387 	 3.6909139156341553 	 0.4969518184661865 	 0.08308672904968262 	 None 	 None 	 None 	 None 	 
2025-08-04 14:11:16.810899 test begin: paddle.argmin(Tensor([3, 104534, 3, 3, 3, 3],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([3, 104534, 3, 3, 3, 3],"float64"), axis=0, ) 	 25401762 	 22702 	 154.4814031124115 	 4.629893779754639 	 6.954519033432007 	 0.20809102058410645 	 None 	 None 	 None 	 None 	 
2025-08-04 14:13:56.507208 test begin: paddle.argmin(Tensor([3, 3, 104534, 3, 3, 3],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([3, 3, 104534, 3, 3, 3],"float64"), axis=0, ) 	 25401762 	 22702 	 154.48647165298462 	 4.621625185012817 	 6.9548962116241455 	 0.20804500579833984 	 None 	 None 	 None 	 None 	 
2025-08-04 14:16:38.128977 test begin: paddle.argmin(Tensor([3, 3, 3, 104534, 3, 3],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([3, 3, 3, 104534, 3, 3],"float64"), axis=0, ) 	 25401762 	 22702 	 154.49231958389282 	 4.62156867980957 	 6.954755783081055 	 0.20802879333496094 	 None 	 None 	 None 	 None 	 
2025-08-04 14:19:17.970519 test begin: paddle.argmin(Tensor([3, 3, 3, 3, 104534, 3],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([3, 3, 3, 3, 104534, 3],"float64"), axis=0, ) 	 25401762 	 22702 	 154.49632477760315 	 4.627668142318726 	 6.955424070358276 	 0.2080698013305664 	 None 	 None 	 None 	 None 	 
2025-08-04 14:21:57.888966 test begin: paddle.argmin(Tensor([3, 3, 3, 3, 3, 104534],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([3, 3, 3, 3, 3, 104534],"float64"), axis=0, ) 	 25401762 	 22702 	 154.49378848075867 	 4.625202178955078 	 6.954745054244995 	 0.20807981491088867 	 None 	 None 	 None 	 None 	 
2025-08-04 14:24:39.281673 test begin: paddle.argmin(Tensor([4, 4, 4, 4, 99226],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([4, 4, 4, 4, 99226],"float64"), axis=0, ) 	 25401856 	 22702 	 116.20793199539185 	 4.343453407287598 	 5.5394721031188965 	 0.1952192783355713 	 None 	 None 	 None 	 None 	 
2025-08-04 14:26:42.941207 test begin: paddle.argmin(Tensor([4, 4, 4, 99226, 4],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([4, 4, 4, 99226, 4],"float64"), axis=0, ) 	 25401856 	 22702 	 115.88400197029114 	 5.580654859542847 	 5.216883897781372 	 0.19517159461975098 	 None 	 None 	 None 	 None 	 
2025-08-04 14:28:46.944430 test begin: paddle.argmin(Tensor([4, 4, 99226, 4, 4],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([4, 4, 99226, 4, 4],"float64"), axis=0, ) 	 25401856 	 22702 	 115.88444995880127 	 4.337061166763306 	 5.216861963272095 	 0.1951615810394287 	 None 	 None 	 None 	 None 	 
2025-08-04 14:30:47.726901 test begin: paddle.argmin(Tensor([4, 99226, 4, 4, 4],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([4, 99226, 4, 4, 4],"float64"), axis=0, ) 	 25401856 	 22702 	 115.88387060165405 	 4.341317653656006 	 5.217164516448975 	 0.195173978805542 	 None 	 None 	 None 	 None 	 
2025-08-04 14:32:48.524786 test begin: paddle.argmin(Tensor([5, 203213, 5, 5],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([5, 203213, 5, 5],"float64"), axis=0, ) 	 25401625 	 22702 	 92.72622299194336 	 4.453725099563599 	 4.174389362335205 	 0.20052766799926758 	 None 	 None 	 None 	 None 	 
2025-08-04 14:34:26.272596 test begin: paddle.argmin(Tensor([5, 5, 203213, 5],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([5, 5, 203213, 5],"float64"), axis=0, ) 	 25401625 	 22702 	 92.72533702850342 	 5.258487224578857 	 4.174264192581177 	 0.20053529739379883 	 None 	 None 	 None 	 None 	 
2025-08-04 14:36:05.484814 test begin: paddle.argmin(Tensor([5, 5, 5, 203213],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([5, 5, 5, 203213],"float64"), axis=0, ) 	 25401625 	 22702 	 92.72398924827576 	 5.577431917190552 	 4.174251317977905 	 0.20036530494689941 	 None 	 None 	 None 	 None 	 
2025-08-04 14:37:45.422613 test begin: paddle.argmin(Tensor([99226, 4, 4, 4, 4],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([99226, 4, 4, 4, 4],"float64"), axis=0, ) 	 25401856 	 22702 	 10.001210451126099 	 7.33552885055542 	 0.4503815174102783 	 0.16474199295043945 	 None 	 None 	 None 	 None 	 
2025-08-04 14:38:03.319652 test begin: paddle.argsort(Tensor([25401601],"float64"), stable=True, )
[Prof] paddle.argsort 	 paddle.argsort(Tensor([25401601],"float64"), stable=True, ) 	 25401601 	 1000 	 15.428302764892578 	 7.489705324172974 	 0.00010752677917480469 	 0.3337521553039551 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 14:38:32.041242 test begin: paddle.argsort(Tensor([50803201],"float32"), stable=True, )
[Prof] paddle.argsort 	 paddle.argsort(Tensor([50803201],"float32"), stable=True, ) 	 50803201 	 1000 	 22.252958059310913 	 7.856600522994995 	 0.00011348724365234375 	 0.5358750820159912 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 14:39:12.524622 test begin: paddle.argsort(Tensor([50803201],"int32"), stable=True, )
[Prof] paddle.argsort 	 paddle.argsort(Tensor([50803201],"int32"), stable=True, ) 	 50803201 	 1000 	 21.013439655303955 	 7.2354512214660645 	 0.00012350082397460938 	 0.4919276237487793 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 14:39:50.920828 test begin: paddle.as_complex(Tensor([320, 15, 207, 8, 32, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([320, 15, 207, 8, 32, 2],"float32"), ) 	 508723200 	 3339679 	 11.258237361907959 	 14.902983903884888 	 7.390975952148438e-05 	 8.463859558105469e-05 	 137.16371941566467 	 184.2524356842041 	 0.0001437664031982422 	 0.0002422332763671875 	 
2025-08-04 14:45:53.323829 test begin: paddle.as_complex(Tensor([320, 15, 8, 207, 32, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([320, 15, 8, 207, 32, 2],"float32"), ) 	 508723200 	 3339679 	 11.24193549156189 	 14.826859474182129 	 0.00011324882507324219 	 8.20159912109375e-05 	 138.39780449867249 	 185.05673098564148 	 0.00011968612670898438 	 0.0002300739288330078 	 
2025-08-04 14:51:57.457613 test begin: paddle.as_complex(Tensor([320, 15, 8, 8, 827, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([320, 15, 8, 8, 827, 2],"float32"), ) 	 508108800 	 3339679 	 11.525975465774536 	 14.939823865890503 	 0.00012183189392089844 	 0.0002799034118652344 	 139.20767259597778 	 183.92338681221008 	 0.00011730194091796875 	 0.00023412704467773438 	 
2025-08-04 14:58:02.674615 test begin: paddle.as_complex(Tensor([320, 388, 8, 8, 32, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([320, 388, 8, 8, 32, 2],"float32"), ) 	 508559360 	 3339679 	 11.353806734085083 	 14.97760009765625 	 0.0001380443572998047 	 0.0001595020294189453 	 140.62067985534668 	 185.20540356636047 	 0.0001087188720703125 	 0.00024628639221191406 	 
2025-08-04 15:04:09.617728 test begin: paddle.as_complex(Tensor([8270, 15, 8, 8, 32, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([8270, 15, 8, 8, 32, 2],"float32"), ) 	 508108800 	 3339679 	 11.303344011306763 	 15.001689434051514 	 0.00011777877807617188 	 0.00017142295837402344 	 140.65171909332275 	 184.40171670913696 	 0.00011682510375976562 	 0.00022983551025390625 	 
2025-08-04 15:10:15.154140 test begin: paddle.as_strided(Tensor([15876010, 32],"float32"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([15876010, 32],"float32"), shape=tuple(3,), stride=tuple(1,), ) 	 508032320 	 199910 	 3.5843143463134766 	 0.8698234558105469 	 0.00010657310485839844 	 8.130073547363281e-05 	 300.34756445884705 	 262.8459746837616 	 0.7669212818145752 	 0.6714160442352295 	 
2025-08-04 15:19:51.577440 test begin: paddle.as_strided(Tensor([31752010, 32],"float16"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([31752010, 32],"float16"), shape=tuple(3,), stride=tuple(1,), ) 	 1016064320 	 199910 	 3.4883008003234863 	 0.9071633815765381 	 0.00012922286987304688 	 0.00011491775512695312 	 302.951509475708 	 262.9647798538208 	 0.7744777202606201 	 0.6716156005859375 	 
2025-08-04 15:29:46.026447 test begin: paddle.as_strided(Tensor([31752010, 32],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([31752010, 32],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), ) 	 1016064320 	 199910 	 3.523907423019409 	 0.9365952014923096 	 0.0001308917999267578 	 0.00010395050048828125 	 302.94783425331116 	 263.12116408348083 	 0.7757682800292969 	 0.6719014644622803 	 
2025-08-04 15:39:39.379486 test begin: paddle.as_strided(Tensor([320, 1587601],"float32"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([320, 1587601],"float32"), shape=tuple(3,), stride=tuple(1,), ) 	 508032320 	 199910 	 3.542309522628784 	 0.8661527633666992 	 0.00013566017150878906 	 7.62939453125e-05 	 300.3533706665039 	 262.8512887954712 	 0.7673614025115967 	 0.6712396144866943 	 
2025-08-04 15:49:15.763412 test begin: paddle.as_strided(Tensor([320, 3175201],"float16"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([320, 3175201],"float16"), shape=tuple(3,), stride=tuple(1,), ) 	 1016064320 	 199910 	 3.514490842819214 	 0.9183063507080078 	 0.00011014938354492188 	 0.00011110305786132812 	 302.88270568847656 	 262.95738554000854 	 0.7739584445953369 	 0.6714129447937012 	 
2025-08-04 15:59:09.196990 test begin: paddle.as_strided(Tensor([320, 3175201],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([320, 3175201],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), ) 	 1016064320 	 199910 	 3.4988276958465576 	 0.9057214260101318 	 0.00011110305786132812 	 0.00010156631469726562 	 303.01727986335754 	 263.1277918815613 	 0.7746427059173584 	 0.6719353199005127 	 
2025-08-04 16:09:01.438003 test begin: paddle.asin(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.asin 	 paddle.asin(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33858 	 9.999727249145508 	 10.067563772201538 	 0.30188870429992676 	 0.3039071559906006 	 15.238427877426147 	 60.37904357910156 	 0.46001124382019043 	 0.36446595191955566 	 
2025-08-04 16:10:41.023953 test begin: paddle.asin(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.asin 	 paddle.asin(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33858 	 10.003323793411255 	 10.082981824874878 	 0.30198073387145996 	 0.3038673400878906 	 15.245545625686646 	 60.377562522888184 	 0.4602012634277344 	 0.3644895553588867 	 
2025-08-04 16:12:19.806297 test begin: paddle.asin(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.asin 	 paddle.asin(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33858 	 10.00286602973938 	 10.067672729492188 	 0.3019752502441406 	 0.30393457412719727 	 15.245119094848633 	 60.378440380096436 	 0.4601161479949951 	 0.3645927906036377 	 
2025-08-04 16:13:58.232445 test begin: paddle.asinh(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.asinh 	 paddle.asinh(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33221 	 10.0183424949646 	 9.985442638397217 	 0.308089017868042 	 0.3070821762084961 	 14.953616857528687 	 44.445730447769165 	 0.45999956130981445 	 0.3419466018676758 	 
2025-08-04 16:15:19.452092 test begin: paddle.asinh(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.asinh 	 paddle.asinh(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33221 	 10.006921529769897 	 9.977082967758179 	 0.3078651428222656 	 0.30695104598999023 	 14.962248086929321 	 44.44543933868408 	 0.4603731632232666 	 0.34186315536499023 	 
2025-08-04 16:16:42.967134 test begin: paddle.asinh(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.asinh 	 paddle.asinh(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33221 	 10.01385235786438 	 9.976640701293945 	 0.3079063892364502 	 0.30686378479003906 	 14.960879802703857 	 44.44536805152893 	 0.4601786136627197 	 0.3418557643890381 	 
2025-08-04 16:18:04.122726 test begin: paddle.atan(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.atan 	 paddle.atan(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33597 	 9.99897575378418 	 9.994980573654175 	 0.3041810989379883 	 0.3038814067840576 	 15.11671495437622 	 35.04233431816101 	 0.4598574638366699 	 0.35536646842956543 	 
2025-08-04 16:19:18.025198 test begin: paddle.atan(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.atan 	 paddle.atan(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33597 	 10.000528812408447 	 10.013578176498413 	 0.30422258377075195 	 0.30387306213378906 	 15.128887414932251 	 35.04157638549805 	 0.46024441719055176 	 0.3553733825683594 	 
2025-08-04 16:20:30.933401 test begin: paddle.atan(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.atan 	 paddle.atan(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33597 	 10.00427794456482 	 9.992232322692871 	 0.30431556701660156 	 0.3039064407348633 	 15.128880977630615 	 35.041722536087036 	 0.46027565002441406 	 0.3554410934448242 	 
2025-08-04 16:21:43.557560 test begin: paddle.atan2(Tensor([100],"float64"), Tensor([254017, 100],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(Tensor([100],"float64"), Tensor([254017, 100],"float64"), ) 	 25401800 	 22449 	 19.79043221473694 	 7.141095876693726 	 0.3004586696624756 	 0.3247241973876953 	 37.83358716964722 	 61.598668813705444 	 0.3440275192260742 	 0.2548959255218506 	 
2025-08-04 16:23:52.080424 test begin: paddle.atan2(Tensor([111, 222, 1031],"float64"), Tensor([222, 1031],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(Tensor([111, 222, 1031],"float64"), Tensor([222, 1031],"float64"), ) 	 25634784 	 22449 	 19.805002212524414 	 7.334605693817139 	 0.30057525634765625 	 0.3336057662963867 	 27.69817566871643 	 67.06176781654358 	 0.3148527145385742 	 0.3053247928619385 	 
2025-08-04 16:25:58.679387 test begin: paddle.atan2(Tensor([111, 688, 333],"float64"), Tensor([688, 333],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(Tensor([111, 688, 333],"float64"), Tensor([688, 333],"float64"), ) 	 25659648 	 22449 	 19.830610752105713 	 7.289099216461182 	 0.3009645938873291 	 0.3335413932800293 	 27.576102018356323 	 67.2459487915039 	 0.3132967948913574 	 0.3061258792877197 	 
2025-08-04 16:28:01.829469 test begin: paddle.atan2(Tensor([344, 222, 333],"float64"), Tensor([222, 333],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(Tensor([344, 222, 333],"float64"), Tensor([222, 333],"float64"), ) 	 25504470 	 22449 	 19.77467393875122 	 7.255667209625244 	 0.3000662326812744 	 0.3325951099395752 	 29.495740175247192 	 67.03350853919983 	 0.3352773189544678 	 0.305145263671875 	 
2025-08-04 16:30:06.531724 test begin: paddle.atan2(x=Tensor([19601, 6, 6, 6, 6],"float64"), y=Tensor([19601, 6, 6, 6, 6],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(x=Tensor([19601, 6, 6, 6, 6],"float64"), y=Tensor([19601, 6, 6, 6, 6],"float64"), ) 	 50805792 	 22449 	 10.015577554702759 	 10.213410139083862 	 0.45598268508911133 	 0.4634437561035156 	 16.444889783859253 	 76.5286500453949 	 0.7486939430236816 	 0.38726305961608887 	 
2025-08-04 16:32:04.492127 test begin: paddle.atan2(x=Tensor([3, 39201, 6, 6, 6],"float64"), y=Tensor([3, 39201, 6, 6, 6],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(x=Tensor([3, 39201, 6, 6, 6],"float64"), y=Tensor([3, 39201, 6, 6, 6],"float64"), ) 	 50804496 	 22449 	 10.01293396949768 	 10.179404735565186 	 0.45586490631103516 	 0.4634218215942383 	 16.445842027664185 	 76.53122305870056 	 0.7486956119537354 	 0.3872671127319336 	 
2025-08-04 16:34:02.409122 test begin: paddle.atan2(x=Tensor([3, 6, 39201, 6, 6],"float64"), y=Tensor([3, 6, 39201, 6, 6],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(x=Tensor([3, 6, 39201, 6, 6],"float64"), y=Tensor([3, 6, 39201, 6, 6],"float64"), ) 	 50804496 	 22449 	 10.012991905212402 	 10.179792881011963 	 0.45583248138427734 	 0.463470458984375 	 16.446025848388672 	 76.52984309196472 	 0.7487344741821289 	 0.38730478286743164 	 
2025-08-04 16:36:00.349479 test begin: paddle.atan2(x=Tensor([3, 6, 6, 39201, 6],"float64"), y=Tensor([3, 6, 6, 39201, 6],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(x=Tensor([3, 6, 6, 39201, 6],"float64"), y=Tensor([3, 6, 6, 39201, 6],"float64"), ) 	 50804496 	 22449 	 10.012964010238647 	 10.179337739944458 	 0.45592355728149414 	 0.4634523391723633 	 16.445838451385498 	 76.52750062942505 	 0.748699426651001 	 0.38724613189697266 	 
2025-08-04 16:37:58.280514 test begin: paddle.atan2(x=Tensor([3, 6, 6, 6, 39201],"float64"), y=Tensor([3, 6, 6, 6, 39201],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(x=Tensor([3, 6, 6, 6, 39201],"float64"), y=Tensor([3, 6, 6, 6, 39201],"float64"), ) 	 50804496 	 22449 	 10.01444673538208 	 10.179417133331299 	 0.4558844566345215 	 0.46346259117126465 	 16.445738554000854 	 76.52901101112366 	 0.7487246990203857 	 0.3872537612915039 	 
2025-08-04 16:39:55.717966 test begin: paddle.atanh(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.atanh 	 paddle.atanh(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33693 	 9.99943470954895 	 10.041134357452393 	 0.3033416271209717 	 0.304584264755249 	 15.15970230102539 	 54.67566418647766 	 0.4597892761230469 	 0.3317248821258545 	 
2025-08-04 16:41:27.334782 test begin: paddle.atanh(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.atanh 	 paddle.atanh(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33693 	 10.003153085708618 	 10.051975011825562 	 0.3034484386444092 	 0.30459117889404297 	 15.171845436096191 	 54.67387342453003 	 0.4601297378540039 	 0.3317077159881592 	 
2025-08-04 16:42:58.972224 test begin: paddle.atanh(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.atanh 	 paddle.atanh(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33693 	 10.004289150238037 	 10.041078805923462 	 0.30346131324768066 	 0.3045539855957031 	 15.170902252197266 	 54.67508029937744 	 0.46027088165283203 	 0.33176684379577637 	 
2025-08-04 16:44:33.212274 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), )
[Error] CUDA out of memory. Tried to allocate 18.93 GiB. GPU 0 has a total capacity of 39.39 GiB of which 18.08 GiB is free. Process 126961 has 21.30 GiB memory in use. Of the allocated memory 1024 bytes is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-04 16:45:53.102807 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
W0804 16:46:41.276306 28789 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 6288311 	 18.198566675186157 	 50.88851547241211 	 0.0001952648162841797 	 0.000263214111328125 	 None 	 None 	 None 	 None 	 
2025-08-04 16:48:07.734103 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 1058401],"float64"), ) 	 2548629608 	 6288311 	 10.425716638565063 	 39.23470640182495 	 5.626678466796875e-05 	 8.416175842285156e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:49:51.216234 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), )
[Error] CUDA out of memory. Tried to allocate 18.99 GiB. GPU 0 has a total capacity of 39.39 GiB of which 1022.31 MiB is free. Process 127561 has 38.39 GiB memory in use. Of the allocated memory 189.00 KiB is allocated by PyTorch, and 1.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-04 16:51:15.061426 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), )
W0804 16:52:19.258069 30602 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), ) 	 2548654290 	 6288311 	 18.78693389892578 	 49.19819259643555 	 0.0001728534698486328 	 0.0002815723419189453 	 None 	 None 	 None 	 None 	 
2025-08-04 16:53:49.100173 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), ) 	 25425720 	 6288311 	 19.3256196975708 	 45.77743864059448 	 0.0001933574676513672 	 0.00025963783264160156 	 None 	 None 	 None 	 None 	 
2025-08-04 16:54:55.010543 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 6288311 	 18.469135522842407 	 44.24259686470032 	 7.414817810058594e-05 	 0.0007410049438476562 	 None 	 None 	 None 	 None 	 
2025-08-04 16:56:50.640894 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 6288311 	 17.934218883514404 	 43.99731922149658 	 6.079673767089844e-05 	 0.00025272369384765625 	 None 	 None 	 None 	 None 	 
2025-08-04 16:58:43.179972 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 6288311 	 17.993339776992798 	 43.99174666404724 	 0.00015091896057128906 	 0.00025010108947753906 	 None 	 None 	 None 	 None 	 
2025-08-04 16:59:45.823269 test begin: paddle.atleast_1d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 6288311 	 18.007067918777466 	 45.13186025619507 	 7.104873657226562e-05 	 0.0002658367156982422 	 None 	 None 	 None 	 None 	 
2025-08-04 17:01:38.349298 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 6288311 	 18.0853533744812 	 43.927881479263306 	 6.103515625e-05 	 0.0002493858337402344 	 None 	 None 	 None 	 None 	 
2025-08-04 17:03:31.218510 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), ) 	 2548633220 	 6288311 	 18.27869439125061 	 44.333274364471436 	 6.341934204101562e-05 	 0.0002574920654296875 	 None 	 None 	 None 	 None 	 
2025-08-04 17:05:26.493113 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), ) 	 2548632618 	 6288311 	 18.167743921279907 	 44.58029508590698 	 7.748603820800781e-05 	 0.0002529621124267578 	 None 	 None 	 None 	 None 	 
2025-08-04 17:07:20.269715 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), ) 	 25406424 	 6288311 	 18.070454120635986 	 44.17747402191162 	 0.0001456737518310547 	 0.00024509429931640625 	 None 	 None 	 None 	 None 	 
2025-08-04 17:08:24.298371 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 6288311 	 18.059369564056396 	 44.21996259689331 	 6.222724914550781e-05 	 0.0002536773681640625 	 None 	 None 	 None 	 None 	 
2025-08-04 17:10:16.450546 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 6288311 	 18.025124311447144 	 44.10377740859985 	 0.0001494884490966797 	 0.0002644062042236328 	 None 	 None 	 None 	 None 	 
2025-08-04 17:11:19.245570 test begin: paddle.atleast_1d(Tensor([301, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 423361, 5],"float64"), ) 	 2548633220 	 6288311 	 10.20768928527832 	 37.446813106536865 	 6.031990051269531e-05 	 8.20159912109375e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 17:12:57.523072 test begin: paddle.atleast_1d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 6288311 	 18.36551022529602 	 44.620750188827515 	 6.556510925292969e-05 	 0.00026106834411621094 	 None 	 None 	 None 	 None 	 
2025-08-04 17:14:50.925275 test begin: paddle.atleast_1d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 6288311 	 18.28988027572632 	 44.52216696739197 	 0.0001399517059326172 	 0.00023937225341796875 	 None 	 None 	 None 	 None 	 
2025-08-04 17:16:49.023563 test begin: paddle.atleast_1d(Tensor([301, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 846721, 2, 5],"float64"), ) 	 2548630210 	 6288311 	 10.226501941680908 	 38.046422719955444 	 5.5789947509765625e-05 	 0.0002186298370361328 	 None 	 None 	 None 	 None 	 
2025-08-04 17:18:37.745678 test begin: paddle.atleast_1d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 6288311 	 18.170202016830444 	 44.43085598945618 	 5.984306335449219e-05 	 0.0002536773681640625 	 None 	 None 	 None 	 None 	 
2025-08-04 17:20:30.193747 test begin: paddle.atleast_1d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 6288311 	 18.072471380233765 	 44.9987051486969 	 0.00014472007751464844 	 0.0002663135528564453 	 None 	 None 	 None 	 None 	 
2025-08-04 17:21:33.924438 test begin: paddle.atleast_1d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 6288311 	 17.7862708568573 	 44.44753837585449 	 0.0001366138458251953 	 0.0002384185791015625 	 None 	 None 	 None 	 None 	 
2025-08-04 17:23:49.294065 test begin: paddle.atleast_1d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 6288311 	 18.033432722091675 	 45.588643074035645 	 0.0001468658447265625 	 0.0002498626708984375 	 None 	 None 	 None 	 None 	 
2025-08-04 17:24:53.560294 test begin: paddle.atleast_1d(Tensor([63504101, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([63504101, 4, 2, 5],"float64"), ) 	 2540164040 	 6288311 	 10.059474229812622 	 38.123390674591064 	 5.936622619628906e-05 	 7.963180541992188e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 17:26:31.817668 test begin: paddle.atleast_1d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 2540164280 	 6288311 	 18.227261781692505 	 44.99156069755554 	 0.00015115737915039062 	 0.0002455711364746094 	 None 	 None 	 None 	 None 	 
2025-08-04 17:28:43.827166 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), ) 	 2540164280 	 5325424 	 20.438838243484497 	 39.598119258880615 	 5.9604644775390625e-05 	 0.00023627281188964844 	 None 	 None 	 None 	 None 	 
2025-08-04 17:30:47.768958 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 2540164280 	 5325424 	 20.4536452293396 	 38.97320008277893 	 0.0001399517059326172 	 0.00025153160095214844 	 None 	 None 	 None 	 None 	 
2025-08-04 17:32:44.099551 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), ) 	 2540160856 	 5325424 	 19.90822172164917 	 39.0285804271698 	 6.127357482910156e-05 	 0.00024390220642089844 	 None 	 None 	 None 	 None 	 
2025-08-04 17:34:40.683964 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 5325424 	 20.027937650680542 	 38.91199254989624 	 5.8650970458984375e-05 	 0.0002543926239013672 	 None 	 None 	 None 	 None 	 
2025-08-04 17:36:43.730115 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 1058401],"float64"), ) 	 2548629608 	 5325424 	 10.28207015991211 	 33.00392007827759 	 5.602836608886719e-05 	 7.843971252441406e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 17:38:16.957817 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548653688 	 5325424 	 20.934881687164307 	 48.29654288291931 	 6.532669067382812e-05 	 0.0002582073211669922 	 None 	 None 	 None 	 None 	 
2025-08-04 17:40:18.602494 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548653688 	 5325424 	 20.528021812438965 	 39.40594983100891 	 5.984306335449219e-05 	 0.0002434253692626953 	 None 	 None 	 None 	 None 	 
2025-08-04 17:42:12.099708 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), ) 	 2548653688 	 5325424 	 20.6562077999115 	 39.45787310600281 	 6.246566772460938e-05 	 0.00025010108947753906 	 None 	 None 	 None 	 None 	 
2025-08-04 17:44:02.672122 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), ) 	 2548657300 	 5325424 	 20.68898916244507 	 39.23649883270264 	 5.888938903808594e-05 	 0.0002453327178955078 	 None 	 None 	 None 	 None 	 
2025-08-04 17:45:53.562930 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), ) 	 2548654290 	 5325424 	 20.86029624938965 	 39.35782051086426 	 0.00014710426330566406 	 0.0002448558807373047 	 None 	 None 	 None 	 None 	 
2025-08-04 17:47:50.042145 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), ) 	 25425720 	 5325424 	 20.584306478500366 	 39.29289412498474 	 0.000156402587890625 	 0.00026035308837890625 	 None 	 None 	 None 	 None 	 
2025-08-04 17:48:50.607836 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 5325424 	 20.70862579345703 	 38.45305871963501 	 0.00012564659118652344 	 0.00023436546325683594 	 None 	 None 	 None 	 None 	 
2025-08-04 17:50:53.743586 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 5325424 	 20.528441429138184 	 39.1240394115448 	 5.626678466796875e-05 	 0.0002498626708984375 	 None 	 None 	 None 	 None 	 
2025-08-04 17:52:46.366238 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 5325424 	 20.46000838279724 	 38.18744659423828 	 0.00014448165893554688 	 0.0002574920654296875 	 None 	 None 	 None 	 None 	 
2025-08-04 17:53:45.679930 test begin: paddle.atleast_2d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 5325424 	 20.55512261390686 	 38.36403775215149 	 5.7697296142578125e-05 	 0.0002536773681640625 	 None 	 None 	 None 	 None 	 
2025-08-04 17:55:34.955584 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 5325424 	 21.191198348999023 	 54.452922105789185 	 0.00014781951904296875 	 0.0002646446228027344 	 None 	 None 	 None 	 None 	 
2025-08-04 17:58:04.110400 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), ) 	 2548633220 	 5325424 	 21.26872706413269 	 55.17526960372925 	 0.00014066696166992188 	 0.0002548694610595703 	 None 	 None 	 None 	 None 	 
2025-08-04 18:00:31.247338 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), ) 	 2548632618 	 5325424 	 21.500938892364502 	 38.56512689590454 	 0.00015115737915039062 	 0.0002484321594238281 	 None 	 None 	 None 	 None 	 
2025-08-04 18:02:27.606611 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), ) 	 25406424 	 5325424 	 20.173128604888916 	 38.47886943817139 	 0.00014162063598632812 	 0.0002503395080566406 	 None 	 None 	 None 	 None 	 
2025-08-04 18:03:28.183435 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 5325424 	 20.258710384368896 	 38.769306659698486 	 6.246566772460938e-05 	 0.0002472400665283203 	 None 	 None 	 None 	 None 	 
2025-08-04 18:05:16.975460 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 5325424 	 20.293423891067505 	 38.62981414794922 	 0.0001442432403564453 	 0.0002665519714355469 	 None 	 None 	 None 	 None 	 
2025-08-04 18:06:18.621439 test begin: paddle.atleast_2d(Tensor([301, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 423361, 5],"float64"), ) 	 2548633220 	 5325424 	 10.273094177246094 	 33.12126564979553 	 5.841255187988281e-05 	 7.653236389160156e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 18:07:51.294543 test begin: paddle.atleast_2d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 5325424 	 20.98027801513672 	 38.75630283355713 	 0.00014853477478027344 	 0.0002148151397705078 	 None 	 None 	 None 	 None 	 
2025-08-04 18:09:45.519752 test begin: paddle.atleast_2d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 5325424 	 20.36716079711914 	 38.78372836112976 	 6.0558319091796875e-05 	 0.0002491474151611328 	 None 	 None 	 None 	 None 	 
2025-08-04 18:11:39.145513 test begin: paddle.atleast_2d(Tensor([301, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 846721, 2, 5],"float64"), ) 	 2548630210 	 5325424 	 10.270973443984985 	 33.042741537094116 	 6.604194641113281e-05 	 8.654594421386719e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 18:13:12.875258 test begin: paddle.atleast_2d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 5325424 	 20.60897994041443 	 38.96004939079285 	 6.198883056640625e-05 	 0.0002472400665283203 	 None 	 None 	 None 	 None 	 
2025-08-04 18:15:03.943880 test begin: paddle.atleast_2d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 5325424 	 20.26624584197998 	 38.65280890464783 	 0.0001480579376220703 	 0.00026488304138183594 	 None 	 None 	 None 	 None 	 
2025-08-04 18:16:05.337645 test begin: paddle.atleast_2d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 5325424 	 19.83042287826538 	 38.409372329711914 	 5.984306335449219e-05 	 0.0002465248107910156 	 None 	 None 	 None 	 None 	 
2025-08-04 18:17:53.072970 test begin: paddle.atleast_2d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 5325424 	 20.259000778198242 	 38.74346089363098 	 0.000148773193359375 	 0.0002980232238769531 	 None 	 None 	 None 	 None 	 
2025-08-04 18:18:54.286727 test begin: paddle.atleast_2d(Tensor([63504101, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([63504101, 4, 2, 5],"float64"), ) 	 2540164040 	 5325424 	 10.242215633392334 	 33.22795796394348 	 6.246566772460938e-05 	 7.271766662597656e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 18:20:26.063779 test begin: paddle.atleast_2d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 2540164280 	 5325424 	 21.04135751724243 	 51.081613063812256 	 0.000152587890625 	 0.0002512931823730469 	 None 	 None 	 None 	 None 	 
2025-08-04 18:22:32.138311 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), ) 	 2540164280 	 4582436 	 21.519423484802246 	 32.576666593551636 	 5.9604644775390625e-05 	 0.00015997886657714844 	 None 	 None 	 None 	 None 	 
2025-08-04 18:24:18.159161 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 2540164280 	 4582436 	 21.667906284332275 	 33.49541997909546 	 5.793571472167969e-05 	 0.00020432472229003906 	 None 	 None 	 None 	 None 	 
2025-08-04 18:26:15.533324 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), ) 	 2540160856 	 4582436 	 21.416531801223755 	 33.05355501174927 	 0.00013756752014160156 	 0.00024175643920898438 	 None 	 None 	 None 	 None 	 
2025-08-04 18:28:20.052797 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 4582436 	 21.30196237564087 	 32.50607776641846 	 5.435943603515625e-05 	 7.224082946777344e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 18:30:04.133357 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 1058401],"float64"), ) 	 2548629608 	 4582436 	 10.366528987884521 	 27.995559453964233 	 5.841255187988281e-05 	 0.0002512931823730469 	 None 	 None 	 None 	 None 	 
2025-08-04 18:31:36.405969 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548653688 	 4582436 	 22.953879356384277 	 48.016308307647705 	 0.00015306472778320312 	 0.0002624988555908203 	 None 	 None 	 None 	 None 	 
2025-08-04 18:33:56.727387 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548653688 	 4582436 	 22.760814905166626 	 32.940340518951416 	 0.000141143798828125 	 7.557868957519531e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 18:36:02.475932 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), ) 	 2548653688 	 4582436 	 22.457091808319092 	 33.09186244010925 	 0.0001361370086669922 	 0.0002453327178955078 	 None 	 None 	 None 	 None 	 
2025-08-04 18:38:04.883232 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), ) 	 2548657300 	 4582436 	 22.157150506973267 	 32.79179644584656 	 6.365776062011719e-05 	 0.0002219676971435547 	 None 	 None 	 None 	 None 	 
2025-08-04 18:39:50.784838 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), ) 	 2548654290 	 4582436 	 22.237497091293335 	 32.90604066848755 	 5.91278076171875e-05 	 0.0002415180206298828 	 None 	 None 	 None 	 None 	 
2025-08-04 18:41:36.963235 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), ) 	 25425720 	 4582436 	 22.52925157546997 	 32.85288143157959 	 0.0001468658447265625 	 0.00024056434631347656 	 None 	 None 	 None 	 None 	 
2025-08-04 18:42:34.216154 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 4582436 	 22.28624176979065 	 33.11651825904846 	 0.00015425682067871094 	 0.0002548694610595703 	 None 	 None 	 None 	 None 	 
2025-08-04 18:44:35.638753 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 4582436 	 22.047526359558105 	 32.92990183830261 	 5.91278076171875e-05 	 0.00020933151245117188 	 None 	 None 	 None 	 None 	 
2025-08-04 18:46:21.559093 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 4582436 	 22.064390420913696 	 33.05052876472473 	 0.00014972686767578125 	 0.0003037452697753906 	 None 	 None 	 None 	 None 	 
2025-08-04 18:47:18.795377 test begin: paddle.atleast_3d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 4582436 	 22.228381633758545 	 32.972416162490845 	 0.0001468658447265625 	 0.00026345252990722656 	 None 	 None 	 None 	 None 	 
2025-08-04 18:49:31.913519 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 4582436 	 22.660046577453613 	 47.206536531448364 	 0.000148773193359375 	 0.00024819374084472656 	 None 	 None 	 None 	 None 	 
2025-08-04 18:51:51.404424 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), ) 	 2548633220 	 4582436 	 24.280814170837402 	 33.00946354866028 	 0.00019073486328125 	 0.0005185604095458984 	 None 	 None 	 None 	 None 	 
2025-08-04 18:53:50.537492 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), ) 	 2548632618 	 4582436 	 22.193131923675537 	 32.70717215538025 	 0.00018525123596191406 	 0.0004937648773193359 	 None 	 None 	 None 	 None 	 
2025-08-04 18:55:52.865554 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), ) 	 25406424 	 4582436 	 22.359964847564697 	 36.180718421936035 	 0.00016355514526367188 	 0.0002624988555908203 	 None 	 None 	 None 	 None 	 
2025-08-04 18:56:52.712396 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 4582436 	 22.120440006256104 	 32.888529777526855 	 6.246566772460938e-05 	 0.00019168853759765625 	 None 	 None 	 None 	 None 	 
2025-08-04 18:58:38.716133 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 4582436 	 21.84582233428955 	 32.63824772834778 	 0.00015592575073242188 	 0.00025534629821777344 	 None 	 None 	 None 	 None 	 
2025-08-04 18:59:36.587647 test begin: paddle.atleast_3d(Tensor([301, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 423361, 5],"float64"), ) 	 2548633220 	 4582436 	 10.481656312942505 	 28.460221529006958 	 5.8650970458984375e-05 	 8.320808410644531e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 19:01:04.028374 test begin: paddle.atleast_3d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 4582436 	 22.461857795715332 	 32.82981991767883 	 5.841255187988281e-05 	 0.00021958351135253906 	 None 	 None 	 None 	 None 	 
2025-08-04 19:02:50.640847 test begin: paddle.atleast_3d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 4582436 	 22.425514221191406 	 32.812071561813354 	 6.198883056640625e-05 	 0.00024962425231933594 	 None 	 None 	 None 	 None 	 
2025-08-04 19:04:37.572601 test begin: paddle.atleast_3d(Tensor([301, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 846721, 2, 5],"float64"), ) 	 2548630210 	 4582436 	 10.445215225219727 	 28.262983560562134 	 5.9604644775390625e-05 	 8.535385131835938e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 19:06:05.723702 test begin: paddle.atleast_3d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 4582436 	 22.47011423110962 	 39.98673176765442 	 7.176399230957031e-05 	 0.00026106834411621094 	 None 	 None 	 None 	 None 	 
2025-08-04 19:08:00.216566 test begin: paddle.atleast_3d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 4582436 	 22.13425350189209 	 33.245521068573 	 0.00013637542724609375 	 0.00025653839111328125 	 None 	 None 	 None 	 None 	 
2025-08-04 19:08:56.259727 test begin: paddle.atleast_3d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 4582436 	 21.932575702667236 	 33.49750208854675 	 6.771087646484375e-05 	 0.0002224445343017578 	 None 	 None 	 None 	 None 	 
2025-08-04 19:10:42.522614 test begin: paddle.atleast_3d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 4582436 	 22.053645849227905 	 33.06992745399475 	 0.00014925003051757812 	 0.00024771690368652344 	 None 	 None 	 None 	 None 	 
2025-08-04 19:11:39.012018 test begin: paddle.atleast_3d(Tensor([63504101, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([63504101, 4, 2, 5],"float64"), ) 	 2540164040 	 4582436 	 10.820275068283081 	 39.032721519470215 	 6.842613220214844e-05 	 8.273124694824219e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 19:13:17.538953 test begin: paddle.atleast_3d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 2540164280 	 4582436 	 22.525845289230347 	 47.89163279533386 	 7.104873657226562e-05 	 0.0002541542053222656 	 None 	 None 	 None 	 None 	 
2025-08-04 19:15:18.823154 test begin: paddle.bincount(Tensor([25401601],"int64"), minlength=Tensor([1],"int32"), )
[Prof] paddle.bincount 	 paddle.bincount(Tensor([25401601],"int64"), minlength=Tensor([1],"int32"), ) 	 25401602 	 10040 	 10.113845348358154 	 8.41879653930664 	 0.0004966259002685547 	 0.000469207763671875 	 None 	 None 	 None 	 None 	 
2025-08-04 19:15:38.678753 test begin: paddle.bincount(Tensor([50803201],"int32"), weights=Tensor([50803201],"float32"), )
[Prof] paddle.bincount 	 paddle.bincount(Tensor([50803201],"int32"), weights=Tensor([50803201],"float32"), ) 	 101606402 	 10040 	 18.232658624649048 	 14.41211223602295 	 0.0010576248168945312 	 0.0010726451873779297 	 None 	 None 	 None 	 None 	 
2025-08-04 19:16:12.784653 test begin: paddle.bincount(x=Tensor([50803201],"int32"), weights=Tensor([50803201],"int32"), )
[Prof] paddle.bincount 	 paddle.bincount(x=Tensor([50803201],"int32"), weights=Tensor([50803201],"int32"), ) 	 101606402 	 10040 	 18.03825569152832 	 18.10465359687805 	 0.0010361671447753906 	 0.0009570121765136719 	 None 	 None 	 None 	 None 	 
2025-08-04 19:16:50.158169 test begin: paddle.bitwise_and(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84592 	 37.90288710594177 	 38.02726888656616 	 0.45781612396240234 	 0.45939159393310547 	 None 	 None 	 None 	 None 	 
2025-08-04 19:18:08.186378 test begin: paddle.bitwise_and(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84592 	 37.90741801261902 	 38.029101610183716 	 0.45780348777770996 	 0.4594416618347168 	 None 	 None 	 None 	 None 	 
2025-08-04 19:19:26.237516 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84592 	 37.90259909629822 	 38.02820897102356 	 0.45787787437438965 	 0.45946717262268066 	 None 	 None 	 None 	 None 	 
2025-08-04 19:20:44.200444 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), ) 	 203213880 	 84592 	 37.914244651794434 	 38.02969026565552 	 0.45822858810424805 	 0.45946407318115234 	 None 	 None 	 None 	 None 	 
2025-08-04 19:22:02.159045 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), ) 	 101607264 	 84592 	 10.01827883720398 	 9.918833017349243 	 0.12102627754211426 	 0.11977195739746094 	 None 	 None 	 None 	 None 	 
2025-08-04 19:22:23.585598 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), ) 	 101607264 	 84592 	 38.026297092437744 	 37.71485424041748 	 0.4594085216522217 	 0.45569396018981934 	 None 	 None 	 None 	 None 	 
2025-08-04 19:23:41.196011 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), ) 	 203213664 	 84592 	 37.90500068664551 	 38.027082204818726 	 0.4580836296081543 	 0.45944786071777344 	 None 	 None 	 None 	 None 	 
2025-08-04 19:25:01.271067 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 50807520 	 84592 	 15.048681259155273 	 19.220128536224365 	 0.1818094253540039 	 0.23216605186462402 	 None 	 None 	 None 	 None 	 
2025-08-04 19:25:37.508533 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 101610720 	 84592 	 26.68807339668274 	 40.573758602142334 	 0.3224804401397705 	 0.4900805950164795 	 None 	 None 	 None 	 None 	 
2025-08-04 19:26:47.287814 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 50807520 	 84592 	 25.038949251174927 	 26.056714057922363 	 0.3025054931640625 	 0.3148529529571533 	 None 	 None 	 None 	 None 	 
2025-08-04 19:27:40.262053 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), ) 	 101608560 	 84592 	 10.01308274269104 	 9.815485000610352 	 0.12095141410827637 	 0.11861133575439453 	 None 	 None 	 None 	 None 	 
2025-08-04 19:28:01.499943 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), ) 	 101608560 	 84592 	 38.03323531150818 	 37.71967339515686 	 0.4595768451690674 	 0.4556586742401123 	 None 	 None 	 None 	 None 	 
2025-08-04 19:29:18.883226 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), ) 	 203214960 	 84592 	 37.917909145355225 	 38.02856731414795 	 0.4581742286682129 	 0.45940661430358887 	 None 	 None 	 None 	 None 	 
2025-08-04 19:30:36.840330 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 50807520 	 84592 	 15.601318836212158 	 19.207891702651978 	 0.1884899139404297 	 0.23205208778381348 	 None 	 None 	 None 	 None 	 
2025-08-04 19:31:12.357384 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 101610720 	 84592 	 10.013558864593506 	 9.828259706497192 	 0.12096858024597168 	 0.11870694160461426 	 None 	 None 	 None 	 None 	 
2025-08-04 19:31:33.572663 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50807520 	 84592 	 25.004075527191162 	 26.05358099937439 	 0.3020641803741455 	 0.3146674633026123 	 None 	 None 	 None 	 None 	 
2025-08-04 19:32:25.231903 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 101610720 	 84592 	 38.033175230026245 	 37.71796202659607 	 0.4594705104827881 	 0.4556856155395508 	 None 	 None 	 None 	 None 	 
2025-08-04 19:33:42.195071 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101610720 	 84592 	 29.161701202392578 	 40.44614291191101 	 0.352292537689209 	 0.4886331558227539 	 None 	 None 	 None 	 None 	 
2025-08-04 19:34:52.801652 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 203217120 	 84592 	 37.91224217414856 	 38.02981185913086 	 0.4579758644104004 	 0.45946502685546875 	 None 	 None 	 None 	 None 	 
2025-08-04 19:36:13.194199 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), ) 	 101607480 	 84592 	 9.990940809249878 	 9.744794607162476 	 0.12069272994995117 	 0.11770772933959961 	 None 	 None 	 None 	 None 	 
2025-08-04 19:36:34.329241 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), ) 	 101607480 	 84592 	 38.03680777549744 	 37.720226764678955 	 0.4596128463745117 	 0.4556128978729248 	 None 	 None 	 None 	 None 	 
2025-08-04 19:37:51.275860 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84592 	 9.991087675094604 	 9.74710464477539 	 0.12071752548217773 	 0.11777138710021973 	 None 	 None 	 None 	 None 	 
2025-08-04 19:38:12.415464 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84592 	 38.03390431404114 	 37.714792013168335 	 0.4595308303833008 	 0.45564746856689453 	 None 	 None 	 None 	 None 	 
2025-08-04 19:39:29.333749 test begin: paddle.bitwise_and(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84592 	 9.99165153503418 	 9.761465311050415 	 0.12070965766906738 	 0.11775636672973633 	 None 	 None 	 None 	 None 	 
2025-08-04 19:39:52.648155 test begin: paddle.bitwise_and(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84592 	 38.03213810920715 	 37.719311475753784 	 0.4595050811767578 	 0.45563673973083496 	 None 	 None 	 None 	 None 	 
2025-08-04 19:41:11.584231 test begin: paddle.bitwise_and(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84592 	 9.99147081375122 	 9.747496366500854 	 0.1207427978515625 	 0.11774206161499023 	 None 	 None 	 None 	 None 	 
2025-08-04 19:41:32.719701 test begin: paddle.bitwise_and(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84592 	 38.031920194625854 	 37.71513223648071 	 0.45952939987182617 	 0.4557032585144043 	 None 	 None 	 None 	 None 	 
2025-08-04 19:42:49.633637 test begin: paddle.bitwise_and(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101608560 	 84592 	 10.012905836105347 	 9.822262048721313 	 0.12099266052246094 	 0.11859965324401855 	 None 	 None 	 None 	 None 	 
2025-08-04 19:43:11.653985 test begin: paddle.bitwise_and(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101608560 	 84592 	 38.033024311065674 	 37.716511726379395 	 0.45955610275268555 	 0.4556262493133545 	 None 	 None 	 None 	 None 	 
2025-08-04 19:44:28.856956 test begin: paddle.bitwise_and(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214960 	 84592 	 37.91433668136597 	 38.02769923210144 	 0.45790576934814453 	 0.4594271183013916 	 None 	 None 	 None 	 None 	 
2025-08-04 19:45:46.813679 test begin: paddle.bitwise_invert(Tensor([12700801, 4, 1],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([12700801, 4, 1],"int32"), ) 	 50803204 	 33827 	 9.997935056686401 	 10.065271854400635 	 0.3020589351654053 	 0.30414891242980957 	 None 	 None 	 None 	 None 	 
2025-08-04 19:46:07.452708 test begin: paddle.bitwise_invert(Tensor([2, 1270081, 4, 5],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([2, 1270081, 4, 5],"int32"), ) 	 50803240 	 33827 	 9.997735261917114 	 10.066757202148438 	 0.30208349227905273 	 0.30416083335876465 	 None 	 None 	 None 	 None 	 
2025-08-04 19:46:28.818709 test begin: paddle.bitwise_invert(Tensor([2, 3, 1693441, 5],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([2, 3, 1693441, 5],"int32"), ) 	 50803230 	 33827 	 9.99777626991272 	 11.058039426803589 	 0.3021094799041748 	 0.3041565418243408 	 None 	 None 	 None 	 None 	 
2025-08-04 19:46:52.185800 test begin: paddle.bitwise_invert(Tensor([2, 3, 4, 2116801],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([2, 3, 4, 2116801],"int32"), ) 	 50803224 	 33827 	 9.997791290283203 	 10.06533408164978 	 0.3021047115325928 	 0.30406928062438965 	 None 	 None 	 None 	 None 	 
2025-08-04 19:47:12.858103 test begin: paddle.bitwise_invert(Tensor([3, 16934401, 1],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([3, 16934401, 1],"int32"), ) 	 50803203 	 33827 	 9.997945308685303 	 10.065405368804932 	 0.30204057693481445 	 0.30409884452819824 	 None 	 None 	 None 	 None 	 
2025-08-04 19:47:35.466022 test begin: paddle.bitwise_invert(Tensor([3, 4, 4233601],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([3, 4, 4233601],"int32"), ) 	 50803212 	 33827 	 9.998012781143188 	 10.067909955978394 	 0.30211710929870605 	 0.30409836769104004 	 None 	 None 	 None 	 None 	 
2025-08-04 19:47:56.126025 test begin: paddle.bitwise_invert(Tensor([846721, 3, 4, 5],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([846721, 3, 4, 5],"int32"), ) 	 50803260 	 33827 	 9.997650861740112 	 10.065510034561157 	 0.30205535888671875 	 0.3041391372680664 	 None 	 None 	 None 	 None 	 
2025-08-04 19:48:16.808618 test begin: paddle.bitwise_left_shift(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), ) 	 101607000 	 22337 	 10.044345617294312 	 9.959794044494629 	 0.4595072269439697 	 0.4557335376739502 	 None 	 None 	 None 	 None 	 
2025-08-04 19:48:38.705703 test begin: paddle.bitwise_left_shift(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), ) 	 101606800 	 22337 	 10.043376684188843 	 9.962178230285645 	 0.45952296257019043 	 0.4557981491088867 	 None 	 None 	 None 	 None 	 
2025-08-04 19:48:59.933062 test begin: paddle.bitwise_left_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), ) 	 203213200 	 22337 	 9.998210668563843 	 10.038915157318115 	 0.45740365982055664 	 0.4593319892883301 	 None 	 None 	 None 	 None 	 
2025-08-04 19:49:22.007358 test begin: paddle.bitwise_left_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, ) 	 203213200 	 22337 	 9.998339891433716 	 10.050132751464844 	 0.45748305320739746 	 0.45939087867736816 	 None 	 None 	 None 	 None 	 
2025-08-04 19:49:44.481221 test begin: paddle.bitwise_left_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), ) 	 203213400 	 22337 	 9.995957851409912 	 10.050945043563843 	 0.4572412967681885 	 0.45937609672546387 	 None 	 None 	 None 	 None 	 
2025-08-04 19:50:07.892326 test begin: paddle.bitwise_left_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, ) 	 203213400 	 22337 	 9.996027708053589 	 10.039181232452393 	 0.4572606086730957 	 0.45937633514404297 	 None 	 None 	 None 	 None 	 
2025-08-04 19:50:29.821198 test begin: paddle.bitwise_not(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101607120 	 33833 	 10.084823369979858 	 10.042312383651733 	 0.3046698570251465 	 0.30254077911376953 	 None 	 None 	 None 	 None 	 
2025-08-04 19:50:51.608664 test begin: paddle.bitwise_not(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), ) 	 101607120 	 33833 	 10.084633350372314 	 11.024275302886963 	 0.3046739101409912 	 0.3025810718536377 	 None 	 None 	 None 	 None 	 
2025-08-04 19:51:15.650570 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), ) 	 101607120 	 33833 	 10.084591150283813 	 10.014181137084961 	 0.30462145805358887 	 0.3025400638580322 	 None 	 None 	 None 	 None 	 
2025-08-04 19:51:38.796980 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), ) 	 101606940 	 33833 	 10.061811685562134 	 10.013691186904907 	 0.30393314361572266 	 0.3024585247039795 	 None 	 None 	 None 	 None 	 
2025-08-04 19:51:59.826098 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), ) 	 50803632 	 33833 	 9.994597434997559 	 10.071341276168823 	 0.3018960952758789 	 0.30408692359924316 	 None 	 None 	 None 	 None 	 
2025-08-04 19:52:21.029833 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), ) 	 101606832 	 33833 	 10.08406138420105 	 10.014009714126587 	 0.3046257495880127 	 0.3025214672088623 	 None 	 None 	 None 	 None 	 
2025-08-04 19:52:42.337433 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), ) 	 50804280 	 33833 	 10.002633571624756 	 10.06752610206604 	 0.30213356018066406 	 0.30410051345825195 	 None 	 None 	 None 	 None 	 
2025-08-04 19:53:03.026194 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), ) 	 101607480 	 33833 	 10.05992341041565 	 10.032108783721924 	 0.3038804531097412 	 0.30249738693237305 	 None 	 None 	 None 	 None 	 
2025-08-04 19:53:24.125492 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 50805360 	 33833 	 9.997042179107666 	 10.067655563354492 	 0.3020515441894531 	 0.30414652824401855 	 None 	 None 	 None 	 None 	 
2025-08-04 19:53:44.803412 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 101608560 	 33833 	 10.06064486503601 	 10.013971328735352 	 0.3039271831512451 	 0.3024766445159912 	 None 	 None 	 None 	 None 	 
2025-08-04 19:54:05.817381 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), ) 	 50803740 	 33833 	 10.003132343292236 	 10.068485736846924 	 0.3021707534790039 	 0.30417370796203613 	 None 	 None 	 None 	 None 	 
2025-08-04 19:54:27.025173 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), ) 	 50803920 	 33833 	 10.000621795654297 	 10.07628607749939 	 0.3020787239074707 	 0.30407071113586426 	 None 	 None 	 None 	 None 	 
2025-08-04 19:54:48.675518 test begin: paddle.bitwise_not(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), ) 	 50803920 	 33833 	 10.000637769699097 	 10.068623065948486 	 0.3020913600921631 	 0.3041565418243408 	 None 	 None 	 None 	 None 	 
2025-08-04 19:55:09.396656 test begin: paddle.bitwise_not(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50803920 	 33833 	 10.000494241714478 	 10.067353010177612 	 0.3021223545074463 	 0.30417490005493164 	 None 	 None 	 None 	 None 	 
2025-08-04 19:55:30.011489 test begin: paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 1, 117601, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 1, 117601, 2],"bool"), ) 	 508036320 	 33833 	 28.646851301193237 	 25.26790142059326 	 0.8653812408447266 	 0.7632994651794434 	 None 	 None 	 None 	 None 	 
2025-08-04 19:56:30.941917 test begin: paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 1, 5, 47041],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 1, 5, 47041],"bool"), ) 	 508042800 	 33833 	 29.01571822166443 	 25.28663682937622 	 0.8765716552734375 	 0.7638115882873535 	 None 	 None 	 None 	 None 	 
2025-08-04 19:57:32.090588 test begin: paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 508053600 	 33833 	 28.69332242012024 	 25.271601676940918 	 0.8667864799499512 	 0.7632861137390137 	 None 	 None 	 None 	 None 	 
2025-08-04 19:58:33.344880 test begin: paddle.bitwise_not(Tensor([20, 3, 3, 3, 94081, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 3, 3, 94081, 1, 5, 2],"bool"), ) 	 508037400 	 33833 	 28.986297130584717 	 26.201104402542114 	 0.8756413459777832 	 0.7633235454559326 	 None 	 None 	 None 	 None 	 
2025-08-04 19:59:38.916367 test begin: paddle.bitwise_not(Tensor([20, 3, 3, 70561, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 3, 70561, 4, 1, 5, 2],"bool"), ) 	 508039200 	 33833 	 28.667264938354492 	 25.28641676902771 	 0.8659994602203369 	 0.763171911239624 	 None 	 None 	 None 	 None 	 
2025-08-04 20:00:42.687128 test begin: paddle.bitwise_not(Tensor([20, 3, 70561, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 70561, 3, 4, 1, 5, 2],"bool"), ) 	 508039200 	 33833 	 28.679982662200928 	 25.267248153686523 	 0.8664159774780273 	 0.7632768154144287 	 None 	 None 	 None 	 None 	 
2025-08-04 20:01:43.713838 test begin: paddle.bitwise_not(Tensor([20, 70561, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 70561, 3, 3, 4, 1, 5, 2],"bool"), ) 	 508039200 	 33833 	 28.680142641067505 	 25.26842451095581 	 0.866340160369873 	 0.7632086277008057 	 None 	 None 	 None 	 None 	 
2025-08-04 20:02:44.832963 test begin: paddle.bitwise_not(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50804280 	 33833 	 10.002281188964844 	 10.069162845611572 	 0.30213117599487305 	 0.3041102886199951 	 None 	 None 	 None 	 None 	 
2025-08-04 20:03:05.510266 test begin: paddle.bitwise_not(Tensor([470410, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([470410, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 508042800 	 33833 	 29.00820565223694 	 25.295182943344116 	 0.876274585723877 	 0.7638306617736816 	 None 	 None 	 None 	 None 	 
2025-08-04 20:04:07.502187 test begin: paddle.bitwise_not(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101607480 	 33833 	 10.060238599777222 	 10.014056205749512 	 0.30388927459716797 	 0.3024759292602539 	 None 	 None 	 None 	 None 	 
2025-08-04 20:04:28.578365 test begin: paddle.bitwise_or(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84965 	 38.07439613342285 	 38.195411920547485 	 0.45790815353393555 	 0.4594156742095947 	 None 	 None 	 None 	 None 	 
2025-08-04 20:05:46.874904 test begin: paddle.bitwise_or(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84965 	 38.071733713150024 	 38.197004079818726 	 0.4579148292541504 	 0.45949769020080566 	 None 	 None 	 None 	 None 	 
2025-08-04 20:07:05.147183 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84965 	 38.072380781173706 	 38.20180797576904 	 0.45786166191101074 	 0.4595460891723633 	 None 	 None 	 None 	 None 	 
2025-08-04 20:08:24.150722 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), ) 	 203213880 	 84965 	 38.0841064453125 	 38.1975953578949 	 0.4584658145904541 	 0.4594283103942871 	 None 	 None 	 None 	 None 	 
2025-08-04 20:09:43.993320 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), ) 	 101607264 	 84965 	 10.019906759262085 	 9.934287309646606 	 0.12050938606262207 	 0.11948180198669434 	 None 	 None 	 None 	 None 	 
2025-08-04 20:10:05.370737 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), ) 	 101607264 	 84965 	 38.19808888435364 	 37.89433813095093 	 0.4594857692718506 	 0.4556746482849121 	 None 	 None 	 None 	 None 	 
2025-08-04 20:11:22.696344 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), ) 	 203213664 	 84965 	 38.074153661727905 	 38.19922232627869 	 0.4585123062133789 	 0.45949268341064453 	 None 	 None 	 None 	 None 	 
2025-08-04 20:12:43.779283 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 50807520 	 84965 	 13.218268394470215 	 19.322123050689697 	 0.15902256965637207 	 0.23242521286010742 	 None 	 None 	 None 	 None 	 
2025-08-04 20:13:17.080897 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 101610720 	 84965 	 26.87896752357483 	 40.75293803215027 	 0.32367897033691406 	 0.4901885986328125 	 None 	 None 	 None 	 None 	 
2025-08-04 20:14:25.713810 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 50807520 	 84965 	 25.148037910461426 	 26.166170597076416 	 0.3025071620941162 	 0.31472039222717285 	 None 	 None 	 None 	 None 	 
2025-08-04 20:15:20.685767 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), ) 	 101608560 	 84965 	 10.017151355743408 	 9.836331605911255 	 0.1205282211303711 	 0.11827898025512695 	 None 	 None 	 None 	 None 	 
2025-08-04 20:15:43.001376 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), ) 	 101608560 	 84965 	 38.20441126823425 	 37.886728048324585 	 0.45952510833740234 	 0.4557626247406006 	 None 	 None 	 None 	 None 	 
2025-08-04 20:17:00.316710 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), ) 	 203214960 	 84965 	 38.08440971374512 	 39.87037920951843 	 0.4581105709075928 	 0.4594917297363281 	 None 	 None 	 None 	 None 	 
2025-08-04 20:18:22.009325 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 50807520 	 84965 	 14.730387926101685 	 19.26585364341736 	 0.17717790603637695 	 0.23152542114257812 	 None 	 None 	 None 	 None 	 
2025-08-04 20:18:59.511170 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 101610720 	 84965 	 10.017413854598999 	 9.848037719726562 	 0.12060165405273438 	 0.11851024627685547 	 None 	 None 	 None 	 None 	 
2025-08-04 20:19:20.806556 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50807520 	 84965 	 25.11396837234497 	 26.171169757843018 	 0.3020620346069336 	 0.3147695064544678 	 None 	 None 	 None 	 None 	 
2025-08-04 20:20:12.720438 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 101610720 	 84965 	 38.20609211921692 	 37.88693690299988 	 0.4595932960510254 	 0.4557185173034668 	 None 	 None 	 None 	 None 	 
2025-08-04 20:21:30.021753 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101610720 	 84965 	 29.300956964492798 	 40.641987800598145 	 0.3524167537689209 	 0.4888730049133301 	 None 	 None 	 None 	 None 	 
2025-08-04 20:22:42.585840 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 203217120 	 84965 	 38.08261561393738 	 38.200011253356934 	 0.45816946029663086 	 0.45952510833740234 	 None 	 None 	 None 	 None 	 
2025-08-04 20:24:00.883628 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), ) 	 101607480 	 84965 	 9.991061925888062 	 9.751112461090088 	 0.12017512321472168 	 0.11725592613220215 	 None 	 None 	 None 	 None 	 
2025-08-04 20:24:22.072092 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), ) 	 101607480 	 84965 	 38.206982374191284 	 37.88391613960266 	 0.45956969261169434 	 0.45566320419311523 	 None 	 None 	 None 	 None 	 
2025-08-04 20:25:39.748142 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84965 	 9.991196155548096 	 9.758244276046753 	 0.12017059326171875 	 0.11735153198242188 	 None 	 None 	 None 	 None 	 
2025-08-04 20:26:00.941232 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84965 	 38.20193815231323 	 39.20863342285156 	 0.4595189094543457 	 0.4557003974914551 	 None 	 None 	 None 	 None 	 
2025-08-04 20:27:22.045160 test begin: paddle.bitwise_or(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84965 	 9.991225242614746 	 9.75454831123352 	 0.12016749382019043 	 0.11732220649719238 	 None 	 None 	 None 	 None 	 
2025-08-04 20:27:43.755261 test begin: paddle.bitwise_or(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84965 	 38.203843116760254 	 37.89842629432678 	 0.45957159996032715 	 0.4557318687438965 	 None 	 None 	 None 	 None 	 
2025-08-04 20:29:02.488277 test begin: paddle.bitwise_or(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84965 	 9.991193771362305 	 9.754609107971191 	 0.12019968032836914 	 0.11732006072998047 	 None 	 None 	 None 	 None 	 
2025-08-04 20:29:23.656066 test begin: paddle.bitwise_or(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84965 	 38.204150915145874 	 37.88993263244629 	 0.4595489501953125 	 0.455676794052124 	 None 	 None 	 None 	 None 	 
2025-08-04 20:30:44.234496 test begin: paddle.bitwise_or(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101608560 	 84965 	 10.016996145248413 	 9.8358895778656 	 0.12046647071838379 	 0.11829137802124023 	 None 	 None 	 None 	 None 	 
2025-08-04 20:31:09.135210 test begin: paddle.bitwise_or(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101608560 	 84965 	 38.20298409461975 	 38.139474391937256 	 0.4595804214477539 	 0.45575594902038574 	 None 	 None 	 None 	 None 	 
2025-08-04 20:32:30.010072 test begin: paddle.bitwise_or(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214960 	 84965 	 38.08545398712158 	 38.1971378326416 	 0.45812535285949707 	 0.45946645736694336 	 None 	 None 	 None 	 None 	 
2025-08-04 20:33:48.284604 test begin: paddle.bitwise_right_shift(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), ) 	 101607000 	 22351 	 10.04999589920044 	 9.965294599533081 	 0.45955824851989746 	 0.4556303024291992 	 None 	 None 	 None 	 None 	 combined
2025-08-04 20:34:12.444605 test begin: paddle.bitwise_right_shift(Tensor([200, 127009],"int64"), Tensor([200, 127009],"int64"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([200, 127009],"int64"), Tensor([200, 127009],"int64"), ) 	 50803600 	 22351 	 10.014764547348022 	 9.953999996185303 	 0.45802974700927734 	 0.4551715850830078 	 None 	 None 	 None 	 None 	 combined
2025-08-04 20:34:33.247689 test begin: paddle.bitwise_right_shift(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), ) 	 101606800 	 22351 	 10.048908948898315 	 9.972101211547852 	 0.4595448970794678 	 0.4556593894958496 	 None 	 None 	 None 	 None 	 combined
2025-08-04 20:34:54.945469 test begin: paddle.bitwise_right_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), ) 	 203213200 	 22351 	 9.99500823020935 	 10.04518175125122 	 0.4569094181060791 	 0.45938587188720703 	 None 	 None 	 None 	 None 	 combined
2025-08-04 20:35:16.968879 test begin: paddle.bitwise_right_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), ) 	 203213400 	 22351 	 9.993101835250854 	 10.045505285263062 	 0.45685863494873047 	 0.4593355655670166 	 None 	 None 	 None 	 None 	 combined
2025-08-04 20:35:42.590321 test begin: paddle.bitwise_right_shift(Tensor([84673, 300],"int64"), Tensor([84673, 300],"int64"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([84673, 300],"int64"), Tensor([84673, 300],"int64"), ) 	 50803800 	 22351 	 10.014420509338379 	 9.953195333480835 	 0.45777225494384766 	 0.4551053047180176 	 None 	 None 	 None 	 None 	 combined
2025-08-04 20:36:03.413988 test begin: paddle.bitwise_xor(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84955 	 38.067405223846436 	 38.21787405014038 	 0.45783495903015137 	 0.4594707489013672 	 None 	 None 	 None 	 None 	 
2025-08-04 20:37:22.810703 test begin: paddle.bitwise_xor(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84955 	 38.07090377807617 	 38.192479610443115 	 0.4579129219055176 	 0.4594581127166748 	 None 	 None 	 None 	 None 	 
2025-08-04 20:38:44.633940 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84955 	 38.06974458694458 	 38.191030979156494 	 0.457874059677124 	 0.45946502685546875 	 None 	 None 	 None 	 None 	 
2025-08-04 20:40:02.932341 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), ) 	 203213880 	 84955 	 38.081358194351196 	 38.20215916633606 	 0.4578073024749756 	 0.45941615104675293 	 None 	 None 	 None 	 None 	 
2025-08-04 20:41:25.930070 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), ) 	 101607264 	 84955 	 10.019234657287598 	 9.955021619796753 	 0.12055063247680664 	 0.1197516918182373 	 None 	 None 	 None 	 None 	 
2025-08-04 20:41:49.402797 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), ) 	 101607264 	 84955 	 38.19297456741333 	 37.885355949401855 	 0.45944714546203613 	 0.4557058811187744 	 None 	 None 	 None 	 None 	 
2025-08-04 20:43:08.898099 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), ) 	 203213664 	 84955 	 38.06863975524902 	 38.2143189907074 	 0.4577784538269043 	 0.45943665504455566 	 None 	 None 	 None 	 None 	 
2025-08-04 20:44:27.205554 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 50807520 	 84955 	 14.295498371124268 	 19.299614191055298 	 0.17195987701416016 	 0.23215532302856445 	 None 	 None 	 None 	 None 	 
2025-08-04 20:45:02.330205 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 101610720 	 84955 	 26.809386014938354 	 40.76811242103577 	 0.32245659828186035 	 0.49056243896484375 	 None 	 None 	 None 	 None 	 
2025-08-04 20:46:10.902904 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 50807520 	 84955 	 25.145832300186157 	 26.18500828742981 	 0.30249667167663574 	 0.31477785110473633 	 None 	 None 	 None 	 None 	 
2025-08-04 20:47:05.431649 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), ) 	 101608560 	 84955 	 10.016030550003052 	 9.870616436004639 	 0.12047123908996582 	 0.11860203742980957 	 None 	 None 	 None 	 None 	 
2025-08-04 20:47:29.286114 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), ) 	 101608560 	 84955 	 38.195744037628174 	 37.87867999076843 	 0.45947933197021484 	 0.45564985275268555 	 None 	 None 	 None 	 None 	 
2025-08-04 20:48:46.533183 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), ) 	 203214960 	 84955 	 38.08483576774597 	 38.19219994544983 	 0.4579906463623047 	 0.45946288108825684 	 None 	 None 	 None 	 None 	 
2025-08-04 20:50:05.157775 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 50807520 	 84955 	 15.00644850730896 	 19.289876461029053 	 0.18053984642028809 	 0.23203468322753906 	 None 	 None 	 None 	 None 	 
2025-08-04 20:50:43.205149 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 101610720 	 84955 	 10.016221523284912 	 9.876235246658325 	 0.12047791481018066 	 0.11872982978820801 	 None 	 None 	 None 	 None 	 
2025-08-04 20:51:04.492401 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50807520 	 84955 	 25.111356735229492 	 26.172362327575684 	 0.3020761013031006 	 0.3148059844970703 	 None 	 None 	 None 	 None 	 
2025-08-04 20:51:57.569664 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 101610720 	 84955 	 38.19779992103577 	 37.88449740409851 	 0.4594902992248535 	 0.4557609558105469 	 None 	 None 	 None 	 None 	 
2025-08-04 20:53:17.884043 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101610720 	 84955 	 29.313386917114258 	 40.66634273529053 	 0.3526475429534912 	 0.4891197681427002 	 None 	 None 	 None 	 None 	 
2025-08-04 20:54:28.860770 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 203217120 	 84955 	 38.07629132270813 	 38.19324064254761 	 0.45827198028564453 	 0.4594614505767822 	 None 	 None 	 None 	 None 	 
2025-08-04 20:55:47.154804 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), ) 	 101607480 	 84955 	 9.98997163772583 	 9.786439180374146 	 0.12015366554260254 	 0.11773037910461426 	 None 	 None 	 None 	 None 	 
2025-08-04 20:56:08.307158 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), ) 	 101607480 	 84955 	 38.19947290420532 	 37.88492226600647 	 0.45954227447509766 	 0.4556617736816406 	 None 	 None 	 None 	 None 	 
2025-08-04 20:57:27.173456 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84955 	 9.989732027053833 	 9.800179719924927 	 0.12016868591308594 	 0.11774659156799316 	 None 	 None 	 None 	 None 	 
2025-08-04 20:57:49.347392 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84955 	 38.195831060409546 	 37.87924933433533 	 0.4595150947570801 	 0.4557182788848877 	 None 	 None 	 None 	 None 	 
2025-08-04 20:59:08.596009 test begin: paddle.bitwise_xor(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84955 	 9.989747524261475 	 9.798979759216309 	 0.12017011642456055 	 0.11776971817016602 	 None 	 None 	 None 	 None 	 
2025-08-04 20:59:29.778627 test begin: paddle.bitwise_xor(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84955 	 38.19675374031067 	 40.04640221595764 	 0.45946598052978516 	 0.4557764530181885 	 None 	 None 	 None 	 None 	 
2025-08-04 21:00:51.537795 test begin: paddle.bitwise_xor(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84955 	 9.989909887313843 	 9.789339065551758 	 0.12016534805297852 	 0.11775445938110352 	 None 	 None 	 None 	 None 	 
2025-08-04 21:01:15.670439 test begin: paddle.bitwise_xor(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84955 	 38.19555950164795 	 37.87742376327515 	 0.459414005279541 	 0.4556424617767334 	 None 	 None 	 None 	 None 	 
2025-08-04 21:02:37.079221 test begin: paddle.bitwise_xor(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101608560 	 84955 	 10.016160249710083 	 9.869054079055786 	 0.12048721313476562 	 0.11854958534240723 	 None 	 None 	 None 	 None 	 
2025-08-04 21:02:59.493992 test begin: paddle.bitwise_xor(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101608560 	 84955 	 38.195632457733154 	 40.84514617919922 	 0.4594995975494385 	 0.4556734561920166 	 None 	 None 	 None 	 None 	 
2025-08-04 21:04:22.469924 test begin: paddle.bitwise_xor(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214960 	 84955 	 38.07881569862366 	 39.58192729949951 	 0.45783400535583496 	 0.4594271183013916 	 None 	 None 	 None 	 None 	 
2025-08-04 21:05:44.814004 test begin: paddle.bmm(Tensor([112, 1043, 435],"float32"), Tensor([112, 435, 64],"float32"), )
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.bmm 	 paddle.bmm(Tensor([112, 1043, 435],"float32"), Tensor([112, 435, 64],"float32"), ) 	 53933040 	 12079 	 10.799712419509888 	 10.807109117507935 	 0.9137759208679199 	 0.9137082099914551 	 19.43250560760498 	 19.432331323623657 	 0.8220007419586182 	 0.8220436573028564 	 
2025-08-04 21:06:47.502164 test begin: paddle.bmm(Tensor([112, 435, 435],"float32"), Tensor([112, 435, 1043],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([112, 435, 435],"float32"), Tensor([112, 435, 1043],"float32"), ) 	 72008160 	 12079 	 40.69458270072937 	 40.68902349472046 	 3.4430503845214844 	 3.4427237510681152 	 83.9112377166748 	 83.91867208480835 	 3.5499322414398193 	 3.550299644470215 	 
2025-08-04 21:10:58.719901 test begin: paddle.bmm(Tensor([14, 81, 7332],"float32"), Tensor([14, 7332, 512],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([14, 81, 7332],"float32"), Tensor([14, 7332, 512],"float32"), ) 	 60870264 	 12079 	 17.719841718673706 	 17.719478368759155 	 1.499340295791626 	 1.4992351531982422 	 17.201815128326416 	 17.19667410850525 	 0.7275981903076172 	 0.7274329662322998 	 
2025-08-04 21:12:11.801714 test begin: paddle.bmm(Tensor([1825, 435, 435],"float32"), Tensor([1825, 435, 64],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([1825, 435, 435],"float32"), Tensor([1825, 435, 64],"float32"), ) 	 396143625 	 12079 	 72.67480993270874 	 72.68033766746521 	 6.1487555503845215 	 6.149680852890015 	 120.18279457092285 	 120.19608187675476 	 5.084632158279419 	 5.084872484207153 	 
2025-08-04 21:18:44.733408 test begin: paddle.bmm(Tensor([26, 1024, 1024],"float32"), Tensor([26, 1024, 1909],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([26, 1024, 1024],"float32"), Tensor([26, 1024, 1909],"float32"), ) 	 78088192 	 12079 	 71.77484607696533 	 72.08591079711914 	 6.072928428649902 	 6.397607803344727 	 145.82079553604126 	 145.79783987998962 	 6.168551683425903 	 6.168015956878662 	 
2025-08-04 21:26:03.257811 test begin: paddle.bmm(Tensor([26, 1909, 1024],"float32"), Tensor([26, 1024, 12],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([26, 1909, 1024],"float32"), Tensor([26, 1024, 12],"float32"), ) 	 51144704 	 12079 	 9.997286796569824 	 9.997892141342163 	 0.8459043502807617 	 0.8459475040435791 	 11.312333583831787 	 11.308523893356323 	 0.4783666133880615 	 0.4784069061279297 	 
2025-08-04 21:26:47.253759 test begin: paddle.bmm(Tensor([269, 435, 435],"float32"), Tensor([269, 435, 64],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([269, 435, 435],"float32"), Tensor([269, 435, 64],"float32"), ) 	 58390485 	 12079 	 10.816454887390137 	 10.819275617599487 	 0.9152050018310547 	 0.9151303768157959 	 17.94180154800415 	 17.943889617919922 	 0.7590317726135254 	 0.7590770721435547 	 
2025-08-04 21:27:46.048809 test begin: paddle.bmm(Tensor([4, 1733, 7332],"float32"), Tensor([4, 7332, 512],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([4, 1733, 7332],"float32"), Tensor([4, 7332, 512],"float32"), ) 	 65841360 	 12079 	 52.96967005729675 	 52.96955943107605 	 4.481762886047363 	 4.481785774230957 	 76.49161672592163 	 76.49044847488403 	 3.2359962463378906 	 3.235792875289917 	 
2025-08-04 21:32:06.191900 test begin: paddle.bmm(Tensor([4, 81, 156801],"float32"), Tensor([4, 156801, 512],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7efffd202a40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:42:45.981022 test begin: paddle.bmm(Tensor([4, 81, 24807],"float32"), Tensor([4, 24807, 512],"float32"), )
W0804 21:42:47.105425 69578 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.bmm 	 paddle.bmm(Tensor([4, 81, 24807],"float32"), Tensor([4, 24807, 512],"float32"), ) 	 58842204 	 12079 	 59.677006244659424 	 59.63634991645813 	 5.049357652664185 	 5.04587984085083 	 16.801841974258423 	 16.79935908317566 	 0.7111444473266602 	 0.7112429141998291 	 
2025-08-04 21:45:20.872779 test begin: paddle.bmm(Tensor([4, 81, 7332],"float32"), Tensor([4, 7332, 1733],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([4, 81, 7332],"float32"), Tensor([4, 7332, 1733],"float32"), ) 	 53200992 	 12079 	 17.72372841835022 	 17.724918127059937 	 1.4996514320373535 	 1.4995784759521484 	 19.85524821281433 	 19.855225563049316 	 0.8399639129638672 	 0.8399388790130615 	 
2025-08-04 21:46:38.184396 test begin: paddle.bmm(Tensor([49, 1024, 1024],"float32"), Tensor([49, 1024, 12],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([49, 1024, 1024],"float32"), Tensor([49, 1024, 12],"float32"), ) 	 51982336 	 12079 	 10.00599193572998 	 9.998774528503418 	 0.8460004329681396 	 0.8460211753845215 	 11.972349882125854 	 11.971087455749512 	 0.506476879119873 	 0.5064077377319336 	 
2025-08-04 21:47:26.196144 test begin: paddle.bmm(Tensor([86, 81, 7332],"float32"), Tensor([86, 7332, 512],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([86, 81, 7332],"float32"), Tensor([86, 7332, 512],"float32"), ) 	 373917336 	 12079 	 70.52416157722473 	 70.52190065383911 	 5.9669694900512695 	 5.9669108390808105 	 101.01988244056702 	 101.00722885131836 	 4.2733213901519775 	 4.273078203201294 	 
2025-08-04 21:53:15.744268 test begin: paddle.broadcast_tensors(list[Tensor([127009, 200],"float64"),Tensor([127009, 200],"float64"),], )
[Prof] paddle.broadcast_tensors 	 paddle.broadcast_tensors(list[Tensor([127009, 200],"float64"),Tensor([127009, 200],"float64"),], ) 	 50803600 	 16272 	 9.993942260742188 	 0.1282041072845459 	 0.313828706741333 	 0.00012683868408203125 	 10.183507680892944 	 1.4397151470184326 	 0.1598834991455078 	 7.82012939453125e-05 	 
2025-08-04 21:53:42.790719 test begin: paddle.broadcast_tensors(list[Tensor([200, 127009],"float64"),Tensor([200, 127009],"float64"),], )
[Prof] paddle.broadcast_tensors 	 paddle.broadcast_tensors(list[Tensor([200, 127009],"float64"),Tensor([200, 127009],"float64"),], ) 	 50803600 	 16272 	 10.006370782852173 	 0.11487340927124023 	 0.31386566162109375 	 3.981590270996094e-05 	 10.183364152908325 	 1.301051378250122 	 0.1598670482635498 	 8.106231689453125e-05 	 
2025-08-04 21:54:09.094962 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([12700801],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4ec315e950>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:04:14.507451 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([12700801],"float64"), out_int32=True, )
W0804 22:04:15.398542 70520 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f38e2b56fe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:14:18.922968 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([12700801],"float64"), right=True, )
W0804 22:14:19.780534 70828 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f29fb4a2da0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:24:23.342392 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([4],"float64"), )
W0804 22:24:23.999796 71189 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f07e826eef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:34:27.958303 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([4],"float64"), out_int32=True, )
W0804 22:34:28.712633 71762 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([4],"float64"), out_int32=True, ) 	 25401606 	 957494 	 270.85336327552795 	 234.57019758224487 	 0.2890748977661133 	 0.2503499984741211 	 None 	 None 	 None 	 None 	 
2025-08-04 22:42:55.319550 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([4],"float64"), right=True, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa9239329b0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:53:00.039086 test begin: paddle.bucketize(Tensor([2, 25401601],"float64"), Tensor([25401601],"float64"), )
W0804 22:53:01.634552 72429 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f801954f0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:03:04.464870 test begin: paddle.bucketize(Tensor([2, 25401601],"float64"), Tensor([25401601],"float64"), out_int32=True, )
W0804 23:03:06.031620 72799 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcec1b66f80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:13:12.623815 test begin: paddle.bucketize(Tensor([2, 25401601],"float64"), Tensor([25401601],"float64"), right=True, )
W0804 23:13:14.326136 73164 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb7f3d6eda0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:23:17.123825 test begin: paddle.bucketize(Tensor([2, 4],"float64"), Tensor([2540160101],"float64"), )
W0804 23:24:07.660460 73545 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 4],"float64"), Tensor([2540160101],"float64"), ) 	 2540160109 	 957494 	 9.801128625869751 	 10.442691326141357 	 5.245208740234375e-05 	 7.319450378417969e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 23:24:47.089014 test begin: paddle.bucketize(Tensor([2, 4],"float64"), Tensor([2540160101],"float64"), right=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 4],"float64"), Tensor([2540160101],"float64"), right=True, ) 	 2540160109 	 957494 	 10.266560316085815 	 9.995729684829712 	 9.799003601074219e-05 	 7.05718994140625e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 23:26:05.780729 test begin: paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), ) 	 25402405 	 957494 	 11.453642129898071 	 15.746397495269775 	 0.00010156631469726562 	 0.0002484321594238281 	 None 	 None 	 None 	 None 	 
2025-08-04 23:26:33.554509 test begin: paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), out_int32=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), out_int32=True, ) 	 25402405 	 957494 	 10.389098167419434 	 9.812777996063232 	 9.369850158691406e-05 	 0.00023508071899414062 	 None 	 None 	 None 	 None 	 
2025-08-04 23:26:54.329580 test begin: paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), right=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), right=True, ) 	 25402405 	 957494 	 18.105003595352173 	 15.94831895828247 	 9.417533874511719e-05 	 0.00024318695068359375 	 None 	 None 	 None 	 None 	 
2025-08-04 23:27:29.138273 test begin: paddle.bucketize(Tensor([6350401, 4],"float64"), Tensor([4],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4e5b4e2c20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:37:33.746008 test begin: paddle.bucketize(Tensor([6350401, 4],"float64"), Tensor([4],"float64"), out_int32=True, )
W0804 23:37:34.603926 74079 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([6350401, 4],"float64"), Tensor([4],"float64"), out_int32=True, ) 	 25401608 	 957494 	 253.35004329681396 	 232.0685269832611 	 0.2703571319580078 	 0.2476212978363037 	 None 	 None 	 None 	 None 	 
2025-08-04 23:45:40.482453 test begin: paddle.bucketize(Tensor([6350401, 4],"float64"), Tensor([4],"float64"), right=True, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcd29702b90>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:55:47.846965 test begin: paddle.cartesian_prod(list[Tensor([20],"complex128"),Tensor([50],"complex128"),Tensor([5080],"complex128"),], )
W0804 23:55:48.071393 74821 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f880287ef20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:05:52.511349 test begin: paddle.cartesian_prod(list[Tensor([30],"complex128"),Tensor([30],"complex128"),Tensor([5080],"complex128"),], )
W0805 00:05:52.701066 75358 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f72dd686f20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:15:57.330860 test begin: paddle.cartesian_prod(list[Tensor([40],"int32"),Tensor([30],"int32"),Tensor([5080],"int32"),], )
W0805 00:15:57.540676 75964 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7efb9a426fe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:26:01.913395 test begin: paddle.cartesian_prod(list[Tensor([40],"int32"),Tensor([400],"int32"),Tensor([508],"int32"),], )
W0805 00:26:02.155355 76675 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f99d2ebafe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:36:06.328527 test begin: paddle.cast(Tensor([1, 1, 32768, 32768],"float16"), dtype=Dtype(float16), )
W0805 00:36:26.849458 77300 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cast 	 paddle.cast(Tensor([1, 1, 32768, 32768],"float16"), dtype=Dtype(float16), ) 	 1073741824 	 33565 	 108.59356451034546 	 0.07521295547485352 	 1.6532351970672607 	 5.078315734863281e-05 	 108.51876401901245 	 1.6609649658203125 	 1.652068853378296 	 7.653236389160156e-05 	 combined
2025-08-05 00:40:27.101276 test begin: paddle.cast(Tensor([128256, 793],"bfloat16"), Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([128256, 793],"bfloat16"), Dtype(float16), ) 	 101707008 	 33565 	 10.017546653747559 	 17.1220223903656 	 0.3050808906555176 	 0.5212054252624512 	 9.998019933700562 	 16.981857299804688 	 0.30440378189086914 	 0.5169713497161865 	 combined
2025-08-05 00:41:25.054848 test begin: paddle.cast(Tensor([2, 1, 1551, 32768],"float16"), dtype=Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([2, 1, 1551, 32768],"float16"), dtype=Dtype(float16), ) 	 101646336 	 33565 	 10.405997514724731 	 0.1103513240814209 	 0.31684112548828125 	 3.814697265625e-05 	 10.402780055999756 	 1.4869945049285889 	 0.31670546531677246 	 7.700920104980469e-05 	 combined
2025-08-05 00:41:51.210137 test begin: paddle.cast(Tensor([2, 1, 32768, 1551],"float16"), dtype=Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([2, 1, 32768, 1551],"float16"), dtype=Dtype(float16), ) 	 101646336 	 33565 	 10.405994892120361 	 0.06548643112182617 	 0.3168909549713135 	 2.6702880859375e-05 	 10.402944087982178 	 1.4871399402618408 	 0.3167541027069092 	 0.00017762184143066406 	 combined
2025-08-05 00:42:19.536809 test begin: paddle.cast(Tensor([2, 1, 32768, 32768],"float16"), dtype=Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([2, 1, 32768, 32768],"float16"), dtype=Dtype(float16), ) 	 2147483648 	 33565 	 217.11852526664734 	 0.06532692909240723 	 3.3055102825164795 	 2.3126602172851562e-05 	 217.2486686706543 	 1.5294735431671143 	 3.307481050491333 	 7.462501525878906e-05 	 combined
2025-08-05 00:51:02.279473 test begin: paddle.cast(Tensor([2, 1024, 50304],"float16"), dtype="float32", )
[Prof] paddle.cast 	 paddle.cast(Tensor([2, 1024, 50304],"float16"), dtype="float32", ) 	 103022592 	 33565 	 16.38771629333496 	 18.6850528717041 	 0.49889612197875977 	 0.5680904388427734 	 15.338063716888428 	 15.427418947219849 	 0.46709489822387695 	 0.4696967601776123 	 combined
2025-08-05 00:52:13.078190 test begin: paddle.cast(Tensor([33076, 3072],"bfloat16"), Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([33076, 3072],"bfloat16"), Dtype(float16), ) 	 101609472 	 33565 	 9.996445894241333 	 17.109369039535522 	 0.30439257621765137 	 0.520519495010376 	 9.994260311126709 	 16.96321129798889 	 0.3043215274810791 	 0.516503095626831 	 combined
2025-08-05 00:53:12.929473 test begin: paddle.cast(Tensor([8, 1024, 12404],"float16"), dtype="float32", )
[Prof] paddle.cast 	 paddle.cast(Tensor([8, 1024, 12404],"float16"), dtype="float32", ) 	 101613568 	 33565 	 16.16526222229004 	 18.41198444366455 	 0.492203950881958 	 0.5605945587158203 	 15.125153541564941 	 15.217128038406372 	 0.46047496795654297 	 0.4633030891418457 	 combined
2025-08-05 00:54:21.319871 test begin: paddle.cast(Tensor([8, 253, 50304],"float16"), dtype="float32", )
[Prof] paddle.cast 	 paddle.cast(Tensor([8, 253, 50304],"float16"), dtype="float32", ) 	 101815296 	 33565 	 16.187008380889893 	 18.44827699661255 	 0.49287867546081543 	 0.5614602565765381 	 15.14671540260315 	 15.24777102470398 	 0.46106624603271484 	 0.4642763137817383 	 combined
2025-08-05 00:55:29.863976 test begin: paddle.cdist(Tensor([12700801, 4],"float32"), Tensor([1, 4],"float32"), p=1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbc8604aa10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:06:01.309464 test begin: paddle.cdist(Tensor([6380, 7963],"float32"), Tensor([1, 7963],"float32"), p=1, )
W0805 01:06:02.346619 78575 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cdist 	 paddle.cdist(Tensor([6380, 7963],"float32"), Tensor([1, 7963],"float32"), p=1, ) 	 50811903 	 22208 	 10.001741886138916 	 4.306011915206909 	 0.23015880584716797 	 0.19813275337219238 	 48.5288770198822 	 26.096981525421143 	 0.7432582378387451 	 0.23984694480895996 	 
2025-08-05 01:07:31.858118 test begin: paddle.cdist(Tensor([8550, 5942],"float32"), Tensor([1, 5942],"float32"), p=1, )
[Prof] paddle.cdist 	 paddle.cdist(Tensor([8550, 5942],"float32"), Tensor([1, 5942],"float32"), p=1, ) 	 50810042 	 22208 	 10.021549463272095 	 4.294580459594727 	 0.23059725761413574 	 0.1976318359375 	 48.651824951171875 	 25.802566528320312 	 0.7451071739196777 	 0.23715996742248535 	 
2025-08-05 01:09:06.194276 test begin: paddle.cdist(Tensor([900, 56449],"float32"), Tensor([1, 56449],"float32"), p=1, )
[Prof] paddle.cdist 	 paddle.cdist(Tensor([900, 56449],"float32"), Tensor([1, 56449],"float32"), p=1, ) 	 50860549 	 22208 	 11.04416537284851 	 6.512407064437866 	 0.16267156600952148 	 0.29969286918640137 	 47.74974703788757 	 26.959914684295654 	 0.7313628196716309 	 0.31009840965270996 	 
2025-08-05 01:10:43.032196 test begin: paddle.ceil(Tensor([12404, 32, 128],"float32"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([12404, 32, 128],"float32"), ) 	 50806784 	 33827 	 10.017361164093018 	 10.069947004318237 	 0.3024601936340332 	 0.30419921875 	 4.531699895858765 	 4.539779186248779 	 0.1368546485900879 	 0.13704299926757812 	 
2025-08-05 01:11:15.235569 test begin: paddle.ceil(Tensor([141121, 6, 3, 1, 2, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([141121, 6, 3, 1, 2, 5],"float64"), ) 	 25401780 	 33827 	 10.078945636749268 	 10.087092399597168 	 0.3043835163116455 	 0.3047044277191162 	 4.52786111831665 	 4.549833297729492 	 0.13674688339233398 	 0.1375880241394043 	 
2025-08-05 01:11:47.144588 test begin: paddle.ceil(Tensor([3, 141121, 3, 4, 1, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 141121, 3, 4, 1, 5],"float64"), ) 	 25401780 	 33827 	 10.074994087219238 	 10.085460186004639 	 0.3043797016143799 	 0.304734468460083 	 4.527456045150757 	 4.5490882396698 	 0.13672184944152832 	 0.1373438835144043 	 
2025-08-05 01:12:17.524544 test begin: paddle.ceil(Tensor([3, 282241, 3, 1, 2, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 282241, 3, 1, 2, 5],"float64"), ) 	 25401690 	 33827 	 10.073941469192505 	 10.095570087432861 	 0.3043692111968994 	 0.3047757148742676 	 4.5313942432403564 	 4.550349235534668 	 0.13683414459228516 	 0.13735198974609375 	 
2025-08-05 01:12:47.921036 test begin: paddle.ceil(Tensor([3, 6, 141121, 1, 2, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 141121, 1, 2, 5],"float64"), ) 	 25401780 	 33827 	 10.074655055999756 	 10.0864577293396 	 0.30434513092041016 	 0.30477356910705566 	 4.5275349617004395 	 4.548527002334595 	 0.13671398162841797 	 0.1374373435974121 	 
2025-08-05 01:13:18.301386 test begin: paddle.ceil(Tensor([3, 6, 3, 1, 2, 235201],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 1, 2, 235201],"float64"), ) 	 25401708 	 33827 	 10.058513402938843 	 10.092202186584473 	 0.303875207901001 	 0.30477356910705566 	 4.528529167175293 	 4.552538633346558 	 0.13753128051757812 	 0.13745641708374023 	 
2025-08-05 01:13:48.697450 test begin: paddle.ceil(Tensor([3, 6, 3, 1, 94081, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 1, 94081, 5],"float64"), ) 	 25401870 	 33827 	 10.079463005065918 	 10.08628249168396 	 0.3044137954711914 	 0.30470776557922363 	 4.528740167617798 	 4.548177003860474 	 0.13675546646118164 	 0.13741755485534668 	 
2025-08-05 01:14:19.171984 test begin: paddle.ceil(Tensor([3, 6, 3, 4, 1, 117601],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 4, 1, 117601],"float64"), ) 	 25401816 	 33827 	 10.076190710067749 	 10.087152481079102 	 0.304462194442749 	 0.3047337532043457 	 4.528384447097778 	 4.549681663513184 	 0.13675403594970703 	 0.13736629486083984 	 
2025-08-05 01:14:49.559305 test begin: paddle.ceil(Tensor([3, 6, 3, 4, 23521, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 4, 23521, 5],"float64"), ) 	 25402680 	 33827 	 10.075647592544556 	 10.086544036865234 	 0.3043994903564453 	 0.30478668212890625 	 4.527966499328613 	 4.548386096954346 	 0.13672947883605957 	 0.1374189853668213 	 
2025-08-05 01:15:21.118400 test begin: paddle.ceil(Tensor([3, 6, 3, 47041, 2, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 47041, 2, 5],"float64"), ) 	 25402140 	 33827 	 10.074650287628174 	 10.446335315704346 	 0.3043828010559082 	 0.3048274517059326 	 4.531567096710205 	 4.549905776977539 	 0.1367194652557373 	 0.13733816146850586 	 
2025-08-05 01:15:54.537424 test begin: paddle.ceil(Tensor([3, 6, 3, 94081, 1, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 94081, 1, 5],"float64"), ) 	 25401870 	 33827 	 10.076364040374756 	 10.853266954421997 	 0.3044438362121582 	 0.3047795295715332 	 4.532287836074829 	 4.548269748687744 	 0.13675260543823242 	 0.13736605644226074 	 
2025-08-05 01:16:28.441100 test begin: paddle.ceil(Tensor([3, 6, 70561, 4, 1, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 70561, 4, 1, 5],"float64"), ) 	 25401960 	 33827 	 10.058918714523315 	 10.100763082504272 	 0.30393457412719727 	 0.3047177791595459 	 4.531284809112549 	 4.548479080200195 	 0.13678383827209473 	 0.1373889446258545 	 
2025-08-05 01:16:59.077258 test begin: paddle.ceil(Tensor([32, 12404, 128],"float32"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([32, 12404, 128],"float32"), ) 	 50806784 	 33827 	 10.011688470840454 	 11.542595863342285 	 0.3024568557739258 	 0.3042623996734619 	 4.5353639125823975 	 4.536545038223267 	 0.136824369430542 	 0.1376638412475586 	 
2025-08-05 01:17:33.817818 test begin: paddle.ceil(Tensor([32, 32, 49613],"float32"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([32, 32, 49613],"float32"), ) 	 50803712 	 33827 	 10.006857872009277 	 10.075016260147095 	 0.30225300788879395 	 0.30422377586364746 	 4.531904935836792 	 4.535656452178955 	 0.13683247566223145 	 0.13697075843811035 	 
2025-08-05 01:18:05.775379 test begin: paddle.ceil(Tensor([70561, 6, 3, 4, 1, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([70561, 6, 3, 4, 1, 5],"float64"), ) 	 25401960 	 33827 	 10.058829069137573 	 10.086313486099243 	 0.30390167236328125 	 0.3046226501464844 	 4.527732610702515 	 4.549142599105835 	 0.13675618171691895 	 0.13741493225097656 	 
2025-08-05 01:18:36.343999 test begin: paddle.chunk(Tensor([115, 216, 64, 64],"float16"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([115, 216, 64, 64],"float16"), 3, axis=1, ) 	 101744640 	 29583 	 12.78149127960205 	 0.22249841690063477 	 0.44116735458374023 	 4.0531158447265625e-05 	 9.12041974067688 	 14.91913366317749 	 0.31504154205322266 	 0.5154399871826172 	 
2025-08-05 01:19:19.289982 test begin: paddle.chunk(Tensor([16, 128, 24807],"float32"), 2, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([16, 128, 24807],"float32"), 2, axis=1, ) 	 50804736 	 29583 	 9.981349468231201 	 0.18901920318603516 	 0.34463000297546387 	 2.7418136596679688e-05 	 9.256559610366821 	 9.08984375 	 0.31987738609313965 	 0.31400275230407715 	 
2025-08-05 01:19:54.082583 test begin: paddle.chunk(Tensor([16, 128, 25500],"float32"), 2, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([16, 128, 25500],"float32"), 2, axis=1, ) 	 52224000 	 29583 	 10.44673776626587 	 0.19195032119750977 	 0.3604307174682617 	 2.7418136596679688e-05 	 9.464011430740356 	 9.319705486297607 	 0.3270132541656494 	 0.32190608978271484 	 
2025-08-05 01:20:25.344966 test begin: paddle.chunk(Tensor([4, 216, 1838, 64],"float16"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([4, 216, 1838, 64],"float16"), 3, axis=1, ) 	 101634048 	 29583 	 12.701977968215942 	 0.3752005100250244 	 0.4389224052429199 	 6.031990051269531e-05 	 9.130021333694458 	 14.84506869316101 	 0.3153719902038574 	 0.5128026008605957 	 
2025-08-05 01:21:07.035292 test begin: paddle.chunk(Tensor([4, 216, 64, 1838],"float16"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([4, 216, 64, 1838],"float16"), 3, axis=1, ) 	 101634048 	 29583 	 12.727550983428955 	 0.2256169319152832 	 0.4387624263763428 	 2.7894973754882812e-05 	 9.130236864089966 	 14.841913938522339 	 0.3153107166290283 	 0.5127458572387695 	 
2025-08-05 01:21:50.081570 test begin: paddle.chunk(Tensor([4, 216, 64, 919],"float32"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([4, 216, 64, 919],"float32"), 3, axis=1, ) 	 50817024 	 29583 	 9.964709281921387 	 0.23139047622680664 	 0.3441956043243408 	 7.748603820800781e-05 	 9.130029201507568 	 9.019779682159424 	 0.31532883644104004 	 0.3116185665130615 	 
2025-08-05 01:22:20.694048 test begin: paddle.chunk(Tensor([4, 216, 919, 64],"float32"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([4, 216, 919, 64],"float32"), 3, axis=1, ) 	 50817024 	 29583 	 9.965772867202759 	 0.37532687187194824 	 0.34421539306640625 	 0.0002484321594238281 	 9.129660367965698 	 9.02029299736023 	 0.31529974937438965 	 0.31163835525512695 	 
2025-08-05 01:22:52.254683 test begin: paddle.chunk(Tensor([58, 216, 64, 64],"float32"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([58, 216, 64, 64],"float32"), 3, axis=1, ) 	 51314688 	 29583 	 10.18609094619751 	 0.2256016731262207 	 0.3518648147583008 	 3.0279159545898438e-05 	 9.197596788406372 	 9.121864557266235 	 0.3178417682647705 	 0.3151583671569824 	 
2025-08-05 01:23:25.079716 test begin: paddle.clip(Tensor([1408, 36082],"float32"), min=-2, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([1408, 36082],"float32"), min=-2, max=2, ) 	 50803456 	 33866 	 10.007930040359497 	 10.089568614959717 	 0.3020191192626953 	 0.304243803024292 	 15.243441104888916 	 24.86249351501465 	 0.45995259284973145 	 0.15006351470947266 	 
2025-08-05 01:24:28.159445 test begin: paddle.clip(Tensor([2, 3840, 10240],"float32"), 0, 255, )
[Prof] paddle.clip 	 paddle.clip(Tensor([2, 3840, 10240],"float32"), 0, 255, ) 	 78643200 	 33866 	 15.426110982894897 	 15.505763053894043 	 0.46544337272644043 	 0.46793508529663086 	 23.503634929656982 	 37.90100646018982 	 0.7092905044555664 	 0.2288074493408203 	 
2025-08-05 01:26:03.197518 test begin: paddle.clip(Tensor([23, 17, 256, 256],"float64"), min=0, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([23, 17, 256, 256],"float64"), min=0, max=2, ) 	 25624576 	 33866 	 10.182305812835693 	 10.187126159667969 	 0.3072831630706787 	 0.30742907524108887 	 15.329231023788452 	 24.562374114990234 	 0.4623751640319824 	 0.14832615852355957 	 
2025-08-05 01:27:07.122486 test begin: paddle.clip(Tensor([24, 17, 244, 256],"float64"), min=0, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([24, 17, 244, 256],"float64"), min=0, max=2, ) 	 25485312 	 33866 	 10.128955841064453 	 10.13292908668518 	 0.30563807487487793 	 0.30575037002563477 	 15.225937604904175 	 24.43658995628357 	 0.4595794677734375 	 0.14754915237426758 	 
2025-08-05 01:28:08.198177 test begin: paddle.clip(Tensor([24, 17, 256, 244],"float64"), min=0, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([24, 17, 256, 244],"float64"), min=0, max=2, ) 	 25485312 	 33866 	 10.129106283187866 	 10.133127689361572 	 0.30566883087158203 	 0.30577874183654785 	 15.223734617233276 	 24.436293601989746 	 0.4593484401702881 	 0.14757132530212402 	 
2025-08-05 01:29:09.314008 test begin: paddle.clip(Tensor([24, 17, 256, 256],"float64"), min=0, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([24, 17, 256, 256],"float64"), min=0, max=2, ) 	 26738688 	 33866 	 10.627548456192017 	 11.363329648971558 	 0.3206920623779297 	 0.32064390182495117 	 15.965806007385254 	 25.58176565170288 	 0.4818713665008545 	 0.15447521209716797 	 
2025-08-05 01:30:17.780557 test begin: paddle.clip(Tensor([3, 1654, 10240],"float32"), 0, 255, )
[Prof] paddle.clip 	 paddle.clip(Tensor([3, 1654, 10240],"float32"), 0, 255, ) 	 50810880 	 33866 	 10.022213220596313 	 10.097564220428467 	 0.3021836280822754 	 0.30426454544067383 	 15.2468101978302 	 24.820841312408447 	 0.46007275581359863 	 0.14980101585388184 	 
2025-08-05 01:31:21.251313 test begin: paddle.clip(Tensor([3, 3840, 4411],"float32"), 0, 255, )
[Prof] paddle.clip 	 paddle.clip(Tensor([3, 3840, 4411],"float32"), 0, 255, ) 	 50814720 	 33866 	 10.887541770935059 	 10.083701610565186 	 0.30210137367248535 	 0.3042726516723633 	 15.248616695404053 	 24.870599031448364 	 0.4601273536682129 	 0.15029025077819824 	 
2025-08-05 01:32:24.853135 test begin: paddle.clip(Tensor([8269, 6144],"float32"), min=-2, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([8269, 6144],"float32"), min=-2, max=2, ) 	 50804736 	 33866 	 11.041718244552612 	 10.091761350631714 	 0.3021845817565918 	 0.30422091484069824 	 15.245681285858154 	 24.816791534423828 	 0.4601120948791504 	 0.14980220794677734 	 
2025-08-05 01:33:29.428985 test begin: paddle.clone(Tensor([145, 12, 112, 261],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([145, 12, 112, 261],"float32"), ) 	 50863680 	 32432 	 10.227984428405762 	 10.168825387954712 	 0.161118745803833 	 0.16012907028198242 	 10.16530466079712 	 1.6477382183074951 	 0.16016674041748047 	 8.177757263183594e-05 	 
2025-08-05 01:34:03.971378 test begin: paddle.clone(Tensor([145, 12, 261, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([145, 12, 261, 112],"float32"), ) 	 50863680 	 32432 	 10.226686716079712 	 10.163975954055786 	 0.16112732887268066 	 0.16012954711914062 	 10.16527271270752 	 1.6968345642089844 	 0.16016459465026855 	 0.00018930435180664062 	 
2025-08-05 01:34:38.408436 test begin: paddle.clone(Tensor([145, 28, 112, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([145, 28, 112, 112],"float32"), ) 	 50928640 	 32432 	 10.557146072387695 	 10.077918529510498 	 0.3175489902496338 	 0.3176097869873047 	 10.080166101455688 	 1.6605572700500488 	 0.3176686763763428 	 9.465217590332031e-05 	 
2025-08-05 01:35:13.410902 test begin: paddle.clone(Tensor([22, 185, 112, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([22, 185, 112, 112],"float32"), ) 	 51054080 	 32432 	 10.205427169799805 	 11.374040126800537 	 0.1607649326324463 	 0.16065239906311035 	 10.205124855041504 	 1.6121976375579834 	 0.16080093383789062 	 9.465217590332031e-05 	 
2025-08-05 01:35:50.280635 test begin: paddle.clone(Tensor([22, 64, 112, 323],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([22, 64, 112, 323],"float32"), ) 	 50935808 	 32432 	 10.075791358947754 	 10.079407691955566 	 0.3174707889556885 	 0.31762051582336426 	 10.082109928131104 	 1.654569149017334 	 0.3177201747894287 	 6.937980651855469e-05 	 
2025-08-05 01:36:23.975049 test begin: paddle.clone(Tensor([22, 64, 323, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([22, 64, 323, 112],"float32"), ) 	 50935808 	 32432 	 10.076931953430176 	 10.080674886703491 	 0.31752753257751465 	 0.3176286220550537 	 10.082082033157349 	 1.6039247512817383 	 0.317659854888916 	 8.225440979003906e-05 	 
2025-08-05 01:36:58.779109 test begin: paddle.clone(Tensor([338, 12, 112, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([338, 12, 112, 112],"float32"), ) 	 50878464 	 32432 	 10.06360411643982 	 10.068765878677368 	 0.31709837913513184 	 0.31733274459838867 	 10.070510387420654 	 1.6220529079437256 	 0.3173336982727051 	 7.128715515136719e-05 	 
2025-08-05 01:37:32.343431 test begin: paddle.clone(Tensor([43, 256, 56, 83],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([43, 256, 56, 83],"float32"), ) 	 51165184 	 32432 	 10.124087810516357 	 10.12316083908081 	 0.3190433979034424 	 0.3190116882324219 	 10.127903699874878 	 1.6540484428405762 	 0.31915926933288574 	 7.677078247070312e-05 	 
2025-08-05 01:38:06.110870 test begin: paddle.clone(Tensor([43, 256, 83, 56],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([43, 256, 83, 56],"float32"), ) 	 51165184 	 32432 	 10.124193668365479 	 10.123398542404175 	 0.3190114498138428 	 0.31897735595703125 	 10.127827882766724 	 1.6598234176635742 	 0.3191494941711426 	 7.581710815429688e-05 	 
2025-08-05 01:38:41.383299 test begin: paddle.clone(Tensor([43, 377, 56, 56],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([43, 377, 56, 56],"float32"), ) 	 50837696 	 32432 	 10.231541872024536 	 10.146171569824219 	 0.16118431091308594 	 0.1598339080810547 	 10.152876853942871 	 1.6748647689819336 	 0.15996861457824707 	 9.512901306152344e-05 	 
2025-08-05 01:39:15.315675 test begin: paddle.clone(Tensor([64, 256, 56, 56],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([64, 256, 56, 56],"float32"), ) 	 51380224 	 32432 	 10.704762935638428 	 11.693780422210693 	 0.3202221393585205 	 0.32048892974853516 	 10.167473793029785 	 1.5894923210144043 	 0.32038354873657227 	 9.489059448242188e-05 	 
2025-08-05 01:39:52.339530 test begin: paddle.clone(Tensor([64, 64, 112, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([64, 64, 112, 112],"float32"), ) 	 51380224 	 32432 	 10.162330865859985 	 10.173458337783813 	 0.320239782333374 	 0.3204307556152344 	 10.167213916778564 	 1.5765607357025146 	 0.3203761577606201 	 8.320808410644531e-05 	 
2025-08-05 01:40:27.503953 test begin: paddle.column_stack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], ) 	 76204872 	 32438 	 30.073491096496582 	 29.949078798294067 	 0.9475116729736328 	 0.9435024261474609 	 30.111319065093994 	 2.2519991397857666 	 0.9487848281860352 	 7.343292236328125e-05 	 
2025-08-05 01:42:07.261713 test begin: paddle.column_stack(list[Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2, 1058401],"float64"),], ) 	 25401624 	 32438 	 10.222819089889526 	 10.14992618560791 	 0.32202649116516113 	 0.15991735458374023 	 10.085091590881348 	 1.7948272228240967 	 0.31772661209106445 	 7.605552673339844e-05 	 
2025-08-05 01:42:41.567707 test begin: paddle.column_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401870 	 32438 	 10.234987020492554 	 10.423157691955566 	 0.322293758392334 	 0.32849860191345215 	 10.13797378540039 	 2.5708560943603516 	 0.3194692134857178 	 7.677078247070312e-05 	 
2025-08-05 01:43:16.368694 test begin: paddle.column_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401870 	 32438 	 10.237382650375366 	 10.421689510345459 	 0.32248973846435547 	 0.32784032821655273 	 10.155396699905396 	 2.345066547393799 	 0.31995606422424316 	 7.915496826171875e-05 	 
2025-08-05 01:43:51.128935 test begin: paddle.column_stack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], ) 	 76204836 	 32438 	 29.987183570861816 	 29.90282440185547 	 0.9449102878570557 	 0.9422833919525146 	 30.18754267692566 	 2.2371034622192383 	 0.9509248733520508 	 6.985664367675781e-05 	 
2025-08-05 01:45:26.874829 test begin: paddle.column_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 25401654 	 32438 	 10.26203465461731 	 10.322627067565918 	 0.32329535484313965 	 0.3254663944244385 	 10.161972045898438 	 2.3827648162841797 	 0.32013463973999023 	 0.00018358230590820312 	 
2025-08-05 01:46:01.433325 test begin: paddle.column_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401654 	 32438 	 10.247421741485596 	 10.334070444107056 	 0.322817325592041 	 0.32543039321899414 	 10.146622896194458 	 2.3946616649627686 	 0.3196704387664795 	 9.322166442871094e-05 	 
2025-08-05 01:46:38.163900 test begin: paddle.column_stack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 76204980 	 32438 	 30.02818012237549 	 30.11643624305725 	 0.9462399482727051 	 0.9486744403839111 	 30.19431471824646 	 2.544217109680176 	 0.9514343738555908 	 6.842613220214844e-05 	 
2025-08-05 01:48:16.264973 test begin: paddle.column_stack(list[Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401660 	 32438 	 10.209859132766724 	 10.159808874130249 	 0.3217315673828125 	 0.15987586975097656 	 10.102628707885742 	 1.8886675834655762 	 0.31822991371154785 	 6.937980651855469e-05 	 
2025-08-05 01:48:50.885996 test begin: paddle.column_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401654 	 32438 	 10.264284372329712 	 10.111649990081787 	 0.32302308082580566 	 0.318676233291626 	 10.155637264251709 	 2.4520184993743896 	 0.319899320602417 	 8.630752563476562e-05 	 
2025-08-05 01:49:25.016373 test begin: paddle.column_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 76204818 	 32438 	 30.16646409034729 	 30.06909441947937 	 0.9505341053009033 	 0.9473006725311279 	 30.65495491027832 	 2.304163694381714 	 0.9656991958618164 	 6.890296936035156e-05 	 
2025-08-05 01:51:01.598410 test begin: paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401870 	 32438 	 10.268092393875122 	 10.230331659317017 	 0.3235011100769043 	 0.32216644287109375 	 10.130844831466675 	 2.343724250793457 	 0.31913232803344727 	 7.081031799316406e-05 	 
2025-08-05 01:51:37.662376 test begin: paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 76204890 	 32438 	 30.218419313430786 	 30.403533458709717 	 0.9518711566925049 	 0.9574790000915527 	 30.55374526977539 	 2.329789161682129 	 0.9626250267028809 	 7.43865966796875e-05 	 
2025-08-05 01:53:15.858108 test begin: paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401630 	 32438 	 10.260840892791748 	 11.69537091255188 	 0.3232259750366211 	 0.15987300872802734 	 10.136551141738892 	 1.788666009902954 	 0.31934380531311035 	 8.940696716308594e-05 	 
2025-08-05 01:53:51.957975 test begin: paddle.column_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], ) 	 76204824 	 32438 	 31.555187940597534 	 45.244688272476196 	 0.9942584037780762 	 1.4255287647247314 	 31.118587732315063 	 2.245697259902954 	 0.9805216789245605 	 7.987022399902344e-05 	 
2025-08-05 01:55:45.469539 test begin: paddle.column_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 76204920 	 32438 	 30.1263108253479 	 33.43295335769653 	 0.9491126537322998 	 1.0532417297363281 	 30.420149326324463 	 2.477306842803955 	 0.958423376083374 	 8.320808410644531e-05 	 
2025-08-05 01:57:25.278673 test begin: paddle.column_stack(list[Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401640 	 32438 	 10.002383708953857 	 10.155506134033203 	 0.315152645111084 	 0.15987372398376465 	 10.617413759231567 	 1.7792446613311768 	 0.33452558517456055 	 7.224082946777344e-05 	 
2025-08-05 01:57:59.636528 test begin: paddle.combinations(Tensor([2540160101],"int64"), 0, True, )
[Prof] paddle.combinations 	 paddle.combinations(Tensor([2540160101],"int64"), 0, True, ) 	 2540160101 	 804184 	 9.776270627975464 	 5.5808799266815186 	 8.559226989746094e-05 	 7.677078247070312e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 01:59:23.477096 test begin: paddle.combinations(Tensor([50803201],"int32"), 1, True, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fabcc297eb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 02:09:28.037664 test begin: paddle.complex(Tensor([1, 793801, 64],"float32"), Tensor([1, 793801, 64],"float32"), )
W0805 02:09:29.767709 80956 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.complex 	 paddle.complex(Tensor([1, 793801, 64],"float32"), Tensor([1, 793801, 64],"float32"), ) 	 101606528 	 20670 	 12.221860408782959 	 12.14820384979248 	 0.6043312549591064 	 0.6005313396453857 	 12.186278104782104 	 1.4216036796569824 	 0.602487325668335 	 7.510185241699219e-05 	 
2025-08-05 02:10:13.240839 test begin: paddle.complex(Tensor([1, 8192, 6202],"float32"), Tensor([1, 8192, 6202],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([1, 8192, 6202],"float32"), Tensor([1, 8192, 6202],"float32"), ) 	 101613568 	 20670 	 12.224420309066772 	 12.16607403755188 	 0.6042866706848145 	 0.6006505489349365 	 12.191858053207397 	 1.4079844951629639 	 0.6028110980987549 	 6.937980651855469e-05 	 
2025-08-05 02:10:56.093290 test begin: paddle.complex(Tensor([1, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([1, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), ) 	 51380224 	 20670 	 10.14094614982605 	 9.76966667175293 	 0.5014925003051758 	 0.4829833507537842 	 10.664602518081665 	 5.9354870319366455 	 0.5272872447967529 	 0.29352784156799316 	 
2025-08-05 02:11:34.656756 test begin: paddle.complex(Tensor([20, 2417, 1051],"float32"), Tensor([20, 2417, 1051],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([20, 2417, 1051],"float32"), Tensor([20, 2417, 1051],"float32"), ) 	 101610680 	 20670 	 12.245654821395874 	 12.148000001907349 	 0.6048133373260498 	 0.6006240844726562 	 12.190374612808228 	 1.401677131652832 	 0.6027853488922119 	 7.534027099609375e-05 	 
2025-08-05 02:12:17.654868 test begin: paddle.complex(Tensor([20, 2538, 1001],"float32"), Tensor([20, 2538, 1001],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([20, 2538, 1001],"float32"), Tensor([20, 2538, 1001],"float32"), ) 	 101621520 	 20670 	 12.219968557357788 	 12.149159908294678 	 0.6042091846466064 	 0.6007287502288818 	 12.195171594619751 	 1.5299160480499268 	 0.6029946804046631 	 7.200241088867188e-05 	 
2025-08-05 02:12:58.598575 test begin: paddle.complex(Tensor([20, 64, 39691],"float32"), Tensor([20, 64, 39691],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([20, 64, 39691],"float32"), Tensor([20, 64, 39691],"float32"), ) 	 101608960 	 20670 	 12.233320951461792 	 12.147808074951172 	 0.6048345565795898 	 0.6005511283874512 	 12.19465947151184 	 1.4063761234283447 	 0.6028399467468262 	 0.00017380714416503906 	 
2025-08-05 02:13:42.336458 test begin: paddle.complex(Tensor([756, 64, 1051],"float32"), Tensor([756, 64, 1051],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([756, 64, 1051],"float32"), Tensor([756, 64, 1051],"float32"), ) 	 101703168 	 20670 	 12.24060583114624 	 12.158041000366211 	 0.604816198348999 	 0.6011111736297607 	 12.197357177734375 	 1.3749339580535889 	 0.6030721664428711 	 9.083747863769531e-05 	 
2025-08-05 02:14:23.263676 test begin: paddle.complex(Tensor([794, 64, 1001],"float32"), Tensor([794, 64, 1001],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([794, 64, 1001],"float32"), Tensor([794, 64, 1001],"float32"), ) 	 101733632 	 20670 	 12.238489866256714 	 12.16332721710205 	 0.6050193309783936 	 0.6012897491455078 	 12.202860116958618 	 1.4991636276245117 	 0.6032660007476807 	 9.34600830078125e-05 	 
2025-08-05 02:15:08.876375 test begin: paddle.complex(Tensor([97, 8192, 64],"float32"), Tensor([1, 8192, 64],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([97, 8192, 64],"float32"), Tensor([1, 8192, 64],"float32"), ) 	 51380224 	 20670 	 9.946207284927368 	 9.76894211769104 	 0.4917762279510498 	 0.48301196098327637 	 10.56355595588684 	 5.93713641166687 	 0.5222978591918945 	 0.29355645179748535 	 
2025-08-05 02:15:47.136250 test begin: paddle.complex(Tensor([97, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([97, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), ) 	 101711872 	 20670 	 12.2409188747406 	 12.169107675552368 	 0.6052613258361816 	 0.6011731624603271 	 12.197092533111572 	 1.3839068412780762 	 0.6030294895172119 	 7.05718994140625e-05 	 
2025-08-05 02:16:27.926382 test begin: paddle.concat(list[Tensor([101606401],"bfloat16"),], )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([101606401],"bfloat16"),], ) 	 101606401 	 31651 	 9.976180076599121 	 9.9084792137146 	 0.16107511520385742 	 0.15996575355529785 	 19.574631690979004 	 14.353284358978271 	 0.31598734855651855 	 0.4634392261505127 	 
2025-08-05 02:17:26.392784 test begin: paddle.concat(list[Tensor([254, 32, 112, 112],"float16"),Tensor([254, 32, 112, 112],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([254, 32, 112, 112],"float16"),Tensor([254, 32, 112, 112],"float16"),], axis=1, ) 	 203915264 	 31651 	 19.320409774780273 	 28.63115358352661 	 0.6238112449645996 	 0.9246325492858887 	 29.583542823791504 	 1.9712152481079102 	 0.955237627029419 	 7.104873657226562e-05 	 
2025-08-05 02:18:53.527506 test begin: paddle.concat(list[Tensor([512, 16, 112, 112],"float16"),Tensor([512, 16, 112, 112],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([512, 16, 112, 112],"float16"),Tensor([512, 16, 112, 112],"float16"),], axis=1, ) 	 205520896 	 31651 	 19.442481994628906 	 28.88101315498352 	 0.6276211738586426 	 0.9326088428497314 	 29.59241223335266 	 2.0097098350524902 	 0.9554874897003174 	 7.152557373046875e-05 	 
2025-08-05 02:20:21.101112 test begin: paddle.concat(list[Tensor([512, 16, 112, 112],"float16"),Tensor([512, 32, 112, 112],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([512, 16, 112, 112],"float16"),Tensor([512, 32, 112, 112],"float16"),], axis=1, ) 	 308281344 	 31651 	 29.185076236724854 	 50.87695336341858 	 0.9423997402191162 	 1.6427009105682373 	 44.98470377922058 	 1.968155860900879 	 1.4523625373840332 	 7.367134094238281e-05 	 
2025-08-05 02:22:42.353520 test begin: paddle.concat(list[Tensor([512, 32, 112, 112],"float16"),Tensor([512, 16, 112, 112],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([512, 32, 112, 112],"float16"),Tensor([512, 16, 112, 112],"float16"),], axis=1, ) 	 308281344 	 31651 	 29.22867202758789 	 48.707329988479614 	 0.9437923431396484 	 1.5727002620697021 	 45.03967547416687 	 1.9630756378173828 	 1.4542315006256104 	 7.176399230957031e-05 	 
2025-08-05 02:24:58.734250 test begin: paddle.concat(list[Tensor([512, 32, 112, 56],"float16"),Tensor([512, 32, 112, 56],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([512, 32, 112, 56],"float16"),Tensor([512, 32, 112, 56],"float16"),], axis=1, ) 	 205520896 	 31651 	 19.429311752319336 	 28.909831762313843 	 0.6274023056030273 	 0.9334061145782471 	 29.583231687545776 	 1.9583675861358643 	 0.9551942348480225 	 0.0001983642578125 	 
2025-08-05 02:26:28.173897 test begin: paddle.concat(list[Tensor([512, 32, 56, 112],"float16"),Tensor([512, 32, 56, 112],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([512, 32, 56, 112],"float16"),Tensor([512, 32, 56, 112],"float16"),], axis=1, ) 	 205520896 	 31651 	 19.46462106704712 	 28.88982343673706 	 0.6284079551696777 	 0.9326727390289307 	 29.589876174926758 	 1.9652924537658691 	 0.9553816318511963 	 7.152557373046875e-05 	 
2025-08-05 02:27:55.830912 test begin: paddle.conj(Tensor([2, 20, 2, 635041],"float32"), )
[Prof] paddle.conj 	 paddle.conj(Tensor([2, 20, 2, 635041],"float32"), ) 	 50803280 	 32583 	 10.016996145248413 	 0.06575584411621094 	 0.31417012214660645 	 2.2649765014648438e-05 	 9.992768049240112 	 1.4807705879211426 	 0.31343746185302734 	 7.176399230957031e-05 	 
2025-08-05 02:28:20.319455 test begin: paddle.conj(Tensor([2, 20, 423361, 3],"float32"), )
[Prof] paddle.conj 	 paddle.conj(Tensor([2, 20, 423361, 3],"float32"), ) 	 50803320 	 32583 	 10.0148184299469 	 0.0854804515838623 	 0.31413841247558594 	 2.4557113647460938e-05 	 9.991063833236694 	 1.5103509426116943 	 0.3133828639984131 	 7.581710815429688e-05 	 
2025-08-05 02:28:44.199428 test begin: paddle.conj(Tensor([2, 4233601, 2, 3],"float32"), )
[Prof] paddle.conj 	 paddle.conj(Tensor([2, 4233601, 2, 3],"float32"), ) 	 50803212 	 32583 	 10.05509090423584 	 0.06644868850708008 	 0.31539034843444824 	 1.9073486328125e-05 	 9.99080491065979 	 1.5271196365356445 	 0.3133511543273926 	 7.152557373046875e-05 	 
2025-08-05 02:29:07.592096 test begin: paddle.conj(Tensor([423361, 20, 2, 3],"float32"), )
[Prof] paddle.conj 	 paddle.conj(Tensor([423361, 20, 2, 3],"float32"), ) 	 50803320 	 32583 	 10.014782190322876 	 0.06655073165893555 	 0.31410813331604004 	 2.002716064453125e-05 	 9.990918159484863 	 1.4848544597625732 	 0.31334400177001953 	 9.679794311523438e-05 	 
2025-08-05 02:29:31.886037 test begin: paddle.copysign(Tensor([12, 1058401, 2],"float64"), Tensor([12, 1058401, 2],"float64"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([12, 1058401, 2],"float64"), Tensor([12, 1058401, 2],"float64"), ) 	 50803248 	 22302 	 10.00133752822876 	 9.873945474624634 	 0.4585998058319092 	 0.4524655342102051 	 16.574337482452393 	 33.7939977645874 	 0.7594363689422607 	 0.30977725982666016 	 
2025-08-05 02:30:44.663400 test begin: paddle.copysign(Tensor([12, 20, 105841],"float64"), Tensor([12, 20, 105841],"float64"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([12, 20, 105841],"float64"), Tensor([12, 20, 105841],"float64"), ) 	 50803680 	 22302 	 10.003751993179321 	 9.874238014221191 	 0.4584653377532959 	 0.45245981216430664 	 16.5833580493927 	 33.795923471450806 	 0.7599165439605713 	 0.30985546112060547 	 
2025-08-05 02:31:56.617266 test begin: paddle.copysign(Tensor([12, 20, 211681],"float32"), Tensor([12, 20, 211681],"float32"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([12, 20, 211681],"float32"), Tensor([12, 20, 211681],"float32"), ) 	 101606880 	 22302 	 10.0520658493042 	 10.301798105239868 	 0.4604222774505615 	 0.4562528133392334 	 26.809750080108643 	 34.62705039978027 	 1.2287144660949707 	 0.3174738883972168 	 
2025-08-05 02:33:23.178444 test begin: paddle.copysign(Tensor([12, 2116801, 2],"float32"), Tensor([12, 2116801, 2],"float32"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([12, 2116801, 2],"float32"), Tensor([12, 2116801, 2],"float32"), ) 	 101606448 	 22302 	 10.044336318969727 	 11.87661623954773 	 0.46029210090637207 	 0.4563114643096924 	 26.856313705444336 	 34.617737770080566 	 1.230926513671875 	 0.31741833686828613 	 
2025-08-05 02:34:51.200862 test begin: paddle.copysign(Tensor([1270081, 20, 2],"float32"), Tensor([1270081, 20, 2],"float32"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([1270081, 20, 2],"float32"), Tensor([1270081, 20, 2],"float32"), ) 	 101606480 	 22302 	 10.044933319091797 	 9.956244945526123 	 0.46030569076538086 	 0.45622706413269043 	 26.817636489868164 	 34.61847901344299 	 1.2290513515472412 	 0.31742238998413086 	 
2025-08-05 02:36:18.831066 test begin: paddle.copysign(Tensor([635041, 20, 2],"float64"), Tensor([635041, 20, 2],"float64"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([635041, 20, 2],"float64"), Tensor([635041, 20, 2],"float64"), ) 	 50803280 	 22302 	 10.007612228393555 	 9.87395191192627 	 0.45848822593688965 	 0.45245909690856934 	 16.5646014213562 	 33.79321002960205 	 0.7590601444244385 	 0.3098180294036865 	 
2025-08-05 02:37:33.699470 test begin: paddle.cos(Tensor([1587601, 32],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([1587601, 32],"float32"), ) 	 50803232 	 33844 	 10.00597858428955 	 10.084561347961426 	 0.3020050525665283 	 0.3045504093170166 	 15.237968444824219 	 35.223143100738525 	 0.46018075942993164 	 0.35454368591308594 	 
2025-08-05 02:38:48.000829 test begin: paddle.cos(Tensor([198451, 256],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([198451, 256],"float32"), ) 	 50803456 	 33844 	 10.003972053527832 	 10.085040807723999 	 0.30196261405944824 	 0.30449390411376953 	 15.237706422805786 	 35.22385907173157 	 0.4600830078125 	 0.35462474822998047 	 
2025-08-05 02:40:00.261525 test begin: paddle.cos(Tensor([32768, 1551],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([32768, 1551],"float32"), ) 	 50823168 	 33844 	 10.001136541366577 	 10.08846378326416 	 0.3020029067993164 	 0.3046455383300781 	 15.242378950119019 	 35.23642349243164 	 0.4603118896484375 	 0.35471487045288086 	 
2025-08-05 02:41:15.530043 test begin: paddle.cos(Tensor([396901, 128],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([396901, 128],"float32"), ) 	 50803328 	 33844 	 10.009899377822876 	 10.091635942459106 	 0.30214881896972656 	 0.3045380115509033 	 15.238788366317749 	 35.22356843948364 	 0.46021199226379395 	 0.35455775260925293 	 
2025-08-05 02:42:27.971772 test begin: paddle.cos(Tensor([5000, 10161],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([5000, 10161],"float32"), ) 	 50805000 	 33844 	 10.01087737083435 	 10.084985256195068 	 0.3022897243499756 	 0.3045029640197754 	 15.242355585098267 	 35.22479748725891 	 0.4603114128112793 	 0.35466647148132324 	 
2025-08-05 02:43:42.466069 test begin: paddle.cos(Tensor([8192, 6202],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([8192, 6202],"float32"), ) 	 50806784 	 33844 	 10.008528232574463 	 10.085310220718384 	 0.3021535873413086 	 0.3045384883880615 	 15.242347478866577 	 35.225274324417114 	 0.46027421951293945 	 0.35459423065185547 	 
2025-08-05 02:44:54.725226 test begin: paddle.cosh(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33925 	 10.01025915145874 	 10.1256422996521 	 0.30156445503234863 	 0.30504608154296875 	 15.27398681640625 	 25.211372137069702 	 0.4601294994354248 	 0.37975525856018066 	 
2025-08-05 02:45:57.814408 test begin: paddle.cosh(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33925 	 10.010100841522217 	 10.125562191009521 	 0.3015317916870117 	 0.30501866340637207 	 15.274106979370117 	 25.21028780937195 	 0.4602665901184082 	 0.3766491413116455 	 
2025-08-05 02:47:00.775441 test begin: paddle.cosh(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33925 	 10.010758876800537 	 10.132567167282104 	 0.30155467987060547 	 0.30502748489379883 	 15.273306369781494 	 25.210644483566284 	 0.4599723815917969 	 0.37970566749572754 	 
2025-08-05 02:48:03.314641 test begin: paddle.cosh(Tensor([28, 32, 241, 241],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([28, 32, 241, 241],"float32"), ) 	 52040576 	 33925 	 10.245182991027832 	 10.372662782669067 	 0.3086073398590088 	 0.31249189376831055 	 15.634913682937622 	 25.822736501693726 	 0.47106170654296875 	 0.38897013664245605 	 
2025-08-05 02:49:07.132005 test begin: paddle.cosh(Tensor([8, 110, 241, 241],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([8, 110, 241, 241],"float32"), ) 	 51111280 	 33925 	 10.063961744308472 	 10.187047958374023 	 0.3031768798828125 	 0.3068201541900635 	 15.370605707168579 	 25.363258838653564 	 0.4629201889038086 	 0.38202786445617676 	 
2025-08-05 02:50:09.852567 test begin: paddle.cosh(Tensor([8, 32, 241, 824],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([8, 32, 241, 824],"float32"), ) 	 50837504 	 33925 	 10.004984617233276 	 10.536025524139404 	 0.30140209197998047 	 0.3051581382751465 	 15.280925273895264 	 25.22772455215454 	 0.4603128433227539 	 0.3799889087677002 	 
2025-08-05 02:51:14.473756 test begin: paddle.cosh(Tensor([8, 32, 824, 241],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([8, 32, 824, 241],"float32"), ) 	 50837504 	 33925 	 10.499028205871582 	 10.132822513580322 	 0.3014247417449951 	 0.3052031993865967 	 15.280968427658081 	 25.22735834121704 	 0.460294246673584 	 0.379957914352417 	 
2025-08-05 02:52:17.865479 test begin: paddle.cosh(x=Tensor([3, 3, 5644801],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(x=Tensor([3, 3, 5644801],"float32"), ) 	 50803209 	 33925 	 10.601184129714966 	 10.126487493515015 	 0.3013927936553955 	 0.30500173568725586 	 15.273589372634888 	 25.210801362991333 	 0.46007657051086426 	 0.3798048496246338 	 
2025-08-05 02:53:21.823827 test begin: paddle.cosh(x=Tensor([3, 5644801, 3],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(x=Tensor([3, 5644801, 3],"float32"), ) 	 50803209 	 33925 	 10.00331425666809 	 10.123565673828125 	 0.30132317543029785 	 0.3049609661102295 	 15.275124549865723 	 25.210721731185913 	 0.46018314361572266 	 0.37973570823669434 	 
2025-08-05 02:54:24.196118 test begin: paddle.cosh(x=Tensor([5644801, 3, 3],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(x=Tensor([5644801, 3, 3],"float32"), ) 	 50803209 	 33925 	 10.003313302993774 	 10.124051332473755 	 0.301405668258667 	 0.3050217628479004 	 15.274023056030273 	 25.210867404937744 	 0.4600965976715088 	 0.37976956367492676 	 
2025-08-05 02:55:27.685547 test begin: paddle.count_nonzero(Tensor([1, 14, 129601, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 14, 129601, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, ) 	 25401796 	 16875 	 10.10935378074646 	 8.885494709014893 	 0.20416593551635742 	 0.1792595386505127 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 02:55:58.516742 test begin: paddle.count_nonzero(Tensor([1, 14, 129601, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 14, 129601, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, ) 	 25401796 	 16875 	 10.10979175567627 	 8.877232551574707 	 0.2041170597076416 	 0.17917633056640625 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 1, 129601, 1]) and output[0] has a shape of torch.Size([1, 129601]).
2025-08-05 02:56:26.564476 test begin: paddle.count_nonzero(Tensor([1, 14, 5, 362881],"float64"), axis=list[1,3,], keepdim=False, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 14, 5, 362881],"float64"), axis=list[1,3,], keepdim=False, name=None, ) 	 25401670 	 16875 	 10.358513593673706 	 9.170913696289062 	 0.15683341026306152 	 0.13879847526550293 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 02:56:56.383613 test begin: paddle.count_nonzero(Tensor([1, 14, 5, 362881],"float64"), axis=list[1,3,], keepdim=True, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 14, 5, 362881],"float64"), axis=list[1,3,], keepdim=True, name=None, ) 	 25401670 	 16875 	 10.359236240386963 	 9.17188024520874 	 0.15678715705871582 	 0.13886785507202148 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 1, 5, 1]) and output[0] has a shape of torch.Size([1, 5]).
2025-08-05 02:57:24.791633 test begin: paddle.count_nonzero(Tensor([1, 362881, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 362881, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, ) 	 25401670 	 16875 	 10.257029056549072 	 9.320003271102905 	 0.1552903652191162 	 0.1411123275756836 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 02:57:53.188699 test begin: paddle.count_nonzero(Tensor([1, 362881, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 362881, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, ) 	 25401670 	 16875 	 10.256779670715332 	 9.318132400512695 	 0.15526390075683594 	 0.14085912704467773 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 1, 5, 1]) and output[0] has a shape of torch.Size([1, 5]).
2025-08-05 02:58:21.591647 test begin: paddle.count_nonzero(Tensor([2, 1270081, 4, 5],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([2, 1270081, 4, 5],"float32"), axis=-1, keepdim=False, ) 	 50803240 	 16875 	 16.424899578094482 	 17.89872169494629 	 0.3317396640777588 	 0.36153149604797363 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 02:59:12.658523 test begin: paddle.count_nonzero(Tensor([2, 3, 1693441, 5],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([2, 3, 1693441, 5],"float32"), axis=-1, keepdim=False, ) 	 50803230 	 16875 	 16.42780113220215 	 17.89850401878357 	 0.3318190574645996 	 0.36155200004577637 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:00:03.715429 test begin: paddle.count_nonzero(Tensor([2, 3, 4, 2116801],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([2, 3, 4, 2116801],"float32"), axis=-1, keepdim=False, ) 	 50803224 	 16875 	 14.736828327178955 	 14.623053073883057 	 0.2230515480041504 	 0.22125029563903809 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:00:47.708262 test begin: paddle.count_nonzero(Tensor([25921, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([25921, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, ) 	 25402580 	 16875 	 9.999963760375977 	 8.922691345214844 	 0.20193791389465332 	 0.17966914176940918 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:01:17.922161 test begin: paddle.count_nonzero(Tensor([25921, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([25921, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, ) 	 25402580 	 16875 	 9.995270490646362 	 8.912672758102417 	 0.2018108367919922 	 0.17968297004699707 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([25921, 1, 5, 1]) and output[0] has a shape of torch.Size([25921, 5]).
2025-08-05 03:01:46.894051 test begin: paddle.count_nonzero(Tensor([846721, 3, 4, 5],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([846721, 3, 4, 5],"float32"), axis=-1, keepdim=False, ) 	 50803260 	 16875 	 16.423381567001343 	 17.89848017692566 	 0.3316497802734375 	 0.361527681350708 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:02:37.965369 test begin: paddle.crop(x=Tensor([201, 14112, 3, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([201, 14112, 3, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 	 25528608 	 547102 	 10.141729354858398 	 10.741451501846313 	 4.3392181396484375e-05 	 7.534027099609375e-05 	 81.09027171134949 	 89.56156969070435 	 0.15147852897644043 	 0.018435955047607422 	 combined
2025-08-05 03:05:53.527341 test begin: paddle.crop(x=Tensor([201, 3, 14112, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([201, 3, 14112, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 	 25528608 	 547102 	 14.82674527168274 	 10.838740110397339 	 4.172325134277344e-05 	 0.00020766258239746094 	 82.10275220870972 	 92.80332732200623 	 0.15354466438293457 	 0.019059181213378906 	 combined
2025-08-05 03:09:15.043395 test begin: paddle.crop(x=Tensor([201, 3, 3, 14112],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([201, 3, 3, 14112],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 	 25528608 	 547102 	 13.279457569122314 	 10.893428564071655 	 8.0108642578125e-05 	 9.72747802734375e-05 	 80.2828152179718 	 90.9575879573822 	 0.1499783992767334 	 0.01886606216430664 	 combined
2025-08-05 03:12:33.408236 test begin: paddle.crop(x=Tensor([301, 84672],"float64"), shape=list[2,2,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([301, 84672],"float64"), shape=list[2,2,], ) 	 25486272 	 547102 	 10.26730990409851 	 7.519103527069092 	 7.581710815429688e-05 	 0.00019502639770507812 	 81.06461501121521 	 82.57105708122253 	 0.15153956413269043 	 0.031032323837280273 	 combined
2025-08-05 03:15:37.856557 test begin: paddle.crop(x=Tensor([8467201, 3],"float64"), shape=list[2,2,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([8467201, 3],"float64"), shape=list[2,2,], ) 	 25401603 	 547102 	 10.246386766433716 	 7.621106147766113 	 6.914138793945312e-05 	 0.000110626220703125 	 81.09988236427307 	 78.38994002342224 	 0.15151000022888184 	 0.036637306213378906 	 combined
2025-08-05 03:18:38.500055 test begin: paddle.cross(x=Tensor([2822401, 3, 3],"float64"), y=Tensor([2822401, 3, 3],"float64"), axis=1, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([2822401, 3, 3],"float64"), y=Tensor([2822401, 3, 3],"float64"), axis=1, ) 	 50803218 	 22358 	 10.396257877349854 	 10.049728393554688 	 0.4598360061645508 	 0.4593162536621094 	 16.790382146835327 	 20.096134901046753 	 0.767505407333374 	 0.45931172370910645 	 
2025-08-05 03:19:41.915246 test begin: paddle.cross(x=Tensor([2822401, 3, 3],"float64"), y=Tensor([2822401, 3, 3],"float64"), axis=2, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([2822401, 3, 3],"float64"), y=Tensor([2822401, 3, 3],"float64"), axis=2, ) 	 50803218 	 22358 	 10.076728343963623 	 10.07747197151184 	 0.4605560302734375 	 0.4607398509979248 	 16.9523024559021 	 20.15219259262085 	 0.7748725414276123 	 0.46062326431274414 	 
2025-08-05 03:20:43.123915 test begin: paddle.cross(x=Tensor([3, 2822401, 3],"float64"), y=Tensor([3, 2822401, 3],"float64"), axis=0, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([3, 2822401, 3],"float64"), y=Tensor([3, 2822401, 3],"float64"), axis=0, ) 	 50803218 	 22358 	 10.008745193481445 	 10.030754566192627 	 0.4574859142303467 	 0.45836782455444336 	 16.561115741729736 	 20.05923342704773 	 0.7570171356201172 	 0.4584202766418457 	 
2025-08-05 03:21:43.123201 test begin: paddle.cross(x=Tensor([3, 2822401, 3],"float64"), y=Tensor([3, 2822401, 3],"float64"), axis=2, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([3, 2822401, 3],"float64"), y=Tensor([3, 2822401, 3],"float64"), axis=2, ) 	 50803218 	 22358 	 10.989784717559814 	 10.079805374145508 	 0.46050429344177246 	 0.46068310737609863 	 16.95284414291382 	 20.150829076766968 	 0.7750036716461182 	 0.4604969024658203 	 
2025-08-05 03:22:44.460135 test begin: paddle.cross(x=Tensor([3, 3, 2822401],"float64"), y=Tensor([3, 3, 2822401],"float64"), axis=0, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([3, 3, 2822401],"float64"), y=Tensor([3, 3, 2822401],"float64"), axis=0, ) 	 50803218 	 22358 	 10.00948429107666 	 10.027894258499146 	 0.4575638771057129 	 0.45838427543640137 	 16.560280084609985 	 20.06003975868225 	 0.7569944858551025 	 0.4598381519317627 	 
2025-08-05 03:23:43.638994 test begin: paddle.cross(x=Tensor([3, 3, 2822401],"float64"), y=Tensor([3, 3, 2822401],"float64"), axis=1, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([3, 3, 2822401],"float64"), y=Tensor([3, 3, 2822401],"float64"), axis=1, ) 	 50803218 	 22358 	 10.010975122451782 	 10.00875186920166 	 0.4578239917755127 	 0.45751309394836426 	 16.59540319442749 	 20.039454460144043 	 0.758552074432373 	 0.4579658508300781 	 
2025-08-05 03:24:42.574899 test begin: paddle.cummax(Tensor([100, 2080],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0533caaa40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 03:34:58.419867 test begin: paddle.cummax(Tensor([100, 2080],"float32"), axis=-1, )
W0805 03:34:58.615634 82895 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cummax 	 paddle.cummax(Tensor([100, 2080],"float32"), axis=-1, ) 	 208000 	 55599 	 9.729720830917358 	 1.8563547134399414 	 0.17885851860046387 	 0.034091949462890625 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:35:16.096645 test begin: paddle.cummax(Tensor([10001, 2080],"float32"), axis=-2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f65aa87b8b0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 03:45:25.865007 test begin: paddle.cummax(Tensor([2080, 100],"float32"), )
W0805 03:45:26.097129 83213 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4fe8927010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 03:55:32.817848 test begin: paddle.cummax(Tensor([2080, 100],"float32"), axis=-2, )
W0805 03:55:33.597530 83592 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cummax 	 paddle.cummax(Tensor([2080, 100],"float32"), axis=-2, ) 	 208000 	 55599 	 23.944278955459595 	 22.788183450698853 	 0.44014692306518555 	 0.4188826084136963 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:56:24.279172 test begin: paddle.cummax(Tensor([208001, 100],"float32"), axis=-1, )
[Prof] paddle.cummax 	 paddle.cummax(Tensor([208001, 100],"float32"), axis=-1, ) 	 20800100 	 55599 	 30.703840255737305 	 194.8180603981018 	 0.5643632411956787 	 3.5807385444641113 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:30:42.110939 test begin: paddle.cummin(Tensor([100, 508033],"float32"), axis=-1, )
W0804 11:30:50.719818 84371 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa4a1bcf0a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 11:40:48.100345 test begin: paddle.cummin(Tensor([100, 508033],"float32"), axis=-2, )
W0804 11:40:49.187777 89185 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cummin 	 paddle.cummin(Tensor([100, 508033],"float32"), axis=-2, ) 	 50803300 	 12631 	 10.030542612075806 	 9.994447469711304 	 0.8111581802368164 	 0.8071866035461426 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:42:27.919579 test begin: paddle.cummin(Tensor([508033, 100],"float32"), axis=-1, )
[Prof] paddle.cummin 	 paddle.cummin(Tensor([508033, 100],"float32"), axis=-1, ) 	 50803300 	 12631 	 16.788803577423096 	 107.82299733161926 	 1.3582863807678223 	 8.727924585342407 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:44:57.551123 test begin: paddle.cummin(Tensor([508033, 100],"float32"), axis=-2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe929a5afe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 11:59:17.346944 test begin: paddle.cumprod(Tensor([2, 127009, 10, 10],"float64"), 1, )
W0804 11:59:18.084848 95782 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fde8bb0eef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 12:09:21.912533 test begin: paddle.cumprod(Tensor([2, 3, 10, 423361],"float64"), 1, )
W0804 12:09:22.559005 99510 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 10, 423361],"float64"), 1, ) 	 25401660 	 33059 	 9.97841477394104 	 10.006846189498901 	 0.30844759941101074 	 0.3087613582611084 	 93.91718864440918 	 67.35216641426086 	 0.0003476142883300781 	 0.0012960433959960938 	 
2025-08-04 12:12:28.667872 test begin: paddle.cumprod(Tensor([2, 3, 3, 4, 705601],"float32"), dim=0, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 3, 4, 705601],"float32"), dim=0, ) 	 50803272 	 33059 	 10.827102899551392 	 11.601536512374878 	 0.33471012115478516 	 0.31819605827331543 	 112.4448094367981 	 69.80736064910889 	 0.00047397613525390625 	 0.0013158321380615234 	 
2025-08-04 12:15:59.269064 test begin: paddle.cumprod(Tensor([2, 3, 3, 4, 705601],"float32"), dim=1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 3, 4, 705601],"float32"), dim=1, ) 	 50803272 	 33059 	 10.617128849029541 	 10.50057339668274 	 0.3281846046447754 	 0.3246161937713623 	 107.65050649642944 	 70.23293471336365 	 0.00041413307189941406 	 0.001325845718383789 	 
2025-08-04 12:19:20.414519 test begin: paddle.cumprod(Tensor([2, 3, 3, 564481, 5],"float32"), dim=0, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 3, 564481, 5],"float32"), dim=0, ) 	 50803290 	 33059 	 10.829096555709839 	 10.292203187942505 	 0.334758996963501 	 0.31818699836730957 	 112.42817974090576 	 69.7856879234314 	 0.0004734992980957031 	 0.0013065338134765625 	 
2025-08-04 12:22:48.751297 test begin: paddle.cumprod(Tensor([2, 3, 3, 564481, 5],"float32"), dim=1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 3, 564481, 5],"float32"), dim=1, ) 	 50803290 	 33059 	 10.629061222076416 	 10.5153489112854 	 0.3286151885986328 	 0.32509660720825195 	 107.71936702728271 	 70.31774592399597 	 0.0004115104675292969 	 0.0013241767883300781 	 
2025-08-04 12:26:13.177690 test begin: paddle.cumprod(Tensor([2, 3, 423361, 10],"float64"), 1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 423361, 10],"float64"), 1, ) 	 25401660 	 33059 	 9.989024639129639 	 10.003942251205444 	 0.30878496170043945 	 0.3091108798980713 	 94.0182933807373 	 67.3778440952301 	 0.0003440380096435547 	 0.0012936592102050781 	 
2025-08-04 12:29:15.792449 test begin: paddle.cumprod(Tensor([2, 3, 423361, 4, 5],"float32"), dim=0, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 423361, 4, 5],"float32"), dim=0, ) 	 50803320 	 33059 	 10.84050703048706 	 10.294931173324585 	 0.3351259231567383 	 0.31828784942626953 	 112.49794912338257 	 69.81274318695068 	 0.0004696846008300781 	 0.0013184547424316406 	 
2025-08-04 12:32:43.027785 test begin: paddle.cumprod(Tensor([2, 3, 423361, 4, 5],"float32"), dim=1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 423361, 4, 5],"float32"), dim=1, ) 	 50803320 	 33059 	 10.626649618148804 	 10.519774436950684 	 0.32835960388183594 	 0.326373815536499 	 107.70537972450256 	 70.30373311042786 	 0.0004105567932128906 	 0.0013306140899658203 	 
2025-08-04 12:36:04.064184 test begin: paddle.cumprod(Tensor([2, 423361, 3, 4, 5],"float32"), dim=0, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 423361, 3, 4, 5],"float32"), dim=0, ) 	 50803320 	 33059 	 10.840423583984375 	 10.294989347457886 	 0.3351414203643799 	 0.3182952404022217 	 112.50647640228271 	 69.86696553230286 	 0.00046825408935546875 	 0.0013175010681152344 	 
2025-08-04 12:39:29.820300 test begin: paddle.cumprod(Tensor([282241, 3, 3, 4, 5],"float32"), dim=1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([282241, 3, 3, 4, 5],"float32"), dim=1, ) 	 50803380 	 33059 	 10.815193176269531 	 10.659081220626831 	 0.3343474864959717 	 0.3295166492462158 	 107.41919136047363 	 70.55221271514893 	 0.00041794776916503906 	 0.0013384819030761719 	 
2025-08-04 12:42:55.374600 test begin: paddle.cumprod(Tensor([84673, 3, 10, 10],"float64"), 1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([84673, 3, 10, 10],"float64"), 1, ) 	 25401900 	 33059 	 10.03719186782837 	 10.078069686889648 	 0.31020021438598633 	 0.3105051517486572 	 93.87027621269226 	 67.45376634597778 	 0.0003478527069091797 	 0.0012912750244140625 	 
2025-08-04 12:46:02.559709 test begin: paddle.cumsum(Tensor([50803201],"float32"), axis=0, )
[Prof] paddle.cumsum 	 paddle.cumsum(Tensor([50803201],"float32"), axis=0, ) 	 50803201 	 28767 	 9.996558904647827 	 9.45461893081665 	 9.918212890625e-05 	 0.16798734664916992 	 11.479127645492554 	 27.176135063171387 	 7.271766662597656e-05 	 0.24155116081237793 	 
2025-08-04 12:47:02.540581 test begin: paddle.deg2rad(Tensor([25401601],"int64"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([25401601],"int64"), ) 	 25401601 	 33772 	 12.799447059631348 	 7.852235317230225 	 0.19359517097473145 	 0.23761987686157227 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 12:47:39.493802 test begin: paddle.deg2rad(Tensor([50803201],"float32"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([50803201],"float32"), ) 	 50803201 	 33772 	 11.191699743270874 	 10.049922943115234 	 0.30231595039367676 	 0.3041403293609619 	 9.995182514190674 	 10.047612190246582 	 0.30240821838378906 	 0.3040170669555664 	 
2025-08-04 12:48:23.580469 test begin: paddle.deg2rad(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33772 	 9.992557048797607 	 10.053119421005249 	 0.3024439811706543 	 0.3041520118713379 	 9.99378752708435 	 10.04769778251648 	 0.3024604320526123 	 0.30405282974243164 	 
2025-08-04 12:49:05.472131 test begin: paddle.deg2rad(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33772 	 9.984012126922607 	 10.055277109146118 	 0.30212926864624023 	 0.30413317680358887 	 9.993866443634033 	 10.047627687454224 	 0.3024258613586426 	 0.3040745258331299 	 
2025-08-04 12:49:47.285525 test begin: paddle.deg2rad(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33772 	 9.98930549621582 	 10.070444583892822 	 0.3022024631500244 	 0.3041250705718994 	 9.994024515151978 	 10.04783320426941 	 0.3024027347564697 	 0.3040812015533447 	 
2025-08-04 12:50:30.582716 test begin: paddle.diag(Tensor([20000, 25402],"float32"), )
[Prof] paddle.diag 	 paddle.diag(Tensor([20000, 25402],"float32"), ) 	 508040000 	 199725 	 1.666649341583252 	 3.340562343597412 	 0.00010061264038085938 	 0.0001087188720703125 	 303.2916045188904 	 264.98088359832764 	 0.7763314247131348 	 0.6781179904937744 	 
2025-08-04 13:00:15.192019 test begin: paddle.diag(Tensor([20000, 25402],"float32"), offset=-1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([20000, 25402],"float32"), offset=-1, ) 	 508040000 	 199725 	 1.7114989757537842 	 3.3415348529815674 	 4.076957702636719e-05 	 9.751319885253906e-05 	 303.32607102394104 	 264.98026871681213 	 0.7757246494293213 	 0.6786344051361084 	 
2025-08-04 13:09:59.952370 test begin: paddle.diag(Tensor([20000, 25402],"float32"), offset=1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([20000, 25402],"float32"), offset=1, ) 	 508040000 	 199725 	 1.758918046951294 	 3.5061750411987305 	 4.696846008300781e-05 	 0.0001380443572998047 	 303.2857913970947 	 264.9769513607025 	 0.7760422229766846 	 0.6777629852294922 	 
2025-08-04 13:19:43.237101 test begin: paddle.diag(Tensor([254020, 2000],"float32"), )
[Prof] paddle.diag 	 paddle.diag(Tensor([254020, 2000],"float32"), ) 	 508040000 	 199725 	 1.7055730819702148 	 3.3500664234161377 	 4.839897155761719e-05 	 9.822845458984375e-05 	 300.89707612991333 	 263.2954168319702 	 0.7699792385101318 	 0.6734702587127686 	 
2025-08-04 13:29:21.272844 test begin: paddle.diag(Tensor([254020, 2000],"float32"), offset=-1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([254020, 2000],"float32"), offset=-1, ) 	 508040000 	 199725 	 1.719496250152588 	 3.343431234359741 	 4.9114227294921875e-05 	 0.00012087821960449219 	 300.9010901451111 	 263.2973654270172 	 0.770026445388794 	 0.6734979152679443 	 
2025-08-04 13:39:00.102175 test begin: paddle.diag(Tensor([254020, 2000],"float32"), offset=1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([254020, 2000],"float32"), offset=1, ) 	 508040000 	 199725 	 1.703711748123169 	 3.336858034133911 	 4.673004150390625e-05 	 7.295608520507812e-05 	 300.9356460571289 	 263.29771542549133 	 0.7697138786315918 	 0.6735703945159912 	 
2025-08-04 13:48:38.331162 test begin: paddle.diag_embed(Tensor([1058401, 3, 8],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([1058401, 3, 8],"float64"), ) 	 25401624 	 3661 	 14.491320610046387 	 9.56012511253357 	 9.632110595703125e-05 	 1.3344783782958984 	 None 	 None 	 None 	 None 	 
2025-08-04 13:49:05.872780 test begin: paddle.diag_embed(Tensor([1411201, 3, 6],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([1411201, 3, 6],"float64"), ) 	 25401618 	 3661 	 19.424577951431274 	 8.033244132995605 	 9.34600830078125e-05 	 1.1216156482696533 	 None 	 None 	 None 	 None 	 
2025-08-04 13:49:33.941107 test begin: paddle.diag_embed(Tensor([2, 1058401, 12],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([2, 1058401, 12],"float64"), ) 	 25401624 	 3661 	 24.8586528301239 	 14.021256685256958 	 0.00011229515075683594 	 0.9790608882904053 	 None 	 None 	 None 	 None 	 
2025-08-04 13:50:13.436838 test begin: paddle.diag_embed(Tensor([2, 1587601, 8],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([2, 1587601, 8],"float64"), ) 	 25401616 	 3661 	 20.80924129486084 	 9.562459468841553 	 0.0001316070556640625 	 1.3345158100128174 	 None 	 None 	 None 	 None 	 
2025-08-04 13:50:44.443596 test begin: paddle.diag_embed(Tensor([2, 2116801, 6],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([2, 2116801, 6],"float64"), ) 	 25401612 	 3661 	 11.899415254592896 	 8.036144971847534 	 4.982948303222656e-05 	 1.1212518215179443 	 None 	 None 	 None 	 None 	 
2025-08-04 13:51:04.953775 test begin: paddle.diag_embed(Tensor([705601, 3, 12],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([705601, 3, 12],"float64"), ) 	 25401636 	 3661 	 19.822895765304565 	 14.02178406715393 	 8.797645568847656e-05 	 0.9786341190338135 	 None 	 None 	 None 	 None 	 
2025-08-04 13:51:42.583576 test begin: paddle.diagonal(x=Tensor([117601, 6, 6, 6],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([117601, 6, 6, 6],"float64"), ) 	 25401816 	 1998070 	 7.23456335067749 	 9.02121877670288 	 7.152557373046875e-05 	 9.584426879882812e-05 	 297.48436856269836 	 276.7974741458893 	 0.07612466812133789 	 0.07073450088500977 	 
2025-08-04 14:01:33.765035 test begin: paddle.diagonal(x=Tensor([176401, 6, 6, 2, 2],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([176401, 6, 6, 2, 2],"float64"), ) 	 25401744 	 1998070 	 7.357117652893066 	 8.821353673934937 	 0.00013589859008789062 	 0.00030922889709472656 	 298.6514434814453 	 276.7767617702484 	 0.07650613784790039 	 0.07072329521179199 	 
2025-08-04 14:11:29.411058 test begin: paddle.diagonal(x=Tensor([601, 1176, 6, 6],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 1176, 6, 6],"float64"), ) 	 25443936 	 1998070 	 7.272353887557983 	 8.869292259216309 	 0.0001277923583984375 	 0.0003077983856201172 	 300.7659468650818 	 278.61659502983093 	 0.07688283920288086 	 0.07121467590332031 	 
2025-08-04 14:21:25.721882 test begin: paddle.diagonal(x=Tensor([601, 1764, 6, 2, 2],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 1764, 6, 2, 2],"float64"), ) 	 25443936 	 1998070 	 7.333929538726807 	 8.946938276290894 	 0.00012445449829101562 	 0.00018095970153808594 	 302.60612773895264 	 278.3826744556427 	 0.07721853256225586 	 0.07111239433288574 	 
2025-08-04 14:31:24.786743 test begin: paddle.diagonal(x=Tensor([601, 6, 1176, 6],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 6, 1176, 6],"float64"), ) 	 25443936 	 1998070 	 7.296695947647095 	 8.841742753982544 	 0.00011372566223144531 	 0.00012826919555664062 	 295.953476190567 	 278.3549599647522 	 0.07552361488342285 	 0.07109856605529785 	 
2025-08-04 14:41:16.671545 test begin: paddle.diagonal(x=Tensor([601, 6, 1764, 2, 2],"float64"), axis1=-1, axis2=2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7faa1cc52e30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 14:51:21.396057 test begin: paddle.diagonal(x=Tensor([601, 6, 6, 1176],"float64"), )
W0804 14:51:22.155910 151678 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 6, 6, 1176],"float64"), ) 	 25443936 	 1998070 	 11.680105209350586 	 9.093554496765137 	 0.000156402587890625 	 0.00011205673217773438 	 292.1750910282135 	 280.5452058315277 	 0.07464861869812012 	 0.07174324989318848 	 
2025-08-04 15:01:18.345584 test begin: paddle.diagonal(x=Tensor([601, 6, 6, 2, 588],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f35804d3070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 15:11:23.139579 test begin: paddle.diagonal(x=Tensor([601, 6, 6, 2, 588],"float64"), axis1=-1, axis2=2, )
W0804 15:11:23.804111 158652 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe3ccf63040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 15:21:27.817287 test begin: paddle.diagonal(x=Tensor([601, 6, 6, 588, 2],"float64"), )
W0804 15:21:31.205322 162049 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 6, 6, 588, 2],"float64"), ) 	 25443936 	 1998070 	 7.373946666717529 	 9.551812171936035 	 0.00022840499877929688 	 0.000354766845703125 	 293.97758650779724 	 278.8249979019165 	 0.07522988319396973 	 0.0712287425994873 	 
2025-08-04 15:31:27.529960 test begin: paddle.diagonal_scatter(Tensor([10, 10160641],"int16"), Tensor([10],"int16"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([10, 10160641],"int16"), Tensor([10],"int16"), offset=0, axis1=0, axis2=1, ) 	 101606420 	 31122 	 9.744624853134155 	 10.080002546310425 	 0.07986259460449219 	 0.10744118690490723 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:32:02.974035 test begin: paddle.diagonal_scatter(Tensor([10, 5080321],"int32"), Tensor([10],"int32"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([10, 5080321],"int32"), Tensor([10],"int32"), offset=0, axis1=0, axis2=1, ) 	 50803220 	 31122 	 9.744654178619385 	 9.835206985473633 	 0.07986140251159668 	 0.10743904113769531 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:32:33.784998 test begin: paddle.diagonal_scatter(Tensor([100, 5080321],"bool"), Tensor([100],"bool"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([100, 5080321],"bool"), Tensor([100],"bool"), offset=0, axis1=0, axis2=1, ) 	 508032200 	 31122 	 24.185849905014038 	 24.09920310974121 	 0.19808578491210938 	 0.2631800174713135 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:34:02.708700 test begin: paddle.diagonal_scatter(Tensor([10160641, 10],"int16"), Tensor([10],"int16"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([10160641, 10],"int16"), Tensor([10],"int16"), offset=0, axis1=0, axis2=1, ) 	 101606420 	 31122 	 10.657996416091919 	 9.824385166168213 	 0.08178067207336426 	 0.10732722282409668 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:34:37.919594 test begin: paddle.diagonal_scatter(Tensor([5080321, 10],"int32"), Tensor([10],"int32"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([5080321, 10],"int32"), Tensor([10],"int32"), offset=0, axis1=0, axis2=1, ) 	 50803220 	 31122 	 10.915662050247192 	 9.82716965675354 	 0.08178043365478516 	 0.1073141098022461 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:35:09.854255 test begin: paddle.diagonal_scatter(Tensor([50803210, 10],"bool"), Tensor([10],"bool"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([50803210, 10],"bool"), Tensor([10],"bool"), offset=0, axis1=0, axis2=1, ) 	 508032110 	 31122 	 24.17685866355896 	 24.082369089126587 	 0.19809770584106445 	 0.2630934715270996 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:36:42.071527 test begin: paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 12388 	 11.712438583374023 	 3.2365877628326416 	 0.3217298984527588 	 0.2669813632965088 	 None 	 None 	 None 	 None 	 
2025-08-04 15:36:57.625148 test begin: paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=-2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=-2, ) 	 25401664 	 12388 	 11.696715593338013 	 3.236412763595581 	 0.32166171073913574 	 0.2669835090637207 	 None 	 None 	 None 	 None 	 
2025-08-04 15:37:13.266673 test begin: paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=2, ) 	 25401664 	 12388 	 11.703778982162476 	 3.23648738861084 	 0.3218202590942383 	 0.26699018478393555 	 None 	 None 	 None 	 None 	 
2025-08-04 15:37:30.634020 test begin: paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 12388 	 10.797100067138672 	 3.236631393432617 	 0.2969181537628174 	 0.26702332496643066 	 None 	 None 	 None 	 None 	 
2025-08-04 15:37:45.368776 test begin: paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=-2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=-2, ) 	 25401664 	 12388 	 10.794397830963135 	 3.236416816711426 	 0.29686641693115234 	 0.2670431137084961 	 None 	 None 	 None 	 None 	 
2025-08-04 15:38:00.097589 test begin: paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=2, ) 	 25401664 	 12388 	 10.79464316368103 	 3.23642635345459 	 0.2968766689300537 	 0.26700925827026367 	 None 	 None 	 None 	 None 	 
2025-08-04 15:38:14.826560 test begin: paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 12388 	 10.793428659439087 	 3.236459970474243 	 0.29683947563171387 	 0.26700568199157715 	 None 	 None 	 None 	 None 	 
2025-08-04 15:38:29.570365 test begin: paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=-2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=-2, ) 	 25401664 	 12388 	 13.25719666481018 	 3.721651077270508 	 0.36466383934020996 	 0.3059704303741455 	 None 	 None 	 None 	 None 	 
2025-08-04 15:38:47.856504 test begin: paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=2, ) 	 25401664 	 12388 	 13.25642704963684 	 3.708458662033081 	 0.3646068572998047 	 0.3059351444244385 	 None 	 None 	 None 	 None 	 
2025-08-04 15:39:05.529100 test begin: paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 12388 	 13.255125522613525 	 3.7085206508636475 	 0.3646109104156494 	 0.3059723377227783 	 None 	 None 	 None 	 None 	 
2025-08-04 15:39:23.182691 test begin: paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=-2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=-2, ) 	 25401664 	 12388 	 9.999769687652588 	 3.254343271255493 	 0.27503252029418945 	 0.26844215393066406 	 None 	 None 	 None 	 None 	 
2025-08-04 15:39:39.379702 test begin: paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=2, ) 	 25401664 	 12388 	 10.008173704147339 	 3.254257917404175 	 0.2750072479248047 	 0.26847028732299805 	 None 	 None 	 None 	 None 	 
2025-08-04 15:39:57.894132 test begin: paddle.digamma(Tensor([16538, 3, 32, 32],"float32"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([16538, 3, 32, 32],"float32"), ) 	 50804736 	 10410 	 10.009246826171875 	 11.1247079372406 	 0.9833991527557373 	 1.0952064990997314 	 46.8004515171051 	 11.159209489822388 	 4.59468674659729 	 0.547753095626831 	 
2025-08-04 15:41:19.691570 test begin: paddle.digamma(Tensor([8, 3, 32, 33076],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3, 32, 33076],"float64"), ) 	 25402368 	 10410 	 12.161738872528076 	 11.88029170036316 	 1.1939287185668945 	 1.166353702545166 	 89.07453846931458 	 11.285133123397827 	 8.738763332366943 	 0.5540614128112793 	 
2025-08-04 15:43:26.524100 test begin: paddle.digamma(Tensor([8, 3, 32, 66151],"float32"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3, 32, 66151],"float32"), ) 	 50803968 	 10410 	 10.468662738800049 	 11.131362199783325 	 0.9819421768188477 	 1.095844030380249 	 46.714961767196655 	 11.158566951751709 	 4.586051940917969 	 0.5477640628814697 	 
2025-08-04 15:44:47.852246 test begin: paddle.digamma(Tensor([8, 3, 33076, 32],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3, 33076, 32],"float64"), ) 	 25402368 	 10410 	 12.162075519561768 	 11.96691107749939 	 1.1940569877624512 	 1.2446262836456299 	 89.01711559295654 	 11.28811502456665 	 8.740103960037231 	 0.5540518760681152 	 
2025-08-04 15:46:56.958668 test begin: paddle.digamma(Tensor([8, 3, 66151, 32],"float32"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3, 66151, 32],"float32"), ) 	 50803968 	 10410 	 9.998230695724487 	 11.147241115570068 	 0.9808104038238525 	 1.097142219543457 	 46.71295189857483 	 11.166699647903442 	 4.586069107055664 	 0.5481216907501221 	 
2025-08-04 15:48:17.808759 test begin: paddle.digamma(Tensor([8, 3101, 32, 32],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3101, 32, 32],"float64"), ) 	 25403392 	 10410 	 12.163331031799316 	 11.955920219421387 	 1.1940844058990479 	 1.2333462238311768 	 89.0034658908844 	 11.285376787185669 	 8.735637426376343 	 0.5539827346801758 	 
2025-08-04 15:50:24.127532 test begin: paddle.digamma(Tensor([8, 6202, 32, 32],"float32"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 6202, 32, 32],"float32"), ) 	 50806784 	 10410 	 10.005769968032837 	 12.473272562026978 	 0.9821524620056152 	 1.0856285095214844 	 46.77201223373413 	 11.160240650177002 	 4.5865092277526855 	 0.5477759838104248 	 
2025-08-04 15:51:47.824576 test begin: paddle.digamma(Tensor([8269, 3, 32, 32],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8269, 3, 32, 32],"float64"), ) 	 25402368 	 10410 	 12.173375844955444 	 11.924925565719604 	 1.1942336559295654 	 1.2081267833709717 	 88.9817807674408 	 11.28764796257019 	 8.735172510147095 	 0.5540192127227783 	 
2025-08-04 15:53:53.888153 test begin: paddle.digamma(x=Tensor([19601, 6, 6, 6, 6],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(x=Tensor([19601, 6, 6, 6, 6],"float64"), ) 	 25402896 	 10410 	 12.162005186080933 	 12.337576627731323 	 1.1937963962554932 	 1.1658935546875 	 88.97799110412598 	 11.287552118301392 	 8.732401847839355 	 0.5540323257446289 	 
2025-08-04 15:56:01.075220 test begin: paddle.digamma(x=Tensor([3, 39201, 6, 6, 6],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(x=Tensor([3, 39201, 6, 6, 6],"float64"), ) 	 25402248 	 10410 	 12.157021045684814 	 11.881690263748169 	 1.193272590637207 	 1.1665360927581787 	 88.95257759094238 	 11.28646993637085 	 8.73290205001831 	 0.5540015697479248 	 
2025-08-04 15:58:06.519081 test begin: paddle.digamma(x=Tensor([3, 6, 39201, 6, 6],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(x=Tensor([3, 6, 39201, 6, 6],"float64"), ) 	 25402248 	 10410 	 12.161907434463501 	 11.87960958480835 	 1.1940720081329346 	 1.1658260822296143 	 88.91451954841614 	 11.286858320236206 	 8.72915267944336 	 0.5540313720703125 	 
2025-08-04 16:00:11.893578 test begin: paddle.digamma(x=Tensor([3, 6, 6, 39201, 6],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(x=Tensor([3, 6, 6, 39201, 6],"float64"), ) 	 25402248 	 10410 	 12.160800218582153 	 11.888810396194458 	 1.1938064098358154 	 1.166579008102417 	 88.93936038017273 	 11.287728309631348 	 8.733026266098022 	 0.5540151596069336 	 
2025-08-04 16:02:17.368383 test begin: paddle.digamma(x=Tensor([3, 6, 6, 6, 39201],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(x=Tensor([3, 6, 6, 6, 39201],"float64"), ) 	 25402248 	 10410 	 12.159406661987305 	 11.881752014160156 	 1.1932992935180664 	 1.1665606498718262 	 88.91548800468445 	 11.287236213684082 	 8.729145050048828 	 0.5540430545806885 	 
2025-08-04 16:04:24.003812 test begin: paddle.dist(x=Tensor([10],"float64"), y=Tensor([2540161, 10],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([10],"float64"), y=Tensor([2540161, 10],"float64"), ) 	 25401620 	 21993 	 9.98709511756897 	 9.998995065689087 	 0.1158301830291748 	 0.15479803085327148 	 147.84260535240173 	 63.18229913711548 	 1.3760707378387451 	 0.2257087230682373 	 
2025-08-04 16:08:18.272702 test begin: paddle.dist(x=Tensor([113401, 1, 1, 4, 4],"float64"), y=Tensor([113401, 8, 7, 1, 4],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([113401, 1, 1, 4, 4],"float64"), y=Tensor([113401, 8, 7, 1, 4],"float64"), ) 	 27216240 	 21993 	 30.27710509300232 	 30.865027904510498 	 0.3510396480560303 	 0.47776293754577637 	 186.9406442642212 	 248.72997617721558 	 1.7363102436065674 	 0.8890445232391357 	 
2025-08-04 16:16:39.437327 test begin: paddle.dist(x=Tensor([1587601, 1, 4, 4],"float64"), y=Tensor([7, 1, 4],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f931a4067a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:26:48.601316 test begin: paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 453601, 7, 1, 4],"float64"), )
W0804 16:26:49.306250 21503 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.dist 	 paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 453601, 7, 1, 4],"float64"), ) 	 25401688 	 21993 	 30.1525399684906 	 30.67951798439026 	 0.3497774600982666 	 0.47493791580200195 	 218.0138075351715 	 248.85331392288208 	 1.6875662803649902 	 0.8259556293487549 	 
2025-08-04 16:35:40.198426 test begin: paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 8, 396901, 1, 4],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 8, 396901, 1, 4],"float64"), ) 	 25401696 	 21993 	 30.148571968078613 	 30.664326429367065 	 0.3494243621826172 	 0.4746420383453369 	 218.26327180862427 	 248.85272073745728 	 1.6893529891967773 	 0.8259165287017822 	 
2025-08-04 16:44:32.458133 test begin: paddle.dist(x=Tensor([2, 1, 3175201, 4],"float64"), y=Tensor([7, 1, 4],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2bea4d2920>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:54:44.121078 test begin: paddle.dist(x=Tensor([2, 1, 4, 4],"float64"), y=Tensor([6350401, 1, 4],"float64"), )
W0804 16:54:44.747045 31993 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4134056cb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:04:49.873656 test begin: paddle.dist(x=Tensor([2, 1, 793801, 4, 4],"float64"), y=Tensor([2, 8, 793801, 1, 4],"float64"), )
W0804 17:04:51.484752 34633 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff7434e6ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:14:55.936720 test begin: paddle.dist(x=Tensor([2, 793801, 1, 4, 4],"float64"), y=Tensor([2, 793801, 7, 1, 4],"float64"), )
W0804 17:14:57.381147 37277 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f08dcd42ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:25:01.557960 test begin: paddle.dist(x=Tensor([25401601],"float64"), y=Tensor([4, 25401601],"float64"), )
W0804 17:25:03.904891 39919 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.dist 	 paddle.dist(x=Tensor([25401601],"float64"), y=Tensor([4, 25401601],"float64"), ) 	 127008005 	 21993 	 51.57046580314636 	 51.797799825668335 	 0.5979077816009521 	 0.8012218475341797 	 186.30503296852112 	 277.13542556762695 	 2.1620121002197266 	 1.1165456771850586 	 
2025-08-04 17:34:34.345205 test begin: paddle.dist(x=Tensor([6350401],"float64"), y=Tensor([4, 6350401],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([6350401],"float64"), y=Tensor([4, 6350401],"float64"), ) 	 31752005 	 21993 	 13.24216079711914 	 13.177722215652466 	 0.15355348587036133 	 0.20389676094055176 	 47.13032293319702 	 70.1690661907196 	 0.5470662117004395 	 0.271681547164917 	 
2025-08-04 17:37:00.813190 test begin: paddle.divide(Tensor([128, 396901],"float32"), Tensor([1, 396901],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([128, 396901],"float32"), Tensor([1, 396901],"float32"), ) 	 51200229 	 33832 	 10.157815217971802 	 10.544243574142456 	 0.30683255195617676 	 0.3180356025695801 	 27.107830047607422 	 62.17208433151245 	 0.40943026542663574 	 0.312824010848999 	 
2025-08-04 17:38:54.438249 test begin: paddle.divide(Tensor([51059, 995],"float32"), Tensor([1, 995],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([51059, 995],"float32"), Tensor([1, 995],"float32"), ) 	 50804700 	 33832 	 10.06128454208374 	 10.49024224281311 	 0.3038780689239502 	 0.31640172004699707 	 30.53805160522461 	 62.975265979766846 	 0.3071866035461426 	 0.27171874046325684 	 
2025-08-04 17:40:52.956248 test begin: paddle.divide(Tensor([51059, 995],"float32"), Tensor([51059, 995],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([51059, 995],"float32"), Tensor([51059, 995],"float32"), ) 	 101607410 	 33832 	 15.244064331054688 	 15.207800388336182 	 0.46044397354125977 	 0.4594118595123291 	 39.26062893867493 	 70.78073716163635 	 1.1860294342041016 	 0.4276726245880127 	 
2025-08-04 17:43:17.722216 test begin: paddle.divide(Tensor([512, 99226],"float32"), Tensor([1, 99226],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([512, 99226],"float32"), Tensor([1, 99226],"float32"), ) 	 50902938 	 33832 	 10.006115436553955 	 10.51617169380188 	 0.30211639404296875 	 0.31708741188049316 	 28.473677158355713 	 62.08711838722229 	 0.2863292694091797 	 0.3128468990325928 	 
2025-08-04 17:45:13.004738 test begin: paddle.divide(Tensor([544, 93431],"float32"), Tensor([1, 93431],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([544, 93431],"float32"), Tensor([1, 93431],"float32"), ) 	 50919895 	 33832 	 10.003194093704224 	 10.496504306793213 	 0.30214929580688477 	 0.3170647621154785 	 30.279811143875122 	 62.20059871673584 	 0.3046119213104248 	 0.3130934238433838 	 
2025-08-04 17:47:07.697156 test begin: paddle.divide(Tensor([544, 93431],"float32"), Tensor([544, 93431],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([544, 93431],"float32"), Tensor([544, 93431],"float32"), ) 	 101652928 	 33832 	 15.25076413154602 	 15.218286275863647 	 0.4606595039367676 	 0.45963621139526367 	 39.27294206619263 	 70.81306147575378 	 1.1863255500793457 	 0.427868127822876 	 
2025-08-04 17:49:31.200274 test begin: paddle.divide(x=Tensor([16934401, 3],"float32"), y=Tensor([3],"float32"), )
[Prof] paddle.divide 	 paddle.divide(x=Tensor([16934401, 3],"float32"), y=Tensor([3],"float32"), ) 	 50803206 	 33832 	 10.034928798675537 	 10.479470491409302 	 0.3031437397003174 	 0.31653428077697754 	 191.756498336792 	 62.75973701477051 	 1.9321911334991455 	 0.27081298828125 	 
2025-08-04 17:54:11.340755 test begin: paddle.divide(x=Tensor([187679, 271],"float32"), y=Tensor([271],"float32"), )
[Prof] paddle.divide 	 paddle.divide(x=Tensor([187679, 271],"float32"), y=Tensor([271],"float32"), ) 	 50861280 	 33832 	 10.070817947387695 	 10.489670991897583 	 0.3041808605194092 	 0.3165140151977539 	 33.942665100097656 	 62.1705527305603 	 0.3415226936340332 	 0.26829957962036133 	 
2025-08-04 17:56:09.744174 test begin: paddle.dot(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), )
Warning: The core code of paddle.dot is too complex.
[Prof] paddle.dot 	 paddle.dot(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), ) 	 50803202 	 33667 	 206.44949221611023 	 9.877172231674194 	 6.2674219608306885 	 0.14989924430847168 	 21.09765648841858 	 20.195960521697998 	 0.32007694244384766 	 0.3065159320831299 	 
2025-08-04 18:00:30.508229 test begin: paddle.dot(x=Tensor([50803201],"float32"), y=Tensor([50803201],"float32"), )
[Prof] paddle.dot 	 paddle.dot(x=Tensor([50803201],"float32"), y=Tensor([50803201],"float32"), ) 	 101606402 	 33667 	 9.978915452957153 	 9.857022047042847 	 0.3029744625091553 	 0.1495654582977295 	 23.917067527770996 	 20.32819104194641 	 0.36297011375427246 	 0.3085474967956543 	 
2025-08-04 18:01:40.394642 test begin: paddle.dot(x=Tensor([5080320],"int32"), y=Tensor([5080320],"int32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f47325f7070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 18:15:11.124723 test begin: paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[-1,1,3,], )
W0804 18:15:15.050072 52348 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0804 18:15:42.042455 52348 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[-1,1,3,], ) 	 254016180 	 587939 	 18.324891567230225 	 6.26230263710022 	 0.00013113021850585938 	 0.00011730194091796875 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:15:50.896060 test begin: paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[-1,], )
W0804 18:16:08.007678 52780 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[-1,], ) 	 254016180 	 587939 	 10.159244298934937 	 4.352135419845581 	 0.00011730194091796875 	 0.0002682209014892578 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:16:14.274976 test begin: paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[2,4,], )
W0804 18:16:38.515352 52792 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[2,4,], ) 	 254016180 	 587939 	 14.282934427261353 	 5.071964263916016 	 0.0001354217529296875 	 8.487701416015625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:16:45.331045 test begin: paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[-1,1,3,], )
W0804 18:17:13.026558 52804 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[-1,1,3,], ) 	 254016240 	 587939 	 18.447915077209473 	 5.747300624847412 	 0.00011086463928222656 	 8.106231689453125e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:17:20.813792 test begin: paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[-1,], )
W0804 18:17:39.077247 52826 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[-1,], ) 	 254016240 	 587939 	 10.119512796401978 	 4.246682405471802 	 0.000110626220703125 	 0.0002658367156982422 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:17:47.186485 test begin: paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[2,4,], )
W0804 18:18:08.254647 53232 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[2,4,], ) 	 254016240 	 587939 	 14.176752090454102 	 5.064451217651367 	 0.0001239776611328125 	 0.00025177001953125 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:18:14.918847 test begin: paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[-1,1,3,], )
W0804 18:18:43.617779 53245 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[-1,1,3,], ) 	 254016120 	 587939 	 18.393208026885986 	 5.804273843765259 	 0.00011658668518066406 	 0.00016021728515625 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:18:55.160798 test begin: paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[-1,], )
W0804 18:19:12.106457 53261 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[-1,], ) 	 254016120 	 587939 	 10.078232765197754 	 4.255009174346924 	 0.00010848045349121094 	 0.00016736984252929688 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:19:17.900221 test begin: paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[2,4,], )
W0804 18:19:41.865584 53272 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[2,4,], ) 	 254016120 	 587939 	 14.141577243804932 	 5.08903956413269 	 0.00012874603271484375 	 0.00015616416931152344 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:19:48.517540 test begin: paddle.dstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], ) 	 76204872 	 31252 	 29.45289421081543 	 29.331626415252686 	 0.963068962097168 	 0.9593627452850342 	 29.817747592926025 	 2.1339797973632812 	 0.9751574993133545 	 7.653236389160156e-05 	 
2025-08-04 18:21:22.665374 test begin: paddle.dstack(list[Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2, 1058401],"float64"),], ) 	 25401624 	 31252 	 10.00547981262207 	 9.784839630126953 	 0.3271324634552002 	 0.15990734100341797 	 9.93513822555542 	 1.680762529373169 	 0.324843168258667 	 6.961822509765625e-05 	 
2025-08-04 18:21:55.231514 test begin: paddle.dstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401900 	 31252 	 10.131771087646484 	 10.08158540725708 	 0.3313136100769043 	 0.3293471336364746 	 10.066229820251465 	 2.326798915863037 	 0.32909655570983887 	 8.463859558105469e-05 	 
2025-08-04 18:22:29.203740 test begin: paddle.dstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401900 	 31252 	 10.131812334060669 	 10.285472393035889 	 0.33129429817199707 	 0.33617377281188965 	 10.03984522819519 	 2.1561195850372314 	 0.3284797668457031 	 8.249282836914062e-05 	 
2025-08-04 18:23:02.981836 test begin: paddle.dstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], ) 	 76204836 	 31252 	 30.283870697021484 	 29.345654726028442 	 0.990370512008667 	 0.959327220916748 	 30.186278581619263 	 2.235675811767578 	 0.9869856834411621 	 7.534027099609375e-05 	 
2025-08-04 18:24:39.121489 test begin: paddle.dstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401660 	 31252 	 10.12463665008545 	 9.882508039474487 	 0.3309972286224365 	 0.32300686836242676 	 10.014761209487915 	 2.14542818069458 	 0.32753562927246094 	 0.00017762184143066406 	 
2025-08-04 18:25:12.426119 test begin: paddle.dstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401660 	 31252 	 10.125539779663086 	 10.17559027671814 	 0.3311498165130615 	 0.33270764350891113 	 10.028263330459595 	 2.278498888015747 	 0.32797884941101074 	 6.890296936035156e-05 	 
2025-08-04 18:25:46.197241 test begin: paddle.dstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2116801],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2116801],"float64"),], ) 	 25401660 	 31252 	 10.123443126678467 	 10.000832557678223 	 0.33109235763549805 	 0.32700324058532715 	 10.055851221084595 	 2.129678964614868 	 0.32866334915161133 	 9.942054748535156e-05 	 
2025-08-04 18:26:19.644563 test begin: paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401900 	 31252 	 10.136764764785767 	 9.982798337936401 	 0.3316652774810791 	 0.3256714344024658 	 10.042022228240967 	 2.1484556198120117 	 0.32833361625671387 	 9.441375732421875e-05 	 
2025-08-04 18:26:54.606270 test begin: paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 76204980 	 31252 	 30.246421813964844 	 29.56930160522461 	 0.9893572330474854 	 0.9670684337615967 	 30.252119302749634 	 2.167440414428711 	 0.989203929901123 	 8.20159912109375e-05 	 
2025-08-04 18:28:30.160362 test begin: paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401660 	 31252 	 10.118629455566406 	 9.779378652572632 	 0.3309600353240967 	 0.15990209579467773 	 10.028280973434448 	 1.755617618560791 	 0.3279225826263428 	 9.298324584960938e-05 	 
2025-08-04 18:29:02.950245 test begin: paddle.dstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 76204818 	 31252 	 48.54194259643555 	 75.08598375320435 	 1.5874311923980713 	 2.4554028511047363 	 81.15692710876465 	 2.11498761177063 	 2.6534006595611572 	 6.961822509765625e-05 	 
2025-08-04 18:32:33.219838 test begin: paddle.dstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 76204890 	 31252 	 30.075953006744385 	 78.81670808792114 	 0.9835176467895508 	 2.5775976181030273 	 33.432636976242065 	 2.2177720069885254 	 1.0934317111968994 	 9.417533874511719e-05 	 
2025-08-04 18:35:02.686772 test begin: paddle.dstack(list[Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401630 	 31252 	 12.18998384475708 	 9.77895450592041 	 0.3986043930053711 	 0.15990781784057617 	 12.376381397247314 	 1.67116379737854 	 0.40471720695495605 	 0.00010180473327636719 	 
2025-08-04 18:35:40.357795 test begin: paddle.dstack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], ) 	 76204824 	 31252 	 48.54626679420471 	 75.08629846572876 	 1.5877220630645752 	 2.4554319381713867 	 81.17084813117981 	 2.7998154163360596 	 2.65523624420166 	 0.00020194053649902344 	 
2025-08-04 18:39:11.231855 test begin: paddle.dstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 76204920 	 31252 	 30.080894947052002 	 78.81708359718323 	 0.9838154315948486 	 2.5773916244506836 	 33.298675298690796 	 2.81012225151062 	 1.0891187191009521 	 8.440017700195312e-05 	 
2025-08-04 18:41:42.334999 test begin: paddle.dstack(list[Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401640 	 31252 	 12.178427696228027 	 9.779260158538818 	 0.3982584476470947 	 0.15994787216186523 	 12.368519306182861 	 2.0044827461242676 	 0.4044809341430664 	 8.96453857421875e-05 	 
2025-08-04 18:42:19.966503 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([200, 8, 498, 498],"float32"), Tensor([200, 8, 498, 64],"float32"), )
Warning: The core code of paddle.einsum is too complex.
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([200, 8, 498, 498],"float32"), Tensor([200, 8, 498, 64],"float32"), ) 	 447801600 	 12187 	 74.01589393615723 	 74.0183777809143 	 6.2069008350372314 	 6.208175420761108 	 150.17609810829163 	 115.56589412689209 	 2.5205652713775635 	 4.845581531524658 	 
2025-08-04 18:49:23.973554 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([209, 8, 477, 477],"float32"), Tensor([209, 8, 477, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([209, 8, 477, 477],"float32"), Tensor([209, 8, 477, 64],"float32"), ) 	 431471304 	 12187 	 74.41140270233154 	 72.92732906341553 	 6.11567497253418 	 6.115764856338501 	 149.87986874580383 	 116.47677397727966 	 2.5153725147247314 	 4.8836143016815186 	 
2025-08-04 18:56:25.801719 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([218, 8, 457, 457],"float32"), Tensor([218, 8, 457, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([218, 8, 457, 457],"float32"), Tensor([218, 8, 457, 64],"float32"), ) 	 415241168 	 12187 	 73.88448691368103 	 73.8743028640747 	 6.196127414703369 	 6.194935321807861 	 151.1297619342804 	 119.4754729270935 	 2.535552740097046 	 5.0111000537872314 	 
2025-08-04 19:03:33.502107 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([26, 8, 498, 498],"float32"), Tensor([26, 8, 498, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([26, 8, 498, 498],"float32"), Tensor([26, 8, 498, 64],"float32"), ) 	 58214208 	 12187 	 9.9985671043396 	 9.998067378997803 	 0.8384499549865723 	 0.838409423828125 	 20.1384334564209 	 15.539828777313232 	 0.337949275970459 	 0.6514451503753662 	 
2025-08-04 19:04:30.454869 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([28, 8, 477, 477],"float32"), Tensor([28, 8, 477, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([28, 8, 477, 477],"float32"), Tensor([28, 8, 477, 64],"float32"), ) 	 57804768 	 12187 	 10.689648151397705 	 12.163259744644165 	 0.896472692489624 	 0.8964078426361084 	 21.26463294029236 	 16.664480686187744 	 0.35682082176208496 	 0.6985983848571777 	 
2025-08-04 19:05:33.215239 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 54, 498, 498],"float32"), Tensor([30, 54, 498, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 54, 498, 498],"float32"), Tensor([30, 54, 498, 64],"float32"), ) 	 453399120 	 12187 	 74.06945037841797 	 74.0646858215332 	 6.211097240447998 	 6.211239814758301 	 151.13983273506165 	 116.1730408668518 	 2.53623628616333 	 4.871492624282837 	 
2025-08-04 19:12:40.009591 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 56, 477, 477],"float32"), Tensor([30, 56, 477, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 56, 477, 477],"float32"), Tensor([30, 56, 477, 64],"float32"), ) 	 433535760 	 12187 	 74.9592502117157 	 74.03797459602356 	 6.208815813064575 	 6.208808183670044 	 151.34683322906494 	 117.74021005630493 	 2.5392932891845703 	 4.936851978302002 	 
2025-08-04 19:19:46.359115 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 58, 457, 457],"float32"), Tensor([30, 58, 457, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 58, 457, 457],"float32"), Tensor([30, 58, 457, 64],"float32"), ) 	 414288780 	 12187 	 73.86629271507263 	 73.87159180641174 	 6.194377422332764 	 6.194194555282593 	 151.50318145751953 	 119.31693148612976 	 3.076615810394287 	 5.00279974937439 	 
2025-08-04 19:26:54.558059 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 7, 498, 498],"float32"), Tensor([30, 7, 498, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 7, 498, 498],"float32"), Tensor([30, 7, 498, 64],"float32"), ) 	 58773960 	 12187 	 9.99852705001831 	 9.998373985290527 	 0.8384661674499512 	 0.8384597301483154 	 20.274288654327393 	 15.587670087814331 	 0.34023475646972656 	 0.6535124778747559 	 
2025-08-04 19:27:52.345293 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 426, 498],"float32"), Tensor([30, 8, 498, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 426, 498],"float32"), Tensor([30, 8, 498, 64],"float32"), ) 	 58564800 	 12187 	 11.222780227661133 	 11.222114324569702 	 0.9411749839782715 	 0.9410395622253418 	 20.60855770111084 	 16.062524557113647 	 0.34577155113220215 	 0.6734223365783691 	 
2025-08-04 19:28:54.090335 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 444, 477],"float32"), Tensor([30, 8, 477, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 444, 477],"float32"), Tensor([30, 8, 477, 64],"float32"), ) 	 58155840 	 12187 	 10.70619010925293 	 10.705599546432495 	 0.8978350162506104 	 0.8977577686309814 	 20.96081280708313 	 16.407789707183838 	 0.3517134189605713 	 0.6879112720489502 	 
2025-08-04 19:29:54.269340 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 457, 457],"float32"), Tensor([30, 8, 457, 464],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 457, 457],"float32"), Tensor([30, 8, 457, 464],"float32"), ) 	 101015280 	 12187 	 40.98651480674744 	 40.98694062232971 	 3.437124490737915 	 3.437206983566284 	 92.81335616111755 	 82.1558473110199 	 1.5572688579559326 	 3.444434881210327 	 
2025-08-04 19:34:14.602046 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 464, 457],"float32"), Tensor([30, 8, 457, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 464, 457],"float32"), Tensor([30, 8, 457, 64],"float32"), ) 	 57911040 	 12187 	 10.357210636138916 	 10.357127666473389 	 0.8685448169708252 	 0.8685426712036133 	 21.24781036376953 	 16.74955916404724 	 0.3565518856048584 	 0.702202320098877 	 
2025-08-04 19:35:14.431852 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 477, 477],"float32"), Tensor([30, 8, 477, 444],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 477, 477],"float32"), Tensor([30, 8, 477, 444],"float32"), ) 	 105436080 	 12187 	 42.40683698654175 	 42.408087968826294 	 3.5562617778778076 	 3.5563488006591797 	 93.27168321609497 	 82.15784811973572 	 1.5650641918182373 	 3.4448230266571045 	 
2025-08-04 19:39:37.472013 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 477, 477],"float32"), Tensor([30, 8, 477, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 477, 477],"float32"), Tensor([30, 8, 477, 64],"float32"), ) 	 61933680 	 12187 	 10.710301876068115 	 10.709625959396362 	 0.8981459140777588 	 0.8980834484100342 	 21.98738384246826 	 17.085843563079834 	 0.3689093589782715 	 0.7163360118865967 	 
2025-08-04 19:40:41.819424 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 498, 498],"float32"), Tensor([30, 8, 498, 426],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 498, 498],"float32"), Tensor([30, 8, 498, 426],"float32"), ) 	 110436480 	 12187 	 44.506335496902466 	 44.76051688194275 	 3.7322165966033936 	 3.7339582443237305 	 94.29670691490173 	 82.88303589820862 	 1.5823945999145508 	 3.4749655723571777 	 
2025-08-04 19:45:12.649010 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 9, 457, 457],"float32"), Tensor([30, 9, 457, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 9, 457, 457],"float32"), Tensor([30, 9, 457, 64],"float32"), ) 	 64286190 	 12187 	 11.49288558959961 	 11.500611066818237 	 0.9637789726257324 	 0.9639191627502441 	 23.67324995994568 	 18.678733587265015 	 0.39723873138427734 	 0.7831144332885742 	 
2025-08-04 19:46:19.291260 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([31, 8, 457, 457],"float32"), Tensor([31, 8, 457, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([31, 8, 457, 457],"float32"), Tensor([31, 8, 457, 64],"float32"), ) 	 59048056 	 12187 	 11.464895963668823 	 11.464712381362915 	 0.9615085124969482 	 0.961449146270752 	 22.690708875656128 	 18.085844039916992 	 0.3807814121246338 	 0.7582352161407471 	 
2025-08-04 19:47:24.184205 test begin: paddle.empty_like(Tensor([1016064010],"uint8"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([1016064010],"uint8"), ) 	 1016064010 	 694973 	 7.765969514846802 	 6.314008712768555 	 7.176399230957031e-05 	 0.0002791881561279297 	 None 	 None 	 None 	 None 	 
2025-08-04 19:47:49.114240 test begin: paddle.empty_like(Tensor([40960, 12404],"bool"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([40960, 12404],"bool"), ) 	 508067840 	 694973 	 8.008947372436523 	 3.6758604049682617 	 8.368492126464844e-05 	 0.00021839141845703125 	 None 	 None 	 None 	 None 	 
2025-08-04 19:48:07.845868 test begin: paddle.empty_like(Tensor([40960, 12404],"float32"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([40960, 12404],"float32"), ) 	 508067840 	 694973 	 7.890241384506226 	 3.69220232963562 	 4.315376281738281e-05 	 0.0002028942108154297 	 None 	 None 	 None 	 None 	 
2025-08-04 19:48:27.796771 test begin: paddle.empty_like(Tensor([7938010, 64],"bool"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([7938010, 64],"bool"), ) 	 508032640 	 694973 	 8.022253036499023 	 4.69896388053894 	 8.177757263183594e-05 	 0.00011372566223144531 	 None 	 None 	 None 	 None 	 
2025-08-04 19:48:49.396197 test begin: paddle.empty_like(Tensor([7938010, 64],"float32"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([7938010, 64],"float32"), ) 	 508032640 	 694973 	 7.78973650932312 	 3.5645198822021484 	 9.036064147949219e-05 	 8.392333984375e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 19:49:09.021770 test begin: paddle.equal(Tensor([4148, 6124],"int64"), Tensor([4148, 6124],"int64"), )
[Prof] paddle.equal 	 paddle.equal(Tensor([4148, 6124],"int64"), Tensor([4148, 6124],"int64"), ) 	 50804704 	 56247 	 17.403652906417847 	 17.60919761657715 	 0.3162267208099365 	 0.31998252868652344 	 None 	 None 	 None 	 None 	 
2025-08-04 19:49:45.010304 test begin: paddle.equal(Tensor([416, 61062],"int64"), 0, )
[Prof] paddle.equal 	 paddle.equal(Tensor([416, 61062],"int64"), 0, ) 	 25401792 	 56247 	 10.01067328453064 	 10.173138618469238 	 0.09095931053161621 	 0.1718151569366455 	 None 	 None 	 None 	 None 	 
2025-08-04 19:50:07.250497 test begin: paddle.equal(Tensor([512, 49613],"int64"), 0, )
[Prof] paddle.equal 	 paddle.equal(Tensor([512, 49613],"int64"), 0, ) 	 25401856 	 56247 	 9.975367784500122 	 9.455137491226196 	 0.09064984321594238 	 0.17180991172790527 	 None 	 None 	 None 	 None 	 
2025-08-04 19:50:27.152607 test begin: paddle.equal(Tensor([846721, 30],"int64"), 0, )
[Prof] paddle.equal 	 paddle.equal(Tensor([846721, 30],"int64"), 0, ) 	 25401630 	 56247 	 10.017090797424316 	 11.335110187530518 	 0.09103083610534668 	 0.17184734344482422 	 None 	 None 	 None 	 None 	 
2025-08-04 19:50:50.755568 test begin: paddle.equal(Tensor([846721, 30],"int64"), Tensor([846721, 30],"int64"), )
[Prof] paddle.equal 	 paddle.equal(Tensor([846721, 30],"int64"), Tensor([846721, 30],"int64"), ) 	 50803260 	 56247 	 17.47006344795227 	 17.613012552261353 	 0.317413330078125 	 0.3200054168701172 	 None 	 None 	 None 	 None 	 
2025-08-04 19:51:27.963160 test begin: paddle.equal_all(Tensor([1, 2, 10, 2540161],"bool"), Tensor([1, 2, 10, 2540161],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 2, 10, 2540161],"bool"), Tensor([1, 2, 10, 2540161],"bool"), ) 	 101606440 	 555286 	 94.33381962776184 	 113.7841010093689 	 0.05781269073486328 	 0.00023555755615234375 	 None 	 None 	 None 	 None 	 
2025-08-04 19:54:57.526667 test begin: paddle.equal_all(Tensor([1, 2, 1587601, 16],"bool"), Tensor([1, 2, 1587601, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 2, 1587601, 16],"bool"), Tensor([1, 2, 1587601, 16],"bool"), ) 	 101606464 	 555286 	 94.3319034576416 	 113.89514803886414 	 0.05780649185180664 	 0.0002396106719970703 	 None 	 None 	 None 	 None 	 
2025-08-04 19:58:27.200961 test begin: paddle.equal_all(Tensor([1, 317521, 10, 16],"bool"), Tensor([1, 317521, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 317521, 10, 16],"bool"), Tensor([1, 317521, 10, 16],"bool"), ) 	 101606720 	 555286 	 94.36013650894165 	 114.18689823150635 	 0.05782771110534668 	 0.0002911090850830078 	 None 	 None 	 None 	 None 	 
2025-08-04 20:01:57.185313 test begin: paddle.equal_all(Tensor([101, 2, 10, 16],"bool"), Tensor([158761, 2, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([101, 2, 10, 16],"bool"), Tensor([158761, 2, 10, 16],"bool"), ) 	 50835840 	 555286 	 9.59346628189087 	 1.4435770511627197 	 6.151199340820312e-05 	 9.751319885253906e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 20:02:08.985814 test begin: paddle.equal_all(Tensor([12801],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([12801],"float32"), Tensor([50803201],"float32"), ) 	 50816002 	 555286 	 11.855380773544312 	 1.4780783653259277 	 0.00010085105895996094 	 7.2479248046875e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 20:02:23.178936 test begin: paddle.equal_all(Tensor([158761, 2, 10, 16],"bool"), Tensor([101, 2, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([158761, 2, 10, 16],"bool"), Tensor([101, 2, 10, 16],"bool"), ) 	 50835840 	 555286 	 9.602570533752441 	 1.6816067695617676 	 0.00010848045349121094 	 0.0002760887145996094 	 None 	 None 	 None 	 None 	 
2025-08-04 20:02:36.638631 test begin: paddle.equal_all(Tensor([158761, 2, 10, 16],"bool"), Tensor([158761, 2, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([158761, 2, 10, 16],"bool"), Tensor([158761, 2, 10, 16],"bool"), ) 	 101607040 	 555286 	 94.15781903266907 	 114.61640691757202 	 0.05769467353820801 	 0.0002384185791015625 	 None 	 None 	 None 	 None 	 
2025-08-04 20:06:07.125467 test begin: paddle.equal_all(Tensor([16, 3175201],"float32"), Tensor([16, 3175201],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([16, 3175201],"float32"), Tensor([16, 3175201],"float32"), ) 	 101606432 	 555286 	 210.97132277488708 	 231.8732409477234 	 0.12923288345336914 	 0.0002455711364746094 	 None 	 None 	 None 	 None 	 
2025-08-04 20:13:34.570652 test begin: paddle.equal_all(Tensor([16, 3175201],"float32"), Tensor([1601, 16],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([16, 3175201],"float32"), Tensor([1601, 16],"float32"), ) 	 50828832 	 555286 	 9.683653593063354 	 1.4304463863372803 	 0.00010728836059570312 	 0.00015997886657714844 	 None 	 None 	 None 	 None 	 
2025-08-04 20:13:46.550193 test begin: paddle.equal_all(Tensor([1601, 16],"float32"), Tensor([16, 3175201],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1601, 16],"float32"), Tensor([16, 3175201],"float32"), ) 	 50828832 	 555286 	 9.639907836914062 	 1.444058895111084 	 0.00011014938354492188 	 0.0001685619354248047 	 None 	 None 	 None 	 None 	 
2025-08-04 20:13:58.496066 test begin: paddle.equal_all(Tensor([1601, 16],"float32"), Tensor([3175201, 16],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1601, 16],"float32"), Tensor([3175201, 16],"float32"), ) 	 50828832 	 555286 	 9.678027629852295 	 1.4432969093322754 	 0.00010967254638671875 	 0.00011920928955078125 	 None 	 None 	 None 	 None 	 
2025-08-04 20:14:12.427128 test begin: paddle.equal_all(Tensor([3175201, 16],"float32"), Tensor([1601, 16],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([3175201, 16],"float32"), Tensor([1601, 16],"float32"), ) 	 50828832 	 555286 	 9.628968715667725 	 1.4483952522277832 	 0.00010657310485839844 	 0.00012063980102539062 	 None 	 None 	 None 	 None 	 
2025-08-04 20:14:24.387381 test begin: paddle.equal_all(Tensor([3175201, 16],"float32"), Tensor([3175201, 16],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([3175201, 16],"float32"), Tensor([3175201, 16],"float32"), ) 	 101606432 	 555286 	 210.9695975780487 	 231.68609237670898 	 0.12921595573425293 	 0.00023937225341796875 	 None 	 None 	 None 	 None 	 
2025-08-04 20:21:48.791450 test begin: paddle.equal_all(Tensor([50803201],"float32"), Tensor([12801],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([50803201],"float32"), Tensor([12801],"float32"), ) 	 50816002 	 555286 	 10.254075288772583 	 1.45607590675354 	 0.0001049041748046875 	 8.7738037109375e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 20:22:01.327105 test begin: paddle.equal_all(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 555286 	 210.96848344802856 	 231.94113850593567 	 0.12923192977905273 	 0.0002334117889404297 	 None 	 None 	 None 	 None 	 
2025-08-04 20:29:29.007067 test begin: paddle.erf(Tensor([11, 2309237],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([11, 2309237],"float64"), ) 	 25401607 	 29636 	 10.229194402694702 	 9.005346775054932 	 0.35463905334472656 	 0.31035947799682617 	 13.26121187210083 	 48.53448843955994 	 0.45731210708618164 	 0.33478689193725586 	 
2025-08-04 20:30:51.293767 test begin: paddle.erf(Tensor([1494212, 17],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([1494212, 17],"float64"), ) 	 25401604 	 29636 	 10.154462099075317 	 9.017996072769165 	 0.3516702651977539 	 0.3104724884033203 	 13.261888980865479 	 48.53745460510254 	 0.4573063850402832 	 0.33484911918640137 	 
2025-08-04 20:32:13.404179 test begin: paddle.erf(Tensor([211681, 2, 3, 5, 4],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([211681, 2, 3, 5, 4],"float64"), ) 	 25401720 	 29636 	 10.147616624832153 	 9.020530462265015 	 0.3507344722747803 	 0.31060338020324707 	 13.277544736862183 	 48.53728890419006 	 0.45792531967163086 	 0.33481621742248535 	 
2025-08-04 20:33:39.772550 test begin: paddle.erf(Tensor([4, 105841, 3, 5, 4],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 105841, 3, 5, 4],"float64"), ) 	 25401840 	 29636 	 10.104482173919678 	 9.006133556365967 	 0.3500998020172119 	 0.3105480670928955 	 13.282921552658081 	 48.538127183914185 	 0.45801258087158203 	 0.3348658084869385 	 
2025-08-04 20:35:01.854576 test begin: paddle.erf(Tensor([4, 2, 158761, 5, 4],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 158761, 5, 4],"float64"), ) 	 25401760 	 29636 	 10.121168851852417 	 9.01255464553833 	 0.34999823570251465 	 0.31051182746887207 	 13.26046109199524 	 48.53730058670044 	 0.4572751522064209 	 0.3347961902618408 	 
2025-08-04 20:36:30.551402 test begin: paddle.erf(Tensor([4, 2, 3, 1058401],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 3, 1058401],"float64"), ) 	 25401624 	 29636 	 10.112199068069458 	 9.005703210830688 	 0.350161075592041 	 0.3105473518371582 	 13.261519193649292 	 48.53657817840576 	 0.45737171173095703 	 0.33474135398864746 	 
2025-08-04 20:37:54.651858 test begin: paddle.erf(Tensor([4, 2, 3, 264601, 4],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 3, 264601, 4],"float64"), ) 	 25401696 	 29636 	 10.204678058624268 	 9.001757860183716 	 0.35472869873046875 	 0.31051182746887207 	 13.264755964279175 	 48.53825616836548 	 0.45749354362487793 	 0.33488893508911133 	 
2025-08-04 20:39:16.799145 test begin: paddle.erf(Tensor([4, 2, 3, 5, 211681],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 3, 5, 211681],"float64"), ) 	 25401720 	 29636 	 10.149142980575562 	 9.256176948547363 	 0.35025763511657715 	 0.3106534481048584 	 13.277968406677246 	 48.53790044784546 	 0.4579176902770996 	 0.3348348140716553 	 
2025-08-04 20:40:44.537495 test begin: paddle.erf(Tensor([4, 2, 635041, 5],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 635041, 5],"float64"), ) 	 25401640 	 29636 	 10.156377077102661 	 8.998681783676147 	 0.3515512943267822 	 0.31033802032470703 	 13.252864599227905 	 48.53717494010925 	 0.4570910930633545 	 0.3347644805908203 	 
2025-08-04 20:42:08.886146 test begin: paddle.erf(Tensor([4, 423361, 3, 5],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 423361, 3, 5],"float64"), ) 	 25401660 	 29636 	 10.21412992477417 	 10.570889711380005 	 0.35262179374694824 	 0.31052350997924805 	 13.253617525100708 	 48.53655123710632 	 0.4570038318634033 	 0.33484983444213867 	 
2025-08-04 20:43:34.094594 test begin: paddle.erf(Tensor([846721, 2, 3, 5],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([846721, 2, 3, 5],"float64"), ) 	 25401630 	 29636 	 10.170064926147461 	 9.004539012908936 	 0.35186123847961426 	 0.3104867935180664 	 13.263769626617432 	 48.53822422027588 	 0.45729947090148926 	 0.33486318588256836 	 
2025-08-04 20:44:58.543144 test begin: paddle.erfinv(x=Tensor([211681, 2, 3, 5, 4],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([211681, 2, 3, 5, 4],"float64"), ) 	 25401720 	 29882 	 10.534972190856934 	 9.821155071258545 	 0.36090922355651855 	 0.3367190361022949 	 13.384526491165161 	 49.06372427940369 	 0.4576561450958252 	 0.3356485366821289 	 
2025-08-04 20:46:22.489000 test begin: paddle.erfinv(x=Tensor([4, 105841, 3, 5, 4],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 105841, 3, 5, 4],"float64"), ) 	 25401840 	 29882 	 10.56336522102356 	 9.815901756286621 	 0.36133456230163574 	 0.33815503120422363 	 13.382089614868164 	 49.06056475639343 	 0.4577207565307617 	 0.3356924057006836 	 
2025-08-04 20:47:46.447751 test begin: paddle.erfinv(x=Tensor([4, 2, 158761, 5, 4],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 158761, 5, 4],"float64"), ) 	 25401760 	 29882 	 10.52442193031311 	 9.814757347106934 	 0.36298346519470215 	 0.33733201026916504 	 13.364238739013672 	 49.057626724243164 	 0.4570658206939697 	 0.335559606552124 	 
2025-08-04 20:49:13.312591 test begin: paddle.erfinv(x=Tensor([4, 2, 3, 1058401],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 3, 1058401],"float64"), ) 	 25401624 	 29882 	 10.529700756072998 	 9.80574655532837 	 0.36069798469543457 	 0.33528780937194824 	 13.362267255783081 	 49.056777477264404 	 0.4549286365509033 	 0.3356499671936035 	 
2025-08-04 20:50:39.474083 test begin: paddle.erfinv(x=Tensor([4, 2, 3, 264601, 4],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 3, 264601, 4],"float64"), ) 	 25401696 	 29882 	 10.628000497817993 	 9.789515256881714 	 0.3630959987640381 	 0.3339416980743408 	 13.365010499954224 	 49.05755805969238 	 0.4570581912994385 	 0.33561158180236816 	 
2025-08-04 20:52:04.215394 test begin: paddle.erfinv(x=Tensor([4, 2, 3, 5, 211681],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 3, 5, 211681],"float64"), ) 	 25401720 	 29882 	 10.563845872879028 	 9.833593606948853 	 0.3600640296936035 	 0.33520960807800293 	 13.383152723312378 	 49.06395196914673 	 0.45761871337890625 	 0.3356809616088867 	 
2025-08-04 20:53:29.093022 test begin: paddle.erfinv(x=Tensor([4, 2, 3175201],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 3175201],"float64"), ) 	 25401608 	 29882 	 10.538683891296387 	 9.827431678771973 	 0.3607769012451172 	 0.3381814956665039 	 13.362175464630127 	 49.062516927719116 	 0.4570009708404541 	 0.3356609344482422 	 
2025-08-04 20:54:53.014359 test begin: paddle.erfinv(x=Tensor([4, 2, 635041, 5],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 635041, 5],"float64"), ) 	 25401640 	 29882 	 10.59951400756836 	 9.740408658981323 	 0.36168503761291504 	 0.33441925048828125 	 13.370450019836426 	 49.05777192115784 	 0.4572925567626953 	 0.3356921672821045 	 
2025-08-04 20:56:17.676988 test begin: paddle.erfinv(x=Tensor([4, 2116801, 3],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2116801, 3],"float64"), ) 	 25401612 	 29882 	 10.424436330795288 	 9.942295789718628 	 0.35694098472595215 	 0.32904839515686035 	 13.362053394317627 	 49.05668902397156 	 0.45699453353881836 	 0.3356208801269531 	 
2025-08-04 20:57:44.215208 test begin: paddle.erfinv(x=Tensor([4, 423361, 3, 5],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 423361, 3, 5],"float64"), ) 	 25401660 	 29882 	 10.367335319519043 	 9.647742748260498 	 0.35532665252685547 	 0.33005475997924805 	 13.37017273902893 	 49.06617259979248 	 0.4572725296020508 	 0.33568620681762695 	 
2025-08-04 20:59:08.619747 test begin: paddle.erfinv(x=Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4233601, 2, 3],"float64"), ) 	 25401606 	 29882 	 10.440982341766357 	 9.704688787460327 	 0.35671567916870117 	 0.3318161964416504 	 13.362165689468384 	 49.05747580528259 	 0.4569554328918457 	 0.33556413650512695 	 
2025-08-04 21:00:32.390834 test begin: paddle.erfinv(x=Tensor([846721, 2, 3, 5],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([846721, 2, 3, 5],"float64"), ) 	 25401630 	 29882 	 10.568897008895874 	 9.848207235336304 	 0.36121273040771484 	 0.3374812602996826 	 13.362149477005005 	 49.06416463851929 	 0.45699119567871094 	 0.3356466293334961 	 
2025-08-04 21:01:59.129214 test begin: paddle.exp(Tensor([125, 1, 640, 640],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([125, 1, 640, 640],"float32"), ) 	 51200000 	 33820 	 10.084043025970459 	 10.147536039352417 	 0.30474281311035156 	 0.30651330947875977 	 15.338585615158081 	 15.215944290161133 	 0.46356916427612305 	 0.45975446701049805 	 
2025-08-04 21:02:51.668353 test begin: paddle.exp(Tensor([13, 243, 1007, 16],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([13, 243, 1007, 16],"float32"), ) 	 50897808 	 33820 	 10.013275623321533 	 10.084357738494873 	 0.302562952041626 	 0.30475664138793945 	 15.248566150665283 	 15.12813138961792 	 0.46070170402526855 	 0.4571194648742676 	 
2025-08-04 21:03:44.592062 test begin: paddle.exp(Tensor([13, 64, 1007, 61],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([13, 64, 1007, 61],"float32"), ) 	 51107264 	 33820 	 10.053886890411377 	 10.125478267669678 	 0.30379629135131836 	 0.30598020553588867 	 15.307947635650635 	 15.188454866409302 	 0.4625856876373291 	 0.45894670486450195 	 
2025-08-04 21:04:38.515642 test begin: paddle.exp(Tensor([13, 64, 3817, 16],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([13, 64, 3817, 16],"float32"), ) 	 50811904 	 33820 	 10.000284194946289 	 10.074023962020874 	 0.3022134304046631 	 0.30422043800354004 	 15.228431224822998 	 15.104959726333618 	 0.4601452350616455 	 0.45645618438720703 	 
2025-08-04 21:05:31.297774 test begin: paddle.exp(Tensor([16, 1, 4962, 640],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([16, 1, 4962, 640],"float32"), ) 	 50810880 	 33820 	 10.010152339935303 	 10.073488473892212 	 0.3025217056274414 	 0.3042011260986328 	 15.227883100509644 	 15.104929447174072 	 0.4602060317993164 	 0.45644712448120117 	 
2025-08-04 21:06:23.501128 test begin: paddle.exp(Tensor([16, 1, 640, 4962],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([16, 1, 640, 4962],"float32"), ) 	 50810880 	 33820 	 10.009885787963867 	 10.067574262619019 	 0.3024790287017822 	 0.30423760414123535 	 15.227686166763306 	 15.104898452758789 	 0.46019577980041504 	 0.4564192295074463 	 
2025-08-04 21:07:15.593267 test begin: paddle.exp(Tensor([16, 8, 640, 640],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([16, 8, 640, 640],"float32"), ) 	 52428800 	 33820 	 10.31536602973938 	 10.386400938034058 	 0.3117187023162842 	 0.31368589401245117 	 15.707354545593262 	 15.57602858543396 	 0.47472500801086426 	 0.4704771041870117 	 
2025-08-04 21:08:11.237033 test begin: paddle.exp(Tensor([50, 64, 1007, 16],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([50, 64, 1007, 16],"float32"), ) 	 51558400 	 33820 	 10.154278755187988 	 10.225223302841187 	 0.30689048767089844 	 0.30860352516174316 	 15.44893217086792 	 15.325147867202759 	 0.4668242931365967 	 0.46309566497802734 	 
2025-08-04 21:09:04.151213 test begin: paddle.exp(Tensor([56, 1, 960, 960],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([56, 1, 960, 960],"float32"), ) 	 51609600 	 33820 	 10.163694858551025 	 10.222451210021973 	 0.3070991039276123 	 0.3088650703430176 	 15.46423625946045 	 15.340054273605347 	 0.46723294258117676 	 0.4635763168334961 	 
2025-08-04 21:09:58.445875 test begin: paddle.exp(Tensor([8, 1, 6616, 960],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([8, 1, 6616, 960],"float32"), ) 	 50810880 	 33820 	 10.010009288787842 	 10.067585706710815 	 0.302476167678833 	 0.3042778968811035 	 15.227806806564331 	 15.105011701583862 	 0.4601783752441406 	 0.4564399719238281 	 
2025-08-04 21:10:50.663155 test begin: paddle.exp(Tensor([8, 1, 960, 6616],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([8, 1, 960, 6616],"float32"), ) 	 50810880 	 33820 	 10.009881734848022 	 10.0809965133667 	 0.30254054069519043 	 0.3042130470275879 	 15.227367162704468 	 15.104740381240845 	 0.4601712226867676 	 0.45643043518066406 	 
2025-08-04 21:11:44.847020 test begin: paddle.exp(Tensor([8, 7, 960, 960],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([8, 7, 960, 960],"float32"), ) 	 51609600 	 33820 	 10.163524627685547 	 10.222463130950928 	 0.307145357131958 	 0.308887243270874 	 15.46340036392212 	 15.339845657348633 	 0.4673471450805664 	 0.4635131359100342 	 
2025-08-04 21:12:39.822984 test begin: paddle.expand_as(Tensor([1621, 80, 1, 1],"float32"), Tensor([1621, 80, 28, 28],"float16"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([1621, 80, 1, 1],"float32"), Tensor([1621, 80, 28, 28],"float16"), ) 	 101798800 	 72722 	 19.640767097473145 	 0.26932501792907715 	 0.276015043258667 	 7.510185241699219e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:13:26.950393 test begin: paddle.expand_as(Tensor([511, 127, 1, 1],"float32"), Tensor([511, 127, 28, 28],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([511, 127, 1, 1],"float32"), Tensor([511, 127, 28, 28],"float32"), ) 	 50944145 	 72722 	 9.98340129852295 	 0.27438879013061523 	 0.14024710655212402 	 9.942054748535156e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:13:50.144470 test begin: paddle.expand_as(Tensor([511, 80, 1, 1243],"float32"), Tensor([511, 80, 28, 1243],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd9e421ecb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:24:02.126371 test begin: paddle.expand_as(Tensor([511, 80, 1, 1],"float32"), Tensor([511, 80, 28, 45],"float32"), )
W0804 21:24:03.110798 68826 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([511, 80, 1, 1],"float32"), Tensor([511, 80, 28, 45],"float32"), ) 	 51549680 	 72722 	 10.124017238616943 	 0.2874741554260254 	 0.14225459098815918 	 6.008148193359375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:24:25.923578 test begin: paddle.expand_as(Tensor([511, 80, 1, 1],"float32"), Tensor([511, 80, 45, 28],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([511, 80, 1, 1],"float32"), Tensor([511, 80, 45, 28],"float32"), ) 	 51549680 	 72722 	 10.125122785568237 	 0.26053619384765625 	 0.14226651191711426 	 4.863739013671875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:24:49.053538 test begin: paddle.expand_as(Tensor([511, 80, 1243, 1],"float32"), Tensor([511, 80, 1243, 28],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7c5bd6f1f0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:35:00.730655 test begin: paddle.expand_as(Tensor([512, 127, 1, 1],"float32"), Tensor([512, 127, 28, 28],"float32"), )
W0804 21:35:01.645231 69232 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 127, 1, 1],"float32"), Tensor([512, 127, 28, 28],"float32"), ) 	 51043840 	 72722 	 10.00392460823059 	 0.294048547744751 	 0.14058446884155273 	 5.316734313964844e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:35:24.375585 test begin: paddle.expand_as(Tensor([512, 254, 1, 1],"float32"), Tensor([512, 254, 28, 28],"float16"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 254, 1, 1],"float32"), Tensor([512, 254, 28, 28],"float16"), ) 	 102087680 	 72722 	 19.719460487365723 	 0.2701694965362549 	 0.27711987495422363 	 7.62939453125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:36:09.640242 test begin: paddle.expand_as(Tensor([512, 80, 1, 1241],"float32"), Tensor([512, 80, 28, 1241],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2c85f03190>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:46:18.615614 test begin: paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 28, 45],"float32"), )
W0804 21:46:19.521921 69712 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 28, 45],"float32"), ) 	 51650560 	 72722 	 10.148423194885254 	 0.3044412136077881 	 0.14258289337158203 	 7.62939453125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:46:42.552700 test begin: paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 28, 89],"float16"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 28, 89],"float16"), ) 	 102113280 	 72722 	 19.72070813179016 	 0.3144857883453369 	 0.27710938453674316 	 0.00016117095947265625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:47:27.421324 test begin: paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 45, 28],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 45, 28],"float32"), ) 	 51650560 	 72722 	 10.142337560653687 	 0.2757716178894043 	 0.1424875259399414 	 9.751319885253906e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:47:50.585517 test begin: paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 89, 28],"float16"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 89, 28],"float16"), ) 	 102113280 	 72722 	 19.72068166732788 	 0.2803628444671631 	 0.2771599292755127 	 9.489059448242188e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:48:35.095497 test begin: paddle.expand_as(Tensor([512, 80, 1241, 1],"float32"), Tensor([512, 80, 1241, 28],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f306b61eb60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:58:46.649345 test begin: paddle.expand_as(Tensor([811, 80, 1, 1],"float32"), Tensor([811, 80, 28, 28],"float32"), )
W0804 21:58:47.606037 70198 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([811, 80, 1, 1],"float32"), Tensor([811, 80, 28, 28],"float32"), ) 	 50930800 	 72722 	 9.985069751739502 	 0.5331647396087646 	 0.14029741287231445 	 8.463859558105469e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:59:10.667063 test begin: paddle.expm1(Tensor([198451, 16, 32],"float16"), )
[Prof] paddle.expm1 	 paddle.expm1(Tensor([198451, 16, 32],"float16"), ) 	 101606912 	 29864 	 9.994358777999878 	 9.074757099151611 	 0.3420860767364502 	 0.3104252815246582 	 13.379286527633667 	 22.25675129890442 	 0.4577333927154541 	 0.380781888961792 	 
2025-08-04 22:00:10.269134 test begin: paddle.expm1(Tensor([8, 16, 793801],"float16"), )
[Prof] paddle.expm1 	 paddle.expm1(Tensor([8, 16, 793801],"float16"), ) 	 101606528 	 29864 	 9.998106002807617 	 9.084352254867554 	 0.34217286109924316 	 0.31032633781433105 	 13.37385869026184 	 22.25594186782837 	 0.4579167366027832 	 0.38083362579345703 	 
2025-08-04 22:01:12.031074 test begin: paddle.expm1(Tensor([8, 396901, 32],"float16"), )
[Prof] paddle.expm1 	 paddle.expm1(Tensor([8, 396901, 32],"float16"), ) 	 101606656 	 29864 	 9.995251178741455 	 10.092929363250732 	 0.3418700695037842 	 0.31037235260009766 	 13.373782873153687 	 22.255457639694214 	 0.4575037956237793 	 0.38074278831481934 	 
2025-08-04 22:02:13.228153 test begin: paddle.fft.fftn(Tensor([226801, 7, 32],"float32"), )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([226801, 7, 32],"float32"), ) 	 50803424 	 1679 	 18.695646047592163 	 26.752259254455566 	 5.054473876953125e-05 	 1.8096516132354736 	 32.88623118400574 	 25.93904447555542 	 1.539461612701416 	 1.9731552600860596 	 
2025-08-04 22:04:01.497088 test begin: paddle.fft.fftn(Tensor([39, 40708, 32],"float32"), )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([39, 40708, 32],"float32"), ) 	 50803584 	 1679 	 15.686592102050781 	 17.87951683998108 	 5.173683166503906e-05 	 1.088796854019165 	 20.528562545776367 	 16.840863466262817 	 1.3548266887664795 	 1.1390860080718994 	 
2025-08-04 22:05:16.198371 test begin: paddle.fft.fftn(Tensor([39, 7, 186093],"float32"), )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([39, 7, 186093],"float32"), ) 	 50803389 	 1679 	 11.680620670318604 	 10.175396203994751 	 5.2928924560546875e-05 	 0.77597975730896 	 12.658524751663208 	 9.415593385696411 	 0.7700841426849365 	 0.8182241916656494 	 
2025-08-04 22:06:02.093162 test begin: paddle.fft.fftn(Tensor([7, 32, 481, 481],"float32"), axes=list[2,3,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([7, 32, 481, 481],"float32"), axes=list[2,3,], ) 	 51824864 	 1679 	 10.02105450630188 	 7.432758331298828 	 4.982948303222656e-05 	 0.9066548347473145 	 9.258846759796143 	 6.869906187057495 	 0.8050851821899414 	 1.1951661109924316 	 
2025-08-04 22:06:37.880317 test begin: paddle.fft.fftn(Tensor([8, 28, 481, 481],"float32"), axes=list[2,3,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([8, 28, 481, 481],"float32"), axes=list[2,3,], ) 	 51824864 	 1679 	 10.006218671798706 	 7.419352054595947 	 5.0067901611328125e-05 	 0.9048140048980713 	 9.259208917617798 	 6.956830263137817 	 0.8051879405975342 	 1.2861285209655762 	 
2025-08-04 22:07:17.110795 test begin: paddle.fft.fftn(Tensor([8, 32, 413, 481],"float32"), axes=list[2,3,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([8, 32, 413, 481],"float32"), axes=list[2,3,], ) 	 50855168 	 1679 	 10.005773067474365 	 7.1402037143707275 	 5.245208740234375e-05 	 0.8701703548431396 	 9.394147634506226 	 6.409483432769775 	 0.8168454170227051 	 0.9760298728942871 	 
2025-08-04 22:07:54.624978 test begin: paddle.fft.fftn(Tensor([8, 32, 481, 413],"float32"), axes=list[2,3,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([8, 32, 481, 413],"float32"), axes=list[2,3,], ) 	 50855168 	 1679 	 10.002434015274048 	 7.099848031997681 	 2.4557113647460938e-05 	 0.8642086982727051 	 9.423415184020996 	 6.354562044143677 	 0.8194189071655273 	 0.966742753982544 	 
2025-08-04 22:08:29.485974 test begin: paddle.fft.fftn(x=Tensor([50, 133, 39, 14, 14],"float32"), axes=list[-3,-2,-1,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(x=Tensor([50, 133, 39, 14, 14],"float32"), axes=list[-3,-2,-1,], ) 	 50832600 	 1679 	 10.079094171524048 	 4.993820667266846 	 5.2928924560546875e-05 	 0.6079282760620117 	 7.346461057662964 	 4.183873653411865 	 0.6388556957244873 	 0.6366486549377441 	 
2025-08-04 22:08:58.727977 test begin: paddle.fft.fftn(x=Tensor([50, 8, 39, 14, 233],"float32"), axes=list[-3,-2,-1,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(x=Tensor([50, 8, 39, 14, 233],"float32"), axes=list[-3,-2,-1,], ) 	 50887200 	 1679 	 11.428159713745117 	 6.190725803375244 	 4.9591064453125e-05 	 0.7541897296905518 	 9.9968581199646 	 5.391329765319824 	 0.8691115379333496 	 0.821552038192749 	 
2025-08-04 22:09:34.187843 test begin: paddle.fft.fftn(x=Tensor([50, 8, 39, 233, 14],"float32"), axes=list[-3,-2,-1,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(x=Tensor([50, 8, 39, 233, 14],"float32"), axes=list[-3,-2,-1,], ) 	 50887200 	 1679 	 12.114588022232056 	 6.930139780044556 	 4.935264587402344e-05 	 0.8443155288696289 	 10.948148012161255 	 6.119285821914673 	 0.951927900314331 	 0.9316327571868896 	 
2025-08-04 22:10:12.411169 test begin: paddle.fft.fftn(x=Tensor([50, 8, 649, 14, 14],"float32"), axes=list[-3,-2,-1,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(x=Tensor([50, 8, 649, 14, 14],"float32"), axes=list[-3,-2,-1,], ) 	 50881600 	 1679 	 10.670301675796509 	 5.9659106731414795 	 3.7670135498046875e-05 	 0.725806474685669 	 8.321521282196045 	 5.154872179031372 	 0.7236514091491699 	 0.7843163013458252 	 
2025-08-04 22:10:46.659239 test begin: paddle.fft.fftn(x=Tensor([831, 8, 39, 14, 14],"float32"), axes=list[-3,-2,-1,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(x=Tensor([831, 8, 39, 14, 14],"float32"), axes=list[-3,-2,-1,], ) 	 50817312 	 1679 	 10.109474182128906 	 5.000514268875122 	 4.9114227294921875e-05 	 0.6078503131866455 	 7.343121290206909 	 4.186109781265259 	 0.63848876953125 	 0.637451171875 	 
2025-08-04 22:11:17.446914 test begin: paddle.fft.ifftn(x=Tensor([4, 4, 6, 264601],"float64"), s=list[2,4,], axes=tuple(0,1,), )
[Prof] paddle.fft.ifftn 	 paddle.fft.ifftn(x=Tensor([4, 4, 6, 264601],"float64"), s=list[2,4,], axes=tuple(0,1,), ) 	 25401696 	 41344 	 123.9229166507721 	 57.49224853515625 	 5.507469177246094e-05 	 0.3552267551422119 	 118.43234300613403 	 78.59814381599426 	 0.36602210998535156 	 0.3887450695037842 	 
2025-08-04 22:17:38.808590 test begin: paddle.fft.ifftn(x=Tensor([4, 4, 793801, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), )
[Prof] paddle.fft.ifftn 	 paddle.fft.ifftn(x=Tensor([4, 4, 793801, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), ) 	 25401632 	 41344 	 117.65783786773682 	 57.486802101135254 	 5.793571472167969e-05 	 0.3551056385040283 	 107.44248342514038 	 78.592538356781 	 0.33190083503723145 	 0.38871026039123535 	 
2025-08-04 22:23:45.449938 test begin: paddle.fft.ifftn(x=Tensor([4, 529201, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), )
[Prof] paddle.fft.ifftn 	 paddle.fft.ifftn(x=Tensor([4, 529201, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), ) 	 25401648 	 41344 	 10.798893928527832 	 20.383310556411743 	 4.315376281738281e-05 	 0.12566471099853516 	 6.765638113021851 	 30.049854516983032 	 0.020953893661499023 	 0.09296393394470215 	 
2025-08-04 22:24:54.043266 test begin: paddle.fft.ifftn(x=Tensor([529201, 4, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), )
[Prof] paddle.fft.ifftn 	 paddle.fft.ifftn(x=Tensor([529201, 4, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), ) 	 25401648 	 41344 	 9.462494373321533 	 20.376409769058228 	 3.552436828613281e-05 	 0.12567639350891113 	 6.770545482635498 	 11.57834529876709 	 0.0209195613861084 	 0.057340145111083984 	 
2025-08-04 22:25:42.817343 test begin: paddle.fft.ihfft(x=Tensor([2, 1411201, 3, 3],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9d34638f40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:35:48.736588 test begin: paddle.fft.ihfft(x=Tensor([2, 1411201, 3, 3],"float64"), n=2, )
W0804 22:35:52.631372 71868 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f95b4736ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:45:53.756377 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 1058401, 3],"float64"), )
W0804 22:45:54.611733 72162 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0ab536f070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:55:58.787496 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 1058401, 3],"float64"), n=2, )
W0804 22:55:59.655568 72632 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc7c267af50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:06:03.514501 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 1058401, 3],"float64"), n=2, axis=1, )
W0804 23:06:04.264904 72916 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd7ee477010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:16:08.383259 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 3, 1058401],"float64"), )
W0804 23:16:09.135066 73292 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f47cca53010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:26:13.161454 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 3, 1058401],"float64"), n=2, axis=1, )
W0804 23:26:13.968695 73668 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9b7339b010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:36:19.846872 test begin: paddle.fft.ihfft(x=Tensor([201, 14112, 3, 3],"float64"), n=2, axis=1, )
W0804 23:36:20.547292 73977 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([201, 14112, 3, 3],"float64"), n=2, axis=1, ) 	 25528608 	 133166 	 12.092926263809204 	 9.262706518173218 	 6.985664367675781e-05 	 0.0002372264862060547 	 23.36622405052185 	 21.55245351791382 	 0.02240276336669922 	 0.0002110004425048828 	 
2025-08-04 23:37:27.421644 test begin: paddle.fft.ihfft(x=Tensor([201, 4, 3, 10584],"float64"), n=2, )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([201, 4, 3, 10584],"float64"), n=2, ) 	 25528608 	 133166 	 9.241978645324707 	 5.0106353759765625 	 5.841255187988281e-05 	 9.775161743164062e-05 	 22.807698488235474 	 20.28468918800354 	 0.021883010864257812 	 0.03112649917602539 	 
2025-08-04 23:38:27.518912 test begin: paddle.fft.ihfft(x=Tensor([705601, 4, 3, 3],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f16eeab30d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:48:32.117538 test begin: paddle.fft.ihfft2(x=Tensor([1270081, 4, 5],"float64"), )
W0804 23:48:32.775662 74561 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1654c4f070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:58:37.072061 test begin: paddle.fft.ihfft2(x=Tensor([2822401, 3, 3],"float64"), s=tuple(1,2,), )
W0804 23:58:40.352550 75023 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuFFT, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/native/cuda/SpectralOps.cpp:269.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([2822401, 3, 3],"float64"), s=tuple(1,2,), ) 	 25401609 	 99947 	 77.21980118751526 	 63.08947396278381 	 0.1316511631011963 	 0.16123342514038086 	 114.94376063346863 	 116.10754013061523 	 0.14693093299865723 	 0.14839959144592285 	 
2025-08-05 00:04:52.527521 test begin: paddle.fft.ihfft2(x=Tensor([3, 1693441, 5],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f33a739ae60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:14:57.244249 test begin: paddle.fft.ihfft2(x=Tensor([3, 4, 2116801],"float64"), )
W0805 00:14:57.883587 75868 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9b45f6ee30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:25:01.815399 test begin: paddle.fft.ihfft2(x=Tensor([4, 2116801, 3],"float64"), s=tuple(1,2,), )
W0805 00:25:02.482997 76576 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuFFT, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/native/cuda/SpectralOps.cpp:269.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([4, 2116801, 3],"float64"), s=tuple(1,2,), ) 	 25401612 	 99947 	 10.034385442733765 	 6.334702491760254 	 5.507469177246094e-05 	 0.00010561943054199219 	 16.356340646743774 	 20.057335138320923 	 0.020918607711791992 	 0.00021529197692871094 	 
2025-08-05 00:25:56.983810 test begin: paddle.fft.ihfft2(x=Tensor([4, 3, 3, 705601],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f55b6bb6ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:36:01.552890 test begin: paddle.fft.ihfft2(x=Tensor([4, 3, 705601, 3],"float64"), )
W0805 00:36:02.305614 77216 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5eefacf010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:46:06.544504 test begin: paddle.fft.ihfft2(x=Tensor([4, 705601, 3, 3],"float64"), )
W0805 00:46:07.279547 77768 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7efcb7016e30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:56:11.214565 test begin: paddle.fft.ihfft2(x=Tensor([401, 21168, 3],"float64"), s=tuple(1,2,), )
W0805 00:56:15.376286 78089 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuFFT, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/native/cuda/SpectralOps.cpp:269.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([401, 21168, 3],"float64"), s=tuple(1,2,), ) 	 25465104 	 99947 	 11.056688070297241 	 6.48009991645813 	 6.031990051269531e-05 	 0.00027489662170410156 	 17.271135091781616 	 18.415271759033203 	 0.022063732147216797 	 0.0002002716064453125 	 
2025-08-05 00:57:09.384264 test begin: paddle.fft.ihfft2(x=Tensor([940801, 3, 3, 3],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5643eeae60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:07:14.027131 test begin: paddle.fft.ihfftn(Tensor([1270081, 4, 5],"float64"), None, tuple(-2,-1,), "backward", None, )
W0805 01:07:14.795919 78677 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([1270081, 4, 5],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401620 	 4772 	 10.013061761856079 	 10.170576095581055 	 0.35741353034973145 	 0.3629648685455322 	 19.068089485168457 	 16.776482105255127 	 0.5833158493041992 	 0.44925594329833984 	 
2025-08-05 01:08:14.053914 test begin: paddle.fft.ihfftn(Tensor([3, 1693441, 5],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([3, 1693441, 5],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401615 	 4772 	 42.19394898414612 	 35.17112064361572 	 0.6953227519989014 	 0.6849386692047119 	 74.23482966423035 	 41.82850193977356 	 1.2267394065856934 	 0.6880559921264648 	 
2025-08-05 01:11:31.584774 test begin: paddle.fft.ihfftn(Tensor([3, 4, 2116801],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([3, 4, 2116801],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401612 	 4772 	 43.4712929725647 	 34.89503312110901 	 0.6199688911437988 	 0.4978909492492676 	 65.08375787734985 	 62.67874193191528 	 0.9957001209259033 	 0.8949704170227051 	 
2025-08-05 01:14:58.867022 test begin: paddle.fft.ihfftn(Tensor([4, 3, 3, 705601],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([4, 3, 3, 705601],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401636 	 4772 	 34.77045798301697 	 34.83535361289978 	 0.495983362197876 	 0.4967496395111084 	 65.0106520652771 	 60.83680748939514 	 0.9944937229156494 	 0.868772029876709 	 
2025-08-05 01:18:19.976874 test begin: paddle.fft.ihfftn(Tensor([4, 3, 705601, 3],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([4, 3, 705601, 3],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401636 	 4772 	 45.18544006347656 	 42.913455963134766 	 0.7447466850280762 	 0.7077980041503906 	 72.10514640808105 	 50.06436371803284 	 1.1030285358428955 	 0.7146718502044678 	 
2025-08-05 01:21:52.118886 test begin: paddle.fft.ihfftn(Tensor([4, 705601, 3, 3],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([4, 705601, 3, 3],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401636 	 4772 	 10.823794841766357 	 11.357309818267822 	 0.3864314556121826 	 0.40543103218078613 	 19.271730422973633 	 18.398836374282837 	 0.5895695686340332 	 0.49242520332336426 	 
2025-08-05 01:22:57.586253 test begin: paddle.fft.ihfftn(Tensor([940801, 3, 3, 3],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([940801, 3, 3, 3],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401627 	 4772 	 10.822758913040161 	 11.357159852981567 	 0.38634824752807617 	 0.40542101860046387 	 19.27213764190674 	 18.39695954322815 	 0.5895514488220215 	 0.4926629066467285 	 
2025-08-05 01:24:00.558781 test begin: paddle.fft.ihfftn(x=Tensor([4, 3, 1058401, 2],"float64"), )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(x=Tensor([4, 3, 1058401, 2],"float64"), ) 	 25401624 	 4772 	 72.62312364578247 	 67.37937760353088 	 0.9141802787780762 	 1.0307400226593018 	 88.16447710990906 	 68.08144783973694 	 1.3695251941680908 	 1.0413360595703125 	 
2025-08-05 01:28:58.812950 test begin: paddle.fft.ihfftn(x=Tensor([4, 3, 5, 423361],"float64"), )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(x=Tensor([4, 3, 5, 423361],"float64"), ) 	 25401660 	 4772 	 34.889713287353516 	 30.49620747566223 	 0.4393031597137451 	 0.4665210247039795 	 73.3170964717865 	 51.988816261291504 	 0.9820480346679688 	 0.7952442169189453 	 
2025-08-05 01:32:13.245508 test begin: paddle.fft.ihfftn(x=Tensor([4, 635041, 5, 2],"float64"), )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(x=Tensor([4, 635041, 5, 2],"float64"), ) 	 25401640 	 4772 	 80.39708113670349 	 74.67127990722656 	 1.019507646560669 	 1.1422886848449707 	 95.42456269264221 	 75.34324860572815 	 1.1351070404052734 	 1.152409553527832 	 
2025-08-05 01:37:41.901607 test begin: paddle.fft.ihfftn(x=Tensor([846721, 3, 5, 2],"float64"), )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(x=Tensor([846721, 3, 5, 2],"float64"), ) 	 25401630 	 4772 	 90.32325148582458 	 74.28080821037292 	 1.1368227005004883 	 1.3239474296569824 	 81.79489016532898 	 74.96848893165588 	 0.9745049476623535 	 1.336357593536377 	 
2025-08-05 01:43:07.545350 test begin: paddle.fft.rfft(Tensor([20, 1210, 2101],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([20, 1210, 2101],"float32"), ) 	 50844200 	 7826 	 20.375055074691772 	 15.728151321411133 	 0.5317544937133789 	 0.6849110126495361 	 38.33929491043091 	 26.39881443977356 	 1.0021016597747803 	 1.1500327587127686 	 
2025-08-05 01:44:49.829293 test begin: paddle.fft.rfft(Tensor([20, 1270, 2001],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([20, 1270, 2001],"float32"), ) 	 50825400 	 7826 	 15.47730803489685 	 10.390334129333496 	 0.40407586097717285 	 0.45235705375671387 	 28.78340196609497 	 15.79986023902893 	 0.7514240741729736 	 0.6881427764892578 	 
2025-08-05 01:46:04.693004 test begin: paddle.fft.rfft(Tensor([20, 64, 39691],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([20, 64, 39691],"float32"), ) 	 50804480 	 7826 	 37.01947474479675 	 32.20442056655884 	 0.4831228256225586 	 0.525782585144043 	 71.13872003555298 	 58.89857339859009 	 0.9282536506652832 	 0.9629311561584473 	 
2025-08-05 01:49:26.282398 test begin: paddle.fft.rfft(Tensor([378, 64, 2101],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([378, 64, 2101],"float32"), ) 	 50827392 	 7826 	 20.36000156402588 	 15.691807508468628 	 0.5313353538513184 	 0.6834220886230469 	 38.147228717803955 	 26.360759496688843 	 0.9954464435577393 	 1.1483800411224365 	 
2025-08-05 01:51:08.297925 test begin: paddle.fft.rfft(Tensor([397, 64, 2001],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([397, 64, 2001],"float32"), ) 	 50841408 	 7826 	 15.512825012207031 	 10.393280982971191 	 0.40467286109924316 	 0.4524507522583008 	 28.570967197418213 	 15.805598020553589 	 0.7458662986755371 	 0.6883566379547119 	 
2025-08-05 01:52:21.964247 test begin: paddle.fft.rfft(Tensor([4, 32, 32, 12404],"float32"), axis=-1, norm="forward", )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([4, 32, 32, 12404],"float32"), axis=-1, norm="forward", ) 	 50806784 	 7826 	 38.231274127960205 	 34.034544944763184 	 0.49887824058532715 	 0.5552411079406738 	 79.14715385437012 	 68.60831594467163 	 0.9399356842041016 	 0.9967985153198242 	 
2025-08-05 01:56:04.176389 test begin: paddle.fft.rfft(Tensor([4, 32, 6202, 64],"float32"), axis=-1, norm="forward", )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([4, 32, 6202, 64],"float32"), axis=-1, norm="forward", ) 	 50806784 	 7826 	 9.999535083770752 	 4.905763149261475 	 0.3263998031616211 	 0.3198394775390625 	 27.47735619544983 	 14.3696928024292 	 0.5979032516479492 	 0.4693121910095215 	 
2025-08-05 01:57:02.358859 test begin: paddle.fft.rfft(Tensor([4, 6202, 32, 64],"float32"), axis=-1, norm="forward", )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([4, 6202, 32, 64],"float32"), axis=-1, norm="forward", ) 	 50806784 	 7826 	 9.999549865722656 	 4.903520345687866 	 0.326430082321167 	 0.31981635093688965 	 27.47774076461792 	 14.370077133178711 	 0.597956657409668 	 0.4693284034729004 	 
2025-08-05 01:58:00.938596 test begin: paddle.fft.rfft(Tensor([776, 32, 32, 64],"float32"), axis=-1, norm="forward", )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([776, 32, 32, 64],"float32"), axis=-1, norm="forward", ) 	 50855936 	 7826 	 10.010777473449707 	 4.918102502822876 	 0.3268873691558838 	 0.3200540542602539 	 27.504525661468506 	 14.380917310714722 	 0.5985124111175537 	 0.4697837829589844 	 
2025-08-05 01:59:00.191663 test begin: paddle.fft.rfft2(Tensor([26, 32, 250, 250],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([26, 32, 250, 250],"float32"), ) 	 52000000 	 6899 	 10.193196773529053 	 5.7327728271484375 	 0.37739992141723633 	 0.423220157623291 	 24.812456369400024 	 13.246418476104736 	 0.6124701499938965 	 0.4907395839691162 	 
2025-08-05 01:59:57.211261 test begin: paddle.fft.rfft2(Tensor([32, 26, 250, 250],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([32, 26, 250, 250],"float32"), ) 	 52000000 	 6899 	 10.193179845809937 	 5.780561208724976 	 0.3774099349975586 	 0.428652286529541 	 24.811081647872925 	 13.24599313735962 	 0.6124076843261719 	 0.49089789390563965 	 
2025-08-05 02:00:54.039528 test begin: paddle.fft.rfft2(Tensor([32, 32, 199, 250],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([32, 32, 199, 250],"float32"), ) 	 50944000 	 6899 	 18.57206106185913 	 10.41825246810913 	 0.6882455348968506 	 0.7730753421783447 	 41.055625438690186 	 22.467761993408203 	 1.0126678943634033 	 0.8330378532409668 	 
2025-08-05 02:02:30.295469 test begin: paddle.fft.rfft2(Tensor([32, 32, 250, 199],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([32, 32, 250, 199],"float32"), ) 	 50944000 	 6899 	 18.875543355941772 	 11.307250499725342 	 0.4657719135284424 	 0.41853785514831543 	 36.07248544692993 	 18.549562215805054 	 0.8898940086364746 	 0.6876471042633057 	 
2025-08-05 02:03:56.603882 test begin: paddle.fft.rfft2(Tensor([8, 102, 250, 250],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([8, 102, 250, 250],"float32"), ) 	 51000000 	 6899 	 10.006181240081787 	 5.620766639709473 	 0.3704874515533447 	 0.4169273376464844 	 24.33767008781433 	 12.994889736175537 	 0.6007440090179443 	 0.48148584365844727 	 
2025-08-05 02:04:51.392280 test begin: paddle.fft.rfft2(Tensor([8, 32, 250, 794],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([8, 32, 250, 794],"float32"), ) 	 50816000 	 6899 	 14.521658182144165 	 11.801080465316772 	 0.4299449920654297 	 0.49831199645996094 	 30.245685577392578 	 18.782150506973267 	 0.7464189529418945 	 0.696319580078125 	 
2025-08-05 02:06:08.654486 test begin: paddle.fft.rfft2(Tensor([8, 32, 794, 250],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([8, 32, 794, 250],"float32"), ) 	 50816000 	 6899 	 16.001059770584106 	 11.370584487915039 	 0.5929105281829834 	 0.8437395095825195 	 36.15299987792969 	 24.198657035827637 	 0.8919570446014404 	 0.8974738121032715 	 
2025-08-05 02:07:40.382859 test begin: paddle.fft.rfft2(x=Tensor([32, 15, 15, 7057],"float32"), axes=tuple(1,2,), norm="ortho", )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(x=Tensor([32, 15, 15, 7057],"float32"), axes=tuple(1,2,), norm="ortho", ) 	 50810400 	 6899 	 11.035632610321045 	 10.938513040542603 	 0.32688236236572266 	 0.40497398376464844 	 27.094768524169922 	 20.912007331848145 	 0.6686644554138184 	 0.6200642585754395 	 
2025-08-05 02:08:53.956847 test begin: paddle.fft.rfft2(x=Tensor([32, 15, 414, 256],"float32"), axes=tuple(1,2,), norm="ortho", )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(x=Tensor([32, 15, 414, 256],"float32"), axes=tuple(1,2,), norm="ortho", ) 	 50872320 	 6899 	 16.01173210144043 	 12.73550009727478 	 0.3388707637786865 	 0.3765993118286133 	 29.43360662460327 	 21.57058811187744 	 0.6228139400482178 	 0.5327210426330566 	 
2025-08-05 02:10:20.223051 test begin: paddle.fft.rfft2(x=Tensor([32, 414, 15, 256],"float32"), axes=tuple(1,2,), norm="ortho", )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(x=Tensor([32, 414, 15, 256],"float32"), axes=tuple(1,2,), norm="ortho", ) 	 50872320 	 6899 	 11.519209146499634 	 11.513458251953125 	 0.3411839008331299 	 0.4261653423309326 	 29.654828786849976 	 23.66464591026306 	 0.6274070739746094 	 0.584240198135376 	 
2025-08-05 02:11:38.438566 test begin: paddle.fft.rfft2(x=Tensor([883, 15, 15, 256],"float32"), axes=tuple(1,2,), norm="ortho", )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(x=Tensor([883, 15, 15, 256],"float32"), axes=tuple(1,2,), norm="ortho", ) 	 50860800 	 6899 	 10.822091579437256 	 10.791967630386353 	 0.32007575035095215 	 0.39957714080810547 	 26.84748888015747 	 20.836944818496704 	 0.6625399589538574 	 0.6177535057067871 	 
2025-08-05 02:12:51.966841 test begin: paddle.fft.rfftn(Tensor([26, 32, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([26, 32, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 52000000 	 6899 	 10.198516368865967 	 5.885292291641235 	 0.37733960151672363 	 0.43573522567749023 	 24.81186532974243 	 13.24697732925415 	 0.6124939918518066 	 0.4908933639526367 	 
2025-08-05 02:13:47.625092 test begin: paddle.fft.rfftn(Tensor([32, 15, 15, 7057],"float32"), None, tuple(1,2,), "ortho", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 15, 15, 7057],"float32"), None, tuple(1,2,), "ortho", None, ) 	 50810400 	 6899 	 11.03183627128601 	 10.950151920318604 	 0.3268008232116699 	 0.40494680404663086 	 27.09432315826416 	 20.910594940185547 	 0.6686906814575195 	 0.6199240684509277 	 
2025-08-05 02:15:00.093020 test begin: paddle.fft.rfftn(Tensor([32, 15, 414, 256],"float32"), None, tuple(1,2,), "ortho", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 15, 414, 256],"float32"), None, tuple(1,2,), "ortho", None, ) 	 50872320 	 6899 	 16.011990070343018 	 12.716908931732178 	 0.33887195587158203 	 0.3765747547149658 	 29.433558225631714 	 21.56902527809143 	 0.6228506565093994 	 0.5326733589172363 	 
2025-08-05 02:16:21.273641 test begin: paddle.fft.rfftn(Tensor([32, 26, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 26, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 52000000 	 6899 	 10.19429087638855 	 5.888317346572876 	 0.377377986907959 	 0.4365663528442383 	 24.81071925163269 	 13.244174003601074 	 0.6124465465545654 	 0.49074387550354004 	 
2025-08-05 02:17:16.893422 test begin: paddle.fft.rfftn(Tensor([32, 32, 199, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 32, 199, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 50944000 	 6899 	 18.565535306930542 	 10.443300008773804 	 0.6880514621734619 	 0.7761805057525635 	 41.07797288894653 	 22.450135707855225 	 1.0132994651794434 	 0.8323793411254883 	 
2025-08-05 02:18:52.436467 test begin: paddle.fft.rfftn(Tensor([32, 32, 250, 199],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 32, 250, 199],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 50944000 	 6899 	 18.868227005004883 	 11.328381776809692 	 0.46555638313293457 	 0.4193911552429199 	 36.06470990180969 	 18.55696940422058 	 0.8896820545196533 	 0.6878745555877686 	 
2025-08-05 02:20:18.730699 test begin: paddle.fft.rfftn(Tensor([32, 414, 15, 256],"float32"), None, tuple(1,2,), "ortho", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 414, 15, 256],"float32"), None, tuple(1,2,), "ortho", None, ) 	 50872320 	 6899 	 11.518071413040161 	 11.51051115989685 	 0.3413095474243164 	 0.426253080368042 	 29.65233087539673 	 23.6628897190094 	 0.6275169849395752 	 0.5842547416687012 	 
2025-08-05 02:21:38.266022 test begin: paddle.fft.rfftn(Tensor([8, 102, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([8, 102, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 51000000 	 6899 	 10.017741680145264 	 5.760794639587402 	 0.3705770969390869 	 0.42642760276794434 	 24.33604884147644 	 12.995038986206055 	 0.6008279323577881 	 0.48148465156555176 	 
2025-08-05 02:22:35.130611 test begin: paddle.fft.rfftn(Tensor([8, 32, 250, 794],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([8, 32, 250, 794],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 50816000 	 6899 	 14.523378849029541 	 10.104696989059448 	 0.4299297332763672 	 0.49863100051879883 	 30.247356414794922 	 18.793144941329956 	 0.7463560104370117 	 0.696774959564209 	 
2025-08-05 02:23:54.398962 test begin: paddle.fft.rfftn(Tensor([8, 32, 794, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([8, 32, 794, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 50816000 	 6899 	 16.009315490722656 	 11.307427406311035 	 0.5932421684265137 	 0.8388636112213135 	 36.16757535934448 	 24.1982638835907 	 0.892277717590332 	 0.8972926139831543 	 
2025-08-05 02:25:25.284443 test begin: paddle.fft.rfftn(Tensor([883, 15, 15, 256],"float32"), None, tuple(1,2,), "ortho", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([883, 15, 15, 256],"float32"), None, tuple(1,2,), "ortho", None, ) 	 50860800 	 6899 	 10.804996252059937 	 10.79222583770752 	 0.3200969696044922 	 0.3995640277862549 	 26.855654001235962 	 20.835590362548828 	 0.662794828414917 	 0.6177818775177002 	 
2025-08-05 02:26:38.788716 test begin: paddle.flatten(Tensor([40510, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([40510, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, ) 	 508157440 	 1742182 	 18.45339846611023 	 7.481979846954346 	 0.00010442733764648438 	 0.00015783309936523438 	 71.93506956100464 	 90.41449904441833 	 9.34600830078125e-05 	 0.00021839141845703125 	 
2025-08-05 02:30:04.202804 test begin: paddle.flatten(Tensor([40960, 254, 7, 7],"float32"), start_axis=1, stop_axis=-1, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([40960, 254, 7, 7],"float32"), start_axis=1, stop_axis=-1, ) 	 509788160 	 1742182 	 18.689204931259155 	 7.55975079536438 	 0.00010085105895996094 	 0.00012755393981933594 	 77.32989931106567 	 91.55130171775818 	 9.894371032714844e-05 	 0.0002200603485107422 	 
2025-08-05 02:33:37.577565 test begin: paddle.flatten(Tensor([40960, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([40960, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, ) 	 513802240 	 1742182 	 10.075309991836548 	 7.482511758804321 	 9.1552734375e-05 	 0.0002524852752685547 	 72.06220006942749 	 91.1070909500122 	 9.322166442871094e-05 	 0.00022673606872558594 	 
2025-08-05 02:36:55.928867 test begin: paddle.flatten(Tensor([4160, 50, 10, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([4160, 50, 10, 256],"float32"), start_axis=2, ) 	 532480000 	 1742182 	 9.843095064163208 	 7.357275009155273 	 6.4849853515625e-05 	 8.606910705566406e-05 	 72.19998097419739 	 90.69647789001465 	 0.00010752677917480469 	 0.00022149085998535156 	 
2025-08-05 02:40:15.474476 test begin: paddle.flatten(Tensor([4160, 50, 7, 349],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([4160, 50, 7, 349],"float32"), start_axis=2, ) 	 508144000 	 1742182 	 9.831253290176392 	 7.366028785705566 	 9.369850158691406e-05 	 9.226799011230469e-05 	 72.69321608543396 	 91.57163119316101 	 9.822845458984375e-05 	 0.00022673606872558594 	 
2025-08-05 02:43:34.686143 test begin: paddle.flatten(Tensor([4160, 69, 7, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([4160, 69, 7, 256],"float32"), start_axis=2, ) 	 514375680 	 1742182 	 9.81941819190979 	 7.280822515487671 	 9.870529174804688e-05 	 0.00012612342834472656 	 72.57106804847717 	 91.77999138832092 	 0.00012063980102539062 	 0.0002200603485107422 	 
2025-08-05 02:46:53.454347 test begin: paddle.flatten(Tensor([5120, 50, 7, 284],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([5120, 50, 7, 284],"float32"), start_axis=2, ) 	 508928000 	 1742182 	 9.793628454208374 	 7.2189390659332275 	 0.00011157989501953125 	 0.00018072128295898438 	 71.52550292015076 	 92.13247299194336 	 9.632110595703125e-05 	 0.00021767616271972656 	 
2025-08-05 02:50:11.190479 test begin: paddle.flatten(Tensor([5120, 50, 8, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([5120, 50, 8, 256],"float32"), start_axis=2, ) 	 524288000 	 1742182 	 9.898078918457031 	 7.324404716491699 	 8.869171142578125e-05 	 0.00024390220642089844 	 72.05135560035706 	 91.66762781143188 	 0.00011777877807617188 	 0.00021791458129882812 	 
2025-08-05 02:53:32.742259 test begin: paddle.flatten(Tensor([5120, 56, 7, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([5120, 56, 7, 256],"float32"), start_axis=2, ) 	 513802240 	 1742182 	 9.826585531234741 	 7.2986955642700195 	 9.012222290039062e-05 	 7.772445678710938e-05 	 72.33485794067383 	 91.2039544582367 	 9.369850158691406e-05 	 0.00021600723266601562 	 
2025-08-05 02:56:50.756617 test begin: paddle.flatten(Tensor([5680, 50, 7, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([5680, 50, 7, 256],"float32"), start_axis=2, ) 	 508928000 	 1742182 	 9.838157176971436 	 7.248654127120972 	 9.298324584960938e-05 	 8.58306884765625e-05 	 71.93447542190552 	 92.09747219085693 	 9.369850158691406e-05 	 0.00021958351135253906 	 
2025-08-05 03:00:09.536179 test begin: paddle.flip(Tensor([127, 8, 224, 224],"float32"), axis=list[3,], )
[Prof] paddle.flip 	 paddle.flip(Tensor([127, 8, 224, 224],"float32"), axis=list[3,], ) 	 50978816 	 10401 	 10.091910362243652 	 3.2541284561157227 	 0.9914777278900146 	 0.3197305202484131 	 10.089382410049438 	 3.25525164604187 	 0.9912712574005127 	 0.3198235034942627 	 
2025-08-05 03:00:38.648467 test begin: paddle.flip(Tensor([1351, 3, 112, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([1351, 3, 112, 112],"float32"), axis=-1, ) 	 50840832 	 10401 	 10.092844009399414 	 3.2507259845733643 	 0.9917879104614258 	 0.31938982009887695 	 10.092245817184448 	 3.2499067783355713 	 0.9916274547576904 	 0.31929731369018555 	 
2025-08-05 03:01:08.992188 test begin: paddle.flip(Tensor([3, 338, 224, 224],"float32"), axis=list[3,], )
[Prof] paddle.flip 	 paddle.flip(Tensor([3, 338, 224, 224],"float32"), axis=list[3,], ) 	 50878464 	 10401 	 10.066498756408691 	 3.2475759983062744 	 0.9891579151153564 	 0.3190882205963135 	 10.066543579101562 	 3.24894642829895 	 0.9891374111175537 	 0.3192138671875 	 
2025-08-05 03:01:37.973689 test begin: paddle.flip(Tensor([3, 8, 224, 9451],"float32"), axis=list[3,], )
[Prof] paddle.flip 	 paddle.flip(Tensor([3, 8, 224, 9451],"float32"), axis=list[3,], ) 	 50808576 	 10401 	 10.068796634674072 	 3.2487728595733643 	 0.989457368850708 	 0.3192007541656494 	 10.07754921913147 	 3.2478134632110596 	 0.9902033805847168 	 0.31911325454711914 	 
2025-08-05 03:02:06.328909 test begin: paddle.flip(Tensor([3, 8, 9451, 224],"float32"), axis=list[3,], )
[Prof] paddle.flip 	 paddle.flip(Tensor([3, 8, 9451, 224],"float32"), axis=list[3,], ) 	 50808576 	 10401 	 10.056491136550903 	 3.249847173690796 	 0.9878542423248291 	 0.3188595771789551 	 10.051982402801514 	 3.2446744441986084 	 0.9877188205718994 	 0.31882739067077637 	 
2025-08-05 03:02:34.656362 test begin: paddle.flip(Tensor([52, 3, 112, 2908],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([52, 3, 112, 2908],"float32"), axis=-1, ) 	 50808576 	 10401 	 10.069597005844116 	 3.2487175464630127 	 0.989469051361084 	 0.31919336318969727 	 10.067410230636597 	 3.2477309703826904 	 0.9891617298126221 	 0.3191049098968506 	 
2025-08-05 03:03:02.997850 test begin: paddle.flip(Tensor([52, 3, 2908, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([52, 3, 2908, 112],"float32"), axis=-1, ) 	 50808576 	 10401 	 10.094731569290161 	 3.2490394115448 	 0.9920110702514648 	 0.3192470073699951 	 10.093163967132568 	 3.2479472160339355 	 0.9918060302734375 	 0.31913304328918457 	 
2025-08-05 03:03:31.395899 test begin: paddle.flip(Tensor([52, 78, 112, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([52, 78, 112, 112],"float32"), axis=-1, ) 	 50878464 	 10401 	 10.103675365447998 	 3.246272325515747 	 0.9929215908050537 	 0.3189561367034912 	 10.102905750274658 	 3.2507972717285156 	 0.9927654266357422 	 0.3194453716278076 	 
2025-08-05 03:03:59.825264 test begin: paddle.flip(Tensor([64, 3, 112, 2363],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([64, 3, 112, 2363],"float32"), axis=-1, ) 	 50813952 	 10401 	 10.078409910202026 	 3.2494263648986816 	 0.9904577732086182 	 0.3192789554595947 	 10.078519582748413 	 3.248481273651123 	 0.9903445243835449 	 0.31917357444763184 	 
2025-08-05 03:04:28.200132 test begin: paddle.flip(Tensor([64, 3, 2363, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([64, 3, 2363, 112],"float32"), axis=-1, ) 	 50813952 	 10401 	 10.082446098327637 	 3.250915765762329 	 0.9907407760620117 	 0.3192307949066162 	 10.081550598144531 	 3.2482683658599854 	 0.990595817565918 	 0.3191566467285156 	 
2025-08-05 03:04:56.668135 test begin: paddle.flip(Tensor([64, 64, 112, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([64, 64, 112, 112],"float32"), axis=-1, ) 	 51380224 	 10401 	 10.19494342803955 	 3.277895927429199 	 1.0022449493408203 	 0.3220682144165039 	 10.193835973739624 	 3.2822823524475098 	 1.0022556781768799 	 0.32250285148620605 	 
2025-08-05 03:05:25.336963 test begin: paddle.floor(Tensor([100000, 170, 3],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([100000, 170, 3],"float32"), ) 	 51000000 	 33815 	 10.03696870803833 	 10.115542888641357 	 0.30340003967285156 	 0.3054506778717041 	 4.554519176483154 	 4.556794166564941 	 0.1377577781677246 	 0.13757920265197754 	 
2025-08-05 03:05:56.372501 test begin: paddle.floor(Tensor([100000, 2, 255],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([100000, 2, 255],"float32"), ) 	 51000000 	 33815 	 10.036867618560791 	 10.107574701309204 	 0.3033590316772461 	 0.3054957389831543 	 4.5500648021698 	 4.555221080780029 	 0.1374351978302002 	 0.1376187801361084 	 
2025-08-05 03:06:27.338013 test begin: paddle.floor(Tensor([322, 157920],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([322, 157920],"float32"), ) 	 50850240 	 33815 	 10.011092901229858 	 10.076313018798828 	 0.30255937576293945 	 0.30456066131591797 	 4.537890911102295 	 4.548301935195923 	 0.13701772689819336 	 0.13813304901123047 	 
2025-08-05 03:06:59.882004 test begin: paddle.floor(Tensor([4, 12700801],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([4, 12700801],"float32"), ) 	 50803204 	 33815 	 9.997920751571655 	 10.06756329536438 	 0.30211949348449707 	 0.30425286293029785 	 4.531578540802002 	 4.535907745361328 	 0.13688182830810547 	 0.13700127601623535 	 
2025-08-05 03:07:30.723131 test begin: paddle.floor(Tensor([8467201, 2, 3],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([8467201, 2, 3],"float32"), ) 	 50803206 	 33815 	 9.997990608215332 	 10.067482709884644 	 0.30220770835876465 	 0.30428528785705566 	 4.531525373458862 	 4.539565324783325 	 0.1368727684020996 	 0.13817691802978516 	 
2025-08-05 03:08:03.338142 test begin: paddle.floor(x=Tensor([100, 352, 38, 38],"float32"), )
[Prof] paddle.floor 	 paddle.floor(x=Tensor([100, 352, 38, 38],"float32"), ) 	 50828800 	 33815 	 10.008018970489502 	 10.072335481643677 	 0.30245447158813477 	 0.30446767807006836 	 4.532417058944702 	 4.539259433746338 	 0.13692426681518555 	 0.13706159591674805 	 
2025-08-05 03:08:34.405190 test begin: paddle.floor(x=Tensor([100, 4, 3343, 38],"float32"), )
[Prof] paddle.floor 	 paddle.floor(x=Tensor([100, 4, 3343, 38],"float32"), ) 	 50813600 	 33815 	 10.000764608383179 	 10.069619417190552 	 0.30226969718933105 	 0.30439233779907227 	 4.531373739242554 	 4.535928726196289 	 0.13690686225891113 	 0.13703083992004395 	 
2025-08-05 03:09:05.240259 test begin: paddle.floor(x=Tensor([100, 4, 38, 3343],"float32"), )
[Prof] paddle.floor 	 paddle.floor(x=Tensor([100, 4, 38, 3343],"float32"), ) 	 50813600 	 33815 	 10.000753402709961 	 10.06973910331726 	 0.3022944927215576 	 0.30433106422424316 	 4.530682563781738 	 4.540998458862305 	 0.13688039779663086 	 0.13821101188659668 	 
2025-08-05 03:09:37.368093 test begin: paddle.floor(x=Tensor([8796, 4, 38, 38],"float32"), )
[Prof] paddle.floor 	 paddle.floor(x=Tensor([8796, 4, 38, 38],"float32"), ) 	 50805696 	 33815 	 9.995398044586182 	 10.068093538284302 	 0.302093505859375 	 0.30428194999694824 	 4.531422138214111 	 4.535379648208618 	 0.13689804077148438 	 0.13700103759765625 	 
2025-08-05 03:10:08.192280 test begin: paddle.floor_divide(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 50803600 	 23479 	 10.040440559387207 	 12.414358854293823 	 0.43723535537719727 	 0.5038418769836426 	 None 	 None 	 None 	 None 	 
2025-08-05 03:10:33.045980 test begin: paddle.floor_divide(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), ) 	 50803600 	 23479 	 9.959617376327515 	 11.523922204971313 	 0.4335188865661621 	 0.5014863014221191 	 None 	 None 	 None 	 None 	 
2025-08-05 03:10:55.388124 test begin: paddle.floor_divide(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 101606800 	 23479 	 10.623872518539429 	 10.68532943725586 	 0.4623124599456787 	 0.46512627601623535 	 None 	 None 	 None 	 None 	 
2025-08-05 03:11:19.634362 test begin: paddle.floor_divide(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), ) 	 50803220 	 23479 	 10.50502872467041 	 10.509388208389282 	 0.4572579860687256 	 0.457470178604126 	 None 	 None 	 None 	 None 	 
2025-08-05 03:11:43.008630 test begin: paddle.floor_divide(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), ) 	 101606420 	 23479 	 10.615206956863403 	 10.684620380401611 	 0.4620177745819092 	 0.4650263786315918 	 None 	 None 	 None 	 None 	 
2025-08-05 03:12:06.077132 test begin: paddle.floor_divide(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), ) 	 50804736 	 23479 	 10.515277624130249 	 10.509500980377197 	 0.45757484436035156 	 0.45745372772216797 	 None 	 None 	 None 	 None 	 
2025-08-05 03:12:27.993075 test begin: paddle.floor_divide(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), ) 	 101606440 	 23479 	 10.616563320159912 	 10.685355424880981 	 0.46219968795776367 	 0.4650130271911621 	 None 	 None 	 None 	 None 	 
2025-08-05 03:12:52.198494 test begin: paddle.fmax(Tensor([10, 50803201],"float32"), Tensor([50803201],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9066a1ebc0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 03:23:45.851103 test begin: paddle.fmax(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), )
W0805 03:23:47.591965 82634 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.fmax 	 paddle.fmax(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), ) 	 101606420 	 33694 	 15.172594785690308 	 15.043213367462158 	 0.4602358341217041 	 0.4562337398529053 	 24.69482135772705 	 89.04756140708923 	 0.749053955078125 	 0.20778775215148926 	 
2025-08-05 03:26:16.721419 test begin: paddle.fmax(Tensor([10, 5080321],"float32"), Tensor([5080321],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([10, 5080321],"float32"), Tensor([5080321],"float32"), ) 	 55883531 	 33694 	 15.182111740112305 	 15.017414093017578 	 0.4603562355041504 	 0.4554166793823242 	 152.5225212574005 	 92.52942848205566 	 4.626205682754517 	 0.20040416717529297 	 
2025-08-05 03:30:53.819420 test begin: paddle.fmax(Tensor([30, 200, 8468],"float32"), Tensor([30, 200, 8468],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([30, 200, 8468],"float32"), Tensor([30, 200, 8468],"float32"), ) 	 101616000 	 33694 	 15.177170753479004 	 16.30367612838745 	 0.46038150787353516 	 0.45627570152282715 	 24.677561044692993 	 88.95985889434814 	 0.748560905456543 	 0.2075960636138916 	 
2025-08-05 03:33:23.955317 test begin: paddle.fmax(Tensor([30, 42337, 40],"float32"), Tensor([30, 42337, 40],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([30, 42337, 40],"float32"), Tensor([30, 42337, 40],"float32"), ) 	 101608800 	 33694 	 15.178136825561523 	 15.054611444473267 	 0.46042633056640625 	 0.45635557174682617 	 24.687101125717163 	 89.12710309028625 	 0.7488398551940918 	 0.20798993110656738 	 
2025-08-05 03:35:52.771329 test begin: paddle.fmax(Tensor([3386881, 15],"float32"), Tensor([15],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([3386881, 15],"float32"), Tensor([15],"float32"), ) 	 50803230 	 33694 	 9.984931230545044 	 10.221024990081787 	 0.3029203414916992 	 0.30983972549438477 	 281.07705903053284 	 83.30240058898926 	 8.669757843017578 	 0.1683211326599121 	 
2025-08-05 03:42:20.309984 test begin: paddle.fmax(Tensor([3386881, 15],"float32"), Tensor([3386881, 15],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([3386881, 15],"float32"), Tensor([3386881, 15],"float32"), ) 	 101606430 	 33694 	 15.172282695770264 	 15.043314695358276 	 0.46017956733703613 	 0.45626354217529297 	 24.748483180999756 	 89.06101131439209 	 0.7507236003875732 	 0.20783758163452148 	 
2025-08-05 03:44:48.060168 test begin: paddle.fmax(Tensor([6351, 200, 40],"float32"), Tensor([6351, 200, 40],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([6351, 200, 40],"float32"), Tensor([6351, 200, 40],"float32"), ) 	 101616000 	 33694 	 15.175886392593384 	 15.04376745223999 	 0.46033215522766113 	 0.45629382133483887 	 24.69764518737793 	 88.95955181121826 	 0.7490363121032715 	 0.20756316184997559 	 
2025-08-05 03:47:14.513284 test begin: paddle.fmin(Tensor([10, 50803201],"float32"), Tensor([50803201],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa4623979a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 03:58:04.525133 test begin: paddle.fmin(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), )
W0805 03:58:06.253387 83708 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.fmin 	 paddle.fmin(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), ) 	 101606420 	 33691 	 15.16831636428833 	 15.04054069519043 	 0.4601292610168457 	 0.4562098979949951 	 24.69501495361328 	 89.02478504180908 	 0.749100923538208 	 0.2077498435974121 	 
2025-08-05 04:00:32.303269 test begin: paddle.fmin(Tensor([10, 5080321],"float32"), Tensor([5080321],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([10, 5080321],"float32"), Tensor([5080321],"float32"), ) 	 55883531 	 33691 	 15.178581237792969 	 15.0137939453125 	 0.460451602935791 	 0.45533156394958496 	 152.3532681465149 	 92.55404806137085 	 4.621871709823608 	 0.20044541358947754 	 
2025-08-05 04:05:11.343734 test begin: paddle.fmin(Tensor([30, 200, 8468],"float32"), Tensor([30, 200, 8468],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([30, 200, 8468],"float32"), Tensor([30, 200, 8468],"float32"), ) 	 101616000 	 33691 	 15.178751468658447 	 15.047891616821289 	 0.46036267280578613 	 0.4563150405883789 	 24.698370218276978 	 88.95406794548035 	 0.7490427494049072 	 0.2075200080871582 	 
2025-08-05 04:07:39.453727 test begin: paddle.fmin(Tensor([30, 42337, 40],"float32"), Tensor([30, 42337, 40],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([30, 42337, 40],"float32"), Tensor([30, 42337, 40],"float32"), ) 	 101608800 	 33691 	 15.188443899154663 	 15.0451500415802 	 0.4616889953613281 	 0.45623278617858887 	 24.7040114402771 	 89.12462306022644 	 0.7505311965942383 	 0.20795702934265137 	 
2025-08-05 04:10:08.510986 test begin: paddle.fmin(Tensor([3386881, 15],"float32"), Tensor([15],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([3386881, 15],"float32"), Tensor([15],"float32"), ) 	 50803230 	 33691 	 9.98714542388916 	 10.216400623321533 	 0.3028404712677002 	 0.3113234043121338 	 281.04486751556396 	 83.4508728981018 	 8.526165962219238 	 0.1685161590576172 	 
2025-08-05 04:16:37.382076 test begin: paddle.fmin(Tensor([3386881, 15],"float32"), Tensor([3386881, 15],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([3386881, 15],"float32"), Tensor([3386881, 15],"float32"), ) 	 101606430 	 33691 	 15.183676719665527 	 15.050298690795898 	 0.46143674850463867 	 0.4562265872955322 	 24.712117195129395 	 89.05813646316528 	 0.7494525909423828 	 0.20779800415039062 	 
2025-08-05 04:19:06.384733 test begin: paddle.fmin(Tensor([6351, 200, 40],"float32"), Tensor([6351, 200, 40],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([6351, 200, 40],"float32"), Tensor([6351, 200, 40],"float32"), ) 	 101616000 	 33691 	 15.178998231887817 	 15.046162843704224 	 0.4616098403930664 	 0.4562966823577881 	 24.654604196548462 	 88.95395541191101 	 0.7477502822875977 	 0.2075483798980713 	 
2025-08-05 04:21:34.546066 test begin: paddle.frac(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 13371 	 11.01406216621399 	 3.981240749359131 	 0.8236355781555176 	 0.30420398712158203 	 15.821062326431274 	 0.638735294342041 	 0.6044323444366455 	 4.9591064453125e-05 	 
2025-08-05 04:22:09.606054 test begin: paddle.frac(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 13371 	 10.775472402572632 	 3.9868834018707275 	 0.822906494140625 	 0.3041496276855469 	 15.819581508636475 	 0.6804192066192627 	 0.6043281555175781 	 7.796287536621094e-05 	 
2025-08-05 04:22:45.833306 test begin: paddle.frac(Tensor([16934401, 3],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([16934401, 3],"float32"), ) 	 50803203 	 13371 	 10.774368286132812 	 3.989405632019043 	 0.8228874206542969 	 0.3041548728942871 	 15.820521354675293 	 0.6337614059448242 	 0.6044182777404785 	 7.915496826171875e-05 	 
2025-08-05 04:23:18.780910 test begin: paddle.frac(Tensor([2, 12700801],"float64"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([2, 12700801],"float64"), ) 	 25401602 	 13371 	 10.003916263580322 	 3.9827075004577637 	 0.7641012668609619 	 0.30434131622314453 	 14.246864557266235 	 0.6325695514678955 	 0.5442502498626709 	 7.700920104980469e-05 	 
2025-08-05 04:23:49.596931 test begin: paddle.frac(Tensor([2, 25401601],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([2, 25401601],"float32"), ) 	 50803202 	 13371 	 10.77450966835022 	 3.993867874145508 	 0.82285475730896 	 0.3041861057281494 	 15.819044351577759 	 0.639167308807373 	 0.6044807434082031 	 6.818771362304688e-05 	 
2025-08-05 04:24:22.590905 test begin: paddle.frac(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 13371 	 10.773681163787842 	 3.9838578701019287 	 0.8228490352630615 	 0.30420351028442383 	 15.818289041519165 	 0.7114591598510742 	 0.6043362617492676 	 7.867813110351562e-05 	 
2025-08-05 04:24:57.345412 test begin: paddle.frac(Tensor([8467201, 3],"float64"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([8467201, 3],"float64"), ) 	 25401603 	 13371 	 10.008556365966797 	 3.984041690826416 	 0.7655115127563477 	 0.30439233779907227 	 14.245670557022095 	 0.6858141422271729 	 0.5443081855773926 	 7.319450378417969e-05 	 
2025-08-05 04:25:27.429450 test begin: paddle.full_like(Tensor([1, 300, 169345],"float32"), 1, )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([1, 300, 169345],"float32"), 1, ) 	 50803500 	 74569 	 9.992018938064575 	 12.073853254318237 	 0.13681936264038086 	 0.136885404586792 	 None 	 None 	 None 	 None 	 
2025-08-05 04:25:51.267556 test begin: paddle.full_like(Tensor([10, 1, 2048, 24807],"bool"), -65504.0, dtype=Dtype(float16), )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([10, 1, 2048, 24807],"bool"), -65504.0, dtype=Dtype(float16), ) 	 508047360 	 74569 	 49.04892587661743 	 49.05601358413696 	 0.673079252243042 	 0.6720707416534424 	 None 	 None 	 None 	 None 	 
2025-08-05 04:27:40.098778 test begin: paddle.full_like(Tensor([10, 1, 24807, 2048],"bool"), -65504.0, dtype=Dtype(float16), )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([10, 1, 24807, 2048],"bool"), -65504.0, dtype=Dtype(float16), ) 	 508047360 	 74569 	 49.04500198364258 	 49.07951784133911 	 0.6741197109222412 	 0.6742732524871826 	 None 	 None 	 None 	 None 	 
2025-08-05 04:29:28.666526 test begin: paddle.full_like(Tensor([10, 13, 2048, 2048],"bool"), -65504.0, dtype=Dtype(float16), )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([10, 13, 2048, 2048],"bool"), -65504.0, dtype=Dtype(float16), ) 	 545259520 	 74569 	 52.629477977752686 	 52.66026210784912 	 0.7226278781890869 	 0.7218852043151855 	 None 	 None 	 None 	 None 	 
2025-08-05 04:31:23.984295 test begin: paddle.full_like(Tensor([199, 256000],"float32"), 0.0, )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([199, 256000],"float32"), 0.0, ) 	 50944000 	 74569 	 10.018283605575562 	 10.046141386032104 	 0.13718557357788086 	 0.13722705841064453 	 None 	 None 	 None 	 None 	 
2025-08-05 04:31:46.683380 test begin: paddle.full_like(Tensor([42, 300, 4096],"float32"), 1, )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([42, 300, 4096],"float32"), 1, ) 	 51609600 	 74569 	 10.149922609329224 	 10.151724100112915 	 0.1389937400817871 	 0.13898015022277832 	 None 	 None 	 None 	 None 	 
2025-08-05 04:32:07.856913 test begin: paddle.full_like(Tensor([6, 8467201],"float32"), 0.0, )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([6, 8467201],"float32"), 0.0, ) 	 50803206 	 74569 	 9.986549377441406 	 9.996748924255371 	 0.13671493530273438 	 0.1369779109954834 	 None 	 None 	 None 	 None 	 
2025-08-05 04:32:28.740313 test begin: paddle.gammainc(Tensor([1270081, 40],"float32"), y=Tensor([1270081, 40],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([1270081, 40],"float32"), y=Tensor([1270081, 40],"float32"), ) 	 101606480 	 2378 	 9.995196104049683 	 5.382938623428345 	 0.003036975860595703 	 2.3120851516723633 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-08-05 04:32:50.411399 test begin: paddle.gammainc(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), ) 	 101606480 	 2378 	 10.022825956344604 	 5.38049840927124 	 0.003028392791748047 	 2.312898635864258 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-08-05 04:33:11.271758 test begin: paddle.gammainc(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), ) 	 101606460 	 2378 	 9.996585607528687 	 5.3807127475738525 	 0.003039836883544922 	 2.311756134033203 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-08-05 04:33:32.122240 test begin: paddle.gammainc(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), ) 	 101606448 	 2378 	 9.984586000442505 	 5.380452871322632 	 0.0030345916748046875 	 2.3117334842681885 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-08-05 04:33:53.087614 test begin: paddle.gammainc(Tensor([3, 16934401],"float32"), y=Tensor([3, 16934401],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([3, 16934401],"float32"), y=Tensor([3, 16934401],"float32"), ) 	 101606406 	 2378 	 9.996309995651245 	 5.381639003753662 	 0.003033876419067383 	 2.312936544418335 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-08-05 04:34:14.654139 test begin: paddle.gammainc(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), ) 	 101606520 	 2378 	 9.991761445999146 	 5.380486011505127 	 0.0030393600463867188 	 2.3118231296539307 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-08-05 04:34:37.440319 test begin: paddle.gammaincc(Tensor([1270081, 40],"float32"), Tensor([1270081, 40],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([1270081, 40],"float32"), Tensor([1270081, 40],"float32"), ) 	 101606480 	 2555 	 10.02601408958435 	 18.135974884033203 	 0.0027627944946289062 	 7.254769563674927 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-08-05 04:35:10.862402 test begin: paddle.gammaincc(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), ) 	 101606480 	 2555 	 10.024494409561157 	 18.13282537460327 	 0.002755880355834961 	 7.252753019332886 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-08-05 04:35:44.185054 test begin: paddle.gammaincc(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), ) 	 101606460 	 2555 	 10.005161762237549 	 18.13612461090088 	 0.0027611255645751953 	 7.2545552253723145 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-08-05 04:36:17.205457 test begin: paddle.gammaincc(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), ) 	 101606448 	 2555 	 9.991093158721924 	 18.13348150253296 	 0.0027620792388916016 	 7.252992630004883 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-08-05 04:36:50.496194 test begin: paddle.gammaincc(Tensor([3, 16934401],"float32"), Tensor([3, 16934401],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([3, 16934401],"float32"), Tensor([3, 16934401],"float32"), ) 	 101606406 	 2555 	 10.030715942382812 	 18.13520860671997 	 0.0027594566345214844 	 7.254745244979858 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-08-05 04:37:23.581822 test begin: paddle.gammaincc(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), ) 	 101606520 	 2555 	 10.016523599624634 	 18.137941360473633 	 0.0027604103088378906 	 7.252702474594116 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-08-05 04:37:59.146993 test begin: paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([482, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([482, 1],"int64"), ) 	 58720738 	 6316 	 0.5515549182891846 	 97.72095084190369 	 0.08922910690307617 	 8.916854858398438e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:39:41.124173 test begin: paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([496, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([496, 1],"int64"), ) 	 58720752 	 6316 	 0.5666265487670898 	 100.41595673561096 	 0.09168577194213867 	 0.00014519691467285156 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:41:25.767350 test begin: paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([512, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([512, 1],"int64"), ) 	 58720768 	 6316 	 0.586524486541748 	 103.74541234970093 	 0.0949103832244873 	 0.00011301040649414062 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:43:13.819596 test begin: paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([482, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([482, 1],"int64"), ) 	 58720738 	 6316 	 0.5606667995452881 	 97.80349969863892 	 0.09072017669677734 	 8.845329284667969e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:44:55.785011 test begin: paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([496, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([496, 1],"int64"), ) 	 58720752 	 6316 	 0.5611481666564941 	 101.65180373191833 	 0.09079742431640625 	 0.00011348724365234375 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:46:41.684178 test begin: paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([512, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([512, 1],"int64"), ) 	 58720768 	 6316 	 0.5836422443389893 	 104.35773134231567 	 0.09441351890563965 	 9.036064147949219e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:48:30.404273 test begin: paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([482, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([482, 1],"int64"), ) 	 50878946 	 6316 	 0.746619701385498 	 97.7359459400177 	 0.12180352210998535 	 0.0001068115234375 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:50:12.080234 test begin: paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([496, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([496, 1],"int64"), ) 	 50878960 	 6316 	 0.7579648494720459 	 100.52070808410645 	 0.12245011329650879 	 9.441375732421875e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:51:56.557856 test begin: paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([512, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([512, 1],"int64"), ) 	 50878976 	 6316 	 0.7826747894287109 	 124.41558051109314 	 0.12663555145263672 	 0.00011467933654785156 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:54:05.074549 test begin: paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([482, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([482, 1],"int64"), ) 	 50878946 	 6316 	 1.789323329925537 	 108.36245512962341 	 0.28948140144348145 	 0.00021839141845703125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:56:02.183966 test begin: paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([496, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([496, 1],"int64"), ) 	 50878960 	 6316 	 1.8152978420257568 	 104.06907987594604 	 0.29349446296691895 	 0.00023818016052246094 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:57:55.163336 test begin: paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([512, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([512, 1],"int64"), ) 	 50878976 	 6316 	 1.8744356632232666 	 105.2646312713623 	 0.30435848236083984 	 0.000240325927734375 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:59:49.535690 test begin: paddle.gather_nd(Tensor([1001, 413, 128],"float32"), index=Tensor([20, 50, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([1001, 413, 128],"float32"), index=Tensor([20, 50, 2],"int64"), ) 	 52918864 	 4935 	 0.05452084541320801 	 398.34675669670105 	 2.4080276489257812e-05 	 0.00022935867309570312 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:06:29.944088 test begin: paddle.gather_nd(Tensor([1001, 413, 128],"float32"), index=Tensor([5, 50, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([1001, 413, 128],"float32"), index=Tensor([5, 50, 2],"int64"), ) 	 52917364 	 4935 	 0.07600951194763184 	 97.39002418518066 	 3.123283386230469e-05 	 0.0002524852752685547 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:08:09.114800 test begin: paddle.gather_nd(Tensor([101, 1417, 716],"bfloat16"), Tensor([778, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([101, 1417, 716],"bfloat16"), Tensor([778, 2],"int64"), ) 	 102473328 	 4935 	 0.05248618125915527 	 314.96816325187683 	 2.4318695068359375e-05 	 0.00023126602172851562 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:13:27.446515 test begin: paddle.gather_nd(Tensor([101, 1417, 716],"bfloat16"), Tensor([816, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([101, 1417, 716],"bfloat16"), Tensor([816, 2],"int64"), ) 	 102473404 	 4935 	 0.05309176445007324 	 336.30383229255676 	 3.9577484130859375e-05 	 9.703636169433594e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:19:07.089874 test begin: paddle.gather_nd(Tensor([101, 819, 1240],"bfloat16"), Tensor([778, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([101, 819, 1240],"bfloat16"), Tensor([778, 2],"int64"), ) 	 102573116 	 4935 	 0.05773138999938965 	 305.3411910533905 	 0.004945993423461914 	 0.00020432472229003906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:24:15.778096 test begin: paddle.gather_nd(Tensor([101, 8192, 124],"bfloat16"), Tensor([816, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([101, 8192, 124],"bfloat16"), Tensor([816, 2],"int64"), ) 	 102598240 	 4935 	 0.05104684829711914 	 315.1850833892822 	 2.765655517578125e-05 	 0.000164031982421875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:29:34.235580 test begin: paddle.gcd(Tensor([10, 50803],"int32"), Tensor([10, 50803],"int32"), )
W0805 05:29:43.996675 39195 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(Tensor([10, 50803],"int32"), Tensor([10, 50803],"int32"), ) 	 1016060 	 1588 	 9.703902006149292 	 0.04126548767089844 	 4.76837158203125e-05 	 0.025342226028442383 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:29:44.045411 test begin: paddle.gcd(Tensor([25401, 20],"int32"), Tensor([25401, 20],"int32"), )
W0805 05:29:54.293254 39280 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(Tensor([25401, 20],"int32"), Tensor([25401, 20],"int32"), ) 	 1016040 	 1588 	 10.207690238952637 	 0.04148149490356445 	 5.078315734863281e-05 	 0.025476694107055664 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:29:54.352518 test begin: paddle.gcd(x=Tensor([12700, 2, 4, 5],"int32"), y=Tensor([12700, 2, 4, 5],"int32"), )
W0805 05:30:04.595012 39657 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(x=Tensor([12700, 2, 4, 5],"int32"), y=Tensor([12700, 2, 4, 5],"int32"), ) 	 1016000 	 1588 	 10.177819967269897 	 0.04127645492553711 	 4.6253204345703125e-05 	 0.025347232818603516 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:30:04.693693 test begin: paddle.gcd(x=Tensor([25401, 1, 4, 5],"int32"), y=Tensor([2, 1, 5],"int32"), )
W0805 05:30:15.141121 40029 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(x=Tensor([25401, 1, 4, 5],"int32"), y=Tensor([2, 1, 5],"int32"), ) 	 508030 	 1588 	 10.359779834747314 	 0.08875203132629395 	 4.601478576660156e-05 	 0.05709242820739746 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:30:16.826513 test begin: paddle.gcd(x=Tensor([6, 1, 16934, 5],"int32"), y=Tensor([2, 1, 5],"int32"), )
W0805 05:30:26.696229 40257 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(x=Tensor([6, 1, 16934, 5],"int32"), y=Tensor([2, 1, 5],"int32"), ) 	 508030 	 1588 	 9.710259199142456 	 0.08881568908691406 	 5.269050598144531e-05 	 0.05757427215576172 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:30:26.956424 test begin: paddle.gcd(x=Tensor([6, 1, 4, 21168],"int32"), y=Tensor([2, 1, 21168],"int32"), )
W0805 05:30:37.187778 40558 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(x=Tensor([6, 1, 4, 21168],"int32"), y=Tensor([2, 1, 21168],"int32"), ) 	 550368 	 1588 	 10.178848266601562 	 0.08870077133178711 	 4.982948303222656e-05 	 0.05705618858337402 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:30:42.170134 test begin: paddle.gcd(x=Tensor([6, 2, 4, 10584],"int32"), y=Tensor([6, 2, 4, 10584],"int32"), )
W0805 05:30:52.334292 41009 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(x=Tensor([6, 2, 4, 10584],"int32"), y=Tensor([6, 2, 4, 10584],"int32"), ) 	 1016064 	 1588 	 10.088871955871582 	 0.04174327850341797 	 5.078315734863281e-05 	 0.025934696197509766 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:30:52.387450 test begin: paddle.gcd(x=Tensor([6, 2, 8467, 5],"int32"), y=Tensor([6, 2, 8467, 5],"int32"), )
W0805 05:31:03.227397 41309 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(x=Tensor([6, 2, 8467, 5],"int32"), y=Tensor([6, 2, 8467, 5],"int32"), ) 	 1016040 	 1588 	 10.812697410583496 	 0.04136300086975098 	 5.412101745605469e-05 	 0.025632858276367188 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:31:03.276161 test begin: paddle.gcd(x=Tensor([6, 4233, 4, 5],"int32"), y=Tensor([6, 4233, 4, 5],"int32"), )
W0805 05:31:13.079631 41613 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(x=Tensor([6, 4233, 4, 5],"int32"), y=Tensor([6, 4233, 4, 5],"int32"), ) 	 1015920 	 1588 	 9.781206607818604 	 0.040995121002197266 	 4.3392181396484375e-05 	 0.025115489959716797 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:31:14.156579 test begin: paddle.geometric.segment_max(Tensor([40, 1270081],"float32"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_max 	 paddle.geometric.segment_max(Tensor([40, 1270081],"float32"), Tensor([40],"int64"), ) 	 50803280 	 9664 	 6.388723134994507 	 93.31335854530334 	 0.0006186962127685547 	 0.00022125244140625 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:33:02.261434 test begin: paddle.geometric.segment_max(Tensor([40, 2540161],"float16"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_max 	 paddle.geometric.segment_max(Tensor([40, 2540161],"float16"), Tensor([40],"int64"), ) 	 101606480 	 9664 	 15.169067621231079 	 102.22945022583008 	 0.0015380382537841797 	 0.0002105236053466797 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:35:11.133179 test begin: paddle.geometric.segment_max(Tensor([40, 635041],"float64"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_max 	 paddle.geometric.segment_max(Tensor([40, 635041],"float64"), Tensor([40],"int64"), ) 	 25401680 	 9664 	 5.223498344421387 	 95.8256311416626 	 0.0005178451538085938 	 0.00030875205993652344 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:37:00.358096 test begin: paddle.geometric.segment_mean(Tensor([1270081, 20],"float64"), Tensor([1270081],"int64"), )
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([1270081, 20],"float64"), Tensor([1270081],"int64"), ) 	 26671701 	 28038 	 17.06153678894043 	 38.33770990371704 	 0.0005609989166259766 	 0.00025844573974609375 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:39:42.636644 test begin: paddle.geometric.segment_mean(Tensor([25401601, 20],"float16"), Tensor([25401601],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3b80569fc0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 05:49:53.927041 test begin: paddle.geometric.segment_mean(Tensor([25401601, 20],"float32"), Tensor([25401601],"int64"), )
W0805 05:50:04.423291 72734 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3442816f50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 05:59:58.676021 test begin: paddle.geometric.segment_mean(Tensor([25401601, 20],"float64"), Tensor([25401601],"int64"), )
W0805 06:00:09.309684 89969 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdc38d6ef50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 06:10:03.810545 test begin: paddle.geometric.segment_mean(Tensor([2540161, 20],"float32"), Tensor([2540161],"int64"), )
W0805 06:10:04.992919 113357 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([2540161, 20],"float32"), Tensor([2540161],"int64"), ) 	 53343381 	 28038 	 18.584623336791992 	 42.925437211990356 	 0.0006299018859863281 	 0.0002410411834716797 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 06:13:43.411763 test begin: paddle.geometric.segment_mean(Tensor([40, 1270081],"float32"), Tensor([40],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fef29856f20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 06:24:09.632494 test begin: paddle.geometric.segment_mean(Tensor([40, 635041],"float64"), Tensor([40],"int64"), )
W0805 06:24:10.333391 148713 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3d5867ee00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 06:34:14.204791 test begin: paddle.geometric.segment_mean(Tensor([5080321, 20],"float16"), Tensor([5080321],"int64"), )
W0805 06:34:16.509068 10544 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([5080321, 20],"float16"), Tensor([5080321],"int64"), ) 	 106686741 	 28038 	 57.431934118270874 	 104.99543571472168 	 0.002001047134399414 	 0.00067138671875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 06:42:47.584979 test begin: paddle.geometric.segment_min(Tensor([1270081, 20],"float64"), Tensor([1270081],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([1270081, 20],"float64"), Tensor([1270081],"int64"), ) 	 26671701 	 28537 	 19.76473617553711 	 35.90043354034424 	 0.0006651878356933594 	 0.00029468536376953125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 06:44:07.956279 test begin: paddle.geometric.segment_min(Tensor([25401601, 20],"float16"), Tensor([25401601],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5b30ea2b00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 06:54:12.929972 test begin: paddle.geometric.segment_min(Tensor([25401601, 20],"float32"), Tensor([25401601],"int64"), )
W0805 06:54:21.762493 61653 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f02a0d66fb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 07:04:17.815393 test begin: paddle.geometric.segment_min(Tensor([25401601, 20],"float64"), Tensor([25401601],"int64"), )
W0805 07:04:32.897648 86644 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f232cf62f50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 07:14:22.836935 test begin: paddle.geometric.segment_min(Tensor([2540161, 20],"float32"), Tensor([2540161],"int64"), )
W0805 07:14:24.217726 113109 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([2540161, 20],"float32"), Tensor([2540161],"int64"), ) 	 53343381 	 28537 	 30.153916597366333 	 47.97985529899597 	 0.0010197162628173828 	 0.00030303001403808594 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:16:11.593644 test begin: paddle.geometric.segment_min(Tensor([40, 1270081],"float32"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([40, 1270081],"float32"), Tensor([40],"int64"), ) 	 50803280 	 28537 	 16.571202754974365 	 18.303954124450684 	 0.0005419254302978516 	 0.00021576881408691406 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:17:07.756819 test begin: paddle.geometric.segment_min(Tensor([40, 2540161],"float16"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([40, 2540161],"float16"), Tensor([40],"int64"), ) 	 101606480 	 28537 	 51.17633771896362 	 71.18728184700012 	 0.0017619132995605469 	 0.0004754066467285156 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:19:44.533679 test begin: paddle.geometric.segment_min(Tensor([40, 635041],"float64"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([40, 635041],"float64"), Tensor([40],"int64"), ) 	 25401680 	 28537 	 14.859539270401001 	 21.847575664520264 	 0.0004794597625732422 	 0.0002162456512451172 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:20:42.153681 test begin: paddle.geometric.segment_min(Tensor([5080321, 20],"float16"), Tensor([5080321],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([5080321, 20],"float16"), Tensor([5080321],"int64"), ) 	 106686741 	 28537 	 61.4752516746521 	 145.26239728927612 	 0.0021202564239501953 	 0.0003151893615722656 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:24:45.048254 test begin: paddle.geometric.segment_sum(Tensor([25401601, 15],"float16"), Tensor([25401601],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([25401601, 15],"float16"), Tensor([25401601],"int64"), ) 	 406425616 	 20010 	 145.45596981048584 	 113.88488841056824 	 0.007211923599243164 	 2.9073736667633057 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:30:47.154268 test begin: paddle.geometric.segment_sum(Tensor([25401601, 15],"float32"), Tensor([25401601],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([25401601, 15],"float32"), Tensor([25401601],"int64"), ) 	 406425616 	 20010 	 105.55393552780151 	 84.6585738658905 	 0.005243778228759766 	 2.164701461791992 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:35:54.491667 test begin: paddle.geometric.segment_sum(Tensor([2540161, 20],"float32"), Tensor([2540161],"int32"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([2540161, 20],"float32"), Tensor([2540161],"int32"), ) 	 53343381 	 20010 	 11.941110849380493 	 9.274329423904419 	 0.0005588531494140625 	 0.23605632781982422 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:36:25.002166 test begin: paddle.geometric.segment_sum(Tensor([30, 1693441],"float32"), Tensor([30],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([30, 1693441],"float32"), Tensor([30],"int64"), ) 	 50803260 	 20010 	 8.63581895828247 	 5.004782199859619 	 0.00039005279541015625 	 0.12808895111083984 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:36:51.088017 test begin: paddle.geometric.segment_sum(Tensor([30, 3386881],"float16"), Tensor([30],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([30, 3386881],"float16"), Tensor([30],"int64"), ) 	 101606460 	 20010 	 28.548275470733643 	 22.91620707511902 	 0.001379251480102539 	 0.5846340656280518 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:38:06.663094 test begin: paddle.geometric.segment_sum(Tensor([3386881, 15],"float32"), Tensor([3386881],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([3386881, 15],"float32"), Tensor([3386881],"int64"), ) 	 54190096 	 20010 	 9.952296257019043 	 7.976041555404663 	 0.0004558563232421875 	 0.20355510711669922 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:38:41.560742 test begin: paddle.geometric.segment_sum(Tensor([40, 1270081],"float32"), Tensor([40],"int32"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([40, 1270081],"float32"), Tensor([40],"int32"), ) 	 50803280 	 20010 	 13.297117471694946 	 10.181282043457031 	 0.0006210803985595703 	 0.25972795486450195 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:39:19.042011 test begin: paddle.geometric.segment_sum(Tensor([50803201, 20],"float32"), Tensor([50803201],"int32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa7a1fce560>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 07:49:25.817288 test begin: paddle.geometric.segment_sum(Tensor([6773761, 15],"float16"), Tensor([6773761],"int64"), )
W0805 07:49:28.118706 37257 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([6773761, 15],"float16"), Tensor([6773761],"int64"), ) 	 108380176 	 20010 	 34.31796383857727 	 27.133319854736328 	 0.0016775131225585938 	 0.6915416717529297 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:50:54.341201 test begin: paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "max", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "max", None, None, ) 	 25401640 	 27790 	 28.06195068359375 	 103.83780312538147 	 0.34388113021850586 	 0.0004429817199707031 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:53:42.396977 test begin: paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mean", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mean", None, None, ) 	 25401640 	 27790 	 29.388872623443604 	 122.86683821678162 	 0.21568655967712402 	 0.0002605915069580078 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:56:41.331097 test begin: paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "min", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "min", None, None, ) 	 25401640 	 27790 	 28.609749794006348 	 102.99750852584839 	 0.35054826736450195 	 0.00045561790466308594 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:59:28.448068 test begin: paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "max", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "max", None, None, ) 	 25401650 	 27790 	 12.50357699394226 	 61.51603960990906 	 0.15321779251098633 	 0.0004458427429199219 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 08:00:47.546111 test begin: paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mean", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mean", None, None, ) 	 25401650 	 27790 	 10.009535312652588 	 87.9579222202301 	 0.07365083694458008 	 0.0002627372741699219 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 08:02:30.593770 test begin: paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "min", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "min", None, None, ) 	 25401650 	 27790 	 12.509399652481079 	 63.20859932899475 	 0.15327858924865723 	 0.0004515647888183594 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 08:03:53.280853 test begin: paddle.geometric.send_ue_recv(Tensor([10, 1693441],"float64"), Tensor([15, 1693441],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, )
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([10, 1693441],"float64"), Tensor([15, 1693441],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, ) 	 42336055 	 40736 	 35.2759895324707 	 113.45621013641357 	 0.1766350269317627 	 0.00022292137145996094 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 08:07:08.333752 test begin: paddle.geometric.send_ue_recv(Tensor([10, 2540161],"float64"), Tensor([15, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, )
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([10, 2540161],"float64"), Tensor([15, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, ) 	 63504055 	 40736 	 52.801085233688354 	 140.24138140678406 	 0.2656519412994385 	 0.000244140625 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 08:11:33.993657 test begin: paddle.geometric.send_ue_recv(Tensor([10, 508033, 5],"float64"), Tensor([15, 508033, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f704e327550>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 08:21:42.982920 test begin: paddle.geometric.send_ue_recv(Tensor([10, 508033, 5],"float64"), Tensor([15, 508033, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, )
W0805 08:21:43.810689 149533 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb689e5ea70>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754353903 (unix time) try "date -d @1754353903" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x246e2) received by PID 149218 (TID 0x7fb680dfa640) from PID 149218 ***]

2025-08-05 08:31:50.495549 test begin: paddle.geometric.send_ue_recv(Tensor([10, 8, 317521],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, )
W0805 08:31:51.211869 32378 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd032ac2ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 08:41:55.259901 test begin: paddle.geometric.send_ue_recv(Tensor([10, 8, 317521],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, )
W0805 08:41:55.955389 77204 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fca8138af50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 08:52:04.400768 test begin: paddle.geometric.send_ue_recv(Tensor([1270081, 20],"float64"), Tensor([15, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, )
W0805 08:52:05.150892 121628 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([1270081, 20],"float64"), Tensor([15, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, ) 	 25401950 	 40736 	 14.774461030960083 	 68.42955684661865 	 0.07405519485473633 	 0.0002772808074951172 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 08:53:39.933952 test begin: paddle.geometric.send_ue_recv(Tensor([635041, 8, 5],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, )
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([635041, 8, 5],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, ) 	 25401790 	 40736 	 22.490195512771606 	 67.14391613006592 	 9.72747802734375e-05 	 0.00046324729919433594 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 08:55:23.097316 test begin: paddle.geometric.send_ue_recv(Tensor([635041, 8, 5],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, )
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([635041, 8, 5],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, ) 	 25401790 	 40736 	 9.70614767074585 	 56.14178013801575 	 9.202957153320312e-05 	 0.00019288063049316406 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 08:56:44.480703 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 20],"float64"), Tensor([25401601],"int64"), Tensor([25401601],"int64"), "add", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9426a06b60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 09:06:49.171558 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 20],"float64"), Tensor([25401601],"int64"), Tensor([25401601],"int64"), "mul", )
W0805 09:06:50.307158 22865 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1e05e46f20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 09:16:54.039982 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 254017],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
W0805 09:16:55.090013 67322 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8916cbee60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 09:26:58.825209 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 254017],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", )
W0805 09:26:59.546887 111613 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4cdb1230a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 09:37:03.770207 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
W0805 09:37:04.542057 155312 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", ) 	 25401750 	 104455 	 9.063604593276978 	 1.2059807777404785 	 8.821487426757812e-05 	 0.00024580955505371094 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:37:31.383590 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", ) 	 25401750 	 104455 	 8.922806024551392 	 1.21244478225708 	 8.96453857421875e-05 	 8.249282836914062e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:38:15.158873 test begin: paddle.geometric.send_uv(Tensor([100, 20],"float64"), Tensor([100, 1],"float64"), Tensor([25401601],"int64"), Tensor([25401601],"int64"), "add", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3398d4aef0>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown
2025-08-05 09:48:23.189531 test begin: paddle.geometric.send_uv(Tensor([100, 20],"float64"), Tensor([25401601, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
W0805 09:48:23.817716 41077 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7faee1ecb100>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 09:58:28.191005 test begin: paddle.geometric.send_uv(Tensor([100, 254017],"float64"), Tensor([100, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
W0805 09:58:28.905000 92972 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f21367ff0a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 10:08:32.727215 test begin: paddle.geometric.send_uv(Tensor([1270081, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
W0805 10:08:33.441855 145765 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([1270081, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", ) 	 26671731 	 104455 	 9.754030704498291 	 1.9716510772705078 	 8.559226989746094e-05 	 0.00022840499877929688 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 10:09:43.333910 test begin: paddle.geometric.send_uv(Tensor([1270081, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([1270081, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", ) 	 26671731 	 104455 	 9.377870321273804 	 1.2373578548431396 	 9.465217590332031e-05 	 7.724761962890625e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 10:11:07.483798 test begin: paddle.geometric.send_uv(Tensor([1270081, 20],"float64"), Tensor([1270081, 1],"float64"), Tensor([1501],"int64"), Tensor([1501],"int64"), "add", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([1270081, 20],"float64"), Tensor([1270081, 1],"float64"), Tensor([1501],"int64"), Tensor([1501],"int64"), "add", ) 	 26674703 	 104455 	 9.918280124664307 	 1.2833011150360107 	 8.273124694824219e-05 	 6.985664367675781e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 10:12:18.930784 test begin: paddle.geometric.send_uv(Tensor([25401601, 1],"float64"), Tensor([25401601, 20],"float64"), Tensor([1501],"int64"), Tensor([1501],"int64"), "mul", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff3f3042a10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 10:22:26.659477 test begin: paddle.geometric.send_uv(Tensor([25401601, 20],"float64"), Tensor([25401601, 1],"float64"), Tensor([1501],"int64"), Tensor([1501],"int64"), "add", )
W0805 10:22:40.661118 54849 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6b987930a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 10:32:31.591174 test begin: paddle.geometric.send_uv(Tensor([25401601, 20],"float64"), Tensor([25401601, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
W0805 10:32:42.313889 107818 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f54364cf0a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 10:42:37.104410 test begin: paddle.greater_equal(Tensor([13, 15266, 16, 4, 1],"int64"), Tensor([13, 15266, 16, 1, 8],"int64"), )
W0805 10:42:40.644367 159968 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 15266, 16, 4, 1],"int64"), Tensor([13, 15266, 16, 1, 8],"int64"), ) 	 38103936 	 53247 	 29.43305253982544 	 26.68620467185974 	 0.5639283657073975 	 0.5114648342132568 	 None 	 None 	 None 	 None 	 
2025-08-05 10:43:40.839219 test begin: paddle.greater_equal(Tensor([13, 2, 122124, 4, 1],"int64"), Tensor([13, 2, 122124, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 122124, 4, 1],"int64"), Tensor([13, 2, 122124, 1, 8],"int64"), ) 	 38102688 	 53247 	 29.66240358352661 	 26.69266104698181 	 0.5683777332305908 	 0.5115251541137695 	 None 	 None 	 None 	 None 	 
2025-08-05 10:44:39.547452 test begin: paddle.greater_equal(Tensor([13, 2, 16, 4, 15266],"int64"), Tensor([13, 2, 16, 1, 15266],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 16, 4, 15266],"int64"), Tensor([13, 2, 16, 1, 15266],"int64"), ) 	 31753280 	 53247 	 11.367997169494629 	 11.68636178970337 	 0.21779155731201172 	 0.22385764122009277 	 None 	 None 	 None 	 None 	 
2025-08-05 10:45:04.090975 test begin: paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 61062],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 61062],"int64"), ) 	 25403456 	 53247 	 30.06893277168274 	 23.655693531036377 	 0.5762040615081787 	 0.45344996452331543 	 None 	 None 	 None 	 None 	 
2025-08-05 10:45:58.279209 test begin: paddle.greater_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), ) 	 25405120 	 53247 	 55.616318225860596 	 48.411051511764526 	 1.065744161605835 	 0.9303057193756104 	 None 	 None 	 None 	 None 	 
2025-08-05 10:47:45.938182 test begin: paddle.greater_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 61062, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 61062, 8],"int64"), ) 	 228616128 	 53247 	 95.22562384605408 	 83.65137529373169 	 1.8246545791625977 	 1.6030726432800293 	 None 	 None 	 None 	 None 	 
2025-08-05 10:50:49.008640 test begin: paddle.greater_equal(Tensor([13, 2, 244247, 4, 1],"int64"), Tensor([13, 2, 244247, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 244247, 4, 1],"int64"), Tensor([13, 2, 244247, 1, 8],"int64"), ) 	 76205064 	 53247 	 58.922720432281494 	 53.11846399307251 	 1.1309397220611572 	 1.020172357559204 	 None 	 None 	 None 	 None 	 
2025-08-05 10:52:42.478052 test begin: paddle.greater_equal(Tensor([13, 30531, 16, 4, 1],"int64"), Tensor([13, 30531, 16, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 30531, 16, 4, 1],"int64"), Tensor([13, 30531, 16, 1, 8],"int64"), ) 	 76205376 	 53247 	 58.51143193244934 	 53.308282136917114 	 1.12662672996521 	 1.2244813442230225 	 None 	 None 	 None 	 None 	 
2025-08-05 10:54:37.248340 test begin: paddle.greater_equal(Tensor([16935, 10, 15, 20],"float32"), Tensor([16935, 10, 15, 20],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([16935, 10, 15, 20],"float32"), Tensor([16935, 10, 15, 20],"float32"), ) 	 101610000 	 53247 	 17.43324565887451 	 17.471977710723877 	 0.3340568542480469 	 0.33603429794311523 	 None 	 None 	 None 	 None 	 
2025-08-05 10:55:14.631262 test begin: paddle.greater_equal(Tensor([198451, 2, 16, 4, 1],"int64"), Tensor([198451, 2, 16, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([198451, 2, 16, 4, 1],"int64"), Tensor([198451, 2, 16, 1, 8],"int64"), ) 	 76205184 	 53247 	 58.95926880836487 	 53.070321798324585 	 1.1325798034667969 	 1.018324851989746 	 None 	 None 	 None 	 None 	 
2025-08-05 10:57:08.058351 test begin: paddle.greater_equal(Tensor([49613, 1024, 1, 1],"float32"), Tensor([1],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([49613, 1024, 1, 1],"float32"), Tensor([1],"float32"), ) 	 50803713 	 53247 	 10.025401592254639 	 12.304969072341919 	 0.1920475959777832 	 0.23583555221557617 	 None 	 None 	 None 	 None 	 
2025-08-05 10:57:32.846846 test begin: paddle.greater_equal(Tensor([5, 10, 15, 67738],"float32"), Tensor([5, 10, 15, 67738],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([5, 10, 15, 67738],"float32"), Tensor([5, 10, 15, 67738],"float32"), ) 	 101607000 	 53247 	 17.436859130859375 	 17.47329592704773 	 0.3342258930206299 	 0.33603596687316895 	 None 	 None 	 None 	 None 	 
2025-08-05 10:58:10.555743 test begin: paddle.greater_equal(Tensor([5, 10, 50804, 20],"float32"), Tensor([5, 10, 50804, 20],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([5, 10, 50804, 20],"float32"), Tensor([5, 10, 50804, 20],"float32"), ) 	 101608000 	 53247 	 17.443195819854736 	 17.456092834472656 	 0.3342118263244629 	 0.33479762077331543 	 None 	 None 	 None 	 None 	 
2025-08-05 10:58:50.195523 test begin: paddle.greater_equal(Tensor([5, 33869, 15, 20],"float32"), Tensor([5, 33869, 15, 20],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([5, 33869, 15, 20],"float32"), Tensor([5, 33869, 15, 20],"float32"), ) 	 101607000 	 53247 	 17.44081211090088 	 17.47460675239563 	 0.3342559337615967 	 0.33605527877807617 	 None 	 None 	 None 	 None 	 
2025-08-05 10:59:26.867592 test begin: paddle.greater_equal(Tensor([8, 1024, 1, 6202],"float32"), Tensor([1],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([8, 1024, 1, 6202],"float32"), Tensor([1],"float32"), ) 	 50806785 	 53247 	 9.981729984283447 	 12.322807788848877 	 0.19135046005249023 	 0.23724865913391113 	 None 	 None 	 None 	 None 	 
2025-08-05 10:59:50.340576 test begin: paddle.greater_equal(Tensor([8, 1024, 6202, 1],"float32"), Tensor([1],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([8, 1024, 6202, 1],"float32"), Tensor([1],"float32"), ) 	 50806785 	 53247 	 9.978545188903809 	 12.307896137237549 	 0.19127631187438965 	 0.23720645904541016 	 None 	 None 	 None 	 None 	 
2025-08-05 11:00:13.689778 test begin: paddle.greater_equal(Tensor([8, 6350401, 1, 1],"float32"), Tensor([1],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([8, 6350401, 1, 1],"float32"), Tensor([1],"float32"), ) 	 50803209 	 53247 	 10.0220947265625 	 12.300918579101562 	 0.19203686714172363 	 0.23575305938720703 	 None 	 None 	 None 	 None 	 
2025-08-05 11:00:39.825364 test begin: paddle.greater_equal(Tensor([99226, 2, 16, 4, 1],"int64"), Tensor([99226, 2, 16, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([99226, 2, 16, 4, 1],"int64"), Tensor([99226, 2, 16, 1, 8],"int64"), ) 	 38102784 	 53247 	 29.396790742874146 	 26.714443683624268 	 0.5633606910705566 	 0.5117778778076172 	 None 	 None 	 None 	 None 	 
2025-08-05 11:01:39.822613 test begin: paddle.greater_than(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 50803600 	 53046 	 10.099414348602295 	 13.199972867965698 	 0.19428205490112305 	 0.2539198398590088 	 None 	 None 	 None 	 None 	 
2025-08-05 11:02:05.889437 test begin: paddle.greater_than(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), ) 	 50803600 	 53046 	 11.062202215194702 	 13.231642961502075 	 0.19205760955810547 	 0.2544412612915039 	 None 	 None 	 None 	 None 	 
2025-08-05 11:02:31.399945 test begin: paddle.greater_than(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 101606800 	 53046 	 17.371007204055786 	 17.407379150390625 	 0.33425045013427734 	 0.33600902557373047 	 None 	 None 	 None 	 None 	 
2025-08-05 11:03:08.015507 test begin: paddle.greater_than(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), ) 	 101606420 	 53046 	 17.3744056224823 	 17.39953875541687 	 0.33425426483154297 	 0.3362867832183838 	 None 	 None 	 None 	 None 	 
2025-08-05 11:03:46.584224 test begin: paddle.greater_than(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float32"), )
W0805 11:03:49.952522 105377 dygraph_functions.cc:90428] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float32"), ) 	 203212812 	 53046 	 60.0508177280426 	 38.12711191177368 	 0.5776231288909912 	 0.7347745895385742 	 None 	 None 	 None 	 None 	 
2025-08-05 11:05:28.529531 test begin: paddle.greater_than(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float64"), ) 	 203212812 	 53046 	 107.75652813911438 	 51.934226751327515 	 1.0390126705169678 	 1.0018181800842285 	 None 	 None 	 None 	 None 	 
2025-08-05 11:08:12.536260 test begin: paddle.greater_than(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), ) 	 101606440 	 53046 	 17.38353991508484 	 17.389812231063843 	 0.33550453186035156 	 0.33481645584106445 	 None 	 None 	 None 	 None 	 
2025-08-05 11:08:50.870752 test begin: paddle.greater_than(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float32"), ) 	 203212816 	 53046 	 60.05668354034424 	 38.14954209327698 	 0.577765941619873 	 0.7332777976989746 	 None 	 None 	 None 	 None 	 
2025-08-05 11:10:32.800608 test begin: paddle.greater_than(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float64"), ) 	 203212816 	 53046 	 108.41890788078308 	 51.94022727012634 	 1.0377864837646484 	 0.9991579055786133 	 None 	 None 	 None 	 None 	 
2025-08-05 11:13:19.961917 test begin: paddle.greater_than(Tensor([4, 3, 2116801],"float16"), Tensor([4, 3, 2116801],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3, 2116801],"float16"), Tensor([4, 3, 2116801],"float64"), ) 	 50803224 	 53046 	 27.44406509399414 	 13.310121774673462 	 0.2640249729156494 	 0.2559683322906494 	 None 	 None 	 None 	 None 	 
2025-08-05 11:14:01.886290 test begin: paddle.greater_than(Tensor([4, 3, 4233601],"float16"), Tensor([4, 3, 4233601],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3, 4233601],"float16"), Tensor([4, 3, 4233601],"float32"), ) 	 101606424 	 53046 	 30.27354335784912 	 19.258586883544922 	 0.29114365577697754 	 0.3704524040222168 	 None 	 None 	 None 	 None 	 
2025-08-05 11:14:53.268899 test begin: paddle.greater_than(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float32"), ) 	 203212824 	 53046 	 60.05280327796936 	 38.12541437149048 	 0.5777430534362793 	 0.7347438335418701 	 None 	 None 	 None 	 None 	 
2025-08-05 11:16:35.196448 test begin: paddle.greater_than(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float64"), ) 	 203212824 	 53046 	 107.76162815093994 	 51.935880184173584 	 1.037743091583252 	 0.9991304874420166 	 None 	 None 	 None 	 None 	 
2025-08-05 11:19:20.806098 test begin: paddle.greater_than(Tensor([4, 3175201, 2],"float16"), Tensor([4, 3175201, 2],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3175201, 2],"float16"), Tensor([4, 3175201, 2],"float64"), ) 	 50803216 	 53046 	 27.444516897201538 	 13.536595344543457 	 0.2639181613922119 	 0.25722551345825195 	 None 	 None 	 None 	 None 	 
2025-08-05 11:20:04.461497 test begin: paddle.greater_than(Tensor([4, 6350401, 2],"float16"), Tensor([4, 6350401, 2],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 6350401, 2],"float16"), Tensor([4, 6350401, 2],"float32"), ) 	 101606416 	 53046 	 30.27202534675598 	 19.26759433746338 	 0.29110217094421387 	 0.37197399139404297 	 None 	 None 	 None 	 None 	 
2025-08-05 11:20:57.259375 test begin: paddle.greater_than(Tensor([4233601, 3, 2],"float16"), Tensor([4233601, 3, 2],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4233601, 3, 2],"float16"), Tensor([4233601, 3, 2],"float64"), ) 	 50803212 	 53046 	 27.45097279548645 	 13.303568363189697 	 0.26522016525268555 	 0.25733137130737305 	 None 	 None 	 None 	 None 	 
2025-08-05 11:21:40.716863 test begin: paddle.greater_than(Tensor([8467201, 3, 2],"float16"), Tensor([8467201, 3, 2],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([8467201, 3, 2],"float16"), Tensor([8467201, 3, 2],"float32"), ) 	 101606412 	 53046 	 30.267640590667725 	 19.265052556991577 	 0.29114603996276855 	 0.3704652786254883 	 None 	 None 	 None 	 None 	 
2025-08-05 11:22:32.080181 test begin: paddle.heaviside(Tensor([24807, 2048],"float32"), Tensor([1],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5ab04cb2b0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 11:33:11.400668 test begin: paddle.heaviside(Tensor([24807, 2048],"float32"), Tensor([2048],"float32"), )
W0805 11:33:12.391647 93993 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([24807, 2048],"float32"), Tensor([2048],"float32"), ) 	 50806784 	 33800 	 10.064295291900635 	 10.34183955192566 	 0.30375218391418457 	 0.31215858459472656 	 None 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-08-05 11:34:19.208441 test begin: paddle.heaviside(Tensor([24807, 2048],"float32"), Tensor([24807, 2048],"float32"), )
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([24807, 2048],"float32"), Tensor([24807, 2048],"float32"), ) 	 101609472 	 33800 	 15.25178074836731 	 15.116616249084473 	 0.4603245258331299 	 0.45638370513916016 	 None 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-08-05 11:35:14.887313 test begin: paddle.heaviside(Tensor([300, 169345],"float32"), Tensor([169345],"float32"), )
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([300, 169345],"float32"), Tensor([169345],"float32"), ) 	 50972845 	 33800 	 10.011534452438354 	 10.344958066940308 	 0.30330753326416016 	 0.3136124610900879 	 None 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-08-05 11:35:59.687806 test begin: paddle.heaviside(Tensor([300, 169345],"float32"), Tensor([1],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f271c796920>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 11:46:43.092436 test begin: paddle.heaviside(Tensor([300, 169345],"float32"), Tensor([300, 169345],"float32"), )
W0805 11:46:44.945922  2575 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([300, 169345],"float32"), Tensor([300, 169345],"float32"), ) 	 101607000 	 33800 	 15.260156393051147 	 15.120527267456055 	 0.4603116512298584 	 0.4563407897949219 	 None 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-08-05 11:47:40.658447 test begin: paddle.histogram(input=Tensor([4, 6350401],"int64"), )
[Prof] paddle.histogram 	 paddle.histogram(input=Tensor([4, 6350401],"int64"), ) 	 25401604 	 1544 	 9.863420248031616 	 1.1829540729522705 	 0.00037026405334472656 	 0.0004138946533203125 	 None 	 None 	 None 	 None 	 
2025-08-05 11:47:52.247833 test begin: paddle.histogram(input=Tensor([6350401, 4],"int64"), )
[Prof] paddle.histogram 	 paddle.histogram(input=Tensor([6350401, 4],"int64"), ) 	 25401604 	 1544 	 9.88200068473816 	 1.1806693077087402 	 0.0003693103790283203 	 0.0004119873046875 	 None 	 None 	 None 	 None 	 
2025-08-05 11:48:03.906387 test begin: paddle.histogram_bin_edges(Tensor([2540161, 20],"float32"), bins=10, min=0, max=1, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([2540161, 20],"float32"), bins=10, min=0, max=1, ) 	 50803220 	 98929 	 10.382341861724854 	 1.5362184047698975 	 5.340576171875e-05 	 0.00010633468627929688 	 None 	 None 	 None 	 None 	 combined
2025-08-05 11:48:16.812829 test begin: paddle.histogram_bin_edges(Tensor([5, 10160641],"float32"), bins=10, min=1, max=1, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([5, 10160641],"float32"), bins=10, min=1, max=1, ) 	 50803205 	 98929 	 10.115142345428467 	 1.5556902885437012 	 5.53131103515625e-05 	 7.128715515136719e-05 	 None 	 None 	 None 	 None 	 combined
2025-08-05 11:48:29.622359 test begin: paddle.histogram_bin_edges(Tensor([50, 10160641],"float32"), bins=10, min=0, max=1, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([50, 10160641],"float32"), bins=10, min=0, max=1, ) 	 508032050 	 98929 	 10.663989305496216 	 1.5600197315216064 	 5.7220458984375e-05 	 0.00011086463928222656 	 None 	 None 	 None 	 None 	 combined
2025-08-05 11:48:50.471042 test begin: paddle.histogram_bin_edges(Tensor([50, 10160641],"float32"), bins=10, min=1, max=1, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([50, 10160641],"float32"), bins=10, min=1, max=1, ) 	 508032050 	 98929 	 10.275499105453491 	 1.6012299060821533 	 0.00012111663818359375 	 0.00022649765014648438 	 None 	 None 	 None 	 None 	 combined
2025-08-05 11:49:13.225601 test begin: paddle.histogramdd(Tensor([1270, 2, 2],"float64"), bins=5, weights=Tensor([1270, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )
/usr/local/lib/python3.10/dist-packages/paddle/tensor/linalg.py:5741: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  edge = paddle.to_tensor(edge)
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([1270, 2, 2],"float64"), bins=5, weights=Tensor([1270, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 	 7620 	 5336 	 10.078702449798584 	 0.3523521423339844 	 6.341934204101562e-05 	 0.00038123130798339844 	 None 	 None 	 None 	 None 	 
2025-08-05 11:49:23.763666 test begin: paddle.histogramdd(Tensor([4, 15876, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 15876],"float64"), ranges=None, density=False, )
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([4, 15876, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 15876],"float64"), ranges=None, density=False, ) 	 317520 	 5336 	 20.78658151626587 	 6.702203750610352 	 3.314018249511719e-05 	 0.0010492801666259766 	 None 	 None 	 None 	 None 	 
2025-08-05 11:49:51.334955 test begin: paddle.histogramdd(Tensor([4, 15876, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 15876],"float64"), ranges=None, density=True, )
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([4, 15876, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 15876],"float64"), ranges=None, density=True, ) 	 317520 	 5336 	 23.191733598709106 	 7.9975666999816895 	 4.57763671875e-05 	 0.0003056526184082031 	 None 	 None 	 None 	 None 	 
2025-08-05 11:50:22.577211 test begin: paddle.histogramdd(Tensor([4, 63504, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 63504],"float64"), ranges=None, density=False, )
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([4, 63504, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 63504],"float64"), ranges=None, density=False, ) 	 1270080 	 5336 	 112.03854465484619 	 26.01796007156372 	 5.1975250244140625e-05 	 0.0003638267517089844 	 None 	 None 	 None 	 None 	 
2025-08-05 11:52:41.383555 test begin: paddle.histogramdd(Tensor([4, 63504, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 63504],"float64"), ranges=None, density=True, )
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([4, 63504, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 63504],"float64"), ranges=None, density=True, ) 	 1270080 	 5336 	 107.08113241195679 	 24.90142059326172 	 8.511543273925781e-05 	 0.001186370849609375 	 None 	 None 	 None 	 None 	 
2025-08-05 11:54:53.443214 test begin: paddle.histogramdd(Tensor([63504, 2, 2],"float64"), bins=5, weights=Tensor([63504, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([63504, 2, 2],"float64"), bins=5, weights=Tensor([63504, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 	 381024 	 5336 	 16.55294156074524 	 0.7002379894256592 	 0.00016832351684570312 	 0.0002593994140625 	 None 	 None 	 None 	 None 	 
2025-08-05 11:55:10.722678 test begin: paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[-1,1,3,], )
W0805 11:55:40.094012 158659 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[-1,1,3,], ) 	 254016180 	 578883 	 19.20679020881653 	 5.595253229141235 	 0.00020194053649902344 	 0.0007953643798828125 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 11:55:48.532632 test begin: paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[-1,], )
W0805 11:56:08.457005 162133 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[-1,], ) 	 254016180 	 578883 	 11.501703023910522 	 4.012130975723267 	 0.000152587890625 	 0.00028324127197265625 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 11:56:14.147003 test begin: paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[2,4,], )
W0805 11:56:37.278404  1173 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[2,4,], ) 	 254016180 	 578883 	 14.52087926864624 	 4.80446195602417 	 0.0001494884490966797 	 0.00030517578125 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 11:56:44.014181 test begin: paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[-1,1,3,], )
W0805 11:57:14.164064  4017 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[-1,1,3,], ) 	 254016120 	 578883 	 19.708585023880005 	 5.614996671676636 	 0.0001277923583984375 	 0.00030040740966796875 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 11:57:22.609409 test begin: paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[-1,], )
W0805 11:57:39.826447  7621 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[-1,], ) 	 254016120 	 578883 	 10.420280933380127 	 4.142555236816406 	 0.0002605915069580078 	 0.0009026527404785156 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 11:57:45.604157 test begin: paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[2,4,], )
W0805 11:58:07.119199  9549 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[2,4,], ) 	 254016120 	 578883 	 14.665952444076538 	 4.8331522941589355 	 0.0001442432403564453 	 0.0001347064971923828 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 11:58:14.714709 test begin: paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[-1,1,3,], )
W0805 11:58:46.253881 12707 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[-1,1,3,], ) 	 254016240 	 578883 	 22.05057668685913 	 5.6685919761657715 	 0.00014543533325195312 	 0.0002446174621582031 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 11:58:53.501362 test begin: paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[-1,], )
W0805 11:59:12.913204 16231 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[-1,], ) 	 254016240 	 578883 	 12.39646029472351 	 7.085554838180542 	 0.00012540817260742188 	 0.0002589225769042969 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 11:59:23.789630 test begin: paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[2,4,], )
W0805 11:59:44.734270 19347 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[2,4,], ) 	 254016240 	 578883 	 14.264012336730957 	 4.886476516723633 	 0.00014734268188476562 	 9.942054748535156e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 11:59:50.974555 test begin: paddle.hstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], ) 	 76204872 	 32425 	 30.158180236816406 	 29.986239194869995 	 0.951617956161499 	 0.943300724029541 	 30.11165738105774 	 2.3695731163024902 	 0.9479672908782959 	 9.894371032714844e-05 	 
2025-08-05 12:01:26.948500 test begin: paddle.hstack(list[Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2, 1058401],"float64"),], ) 	 25401624 	 32425 	 10.228942155838013 	 10.17317271232605 	 0.32218003273010254 	 0.16132116317749023 	 10.077861547470093 	 1.7852797508239746 	 0.3199920654296875 	 9.894371032714844e-05 	 
2025-08-05 12:02:00.311568 test begin: paddle.hstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401870 	 32425 	 10.282029628753662 	 10.437185287475586 	 0.3248097896575928 	 0.32817649841308594 	 10.144273281097412 	 2.31569766998291 	 0.3192164897918701 	 0.00011110305786132812 	 
2025-08-05 12:02:34.772130 test begin: paddle.hstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401870 	 32425 	 10.256365776062012 	 10.430981874465942 	 0.32310032844543457 	 0.32753872871398926 	 10.15963077545166 	 2.2819902896881104 	 0.321230411529541 	 0.00012087821960449219 	 
2025-08-05 12:03:09.097754 test begin: paddle.hstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], ) 	 76204836 	 32425 	 30.08066987991333 	 29.946995496749878 	 0.9466385841369629 	 0.9463968276977539 	 30.177419424057007 	 2.3370566368103027 	 0.9510700702667236 	 8.559226989746094e-05 	 
2025-08-05 12:04:44.929829 test begin: paddle.hstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 25401654 	 32425 	 10.307076215744019 	 10.336967706680298 	 0.3254828453063965 	 0.32515883445739746 	 10.143447875976562 	 2.2287087440490723 	 0.3193953037261963 	 0.00018906593322753906 	 
2025-08-05 12:05:19.090372 test begin: paddle.hstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401654 	 32425 	 10.275401830673218 	 10.374429702758789 	 0.3233144283294678 	 0.32690954208374023 	 10.137453079223633 	 2.2272682189941406 	 0.31925010681152344 	 0.00019788742065429688 	 
2025-08-05 12:05:53.247948 test begin: paddle.hstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 76204980 	 32425 	 30.0947744846344 	 30.146485328674316 	 0.9498047828674316 	 0.9485924243927002 	 30.18764638900757 	 2.3348827362060547 	 0.952613115310669 	 0.00010585784912109375 	 
2025-08-05 12:07:29.288215 test begin: paddle.hstack(list[Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401660 	 32425 	 10.200709581375122 	 10.169654130935669 	 0.3212263584136963 	 0.161146879196167 	 10.09919810295105 	 1.7783539295196533 	 0.3177671432495117 	 0.00010824203491210938 	 
2025-08-05 12:08:02.706189 test begin: paddle.hstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401654 	 32425 	 10.277636528015137 	 10.122408151626587 	 0.32353663444519043 	 0.31852126121520996 	 10.137280702590942 	 2.286709785461426 	 0.3191988468170166 	 0.00021648406982421875 	 
2025-08-05 12:08:37.334760 test begin: paddle.hstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 76204818 	 32425 	 30.275877475738525 	 30.108553886413574 	 0.9538300037384033 	 0.9472055435180664 	 30.651346921920776 	 2.3020992279052734 	 0.967731237411499 	 0.00010657310485839844 	 
2025-08-05 12:10:14.274057 test begin: paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401870 	 32425 	 10.266403675079346 	 10.254515886306763 	 0.32433509826660156 	 0.3246297836303711 	 10.14164423942566 	 2.2838194370269775 	 0.3193347454071045 	 0.00010704994201660156 	 
2025-08-05 12:10:49.855159 test begin: paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 76204890 	 32425 	 30.303941249847412 	 30.435105323791504 	 0.955833911895752 	 0.9576938152313232 	 30.539828777313232 	 2.3093173503875732 	 0.9656083583831787 	 8.225440979003906e-05 	 
2025-08-05 12:12:26.936867 test begin: paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401630 	 32425 	 10.262953281402588 	 10.176015377044678 	 0.32311463356018066 	 0.16124606132507324 	 10.124970436096191 	 1.8060381412506104 	 0.3188347816467285 	 7.843971252441406e-05 	 
2025-08-05 12:13:00.418669 test begin: paddle.hstack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], ) 	 76204824 	 32425 	 31.710930347442627 	 45.271334409713745 	 1.0014631748199463 	 1.4301278591156006 	 31.156034469604492 	 2.24122953414917 	 0.9821975231170654 	 8.511543273925781e-05 	 
2025-08-05 12:14:54.346059 test begin: paddle.hstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 76204920 	 32425 	 30.07557773590088 	 33.483755588531494 	 0.9504222869873047 	 1.056457281112671 	 30.471925497055054 	 2.2930121421813965 	 0.9599790573120117 	 7.891654968261719e-05 	 
2025-08-05 12:16:33.993800 test begin: paddle.hstack(list[Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401640 	 32425 	 9.986421585083008 	 10.166544914245605 	 0.3144826889038086 	 0.16127872467041016 	 10.596791744232178 	 1.785571813583374 	 0.3346424102783203 	 8.058547973632812e-05 	 
2025-08-05 12:17:08.694475 test begin: paddle.hypot(Tensor([10, 5080321],"float32"), Tensor([10, 1],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([10, 5080321],"float32"), Tensor([10, 1],"float32"), ) 	 50803220 	 10387 	 10.003862142562866 	 3.3431811332702637 	 0.24576187133789062 	 0.3291435241699219 	 11.666099071502686 	 19.054788827896118 	 0.22912955284118652 	 0.3115196228027344 	 
2025-08-05 12:17:56.445459 test begin: paddle.hypot(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), ) 	 101606420 	 10387 	 15.384796857833862 	 4.674298286437988 	 0.3778414726257324 	 0.45798420906066895 	 17.34291648864746 	 18.643478631973267 	 0.34101104736328125 	 0.4604763984680176 	 
2025-08-05 12:18:56.043907 test begin: paddle.hypot(Tensor([2540161, 20],"float32"), Tensor([2540161, 20],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([2540161, 20],"float32"), Tensor([2540161, 20],"float32"), ) 	 101606440 	 10387 	 15.384708404541016 	 4.676700830459595 	 0.377826452255249 	 0.45802736282348633 	 17.3414568901062 	 18.642539262771606 	 0.3410158157348633 	 0.45787501335144043 	 
2025-08-05 12:19:55.760273 test begin: paddle.hypot(Tensor([50803201],"float32"), Tensor([1],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([50803201],"float32"), Tensor([1],"float32"), ) 	 50803202 	 10387 	 10.012698888778687 	 3.281364679336548 	 0.24711298942565918 	 0.32093381881713867 	 11.01386046409607 	 18.753074407577515 	 0.21761751174926758 	 0.3068387508392334 	 
2025-08-05 12:20:40.694282 test begin: paddle.hypot(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 10387 	 15.376986265182495 	 4.662919998168945 	 0.3776688575744629 	 0.4579505920410156 	 17.341167449951172 	 18.63578701019287 	 0.34105706214904785 	 0.45789623260498047 	 
2025-08-05 12:21:41.734868 test begin: paddle.hypot(Tensor([5080321, 10],"float32"), Tensor([5080321, 1],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([5080321, 10],"float32"), Tensor([5080321, 1],"float32"), ) 	 55883531 	 10387 	 10.546659231185913 	 3.4844558238983154 	 0.2591400146484375 	 0.3433952331542969 	 13.50323224067688 	 21.784873723983765 	 0.33141279220581055 	 0.42925405502319336 	 
2025-08-05 12:22:32.842167 test begin: paddle.i0(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 21481 	 10.001972198486328 	 8.75917387008667 	 0.4768681526184082 	 0.41588473320007324 	 9.614486694335938 	 18.38689136505127 	 0.4567723274230957 	 0.4366152286529541 	 
2025-08-05 12:23:21.390484 test begin: paddle.i0(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 21481 	 10.043008089065552 	 8.747462034225464 	 0.47832155227661133 	 0.4183528423309326 	 9.562311887741089 	 18.38486671447754 	 0.4570882320404053 	 0.43779993057250977 	 
2025-08-05 12:24:09.959089 test begin: paddle.i0(Tensor([25401601],"float64"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([25401601],"float64"), ) 	 25401601 	 21481 	 10.642165899276733 	 9.978141784667969 	 0.5072135925292969 	 0.4741377830505371 	 11.343689441680908 	 19.691317081451416 	 0.539353609085083 	 0.4691908359527588 	 
2025-08-05 12:25:02.908932 test begin: paddle.i0(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 21481 	 10.048415660858154 	 8.755616426467896 	 0.4773547649383545 	 0.41577601432800293 	 9.566391706466675 	 18.378164768218994 	 0.4544520378112793 	 0.43917036056518555 	 
2025-08-05 12:25:51.419263 test begin: paddle.i0(Tensor([50803201],"float32"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([50803201],"float32"), ) 	 50803201 	 21481 	 10.044739961624146 	 8.776274919509888 	 0.47739529609680176 	 0.4157676696777344 	 9.565675735473633 	 18.38620638847351 	 0.45430922508239746 	 0.43680310249328613 	 
2025-08-05 12:26:40.026479 test begin: paddle.i0e(Tensor([25401601],"float64"), )
[Prof] paddle.i0e 	 paddle.i0e(Tensor([25401601],"float64"), ) 	 25401601 	 25383 	 10.257266521453857 	 9.561975002288818 	 0.4157559871673584 	 0.384080171585083 	 14.9625563621521 	 50.89925789833069 	 0.6016199588775635 	 0.41080665588378906 	 
2025-08-05 12:28:07.078372 test begin: paddle.i0e(Tensor([50803201],"float32"), )
[Prof] paddle.i0e 	 paddle.i0e(Tensor([50803201],"float32"), ) 	 50803201 	 25383 	 10.913949489593506 	 9.52573013305664 	 0.4388740062713623 	 0.38209056854248047 	 14.912248373031616 	 49.08580231666565 	 0.5994277000427246 	 0.3947432041168213 	 
2025-08-05 12:29:33.214828 test begin: paddle.i1(Tensor([25401601],"float64"), )
[Prof] paddle.i1 	 paddle.i1(Tensor([25401601],"float64"), ) 	 25401601 	 19933 	 9.966410398483276 	 9.512711763381958 	 0.5119161605834961 	 0.48740434646606445 	 12.358164072036743 	 64.00679230690002 	 0.6350183486938477 	 0.2980475425720215 	 
2025-08-05 12:31:11.600091 test begin: paddle.i1(Tensor([50803201],"float32"), )
[Prof] paddle.i1 	 paddle.i1(Tensor([50803201],"float32"), ) 	 50803201 	 19933 	 6.802424192428589 	 8.3096342086792 	 0.34894394874572754 	 0.4242434501647949 	 11.731343507766724 	 66.63674974441528 	 0.6033585071563721 	 0.31149744987487793 	 
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 9, in <module>
    from tester import (APIConfig, APITestAccuracy, APITestCINNVSDygraph,
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/__init__.py", line 50, in __getattr__
    from .accuracy import APITestAccuracy
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/accuracy.py", line 9, in <module>
    from .api_config.log_writer import write_to_log
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/log_writer.py", line 7, in <module>
    import pandas as pd
  File "/usr/local/lib/python3.10/dist-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/usr/local/lib/python3.10/dist-packages/pandas/core/api.py", line 47, in <module>
    from pandas.core.groupby import (
  File "/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/__init__.py", line 1, in <module>
    from pandas.core.groupby.generic import (
  File "/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/generic.py", line 68, in <module>
    from pandas.core.frame import DataFrame
  File "/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py", line 149, in <module>
    from pandas.core.generic import (
  File "/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py", line 152, in <module>
    from pandas.core import (
  File "/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py", line 79, in <module>
    from pandas.core.indexes.api import (
  File "/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/api.py", line 20, in <module>
    from pandas.core.indexes.base import (
  File "/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py", line 190, in <module>
    from pandas.core.strings.accessor import StringMethods
  File "/usr/local/lib/python3.10/dist-packages/pandas/core/strings/accessor.py", line 160, in <module>
    class StringMethods(NoNewAttributesMixin):
  File "/usr/local/lib/python3.10/dist-packages/pandas/core/strings/accessor.py", line 3250, in StringMethods
    @forbid_nonstring_types(["bytes"])
  File "/usr/local/lib/python3.10/dist-packages/pandas/core/strings/accessor.py", line 125, in forbid_nonstring_types
    allowed_types = {"string", "empty", "bytes", "mixed", "mixed-integer"} - set(
KeyboardInterrupt
2025-08-04 11:30:44.393149 test begin: paddle.i1e(Tensor([25401601],"float64"), )
W0804 11:30:50.727612 84476 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.i1e 	 paddle.i1e(Tensor([25401601],"float64"), ) 	 25401601 	 31796 	 12.594427824020386 	 11.883344173431396 	 0.40476274490356445 	 0.38191676139831543 	 18.760042905807495 	 122.27874946594238 	 0.6029789447784424 	 0.3027067184448242 	 
2025-08-04 11:33:39.963412 test begin: paddle.i1e(Tensor([50803201],"float32"), )
[Prof] paddle.i1e 	 paddle.i1e(Tensor([50803201],"float32"), ) 	 50803201 	 31796 	 11.792380094528198 	 9.450494766235352 	 0.32100510597229004 	 0.3036618232727051 	 18.66757607460022 	 128.57859230041504 	 0.6000497341156006 	 0.3182516098022461 	 
2025-08-04 11:36:31.270148 test begin: paddle.incubate.nn.functional.fused_dropout_add(Tensor([7576, 13412],"bfloat16"), Tensor([7576, 13412],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, )
/usr/local/lib/python3.10/dist-packages/paddle/incubate/nn/functional/fused_dropout_add.py:100: UserWarning: Currently, fused_dropout_add maybe has precision problem, so it falls back to dropout + add. 
  warnings.warn(
[Prof] paddle.incubate.nn.functional.fused_dropout_add 	 paddle.incubate.nn.functional.fused_dropout_add(Tensor([7576, 13412],"bfloat16"), Tensor([7576, 13412],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, ) 	 203218624 	 22272 	 9.993403673171997 	 10.025165796279907 	 0.45876312255859375 	 0.4599740505218506 	 21.446516036987305 	 10.098841190338135 	 0.9840717315673828 	 0.46338582038879395 	 combined
2025-08-04 11:37:28.777523 test begin: paddle.incubate.nn.functional.fused_dropout_add(Tensor([7712, 13176],"bfloat16"), Tensor([7712, 13176],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, )
[Prof] paddle.incubate.nn.functional.fused_dropout_add 	 paddle.incubate.nn.functional.fused_dropout_add(Tensor([7712, 13176],"bfloat16"), Tensor([7712, 13176],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, ) 	 203226624 	 22272 	 10.008058547973633 	 10.029354572296143 	 0.4585998058319092 	 0.4600107669830322 	 21.434465408325195 	 10.10111403465271 	 0.9834568500518799 	 0.46345067024230957 	 combined
2025-08-04 11:38:27.458459 test begin: paddle.incubate.nn.functional.fused_dropout_add(Tensor([79381, 1280],"bfloat16"), Tensor([79381, 1280],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, )
[Prof] paddle.incubate.nn.functional.fused_dropout_add 	 paddle.incubate.nn.functional.fused_dropout_add(Tensor([79381, 1280],"bfloat16"), Tensor([79381, 1280],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, ) 	 203215360 	 22272 	 9.983794927597046 	 10.023438453674316 	 0.4582548141479492 	 0.459972620010376 	 21.448353052139282 	 10.100632190704346 	 0.9841370582580566 	 0.46352601051330566 	 combined
2025-08-04 11:39:26.187786 test begin: paddle.incubate.nn.functional.fused_dropout_add(Tensor([8168, 12440],"bfloat16"), Tensor([8168, 12440],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, )
[Prof] paddle.incubate.nn.functional.fused_dropout_add 	 paddle.incubate.nn.functional.fused_dropout_add(Tensor([8168, 12440],"bfloat16"), Tensor([8168, 12440],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, ) 	 203219840 	 22272 	 9.9793221950531 	 10.023300170898438 	 0.45800256729125977 	 0.4599475860595703 	 21.447885036468506 	 10.100857973098755 	 0.984154224395752 	 0.4634988307952881 	 combined
2025-08-04 11:40:24.968778 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 14176, 7168],"bfloat16"), Tensor([7168, 12800],"bfloat16"), Tensor([12800],"bfloat16"), )
W0804 11:40:51.143810 88649 backward.cc:462] While running Node (FusedGemmEpilogueGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([1, 14176, 7168],"bfloat16"), Tensor([7168, 12800],"bfloat16"), Tensor([12800],"bfloat16"), ) 	 193376768 	 1922 	 20.57446813583374 	 20.726672172546387 	 10.967820644378662 	 11.056071281433105 	 None 	 None 	 None 	 None 	 combined
2025-08-04 11:41:57.973672 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 12404],"bfloat16"), Tensor([12404, 8192],"bfloat16"), None, False, None, )
W0804 11:42:11.325668 89361 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 12404],"bfloat16"), Tensor([12404, 8192],"bfloat16"), None, False, None, ) 	 152420352 	 1922 	 10.060259580612183 	 10.224589824676514 	 2.6796765327453613 	 2.7188005447387695 	 None 	 None 	 None 	 None 	 combined
2025-08-04 11:42:42.813077 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 16384],"bfloat16"), Tensor([16384, 6202],"bfloat16"), None, False, None, )
W0804 11:42:56.907984 89474 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 16384],"bfloat16"), Tensor([16384, 6202],"bfloat16"), None, False, None, ) 	 168722432 	 1922 	 11.17523455619812 	 11.182004690170288 	 2.9727234840393066 	 2.9741601943969727 	 None 	 None 	 None 	 None 	 combined
2025-08-04 11:43:31.604502 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 24807],"bfloat16"), Tensor([24807, 8192],"bfloat16"), None, False, None, )
W0804 11:44:17.670858 90003 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 24807],"bfloat16"), Tensor([24807, 8192],"bfloat16"), None, False, None, ) 	 304828416 	 1922 	 37.46781849861145 	 40.83132266998291 	 9.628164291381836 	 10.855393409729004 	 None 	 None 	 None 	 None 	 combined
2025-08-04 11:46:30.152172 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 8192],"bfloat16"), Tensor([8192, 12404],"bfloat16"), None, transpose_weight=False, )
W0804 11:46:43.594831 90920 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 8192],"bfloat16"), Tensor([8192, 12404],"bfloat16"), None, transpose_weight=False, ) 	 135168000 	 1922 	 10.716027975082397 	 10.806949138641357 	 5.698376655578613 	 5.743244647979736 	 None 	 None 	 None 	 None 	 combined
2025-08-04 11:47:17.647374 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 6202, 16384],"bfloat16"), Tensor([16384, 8192],"bfloat16"), None, False, None, )
W0804 11:47:38.564219 91434 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([1, 6202, 16384],"bfloat16"), Tensor([16384, 8192],"bfloat16"), None, False, None, ) 	 235831296 	 1922 	 13.386392831802368 	 14.973543643951416 	 7.116427183151245 	 7.108110189437866 	 None 	 None 	 None 	 None 	 combined
2025-08-04 11:48:23.922080 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 8192, 12404],"bfloat16"), Tensor([12404, 12800],"bfloat16"), Tensor([12800],"bfloat16"), )
W0804 11:49:00.759243 91597 backward.cc:462] While running Node (FusedGemmEpilogueGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([1, 8192, 12404],"bfloat16"), Tensor([12404, 12800],"bfloat16"), Tensor([12800],"bfloat16"), ) 	 260397568 	 1922 	 31.425963163375854 	 31.802839994430542 	 16.71778678894043 	 16.923475980758667 	 None 	 None 	 None 	 None 	 combined
2025-08-04 11:50:35.790225 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 8192, 7168],"bfloat16"), Tensor([7168, 14176],"bfloat16"), Tensor([14176],"bfloat16"), )
W0804 11:50:58.759121 92319 backward.cc:462] While running Node (FusedGemmEpilogueGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([1, 8192, 7168],"bfloat16"), Tensor([7168, 14176],"bfloat16"), Tensor([14176],"bfloat16"), ) 	 160348000 	 1922 	 13.581571578979492 	 13.59161376953125 	 7.216062068939209 	 7.227603435516357 	 None 	 None 	 None 	 None 	 combined
2025-08-04 11:51:42.333628 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([2, 4096, 16384],"bfloat16"), Tensor([16384, 8192],"bfloat16"), None, False, None, )
W0804 11:52:04.536573 92884 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([2, 4096, 16384],"bfloat16"), Tensor([16384, 8192],"bfloat16"), None, False, None, ) 	 268435456 	 1922 	 17.08770728111267 	 17.03934097290039 	 9.090693235397339 	 9.062299251556396 	 None 	 None 	 None 	 None 	 combined
2025-08-04 11:52:57.302061 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([2, 8192, 7168],"bfloat16"), Tensor([7168, 12800],"bfloat16"), Tensor([12800],"bfloat16"), )
W0804 11:53:27.492259 93464 backward.cc:462] While running Node (FusedGemmEpilogueGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([2, 8192, 7168],"bfloat16"), Tensor([7168, 12800],"bfloat16"), Tensor([12800],"bfloat16"), ) 	 209203712 	 1922 	 23.943213939666748 	 23.892399072647095 	 12.736761331558228 	 12.717836380004883 	 None 	 None 	 None 	 None 	 combined
2025-08-04 11:54:43.904411 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 4096, 8192],"bfloat16"), Tensor([8192, 100352],"bfloat16"), None, transpose_weight=False, )
W0804 11:58:58.997965 93732 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f029d9eda50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 12:05:22.878569 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1016065, 50],"float32"), Tensor([40, 50],"float32"), None, False, True, )
W0804 12:05:23.873383 98029 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1016065, 50],"float32"), Tensor([40, 50],"float32"), None, False, True, ) 	 50805250 	 33751 	 15.8766348361969 	 15.987971782684326 	 0.482741117477417 	 0.4852316379547119 	 30.680619478225708 	 30.782428741455078 	 0.3107645511627197 	 0.3102240562438965 	 
2025-08-04 12:06:58.790172 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1016065, 50],"float32"), Tensor([50, 40],"float32"), None, False, False, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1016065, 50],"float32"), Tensor([50, 40],"float32"), None, False, False, ) 	 50805250 	 33751 	 15.699074029922485 	 15.716487407684326 	 0.4741981029510498 	 0.4760763645172119 	 30.763344764709473 	 30.85074520111084 	 0.31099963188171387 	 0.31093692779541016 	 
2025-08-04 12:08:33.842453 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1270081, 30],"float32"), Tensor([40, 1270081],"float32"), None, True, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1270081, 30],"float32"), Tensor([40, 1270081],"float32"), None, True, True, ) 	 88905670 	 33751 	 13.375068187713623 	 13.296993017196655 	 0.20250177383422852 	 0.20129919052124023 	 23.716118335723877 	 23.700806140899658 	 0.17955899238586426 	 0.17901158332824707 	 
2025-08-04 12:09:49.462965 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1693441, 30],"float32"), Tensor([40, 1693441],"float32"), None, True, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1693441, 30],"float32"), Tensor([40, 1693441],"float32"), None, True, True, ) 	 118540870 	 33751 	 17.54022240638733 	 17.457304000854492 	 0.2655818462371826 	 0.26431822776794434 	 31.493319749832153 	 31.239484071731567 	 0.23820853233337402 	 0.23626422882080078 	 
2025-08-04 12:11:31.799195 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1270081],"float32"), Tensor([1270081, 40],"float32"), None, False, False, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1270081],"float32"), Tensor([1270081, 40],"float32"), None, False, False, ) 	 88905670 	 33751 	 14.156818628311157 	 13.985231161117554 	 0.21436190605163574 	 0.21176433563232422 	 24.79585886001587 	 24.76833748817444 	 0.18773102760314941 	 0.18762993812561035 	 
2025-08-04 12:12:51.033370 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1270081],"float32"), Tensor([40, 1270081],"float32"), None, False, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1270081],"float32"), Tensor([40, 1270081],"float32"), None, False, True, ) 	 88905670 	 33751 	 13.644336938858032 	 13.571125507354736 	 0.20655274391174316 	 0.2054581642150879 	 23.4049232006073 	 23.45673370361328 	 0.1797795295715332 	 0.17815589904785156 	 
2025-08-04 12:14:06.624044 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1693441],"float32"), Tensor([1693441, 40],"float32"), None, False, False, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1693441],"float32"), Tensor([1693441, 40],"float32"), None, False, False, ) 	 118540870 	 33751 	 18.31877899169922 	 18.19487500190735 	 0.27814817428588867 	 0.27534914016723633 	 32.467018365859985 	 32.44819617271423 	 0.24550247192382812 	 0.24629855155944824 	 
2025-08-04 12:15:51.294944 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1693441],"float32"), Tensor([40, 1693441],"float32"), None, False, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1693441],"float32"), Tensor([40, 1693441],"float32"), None, False, True, ) 	 118540870 	 33751 	 17.91645336151123 	 17.833484888076782 	 0.27132558822631836 	 0.27007031440734863 	 31.160893440246582 	 30.668681859970093 	 0.23619985580444336 	 0.23282718658447266 	 
2025-08-04 12:17:31.704124 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([1016065, 50],"float32"), None, False, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([1016065, 50],"float32"), None, False, True, ) 	 50804750 	 33751 	 10.421709775924683 	 10.462636947631836 	 0.3156249523162842 	 0.3170313835144043 	 21.40471363067627 	 21.37084460258484 	 0.2162165641784668 	 0.21561956405639648 	 
2025-08-04 12:18:39.152043 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([50, 1016065],"float32"), None, False, False, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([50, 1016065],"float32"), None, False, False, ) 	 50804750 	 33751 	 10.173176527023315 	 10.14337158203125 	 0.3083970546722412 	 0.3082423210144043 	 21.95242929458618 	 22.038148403167725 	 0.22300052642822266 	 0.22190451622009277 	 
2025-08-04 12:19:47.488550 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 1016065],"float32"), Tensor([40, 50],"float32"), None, True, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 1016065],"float32"), Tensor([40, 50],"float32"), None, True, True, ) 	 50805250 	 33751 	 15.562041282653809 	 15.643046855926514 	 0.4697556495666504 	 0.47196364402770996 	 29.97178053855896 	 29.834729433059692 	 0.3024780750274658 	 0.30089735984802246 	 
2025-08-04 12:21:20.821777 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([1016065, 50],"float32"), None, True, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([1016065, 50],"float32"), None, True, True, ) 	 50804750 	 33751 	 10.447204113006592 	 10.451055765151978 	 0.3155636787414551 	 0.3173544406890869 	 22.341835021972656 	 22.303146600723267 	 0.22541332244873047 	 0.22649765014648438 	 
2025-08-04 12:22:28.410579 test begin: paddle.incubate.nn.functional.swiglu(Tensor([14176, 7168],"bfloat16"), )
W0804 12:22:41.458428 103890 backward.cc:462] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.swiglu 	 paddle.incubate.nn.functional.swiglu(Tensor([14176, 7168],"bfloat16"), ) 	 101613568 	 43514 	 9.94609808921814 	 22.742486476898193 	 0.2335515022277832 	 0.26692700386047363 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:24:13.202037 test begin: paddle.incubate.segment_max(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_max" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_max" instead.
    Reason: paddle.incubate.segment_max will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_max" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_max" instead.
    Reason: paddle.incubate.segment_max will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.incubate.segment_max 	 paddle.incubate.segment_max(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), ) 	 50803206 	 9957 	 10.31272554397583 	 10.621509313583374 	 0.0010039806365966797 	 0.0002162456512451172 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 12:24:43.432961 test begin: paddle.incubate.segment_mean(Tensor([301, 16934],"float32"), Tensor([301],"int32"), )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_mean" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_mean" instead.
    Reason: paddle.incubate.segment_mean will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_mean" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_mean" instead.
    Reason: paddle.incubate.segment_mean will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6d36502950>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 12:34:56.230552 test begin: paddle.incubate.segment_min(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), )
W0804 12:34:57.362578 108828 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_min" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_min" instead.
    Reason: paddle.incubate.segment_min will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_min" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_min" instead.
    Reason: paddle.incubate.segment_min will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.incubate.segment_min 	 paddle.incubate.segment_min(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), ) 	 50803206 	 8591 	 5.757876634597778 	 7.295950651168823 	 0.000640869140625 	 0.0003333091735839844 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 12:35:17.336113 test begin: paddle.incubate.segment_sum(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_sum" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_sum" instead.
    Reason: paddle.incubate.segment_sum will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_sum" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_sum" instead.
    Reason: paddle.incubate.segment_sum will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.incubate.segment_sum 	 paddle.incubate.segment_sum(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), ) 	 50803206 	 16032 	 8.398250818252563 	 8.178425073623657 	 0.0004892349243164062 	 0.26029062271118164 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 12:35:47.427814 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1013, 1, 224, 224],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1013, 1, 224, 224],"float32"), ) 	 50828288 	 38131 	 10.030184030532837 	 23.22134017944336 	 0.26880526542663574 	 0.15591096878051758 	 12.429317951202393 	 34.07137393951416 	 0.33309149742126465 	 0.4565119743347168 	 combined
2025-08-04 12:37:11.888423 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([145, 7, 224, 224],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([145, 7, 224, 224],"float32"), ) 	 50928640 	 38131 	 9.979695320129395 	 23.269261598587036 	 0.2674722671508789 	 0.15609049797058105 	 12.48288106918335 	 34.13821768760681 	 0.3344860076904297 	 0.4574453830718994 	 combined
2025-08-04 12:38:38.496940 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([3, 338, 224, 224],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([3, 338, 224, 224],"float32"), ) 	 50878464 	 38131 	 10.159952640533447 	 23.23524785041809 	 0.26625728607177734 	 0.1559312343597412 	 12.58863353729248 	 34.1072051525116 	 0.3374443054199219 	 0.4570941925048828 	 combined
2025-08-04 12:40:01.734722 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([4511, 11, 32, 32],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([4511, 11, 32, 32],"float32"), ) 	 50811904 	 38131 	 28.52051281929016 	 25.7654869556427 	 0.7644081115722656 	 0.17290782928466797 	 17.119792222976685 	 34.087724685668945 	 0.4588449001312256 	 0.45674800872802734 	 combined
2025-08-04 12:41:52.231758 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([5, 203, 224, 224],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([5, 203, 224, 224],"float32"), ) 	 50928640 	 38131 	 9.971840143203735 	 23.256757259368896 	 0.2665829658508301 	 0.15609192848205566 	 12.59967827796936 	 34.13892078399658 	 0.33763575553894043 	 0.4574894905090332 	 combined
2025-08-04 12:43:13.968088 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([7, 7088, 32, 32],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([7, 7088, 32, 32],"float32"), ) 	 50806784 	 38131 	 28.482144594192505 	 25.75626516342163 	 0.76336669921875 	 0.1728801727294922 	 17.117042541503906 	 34.08410453796387 	 0.45880603790283203 	 0.4567732810974121 	 combined
2025-08-04 12:45:01.152068 test begin: paddle.index_add(Tensor([100, 100, 25402],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 25402],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 100, 25402],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 25402],"float32"), ) 	 304824020 	 32129 	 64.99361109733582 	 110.07346749305725 	 0.6882774829864502 	 0.00021600723266601562 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 12:49:15.134859 test begin: paddle.index_add(Tensor([100, 100, 25],"float32"), Tensor([5081],"int32"), 2, Tensor([100, 100, 5081],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f134befabf0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 12:59:20.108274 test begin: paddle.index_add(Tensor([100, 100, 5081],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 5081],"float32"), )
W0804 12:59:21.293777 117846 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 100, 5081],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 5081],"float32"), ) 	 60972020 	 32129 	 13.312628507614136 	 59.25194215774536 	 0.14098525047302246 	 0.0002181529998779297 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:00:49.430719 test begin: paddle.index_add(Tensor([100, 100, 5081],"float32"), Tensor([20],"int32"), 2, Tensor([100, 100, 20],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 100, 5081],"float32"), Tensor([20],"int32"), 2, Tensor([100, 100, 20],"float32"), ) 	 51010020 	 32129 	 10.9581458568573 	 58.265807151794434 	 0.11598563194274902 	 0.0002167224884033203 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:02:12.503120 test begin: paddle.index_add(Tensor([100, 101607, 5],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 5],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 101607, 5],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 5],"float32"), ) 	 50813520 	 32129 	 9.988110065460205 	 58.588409423828125 	 0.10569548606872559 	 0.0002086162567138672 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:03:33.075610 test begin: paddle.index_add(Tensor([100, 2540161],"float32"), Tensor([20],"int32"), 0, Tensor([20, 2540161],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 2540161],"float32"), Tensor([20],"int32"), 0, Tensor([20, 2540161],"float32"), ) 	 304819340 	 32129 	 63.877437353134155 	 105.88359785079956 	 0.6763348579406738 	 0.0002155303955078125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:07:44.851668 test begin: paddle.index_add(Tensor([100, 508033],"float32"), Tensor([20],"int32"), 0, Tensor([20, 508033],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 508033],"float32"), Tensor([20],"int32"), 0, Tensor([20, 508033],"float32"), ) 	 60963980 	 32129 	 13.024549722671509 	 55.251415491104126 	 0.13791894912719727 	 0.0002295970916748047 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:09:09.071394 test begin: paddle.index_add(Tensor([10160641, 5],"float32"), Tensor([20],"int32"), 0, Tensor([20, 5],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([10160641, 5],"float32"), Tensor([20],"int32"), 0, Tensor([20, 5],"float32"), ) 	 50803325 	 32129 	 10.22181487083435 	 55.528390884399414 	 0.1081383228302002 	 0.00020694732666015625 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:10:26.762053 test begin: paddle.index_fill(Tensor([10, 1016065, 10],"float16"), Tensor([5],"int64"), 1, 0.5, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 1016065, 10],"float16"), Tensor([5],"int64"), 1, 0.5, ) 	 101606505 	 29793 	 24.35464859008789 	 9.451618194580078 	 0.000675201416015625 	 0.10767650604248047 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:11:28.564908 test begin: paddle.index_fill(Tensor([10, 15, 169345],"int64"), Tensor([5],"int32"), 1, -1, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 15, 169345],"int64"), Tensor([5],"int32"), 1, -1, ) 	 25401755 	 29793 	 29.807772397994995 	 10.976608991622925 	 0.06365609169006348 	 0.12532401084899902 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 13:12:43.190581 test begin: paddle.index_fill(Tensor([10, 15, 338689],"bool"), Tensor([5],"int32"), 1, True, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 15, 338689],"bool"), Tensor([5],"int32"), 1, True, ) 	 50803355 	 29793 	 27.257591724395752 	 5.09844708442688 	 0.0007791519165039062 	 0.058125972747802734 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 13:13:44.666900 test begin: paddle.index_fill(Tensor([10, 15, 677377],"float16"), Tensor([5],"int64"), 1, 0.5, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 15, 677377],"float16"), Tensor([5],"int64"), 1, 0.5, ) 	 101606555 	 29793 	 60.73298501968384 	 14.437382221221924 	 0.0018928050994873047 	 0.1647815704345703 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:16:03.105678 test begin: paddle.index_fill(Tensor([10, 254017, 10],"int64"), Tensor([5],"int32"), 1, -1, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 254017, 10],"int64"), Tensor([5],"int32"), 1, -1, ) 	 25401705 	 29793 	 19.64960241317749 	 9.457916975021362 	 0.04484677314758301 	 0.1076667308807373 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 13:16:53.018763 test begin: paddle.index_fill(Tensor([10, 508033, 10],"bool"), Tensor([5],"int32"), 1, True, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 508033, 10],"bool"), Tensor([5],"int32"), 1, True, ) 	 50803305 	 29793 	 10.160955905914307 	 2.608164072036743 	 0.0001533031463623047 	 0.029783248901367188 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 13:17:15.988167 test begin: paddle.index_fill(Tensor([169345, 15, 10],"int64"), Tensor([5],"int32"), 1, -1, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([169345, 15, 10],"int64"), Tensor([5],"int32"), 1, -1, ) 	 25401755 	 29793 	 32.67212772369385 	 14.861533164978027 	 0.06976747512817383 	 0.16979718208312988 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 13:18:39.132105 test begin: paddle.index_fill(Tensor([338689, 15, 10],"bool"), Tensor([5],"int32"), 1, True, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([338689, 15, 10],"bool"), Tensor([5],"int32"), 1, True, ) 	 50803355 	 29793 	 32.463188886642456 	 4.8944408893585205 	 0.0009489059448242188 	 0.05587410926818848 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 13:19:49.158836 test begin: paddle.index_fill(Tensor([677377, 15, 10],"float16"), Tensor([5],"int64"), 1, 0.5, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([677377, 15, 10],"float16"), Tensor([5],"int64"), 1, 0.5, ) 	 101606555 	 29793 	 72.99674201011658 	 15.582552433013916 	 0.0022995471954345703 	 0.17804598808288574 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:22:33.240008 test begin: paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, ) 	 25628144 	 28767 	 10.560489177703857 	 12.949359893798828 	 0.022016286849975586 	 0.012166500091552734 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:23:08.575371 test begin: paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), False, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), False, ) 	 25628144 	 28767 	 10.289613246917725 	 9.460721254348755 	 0.02609848976135254 	 0.08388042449951172 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:23:42.321802 test begin: paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, ) 	 25628144 	 28767 	 10.289692163467407 	 12.632401943206787 	 0.026097536087036133 	 0.013064384460449219 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:24:16.458995 test begin: paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, ) 	 25541904 	 28767 	 10.333290576934814 	 14.57393479347229 	 0.021555423736572266 	 0.010348081588745117 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:24:55.042458 test begin: paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), False, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), False, ) 	 25541904 	 28767 	 10.056288480758667 	 9.430644750595093 	 0.02550792694091797 	 0.08359026908874512 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:25:26.136564 test begin: paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, ) 	 25541904 	 28767 	 10.056861639022827 	 12.610951900482178 	 0.02550673484802246 	 0.013040542602539062 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:26:00.232970 test begin: paddle.index_put(Tensor([193, 42, 56, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([193, 42, 56, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, ) 	 25435280 	 28767 	 10.281995296478271 	 12.858210802078247 	 0.021440505981445312 	 0.01208806037902832 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:26:34.958245 test begin: paddle.index_put(Tensor([193, 42, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([193, 42, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, ) 	 25435280 	 28767 	 10.006831407546997 	 12.55870532989502 	 0.025379657745361328 	 0.012892007827758789 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:27:08.982148 test begin: paddle.index_sample(Tensor([1865664, 100],"float32"), Tensor([1865664, 14],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([1865664, 100],"float32"), Tensor([1865664, 14],"int64"), ) 	 212685696 	 83783 	 62.71258234977722 	 95.31931018829346 	 0.7649738788604736 	 0.3879275321960449 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:32:01.522699 test begin: paddle.index_sample(Tensor([1865664, 28],"float32"), Tensor([1865664, 14],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([1865664, 28],"float32"), Tensor([1865664, 14],"int64"), ) 	 78357888 	 83783 	 43.8825523853302 	 67.57417249679565 	 0.5353071689605713 	 0.275007963180542 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:34:49.989170 test begin: paddle.index_sample(Tensor([1865664, 28],"float32"), Tensor([1865664, 1],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([1865664, 28],"float32"), Tensor([1865664, 1],"int64"), ) 	 54104256 	 83783 	 34.71567153930664 	 12.9049813747406 	 0.4234650135040283 	 0.0786736011505127 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:36:18.760685 test begin: paddle.index_sample(Tensor([25401601, 100],"float32"), Tensor([25401601, 1],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f76c7a65240>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 13:46:28.786530 test begin: paddle.index_sample(Tensor([25401601, 20],"float32"), Tensor([25401601, 1],"int64"), )
W0804 13:46:41.847538 134037 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0ac4d4ad70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 13:56:33.568345 test begin: paddle.index_sample(Tensor([2540161, 20],"float32"), Tensor([2540161, 1],"int64"), )
W0804 13:56:34.873770 134448 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([2540161, 20],"float32"), Tensor([2540161, 1],"int64"), ) 	 53343381 	 83783 	 45.01997447013855 	 16.612874746322632 	 0.549126148223877 	 0.10128378868103027 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:58:28.500241 test begin: paddle.index_sample(Tensor([508033, 100],"float32"), Tensor([508033, 1],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([508033, 100],"float32"), Tensor([508033, 1],"int64"), ) 	 51311333 	 83783 	 9.990783214569092 	 4.414872407913208 	 0.1218411922454834 	 0.026920080184936523 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:59:08.528682 test begin: paddle.index_sample(Tensor([5135296, 10],"float32"), Tensor([5135296, 1],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([5135296, 10],"float32"), Tensor([5135296, 1],"int64"), ) 	 56488256 	 83783 	 80.19311571121216 	 26.563544988632202 	 0.9782342910766602 	 0.16190457344055176 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 14:02:10.595333 test begin: paddle.index_sample(Tensor([5135296, 10],"float32"), Tensor([5135296, 5],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([5135296, 10],"float32"), Tensor([5135296, 5],"int64"), ) 	 77029440 	 83783 	 90.25391268730164 	 72.15576505661011 	 1.684140920639038 	 0.29359889030456543 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 14:06:12.070913 test begin: paddle.index_sample(Tensor([5135296, 20],"float32"), Tensor([5135296, 5],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([5135296, 20],"float32"), Tensor([5135296, 5],"int64"), ) 	 128382400 	 83783 	 93.70282673835754 	 82.90408706665039 	 1.143059492111206 	 0.33730578422546387 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 14:10:46.935341 test begin: paddle.index_sample(Tensor([932832, 100],"float32"), Tensor([932832, 28],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([932832, 100],"float32"), Tensor([932832, 28],"int64"), ) 	 119402496 	 83783 	 42.42386078834534 	 73.66590619087219 	 0.517498254776001 	 0.2998325824737549 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 14:14:06.721869 test begin: paddle.index_sample(Tensor([932832, 55],"float32"), Tensor([932832, 1],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([932832, 55],"float32"), Tensor([932832, 1],"int64"), ) 	 52238592 	 83783 	 17.795177698135376 	 8.21605920791626 	 0.21706247329711914 	 0.04477238655090332 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 14:15:02.259872 test begin: paddle.index_sample(Tensor([932832, 55],"float32"), Tensor([932832, 28],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([932832, 55],"float32"), Tensor([932832, 28],"int64"), ) 	 77425056 	 83783 	 32.60313677787781 	 65.13367938995361 	 0.39766550064086914 	 0.2650728225708008 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 14:17:35.126253 test begin: paddle.index_select(Tensor([16, 11109, 286],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([16, 11109, 286],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50834864 	 49358 	 10.59439754486084 	 10.861242055892944 	 0.20486927032470703 	 0.22489547729492188 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 14:18:26.856897 test begin: paddle.index_select(Tensor([16, 12096, 263],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([16, 12096, 263],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50900048 	 49358 	 10.513570547103882 	 11.399982929229736 	 0.21772265434265137 	 0.2360093593597412 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 14:19:17.268953 test begin: paddle.index_select(Tensor([16, 39201, 81],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([16, 39201, 81],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50804576 	 49358 	 31.180105686187744 	 27.534054279327393 	 0.6456317901611328 	 0.5701932907104492 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 14:20:54.820839 test begin: paddle.index_select(Tensor([205, 3060, 81],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([205, 3060, 81],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50811380 	 49358 	 31.141649961471558 	 27.483345985412598 	 0.6449196338653564 	 0.5690550804138184 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 14:22:32.972538 test begin: paddle.index_select(Tensor([52, 12096, 81],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([52, 12096, 81],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50948432 	 49358 	 31.277734994888306 	 27.69927954673767 	 0.6476023197174072 	 0.5735025405883789 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 14:24:15.547696 test begin: paddle.index_select(Tensor([57, 11109, 81],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([57, 11109, 81],"float32"), Tensor([80],"int64"), axis=-1, ) 	 51290333 	 49358 	 31.427217960357666 	 27.806087970733643 	 0.6506557464599609 	 0.5756876468658447 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 14:25:56.581932 test begin: paddle.index_select(Tensor([64, 3060, 260],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([64, 3060, 260],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50918480 	 49358 	 10.554271697998047 	 11.721551895141602 	 0.21851587295532227 	 0.23343491554260254 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 14:26:49.111067 test begin: paddle.index_select(Tensor([64, 9801, 81],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([64, 9801, 81],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50808464 	 49358 	 31.180147171020508 	 27.611518383026123 	 0.6455531120300293 	 0.5717873573303223 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 14:28:27.977840 test begin: paddle.inner(Tensor([20, 1270081],"float64"), Tensor([1270081],"float64"), )
[Prof] paddle.inner 	 paddle.inner(Tensor([20, 1270081],"float64"), Tensor([1270081],"float64"), ) 	 26671701 	 62291 	 10.300674438476562 	 10.176515817642212 	 0.08451557159423828 	 0.08348441123962402 	 22.279785633087158 	 22.695428133010864 	 0.121795654296875 	 0.124053955078125 	 
2025-08-04 14:29:34.221705 test begin: paddle.inner(Tensor([20, 25401601],"float64"), Tensor([25401601],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f45da823d60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 14:39:45.968620 test begin: paddle.inner(Tensor([508033, 50],"float64"), Tensor([50],"float64"), )
W0804 14:39:46.627141 147786 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.inner 	 paddle.inner(Tensor([508033, 50],"float64"), Tensor([50],"float64"), ) 	 25401700 	 62291 	 9.840016841888428 	 9.892009258270264 	 0.16144323348999023 	 0.16233539581298828 	 23.57798457145691 	 23.587190866470337 	 0.12871122360229492 	 0.12876272201538086 	 
2025-08-04 14:40:54.547814 test begin: paddle.is_complex(Tensor([1003520, 507],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([1003520, 507],"float32"), ) 	 508784640 	 2834372 	 10.204217195510864 	 5.44786524772644 	 0.0001049041748046875 	 0.00030303001403808594 	 None 	 None 	 None 	 None 	 
2025-08-04 14:41:19.123932 test begin: paddle.is_complex(Tensor([5070, 100352],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([5070, 100352],"float32"), ) 	 508784640 	 2834372 	 10.62618899345398 	 5.214014530181885 	 0.0001068115234375 	 0.0003001689910888672 	 None 	 None 	 None 	 None 	 
2025-08-04 14:41:43.799556 test begin: paddle.is_complex(Tensor([62020, 8192],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([62020, 8192],"float32"), ) 	 508067840 	 2834372 	 10.442600727081299 	 5.370784759521484 	 0.00010204315185546875 	 0.00028586387634277344 	 None 	 None 	 None 	 None 	 
2025-08-04 14:42:08.371600 test begin: paddle.is_complex(Tensor([81920, 6202],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([81920, 6202],"float32"), ) 	 508067840 	 2834372 	 10.68062448501587 	 6.099993944168091 	 0.00010538101196289062 	 0.00029921531677246094 	 None 	 None 	 None 	 None 	 
2025-08-04 14:42:34.082322 test begin: paddle.is_complex(Tensor([8860, 57344],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([8860, 57344],"float32"), ) 	 508067840 	 2834372 	 10.44359302520752 	 5.3321592807769775 	 0.00010514259338378906 	 0.0002903938293457031 	 None 	 None 	 None 	 None 	 
2025-08-04 14:42:58.512653 test begin: paddle.is_empty(Tensor([101606410, 5],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(Tensor([101606410, 5],"float32"), ) 	 508032050 	 2953527 	 11.36268401145935 	 4.945016145706177 	 0.00011491775512695312 	 0.0002777576446533203 	 None 	 None 	 None 	 None 	 combined
2025-08-04 14:43:23.651973 test begin: paddle.is_empty(Tensor([169344010, 3],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(Tensor([169344010, 3],"float32"), ) 	 508032030 	 2953527 	 11.267499685287476 	 4.928982496261597 	 0.00014901161193847656 	 0.0002117156982421875 	 None 	 None 	 None 	 None 	 combined
2025-08-04 14:43:48.711288 test begin: paddle.is_empty(Tensor([20, 25401601],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(Tensor([20, 25401601],"float32"), ) 	 508032020 	 2953527 	 11.205691576004028 	 4.805969476699829 	 0.00011301040649414062 	 0.0002620220184326172 	 None 	 None 	 None 	 None 	 combined
2025-08-04 14:44:13.465038 test begin: paddle.is_empty(Tensor([30, 16934401],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(Tensor([30, 16934401],"float32"), ) 	 508032030 	 2953527 	 11.197946786880493 	 4.726471424102783 	 0.00011420249938964844 	 0.0002684593200683594 	 None 	 None 	 None 	 None 	 combined
2025-08-04 14:44:38.991874 test begin: paddle.is_empty(x=Tensor([40, 32, 396901],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(x=Tensor([40, 32, 396901],"float32"), ) 	 508033280 	 2953527 	 12.029578924179077 	 6.274766683578491 	 0.00013947486877441406 	 0.0003075599670410156 	 None 	 None 	 None 	 None 	 combined
2025-08-04 14:45:06.557981 test begin: paddle.is_empty(x=Tensor([40, 396901, 32],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(x=Tensor([40, 396901, 32],"float32"), ) 	 508033280 	 2953527 	 11.72248125076294 	 4.931678056716919 	 0.00013113021850585938 	 0.0002880096435546875 	 None 	 None 	 None 	 None 	 combined
2025-08-04 14:45:31.971841 test begin: paddle.is_empty(x=Tensor([496130, 32, 32],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(x=Tensor([496130, 32, 32],"float32"), ) 	 508037120 	 2953527 	 17.43608283996582 	 4.8034584522247314 	 0.0001392364501953125 	 0.0001404285430908203 	 None 	 None 	 None 	 None 	 combined
2025-08-04 14:46:02.972476 test begin: paddle.isclose(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), rtol=1e-05, atol=1e-08, )
[Prof] paddle.isclose 	 paddle.isclose(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), rtol=1e-05, atol=1e-08, ) 	 50803220 	 27502 	 9.906193971633911 	 84.73997974395752 	 0.3680713176727295 	 0.24178123474121094 	 None 	 None 	 None 	 None 	 
2025-08-04 14:47:41.608912 test begin: paddle.isclose(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), rtol=1e-05, atol=1e-08, )
[Prof] paddle.isclose 	 paddle.isclose(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), rtol=1e-05, atol=1e-08, ) 	 50803220 	 27502 	 9.90715765953064 	 84.73476076126099 	 0.36806821823120117 	 0.24178028106689453 	 None 	 None 	 None 	 None 	 
2025-08-04 14:49:17.397073 test begin: paddle.isclose(x=Tensor([1270081, 4, 5],"float64"), y=Tensor([1270081, 4, 5],"float64"), )
[Prof] paddle.isclose 	 paddle.isclose(x=Tensor([1270081, 4, 5],"float64"), y=Tensor([1270081, 4, 5],"float64"), ) 	 50803240 	 27502 	 9.902164459228516 	 84.73724222183228 	 0.36788439750671387 	 0.24177885055541992 	 None 	 None 	 None 	 None 	 
2025-08-04 14:50:54.223500 test begin: paddle.isclose(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), )
[Prof] paddle.isclose 	 paddle.isclose(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), ) 	 50803202 	 27502 	 9.908508539199829 	 84.73594307899475 	 0.3680429458618164 	 0.24171066284179688 	 None 	 None 	 None 	 None 	 
2025-08-04 14:52:30.012247 test begin: paddle.isclose(x=Tensor([3, 1693441, 5],"float64"), y=Tensor([3, 1693441, 5],"float64"), )
[Prof] paddle.isclose 	 paddle.isclose(x=Tensor([3, 1693441, 5],"float64"), y=Tensor([3, 1693441, 5],"float64"), ) 	 50803230 	 27502 	 9.905812978744507 	 84.73440170288086 	 0.3680570125579834 	 0.2417440414428711 	 None 	 None 	 None 	 None 	 
2025-08-04 14:54:06.471909 test begin: paddle.isclose(x=Tensor([3, 4, 2116801],"float64"), y=Tensor([3, 4, 2116801],"float64"), )
[Prof] paddle.isclose 	 paddle.isclose(x=Tensor([3, 4, 2116801],"float64"), y=Tensor([3, 4, 2116801],"float64"), ) 	 50803224 	 27502 	 9.902438640594482 	 84.73533868789673 	 0.36794042587280273 	 0.2417430877685547 	 None 	 None 	 None 	 None 	 
2025-08-04 14:55:42.682244 test begin: paddle.isfinite(Tensor([1738, 94, 311],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([1738, 94, 311],"float32"), ) 	 50808692 	 42769 	 9.989120960235596 	 33.709147453308105 	 0.23868370056152344 	 0.2013111114501953 	 None 	 None 	 None 	 None 	 
2025-08-04 14:56:29.468674 test begin: paddle.isfinite(Tensor([28462, 17, 5, 6, 7],"float16"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([28462, 17, 5, 6, 7],"float16"), ) 	 101609340 	 42769 	 16.818917989730835 	 41.55891823768616 	 0.4019742012023926 	 0.24826955795288086 	 None 	 None 	 None 	 None 	 
2025-08-04 14:57:32.918115 test begin: paddle.isfinite(Tensor([4, 280, 376, 25, 5],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 280, 376, 25, 5],"float32"), ) 	 52640000 	 42769 	 10.339256048202515 	 34.82217860221863 	 0.24707245826721191 	 0.20782184600830078 	 None 	 None 	 None 	 None 	 
2025-08-04 14:58:19.004467 test begin: paddle.isfinite(Tensor([4, 280, 376, 41, 3],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 280, 376, 41, 3],"float32"), ) 	 51797760 	 42769 	 10.171372890472412 	 34.32158660888672 	 0.24304819107055664 	 0.20493793487548828 	 None 	 None 	 None 	 None 	 
2025-08-04 14:59:04.357991 test begin: paddle.isfinite(Tensor([4, 280, 605, 25, 3],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 280, 605, 25, 3],"float32"), ) 	 50820000 	 42769 	 10.005604982376099 	 33.71474504470825 	 0.23908019065856934 	 0.20130252838134766 	 None 	 None 	 None 	 None 	 
2025-08-04 14:59:48.959516 test begin: paddle.isfinite(Tensor([4, 40839, 311],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 40839, 311],"float32"), ) 	 50803716 	 42769 	 9.99164891242981 	 33.64664649963379 	 0.23874640464782715 	 0.2008037567138672 	 None 	 None 	 None 	 None 	 
2025-08-04 15:00:33.479494 test begin: paddle.isfinite(Tensor([4, 451, 376, 25, 3],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 451, 376, 25, 3],"float32"), ) 	 50872800 	 42769 	 10.022367715835571 	 33.68862247467041 	 0.23950695991516113 	 0.20113635063171387 	 None 	 None 	 None 	 None 	 
2025-08-04 15:01:18.062802 test begin: paddle.isfinite(Tensor([4, 94, 135115],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 94, 135115],"float32"), ) 	 50803240 	 42769 	 10.005710363388062 	 33.66267442703247 	 0.23908472061157227 	 0.20097684860229492 	 None 	 None 	 None 	 None 	 
2025-08-04 15:02:02.614656 test begin: paddle.isfinite(Tensor([7, 280, 376, 25, 3],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([7, 280, 376, 25, 3],"float32"), ) 	 55272000 	 42769 	 10.864266633987427 	 36.51527667045593 	 0.25963664054870605 	 0.21802377700805664 	 None 	 None 	 None 	 None 	 
2025-08-04 15:02:52.673331 test begin: paddle.isfinite(Tensor([8, 17, 17789, 6, 7],"float16"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([8, 17, 17789, 6, 7],"float16"), ) 	 101610768 	 42769 	 16.82770848274231 	 41.48632192611694 	 0.40213847160339355 	 0.24778294563293457 	 None 	 None 	 None 	 None 	 
2025-08-04 15:03:52.944994 test begin: paddle.isfinite(Tensor([8, 17, 5, 21346, 7],"float16"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([8, 17, 5, 21346, 7],"float16"), ) 	 101606960 	 42769 	 16.82290005683899 	 41.515286684036255 	 0.4020538330078125 	 0.24792933464050293 	 None 	 None 	 None 	 None 	 
2025-08-04 15:04:53.228022 test begin: paddle.isfinite(Tensor([8, 17, 5, 6, 24904],"float16"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([8, 17, 5, 6, 24904],"float16"), ) 	 101608320 	 42769 	 16.818185091018677 	 41.56327843666077 	 0.40194249153137207 	 0.2482590675354004 	 None 	 None 	 None 	 None 	 
2025-08-04 15:05:53.707914 test begin: paddle.isfinite(Tensor([8, 60481, 5, 6, 7],"float16"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([8, 60481, 5, 6, 7],"float16"), ) 	 101608080 	 42769 	 16.839093446731567 	 41.5377299785614 	 0.4024183750152588 	 0.24808859825134277 	 None 	 None 	 None 	 None 	 
2025-08-04 15:06:54.191884 test begin: paddle.isin(Tensor([396901, 64],"float64"), Tensor([4, 256],"float64"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([396901, 64],"float64"), Tensor([4, 256],"float64"), False, False, ) 	 25402688 	 3671 	 9.958699703216553 	 78.04544234275818 	 0.002484560012817383 	 0.0008726119995117188 	 None 	 None 	 None 	 None 	 
2025-08-04 15:08:22.911878 test begin: paddle.isin(Tensor([793801, 64],"float32"), Tensor([4, 256],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([793801, 64],"float32"), Tensor([4, 256],"float32"), False, False, ) 	 50804288 	 3671 	 16.30861806869507 	 92.63323879241943 	 0.004169464111328125 	 0.0020360946655273438 	 None 	 None 	 None 	 None 	 
2025-08-04 15:10:14.087720 test begin: paddle.isin(Tensor([793801, 64],"float32"), Tensor([4, 256],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([793801, 64],"float32"), Tensor([4, 256],"float32"), False, True, ) 	 50804288 	 3671 	 16.634811639785767 	 92.67527723312378 	 0.004250049591064453 	 0.002038240432739258 	 None 	 None 	 None 	 None 	 
2025-08-04 15:12:04.305649 test begin: paddle.isin(Tensor([793801, 64],"float32"), Tensor([793801, 256],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([793801, 64],"float32"), Tensor([793801, 256],"float32"), False, False, ) 	 254016320 	 3671 	 351.77910709381104 	 165.8886170387268 	 0.052568912506103516 	 0.0026068687438964844 	 None 	 None 	 None 	 None 	 
2025-08-04 15:20:46.342366 test begin: paddle.isin(Tensor([793801, 64],"float32"), Tensor([793801, 256],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([793801, 64],"float32"), Tensor([793801, 256],"float32"), False, True, ) 	 254016320 	 3671 	 347.71339559555054 	 165.85578870773315 	 0.05250954627990723 	 0.002587556838989258 	 None 	 None 	 None 	 None 	 
2025-08-04 15:29:27.840933 test begin: paddle.isin(Tensor([8, 3175201],"float64"), Tensor([4, 256],"float64"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 3175201],"float64"), Tensor([4, 256],"float64"), False, False, ) 	 25402632 	 3671 	 10.011432647705078 	 78.09529638290405 	 0.002474546432495117 	 0.0008571147918701172 	 None 	 None 	 None 	 None 	 
2025-08-04 15:30:57.292405 test begin: paddle.isin(Tensor([8, 3175201],"float64"), Tensor([4, 3175201],"float64"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 3175201],"float64"), Tensor([4, 3175201],"float64"), False, False, ) 	 38102412 	 3671 	 108.00927782058716 	 99.70096230506897 	 0.017457962036132812 	 0.0009806156158447266 	 None 	 None 	 None 	 None 	 
2025-08-04 15:34:25.940576 test begin: paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 256],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 256],"float32"), False, False, ) 	 50804232 	 3671 	 16.284785270690918 	 92.67929196357727 	 0.0042133331298828125 	 0.002044200897216797 	 None 	 None 	 None 	 None 	 
2025-08-04 15:36:15.835903 test begin: paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 256],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 256],"float32"), False, True, ) 	 50804232 	 3671 	 16.66456961631775 	 92.73207807540894 	 0.004235506057739258 	 0.002044677734375 	 None 	 None 	 None 	 None 	 
2025-08-04 15:38:06.246228 test begin: paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 6350401],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 6350401],"float32"), False, False, ) 	 76204812 	 3671 	 196.27514743804932 	 110.00119161605835 	 0.03573346138000488 	 0.002202749252319336 	 None 	 None 	 None 	 None 	 
2025-08-04 15:43:13.898339 test begin: paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 6350401],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 6350401],"float32"), False, True, ) 	 76204812 	 3671 	 182.95168614387512 	 109.95432209968567 	 0.035750389099121094 	 0.0022125244140625 	 None 	 None 	 None 	 None 	 
2025-08-04 15:48:08.327009 test begin: paddle.isin(Tensor([8, 64],"float32"), Tensor([198451, 256],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float32"), Tensor([198451, 256],"float32"), False, False, ) 	 50803968 	 3671 	 71.06197237968445 	 30.406065940856934 	 5.173683166503906e-05 	 0.00027942657470703125 	 None 	 None 	 None 	 None 	 
2025-08-04 15:49:50.763624 test begin: paddle.isin(Tensor([8, 64],"float32"), Tensor([198451, 256],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float32"), Tensor([198451, 256],"float32"), False, True, ) 	 50803968 	 3671 	 72.34437537193298 	 30.436943292617798 	 9.131431579589844e-05 	 0.00027251243591308594 	 None 	 None 	 None 	 None 	 
2025-08-04 15:51:34.490055 test begin: paddle.isin(Tensor([8, 64],"float32"), Tensor([4, 12700801],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float32"), Tensor([4, 12700801],"float32"), False, False, ) 	 50803716 	 3671 	 74.26245856285095 	 30.423367500305176 	 5.078315734863281e-05 	 0.00028586387634277344 	 None 	 None 	 None 	 None 	 
2025-08-04 15:53:21.462841 test begin: paddle.isin(Tensor([8, 64],"float32"), Tensor([4, 12700801],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float32"), Tensor([4, 12700801],"float32"), False, True, ) 	 50803716 	 3671 	 70.51556277275085 	 30.45178723335266 	 4.9114227294921875e-05 	 0.00028014183044433594 	 None 	 None 	 None 	 None 	 
2025-08-04 15:55:03.351976 test begin: paddle.isin(Tensor([8, 64],"float64"), Tensor([4, 6350401],"float64"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float64"), Tensor([4, 6350401],"float64"), False, False, ) 	 25402116 	 3671 	 47.15497183799744 	 43.94806265830994 	 5.030632019042969e-05 	 0.00024700164794921875 	 None 	 None 	 None 	 None 	 
2025-08-04 15:56:35.093154 test begin: paddle.isin(Tensor([8, 64],"float64"), Tensor([99226, 256],"float64"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float64"), Tensor([99226, 256],"float64"), False, False, ) 	 25402368 	 3671 	 47.57094764709473 	 44.0745689868927 	 5.269050598144531e-05 	 0.00023865699768066406 	 None 	 None 	 None 	 None 	 
2025-08-04 15:58:08.660634 test begin: paddle.isinf(Tensor([14, 226801, 16],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([14, 226801, 16],"float32"), ) 	 50803424 	 42950 	 10.023922681808472 	 20.848896026611328 	 0.2385704517364502 	 0.24802613258361816 	 None 	 None 	 None 	 None 	 
2025-08-04 15:58:41.359524 test begin: paddle.isinf(Tensor([14, 36655, 99],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([14, 36655, 99],"float32"), ) 	 50803830 	 42950 	 10.031010866165161 	 20.84890651702881 	 0.23872923851013184 	 0.24804425239562988 	 None 	 None 	 None 	 None 	 
2025-08-04 15:59:13.111376 test begin: paddle.isinf(Tensor([14, 64, 56701],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([14, 64, 56701],"float32"), ) 	 50804096 	 42950 	 10.038496732711792 	 20.84924602508545 	 0.23888611793518066 	 0.24800515174865723 	 None 	 None 	 None 	 None 	 
2025-08-04 15:59:44.886388 test begin: paddle.isinf(Tensor([14, 7, 518401],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([14, 7, 518401],"float32"), ) 	 50803298 	 42950 	 10.040771007537842 	 20.848464727401733 	 0.23899507522583008 	 0.24804115295410156 	 None 	 None 	 None 	 None 	 
2025-08-04 16:00:16.690251 test begin: paddle.isinf(Tensor([28462, 17, 5, 6, 7],"float16"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([28462, 17, 5, 6, 7],"float16"), ) 	 101609340 	 42950 	 16.67781138420105 	 22.589476108551025 	 0.39683985710144043 	 0.2659733295440674 	 None 	 None 	 None 	 None 	 
2025-08-04 16:01:00.493478 test begin: paddle.isinf(Tensor([49613, 64, 16],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([49613, 64, 16],"float32"), ) 	 50803712 	 42950 	 9.982587814331055 	 20.84863567352295 	 0.23759937286376953 	 0.24805665016174316 	 None 	 None 	 None 	 None 	 
2025-08-04 16:01:32.207948 test begin: paddle.isinf(Tensor([73310, 7, 99],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([73310, 7, 99],"float32"), ) 	 50803830 	 42950 	 10.031427383422852 	 20.8487069606781 	 0.23872637748718262 	 0.24811506271362305 	 None 	 None 	 None 	 None 	 
2025-08-04 16:02:03.981126 test begin: paddle.isinf(Tensor([8, 17, 17789, 6, 7],"float16"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([8, 17, 17789, 6, 7],"float16"), ) 	 101610768 	 42950 	 16.688092947006226 	 22.35260534286499 	 0.39711952209472656 	 0.26599717140197754 	 None 	 None 	 None 	 None 	 
2025-08-04 16:02:44.965220 test begin: paddle.isinf(Tensor([8, 17, 5, 21346, 7],"float16"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([8, 17, 5, 21346, 7],"float16"), ) 	 101606960 	 42950 	 16.67061138153076 	 22.351188898086548 	 0.39668750762939453 	 0.26594114303588867 	 None 	 None 	 None 	 None 	 
2025-08-04 16:03:25.895138 test begin: paddle.isinf(Tensor([8, 17, 5, 6, 24904],"float16"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([8, 17, 5, 6, 24904],"float16"), ) 	 101608320 	 42950 	 16.684221744537354 	 22.36486053466797 	 0.39701008796691895 	 0.26718735694885254 	 None 	 None 	 None 	 None 	 
2025-08-04 16:04:06.917544 test begin: paddle.isinf(Tensor([8, 60481, 5, 6, 7],"float16"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([8, 60481, 5, 6, 7],"float16"), ) 	 101608080 	 42950 	 16.68824052810669 	 22.351805925369263 	 0.3971400260925293 	 0.26598262786865234 	 None 	 None 	 None 	 None 	 
2025-08-04 16:04:47.858982 test begin: paddle.isnan(Tensor([10445, 4864],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([10445, 4864],"float32"), ) 	 50804480 	 42698 	 9.962608814239502 	 7.937100887298584 	 0.2384943962097168 	 0.18996620178222656 	 None 	 None 	 None 	 None 	 
2025-08-04 16:05:06.608859 test begin: paddle.isnan(Tensor([16, 64, 320, 320],"float16"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([16, 64, 320, 320],"float16"), ) 	 104857600 	 42698 	 16.983147144317627 	 9.892745971679688 	 0.4065887928009033 	 0.23683667182922363 	 None 	 None 	 None 	 None 	 
2025-08-04 16:05:38.131927 test begin: paddle.isnan(Tensor([4, 125, 320, 320],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 125, 320, 320],"float32"), ) 	 51200000 	 42698 	 10.012990474700928 	 8.000048398971558 	 0.23968219757080078 	 0.1913619041442871 	 None 	 None 	 None 	 None 	 
2025-08-04 16:05:58.917315 test begin: paddle.isnan(Tensor([4, 249, 320, 320],"float16"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 249, 320, 320],"float16"), ) 	 101990400 	 42698 	 16.49871301651001 	 9.62891674041748 	 0.3948967456817627 	 0.23047733306884766 	 None 	 None 	 None 	 None 	 
2025-08-04 16:06:26.971414 test begin: paddle.isnan(Tensor([4, 64, 1241, 320],"float16"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 64, 1241, 320],"float16"), ) 	 101662720 	 42698 	 16.440386056900024 	 9.599032878875732 	 0.3934774398803711 	 0.22973871231079102 	 None 	 None 	 None 	 None 	 
2025-08-04 16:06:54.931957 test begin: paddle.isnan(Tensor([4, 64, 320, 1241],"float16"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 64, 320, 1241],"float16"), ) 	 101662720 	 42698 	 16.440306425094604 	 9.599225282669067 	 0.39352917671203613 	 0.22974586486816406 	 None 	 None 	 None 	 None 	 
2025-08-04 16:07:22.912840 test begin: paddle.isnan(Tensor([4, 64, 320, 621],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 64, 320, 621],"float32"), ) 	 50872320 	 42698 	 9.969894409179688 	 7.951011896133423 	 0.23863768577575684 	 0.19020438194274902 	 None 	 None 	 None 	 None 	 
2025-08-04 16:07:43.689140 test begin: paddle.isnan(Tensor([4, 64, 621, 320],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 64, 621, 320],"float32"), ) 	 50872320 	 42698 	 9.969712972640991 	 7.947373390197754 	 0.23862695693969727 	 0.19023370742797852 	 None 	 None 	 None 	 None 	 
2025-08-04 16:08:02.463314 test begin: paddle.isnan(Tensor([4864, 10445],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4864, 10445],"float32"), ) 	 50804480 	 42698 	 9.962449312210083 	 7.945340394973755 	 0.23846721649169922 	 0.18998003005981445 	 None 	 None 	 None 	 None 	 
2025-08-04 16:08:21.248072 test begin: paddle.isnan(Tensor([8, 64, 320, 320],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([8, 64, 320, 320],"float32"), ) 	 52428800 	 42698 	 10.263822317123413 	 8.178282976150513 	 0.24568510055541992 	 0.19577479362487793 	 None 	 None 	 None 	 None 	 
2025-08-04 16:08:42.687946 test begin: paddle.isneginf(Tensor([11, 17, 2716],"int32"), )
[Prof] paddle.isneginf 	 paddle.isneginf(Tensor([11, 17, 2716],"int32"), ) 	 507892 	 1000 	 22.233261823654175 	 0.010918140411376953 	 4.506111145019531e-05 	 2.5510787963867188e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:09:05.011497 test begin: paddle.isneginf(Tensor([11, 17, 5433],"int16"), )
[Prof] paddle.isneginf 	 paddle.isneginf(Tensor([11, 17, 5433],"int16"), ) 	 1015971 	 1000 	 46.35922122001648 	 0.011037349700927734 	 4.76837158203125e-05 	 4.2438507080078125e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:09:51.442473 test begin: paddle.isneginf(Tensor([11, 4618, 10],"int32"), )
[Prof] paddle.isneginf 	 paddle.isneginf(Tensor([11, 4618, 10],"int32"), ) 	 507980 	 1000 	 20.01374888420105 	 0.010897159576416016 	 5.030632019042969e-05 	 3.0279159545898438e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:10:11.497752 test begin: paddle.isneginf(Tensor([11, 46184],"float32"), )
[Prof] paddle.isneginf 	 paddle.isneginf(Tensor([11, 46184],"float32"), ) 	 508024 	 1000 	 20.096771478652954 	 0.010190010070800781 	 4.649162292480469e-05 	 3.361701965332031e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:10:32.840856 test begin: paddle.isneginf(Tensor([11, 9236, 10],"int16"), )
[Prof] paddle.isneginf 	 paddle.isneginf(Tensor([11, 9236, 10],"int16"), ) 	 1015960 	 1000 	 46.408923625946045 	 0.011183500289916992 	 4.5299530029296875e-05 	 4.291534423828125e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:11:19.319048 test begin: paddle.isneginf(Tensor([2988, 17, 10],"int32"), )
[Prof] paddle.isneginf 	 paddle.isneginf(Tensor([2988, 17, 10],"int32"), ) 	 507960 	 1000 	 20.17255163192749 	 0.010807991027832031 	 4.76837158203125e-05 	 2.5987625122070312e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:11:39.559255 test begin: paddle.isneginf(Tensor([29884, 17],"float32"), )
[Prof] paddle.isneginf 	 paddle.isneginf(Tensor([29884, 17],"float32"), ) 	 508028 	 1000 	 20.063011407852173 	 0.010273933410644531 	 4.6253204345703125e-05 	 4.5299530029296875e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:11:59.662514 test begin: paddle.isneginf(Tensor([5976, 17, 10],"int16"), )
[Prof] paddle.isneginf 	 paddle.isneginf(Tensor([5976, 17, 10],"int16"), ) 	 1015920 	 1000 	 46.77818036079407 	 0.01377415657043457 	 4.9114227294921875e-05 	 4.315376281738281e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:12:46.517876 test begin: paddle.isposinf(Tensor([11, 17, 2716],"int32"), )
[Prof] paddle.isposinf 	 paddle.isposinf(Tensor([11, 17, 2716],"int32"), ) 	 507892 	 1000 	 19.94659924507141 	 0.010837554931640625 	 4.649162292480469e-05 	 2.956390380859375e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:13:06.510058 test begin: paddle.isposinf(Tensor([11, 17, 5433],"int16"), )
[Prof] paddle.isposinf 	 paddle.isposinf(Tensor([11, 17, 5433],"int16"), ) 	 1015971 	 1000 	 39.898839235305786 	 0.01112985610961914 	 5.2928924560546875e-05 	 3.647804260253906e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:13:46.472814 test begin: paddle.isposinf(Tensor([11, 4618, 10],"int32"), )
[Prof] paddle.isposinf 	 paddle.isposinf(Tensor([11, 4618, 10],"int32"), ) 	 507980 	 1000 	 19.863059997558594 	 0.01092529296875 	 4.982948303222656e-05 	 2.86102294921875e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:14:06.669530 test begin: paddle.isposinf(Tensor([11, 46184],"float32"), )
[Prof] paddle.isposinf 	 paddle.isposinf(Tensor([11, 46184],"float32"), ) 	 508024 	 1000 	 19.883265018463135 	 0.010172128677368164 	 5.054473876953125e-05 	 3.0994415283203125e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:14:26.593915 test begin: paddle.isposinf(Tensor([11, 9236, 10],"int16"), )
[Prof] paddle.isposinf 	 paddle.isposinf(Tensor([11, 9236, 10],"int16"), ) 	 1015960 	 1000 	 39.49613308906555 	 0.011162757873535156 	 4.9591064453125e-05 	 4.220008850097656e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:15:06.152194 test begin: paddle.isposinf(Tensor([2988, 17, 10],"int32"), )
[Prof] paddle.isposinf 	 paddle.isposinf(Tensor([2988, 17, 10],"int32"), ) 	 507960 	 1000 	 19.905309438705444 	 0.010665655136108398 	 4.553794860839844e-05 	 2.8133392333984375e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:15:26.096557 test begin: paddle.isposinf(Tensor([29884, 17],"float32"), )
[Prof] paddle.isposinf 	 paddle.isposinf(Tensor([29884, 17],"float32"), ) 	 508028 	 1000 	 19.833991765975952 	 0.010164499282836914 	 4.982948303222656e-05 	 2.956390380859375e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:15:45.986554 test begin: paddle.isposinf(Tensor([5976, 17, 10],"int16"), )
[Prof] paddle.isposinf 	 paddle.isposinf(Tensor([5976, 17, 10],"int16"), ) 	 1015920 	 1000 	 39.742757081985474 	 0.01796746253967285 	 5.078315734863281e-05 	 3.981590270996094e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 16:16:25.798607 test begin: paddle.isreal(Tensor([15876010, 32],"bool"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([15876010, 32],"bool"), ) 	 508032320 	 25942 	 9.980527639389038 	 8.581591129302979 	 0.3928871154785156 	 0.337479829788208 	 None 	 None 	 None 	 None 	 
2025-08-04 16:16:51.779512 test begin: paddle.isreal(Tensor([31752010, 32],"bfloat16"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([31752010, 32],"bfloat16"), ) 	 1016064320 	 25942 	 19.85219645500183 	 17.800929069519043 	 0.7818024158477783 	 0.6719245910644531 	 None 	 None 	 None 	 None 	 
2025-08-04 16:17:50.074324 test begin: paddle.isreal(Tensor([31752010, 32],"float16"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([31752010, 32],"float16"), ) 	 1016064320 	 25942 	 19.89266347885132 	 17.3122079372406 	 0.7818822860717773 	 0.6728532314300537 	 None 	 None 	 None 	 None 	 
2025-08-04 16:18:54.335557 test begin: paddle.isreal(Tensor([640, 1587601],"bfloat16"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([640, 1587601],"bfloat16"), ) 	 1016064640 	 25942 	 19.864020586013794 	 17.069605350494385 	 0.7817330360412598 	 0.671837329864502 	 None 	 None 	 None 	 None 	 
2025-08-04 16:19:48.670617 test begin: paddle.isreal(Tensor([640, 1587601],"float16"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([640, 1587601],"float16"), ) 	 1016064640 	 25942 	 19.863383531570435 	 17.073615550994873 	 0.7822198867797852 	 0.672074556350708 	 None 	 None 	 None 	 None 	 
2025-08-04 16:20:49.154595 test begin: paddle.isreal(Tensor([640, 793801],"bool"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([640, 793801],"bool"), ) 	 508032640 	 25942 	 9.984985113143921 	 8.57811188697815 	 0.39282894134521484 	 0.33731961250305176 	 None 	 None 	 None 	 None 	 
2025-08-04 16:21:17.506788 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([22336, 5, 4, 3, 2],"float32"), )
[Prof] paddle.kron 	 paddle.kron(Tensor([10, 10],"float32"), Tensor([22336, 5, 4, 3, 2],"float32"), ) 	 2680420 	 1311 	 12.782527208328247 	 1.8096892833709717 	 9.202957153320312e-05 	 1.3959801197052002 	 19.65829849243164 	 7.858638048171997 	 6.628036499023438e-05 	 1.225414752960205 	 
2025-08-04 16:22:04.128935 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 22336, 4, 3, 2],"float32"), )
[Prof] paddle.kron 	 paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 22336, 4, 3, 2],"float32"), ) 	 2680420 	 1311 	 12.76822829246521 	 1.8082036972045898 	 0.00010204315185546875 	 1.3958861827850342 	 19.612436294555664 	 7.789032697677612 	 6.961822509765625e-05 	 1.2150263786315918 	 
2025-08-04 16:22:53.201528 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 13868, 3, 2],"float32"), )
[Prof] paddle.kron 	 paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 13868, 3, 2],"float32"), ) 	 2080300 	 1311 	 9.956613063812256 	 3.5387542247772217 	 9.918212890625e-05 	 1.0845532417297363 	 15.245650291442871 	 6.301764488220215 	 6.771087646484375e-05 	 0.9824302196502686 	 
2025-08-04 16:23:33.488264 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 4, 15401, 2],"float32"), )
[Prof] paddle.kron 	 paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 4, 15401, 2],"float32"), ) 	 3080300 	 1311 	 14.627369403839111 	 2.060349702835083 	 9.870529174804688e-05 	 1.6058552265167236 	 22.160563230514526 	 8.490760803222656 	 6.723403930664062e-05 	 1.3238883018493652 	 
2025-08-04 16:24:30.042596 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 4, 3, 8934],"float32"), )
[Prof] paddle.kron 	 paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 4, 3, 8934],"float32"), ) 	 2680300 	 1311 	 12.714076280593872 	 1.7918672561645508 	 8.988380432128906e-05 	 1.3965401649475098 	 22.599422216415405 	 6.54988694190979 	 6.937980651855469e-05 	 1.020787239074707 	 
2025-08-04 16:25:18.236414 test begin: paddle.kthvalue(Tensor([30, 200, 8468],"float32"), k=1, axis=1, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([30, 200, 8468],"float32"), k=1, axis=1, ) 	 50808000 	 3230 	 12.961170434951782 	 13.388606309890747 	 1.0231716632843018 	 4.2366461753845215 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:25:46.207558 test begin: paddle.kthvalue(Tensor([30, 200, 8468],"float32"), k=1, axis=1, keepdim=True, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([30, 200, 8468],"float32"), k=1, axis=1, keepdim=True, ) 	 50808000 	 3230 	 12.96670389175415 	 13.397876024246216 	 1.0238077640533447 	 4.239449739456177 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:26:14.170846 test begin: paddle.kthvalue(Tensor([30, 200, 8468],"float32"), k=2, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([30, 200, 8468],"float32"), k=2, ) 	 50808000 	 3230 	 10.000716924667358 	 8.056963443756104 	 3.1645328998565674 	 2.5493524074554443 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:26:33.744022 test begin: paddle.kthvalue(Tensor([30, 42337, 40],"float32"), k=1, axis=1, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([30, 42337, 40],"float32"), k=1, axis=1, ) 	 50804400 	 3230 	 14.352941036224365 	 35.99184250831604 	 1.1327714920043945 	 11.380662441253662 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:27:27.756782 test begin: paddle.kthvalue(Tensor([30, 42337, 40],"float32"), k=1, axis=1, keepdim=True, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([30, 42337, 40],"float32"), k=1, axis=1, keepdim=True, ) 	 50804400 	 3230 	 14.409284114837646 	 35.65679478645325 	 1.1380045413970947 	 11.28092622756958 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:28:21.504845 test begin: paddle.kthvalue(Tensor([30, 42337, 40],"float32"), k=2, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([30, 42337, 40],"float32"), k=2, ) 	 50804400 	 3230 	 16.91219449043274 	 17.91532850265503 	 5.351373195648193 	 5.264569997787476 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:28:59.733106 test begin: paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=1, axis=1, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=1, axis=1, ) 	 50808000 	 3230 	 12.888563871383667 	 13.405798435211182 	 1.0174109935760498 	 4.24198579788208 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:29:27.729623 test begin: paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=1, axis=1, keepdim=True, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=1, axis=1, keepdim=True, ) 	 50808000 	 3230 	 13.076863288879395 	 13.401342868804932 	 1.215562343597412 	 4.2395148277282715 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:29:57.396322 test begin: paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=2, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=2, ) 	 50808000 	 3230 	 16.9012508392334 	 16.638617277145386 	 5.347815990447998 	 5.263815879821777 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:30:35.338131 test begin: paddle.lcm(Tensor([1],"int64"), Tensor([25401601],"int64"), )
[Prof] paddle.lcm 	 paddle.lcm(Tensor([1],"int64"), Tensor([25401601],"int64"), ) 	 25401602 	 1000 	 92.37629055976868 	 5.770044565200806 	 0.0021865367889404297 	 0.0009202957153320312 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:32:17.040547 test begin: paddle.lcm(Tensor([25401601],"int64"), Tensor([1],"int64"), )
[Prof] paddle.lcm 	 paddle.lcm(Tensor([25401601],"int64"), Tensor([1],"int64"), ) 	 25401602 	 1000 	 92.40720319747925 	 5.686815977096558 	 0.0022466182708740234 	 0.0009124279022216797 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:33:56.173294 test begin: paddle.lcm(Tensor([25401601],"int64"), Tensor([25401601],"int64"), )
[Prof] paddle.lcm 	 paddle.lcm(Tensor([25401601],"int64"), Tensor([25401601],"int64"), ) 	 50803202 	 1000 	 106.62111186981201 	 5.9231603145599365 	 0.002416372299194336 	 0.0009143352508544922 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:35:50.752281 test begin: paddle.lcm(Tensor([50803201],"int32"), Tensor([1],"int32"), )
[Prof] paddle.lcm 	 paddle.lcm(Tensor([50803201],"int32"), Tensor([1],"int32"), ) 	 50803202 	 1000 	 86.41155242919922 	 7.999487638473511 	 0.0023238658905029297 	 0.0013108253479003906 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:37:26.517312 test begin: paddle.ldexp(Tensor([25401601],"float64"), Tensor([25401601],"int32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f915887e950>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:47:59.736710 test begin: paddle.ldexp(Tensor([25401601],"int64"), Tensor([25401601],"int32"), )
W0804 16:48:00.551867 29549 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.ldexp 	 paddle.ldexp(Tensor([25401601],"int64"), Tensor([25401601],"int32"), ) 	 50803202 	 12113 	 9.988956451416016 	 7.797068119049072 	 0.16857361793518066 	 0.21908068656921387 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:54:49.387739 test begin: paddle.ldexp(Tensor([50803201],"float64"), Tensor([50803201],"int32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd60c2267a0>,)) (kwargs={}) timed out after 600.000000 seconds.

W0804 17:04:49.515247 32005 backward.cc:462] While running Node (ElementwisePowGradNode) raises an EnforceNotMet exception
terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754298289 (unix time) try "date -d @1754298289" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x72fd) received by PID 29437 (TID 0x7fd6039f8640) from PID 29437 ***]

2025-08-04 17:04:58.624577 test begin: paddle.ldexp(Tensor([50803201],"int32"), Tensor([50803201],"int32"), )
W0804 17:04:59.827567 34728 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc19142ef50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:15:03.107808 test begin: paddle.ldexp(Tensor([50803201],"int64"), Tensor([50803201],"int32"), )
W0804 17:15:04.461333 37373 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd92a0d2fb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:25:07.852933 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 28, 604801],"float32"), 0.36, )
W0804 17:25:08.698361 40021 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 28, 604801],"float32"), 0.36, ) 	 50803285 	 33528 	 10.024775266647339 	 10.152073383331299 	 0.15276050567626953 	 0.30905914306640625 	 21.226274013519287 	 25.108671188354492 	 0.21537351608276367 	 0.1910867691040039 	 
2025-08-04 17:26:18.294686 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 604801, 28],"float32"), 0.36, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 604801, 28],"float32"), 0.36, ) 	 50803285 	 33528 	 10.025472402572632 	 10.138208150863647 	 0.15279936790466309 	 0.30904459953308105 	 21.2497661113739 	 25.104599952697754 	 0.2156519889831543 	 0.19103622436523438 	 
2025-08-04 17:27:26.920526 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([64801, 28, 28],"float32"), 0.36, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([64801, 28, 28],"float32"), 0.36, ) 	 50803985 	 33528 	 10.021933317184448 	 10.13832688331604 	 0.15272307395935059 	 0.30901598930358887 	 21.249618768692017 	 25.10596203804016 	 0.2156364917755127 	 0.19108343124389648 	 
2025-08-04 17:28:38.557005 test begin: paddle.lerp(Tensor([1, 1814401, 28],"float32"), Tensor([3, 1814401, 28],"float32"), 1.0, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 1814401, 28],"float32"), Tensor([3, 1814401, 28],"float32"), 1.0, ) 	 203212912 	 33528 	 44.94422888755798 	 44.434797048568726 	 0.684898853302002 	 1.3545310497283936 	 74.40307450294495 	 78.95528435707092 	 1.1340694427490234 	 0.8021290302276611 	 
2025-08-04 17:32:48.185620 test begin: paddle.lerp(Tensor([1, 28, 1814401],"float32"), Tensor([3, 28, 1814401],"float32"), 1.0, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 28, 1814401],"float32"), Tensor([3, 28, 1814401],"float32"), 1.0, ) 	 203212912 	 33528 	 44.941301345825195 	 44.4365074634552 	 0.6849632263183594 	 1.3545622825622559 	 74.40299534797668 	 78.95532321929932 	 1.1340315341949463 	 0.8021981716156006 	 
2025-08-04 17:37:00.438703 test begin: paddle.lerp(Tensor([1, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.0, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.0, ) 	 50804768 	 33528 	 10.013165950775146 	 10.919299602508545 	 0.15257811546325684 	 0.3111259937286377 	 26.628994464874268 	 26.209179878234863 	 0.27037930488586426 	 0.19954586029052734 	 
2025-08-04 17:38:18.137191 test begin: paddle.lerp(Tensor([3, 28, 604801],"float32"), Tensor([3, 28, 604801],"float32"), 1.2, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([3, 28, 604801],"float32"), Tensor([3, 28, 604801],"float32"), 1.2, ) 	 101606568 	 33528 	 15.180157661437988 	 14.978463649749756 	 0.23134970664978027 	 0.45658373832702637 	 15.864562034606934 	 19.958650588989258 	 0.48359179496765137 	 0.3041713237762451 	 
2025-08-04 17:39:28.768432 test begin: paddle.lerp(Tensor([3, 604801, 28],"float32"), Tensor([3, 604801, 28],"float32"), 1.2, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([3, 604801, 28],"float32"), Tensor([3, 604801, 28],"float32"), 1.2, ) 	 101606568 	 33528 	 15.181487798690796 	 14.978471040725708 	 0.23133063316345215 	 0.4565863609313965 	 15.864979982376099 	 19.958329677581787 	 0.48355770111083984 	 0.3042166233062744 	 
2025-08-04 17:40:39.234631 test begin: paddle.lerp(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.0, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.0, ) 	 101607968 	 33528 	 15.181750535964966 	 14.978925943374634 	 0.23132658004760742 	 0.45665979385375977 	 15.86497449874878 	 19.95928168296814 	 0.48354530334472656 	 0.3042433261871338 	 
2025-08-04 17:41:50.917086 test begin: paddle.lerp(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.2, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.2, ) 	 101607968 	 33528 	 15.181711673736572 	 14.978623390197754 	 0.23136162757873535 	 0.4565420150756836 	 15.866239309310913 	 19.958662033081055 	 0.483597993850708 	 0.3041665554046631 	 
2025-08-04 17:43:00.262884 test begin: paddle.less(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 50803600 	 53034 	 10.13760757446289 	 12.902063608169556 	 0.19534873962402344 	 0.2487010955810547 	 None 	 None 	 None 	 None 	 
2025-08-04 17:43:24.200069 test begin: paddle.less(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), ) 	 50803600 	 53034 	 10.001100301742554 	 12.940284967422485 	 0.19276213645935059 	 0.24932432174682617 	 None 	 None 	 None 	 None 	 
2025-08-04 17:43:48.093783 test begin: paddle.less(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 101606800 	 53034 	 17.34037470817566 	 17.376396894454956 	 0.33414244651794434 	 0.3348720073699951 	 None 	 None 	 None 	 None 	 
2025-08-04 17:44:24.575129 test begin: paddle.less(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), ) 	 101606420 	 53034 	 17.34575653076172 	 17.37660002708435 	 0.33423829078674316 	 0.33484745025634766 	 None 	 None 	 None 	 None 	 
2025-08-04 17:45:04.248086 test begin: paddle.less(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), ) 	 101606420 	 53034 	 17.357322692871094 	 17.378937005996704 	 0.33443188667297363 	 0.33492207527160645 	 None 	 None 	 None 	 None 	 
2025-08-04 17:45:44.595924 test begin: paddle.less(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), ) 	 101606440 	 53034 	 17.35620355606079 	 17.375179290771484 	 0.3344419002532959 	 0.3348512649536133 	 None 	 None 	 None 	 None 	 
2025-08-04 17:46:21.037243 test begin: paddle.less(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float32"), ) 	 101607424 	 53034 	 17.349138975143433 	 17.393949031829834 	 0.3343777656555176 	 0.33490943908691406 	 None 	 None 	 None 	 None 	 
2025-08-04 17:47:00.806613 test begin: paddle.less(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 53034 	 17.338602542877197 	 17.376760959625244 	 0.33414316177368164 	 0.33484816551208496 	 None 	 None 	 None 	 None 	 
2025-08-04 17:47:40.211226 test begin: paddle.less_equal(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 50803600 	 53036 	 10.101981163024902 	 12.907622337341309 	 0.1946728229522705 	 0.24875593185424805 	 None 	 None 	 None 	 None 	 
2025-08-04 17:48:05.053574 test begin: paddle.less_equal(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), ) 	 50803600 	 53036 	 9.96253752708435 	 12.942705869674683 	 0.19192934036254883 	 0.24939823150634766 	 None 	 None 	 None 	 None 	 
2025-08-04 17:48:28.854000 test begin: paddle.less_equal(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 101606800 	 53036 	 17.353893280029297 	 17.385982036590576 	 0.3343994617462158 	 0.3348066806793213 	 None 	 None 	 None 	 None 	 
2025-08-04 17:49:07.477179 test begin: paddle.less_equal(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), ) 	 101606420 	 53036 	 17.350603580474854 	 17.379803895950317 	 0.3343544006347656 	 0.3349027633666992 	 None 	 None 	 None 	 None 	 
2025-08-04 17:49:46.144488 test begin: paddle.less_equal(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float32"), )
W0804 17:49:49.446828 46401 dygraph_functions.cc:90806] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float32"), ) 	 203212812 	 53036 	 59.96337032318115 	 38.15411448478699 	 0.5777065753936768 	 0.7348537445068359 	 None 	 None 	 None 	 None 	 
2025-08-04 17:51:27.859310 test begin: paddle.less_equal(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), ) 	 101606440 	 53036 	 17.346819162368774 	 17.376278162002563 	 0.3342714309692383 	 0.3348968029022217 	 None 	 None 	 None 	 None 	 
2025-08-04 17:52:04.269953 test begin: paddle.less_equal(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float32"), ) 	 203212816 	 53036 	 59.96898412704468 	 38.162792921066284 	 0.5777692794799805 	 0.7348513603210449 	 None 	 None 	 None 	 None 	 
2025-08-04 17:53:46.007426 test begin: paddle.less_equal(Tensor([4, 3, 4233601],"float16"), Tensor([4, 3, 4233601],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([4, 3, 4233601],"float16"), Tensor([4, 3, 4233601],"float32"), ) 	 101606424 	 53036 	 30.22220754623413 	 19.29605770111084 	 0.2911231517791748 	 0.37209486961364746 	 None 	 None 	 None 	 None 	 
2025-08-04 17:54:39.584077 test begin: paddle.less_equal(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float32"), ) 	 203212824 	 53036 	 59.97635197639465 	 38.17952919006348 	 0.5778465270996094 	 0.7365119457244873 	 None 	 None 	 None 	 None 	 
2025-08-04 17:56:23.158484 test begin: paddle.less_equal(Tensor([4, 6350401, 2],"float16"), Tensor([4, 6350401, 2],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([4, 6350401, 2],"float16"), Tensor([4, 6350401, 2],"float32"), ) 	 101606416 	 53036 	 30.220223426818848 	 20.3348445892334 	 0.2911539077758789 	 0.3720111846923828 	 None 	 None 	 None 	 None 	 
2025-08-04 17:57:18.451960 test begin: paddle.less_equal(Tensor([8467201, 3, 2],"float16"), Tensor([8467201, 3, 2],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([8467201, 3, 2],"float16"), Tensor([8467201, 3, 2],"float32"), ) 	 101606412 	 53036 	 30.22171688079834 	 19.278372287750244 	 0.29114699363708496 	 0.37117576599121094 	 None 	 None 	 None 	 None 	 
2025-08-04 17:58:09.798042 test begin: paddle.less_than(Tensor([1, 128, 198451],"int64"), Tensor([1, 128, 198451],"int64"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 128, 198451],"int64"), Tensor([1, 128, 198451],"int64"), ) 	 50803456 	 56808 	 17.582666873931885 	 17.790921688079834 	 0.31626319885253906 	 0.3200855255126953 	 None 	 None 	 None 	 None 	 
2025-08-04 17:58:46.148730 test begin: paddle.less_than(Tensor([1, 128, 256],"float32"), Tensor([1551, 128, 256],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 128, 256],"float32"), Tensor([1551, 128, 256],"float32"), ) 	 50855936 	 56808 	 10.823163270950317 	 13.853055953979492 	 0.19469499588012695 	 0.24921035766601562 	 None 	 None 	 None 	 None 	 
2025-08-04 17:59:15.207466 test begin: paddle.less_than(Tensor([1, 128, 256],"int64"), Tensor([776, 128, 256],"int64"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 128, 256],"int64"), Tensor([776, 128, 256],"int64"), ) 	 25460736 	 56808 	 11.703684329986572 	 10.317167043685913 	 0.21054959297180176 	 0.18560004234313965 	 None 	 None 	 None 	 None 	 
2025-08-04 17:59:39.144214 test begin: paddle.less_than(Tensor([1, 128, 396901],"float32"), Tensor([1, 128, 396901],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 128, 396901],"float32"), Tensor([1, 128, 396901],"float32"), ) 	 101606656 	 56808 	 18.572028875350952 	 18.61299419403076 	 0.33399486541748047 	 0.334888219833374 	 None 	 None 	 None 	 None 	 
2025-08-04 18:00:21.256013 test begin: paddle.less_than(Tensor([1, 198451, 256],"float32"), Tensor([1, 198451, 256],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 198451, 256],"float32"), Tensor([1, 198451, 256],"float32"), ) 	 101606912 	 56808 	 18.5881667137146 	 18.62314462661743 	 0.33429861068725586 	 0.3348655700683594 	 None 	 None 	 None 	 None 	 
2025-08-04 18:01:04.214502 test begin: paddle.less_than(Tensor([1, 99226, 256],"int64"), Tensor([1, 99226, 256],"int64"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 99226, 256],"int64"), Tensor([1, 99226, 256],"int64"), ) 	 50803712 	 56808 	 17.561466217041016 	 17.78269338607788 	 0.31596922874450684 	 0.31999683380126953 	 None 	 None 	 None 	 None 	 
2025-08-04 18:01:40.662780 test begin: paddle.less_than(Tensor([1551, 128, 256],"float32"), Tensor([1, 128, 256],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1551, 128, 256],"float32"), Tensor([1, 128, 256],"float32"), ) 	 50855936 	 56808 	 10.709148406982422 	 13.870145797729492 	 0.19267582893371582 	 0.24951863288879395 	 None 	 None 	 None 	 None 	 
2025-08-04 18:02:08.429439 test begin: paddle.less_than(Tensor([1551, 128, 256],"float32"), Tensor([1551, 128, 256],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1551, 128, 256],"float32"), Tensor([1551, 128, 256],"float32"), ) 	 101646336 	 56808 	 18.595080375671387 	 18.618533611297607 	 0.3345673084259033 	 0.3350191116333008 	 None 	 None 	 None 	 None 	 
2025-08-04 18:02:49.640945 test begin: paddle.less_than(Tensor([3101, 1, 128, 128],"float32"), Tensor([3101, 1, 128, 128],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([3101, 1, 128, 128],"float32"), Tensor([3101, 1, 128, 128],"float32"), ) 	 101613568 	 56808 	 18.59670639038086 	 18.61230492591858 	 0.33446550369262695 	 0.33486247062683105 	 None 	 None 	 None 	 None 	 
2025-08-04 18:03:31.872538 test begin: paddle.less_than(Tensor([776, 128, 256],"int64"), Tensor([1, 128, 256],"int64"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([776, 128, 256],"int64"), Tensor([1, 128, 256],"int64"), ) 	 25460736 	 56808 	 9.949652433395386 	 10.316174030303955 	 0.17898273468017578 	 0.18553423881530762 	 None 	 None 	 None 	 None 	 
2025-08-04 18:03:53.627704 test begin: paddle.less_than(Tensor([776, 128, 256],"int64"), Tensor([776, 128, 256],"int64"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([776, 128, 256],"int64"), Tensor([776, 128, 256],"int64"), ) 	 50855936 	 56808 	 17.594605207443237 	 17.796022653579712 	 0.3164231777191162 	 0.3202524185180664 	 None 	 None 	 None 	 None 	 
2025-08-04 18:04:29.883944 test begin: paddle.less_than(Tensor([8, 1, 128, 128],"float32"), Tensor([8, 388, 128, 128],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([8, 1, 128, 128],"float32"), Tensor([8, 388, 128, 128],"float32"), ) 	 50987008 	 56808 	 10.949689388275146 	 14.722949028015137 	 0.19701576232910156 	 0.26459598541259766 	 None 	 None 	 None 	 None 	 
2025-08-04 18:04:57.338649 test begin: paddle.less_than(Tensor([8, 1, 128, 49613],"float32"), Tensor([8, 1, 128, 49613],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([8, 1, 128, 49613],"float32"), Tensor([8, 1, 128, 49613],"float32"), ) 	 101607424 	 56808 	 18.57543659210205 	 18.61412000656128 	 0.3341712951660156 	 0.33486461639404297 	 None 	 None 	 None 	 None 	 
2025-08-04 18:05:39.897459 test begin: paddle.less_than(Tensor([8, 1, 49613, 128],"float32"), Tensor([8, 1, 49613, 128],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([8, 1, 49613, 128],"float32"), Tensor([8, 1, 49613, 128],"float32"), ) 	 101607424 	 56808 	 18.58866786956787 	 18.61224102973938 	 0.3342936038970947 	 0.334827184677124 	 None 	 None 	 None 	 None 	 
2025-08-04 18:06:22.631778 test begin: paddle.less_than(Tensor([8, 388, 128, 128],"float32"), Tensor([8, 1, 128, 128],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([8, 388, 128, 128],"float32"), Tensor([8, 1, 128, 128],"float32"), ) 	 50987008 	 56808 	 10.841389656066895 	 14.735230445861816 	 0.19497990608215332 	 0.26503968238830566 	 None 	 None 	 None 	 None 	 
2025-08-04 18:06:49.104573 test begin: paddle.less_than(Tensor([8, 388, 128, 128],"float32"), Tensor([8, 388, 128, 128],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([8, 388, 128, 128],"float32"), Tensor([8, 388, 128, 128],"float32"), ) 	 101711872 	 56808 	 18.60689353942871 	 18.62949514389038 	 0.33476877212524414 	 0.33514904975891113 	 None 	 None 	 None 	 None 	 
2025-08-04 18:07:30.271834 test begin: paddle.lgamma(Tensor([10, 10, 10, 25402],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([10, 10, 10, 25402],"float64"), ) 	 25402000 	 25074 	 17.92798924446106 	 17.35147500038147 	 0.7305886745452881 	 0.7070915699005127 	 34.64994835853577 	 39.860203981399536 	 1.4117774963378906 	 0.8121235370635986 	 
2025-08-04 18:09:21.147334 test begin: paddle.lgamma(Tensor([10, 10, 127009, 2],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([10, 10, 127009, 2],"float64"), ) 	 25401800 	 25074 	 17.89696741104126 	 17.332569360733032 	 0.7291877269744873 	 0.7061502933502197 	 34.714208364486694 	 39.81043529510498 	 1.414759635925293 	 0.8111813068389893 	 
2025-08-04 18:11:13.652323 test begin: paddle.lgamma(Tensor([10, 127009, 10, 2],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([10, 127009, 10, 2],"float64"), ) 	 25401800 	 25074 	 17.881993532180786 	 17.318212509155273 	 0.7287287712097168 	 0.7058439254760742 	 34.70227670669556 	 39.79899191856384 	 1.4144718647003174 	 0.8109776973724365 	 
2025-08-04 18:13:04.449759 test begin: paddle.lgamma(Tensor([100, 254017],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([100, 254017],"float64"), ) 	 25401700 	 25074 	 18.835686922073364 	 17.310115098953247 	 0.7284541130065918 	 0.7054646015167236 	 34.65750980377197 	 39.79033803939819 	 1.4126522541046143 	 0.8108325004577637 	 
2025-08-04 18:14:58.116273 test begin: paddle.lgamma(Tensor([127009, 10, 10, 2],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([127009, 10, 10, 2],"float64"), ) 	 25401800 	 25074 	 17.871085166931152 	 17.307718753814697 	 0.7283165454864502 	 0.7054145336151123 	 34.684842109680176 	 39.79433584213257 	 1.413797378540039 	 0.8110394477844238 	 
2025-08-04 18:16:48.854982 test begin: paddle.lgamma(Tensor([1948, 26080],"float32"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([1948, 26080],"float32"), ) 	 50803840 	 25074 	 9.988551139831543 	 9.748610258102417 	 0.40778279304504395 	 0.39690160751342773 	 24.125006198883057 	 37.850188970565796 	 0.9833323955535889 	 0.7713196277618408 	 
2025-08-04 18:18:12.456585 test begin: paddle.lgamma(Tensor([254017, 100],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([254017, 100],"float64"), ) 	 25401700 	 25074 	 17.86924958229065 	 17.311929941177368 	 0.7283535003662109 	 0.705683708190918 	 34.65848445892334 	 39.7961483001709 	 1.4127423763275146 	 0.8110101222991943 	 
2025-08-04 18:20:03.290477 test begin: paddle.lgamma(Tensor([50803201, 1],"float32"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([50803201, 1],"float32"), ) 	 50803201 	 25074 	 11.513657093048096 	 9.605855226516724 	 0.41175293922424316 	 0.38857603073120117 	 24.12172269821167 	 37.859254121780396 	 0.9830689430236816 	 0.7717173099517822 	 
2025-08-04 18:21:29.954140 test begin: paddle.linalg.cond(x=Tensor([4, 396901, 4, 4],"float64"), p=-1, )
[Prof] paddle.linalg.cond 	 paddle.linalg.cond(x=Tensor([4, 396901, 4, 4],"float64"), p=-1, ) 	 25401664 	 1000 	 73.47384810447693 	 3.4986722469329834 	 0.0012276172637939453 	 0.05357074737548828 	 None 	 None 	 None 	 None 	 
[Error] one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.DoubleTensor [4, 396901, 4, 4]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
2025-08-04 18:23:08.573331 test begin: paddle.linalg.cond(x=Tensor([4, 396901, 4, 4],"float64"), p=-math.inf, )
[Prof] paddle.linalg.cond 	 paddle.linalg.cond(x=Tensor([4, 396901, 4, 4],"float64"), p=-math.inf, ) 	 25401664 	 1000 	 82.53627109527588 	 3.4768714904785156 	 0.001168966293334961 	 0.05332827568054199 	 None 	 None 	 None 	 None 	 
[Error] one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.DoubleTensor [4, 396901, 4, 4]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
2025-08-04 18:24:55.673609 test begin: paddle.linalg.cond(x=Tensor([793801, 2, 4, 4],"float64"), p=-1, )
[Prof] paddle.linalg.cond 	 paddle.linalg.cond(x=Tensor([793801, 2, 4, 4],"float64"), p=-1, ) 	 25401632 	 1000 	 74.16808819770813 	 3.4890542030334473 	 0.0012116432189941406 	 0.05355072021484375 	 None 	 None 	 None 	 None 	 
[Error] one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.DoubleTensor [793801, 2, 4, 4]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
2025-08-04 18:26:34.452564 test begin: paddle.linalg.cond(x=Tensor([793801, 2, 4, 4],"float64"), p=-math.inf, )
[Prof] paddle.linalg.cond 	 paddle.linalg.cond(x=Tensor([793801, 2, 4, 4],"float64"), p=-math.inf, ) 	 25401632 	 1000 	 74.77919626235962 	 3.4752535820007324 	 0.0011138916015625 	 0.05330204963684082 	 None 	 None 	 None 	 None 	 
[Error] one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.DoubleTensor [793801, 2, 4, 4]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
2025-08-04 18:28:14.451336 test begin: paddle.linalg.corrcoef(Tensor([4, 12700801],"float32"), )
[Prof] paddle.linalg.corrcoef 	 paddle.linalg.corrcoef(Tensor([4, 12700801],"float32"), ) 	 50803204 	 5085 	 14.689186334609985 	 11.888499975204468 	 0.1643061637878418 	 0.0020041465759277344 	 23.062028646469116 	 17.496017932891846 	 0.10402417182922363 	 0.06360340118408203 	 
2025-08-04 18:29:22.806161 test begin: paddle.linalg.corrcoef(Tensor([4, 6350401],"float64"), )
[Prof] paddle.linalg.corrcoef 	 paddle.linalg.corrcoef(Tensor([4, 6350401],"float64"), ) 	 25401604 	 5085 	 10.024207830429077 	 6.426168203353882 	 0.11204361915588379 	 0.0009667873382568359 	 25.16154408454895 	 17.00513529777527 	 0.1533505916595459 	 0.0784749984741211 	 
2025-08-04 18:30:23.894887 test begin: paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=None, aweights=Tensor([50803201],"int32"), )
W0804 18:30:35.067766 56024 backward.cc:462] While running Node (CastGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=None, aweights=Tensor([50803201],"int32"), ) 	 50803401 	 18486 	 11.160431385040283 	 7.689230918884277 	 4.029273986816406e-05 	 0.00024318695068359375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:30:42.764803 test begin: paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int64"), aweights=Tensor([25401601],"float64"), )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int64"), aweights=Tensor([25401601],"float64"), ) 	 25401811 	 18486 	 18.556247234344482 	 9.019819259643555 	 4.3392181396484375e-05 	 0.0001304149627685547 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:31:21.598003 test begin: paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([25401601],"int64"), aweights=Tensor([10],"float64"), )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([25401601],"int64"), aweights=Tensor([10],"float64"), ) 	 25401811 	 18486 	 18.53708028793335 	 10.162859916687012 	 4.029273986816406e-05 	 0.0002117156982421875 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:32:01.564644 test begin: paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([50803201],"int32"), aweights=None, )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([50803201],"int32"), aweights=None, ) 	 50803401 	 18486 	 12.79169511795044 	 7.849246263504028 	 6.270408630371094e-05 	 0.00012135505676269531 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:32:30.955621 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=None, aweights=Tensor([10],"int32"), )
W0804 18:33:11.339223 56474 backward.cc:462] While running Node (CastGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=None, aweights=Tensor([10],"int32"), ) 	 25401630 	 18486 	 39.4558641910553 	 31.15027928352356 	 0.0016467571258544922 	 0.0009112358093261719 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:33:42.591900 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=None, aweights=Tensor([1270081],"int32"), )
W0804 18:34:23.004951 56504 backward.cc:462] While running Node (CastGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=None, aweights=Tensor([1270081],"int32"), ) 	 26671701 	 18486 	 39.50265049934387 	 31.228684902191162 	 0.0016393661499023438 	 0.0009171962738037109 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:34:56.237391 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int32"), aweights=None, )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int32"), aweights=None, ) 	 25401630 	 18486 	 41.761892318725586 	 31.21187663078308 	 0.001665353775024414 	 0.0008864402770996094 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:37:20.915507 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int64"), aweights=Tensor([10],"float64"), )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int64"), aweights=Tensor([10],"float64"), ) 	 25401640 	 18486 	 46.557340145111084 	 33.75457572937012 	 0.001542806625366211 	 0.0008938312530517578 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:39:55.944437 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([1270081],"int32"), aweights=None, )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([1270081],"int32"), aweights=None, ) 	 26671701 	 18486 	 41.70388102531433 	 31.0292911529541 	 0.0016689300537109375 	 0.0009179115295410156 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:42:18.296398 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([1270081],"int64"), aweights=Tensor([1270081],"float64"), )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([1270081],"int64"), aweights=Tensor([1270081],"float64"), ) 	 27941782 	 18486 	 44.63236618041992 	 33.75503587722778 	 0.0016489028930664062 	 0.0008993148803710938 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:44:49.329048 test begin: paddle.linalg.det(Tensor([12737, 3, 5, 5],"float32"), )
[Prof] paddle.linalg.det 	 paddle.linalg.det(Tensor([12737, 3, 5, 5],"float32"), ) 	 955275 	 1000 	 11.403581619262695 	 0.17928218841552734 	 0.00010061264038085938 	 0.00017118453979492188 	 2.0109243392944336 	 0.37187790870666504 	 4.2438507080078125e-05 	 5.269050598144531e-05 	 
2025-08-04 18:45:06.369134 test begin: paddle.linalg.det(Tensor([3, 12737, 5, 5],"float32"), )
[Prof] paddle.linalg.det 	 paddle.linalg.det(Tensor([3, 12737, 5, 5],"float32"), ) 	 955275 	 1000 	 11.318394184112549 	 0.17557930946350098 	 5.817413330078125e-05 	 7.939338684082031e-05 	 2.0041391849517822 	 0.3720734119415283 	 6.079673767089844e-05 	 5.030632019042969e-05 	 
2025-08-04 18:45:20.290356 test begin: paddle.linalg.inv(x=Tensor([5, 31752, 4, 4],"float64"), )
[Prof] paddle.linalg.inv 	 paddle.linalg.inv(x=Tensor([5, 31752, 4, 4],"float64"), ) 	 2540160 	 1322 	 9.751730918884277 	 0.4676837921142578 	 0.00010514259338378906 	 6.461143493652344e-05 	 7.1416990756988525 	 2.597909688949585 	 0.9211869239807129 	 0.2869243621826172 	 
2025-08-04 18:45:40.990736 test begin: paddle.linalg.inv(x=Tensor([52920, 3, 4, 4],"float64"), )
[Prof] paddle.linalg.inv 	 paddle.linalg.inv(x=Tensor([52920, 3, 4, 4],"float64"), ) 	 2540160 	 1322 	 9.602694034576416 	 0.464827299118042 	 0.00010728836059570312 	 6.0558319091796875e-05 	 7.1479270458221436 	 2.5984296798706055 	 0.92214035987854 	 0.2869703769683838 	 
2025-08-04 18:46:00.917209 test begin: paddle.linalg.lu(Tensor([103, 5, 5, 5],"float64"), )
/usr/local/lib/python3.10/dist-packages/torch/functional.py:2162: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
LU, pivots = torch.lu(A, compute_pivots)
should be replaced with
LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
and
LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
should be replaced with
LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2055.)
  return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([103, 5, 5, 5],"float64"), ) 	 12875 	 22178 	 321.9469747543335 	 0.864215612411499 	 0.00010132789611816406 	 6.771087646484375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:53:10.784279 test begin: paddle.linalg.lu(Tensor([106, 5, 5, 5],"float32"), )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([106, 5, 5, 5],"float32"), ) 	 13250 	 22178 	 293.57353234291077 	 0.8526816368103027 	 0.00010251998901367188 	 6.389617919921875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:59:50.845261 test begin: paddle.linalg.lu(Tensor([3, 138, 5, 5],"float64"), )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 138, 5, 5],"float64"), ) 	 10350 	 22178 	 238.7842881679535 	 1.1567134857177734 	 9.679794311523438e-05 	 7.534027099609375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:05:15.320807 test begin: paddle.linalg.lu(Tensor([3, 177, 5, 5],"float32"), )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 177, 5, 5],"float32"), ) 	 13275 	 22178 	 291.684508562088 	 1.1541621685028076 	 9.918212890625e-05 	 8.249282836914062e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:12:14.894415 test begin: paddle.linalg.lu(Tensor([3, 177, 5, 5],"float32"), pivot=True, get_infos=True, )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 177, 5, 5],"float32"), pivot=True, get_infos=True, ) 	 13275 	 22178 	 290.6892445087433 	 0.848811149597168 	 0.00010180473327636719 	 7.104873657226562e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:18:52.841177 test begin: paddle.linalg.lu(Tensor([3, 5, 138, 5],"float64"), )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 5, 138, 5],"float64"), ) 	 10350 	 22178 	 10.765793085098267 	 2.6409425735473633 	 8.630752563476562e-05 	 0.00017547607421875 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:19:17.160826 test begin: paddle.linalg.lu(Tensor([3, 5, 177, 5],"float32"), )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 5, 177, 5],"float32"), ) 	 13275 	 22178 	 10.539992809295654 	 2.9605207443237305 	 0.00010204315185546875 	 0.0002415180206298828 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:19:45.575999 test begin: paddle.linalg.lu(Tensor([3, 5, 177, 5],"float32"), pivot=True, get_infos=True, )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 5, 177, 5],"float32"), pivot=True, get_infos=True, ) 	 13275 	 22178 	 10.288044929504395 	 2.9052276611328125 	 0.00010418891906738281 	 0.000209808349609375 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:20:13.744609 test begin: paddle.linalg.lu(Tensor([3, 5, 5, 138],"float64"), )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 5, 5, 138],"float64"), ) 	 10350 	 22178 	 12.970243453979492 	 3.353256940841675 	 7.343292236328125e-05 	 7.367134094238281e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:20:44.734323 test begin: paddle.linalg.lu(Tensor([3, 5, 5, 177],"float32"), )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 5, 5, 177],"float32"), ) 	 13275 	 22178 	 12.589859008789062 	 3.7642805576324463 	 9.584426879882812e-05 	 5.793571472167969e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:21:15.797495 test begin: paddle.linalg.lu(Tensor([3, 5, 5, 177],"float32"), pivot=True, get_infos=True, )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 5, 5, 177],"float32"), pivot=True, get_infos=True, ) 	 13275 	 22178 	 12.502697706222534 	 3.4703121185302734 	 0.00011229515075683594 	 5.984306335449219e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:21:44.088326 test begin: paddle.linalg.lu_unpack(Tensor([20321, 5, 5, 5],"float32"), Tensor([203, 5, 5],"int32"), )
[Prof] paddle.linalg.lu_unpack 	 paddle.linalg.lu_unpack(Tensor([20321, 5, 5, 5],"float32"), Tensor([203, 5, 5],"int32"), ) 	 2545200 	 10765 	 98.55096936225891 	 1.189267635345459 	 9.584426879882812e-05 	 0.014089822769165039 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([203, 5, 5, 5]) and output[0] has a shape of torch.Size([20321, 5, 5, 5]).
2025-08-04 19:23:25.983500 test begin: paddle.linalg.lu_unpack(Tensor([20321, 5, 5, 5],"float64"), Tensor([203, 5, 5],"int32"), )
[Prof] paddle.linalg.lu_unpack 	 paddle.linalg.lu_unpack(Tensor([20321, 5, 5, 5],"float64"), Tensor([203, 5, 5],"int32"), ) 	 2545200 	 10765 	 137.40977931022644 	 1.579651117324829 	 0.00010538101196289062 	 0.018733501434326172 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([203, 5, 5, 5]) and output[0] has a shape of torch.Size([20321, 5, 5, 5]).
2025-08-04 19:25:48.612624 test begin: paddle.linalg.lu_unpack(Tensor([3, 5, 5, 338689],"float64"), Tensor([3, 5, 5],"int32"), )
[Prof] paddle.linalg.lu_unpack 	 paddle.linalg.lu_unpack(Tensor([3, 5, 5, 338689],"float64"), Tensor([3, 5, 5],"int32"), ) 	 25401750 	 10765 	 10.290523290634155 	 3.8915107250213623 	 9.465217590332031e-05 	 0.046228885650634766 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:26:20.077371 test begin: paddle.linalg.lu_unpack(Tensor([3, 5, 5, 677377],"float32"), Tensor([3, 5, 5],"int32"), )
[Prof] paddle.linalg.lu_unpack 	 paddle.linalg.lu_unpack(Tensor([3, 5, 5, 677377],"float32"), Tensor([3, 5, 5],"int32"), ) 	 50803350 	 10765 	 12.886781215667725 	 4.343617677688599 	 7.82012939453125e-05 	 0.05161333084106445 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:26:56.522649 test begin: paddle.linalg.lu_unpack(Tensor([4064, 5, 5, 5],"float32"), Tensor([406, 5, 5],"int32"), )
[Prof] paddle.linalg.lu_unpack 	 paddle.linalg.lu_unpack(Tensor([4064, 5, 5, 5],"float32"), Tensor([406, 5, 5],"int32"), ) 	 518150 	 10765 	 184.5187075138092 	 0.7152259349822998 	 9.489059448242188e-05 	 0.00013136863708496094 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([406, 5, 5, 5]) and output[0] has a shape of torch.Size([4064, 5, 5, 5]).
2025-08-04 19:30:02.901822 test begin: paddle.linalg.lu_unpack(Tensor([6773, 5, 5, 3],"float32"), Tensor([277, 5, 3],"int32"), )
[Prof] paddle.linalg.lu_unpack 	 paddle.linalg.lu_unpack(Tensor([6773, 5, 5, 3],"float32"), Tensor([277, 5, 3],"int32"), ) 	 512130 	 10765 	 160.10173416137695 	 0.7558314800262451 	 0.00011754035949707031 	 7.367134094238281e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([277, 5, 5, 5]) and output[0] has a shape of torch.Size([6773, 5, 5, 5]).
2025-08-04 19:32:44.911390 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 4233601],"float64"), p="fro", axis=list[0,1,], keepdim=False, )
[Prof] paddle.linalg.matrix_norm 	 paddle.linalg.matrix_norm(x=Tensor([2, 3, 4233601],"float64"), p="fro", axis=list[0,1,], keepdim=False, ) 	 25401606 	 42429 	 9.925511837005615 	 7.44876766204834 	 0.11953949928283691 	 0.17941045761108398 	 54.95964574813843 	 53.25067186355591 	 0.4413275718688965 	 0.32082581520080566 	 
2025-08-04 19:34:53.190598 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 846721, 5],"float64"), p=-2, axis=list[1,2,], keepdim=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f90f79dac20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 19:44:58.013080 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 846721, 5],"float64"), p=-2, axis=list[1,2,], keepdim=True, )
W0804 19:44:58.709473 65582 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4572476cb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 19:55:02.688988 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3175201, 4],"float64"), p="fro", axis=list[0,1,], keepdim=False, )
W0804 19:55:03.420670 65989 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.linalg.matrix_norm 	 paddle.linalg.matrix_norm(x=Tensor([2, 3175201, 4],"float64"), p="fro", axis=list[0,1,], keepdim=False, ) 	 25401608 	 42429 	 223.34897184371948 	 6.739492416381836 	 1.7945358753204346 	 0.08121371269226074 	 45.95112180709839 	 38.50769782066345 	 0.3689413070678711 	 0.2320261001586914 	 
2025-08-04 20:00:21.095327 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 635041, 4, 5],"float64"), p=-2, axis=list[1,2,], keepdim=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa09cf66da0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 20:10:25.979980 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 635041, 4, 5],"float64"), p=-2, axis=list[1,2,], keepdim=True, )
W0804 20:10:26.632339 66467 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0595852d10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 20:20:30.781360 test begin: paddle.linalg.matrix_norm(x=Tensor([2116801, 3, 4],"float64"), p="fro", axis=list[0,1,], keepdim=False, )
W0804 20:20:31.467825 66773 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.linalg.matrix_norm 	 paddle.linalg.matrix_norm(x=Tensor([2116801, 3, 4],"float64"), p="fro", axis=list[0,1,], keepdim=False, ) 	 25401612 	 42429 	 223.33769536018372 	 6.735708951950073 	 1.794743299484253 	 0.0811622142791748 	 45.953020095825195 	 38.489962100982666 	 0.36892056465148926 	 0.23189115524291992 	 
2025-08-04 20:25:47.068129 test begin: paddle.linalg.matrix_power(x=Tensor([2068, 2, 3, 2, 1, 32, 32],"float64"), n=-10, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([2068, 2, 3, 2, 1, 32, 32],"float64"), n=-10, ) 	 25411584 	 2109 	 15.172228574752808 	 13.281899213790894 	 0.003218412399291992 	 0.0015256404876708984 	 43.335198640823364 	 14.247056484222412 	 0.016382217407226562 	 0.460113525390625 	 
2025-08-04 20:27:16.715025 test begin: paddle.linalg.matrix_power(x=Tensor([2068, 2, 3, 2, 1, 32, 32],"float64"), n=-2, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([2068, 2, 3, 2, 1, 32, 32],"float64"), n=-2, ) 	 25411584 	 2109 	 9.998425245285034 	 10.630302429199219 	 0.0003120899200439453 	 0.0003638267517089844 	 13.011642456054688 	 5.517114877700806 	 0.002207040786743164 	 0.44535374641418457 	 
2025-08-04 20:28:00.012458 test begin: paddle.linalg.matrix_power(x=Tensor([3, 1379, 3, 2, 1, 32, 32],"float64"), n=-10, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 1379, 3, 2, 1, 32, 32],"float64"), n=-10, ) 	 25417728 	 2109 	 15.181863069534302 	 13.171597719192505 	 0.0032079219818115234 	 0.0015377998352050781 	 43.341625690460205 	 14.253603458404541 	 0.0163729190826416 	 0.46039295196533203 	 
2025-08-04 20:29:34.171391 test begin: paddle.linalg.matrix_power(x=Tensor([3, 1379, 3, 2, 1, 32, 32],"float64"), n=-2, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 1379, 3, 2, 1, 32, 32],"float64"), n=-2, ) 	 25417728 	 2109 	 10.219631433486938 	 10.680372476577759 	 0.00031375885009765625 	 0.00035953521728515625 	 13.029994010925293 	 5.518219947814941 	 0.0021948814392089844 	 0.4455864429473877 	 
2025-08-04 20:30:14.954079 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 2005, 6, 1, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 2005, 6, 1, 11, 4, 4],"float64"), n=3, ) 	 25407360 	 2109 	 36.71315908432007 	 36.02524399757385 	 0.3430635929107666 	 0.35014772415161133 	 97.99300909042358 	 89.96494317054749 	 0.3641781806945801 	 0.42635345458984375 	 
2025-08-04 20:34:40.175399 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 1719, 1, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 1719, 1, 11, 4, 4],"float64"), n=3, ) 	 25413696 	 2109 	 37.9156334400177 	 36.04822540283203 	 0.3434484004974365 	 0.3503305912017822 	 97.7811131477356 	 89.99150395393372 	 0.3644247055053711 	 0.42820167541503906 	 
2025-08-04 20:39:08.426574 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 6, 1, 3151, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 6, 1, 3151, 4, 4],"float64"), n=3, ) 	 25409664 	 2109 	 36.77595019340515 	 36.0411491394043 	 0.34354162216186523 	 0.3502316474914551 	 97.86290717124939 	 89.83299493789673 	 0.36443138122558594 	 0.425828218460083 	 
2025-08-04 20:43:31.970165 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 6, 287, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 6, 287, 11, 4, 4],"float64"), n=3, ) 	 25458048 	 2109 	 36.84116077423096 	 36.092801332473755 	 0.34418249130249023 	 0.3507983684539795 	 98.21575355529785 	 89.93003249168396 	 0.36503076553344727 	 0.4274466037750244 	 
2025-08-04 20:47:54.511462 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2068, 2, 1, 32, 32],"float64"), n=-10, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2068, 2, 1, 32, 32],"float64"), n=-10, ) 	 25411584 	 2109 	 15.173656463623047 	 13.211574077606201 	 0.003216981887817383 	 0.0015361309051513672 	 43.342387199401855 	 14.247453927993774 	 0.01636648178100586 	 0.46013712882995605 	 
2025-08-04 20:49:21.761302 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2068, 2, 1, 32, 32],"float64"), n=-2, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2068, 2, 1, 32, 32],"float64"), n=-2, ) 	 25411584 	 2109 	 10.055450201034546 	 10.626709461212158 	 0.00031304359436035156 	 0.00035643577575683594 	 13.038358926773071 	 5.516605615615845 	 0.0021088123321533203 	 0.4454967975616455 	 
2025-08-04 20:50:03.853166 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 1379, 1, 32, 32],"float64"), n=-10, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 1379, 1, 32, 32],"float64"), n=-10, ) 	 25417728 	 2109 	 15.187912940979004 	 13.210019826889038 	 0.0032236576080322266 	 0.0015120506286621094 	 43.34907293319702 	 14.250642776489258 	 0.016240835189819336 	 0.46013474464416504 	 
2025-08-04 20:51:31.319075 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 1379, 1, 32, 32],"float64"), n=-2, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 1379, 1, 32, 32],"float64"), n=-2, ) 	 25417728 	 2109 	 10.045312881469727 	 10.674753904342651 	 0.0003132820129394531 	 0.000347137451171875 	 13.037811756134033 	 5.517774820327759 	 0.0022482872009277344 	 0.4454689025878906 	 
2025-08-04 20:52:14.300992 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 690, 32, 32],"float64"), n=-10, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 690, 32, 32],"float64"), n=-10, ) 	 25436160 	 2109 	 15.20663571357727 	 13.22052550315857 	 0.0031843185424804688 	 0.001535177230834961 	 43.3881151676178 	 14.259948492050171 	 0.0163726806640625 	 0.4604463577270508 	 
2025-08-04 20:53:43.505107 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 690, 32, 32],"float64"), n=-2, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 690, 32, 32],"float64"), n=-2, ) 	 25436160 	 2109 	 10.301892518997192 	 10.620089292526245 	 0.0003135204315185547 	 0.00035881996154785156 	 13.04917049407959 	 5.518805742263794 	 0.0022144317626953125 	 0.44559478759765625 	 
2025-08-04 20:54:24.394938 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 573, 7, 6, 1, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 573, 7, 6, 1, 11, 4, 4],"float64"), n=3, ) 	 25413696 	 2109 	 36.730653285980225 	 37.646209716796875 	 0.34312987327575684 	 0.3504352569580078 	 98.01094317436218 	 89.71995162963867 	 0.36442041397094727 	 0.42611050605773926 	 
2025-08-04 20:58:47.929892 test begin: paddle.linalg.matrix_power(x=Tensor([3, 573, 2, 7, 6, 1, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 573, 2, 7, 6, 1, 11, 4, 4],"float64"), n=3, ) 	 25413696 	 2109 	 36.77263045310974 	 36.04457998275757 	 0.3434760570526123 	 0.3503148555755615 	 97.70954298973083 	 89.95251679420471 	 0.36273884773254395 	 0.42804551124572754 	 
2025-08-04 21:03:10.121571 test begin: paddle.linalg.matrix_power(x=Tensor([860, 2, 2, 7, 6, 1, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([860, 2, 2, 7, 6, 1, 11, 4, 4],"float64"), n=3, ) 	 25428480 	 2109 	 36.82808804512024 	 36.038087368011475 	 0.34970784187316895 	 0.35022521018981934 	 98.01076602935791 	 89.78174948692322 	 0.3686492443084717 	 0.4260711669921875 	 
2025-08-04 21:07:33.968401 test begin: paddle.linalg.matrix_transpose(Tensor([20, 3, 8467201],"float32"), )
[Prof] paddle.linalg.matrix_transpose 	 paddle.linalg.matrix_transpose(Tensor([20, 3, 8467201],"float32"), ) 	 508032060 	 2337440 	 10.174420356750488 	 8.921359539031982 	 0.0001285076141357422 	 0.00030922889709472656 	 102.53036761283875 	 134.07849216461182 	 0.00010800361633300781 	 0.00022721290588378906 	 combined
2025-08-04 21:12:11.850492 test begin: paddle.linalg.matrix_transpose(Tensor([20, 6350401, 4],"float32"), )
[Prof] paddle.linalg.matrix_transpose 	 paddle.linalg.matrix_transpose(Tensor([20, 6350401, 4],"float32"), ) 	 508032080 	 2337440 	 10.184690952301025 	 9.505305528640747 	 6.67572021484375e-05 	 0.00027751922607421875 	 106.87553071975708 	 134.29469990730286 	 0.00010013580322265625 	 0.0002646446228027344 	 combined
2025-08-04 21:16:54.742788 test begin: paddle.linalg.matrix_transpose(Tensor([42336010, 3, 4],"float32"), )
[Prof] paddle.linalg.matrix_transpose 	 paddle.linalg.matrix_transpose(Tensor([42336010, 3, 4],"float32"), ) 	 508032120 	 2337440 	 11.108590602874756 	 8.93126654624939 	 0.00014209747314453125 	 0.00013494491577148438 	 101.17244577407837 	 134.29833316802979 	 9.870529174804688e-05 	 0.00022268295288085938 	 combined
2025-08-04 21:21:27.652005 test begin: paddle.linalg.multi_dot(list[Tensor([25401601],"float64"),Tensor([25401601, 31],"float64"),], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcaddf0bbb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:31:32.762158 test begin: paddle.linalg.multi_dot(list[Tensor([4, 4],"float64"),Tensor([4, 6350401],"float64"),], )
W0804 21:31:33.393445 69104 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([4, 4],"float64"),Tensor([4, 6350401],"float64"),], ) 	 25401620 	 60322 	 46.69254946708679 	 46.76579737663269 	 0.11302351951599121 	 0.11316442489624023 	 122.60848808288574 	 123.47323608398438 	 0.23096537590026855 	 0.23227334022521973 	 
2025-08-04 21:37:14.167337 test begin: paddle.linalg.multi_dot(list[Tensor([4233601, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 5],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([4233601, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 5],"float64"),], ) 	 25401656 	 60322 	 116.21041345596313 	 63.43121862411499 	 0.15116071701049805 	 0.15344786643981934 	 179.18278241157532 	 122.19650363922119 	 0.1514415740966797 	 0.18789410591125488 	 
2025-08-04 21:45:16.478737 test begin: paddle.linalg.multi_dot(list[Tensor([4],"float64"),Tensor([4, 6350401],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([4],"float64"),Tensor([4, 6350401],"float64"),], ) 	 25401608 	 60322 	 11.719902992248535 	 11.68594741821289 	 0.1984262466430664 	 0.1979656219482422 	 34.61432337760925 	 34.31387257575989 	 0.06519627571105957 	 0.06456398963928223 	 
2025-08-04 21:46:52.171330 test begin: paddle.linalg.multi_dot(list[Tensor([6350401, 4],"float64"),Tensor([4, 31],"float64"),], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8be467ef50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:56:57.477114 test begin: paddle.linalg.multi_dot(list[Tensor([8, 3175201],"float64"),Tensor([3175201, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 5],"float64"),], )
W0804 21:56:58.408452 70089 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 3175201],"float64"),Tensor([3175201, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 5],"float64"),], ) 	 34927243 	 60322 	 24.21348810195923 	 24.612757444381714 	 0.10236787796020508 	 0.10403251647949219 	 118.1277368068695 	 97.33655977249146 	 0.11125421524047852 	 0.13716745376586914 	 
2025-08-04 22:01:23.907046 test begin: paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 6350401],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 6350401],"float64"),], ) 	 25401682 	 60322 	 82.11591601371765 	 84.95079946517944 	 0.15473628044128418 	 0.16001248359680176 	 209.26208686828613 	 128.3880181312561 	 0.1475367546081543 	 0.16700148582458496 	 
2025-08-04 22:09:51.946786 test begin: paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 5080321],"float64"),Tensor([5080321, 5],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 5080321],"float64"),Tensor([5080321, 5],"float64"),], ) 	 40642634 	 60322 	 39.26467442512512 	 38.29250884056091 	 0.1659855842590332 	 0.16182184219360352 	 184.3564031124115 	 150.78359460830688 	 0.1561880111694336 	 0.18249940872192383 	 
2025-08-04 22:16:47.410354 test begin: paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 8467201],"float64"),Tensor([8467201, 5],"float64"),], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1364f4eaa0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:26:52.234820 test begin: paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 4233601],"float64"),Tensor([4233601, 4],"float64"),Tensor([4, 5],"float64"),], )
W0804 22:26:53.210064 71382 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 4233601],"float64"),Tensor([4233601, 4],"float64"),Tensor([4, 5],"float64"),], ) 	 42336078 	 60322 	 31.852172374725342 	 31.94678258895874 	 0.13462209701538086 	 0.1350111961364746 	 155.76053380966187 	 128.79204654693604 	 0.1319112777709961 	 0.1558396816253662 	 
2025-08-04 22:32:46.520350 test begin: paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 6350401],"float64"),Tensor([6350401, 4],"float64"),Tensor([4, 5],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 6350401],"float64"),Tensor([6350401, 4],"float64"),Tensor([4, 5],"float64"),], ) 	 63504078 	 60322 	 46.595784425735474 	 46.90887713432312 	 0.1968669891357422 	 0.19824743270874023 	 231.50231409072876 	 192.61127758026123 	 0.1642625331878662 	 0.18198847770690918 	 
2025-08-04 22:41:27.477439 test begin: paddle.linalg.multi_dot(list[Tensor([8, 8467201],"float64"),Tensor([8467201, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 5],"float64"),], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0dafffe710>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:51:32.369823 test begin: paddle.linalg.multi_dot(list[Tensor([819407],"float64"),Tensor([819407, 31],"float64"),], )
W0804 22:51:33.003857 72322 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([819407],"float64"),Tensor([819407, 31],"float64"),], ) 	 26221024 	 60322 	 10.123720407485962 	 9.989618301391602 	 0.08566784858703613 	 0.0845191478729248 	 26.934213876724243 	 20.313607215881348 	 0.22812247276306152 	 0.1720278263092041 	 
2025-08-04 22:52:44.983949 test begin: paddle.linalg.norm(Tensor([12700801, 1, 4],"float32"), p=1.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([12700801, 1, 4],"float32"), p=1.0, axis=-1, ) 	 50803204 	 66479 	 26.880517721176147 	 32.46515202522278 	 0.4131479263305664 	 0.4991025924682617 	 131.89715766906738 	 42.48985004425049 	 2.02785062789917 	 0.3265876770019531 	 
2025-08-04 22:56:39.870216 test begin: paddle.linalg.norm(Tensor([25402, 50, 20],"float64"), p=2.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([25402, 50, 20],"float64"), p=2.0, axis=-1, ) 	 25402000 	 66479 	 22.401538372039795 	 17.75416851043701 	 0.16316580772399902 	 0.2728922367095947 	 97.45125603675842 	 62.14404320716858 	 1.4971120357513428 	 0.23893117904663086 	 
2025-08-04 23:00:03.693833 test begin: paddle.linalg.norm(Tensor([50, 25402, 20],"float64"), p=2.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([50, 25402, 20],"float64"), p=2.0, axis=-1, ) 	 25402000 	 66479 	 21.228858709335327 	 17.73845148086548 	 0.1631331443786621 	 0.2727019786834717 	 97.28861880302429 	 62.146666288375854 	 1.4951496124267578 	 0.23894762992858887 	 
2025-08-04 23:03:24.370034 test begin: paddle.linalg.norm(Tensor([50, 50, 10161],"float64"), p=2.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([50, 50, 10161],"float64"), p=2.0, axis=-1, ) 	 25402500 	 66479 	 10.445940971374512 	 10.073929786682129 	 0.08028268814086914 	 0.15485858917236328 	 97.80387783050537 	 60.490010499954224 	 1.5036163330078125 	 0.23259735107421875 	 
2025-08-04 23:06:24.950438 test begin: paddle.linalg.norm(Tensor([50803201],"float32"), p=2, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([50803201],"float32"), p=2, ) 	 50803201 	 66479 	 10.12435793876648 	 10.117600440979004 	 0.05176973342895508 	 0.07774972915649414 	 66.51067328453064 	 60.4798104763031 	 1.2138402462005615 	 0.23257040977478027 	 
2025-08-04 23:08:54.119977 test begin: paddle.linalg.norm(Tensor([8550, 1, 5942],"float32"), p=1.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([8550, 1, 5942],"float32"), p=1.0, axis=-1, ) 	 50804100 	 66479 	 9.988462686538696 	 10.450289964675903 	 0.15348219871520996 	 0.160689115524292 	 127.62242984771729 	 40.264604568481445 	 1.9617125988006592 	 0.3094933032989502 	 
2025-08-04 23:12:06.060074 test begin: paddle.linalg.norm(Tensor([8550, 1486, 4],"float32"), p=1.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([8550, 1486, 4],"float32"), p=1.0, axis=-1, ) 	 50821200 	 66479 	 26.88070774078369 	 32.46824073791504 	 0.4132354259490967 	 0.4991614818572998 	 131.9519579410553 	 42.49786114692688 	 2.0289900302886963 	 0.3266301155090332 	 
2025-08-04 23:16:01.077889 test begin: paddle.linalg.pinv(Tensor([21, 6, 5, 4],"float64"), rcond=1e-15, hermitian=False, )
[Prof] paddle.linalg.pinv 	 paddle.linalg.pinv(Tensor([21, 6, 5, 4],"float64"), rcond=1e-15, hermitian=False, ) 	 2520 	 1074 	 59.08227491378784 	 0.45093560218811035 	 5.078315734863281e-05 	 7.796287536621094e-05 	 0.5115461349487305 	 0.3354628086090088 	 3.790855407714844e-05 	 7.963180541992188e-05 	 
2025-08-04 23:17:01.840858 test begin: paddle.linalg.pinv(Tensor([22, 20, 3],"float64"), rcond=1e-15, hermitian=False, )
[Prof] paddle.linalg.pinv 	 paddle.linalg.pinv(Tensor([22, 20, 3],"float64"), rcond=1e-15, hermitian=False, ) 	 1320 	 1074 	 10.068742513656616 	 0.38924551010131836 	 2.0503997802734375e-05 	 0.00011849403381347656 	 0.49115800857543945 	 0.30843162536621094 	 6.604194641113281e-05 	 3.719329833984375e-05 	 
2025-08-04 23:17:13.121738 test begin: paddle.linalg.pinv(Tensor([3, 22, 5, 4],"float64"), rcond=1e-15, hermitian=False, )
[Prof] paddle.linalg.pinv 	 paddle.linalg.pinv(Tensor([3, 22, 5, 4],"float64"), rcond=1e-15, hermitian=False, ) 	 1320 	 1074 	 30.85859179496765 	 0.44530630111694336 	 4.458427429199219e-05 	 0.00010824203491210938 	 0.5183806419372559 	 0.33295488357543945 	 4.410743713378906e-05 	 7.557868957519531e-05 	 
2025-08-04 23:17:45.943747 test begin: paddle.linalg.pinv(x=Tensor([58, 4, 4],"float64"), )
[Prof] paddle.linalg.pinv 	 paddle.linalg.pinv(x=Tensor([58, 4, 4],"float64"), ) 	 928 	 1074 	 26.754045009613037 	 0.3966965675354004 	 8.106231689453125e-05 	 5.7697296142578125e-05 	 0.44675564765930176 	 0.3293941020965576 	 7.319450378417969e-05 	 3.886222839355469e-05 	 
2025-08-04 23:18:13.914600 test begin: paddle.linalg.qr(Tensor([105, 3, 50, 8],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([105, 3, 50, 8],"float64"), ) 	 126000 	 5519 	 158.43602108955383 	 56.99601697921753 	 0.0001266002655029297 	 0.0036439895629882812 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:21:49.420939 test begin: paddle.linalg.qr(Tensor([112, 3, 20, 6],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([112, 3, 20, 6],"float64"), ) 	 40320 	 5519 	 154.48679757118225 	 63.2450270652771 	 0.0001990795135498047 	 0.0003490447998046875 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:25:27.200223 test begin: paddle.linalg.qr(Tensor([2, 105, 100, 12],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 105, 100, 12],"float64"), ) 	 252000 	 5519 	 164.37452602386475 	 41.874449729919434 	 0.00012731552124023438 	 0.0039920806884765625 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:28:53.504867 test begin: paddle.linalg.qr(Tensor([2, 158, 100, 8],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 158, 100, 8],"float64"), ) 	 252800 	 5519 	 167.955632686615 	 62.20011496543884 	 0.00011730194091796875 	 0.00026726722717285156 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:32:45.047370 test begin: paddle.linalg.qr(Tensor([2, 211, 100, 6],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 211, 100, 6],"float64"), ) 	 253200 	 5519 	 215.734361410141 	 82.15363311767578 	 0.0001220703125 	 0.003665447235107422 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:37:43.673085 test begin: paddle.linalg.qr(Tensor([2, 3, 100, 423],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 3, 100, 423],"float64"), ) 	 253800 	 5519 	 24.145146369934082 	 261.30498003959656 	 0.00012421607971191406 	 0.5680844783782959 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:42:29.193439 test begin: paddle.linalg.qr(Tensor([2, 3, 3528, 12],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 3, 3528, 12],"float64"), ) 	 254016 	 5519 	 9.780450582504272 	 8.33033299446106 	 5.984306335449219e-05 	 0.01995372772216797 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:42:47.326992 test begin: paddle.linalg.qr(Tensor([2, 3, 529201, 8],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 3, 529201, 8],"float64"), ) 	 25401648 	 5519 	 70.39178013801575 	 62.76934766769409 	 0.0008680820465087891 	 0.14066076278686523 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:45:01.089234 test begin: paddle.linalg.qr(Tensor([2, 3, 705601, 6],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 3, 705601, 6],"float64"), ) 	 25401636 	 5519 	 75.90820622444153 	 67.46257877349854 	 0.0008990764617919922 	 0.1520087718963623 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:47:25.074930 test begin: paddle.linalg.qr(Tensor([70, 3, 50, 12],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([70, 3, 50, 12],"float64"), ) 	 126000 	 5519 	 124.39316582679749 	 41.05138897895813 	 9.226799011230469e-05 	 0.003937482833862305 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:50:10.558228 test begin: paddle.linalg.slogdet(Tensor([3, 6773, 5, 5],"float32"), )
[Prof] paddle.linalg.slogdet 	 paddle.linalg.slogdet(Tensor([3, 6773, 5, 5],"float32"), ) 	 507975 	 1538 	 9.610436916351318 	 0.2516167163848877 	 4.6253204345703125e-05 	 8.225440979003906e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 3, 6773]) and output[0] has a shape of torch.Size([3, 6773]).
2025-08-04 23:50:23.072973 test begin: paddle.linalg.slogdet(Tensor([6773, 3, 5, 5],"float32"), )
[Prof] paddle.linalg.slogdet 	 paddle.linalg.slogdet(Tensor([6773, 3, 5, 5],"float32"), ) 	 507975 	 1538 	 9.55176568031311 	 0.24667787551879883 	 6.29425048828125e-05 	 8.392333984375e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 6773, 3]) and output[0] has a shape of torch.Size([6773, 3]).
2025-08-04 23:50:34.718919 test begin: paddle.linalg.solve(x=Tensor([129601, 14, 14],"float64"), y=Tensor([129601, 14, 2],"float64"), )
[Prof] paddle.linalg.solve 	 paddle.linalg.solve(x=Tensor([129601, 14, 14],"float64"), y=Tensor([129601, 14, 2],"float64"), ) 	 29030624 	 2715 	 9.950407981872559 	 6.4183855056762695 	 0.0012307167053222656 	 0.00010538101196289062 	 16.15221071243286 	 7.524327516555786 	 0.0026841163635253906 	 0.2577047348022461 	 
2025-08-04 23:51:27.675434 test begin: paddle.linalg.solve(x=Tensor([14, 14],"float64"), y=Tensor([14, 1814401],"float64"), )
[Prof] paddle.linalg.solve 	 paddle.linalg.solve(x=Tensor([14, 14],"float64"), y=Tensor([14, 1814401],"float64"), ) 	 25401810 	 2715 	 13.649167776107788 	 10.5313880443573 	 0.0039103031158447266 	 0.00025153160095214844 	 16.327906131744385 	 11.264587163925171 	 0.004532337188720703 	 0.4714531898498535 	 
2025-08-04 23:52:22.837432 test begin: paddle.linalg.solve(x=Tensor([4, 14, 14],"float64"), y=Tensor([4, 14, 453601],"float64"), )
[Prof] paddle.linalg.solve 	 paddle.linalg.solve(x=Tensor([4, 14, 14],"float64"), y=Tensor([4, 14, 453601],"float64"), ) 	 25402440 	 2715 	 11.298864364624023 	 7.819526433944702 	 0.002992868423461914 	 0.0002338886260986328 	 29.81263780593872 	 27.91645622253418 	 0.00942683219909668 	 0.5855147838592529 	 
2025-08-04 23:53:42.895999 test begin: paddle.linalg.solve(x=Tensor([907201, 14, 14],"float64"), y=Tensor([907201, 14, 2],"float64"), )
[Prof] paddle.linalg.solve 	 paddle.linalg.solve(x=Tensor([907201, 14, 14],"float64"), y=Tensor([907201, 14, 2],"float64"), ) 	 203213024 	 2715 	 70.38163781166077 	 42.03727984428406 	 0.008527994155883789 	 0.00040268898010253906 	 111.69947052001953 	 51.51264691352844 	 0.018815040588378906 	 0.41446805000305176 	 
2025-08-04 23:58:30.219328 test begin: paddle.linalg.svdvals(Tensor([10, 3, 8467],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f40fce328c0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:08:37.879549 test begin: paddle.linalg.svdvals(Tensor([10, 4233, 6],"float64"), )
W0805 00:08:41.456709 75552 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8bb1e6f010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:18:47.347607 test begin: paddle.linalg.svdvals(Tensor([10, 5080],"float32"), )
W0805 00:18:47.552608 76162 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.linalg.svdvals 	 paddle.linalg.svdvals(Tensor([10, 5080],"float32"), ) 	 50800 	 21631 	 51.046894550323486 	 18.237495183944702 	 8.916854858398438e-05 	 0.00023055076599121094 	 138.08368515968323 	 1.9128987789154053 	 8.153915405273438e-05 	 0.00014448165893554688 	 
2025-08-05 00:22:20.850222 test begin: paddle.linalg.svdvals(Tensor([40, 6350],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f01f626f070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:32:25.393821 test begin: paddle.linalg.svdvals(Tensor([611, 3, 6],"float64"), )
W0805 00:32:25.597949 76915 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.linalg.svdvals 	 paddle.linalg.svdvals(Tensor([611, 3, 6],"float64"), ) 	 10998 	 21631 	 94.43907856941223 	 10.786693096160889 	 0.00013446807861328125 	 0.00029969215393066406 	 117.87649345397949 	 2.3728721141815186 	 0.00017452239990234375 	 0.0002465248107910156 	 
2025-08-05 00:36:11.630426 test begin: paddle.linalg.svdvals(Tensor([623, 12],"float32"), )
[Prof] paddle.linalg.svdvals 	 paddle.linalg.svdvals(Tensor([623, 12],"float32"), ) 	 7476 	 21631 	 9.32205867767334 	 18.08716917037964 	 4.5299530029296875e-05 	 0.0002999305725097656 	 30.05861735343933 	 2.223680019378662 	 0.00010895729064941406 	 8.130073547363281e-05 	 
2025-08-05 00:37:11.398762 test begin: paddle.linalg.svdvals(Tensor([635, 40],"float64"), )
[Prof] paddle.linalg.svdvals 	 paddle.linalg.svdvals(Tensor([635, 40],"float64"), ) 	 25400 	 21631 	 79.64889979362488 	 61.830127477645874 	 0.00012922286987304688 	 0.00025177001953125 	 250.47712802886963 	 2.1637508869171143 	 8.535385131835938e-05 	 8.440017700195312e-05 	 
2025-08-05 00:43:45.835496 test begin: paddle.log(Tensor([192, 40, 6625],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([192, 40, 6625],"float32"), ) 	 50880000 	 33842 	 10.03130292892456 	 10.078951358795166 	 0.30292510986328125 	 0.3043851852416992 	 15.26098918914795 	 15.238022089004517 	 0.46092867851257324 	 0.46016645431518555 	 
2025-08-05 00:44:41.171191 test begin: paddle.log(Tensor([307, 25, 6626],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([307, 25, 6626],"float32"), ) 	 50854550 	 33842 	 10.022063970565796 	 10.075935363769531 	 0.302537202835083 	 0.30428409576416016 	 15.256285429000854 	 15.229932308197021 	 0.46080780029296875 	 0.4599165916442871 	 
2025-08-05 00:45:33.628967 test begin: paddle.log(Tensor([64, 120, 6625],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([64, 120, 6625],"float32"), ) 	 50880000 	 33842 	 10.031941413879395 	 10.080181121826172 	 0.3029060363769531 	 0.3044004440307617 	 15.254915475845337 	 15.238802909851074 	 0.4608957767486572 	 0.4601156711578369 	 
2025-08-05 00:46:26.655705 test begin: paddle.log(Tensor([64, 120, 6626],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([64, 120, 6626],"float32"), ) 	 50887680 	 33842 	 10.02933120727539 	 10.599831104278564 	 0.30288052558898926 	 0.30446529388427734 	 15.261600017547607 	 15.240043640136719 	 0.46094369888305664 	 0.4602351188659668 	 
2025-08-05 00:47:22.017930 test begin: paddle.log(Tensor([64, 25, 31753],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([64, 25, 31753],"float32"), ) 	 50804800 	 33842 	 10.006103277206421 	 10.066541194915771 	 0.30214881896972656 	 0.30401086807250977 	 15.237856149673462 	 15.214367151260376 	 0.4604072570800781 	 0.45948028564453125 	 
2025-08-05 00:48:14.401077 test begin: paddle.log(Tensor([64, 40, 19846],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([64, 40, 19846],"float32"), ) 	 50805760 	 33842 	 10.015004396438599 	 10.084428071975708 	 0.3023712635040283 	 0.3039734363555908 	 15.243248462677002 	 15.214831590652466 	 0.46038031578063965 	 0.4594449996948242 	 
2025-08-05 00:49:08.391033 test begin: paddle.log(Tensor([64, 80, 9923],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([64, 80, 9923],"float32"), ) 	 50805760 	 33842 	 10.015149116516113 	 11.392817735671997 	 0.3024117946624756 	 0.3040738105773926 	 15.24296236038208 	 15.214215755462646 	 0.4602842330932617 	 0.45951080322265625 	 
2025-08-05 00:50:03.105180 test begin: paddle.log(Tensor([96, 80, 6625],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([96, 80, 6625],"float32"), ) 	 50880000 	 33842 	 10.026190280914307 	 10.078925371170044 	 0.3027667999267578 	 0.3043684959411621 	 15.265367031097412 	 15.238434791564941 	 0.46103453636169434 	 0.4602220058441162 	 
2025-08-05 00:50:55.578685 test begin: paddle.log10(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.log10 	 paddle.log10(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33848 	 10.01580286026001 	 10.066648006439209 	 0.30239105224609375 	 0.3039116859436035 	 15.240558624267578 	 25.24429678916931 	 0.46016788482666016 	 0.3811154365539551 	 
2025-08-05 00:51:58.664100 test begin: paddle.log10(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.log10 	 paddle.log10(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33848 	 10.01172661781311 	 10.066484212875366 	 0.30233073234558105 	 0.30393290519714355 	 15.240279197692871 	 25.245839834213257 	 0.46024370193481445 	 0.3810257911682129 	 
2025-08-05 00:53:02.385614 test begin: paddle.log10(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.log10 	 paddle.log10(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33848 	 10.01198935508728 	 10.066843271255493 	 0.3022897243499756 	 0.30391764640808105 	 15.239704370498657 	 25.244394063949585 	 0.4601130485534668 	 0.3811159133911133 	 
2025-08-05 00:54:05.328911 test begin: paddle.log10(x=Tensor([12700801, 2],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([12700801, 2],"float64"), ) 	 25401602 	 33848 	 10.353663444519043 	 10.37939977645874 	 0.3126087188720703 	 0.31327199935913086 	 15.166675329208374 	 25.21708655357361 	 0.45758819580078125 	 0.38071632385253906 	 
2025-08-05 00:55:11.091048 test begin: paddle.log10(x=Tensor([2, 12700801],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([2, 12700801],"float64"), ) 	 25401602 	 33848 	 10.35342288017273 	 10.401765584945679 	 0.3125910758972168 	 0.3134009838104248 	 15.167240142822266 	 25.21848964691162 	 0.45799899101257324 	 0.38072681427001953 	 
2025-08-05 00:56:14.970365 test begin: paddle.log10(x=Tensor([2, 3, 2, 2116801],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([2, 3, 2, 2116801],"float64"), ) 	 25401612 	 33848 	 10.356332302093506 	 10.395249843597412 	 0.31266021728515625 	 0.3133401870727539 	 15.165549278259277 	 25.216382265090942 	 0.4581737518310547 	 0.3807106018066406 	 
2025-08-05 00:57:18.081944 test begin: paddle.log10(x=Tensor([2, 3, 2116801, 2],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([2, 3, 2116801, 2],"float64"), ) 	 25401612 	 33848 	 10.35385799407959 	 10.377803564071655 	 0.312638521194458 	 0.3133423328399658 	 15.166449546813965 	 25.21769070625305 	 0.4575974941253662 	 0.3807032108306885 	 
2025-08-05 00:58:20.465770 test begin: paddle.log10(x=Tensor([2, 3175201, 2, 2],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([2, 3175201, 2, 2],"float64"), ) 	 25401608 	 33848 	 10.353567600250244 	 10.377834558486938 	 0.3126096725463867 	 0.3133416175842285 	 15.167223930358887 	 25.216835975646973 	 0.4581873416900635 	 0.38069772720336914 	 
2025-08-05 00:59:23.564170 test begin: paddle.log10(x=Tensor([2116801, 3, 2, 2],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([2116801, 3, 2, 2],"float64"), ) 	 25401612 	 33848 	 10.35371732711792 	 10.38201355934143 	 0.31259751319885254 	 0.3134338855743408 	 15.16450810432434 	 25.216503143310547 	 0.45752787590026855 	 0.3806777000427246 	 
2025-08-05 01:00:26.946112 test begin: paddle.log1p(Tensor([10, 16935, 300],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([10, 16935, 300],"float32"), ) 	 50805000 	 33832 	 10.00454306602478 	 10.127717018127441 	 0.30221128463745117 	 0.30532121658325195 	 15.23404049873352 	 25.234716415405273 	 0.4601609706878662 	 0.38109397888183594 	 
2025-08-05 01:01:29.301156 test begin: paddle.log1p(Tensor([10, 200, 25402],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([10, 200, 25402],"float32"), ) 	 50804000 	 33832 	 10.008690357208252 	 10.10833191871643 	 0.3022751808166504 	 0.3053774833679199 	 15.233432531356812 	 25.232678651809692 	 0.46022534370422363 	 0.3811054229736328 	 
2025-08-05 01:02:32.589623 test begin: paddle.log1p(Tensor([1016065, 5, 5],"float64"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([1016065, 5, 5],"float64"), ) 	 25401625 	 33832 	 10.322722673416138 	 11.381970643997192 	 0.31182217597961426 	 0.34379124641418457 	 15.15713357925415 	 25.20485830307007 	 0.45738959312438965 	 0.38074588775634766 	 
2025-08-05 01:03:39.156370 test begin: paddle.log1p(Tensor([108, 157920, 3],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([108, 157920, 3],"float32"), ) 	 51166080 	 33832 	 10.093000411987305 	 10.176526308059692 	 0.3045969009399414 	 0.3074350357055664 	 15.340146780014038 	 25.41537070274353 	 0.4633908271789551 	 0.383864164352417 	 
2025-08-05 01:04:45.064287 test begin: paddle.log1p(Tensor([4, 157920, 81],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([4, 157920, 81],"float32"), ) 	 51166080 	 33832 	 10.087623834609985 	 10.17654824256897 	 0.30461788177490234 	 0.30739545822143555 	 15.338541746139526 	 25.415435552597046 	 0.46337318420410156 	 0.38388729095458984 	 
2025-08-05 01:05:47.787537 test begin: paddle.log1p(Tensor([4, 4233601, 3],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([4, 4233601, 3],"float32"), ) 	 50803212 	 33832 	 10.00554347038269 	 10.106775760650635 	 0.30227136611938477 	 0.30530881881713867 	 15.23502492904663 	 25.232215642929077 	 0.46012330055236816 	 0.38110947608947754 	 
2025-08-05 01:06:53.024260 test begin: paddle.log1p(Tensor([50000, 102, 5],"float64"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([50000, 102, 5],"float64"), ) 	 25500000 	 33832 	 10.367424488067627 	 11.420024633407593 	 0.3130629062652588 	 0.34501123428344727 	 15.222931385040283 	 25.301698684692383 	 0.46013808250427246 	 0.3821103572845459 	 
2025-08-05 01:07:57.666165 test begin: paddle.log1p(Tensor([50000, 5, 102],"float64"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([50000, 5, 102],"float64"), ) 	 25500000 	 33832 	 10.366594791412354 	 11.421247720718384 	 0.31308865547180176 	 0.34498167037963867 	 15.220388650894165 	 25.30111813545227 	 0.45940256118774414 	 0.3821299076080322 	 
2025-08-05 01:09:06.198281 test begin: paddle.log1p(Tensor([847, 200, 300],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([847, 200, 300],"float32"), ) 	 50820000 	 33832 	 11.5557861328125 	 12.17973518371582 	 0.3023722171783447 	 0.3054835796356201 	 15.241815090179443 	 25.240634441375732 	 0.4604511260986328 	 0.3812596797943115 	 
2025-08-05 01:10:14.433360 test begin: paddle.log2(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33844 	 10.02306580543518 	 10.43069076538086 	 0.30214357376098633 	 0.30403709411621094 	 15.236402988433838 	 25.24080729484558 	 0.46013975143432617 	 0.38108015060424805 	 
2025-08-05 01:11:18.424789 test begin: paddle.log2(Tensor([10, 2540161],"float64"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([10, 2540161],"float64"), ) 	 25401610 	 33844 	 10.353036403656006 	 10.379850149154663 	 0.31265830993652344 	 0.31321048736572266 	 15.161553621292114 	 25.20784568786621 	 0.4581868648529053 	 0.3805873394012451 	 
2025-08-05 01:12:20.603953 test begin: paddle.log2(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33844 	 10.009233474731445 	 10.067341804504395 	 0.3022801876068115 	 0.30399417877197266 	 15.241629362106323 	 25.24054527282715 	 0.4602365493774414 	 0.38111138343811035 	 
2025-08-05 01:13:22.870829 test begin: paddle.log2(Tensor([10, 5080321],"float32"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([10, 5080321],"float32"), ) 	 50803210 	 33844 	 10.009205341339111 	 11.38206672668457 	 0.30225491523742676 	 0.30397939682006836 	 15.240039825439453 	 25.240737676620483 	 0.46022868156433105 	 0.3810615539550781 	 
2025-08-05 01:14:28.426324 test begin: paddle.log2(Tensor([2116801, 12],"float64"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([2116801, 12],"float64"), ) 	 25401612 	 33844 	 10.353278875350952 	 12.084097862243652 	 0.3126254081726074 	 0.3132469654083252 	 15.16318392753601 	 25.207552433013916 	 0.45781755447387695 	 0.3806140422821045 	 
2025-08-05 01:15:34.314632 test begin: paddle.log2(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33844 	 10.01692819595337 	 10.067490339279175 	 0.30228424072265625 	 0.30403971672058105 	 15.240834951400757 	 25.240824699401855 	 0.460176944732666 	 0.38114500045776367 	 
2025-08-05 01:16:39.695002 test begin: paddle.log2(Tensor([4233601, 12],"float32"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([4233601, 12],"float32"), ) 	 50803212 	 33844 	 11.399245500564575 	 10.067477703094482 	 0.3022465705871582 	 0.30407118797302246 	 15.240439176559448 	 25.240676879882812 	 0.4602165222167969 	 0.3811204433441162 	 
2025-08-05 01:17:46.458231 test begin: paddle.logaddexp(Tensor([10, 16935, 300],"float32"), Tensor([10, 16935, 300],"float32"), )
[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([10, 16935, 300],"float32"), Tensor([10, 16935, 300],"float32"), ) 	 101610000 	 4307 	 10.918832540512085 	 1.9403164386749268 	 0.37006568908691406 	 0.4603564739227295 	 19.917324542999268 	 12.83089804649353 	 0.5247526168823242 	 0.3805055618286133 	 
2025-08-05 01:18:35.655057 test begin: paddle.logaddexp(Tensor([10, 16935, 300],"int32"), Tensor([10, 16935, 300],"int32"), )
W0805 01:18:53.121909 79026 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int32) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():7.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([10, 16935, 300],"int32"), Tensor([10, 16935, 300],"int32"), ) 	 101610000 	 4307 	 12.192742347717285 	 1.9400551319122314 	 0.36151909828186035 	 0.46032166481018066 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 01:18:55.521195 test begin: paddle.logaddexp(Tensor([10, 200, 12701],"int64"), Tensor([10, 200, 12701],"int64"), )
W0805 01:19:06.540879 79123 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int64) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():9.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([10, 200, 12701],"int64"), Tensor([10, 200, 12701],"int64"), ) 	 50804000 	 4307 	 9.992047548294067 	 0.9891667366027832 	 0.2961139678955078 	 0.23462986946105957 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 01:19:07.618323 test begin: paddle.logaddexp(Tensor([10, 200, 25402],"float32"), Tensor([10, 200, 25402],"float32"), )
[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([10, 200, 25402],"float32"), Tensor([10, 200, 25402],"float32"), ) 	 101608000 	 4307 	 10.91917896270752 	 1.9596481323242188 	 0.37021541595458984 	 0.4603452682495117 	 19.916059017181396 	 12.8307945728302 	 0.5246889591217041 	 0.38053369522094727 	 
2025-08-05 01:20:00.924342 test begin: paddle.logaddexp(Tensor([10, 200, 25402],"int32"), Tensor([10, 200, 25402],"int32"), )
W0805 01:20:14.854578 79137 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int32) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():7.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([10, 200, 25402],"int32"), Tensor([10, 200, 25402],"int32"), ) 	 101608000 	 4307 	 12.189171314239502 	 1.9423582553863525 	 0.36153721809387207 	 0.46027374267578125 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 01:20:17.029284 test begin: paddle.logaddexp(Tensor([10, 8468, 300],"int64"), Tensor([10, 8468, 300],"int64"), )
W0805 01:20:28.042290 79143 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int64) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():9.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([10, 8468, 300],"int64"), Tensor([10, 8468, 300],"int64"), ) 	 50808000 	 4307 	 9.992383480072021 	 0.9890403747558594 	 0.29619789123535156 	 0.2346324920654297 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 01:20:29.233654 test begin: paddle.logaddexp(Tensor([424, 200, 300],"int64"), Tensor([424, 200, 300],"int64"), )
W0805 01:20:41.637522 79145 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int64) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():9.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([424, 200, 300],"int64"), Tensor([424, 200, 300],"int64"), ) 	 50880000 	 4307 	 10.010831832885742 	 1.000284194946289 	 0.2967245578765869 	 0.2350924015045166 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 01:20:44.031008 test begin: paddle.logaddexp(Tensor([847, 200, 300],"float32"), Tensor([847, 200, 300],"float32"), )
[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([847, 200, 300],"float32"), Tensor([847, 200, 300],"float32"), ) 	 101640000 	 4307 	 10.920694351196289 	 1.9405980110168457 	 0.37019896507263184 	 0.4604454040527344 	 19.917348861694336 	 12.834977149963379 	 0.5246546268463135 	 0.38063740730285645 	 
2025-08-05 01:21:32.653300 test begin: paddle.logaddexp(Tensor([847, 200, 300],"int32"), Tensor([847, 200, 300],"int32"), )
W0805 01:21:46.554423 79162 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int32) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():7.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([847, 200, 300],"int32"), Tensor([847, 200, 300],"int32"), ) 	 101640000 	 4307 	 12.192168951034546 	 1.940941572189331 	 0.3616478443145752 	 0.4605438709259033 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 01:21:48.767603 test begin: paddle.logcumsumexp(Tensor([10, 10, 508033],"float32"), axis=-1, )
[Prof] paddle.logcumsumexp 	 paddle.logcumsumexp(Tensor([10, 10, 508033],"float32"), axis=-1, ) 	 50803300 	 2955 	 10.027486801147461 	 7.341205596923828 	 3.4688596725463867 	 2.538193464279175 	 32.25414538383484 	 31.99472451210022 	 1.0146806240081787 	 0.5527691841125488 	 
2025-08-05 01:23:15.946684 test begin: paddle.logcumsumexp(Tensor([10, 10, 508033],"float32"), axis=0, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2c631b2920>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:33:33.296664 test begin: paddle.logcumsumexp(Tensor([10, 508033, 10],"float32"), axis=-1, )
W0805 01:33:36.833947 79572 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5bd0a5ee90>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:43:44.326206 test begin: paddle.logcumsumexp(Tensor([10, 508033, 10],"float32"), axis=0, )
W0805 01:43:45.318655 79884 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe95629ae60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:53:48.972997 test begin: paddle.logcumsumexp(Tensor([508033, 10, 10],"float32"), axis=-1, )
W0805 01:53:49.931155 80339 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5862072ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 02:03:53.662702 test begin: paddle.logical_and(Tensor([138, 369303],"bool"), Tensor([138, 369303],"bool"), )
W0805 02:03:55.272068 80710 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.logical_and 	 paddle.logical_and(Tensor([138, 369303],"bool"), Tensor([138, 369303],"bool"), ) 	 101927628 	 84691 	 10.007002353668213 	 9.799423694610596 	 0.12078499794006348 	 0.11821460723876953 	 None 	 None 	 None 	 None 	 
2025-08-05 02:04:16.972971 test begin: paddle.logical_and(Tensor([146, 349866],"bool"), Tensor([146, 349866],"bool"), )
[Prof] paddle.logical_and 	 paddle.logical_and(Tensor([146, 349866],"bool"), Tensor([146, 349866],"bool"), ) 	 102160872 	 84691 	 9.9892897605896 	 9.807418584823608 	 0.12050032615661621 	 0.11835598945617676 	 None 	 None 	 None 	 None 	 
2025-08-05 02:04:38.393956 test begin: paddle.logical_and(Tensor([49, 1036801],"bool"), Tensor([49, 1036801],"bool"), )
[Prof] paddle.logical_and 	 paddle.logical_and(Tensor([49, 1036801],"bool"), Tensor([49, 1036801],"bool"), ) 	 101606498 	 84691 	 9.995848417282104 	 9.825507879257202 	 0.12048196792602539 	 0.1183934211730957 	 None 	 None 	 None 	 None 	 
2025-08-05 02:05:01.175976 test begin: paddle.logical_and(Tensor([53, 958551],"bool"), Tensor([53, 958551],"bool"), )
[Prof] paddle.logical_and 	 paddle.logical_and(Tensor([53, 958551],"bool"), Tensor([53, 958551],"bool"), ) 	 101606406 	 84691 	 9.933845520019531 	 9.814781904220581 	 0.11986589431762695 	 0.11839795112609863 	 None 	 None 	 None 	 None 	 
2025-08-05 02:05:24.088130 test begin: paddle.logical_and(Tensor([55, 923695],"bool"), Tensor([55, 923695],"bool"), )
[Prof] paddle.logical_and 	 paddle.logical_and(Tensor([55, 923695],"bool"), Tensor([55, 923695],"bool"), ) 	 101606450 	 84691 	 9.977574110031128 	 9.833516359329224 	 0.12038779258728027 	 0.11842799186706543 	 None 	 None 	 None 	 None 	 
2025-08-05 02:05:47.671817 test begin: paddle.logical_not(Tensor([2150400, 237],"bool"), )
[Prof] paddle.logical_not 	 paddle.logical_not(Tensor([2150400, 237],"bool"), ) 	 509644800 	 12728 	 9.945455551147461 	 9.5340735912323 	 0.7985563278198242 	 0.7655799388885498 	 None 	 None 	 None 	 None 	 
2025-08-05 02:06:14.129614 test begin: paddle.logical_not(Tensor([2204160, 231],"bool"), )
[Prof] paddle.logical_not 	 paddle.logical_not(Tensor([2204160, 231],"bool"), ) 	 509160960 	 12728 	 10.003865957260132 	 9.532400608062744 	 0.8032488822937012 	 0.7651221752166748 	 None 	 None 	 None 	 None 	 
2025-08-05 02:06:41.375553 test begin: paddle.logical_not(Tensor([2257920, 226],"bool"), )
[Prof] paddle.logical_not 	 paddle.logical_not(Tensor([2257920, 226],"bool"), ) 	 510289920 	 12728 	 9.957764387130737 	 9.546370506286621 	 0.7994511127471924 	 0.7664961814880371 	 None 	 None 	 None 	 None 	 
2025-08-05 02:07:07.825868 test begin: paddle.logical_not(Tensor([6350410, 80],"bool"), )
[Prof] paddle.logical_not 	 paddle.logical_not(Tensor([6350410, 80],"bool"), ) 	 508032800 	 12728 	 9.909778833389282 	 9.521812677383423 	 0.7957627773284912 	 0.764653205871582 	 None 	 None 	 None 	 None 	 
2025-08-05 02:07:34.231094 test begin: paddle.logical_or(Tensor([50803201],"bool"), Tensor([50803201],"bool"), )
[Prof] paddle.logical_or 	 paddle.logical_or(Tensor([50803201],"bool"), Tensor([50803201],"bool"), ) 	 101606402 	 85027 	 10.034617900848389 	 9.82714581489563 	 0.12061834335327148 	 0.11809396743774414 	 None 	 None 	 None 	 None 	 
2025-08-05 02:07:55.498448 test begin: paddle.logical_or(Tensor([640, 79381],"bool"), Tensor([640, 79381],"bool"), )
[Prof] paddle.logical_or 	 paddle.logical_or(Tensor([640, 79381],"bool"), Tensor([640, 79381],"bool"), ) 	 101607680 	 85027 	 10.033686637878418 	 9.748684883117676 	 0.12060070037841797 	 0.1171724796295166 	 None 	 None 	 None 	 None 	 
2025-08-05 02:08:16.672311 test begin: paddle.logical_or(Tensor([79381, 640],"bool"), Tensor([79381, 640],"bool"), )
[Prof] paddle.logical_or 	 paddle.logical_or(Tensor([79381, 640],"bool"), Tensor([79381, 640],"bool"), ) 	 101607680 	 85027 	 10.033799171447754 	 9.748573780059814 	 0.12063193321228027 	 0.11716508865356445 	 None 	 None 	 None 	 None 	 
2025-08-05 02:08:38.467578 test begin: paddle.logical_xor(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.logical_xor 	 paddle.logical_xor(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 50803600 	 52894 	 10.102895021438599 	 12.58209776878357 	 0.1952211856842041 	 0.2430403232574463 	 None 	 None 	 None 	 None 	 
2025-08-05 02:09:01.972741 test begin: paddle.logical_xor(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.logical_xor 	 paddle.logical_xor(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), ) 	 50803600 	 52894 	 9.956424474716187 	 12.596254825592041 	 0.19234538078308105 	 0.24338865280151367 	 None 	 None 	 None 	 None 	 
2025-08-05 02:09:25.341700 test begin: paddle.logical_xor(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.logical_xor 	 paddle.logical_xor(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 101606800 	 52894 	 17.26395010948181 	 17.329328536987305 	 0.33356142044067383 	 0.3347306251525879 	 None 	 None 	 None 	 None 	 
2025-08-05 02:10:01.664715 test begin: paddle.logical_xor(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.logical_xor 	 paddle.logical_xor(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), ) 	 101606420 	 52894 	 17.277881383895874 	 17.32548713684082 	 0.333831787109375 	 0.3347320556640625 	 None 	 None 	 None 	 None 	 
2025-08-05 02:10:39.116208 test begin: paddle.logical_xor(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.logical_xor 	 paddle.logical_xor(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), ) 	 101606440 	 52894 	 17.277897119522095 	 17.325435876846313 	 0.33379244804382324 	 0.3347311019897461 	 None 	 None 	 None 	 None 	 
2025-08-05 02:11:18.140361 test begin: paddle.logit(Tensor([10, 20, 254017],"float32"), 0.001, )
[Prof] paddle.logit 	 paddle.logit(Tensor([10, 20, 254017],"float32"), 0.001, ) 	 50803400 	 33608 	 9.997673749923706 	 10.069022417068481 	 0.303983211517334 	 0.30620479583740234 	 15.135992288589478 	 15.117832899093628 	 0.46023035049438477 	 0.4596424102783203 	 
2025-08-05 02:12:11.655917 test begin: paddle.logit(Tensor([10, 5080321, 1],"float32"), 0.001, )
[Prof] paddle.logit 	 paddle.logit(Tensor([10, 5080321, 1],"float32"), 0.001, ) 	 50803210 	 33608 	 11.345026016235352 	 10.879783630371094 	 0.3045029640197754 	 0.3061976432800293 	 15.13534927368164 	 15.117526292800903 	 0.46024441719055176 	 0.4596366882324219 	 
2025-08-05 02:13:07.509059 test begin: paddle.logit(Tensor([2540161, 20, 1],"float32"), 0.001, )
[Prof] paddle.logit 	 paddle.logit(Tensor([2540161, 20, 1],"float32"), 0.001, ) 	 50803220 	 33608 	 10.015623569488525 	 10.06928014755249 	 0.30448460578918457 	 0.3061408996582031 	 15.134289026260376 	 15.117728471755981 	 0.4602079391479492 	 0.45969319343566895 	 
2025-08-05 02:14:00.236574 test begin: paddle.logit(Tensor([50803201],"float32"), 1e-08, )
[Prof] paddle.logit 	 paddle.logit(Tensor([50803201],"float32"), 1e-08, ) 	 50803201 	 33608 	 10.017018795013428 	 10.083492040634155 	 0.3045074939727783 	 0.30618882179260254 	 15.134672164916992 	 15.117843866348267 	 0.4603402614593506 	 0.45974016189575195 	 
2025-08-05 02:14:53.371828 test begin: paddle.logit(x=Tensor([4, 3, 2, 1058401],"float64"), eps=0.2, )
[Prof] paddle.logit 	 paddle.logit(x=Tensor([4, 3, 2, 1058401],"float64"), eps=0.2, ) 	 25401624 	 33608 	 10.930262088775635 	 10.176219701766968 	 0.33216428756713867 	 0.30947184562683105 	 14.904391050338745 	 15.078103065490723 	 0.45326972007751465 	 0.4585092067718506 	 
2025-08-05 02:15:45.679632 test begin: paddle.logit(x=Tensor([4, 3, 423361, 5],"float64"), eps=0.2, )
[Prof] paddle.logit 	 paddle.logit(x=Tensor([4, 3, 423361, 5],"float64"), eps=0.2, ) 	 25401660 	 33608 	 11.00498342514038 	 10.17431092262268 	 0.3353900909423828 	 0.30938196182250977 	 14.896146535873413 	 15.077011823654175 	 0.45299291610717773 	 0.45845746994018555 	 
2025-08-05 02:16:39.256242 test begin: paddle.logit(x=Tensor([4, 635041, 2, 5],"float64"), eps=0.2, )
[Prof] paddle.logit 	 paddle.logit(x=Tensor([4, 635041, 2, 5],"float64"), eps=0.2, ) 	 25401640 	 33608 	 11.912578105926514 	 10.174368143081665 	 0.3361213207244873 	 0.3094947338104248 	 14.895411252975464 	 15.076804637908936 	 0.4529578685760498 	 0.4584815502166748 	 
2025-08-05 02:17:33.060238 test begin: paddle.logit(x=Tensor([846721, 3, 2, 5],"float64"), eps=0.2, )
[Prof] paddle.logit 	 paddle.logit(x=Tensor([846721, 3, 2, 5],"float64"), eps=0.2, ) 	 25401630 	 33608 	 10.95061707496643 	 10.19298267364502 	 0.3343043327331543 	 0.30935049057006836 	 14.905152559280396 	 15.078348159790039 	 0.45321202278137207 	 0.4585456848144531 	 
2025-08-05 02:18:26.136308 test begin: paddle.logsumexp(Tensor([1024, 49613],"float32"), axis=1, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([1024, 49613],"float32"), axis=1, ) 	 50803712 	 15717 	 11.171191215515137 	 14.660461902618408 	 0.10375475883483887 	 0.10614538192749023 	 12.728323221206665 	 14.225213766098022 	 0.8275110721588135 	 0.30831241607666016 	 
2025-08-05 02:19:22.681274 test begin: paddle.logsumexp(Tensor([30, 200, 8468],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([30, 200, 8468],"float32"), axis=-1, keepdim=False, ) 	 50808000 	 15717 	 9.961711883544922 	 14.737385749816895 	 0.12929153442382812 	 0.10669088363647461 	 19.020257711410522 	 14.291276216506958 	 1.2366650104522705 	 0.30977678298950195 	 
2025-08-05 02:20:21.573962 test begin: paddle.logsumexp(Tensor([30, 200, 8468],"float32"), axis=list[0,2,], keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([30, 200, 8468],"float32"), axis=list[0,2,], keepdim=False, ) 	 50808000 	 15717 	 11.503331184387207 	 15.451051712036133 	 0.10680270195007324 	 0.09140419960021973 	 18.95935821533203 	 14.336759328842163 	 1.233647346496582 	 0.3107171058654785 	 
2025-08-05 02:21:23.318761 test begin: paddle.logsumexp(Tensor([30, 42337, 40],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([30, 42337, 40],"float32"), axis=-1, keepdim=False, ) 	 50804400 	 15717 	 10.42190146446228 	 25.594645023345947 	 0.33890390396118164 	 0.1850748062133789 	 19.390294790267944 	 14.435216426849365 	 1.2610540390014648 	 0.31284093856811523 	 
2025-08-05 02:22:34.266626 test begin: paddle.logsumexp(Tensor([30, 42337, 40],"float32"), axis=list[0,2,], keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([30, 42337, 40],"float32"), axis=list[0,2,], keepdim=False, ) 	 50804400 	 15717 	 11.283929347991943 	 14.924870252609253 	 0.1464691162109375 	 0.10807991027832031 	 19.081382513046265 	 14.347111463546753 	 1.2411813735961914 	 0.3109762668609619 	 
2025-08-05 02:23:35.522387 test begin: paddle.logsumexp(Tensor([6351, 200, 40],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([6351, 200, 40],"float32"), axis=-1, keepdim=False, ) 	 50808000 	 15717 	 10.429224967956543 	 25.601805925369263 	 0.3391275405883789 	 0.18512845039367676 	 19.38937735557556 	 14.436599969863892 	 1.2607107162475586 	 0.31287646293640137 	 
2025-08-05 02:24:46.281367 test begin: paddle.logsumexp(Tensor([6351, 200, 40],"float32"), axis=list[0,2,], keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([6351, 200, 40],"float32"), axis=list[0,2,], keepdim=False, ) 	 50808000 	 15717 	 11.585636854171753 	 15.538180828094482 	 0.10765957832336426 	 0.09190130233764648 	 18.97480273246765 	 14.335416555404663 	 1.233715295791626 	 0.3106803894042969 	 
2025-08-05 02:25:47.581482 test begin: paddle.masked_fill(Tensor([20, 127009, 20],"int32"), Tensor([20, 127009, 20],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([20, 127009, 20],"int32"), Tensor([20, 127009, 20],"bool"), 0, ) 	 101607200 	 26391 	 9.962911367416382 	 17.187084197998047 	 0.09653878211975098 	 0.2217707633972168 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 02:26:25.084843 test begin: paddle.masked_fill(Tensor([20, 60, 42337],"int32"), Tensor([20, 60, 42337],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([20, 60, 42337],"int32"), Tensor([20, 60, 42337],"bool"), 0, ) 	 101608800 	 26391 	 9.919062852859497 	 17.183838605880737 	 0.09612822532653809 	 0.22162938117980957 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 02:27:05.972658 test begin: paddle.masked_fill(Tensor([28225, 60, 30],"int32"), Tensor([28225, 60, 30],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([28225, 60, 30],"int32"), Tensor([28225, 60, 30],"bool"), 0, ) 	 101610000 	 26391 	 12.87294888496399 	 17.155064582824707 	 0.09650802612304688 	 0.2213287353515625 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 02:27:45.346120 test begin: paddle.masked_fill(Tensor([30, 56449, 30],"int32"), Tensor([30, 56449, 30],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([30, 56449, 30],"int32"), Tensor([30, 56449, 30],"bool"), 0, ) 	 101608200 	 26391 	 9.963755369186401 	 17.159884452819824 	 0.0965576171875 	 0.2213456630706787 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 02:28:21.851606 test begin: paddle.masked_fill(Tensor([30, 60, 28225],"int32"), Tensor([30, 60, 28225],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([30, 60, 28225],"int32"), Tensor([30, 60, 28225],"bool"), 0, ) 	 101610000 	 26391 	 9.961165428161621 	 17.177213668823242 	 0.09652042388916016 	 0.22130823135375977 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 02:28:59.702968 test begin: paddle.masked_fill(Tensor([42337, 60, 20],"int32"), Tensor([42337, 60, 20],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([42337, 60, 20],"int32"), Tensor([42337, 60, 20],"bool"), 0, ) 	 101608800 	 26391 	 9.92286467552185 	 17.175882816314697 	 0.09614968299865723 	 0.2216033935546875 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 02:29:38.926092 test begin: paddle.masked_scatter(Tensor([120],"float32"), Tensor([300, 120],"bool"), Tensor([169345, 300],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([120],"float32"), Tensor([300, 120],"bool"), Tensor([169345, 300],"float32"), ) 	 50839620 	 22631 	 10.200845003128052 	 0.8412933349609375 	 4.601478576660156e-05 	 0.00011610984802246094 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 02:29:55.881325 test begin: paddle.masked_scatter(Tensor([120],"float32"), Tensor([300, 120],"bool"), Tensor([300, 169345],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([120],"float32"), Tensor([300, 120],"bool"), Tensor([300, 169345],"float32"), ) 	 50839620 	 22631 	 10.219059705734253 	 0.7934741973876953 	 4.482269287109375e-05 	 7.319450378417969e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 02:30:12.726207 test begin: paddle.masked_scatter(Tensor([300, 40],"float32"), Tensor([40],"bool"), Tensor([169345, 300],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([300, 40],"float32"), Tensor([40],"bool"), Tensor([169345, 300],"float32"), ) 	 50815540 	 22631 	 9.700542688369751 	 0.9923434257507324 	 4.6253204345703125e-05 	 6.914138793945312e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 02:30:30.369665 test begin: paddle.masked_scatter(Tensor([300, 40],"float32"), Tensor([40],"bool"), Tensor([300, 169345],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([300, 40],"float32"), Tensor([40],"bool"), Tensor([300, 169345],"float32"), ) 	 50815540 	 22631 	 9.736447095870972 	 0.9993822574615479 	 4.506111145019531e-05 	 7.033348083496094e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 02:30:45.934185 test begin: paddle.masked_scatter(Tensor([6, 8, 9, 18],"float32"), Tensor([6, 8, 9, 18],"bool"), Tensor([169345, 300],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([6, 8, 9, 18],"float32"), Tensor([6, 8, 9, 18],"bool"), Tensor([169345, 300],"float32"), ) 	 50819052 	 22631 	 9.844132900238037 	 0.783759593963623 	 4.76837158203125e-05 	 7.62939453125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 02:31:01.054799 test begin: paddle.masked_scatter(Tensor([6, 8, 9, 18],"float32"), Tensor([6, 8, 9, 18],"bool"), Tensor([300, 169345],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([6, 8, 9, 18],"float32"), Tensor([6, 8, 9, 18],"bool"), Tensor([300, 169345],"float32"), ) 	 50819052 	 22631 	 9.77440857887268 	 0.7709343433380127 	 4.482269287109375e-05 	 6.413459777832031e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 02:31:16.087718 test begin: paddle.masked_select(Tensor([16, 10164, 313],"float32"), Tensor([16, 10164, 313],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([16, 10164, 313],"float32"), Tensor([16, 10164, 313],"bool"), ) 	 101802624 	 7185 	 10.021643877029419 	 22.43263006210327 	 0.00086212158203125 	 0.0029964447021484375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 02:32:01.558115 test begin: paddle.masked_select(Tensor([16, 11109, 286],"float32"), Tensor([16, 11109, 286],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([16, 11109, 286],"float32"), Tensor([16, 11109, 286],"bool"), ) 	 101669568 	 7185 	 9.905073881149292 	 22.425153493881226 	 0.0008559226989746094 	 0.0029997825622558594 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 02:32:46.428384 test begin: paddle.masked_select(Tensor([16, 12096, 263],"float32"), Tensor([16, 12096, 263],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([16, 12096, 263],"float32"), Tensor([16, 12096, 263],"bool"), ) 	 101799936 	 7185 	 9.919275283813477 	 22.427579879760742 	 0.0008461475372314453 	 0.0030088424682617188 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 02:33:32.093419 test begin: paddle.masked_select(Tensor([16, 46695, 68],"float32"), Tensor([16, 46695, 68],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([16, 46695, 68],"float32"), Tensor([16, 46695, 68],"bool"), ) 	 101608320 	 7185 	 9.948932409286499 	 22.377116680145264 	 0.0008466243743896484 	 0.002997159957885742 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 02:34:16.914755 test begin: paddle.masked_select(Tensor([62, 12096, 68],"float32"), Tensor([62, 12096, 68],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([62, 12096, 68],"float32"), Tensor([62, 12096, 68],"bool"), ) 	 101993472 	 7185 	 9.954827070236206 	 22.53612208366394 	 0.0008499622344970703 	 0.003017425537109375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 02:35:05.076357 test begin: paddle.masked_select(Tensor([68, 11109, 68],"float32"), Tensor([68, 11109, 68],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([68, 11109, 68],"float32"), Tensor([68, 11109, 68],"bool"), ) 	 102736032 	 7185 	 10.08534049987793 	 22.807013750076294 	 0.0008602142333984375 	 0.003051280975341797 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 02:35:50.662558 test begin: paddle.masked_select(Tensor([74, 10164, 68],"float32"), Tensor([74, 10164, 68],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([74, 10164, 68],"float32"), Tensor([74, 10164, 68],"bool"), ) 	 102290496 	 7185 	 9.987603187561035 	 22.530840158462524 	 0.0008366107940673828 	 0.0030181407928466797 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 02:36:38.752238 test begin: paddle.matmul(Tensor([1, 32, 388, 4096],"float32"), Tensor([1, 32, 4096, 128],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([1, 32, 388, 4096],"float32"), Tensor([1, 32, 4096, 128],"float32"), ) 	 67633152 	 12781 	 21.083516597747803 	 21.071120500564575 	 1.6854441165924072 	 1.6844706535339355 	 23.452177047729492 	 23.451050519943237 	 0.9374711513519287 	 0.9375395774841309 	 
2025-08-05 02:38:11.640395 test begin: paddle.matmul(Tensor([1, 32, 4096, 4096],"float32"), Tensor([1, 32, 4096, 128],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa1076f7fa0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 02:48:24.088945 test begin: paddle.matmul(Tensor([1, 32, 4096, 4096],"float32"), Tensor([1, 32, 4096, 388],"float32"), )
W0805 02:48:33.860917 81734 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa85857ed70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 02:58:28.914664 test begin: paddle.matmul(Tensor([1, 32, 4096, 4096],"float32"), Tensor([4, 32, 4096, 128],"float32"), )
W0805 02:58:41.965377 82037 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3ebc1bef50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 03:08:33.972232 test begin: paddle.matmul(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 4, 4096, 128],"float32"), )
W0805 03:08:37.442859 82262 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.matmul 	 paddle.matmul(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 4, 4096, 128],"float32"), ) 	 69206016 	 12781 	 21.03189754486084 	 21.025579929351807 	 1.6819593906402588 	 1.6822097301483154 	 34.20016527175903 	 34.20001769065857 	 1.3673980236053467 	 1.3673787117004395 	 
2025-08-05 03:10:30.056679 test begin: paddle.matmul(Tensor([1, 97, 4096, 4096],"float32"), Tensor([1, 97, 4096, 128],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f984cd3f160>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 03:20:58.835551 test begin: paddle.matmul(Tensor([10, 23, 499, 3600],"float32"), Tensor([10, 23, 3600, 64],"float32"), )
W0805 03:21:09.722321 82514 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 23, 499, 3600],"float32"), Tensor([10, 23, 3600, 64],"float32"), ) 	 466164000 	 12781 	 82.77838921546936 	 82.72862410545349 	 6.613812685012817 	 6.619432687759399 	 126.60312747955322 	 126.53356504440308 	 5.0632312297821045 	 5.0535888671875 	 
2025-08-05 03:28:14.405906 test begin: paddle.matmul(Tensor([10, 3, 499, 3600],"float32"), Tensor([10, 3, 3600, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 3, 499, 3600],"float32"), Tensor([10, 3, 3600, 64],"float32"), ) 	 60804000 	 12781 	 18.468283653259277 	 18.4730122089386 	 1.4767162799835205 	 1.4773449897766113 	 17.922091245651245 	 17.92515277862549 	 0.7165088653564453 	 0.7166237831115723 	 
2025-08-05 03:29:28.306762 test begin: paddle.matmul(Tensor([10, 8, 177, 3600],"float32"), Tensor([10, 8, 3600, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 8, 177, 3600],"float32"), Tensor([10, 8, 3600, 64],"float32"), ) 	 69408000 	 12781 	 18.478734970092773 	 18.488316774368286 	 1.4772610664367676 	 1.478200912475586 	 18.85375690460205 	 18.875788688659668 	 0.753737211227417 	 0.7546398639678955 	 
2025-08-05 03:30:44.260791 test begin: paddle.matmul(Tensor([10, 8, 499, 1273],"float32"), Tensor([10, 8, 1273, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 8, 499, 1273],"float32"), Tensor([10, 8, 1273, 64],"float32"), ) 	 57335920 	 12781 	 9.97141170501709 	 9.966850996017456 	 0.7969686985015869 	 0.7969784736633301 	 16.130686044692993 	 16.130618572235107 	 0.6449573040008545 	 0.6448414325714111 	 
2025-08-05 03:31:40.612352 test begin: paddle.matmul(Tensor([10, 8, 499, 3600],"float32"), Tensor([10, 8, 3600, 177],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 8, 499, 3600],"float32"), Tensor([10, 8, 3600, 177],"float32"), ) 	 194688000 	 12781 	 55.316081285476685 	 55.17780327796936 	 4.423287391662598 	 4.410288095474243 	 97.73584699630737 	 97.60694313049316 	 3.9033710956573486 	 3.902182102203369 	 
2025-08-05 03:36:49.861035 test begin: paddle.matmul(Tensor([10, 8, 499, 9923],"float32"), Tensor([10, 8, 9923, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 8, 499, 9923],"float32"), Tensor([10, 8, 9923, 64],"float32"), ) 	 446931920 	 12781 	 75.96289682388306 	 76.02214527130127 	 6.072927236557007 	 6.078667163848877 	 118.41888809204102 	 118.44566011428833 	 4.736271381378174 	 4.736754417419434 	 
2025-08-05 03:43:26.368108 test begin: paddle.matmul(Tensor([1379, 4, 256, 256],"float32"), Tensor([1379, 4, 256, 36],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([1379, 4, 256, 256],"float32"), Tensor([1379, 4, 256, 36],"float32"), ) 	 412332032 	 12781 	 68.51837015151978 	 68.35598826408386 	 5.615781784057617 	 5.465784788131714 	 93.20351767539978 	 93.15102863311768 	 3.7253637313842773 	 3.7241904735565186 	 
2025-08-05 03:48:57.367979 test begin: paddle.matmul(Tensor([194, 4, 256, 256],"float32"), Tensor([194, 4, 256, 36],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([194, 4, 256, 256],"float32"), Tensor([194, 4, 256, 36],"float32"), ) 	 58007552 	 12781 	 10.085274457931519 	 10.084518432617188 	 0.8064625263214111 	 0.8063509464263916 	 13.703338861465454 	 13.680354118347168 	 0.5477824211120605 	 0.5468494892120361 	 
2025-08-05 03:49:46.029296 test begin: paddle.matmul(Tensor([28, 8, 499, 3600],"float32"), Tensor([28, 8, 3600, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([28, 8, 499, 3600],"float32"), Tensor([28, 8, 3600, 64],"float32"), ) 	 454003200 	 12781 	 83.59055638313293 	 82.63907980918884 	 6.607667446136475 	 6.607222080230713 	 123.73376679420471 	 123.70262908935547 	 4.946973085403442 	 4.945954084396362 	 
2025-08-05 03:56:47.997890 test begin: paddle.matmul(Tensor([4, 32, 4096, 4096],"float32"), Tensor([4, 32, 4096, 128],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2ac9c6eda0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 04:07:24.804077 test begin: paddle.matmul(Tensor([4, 8, 499, 3600],"float32"), Tensor([4, 8, 3600, 64],"float32"), )
W0805 04:07:26.112848 89685 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.matmul 	 paddle.matmul(Tensor([4, 8, 499, 3600],"float32"), Tensor([4, 8, 3600, 64],"float32"), ) 	 64857600 	 12781 	 18.463684558868408 	 18.47613763809204 	 1.477252721786499 	 1.478623628616333 	 18.331199407577515 	 18.334869146347046 	 0.7327477931976318 	 0.7341148853302002 	 
2025-08-05 04:08:42.300304 test begin: paddle.matmul(Tensor([512, 11, 256, 256],"float32"), Tensor([512, 11, 256, 36],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([512, 11, 256, 256],"float32"), Tensor([512, 11, 256, 36],"float32"), ) 	 421003264 	 12781 	 69.74482655525208 	 69.72929525375366 	 5.5766167640686035 	 5.575578689575195 	 95.07276010513306 	 95.03831148147583 	 3.7993838787078857 	 3.800210952758789 	 
2025-08-05 04:14:19.811066 test begin: paddle.matmul(Tensor([512, 2, 256, 256],"float32"), Tensor([512, 2, 256, 36],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([512, 2, 256, 256],"float32"), Tensor([512, 2, 256, 36],"float32"), ) 	 76546048 	 12781 	 12.773957967758179 	 12.795475721359253 	 1.0211195945739746 	 1.0212671756744385 	 17.448084354400635 	 17.45267605781555 	 0.6975042819976807 	 0.6975448131561279 	 
2025-08-05 04:15:23.391630 test begin: paddle.matmul(Tensor([512, 4, 256, 256],"float32"), Tensor([512, 4, 256, 97],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([512, 4, 256, 256],"float32"), Tensor([512, 4, 256, 97],"float32"), ) 	 185073664 	 12781 	 25.56497621536255 	 25.571025848388672 	 2.0452098846435547 	 2.0436923503875732 	 47.2442729473114 	 47.27932620048523 	 1.8885865211486816 	 1.890172004699707 	 
2025-08-05 04:17:53.980219 test begin: paddle.matmul(Tensor([512, 4, 97, 256],"float32"), Tensor([512, 4, 256, 36],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([512, 4, 97, 256],"float32"), Tensor([512, 4, 256, 36],"float32"), ) 	 69730304 	 12781 	 12.747309446334839 	 12.828021049499512 	 1.0191099643707275 	 1.0944435596466064 	 15.636007308959961 	 15.642980098724365 	 0.6250638961791992 	 0.6252734661102295 	 
2025-08-05 04:18:54.395653 test begin: paddle.matrix_transpose(Tensor([20, 12700801, 4],"float16"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 12700801, 4],"float16"), ) 	 1016064080 	 2360594 	 10.553014516830444 	 8.914508819580078 	 0.00012159347534179688 	 0.00025010108947753906 	 95.83370447158813 	 125.8464560508728 	 0.00010085105895996094 	 0.0002117156982421875 	 combined
2025-08-05 04:23:43.847509 test begin: paddle.matrix_transpose(Tensor([20, 3, 16934401],"float16"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 3, 16934401],"float16"), ) 	 1016064060 	 2360594 	 10.40575385093689 	 8.844072103500366 	 0.0001323223114013672 	 9.369850158691406e-05 	 94.99890613555908 	 126.3140230178833 	 0.00010323524475097656 	 0.0002143383026123047 	 combined
2025-08-05 04:28:27.180253 test begin: paddle.matrix_transpose(Tensor([20, 3, 4233601],"float64"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 3, 4233601],"float64"), ) 	 254016060 	 2360594 	 10.429065465927124 	 8.778316497802734 	 8.296966552734375e-05 	 0.0001201629638671875 	 95.0172324180603 	 126.17675399780273 	 9.942054748535156e-05 	 0.00020813941955566406 	 combined
2025-08-05 04:32:40.144758 test begin: paddle.matrix_transpose(Tensor([20, 3, 8467201],"float32"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 3, 8467201],"float32"), ) 	 508032060 	 2360594 	 10.344429731369019 	 8.8510000705719 	 0.00010919570922851562 	 0.00012612342834472656 	 94.27151417732239 	 127.54623556137085 	 0.000102996826171875 	 0.0002129077911376953 	 combined
2025-08-05 04:36:58.227539 test begin: paddle.matrix_transpose(Tensor([20, 3175201, 4],"float64"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 3175201, 4],"float64"), ) 	 254016080 	 2360594 	 10.32964539527893 	 8.797862768173218 	 6.985664367675781e-05 	 8.845329284667969e-05 	 94.52443337440491 	 126.90037298202515 	 0.00010180473327636719 	 0.00021982192993164062 	 combined
2025-08-05 04:41:11.505820 test begin: paddle.matrix_transpose(Tensor([20, 6350401, 4],"float32"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 6350401, 4],"float32"), ) 	 508032080 	 2360594 	 13.959101438522339 	 8.790456056594849 	 0.00014734268188476562 	 0.00037670135498046875 	 97.80809116363525 	 144.15984845161438 	 0.0001201629638671875 	 0.0002739429473876953 	 combined
2025-08-05 04:45:56.111946 test begin: paddle.matrix_transpose(Tensor([21168010, 3, 4],"float64"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([21168010, 3, 4],"float64"), ) 	 254016120 	 2360594 	 10.448238611221313 	 8.837928056716919 	 0.0002503395080566406 	 0.0001609325408935547 	 95.79871964454651 	 127.16186261177063 	 0.00012421607971191406 	 0.0002148151397705078 	 combined
2025-08-05 04:50:09.572857 test begin: paddle.matrix_transpose(Tensor([42336010, 3, 4],"float32"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([42336010, 3, 4],"float32"), ) 	 508032120 	 2360594 	 10.265217542648315 	 8.892653942108154 	 0.0001285076141357422 	 8.916854858398438e-05 	 95.52752089500427 	 126.4828929901123 	 0.00011563301086425781 	 0.00021219253540039062 	 combined
2025-08-05 04:54:27.947831 test begin: paddle.matrix_transpose(Tensor([84672010, 3, 4],"float16"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([84672010, 3, 4],"float16"), ) 	 1016064120 	 2360594 	 10.372911930084229 	 8.881601810455322 	 0.00012564659118652344 	 0.00013518333435058594 	 96.29207587242126 	 126.50364685058594 	 0.00011420249938964844 	 0.00021982192993164062 	 combined
2025-08-05 04:59:16.596467 test begin: paddle.max(Tensor([416, 50, 10, 256],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([416, 50, 10, 256],"float32"), axis=1, ) 	 53248000 	 65890 	 12.914084911346436 	 10.594419717788696 	 0.2001819610595703 	 0.16428375244140625 	 74.98086142539978 	 91.82074689865112 	 0.29062581062316895 	 0.2844560146331787 	 
2025-08-05 05:02:28.674646 test begin: paddle.max(Tensor([416, 50, 7, 349],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([416, 50, 7, 349],"float32"), axis=1, ) 	 50814400 	 65890 	 12.908019065856934 	 10.705749273300171 	 0.2001500129699707 	 0.1671462059020996 	 73.00077748298645 	 87.99029040336609 	 0.2829475402832031 	 0.2727165222167969 	 
2025-08-05 05:05:34.192819 test begin: paddle.max(Tensor([416, 69, 7, 256],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([416, 69, 7, 256],"float32"), axis=1, ) 	 51437568 	 65890 	 12.676552772521973 	 10.228545188903809 	 0.19652605056762695 	 0.15858149528503418 	 72.10530066490173 	 88.18243837356567 	 0.2808961868286133 	 0.2732574939727783 	 
2025-08-05 05:08:40.060966 test begin: paddle.max(Tensor([49, 1024, 1024],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.max 	 paddle.max(Tensor([49, 1024, 1024],"float32"), axis=-1, keepdim=True, ) 	 51380224 	 65890 	 10.371482372283936 	 9.841758489608765 	 0.16081452369689941 	 0.15259814262390137 	 69.700124502182 	 84.91336464881897 	 0.27022862434387207 	 0.26310062408447266 	 
2025-08-05 05:11:38.029544 test begin: paddle.max(Tensor([512, 50, 7, 284],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([512, 50, 7, 284],"float32"), axis=1, ) 	 50892800 	 65890 	 13.358116626739502 	 10.227630615234375 	 0.2070612907409668 	 0.15856170654296875 	 73.66032075881958 	 88.00310826301575 	 0.28556227684020996 	 0.2727165222167969 	 
2025-08-05 05:14:44.182830 test begin: paddle.max(Tensor([512, 50, 8, 256],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([512, 50, 8, 256],"float32"), axis=1, ) 	 52428800 	 65890 	 12.759809732437134 	 10.410334348678589 	 0.1978151798248291 	 0.16128754615783691 	 74.07663488388062 	 90.44402289390564 	 0.2871286869049072 	 0.28020811080932617 	 
2025-08-05 05:17:52.929246 test begin: paddle.max(Tensor([512, 56, 7, 256],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([512, 56, 7, 256],"float32"), axis=1, ) 	 51380224 	 65890 	 12.930770635604858 	 10.137827634811401 	 0.20047879219055176 	 0.1570901870727539 	 72.17470455169678 	 88.29208588600159 	 0.2797701358795166 	 0.27350592613220215 	 
2025-08-05 05:20:57.608760 test begin: paddle.max(Tensor([568, 50, 7, 256],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([568, 50, 7, 256],"float32"), axis=1, ) 	 50892800 	 65890 	 12.50662636756897 	 10.104907035827637 	 0.19396138191223145 	 0.1566753387451172 	 72.07751965522766 	 87.84240126609802 	 0.28062891960144043 	 0.273453950881958 	 
2025-08-05 05:24:01.031139 test begin: paddle.max(Tensor([8, 1024, 6202],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.max 	 paddle.max(Tensor([8, 1024, 6202],"float32"), axis=-1, keepdim=True, ) 	 50806784 	 65890 	 10.004175424575806 	 10.759544134140015 	 0.15508604049682617 	 0.1668245792388916 	 69.13221025466919 	 85.01474785804749 	 0.26802802085876465 	 0.26346707344055176 	 
2025-08-05 05:26:56.931432 test begin: paddle.max(Tensor([8, 6202, 1024],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.max 	 paddle.max(Tensor([8, 6202, 1024],"float32"), axis=-1, keepdim=True, ) 	 50806784 	 65890 	 10.265836000442505 	 9.738475561141968 	 0.15911436080932617 	 0.15094232559204102 	 68.94259595870972 	 84.04219222068787 	 0.26850247383117676 	 0.2604033946990967 	 
2025-08-05 05:29:50.931970 test begin: paddle.maximum(Tensor([11585, 4386],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([11585, 4386],"float32"), Tensor([1],"float32"), ) 	 50811811 	 33726 	 10.010652542114258 	 10.270216941833496 	 0.3032717704772949 	 0.3110489845275879 	 24.990196228027344 	 111.50913333892822 	 0.2520260810852051 	 0.28102874755859375 	 
2025-08-05 05:32:33.596085 test begin: paddle.maximum(Tensor([120961, 420],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([120961, 420],"float32"), Tensor([1],"float32"), ) 	 50803621 	 33726 	 10.007738828659058 	 10.26924443244934 	 0.3031895160675049 	 0.3109774589538574 	 24.975818395614624 	 111.51909685134888 	 0.25180721282958984 	 0.281048059463501 	 
2025-08-05 05:35:12.175796 test begin: paddle.maximum(Tensor([121539, 418],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([121539, 418],"float32"), Tensor([1],"float32"), ) 	 50803303 	 33726 	 10.004034280776978 	 10.266616344451904 	 0.3030130863189697 	 0.3110036849975586 	 24.973806381225586 	 111.4015064239502 	 0.25179076194763184 	 0.2808380126953125 	 
2025-08-05 05:37:50.556093 test begin: paddle.maximum(Tensor([14877, 3415],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([14877, 3415],"float32"), Tensor([1],"float32"), ) 	 50804956 	 33726 	 10.010136365890503 	 10.273235082626343 	 0.30437302589416504 	 0.31116485595703125 	 24.987215757369995 	 111.33665633201599 	 0.2519197463989258 	 0.28061866760253906 	 
2025-08-05 05:40:31.296598 test begin: paddle.maximum(Tensor([16121, 3152],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([16121, 3152],"float32"), Tensor([1],"float32"), ) 	 50813393 	 33726 	 10.009114503860474 	 10.292662143707275 	 0.3032362461090088 	 0.31241464614868164 	 24.996079683303833 	 111.3371729850769 	 0.2520284652709961 	 0.2806131839752197 	 
2025-08-05 05:43:11.244780 test begin: paddle.maximum(Tensor([62643, 811],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([62643, 811],"float32"), Tensor([1],"float32"), ) 	 50803474 	 33726 	 10.008643865585327 	 10.271037817001343 	 0.3031926155090332 	 0.3109855651855469 	 24.980774879455566 	 111.47725129127502 	 0.2518188953399658 	 0.28095340728759766 	 
2025-08-05 05:45:50.025021 test begin: paddle.mean(Tensor([7573, 11, 1280],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([7573, 11, 1280],"bfloat16"), axis=1, ) 	 106627840 	 54851 	 11.521017074584961 	 10.768685340881348 	 0.21452760696411133 	 0.20040059089660645 	 19.066673755645752 	 24.71663808822632 	 0.3551137447357178 	 0.2301480770111084 	 
2025-08-05 05:46:58.493508 test begin: paddle.mean(Tensor([7573, 8, 1678],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([7573, 8, 1678],"bfloat16"), axis=1, ) 	 101659952 	 54851 	 10.159917831420898 	 10.012900829315186 	 0.18912577629089355 	 0.18635010719299316 	 19.15871238708496 	 24.318291187286377 	 0.3567664623260498 	 0.22640514373779297 	 
2025-08-05 05:48:05.367521 test begin: paddle.mean(Tensor([7710, 11, 1280],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([7710, 11, 1280],"bfloat16"), axis=1, ) 	 108556800 	 54851 	 11.750861167907715 	 11.510452270507812 	 0.21876001358032227 	 0.2033839225769043 	 19.315365076065063 	 25.152833938598633 	 0.3599586486816406 	 0.23420429229736328 	 
2025-08-05 05:49:16.364370 test begin: paddle.mean(Tensor([7710, 8, 1648],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([7710, 8, 1648],"bfloat16"), axis=1, ) 	 101648640 	 54851 	 10.180929899215698 	 9.81872820854187 	 0.18963956832885742 	 0.1824498176574707 	 19.192059993743896 	 24.455053091049194 	 0.3573887348175049 	 0.22913551330566406 	 
2025-08-05 05:50:23.207098 test begin: paddle.mean(Tensor([8162, 10, 1280],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([8162, 10, 1280],"bfloat16"), axis=1, ) 	 104473600 	 54851 	 11.26044487953186 	 10.526401281356812 	 0.20971035957336426 	 0.19573354721069336 	 18.94726586341858 	 24.59354019165039 	 0.35294389724731445 	 0.2289872169494629 	 
2025-08-05 05:51:33.926840 test begin: paddle.mean(Tensor([8162, 8, 1557],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([8162, 8, 1557],"bfloat16"), axis=1, ) 	 101665872 	 54851 	 10.160560846328735 	 11.867390394210815 	 0.18925905227661133 	 0.2203996181488037 	 19.203615188598633 	 25.26157784461975 	 0.3576469421386719 	 0.23519444465637207 	 
2025-08-05 05:52:43.766550 test begin: paddle.mean(Tensor([9923, 8, 1280],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([9923, 8, 1280],"bfloat16"), axis=1, ) 	 101611520 	 54851 	 10.003445863723755 	 9.694469690322876 	 0.18628334999084473 	 0.1801462173461914 	 19.244620323181152 	 24.832693815231323 	 0.35837531089782715 	 0.23122096061706543 	 
2025-08-05 05:53:49.549419 test begin: paddle.median(Tensor([2, 254016],"float32"), axis=1, mode="min", )
[Prof] paddle.median 	 paddle.median(Tensor([2, 254016],"float32"), axis=1, mode="min", ) 	 508032 	 3680 	 35.28916692733765 	 4.001901865005493 	 0.19379711151123047 	 1.1108055114746094 	 None 	 None 	 None 	 None 	 combined
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:54:29.975819 test begin: paddle.median(Tensor([254016],"int64"), )
W0805 05:54:51.041142 80496 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.median 	 paddle.median(Tensor([254016],"int64"), ) 	 254016 	 3680 	 21.052018880844116 	 0.6417756080627441 	 0.24236607551574707 	 0.007077217102050781 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:54:51.755958 test begin: paddle.median(Tensor([5080, 100],"float32"), axis=1, mode="min", )
[Prof] paddle.median 	 paddle.median(Tensor([5080, 100],"float32"), axis=1, mode="min", ) 	 508000 	 3680 	 9.959776163101196 	 0.16155171394348145 	 0.07030797004699707 	 0.044881582260131836 	 None 	 None 	 None 	 None 	 combined
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:55:02.825628 test begin: paddle.median(Tensor([508032],"float32"), )
[Prof] paddle.median 	 paddle.median(Tensor([508032],"float32"), ) 	 508032 	 3680 	 25.116867542266846 	 0.5871996879577637 	 0.38523006439208984 	 0.009476423263549805 	 1.8703877925872803 	 0.5615880489349365 	 0.043553829193115234 	 7.796287536621094e-05 	 combined
2025-08-05 05:55:31.001500 test begin: paddle.min(Tensor([10, 10, 28225, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.min 	 paddle.min(Tensor([10, 10, 28225, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402502 	 65799 	 21.368583917617798 	 11.526952505111694 	 0.00021839141845703125 	 0.0895085334777832 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:57:19.515161 test begin: paddle.min(Tensor([10, 10, 9, 28225],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.min 	 paddle.min(Tensor([10, 10, 9, 28225],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402502 	 65799 	 20.374809503555298 	 10.703527450561523 	 0.0001881122589111328 	 0.16727209091186523 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:59:08.642654 test begin: paddle.min(Tensor([10, 31361, 9, 9],"float64"), Tensor([2],"int64"), )
[Prof] paddle.min 	 paddle.min(Tensor([10, 31361, 9, 9],"float64"), Tensor([2],"int64"), ) 	 25402412 	 65799 	 14.440776586532593 	 12.808236122131348 	 0.00017189979553222656 	 0.09951329231262207 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 06:00:49.935321 test begin: paddle.min(Tensor([10, 31361, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6217e06980>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 06:10:55.278627 test begin: paddle.min(Tensor([10, 5, 56449, 9],"float64"), Tensor([2],"int64"), )
W0805 06:10:55.959734 115633 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f28b020ac20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 06:20:59.811472 test begin: paddle.min(Tensor([10, 5, 9, 56449],"float64"), Tensor([2],"int64"), )
W0805 06:21:00.553326 141088 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.min 	 paddle.min(Tensor([10, 5, 9, 56449],"float64"), Tensor([2],"int64"), ) 	 25402052 	 65799 	 13.223426818847656 	 11.219914674758911 	 0.00019621849060058594 	 0.08720231056213379 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 06:22:38.498463 test begin: paddle.min(Tensor([31361, 10, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.min 	 paddle.min(Tensor([31361, 10, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402412 	 65799 	 22.65334463119507 	 12.103665590286255 	 0.00023102760314941406 	 0.09391260147094727 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 06:24:31.649067 test begin: paddle.min(Tensor([62721, 5, 9, 9],"float64"), Tensor([2],"int64"), )
[Prof] paddle.min 	 paddle.min(Tensor([62721, 5, 9, 9],"float64"), Tensor([2],"int64"), ) 	 25402007 	 65799 	 14.74090838432312 	 13.629295349121094 	 0.00016880035400390625 	 0.10582590103149414 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 06:26:12.683844 test begin: paddle.min(Tensor([64, 1, 28, 28351],"float32"), )
[Prof] paddle.min 	 paddle.min(Tensor([64, 1, 28, 28351],"float32"), ) 	 50804992 	 65799 	 10.00747561454773 	 10.074158430099487 	 0.0788731575012207 	 0.07817840576171875 	 68.74244737625122 	 82.20949411392212 	 0.21355390548706055 	 0.21256804466247559 	 
2025-08-05 06:29:04.808263 test begin: paddle.min(Tensor([64, 1, 28351, 28],"float32"), )
[Prof] paddle.min 	 paddle.min(Tensor([64, 1, 28351, 28],"float32"), ) 	 50804992 	 65799 	 10.003856897354126 	 10.066075563430786 	 0.07763099670410156 	 0.07815122604370117 	 68.75872564315796 	 82.21373987197876 	 0.2136228084564209 	 0.2126164436340332 	 
2025-08-05 06:31:56.903096 test begin: paddle.min(Tensor([64, 1013, 28, 28],"float32"), )
[Prof] paddle.min 	 paddle.min(Tensor([64, 1013, 28, 28],"float32"), ) 	 50828288 	 65799 	 9.999611616134644 	 10.081270933151245 	 0.07764101028442383 	 0.07825994491577148 	 68.78195214271545 	 82.25223898887634 	 0.21367955207824707 	 0.21271514892578125 	 
2025-08-05 06:34:49.047651 test begin: paddle.min(Tensor([64801, 1, 28, 28],"float32"), )
[Prof] paddle.min 	 paddle.min(Tensor([64801, 1, 28, 28],"float32"), ) 	 50803984 	 65799 	 9.987662315368652 	 10.07484221458435 	 0.07748866081237793 	 0.0782616138458252 	 68.72676730155945 	 82.19689345359802 	 0.21350836753845215 	 0.21251916885375977 	 
2025-08-05 06:37:42.993960 test begin: paddle.minimum(Tensor([13, 1, 113],"float32"), Tensor([451143, 113],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([13, 1, 113],"float32"), Tensor([451143, 113],"float32"), ) 	 50980628 	 3827 	 14.506511926651001 	 15.695003509521484 	 3.8747475147247314 	 2.093273162841797 	 73.11919832229614 	 180.37744855880737 	 4.875141620635986 	 2.184532403945923 	 
2025-08-05 06:42:42.790923 test begin: paddle.minimum(Tensor([13, 1, 2],"float32"), Tensor([25401601, 2],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([13, 1, 2],"float32"), Tensor([25401601, 2],"float32"), ) 	 50803228 	 3827 	 14.452441692352295 	 15.61181640625 	 3.8586905002593994 	 2.083150863647461 	 124.6843056678772 	 178.80582523345947 	 8.313051223754883 	 2.1625592708587646 	 
2025-08-05 06:48:29.738905 test begin: paddle.minimum(Tensor([16, 1, 113],"float32"), Tensor([451143, 113],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([16, 1, 113],"float32"), Tensor([451143, 113],"float32"), ) 	 50980967 	 3827 	 17.842398166656494 	 19.282134771347046 	 4.7666990756988525 	 2.575543165206909 	 88.98860478401184 	 219.49824118614197 	 5.9340386390686035 	 2.65516996383667 	 
2025-08-05 06:54:32.159048 test begin: paddle.minimum(Tensor([16, 1, 2],"float32"), Tensor([25401601, 2],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([16, 1, 2],"float32"), Tensor([25401601, 2],"float32"), ) 	 50803234 	 3827 	 17.77012538909912 	 19.196880340576172 	 4.747381925582886 	 2.561673641204834 	 148.9005696773529 	 224.97267627716064 	 9.930514574050903 	 2.7225358486175537 	 
2025-08-05 07:01:41.863334 test begin: paddle.minimum(Tensor([9, 1, 113],"float32"), Tensor([451143, 113],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([9, 1, 113],"float32"), Tensor([451143, 113],"float32"), ) 	 50980176 	 3827 	 10.06542682647705 	 10.846383810043335 	 2.6860969066619873 	 2.896199941635132 	 51.61446666717529 	 122.7249174118042 	 3.44185733795166 	 2.5194270610809326 	 
2025-08-05 07:05:05.872090 test begin: paddle.minimum(Tensor([9, 1, 2],"float32"), Tensor([25401601, 2],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([9, 1, 2],"float32"), Tensor([25401601, 2],"float32"), ) 	 50803220 	 3827 	 10.011722803115845 	 10.815183639526367 	 2.673807144165039 	 2.881547212600708 	 90.84434175491333 	 122.5057601928711 	 6.059469223022461 	 2.512115240097046 	 
2025-08-05 07:09:11.928333 test begin: paddle.mm(Tensor([1838, 6, 144, 144],"float32"), Tensor([1838, 6, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([1838, 6, 144, 144],"float32"), Tensor([1838, 6, 144, 32],"float32"), ) 	 279493632 	 7310 	 44.3489511013031 	 44.35166907310486 	 6.2018818855285645 	 6.202316522598267 	 69.45919394493103 	 69.47734665870667 	 4.854424476623535 	 4.8573081493377686 	 
2025-08-05 07:13:07.307772 test begin: paddle.mm(Tensor([2048, 2, 144, 144],"float32"), Tensor([2048, 2, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([2048, 2, 144, 144],"float32"), Tensor([2048, 2, 144, 32],"float32"), ) 	 103809024 	 7310 	 16.540806531906128 	 16.534907817840576 	 2.3131508827209473 	 2.309861660003662 	 25.90623164176941 	 25.911831378936768 	 1.8126485347747803 	 1.8109889030456543 	 
2025-08-05 07:14:34.534119 test begin: paddle.mm(Tensor([2048, 6, 144, 144],"float32"), Tensor([2048, 6, 144, 29],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([2048, 6, 144, 144],"float32"), Tensor([2048, 6, 144, 29],"float32"), ) 	 306118656 	 7310 	 49.366339921951294 	 49.36119318008423 	 6.901940822601318 	 6.899927139282227 	 75.41281652450562 	 75.40521216392517 	 5.272068500518799 	 5.2707884311676025 	 
2025-08-05 07:18:53.174943 test begin: paddle.mm(Tensor([2048, 6, 144, 144],"float32"), Tensor([2048, 6, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([2048, 6, 144, 144],"float32"), Tensor([2048, 6, 144, 32],"float32"), ) 	 311427072 	 7310 	 49.35922980308533 	 49.985488414764404 	 6.904357433319092 	 6.905902147293091 	 77.32632684707642 	 77.35605883598328 	 5.406266450881958 	 5.406653165817261 	 
2025-08-05 07:23:16.160611 test begin: paddle.mm(Tensor([2048, 6, 29, 144],"float32"), Tensor([2048, 6, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([2048, 6, 29, 144],"float32"), Tensor([2048, 6, 144, 32],"float32"), ) 	 107937792 	 7310 	 24.722211837768555 	 24.72564125061035 	 3.453857421875 	 3.455782175064087 	 27.354187965393066 	 27.34647512435913 	 1.9145288467407227 	 1.9129307270050049 	 
2025-08-05 07:25:02.587352 test begin: paddle.mm(Tensor([2757, 4, 144, 144],"float32"), Tensor([2757, 4, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([2757, 4, 144, 144],"float32"), Tensor([2757, 4, 144, 32],"float32"), ) 	 279493632 	 7310 	 44.34794497489929 	 44.348148584365845 	 6.20109224319458 	 6.202260494232178 	 69.46570563316345 	 69.46118760108948 	 4.855908632278442 	 4.856837272644043 	 
2025-08-05 07:28:56.962574 test begin: paddle.mm(Tensor([3840, 1, 144, 144],"float32"), Tensor([3840, 1, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([3840, 1, 144, 144],"float32"), Tensor([3840, 1, 144, 32],"float32"), ) 	 97320960 	 7310 	 15.6048583984375 	 17.169751167297363 	 2.1813440322875977 	 2.1852784156799316 	 24.413561582565308 	 24.426632165908813 	 1.7055976390838623 	 1.708974838256836 	 
2025-08-05 07:30:23.602640 test begin: paddle.mm(Tensor([3840, 1, 144, 144],"float32"), Tensor([3840, 4, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([3840, 1, 144, 144],"float32"), Tensor([3840, 4, 144, 32],"float32"), ) 	 150405120 	 7310 	 69.66839647293091 	 72.43946242332458 	 0.00010657310485839844 	 5.060878753662109 	 106.91575622558594 	 104.96251797676086 	 0.0011932849884033203 	 4.887085676193237 	 
2025-08-05 07:36:23.546605 test begin: paddle.mm(Tensor([3840, 3, 144, 144],"float32"), Tensor([3840, 3, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([3840, 3, 144, 144],"float32"), Tensor([3840, 3, 144, 32],"float32"), ) 	 291962880 	 7310 	 46.293859243392944 	 46.285306215286255 	 6.473790884017944 	 6.473083019256592 	 72.50293111801147 	 72.50072193145752 	 5.068318843841553 	 5.068641424179077 	 
2025-08-05 07:40:29.491869 test begin: paddle.mm(Tensor([3840, 4, 144, 144],"float32"), Tensor([3840, 4, 144, 23],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([3840, 4, 144, 144],"float32"), Tensor([3840, 4, 144, 23],"float32"), ) 	 369377280 	 7310 	 61.650920152664185 	 61.66871976852417 	 8.621502161026001 	 8.624863147735596 	 87.35710978507996 	 87.40185022354126 	 6.107383728027344 	 6.110747575759888 	 
2025-08-05 07:45:43.156308 test begin: paddle.mm(Tensor([3840, 4, 23, 144],"float32"), Tensor([3840, 4, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([3840, 4, 23, 144],"float32"), Tensor([3840, 4, 144, 32],"float32"), ) 	 121651200 	 7310 	 30.91186833381653 	 30.915664434432983 	 4.321733474731445 	 4.322849273681641 	 30.89747166633606 	 30.8894522190094 	 2.1583092212677 	 2.159075975418091 	 
2025-08-05 07:47:49.290375 test begin: paddle.mm(Tensor([409, 6, 144, 144],"float32"), Tensor([409, 6, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([409, 6, 144, 144],"float32"), Tensor([409, 6, 144, 32],"float32"), ) 	 62194176 	 7310 	 10.015967845916748 	 10.030097246170044 	 1.3991198539733887 	 1.401545763015747 	 15.656880855560303 	 15.654686212539673 	 1.095078945159912 	 1.0950343608856201 	 
2025-08-05 07:48:41.940759 test begin: paddle.mm(Tensor([4096, 1, 144, 144],"float32"), Tensor([4096, 1, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([4096, 1, 144, 144],"float32"), Tensor([4096, 1, 144, 32],"float32"), ) 	 103809024 	 7310 	 16.536173105239868 	 16.5274338722229 	 2.3107151985168457 	 2.3131051063537598 	 25.899549961090088 	 25.902392387390137 	 1.8133056163787842 	 1.8105475902557373 	 
2025-08-05 07:50:08.909349 test begin: paddle.mm(Tensor([4096, 1, 144, 144],"float32"), Tensor([4096, 4, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([4096, 1, 144, 144],"float32"), Tensor([4096, 4, 144, 32],"float32"), ) 	 160432128 	 7310 	 74.19734072685242 	 77.19114089012146 	 0.00011801719665527344 	 5.394576787948608 	 113.71095371246338 	 111.95853638648987 	 0.0012717247009277344 	 5.214339971542358 	 
2025-08-05 07:56:30.121301 test begin: paddle.mm(Tensor([4096, 3, 144, 144],"float32"), Tensor([4096, 3, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([4096, 3, 144, 144],"float32"), Tensor([4096, 3, 144, 32],"float32"), ) 	 311427072 	 7310 	 49.363603591918945 	 49.336917877197266 	 6.8975584506988525 	 6.895899772644043 	 77.27775359153748 	 77.28587365150452 	 5.398951053619385 	 5.402188539505005 	 
2025-08-05 08:00:49.439106 test begin: paddle.mm(Tensor([4096, 4, 144, 144],"float32"), Tensor([4096, 4, 144, 22],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([4096, 4, 144, 144],"float32"), Tensor([4096, 4, 144, 22],"float32"), ) 	 391643136 	 7310 	 65.73925447463989 	 66.42989182472229 	 9.193378925323486 	 9.19345998764038 	 93.16441917419434 	 93.2431960105896 	 6.514384508132935 	 6.518907070159912 	 
Error: Can not import paddle core while this file exists: /usr/local/lib/python3.10/dist-packages/paddle/base/libpaddle.so
KeyboardInterrupt

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 9, in <module>
    from tester import (APIConfig, APITestAccuracy, APITestCINNVSDygraph,
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/__init__.py", line 74, in __getattr__
    from .api_config import APIConfig
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/__init__.py", line 22, in __getattr__
    from .config_analyzer import APIConfig
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py", line 8, in <module>
    import paddle
  File "/usr/local/lib/python3.10/dist-packages/paddle/__init__.py", line 38, in <module>
    from .base import core  # noqa: F401
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/__init__.py", line 38, in <module>
    from . import (  # noqa: F401
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/backward.py", line 28, in <module>
    from . import core, framework, log_helper, unique_name
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/core.py", line 388, in <module>
    raise e
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/core.py", line 267, in <module>
    from . import libpaddle
ImportError: initialization failed
2025-08-04 11:30:52.953919 test begin: paddle.mm(Tensor([4096, 4, 22, 144],"float32"), Tensor([4096, 4, 144, 32],"float32"), )
W0804 11:30:55.281075 85182 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.mm 	 paddle.mm(Tensor([4096, 4, 22, 144],"float32"), Tensor([4096, 4, 144, 32],"float32"), ) 	 127401984 	 7310 	 32.82817530632019 	 32.81664299964905 	 4.589702844619751 	 4.587550640106201 	 32.799176931381226 	 32.77571153640747 	 2.291754722595215 	 2.2908833026885986 	 
2025-08-04 11:33:10.443562 test begin: paddle.mm(Tensor([613, 4, 144, 144],"float32"), Tensor([613, 4, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([613, 4, 144, 144],"float32"), Tensor([613, 4, 144, 32],"float32"), ) 	 62143488 	 7310 	 9.988147974014282 	 10.000271558761597 	 1.3963942527770996 	 1.3963518142700195 	 15.846531629562378 	 15.606604099273682 	 1.330420970916748 	 1.090897798538208 	 
2025-08-04 11:34:05.575510 test begin: paddle.mod(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), ) 	 50803220 	 22295 	 9.984462261199951 	 9.962805032730103 	 0.45771217346191406 	 0.45664143562316895 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:34:45.264330 test begin: paddle.mod(Tensor([10, 5080321],"int32"), Tensor([10, 5080321],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([10, 5080321],"int32"), Tensor([10, 5080321],"int32"), ) 	 101606420 	 22295 	 10.05528736114502 	 10.016188621520996 	 0.46060776710510254 	 0.45914411544799805 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:35:33.378497 test begin: paddle.mod(Tensor([1270081, 2, 4, 5],"int32"), Tensor([1270081, 2, 4, 5],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([1270081, 2, 4, 5],"int32"), Tensor([1270081, 2, 4, 5],"int32"), ) 	 101606480 	 22295 	 10.051474809646606 	 10.016539096832275 	 0.4607076644897461 	 0.45917677879333496 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:36:23.624538 test begin: paddle.mod(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), ) 	 50804736 	 22295 	 9.994866371154785 	 9.96115255355835 	 0.4579007625579834 	 0.4566366672515869 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:37:01.583739 test begin: paddle.mod(Tensor([2540161, 20],"int32"), Tensor([2540161, 20],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([2540161, 20],"int32"), Tensor([2540161, 20],"int32"), ) 	 101606440 	 22295 	 10.05062747001648 	 10.970755100250244 	 0.46073317527770996 	 0.4592430591583252 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:37:53.898293 test begin: paddle.mod(Tensor([6, 2, 4, 1058401],"int32"), Tensor([6, 2, 4, 1058401],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([6, 2, 4, 1058401],"int32"), Tensor([6, 2, 4, 1058401],"int32"), ) 	 101606496 	 22295 	 10.051103591918945 	 10.018040418624878 	 0.4607863426208496 	 0.459247350692749 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:38:43.810925 test begin: paddle.mod(Tensor([6, 2, 846721, 5],"int32"), Tensor([6, 2, 846721, 5],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([6, 2, 846721, 5],"int32"), Tensor([6, 2, 846721, 5],"int32"), ) 	 101606520 	 22295 	 10.374371767044067 	 10.017988204956055 	 0.4606821537017822 	 0.45920634269714355 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:39:34.293299 test begin: paddle.mod(Tensor([6, 423361, 4, 5],"int32"), Tensor([6, 423361, 4, 5],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([6, 423361, 4, 5],"int32"), Tensor([6, 423361, 4, 5],"int32"), ) 	 101606640 	 22295 	 10.05124807357788 	 10.019628763198853 	 0.4606621265411377 	 0.4591825008392334 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:40:28.061484 test begin: paddle.mode(Tensor([2, 10, 12],"float64"), -1, )
[Prof] paddle.mode 	 paddle.mode(Tensor([2, 10, 12],"float64"), -1, ) 	 240 	 1169 	 10.939589262008667 	 0.03156113624572754 	 9.560585021972656e-05 	 5.0067901611328125e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:40:39.234250 test begin: paddle.mode(Tensor([2, 10, 12],"float64"), -1, keepdim=True, )
[Prof] paddle.mode 	 paddle.mode(Tensor([2, 10, 12],"float64"), -1, keepdim=True, ) 	 240 	 1169 	 10.101086854934692 	 0.019553661346435547 	 9.799003601074219e-05 	 3.7670135498046875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:40:49.434302 test begin: paddle.mode(Tensor([2, 10, 12],"float64"), 1, )
[Prof] paddle.mode 	 paddle.mode(Tensor([2, 10, 12],"float64"), 1, ) 	 240 	 1169 	 12.1726393699646 	 0.03695368766784668 	 9.846687316894531e-05 	 5.698204040527344e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:41:01.755539 test begin: paddle.mode(Tensor([2, 12, 10],"float64"), -1, )
[Prof] paddle.mode 	 paddle.mode(Tensor([2, 12, 10],"float64"), -1, ) 	 240 	 1169 	 12.1242995262146 	 0.02237391471862793 	 9.512901306152344e-05 	 4.744529724121094e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:41:15.148953 test begin: paddle.mode(Tensor([2, 12, 10],"float64"), -1, keepdim=True, )
[Prof] paddle.mode 	 paddle.mode(Tensor([2, 12, 10],"float64"), -1, keepdim=True, ) 	 240 	 1169 	 12.160172462463379 	 0.01956319808959961 	 8.96453857421875e-05 	 3.075599670410156e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:41:27.411008 test begin: paddle.mode(Tensor([2, 12, 10],"float64"), 1, )
[Prof] paddle.mode 	 paddle.mode(Tensor([2, 12, 10],"float64"), 1, ) 	 240 	 1169 	 10.272780179977417 	 0.05228686332702637 	 9.632110595703125e-05 	 6.413459777832031e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:41:38.463440 test begin: paddle.moveaxis(Tensor([20, 3, 120961, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([20, 3, 120961, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 254018100 	 1394939 	 11.058640718460083 	 8.129031658172607 	 0.00012230873107910156 	 0.0002472400665283203 	 58.04714584350586 	 76.8624496459961 	 0.00011706352233886719 	 0.0002105236053466797 	 
2025-08-04 11:44:23.738295 test begin: paddle.moveaxis(Tensor([20, 3, 4, 151201, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([20, 3, 4, 151201, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 254017680 	 1394939 	 11.191552877426147 	 12.30506420135498 	 0.0001304149627685547 	 0.0002655982971191406 	 57.484561920166016 	 77.8503303527832 	 0.00010275840759277344 	 0.00021457672119140625 	 
2025-08-04 11:47:15.310125 test begin: paddle.moveaxis(Tensor([20, 3, 4, 5, 211681],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([20, 3, 4, 5, 211681],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 254017200 	 1394939 	 11.161912202835083 	 8.178595781326294 	 7.200241088867188e-05 	 8.606910705566406e-05 	 57.60602807998657 	 78.09172821044922 	 0.00010943412780761719 	 0.00021147727966308594 	 
2025-08-04 11:50:03.016583 test begin: paddle.moveaxis(Tensor([20, 90721, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([20, 90721, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 254018800 	 1394939 	 10.928030252456665 	 8.143234491348267 	 0.00011658668518066406 	 0.0002655982971191406 	 57.86358451843262 	 76.76592254638672 	 0.00010967254638671875 	 0.00020933151245117188 	 
2025-08-04 11:52:49.745101 test begin: paddle.moveaxis(Tensor([604810, 3, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([604810, 3, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 254020200 	 1394939 	 11.296777248382568 	 8.135161876678467 	 0.00011491775512695312 	 0.00012421607971191406 	 57.82480216026306 	 77.43461847305298 	 0.00010418891906738281 	 0.00021600723266601562 	 
2025-08-04 11:55:38.887567 test begin: paddle.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=0, destination=2, ) 	 254018100 	 1394939 	 9.649807453155518 	 6.298733472824097 	 7.62939453125e-05 	 0.0001277923583984375 	 57.587810754776 	 77.00222849845886 	 0.0001087188720703125 	 0.00021219253540039062 	 
2025-08-04 11:58:20.447643 test begin: paddle.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254018100 	 1394939 	 10.271755456924438 	 8.094578981399536 	 0.0001010894775390625 	 0.00025844573974609375 	 57.51384520530701 	 76.46384859085083 	 0.00011014938354492188 	 0.0002143383026123047 	 
2025-08-04 12:01:06.883216 test begin: paddle.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=0, destination=2, ) 	 254017680 	 1394939 	 9.473404169082642 	 6.4381513595581055 	 0.00011730194091796875 	 8.702278137207031e-05 	 57.607306480407715 	 76.1991114616394 	 0.00010967254638671875 	 0.0002155303955078125 	 
2025-08-04 12:03:47.582618 test begin: paddle.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254017680 	 1394939 	 10.322973489761353 	 8.089050769805908 	 0.00011491775512695312 	 0.00012350082397460938 	 57.43123388290405 	 77.33184909820557 	 0.00010013580322265625 	 0.00020623207092285156 	 
2025-08-04 12:06:31.997390 test begin: paddle.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=0, destination=2, ) 	 254017200 	 1394939 	 9.423546075820923 	 6.345975160598755 	 0.00012421607971191406 	 0.00011801719665527344 	 58.077438831329346 	 77.02387189865112 	 0.000110626220703125 	 0.00021409988403320312 	 
2025-08-04 12:09:14.956881 test begin: paddle.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254017200 	 1394939 	 10.364840507507324 	 8.080052852630615 	 7.653236389160156e-05 	 8.893013000488281e-05 	 57.6875855922699 	 78.07900214195251 	 0.00011205673217773438 	 0.00022125244140625 	 
2025-08-04 12:12:01.972219 test begin: paddle.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=0, destination=2, ) 	 254018800 	 1394939 	 9.623572587966919 	 6.359260559082031 	 5.173683166503906e-05 	 0.0002486705780029297 	 58.416157960891724 	 76.71476912498474 	 0.00010657310485839844 	 0.0002219676971435547 	 
2025-08-04 12:14:44.367925 test begin: paddle.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254018800 	 1394939 	 10.367123126983643 	 8.08279037475586 	 0.00012755393981933594 	 0.00012636184692382812 	 58.49502444267273 	 76.23334193229675 	 0.00010013580322265625 	 0.00022101402282714844 	 
2025-08-04 12:17:29.071763 test begin: paddle.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=0, destination=2, ) 	 254020200 	 1394939 	 9.597205638885498 	 11.694514274597168 	 5.745887756347656e-05 	 8.463859558105469e-05 	 65.89934587478638 	 77.72668552398682 	 0.00011658668518066406 	 0.00021529197692871094 	 
2025-08-04 12:20:25.559536 test begin: paddle.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254020200 	 1394939 	 17.613518953323364 	 8.08738398551941 	 0.00014519691467285156 	 8.654594421386719e-05 	 60.29179239273071 	 76.64935255050659 	 0.00011014938354492188 	 0.00021505355834960938 	 
2025-08-04 12:23:20.221898 test begin: paddle.multigammaln(Tensor([10, 2540161],"float64"), 2, )
[Prof] paddle.multigammaln 	 paddle.multigammaln(Tensor([10, 2540161],"float64"), 2, ) 	 25401610 	 4159 	 12.285276412963867 	 11.64859914779663 	 0.5029044151306152 	 0.5722119808197021 	 16.77740454673767 	 15.259959936141968 	 1.031911849975586 	 0.7495322227478027 	 
2025-08-04 12:24:17.505454 test begin: paddle.multigammaln(Tensor([10, 5080321],"float32"), 2, )
[Prof] paddle.multigammaln 	 paddle.multigammaln(Tensor([10, 5080321],"float32"), 2, ) 	 50803210 	 4159 	 9.992039680480957 	 10.753770351409912 	 0.4094657897949219 	 0.5285942554473877 	 14.391781568527222 	 16.585784196853638 	 0.8819887638092041 	 0.8148736953735352 	 
2025-08-04 12:25:11.178986 test begin: paddle.multigammaln(Tensor([1270081, 20],"float64"), 2, )
[Prof] paddle.multigammaln 	 paddle.multigammaln(Tensor([1270081, 20],"float64"), 2, ) 	 25401620 	 4159 	 12.287495851516724 	 11.673971891403198 	 0.5029861927032471 	 0.5721137523651123 	 17.121618032455444 	 15.258443355560303 	 1.3798658847808838 	 0.7493236064910889 	 
2025-08-04 12:26:10.547769 test begin: paddle.multigammaln(Tensor([2540161, 20],"float32"), 2, )
[Prof] paddle.multigammaln 	 paddle.multigammaln(Tensor([2540161, 20],"float32"), 2, ) 	 50803220 	 4159 	 10.014605522155762 	 10.761136531829834 	 0.41051769256591797 	 0.5282402038574219 	 14.380716562271118 	 16.573545455932617 	 0.8844509124755859 	 0.8143439292907715 	 
2025-08-04 12:27:04.227148 test begin: paddle.multiplex(inputs=list[Tensor([12700801, 4],"float32"),Tensor([12700801, 4],"float32"),], index=Tensor([127, 1],"int32"), )
[Prof] paddle.multiplex 	 paddle.multiplex(inputs=list[Tensor([12700801, 4],"float32"),Tensor([12700801, 4],"float32"),], index=Tensor([127, 1],"int32"), ) 	 101606535 	 29797 	 13.922314643859863 	 103.71510529518127 	 9.107589721679688e-05 	 0.00023031234741210938 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 12:29:22.590975 test begin: paddle.multiplex(inputs=list[Tensor([12700801, 4],"float32"),Tensor([12700801, 4],"float32"),], index=Tensor([601, 1],"int32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f30ae01fe50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 12:39:32.470887 test begin: paddle.multiplex(inputs=list[Tensor([7, 7257601],"float32"),Tensor([7, 7257601],"float32"),], index=Tensor([6, 1],"int32"), )
W0804 12:39:34.492448 110545 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.multiplex 	 paddle.multiplex(inputs=list[Tensor([7, 7257601],"float32"),Tensor([7, 7257601],"float32"),], index=Tensor([6, 1],"int32"), ) 	 101606420 	 29797 	 10.042908668518066 	 13.82802152633667 	 0.0002758502960205078 	 0.00027298927307128906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 12:40:17.275730 test begin: paddle.multiply(Tensor([298, 872, 14, 14],"float32"), Tensor([298, 872, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([298, 872, 14, 14],"float32"), Tensor([298, 872, 1, 1],"float32"), ) 	 51191632 	 33806 	 10.049787998199463 	 10.379716157913208 	 0.3038034439086914 	 0.3136870861053467 	 29.68268370628357 	 30.82098913192749 	 0.4486708641052246 	 0.3103761672973633 	 
2025-08-04 12:41:40.135251 test begin: paddle.multiply(Tensor([512, 507, 14, 14],"float32"), Tensor([512, 507, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([512, 507, 14, 14],"float32"), Tensor([512, 507, 1, 1],"float32"), ) 	 51138048 	 33806 	 10.041892290115356 	 10.367307662963867 	 0.3033485412597656 	 0.3134005069732666 	 29.711721658706665 	 30.794395685195923 	 0.4491305351257324 	 0.3102223873138428 	 
2025-08-04 12:43:10.817244 test begin: paddle.multiply(Tensor([512, 872, 14, 9],"float32"), Tensor([512, 872, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([512, 872, 14, 9],"float32"), Tensor([512, 872, 1, 1],"float32"), ) 	 56700928 	 33806 	 11.09843373298645 	 11.486194849014282 	 0.3355379104614258 	 0.3469986915588379 	 30.297334671020508 	 34.79511475563049 	 0.4579634666442871 	 0.35052061080932617 	 
2025-08-04 12:44:44.850382 test begin: paddle.multiply(Tensor([512, 872, 14, 9],"float32"), Tensor([512, 872, 1, 9],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([512, 872, 14, 9],"float32"), Tensor([512, 872, 1, 9],"float32"), ) 	 60272640 	 33806 	 11.454452276229858 	 12.008475542068481 	 0.34564733505249023 	 0.3626573085784912 	 29.446622848510742 	 35.156362533569336 	 0.44506025314331055 	 0.3541252613067627 	 
2025-08-04 12:46:19.284038 test begin: paddle.multiply(Tensor([512, 872, 9, 14],"float32"), Tensor([512, 872, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([512, 872, 9, 14],"float32"), Tensor([512, 872, 1, 1],"float32"), ) 	 56700928 	 33806 	 11.099156379699707 	 11.481305122375488 	 0.335507869720459 	 0.34700512886047363 	 30.291940689086914 	 34.794079303741455 	 0.45782971382141113 	 0.3504910469055176 	 
2025-08-04 12:47:51.414839 test begin: paddle.multiply(Tensor([512, 872, 9, 14],"float32"), Tensor([512, 872, 9, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([512, 872, 9, 14],"float32"), Tensor([512, 872, 9, 1],"float32"), ) 	 60272640 	 33806 	 12.101988315582275 	 11.869641542434692 	 0.3457767963409424 	 0.3586156368255615 	 31.51409411430359 	 40.12948727607727 	 0.4763615131378174 	 0.40428757667541504 	 
2025-08-04 12:49:31.577127 test begin: paddle.multiply(x=Tensor([128, 127, 56, 56],"float32"), y=Tensor([128, 127, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 127, 56, 56],"float32"), y=Tensor([128, 127, 1, 1],"float32"), ) 	 50995072 	 33806 	 10.03329873085022 	 10.47268295288086 	 0.30336499214172363 	 0.31597018241882324 	 25.06373143196106 	 30.591528177261353 	 0.378887414932251 	 0.30813002586364746 	 
2025-08-04 12:50:51.437233 test begin: paddle.multiply(x=Tensor([128, 224, 32, 56],"float32"), y=Tensor([128, 224, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 224, 32, 56],"float32"), y=Tensor([128, 224, 1, 1],"float32"), ) 	 51408896 	 33806 	 10.110570669174194 	 10.575578212738037 	 0.30564308166503906 	 0.31973695755004883 	 25.206237316131592 	 30.85845708847046 	 0.3809816837310791 	 0.3108041286468506 	 
2025-08-04 12:52:11.760416 test begin: paddle.multiply(x=Tensor([128, 224, 32, 56],"float32"), y=Tensor([128, 224, 32, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 224, 32, 56],"float32"), y=Tensor([128, 224, 32, 1],"float32"), ) 	 52297728 	 33806 	 10.198722124099731 	 10.55443024635315 	 0.3083672523498535 	 0.3190882205963135 	 29.08094096183777 	 36.80972909927368 	 0.4395153522491455 	 0.37085771560668945 	 
2025-08-04 12:53:40.272363 test begin: paddle.multiply(x=Tensor([128, 224, 56, 32],"float32"), y=Tensor([128, 224, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 224, 56, 32],"float32"), y=Tensor([128, 224, 1, 1],"float32"), ) 	 51408896 	 33806 	 10.968616008758545 	 10.575504064559937 	 0.30568861961364746 	 0.31973981857299805 	 25.20548391342163 	 30.858434915542603 	 0.3809988498687744 	 0.3108482360839844 	 
2025-08-04 12:55:00.096078 test begin: paddle.multiply(x=Tensor([128, 224, 56, 32],"float32"), y=Tensor([128, 224, 1, 32],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 224, 56, 32],"float32"), y=Tensor([128, 224, 1, 32],"float32"), ) 	 52297728 	 33806 	 10.192200660705566 	 10.671323299407959 	 0.3080916404724121 	 0.3224923610687256 	 26.541858673095703 	 31.054540872573853 	 0.40118408203125 	 0.3128039836883545 	 
2025-08-04 12:56:21.071736 test begin: paddle.multiply(x=Tensor([128, 256, 28, 56],"float32"), y=Tensor([128, 256, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 256, 28, 56],"float32"), y=Tensor([128, 256, 1, 1],"float32"), ) 	 51412992 	 33806 	 10.110543251037598 	 10.542241096496582 	 0.30568838119506836 	 0.31872129440307617 	 25.303295135498047 	 30.87985372543335 	 0.3824501037597656 	 0.31107354164123535 	 
2025-08-04 12:57:43.687480 test begin: paddle.multiply(x=Tensor([128, 256, 28, 56],"float32"), y=Tensor([128, 256, 28, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 256, 28, 56],"float32"), y=Tensor([128, 256, 28, 1],"float32"), ) 	 52297728 	 33806 	 10.202437162399292 	 10.558701276779175 	 0.30829763412475586 	 0.3191237449645996 	 29.093419313430786 	 36.82883429527283 	 0.4397726058959961 	 0.37103915214538574 	 
2025-08-04 12:59:13.353967 test begin: paddle.multiply(x=Tensor([128, 256, 56, 28],"float32"), y=Tensor([128, 256, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 256, 56, 28],"float32"), y=Tensor([128, 256, 1, 1],"float32"), ) 	 51412992 	 33806 	 10.11179780960083 	 10.545228481292725 	 0.3056485652923584 	 0.31876659393310547 	 25.304094076156616 	 30.88001298904419 	 0.3824942111968994 	 0.3110361099243164 	 
2025-08-04 13:00:33.362495 test begin: paddle.multiply(x=Tensor([128, 256, 56, 28],"float32"), y=Tensor([128, 256, 1, 28],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 256, 56, 28],"float32"), y=Tensor([128, 256, 1, 28],"float32"), ) 	 52297728 	 33806 	 10.1956045627594 	 10.693105459213257 	 0.3082313537597656 	 0.32327985763549805 	 34.06138563156128 	 31.20022749900818 	 0.5148608684539795 	 0.31429362297058105 	 
2025-08-04 13:02:01.248414 test begin: paddle.multiply(x=Tensor([64, 256, 56, 56],"float32"), y=Tensor([64, 256, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([64, 256, 56, 56],"float32"), y=Tensor([64, 256, 1, 1],"float32"), ) 	 51396608 	 33806 	 10.107285022735596 	 10.534454345703125 	 0.3055379390716553 	 0.3181765079498291 	 25.240946292877197 	 30.82234811782837 	 0.38155150413513184 	 0.3104548454284668 	 
2025-08-04 13:03:20.274366 test begin: paddle.multiply(x=Tensor([73, 224, 56, 56],"float32"), y=Tensor([73, 224, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([73, 224, 56, 56],"float32"), y=Tensor([73, 224, 1, 1],"float32"), ) 	 51296224 	 33806 	 10.092520475387573 	 10.504423379898071 	 0.3051462173461914 	 0.3175485134124756 	 25.20041513442993 	 30.764645099639893 	 0.3809084892272949 	 0.30983614921569824 	 
2025-08-04 13:04:42.482887 test begin: paddle.mv(Tensor([1411201, 36],"float32"), Tensor([36],"float32"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([1411201, 36],"float32"), Tensor([36],"float32"), ) 	 50803272 	 64031 	 12.65233564376831 	 12.416558027267456 	 0.20168447494506836 	 0.1981499195098877 	 31.444554090499878 	 26.80399465560913 	 0.1671159267425537 	 0.1425316333770752 	 
2025-08-04 13:06:09.186221 test begin: paddle.mv(Tensor([254017, 100],"float64"), Tensor([100],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([254017, 100],"float64"), Tensor([100],"float64"), ) 	 25401800 	 64031 	 9.973671436309814 	 9.999117851257324 	 0.15936970710754395 	 0.15970706939697266 	 21.59895968437195 	 21.175402402877808 	 0.11485409736633301 	 0.11259675025939941 	 
2025-08-04 13:07:13.589019 test begin: paddle.mv(Tensor([3, 16934401],"float32"), Tensor([16934401],"float32"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([3, 16934401],"float32"), Tensor([16934401],"float32"), ) 	 67737604 	 64031 	 15.082568883895874 	 15.024172306060791 	 0.12035965919494629 	 0.11994600296020508 	 40.275059938430786 	 38.8011748790741 	 0.32137560844421387 	 0.309633731842041 	 
2025-08-04 13:09:03.951632 test begin: paddle.mv(Tensor([3, 50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([3, 50803201],"float32"), Tensor([50803201],"float32"), ) 	 203212804 	 64031 	 43.437235832214355 	 43.38328766822815 	 0.34662413597106934 	 0.3462684154510498 	 118.82097506523132 	 115.45410299301147 	 0.9481990337371826 	 0.921262264251709 	 
2025-08-04 13:14:28.497865 test begin: paddle.mv(Tensor([5, 25401601],"float64"), Tensor([25401601],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([5, 25401601],"float64"), Tensor([25401601],"float64"), ) 	 152409606 	 64031 	 60.62924361228943 	 60.32822108268738 	 0.4838721752166748 	 0.4814035892486572 	 153.75249028205872 	 154.23058032989502 	 1.2270917892456055 	 1.2307689189910889 	 
2025-08-04 13:21:47.485388 test begin: paddle.mv(Tensor([5, 5080321],"float64"), Tensor([5080321],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([5, 5080321],"float64"), Tensor([5080321],"float64"), ) 	 30481926 	 64031 	 14.88864278793335 	 14.325003862380981 	 0.11885833740234375 	 0.1143350601196289 	 31.424812078475952 	 31.545480728149414 	 0.25076842308044434 	 0.25171446800231934 	 
2025-08-04 13:23:20.399319 test begin: paddle.mv(Tensor([64, 396901],"float64"), Tensor([396901],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([64, 396901],"float64"), Tensor([396901],"float64"), ) 	 25798565 	 64031 	 9.990029335021973 	 9.90744137763977 	 0.07975339889526367 	 0.07908797264099121 	 20.874669551849365 	 21.217923641204834 	 0.16661548614501953 	 0.16924834251403809 	 
2025-08-04 13:24:23.435953 test begin: paddle.mv(Tensor([793801, 32],"float64"), Tensor([32],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([793801, 32],"float64"), Tensor([32],"float64"), ) 	 25401664 	 64031 	 10.994876623153687 	 10.403792381286621 	 0.1755049228668213 	 0.1660289764404297 	 21.420876264572144 	 21.5234375 	 0.1139366626739502 	 0.11436963081359863 	 
2025-08-04 13:25:28.412466 test begin: paddle.nan_to_num(Tensor([148, 114422, 3],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([148, 114422, 3],"float32"), neginf=-1.1920928955078125e-07, ) 	 50803368 	 3470 	 10.436628341674805 	 1.0426976680755615 	 0.27954649925231934 	 0.30434751510620117 	 4.359922885894775 	 4.024981737136841 	 0.42806291580200195 	 0.23708772659301758 	 
2025-08-04 13:25:50.190007 test begin: paddle.nan_to_num(Tensor([148, 5, 68653],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([148, 5, 68653],"float32"), neginf=-1.1920928955078125e-07, ) 	 50803220 	 3470 	 10.440163373947144 	 1.033566951751709 	 0.2796344757080078 	 0.30437636375427246 	 4.361202716827393 	 4.023043155670166 	 0.42815399169921875 	 0.2369551658630371 	 
2025-08-04 13:26:14.535079 test begin: paddle.nan_to_num(Tensor([1948, 26080],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([1948, 26080],"float32"), neginf=-1.1920928955078125e-07, ) 	 50803840 	 3470 	 10.440938472747803 	 1.033628225326538 	 0.2795896530151367 	 0.30438804626464844 	 4.370850086212158 	 4.0199830532073975 	 0.4291200637817383 	 0.23679494857788086 	 
2025-08-04 13:26:38.551676 test begin: paddle.nan_to_num(Tensor([25401601, 1],"float64"), neginf=-2.220446049250313e-16, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([25401601, 1],"float64"), neginf=-2.220446049250313e-16, ) 	 25401601 	 3470 	 10.016026496887207 	 1.0415620803833008 	 0.26810288429260254 	 0.3055107593536377 	 3.418346881866455 	 3.573423147201538 	 0.33558201789855957 	 0.21051454544067383 	 
2025-08-04 13:27:00.696902 test begin: paddle.nan_to_num(Tensor([25401601, 2],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([25401601, 2],"float32"), neginf=-1.1920928955078125e-07, ) 	 50803202 	 3470 	 10.44098949432373 	 1.0334815979003906 	 0.2797257900238037 	 0.3043057918548584 	 4.3614630699157715 	 4.023133993148804 	 0.42816710472106934 	 0.2369832992553711 	 
2025-08-04 13:27:24.344866 test begin: paddle.nan_to_num(Tensor([3386881, 5, 3],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([3386881, 5, 3],"float32"), neginf=-1.1920928955078125e-07, ) 	 50803215 	 3470 	 10.443223237991333 	 1.0336055755615234 	 0.27964282035827637 	 0.30439329147338867 	 4.3615007400512695 	 4.023077964782715 	 0.4281797409057617 	 0.23696112632751465 	 
2025-08-04 13:27:49.694923 test begin: paddle.nan_to_num(Tensor([400, 63505],"float64"), neginf=-2.220446049250313e-16, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([400, 63505],"float64"), neginf=-2.220446049250313e-16, ) 	 25402000 	 3470 	 10.008094072341919 	 1.0374550819396973 	 0.2680847644805908 	 0.30555272102355957 	 3.416337490081787 	 3.5774338245391846 	 0.33537793159484863 	 0.21065664291381836 	 
2025-08-04 13:28:08.890566 test begin: paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), -1, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), -1, False, ) 	 50803240 	 5042 	 12.530772686004639 	 8.529668092727661 	 0.25364208221435547 	 0.28812408447265625 	 3.5892727375030518 	 3.731541156768799 	 0.24255132675170898 	 0.18912100791931152 	 
2025-08-04 13:28:38.470425 test begin: paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), 2, True, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), 2, True, ) 	 50803240 	 5042 	 14.253962516784668 	 6.944623231887817 	 0.28826069831848145 	 0.23453903198242188 	 3.7307302951812744 	 3.966625690460205 	 0.2521860599517822 	 0.20103216171264648 	 
2025-08-04 13:29:11.785264 test begin: paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), None, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), None, False, ) 	 50803240 	 5042 	 9.991776704788208 	 5.549968719482422 	 0.16842412948608398 	 0.1406099796295166 	 2.836756706237793 	 2.9988698959350586 	 0.1918032169342041 	 0.15205669403076172 	 
2025-08-04 13:29:34.128301 test begin: paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), -1, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), -1, False, ) 	 50803230 	 5042 	 12.537623643875122 	 8.533767700195312 	 0.25377583503723145 	 0.2880990505218506 	 3.562035322189331 	 3.7314069271087646 	 0.24082303047180176 	 0.18909978866577148 	 
2025-08-04 13:30:06.246894 test begin: paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), 2, True, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), 2, True, ) 	 50803230 	 5042 	 71.60495686531067 	 5.698445796966553 	 1.5630528926849365 	 0.14440345764160156 	 2.8305742740631104 	 3.18056058883667 	 0.1913914680480957 	 0.16128063201904297 	 
2025-08-04 13:31:30.480015 test begin: paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), None, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), None, False, ) 	 50803230 	 5042 	 9.991098403930664 	 5.551435470581055 	 0.16855311393737793 	 0.1406419277191162 	 2.8082127571105957 	 2.9976723194122314 	 0.189894437789917 	 0.15195822715759277 	 
2025-08-04 13:31:52.828594 test begin: paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), -1, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), -1, False, ) 	 50803224 	 5042 	 10.287074565887451 	 5.556040525436401 	 0.1734623908996582 	 0.14083576202392578 	 2.813164234161377 	 3.085510015487671 	 0.19021081924438477 	 0.1564342975616455 	 
2025-08-04 13:32:17.903130 test begin: paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), 2, True, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), 2, True, ) 	 50803224 	 5042 	 11.7463538646698 	 6.9364707469940186 	 0.23785090446472168 	 0.23421525955200195 	 3.7982277870178223 	 3.991525173187256 	 0.25676631927490234 	 0.20231246948242188 	 
2025-08-04 13:32:45.548735 test begin: paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), None, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), None, False, ) 	 50803224 	 5042 	 9.991344690322876 	 5.55066704750061 	 0.16842961311340332 	 0.14067316055297852 	 2.8077449798583984 	 2.997480630874634 	 0.18986296653747559 	 0.15198493003845215 	 
2025-08-04 13:33:07.780102 test begin: paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), -1, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), -1, False, ) 	 50803260 	 5042 	 12.529582023620605 	 8.528969526290894 	 0.25379252433776855 	 0.2880897521972656 	 3.587441921234131 	 3.7309181690216064 	 0.24255681037902832 	 0.18908143043518066 	 
2025-08-04 13:33:39.085135 test begin: paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), 2, True, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), 2, True, ) 	 50803260 	 5042 	 14.23909592628479 	 6.9444580078125 	 0.288236141204834 	 0.2344660758972168 	 3.7304933071136475 	 3.966304063796997 	 0.2521939277648926 	 0.20099425315856934 	 
2025-08-04 13:34:11.617790 test begin: paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), None, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), None, False, ) 	 50803260 	 5042 	 9.991798162460327 	 5.550004243850708 	 0.16843366622924805 	 0.1406419277191162 	 2.8375275135040283 	 2.9982693195343018 	 0.19178462028503418 	 0.15204215049743652 	 
2025-08-04 13:34:35.088622 test begin: paddle.nanmedian(Tensor([2, 254016],"float32"), axis=1, mode="min", )
[Prof] paddle.nanmedian 	 paddle.nanmedian(Tensor([2, 254016],"float32"), axis=1, mode="min", ) 	 508032 	 2836 	 10.008301258087158 	 3.60404372215271 	 0.0034003257751464844 	 1.2987797260284424 	 None 	 None 	 None 	 None 	 combined
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 13:34:48.953527 test begin: paddle.nanmedian(Tensor([508032],"float32"), keepdim=False, )
[Prof] paddle.nanmedian 	 paddle.nanmedian(Tensor([508032],"float32"), keepdim=False, ) 	 508032 	 2836 	 19.528294324874878 	 0.8039138317108154 	 0.0067675113677978516 	 7.343292236328125e-05 	 0.19343066215515137 	 1.5497395992279053 	 2.5987625122070312e-05 	 0.000339508056640625 	 combined
2025-08-04 13:35:11.646483 test begin: paddle.nanmedian(Tensor([508032],"float32"), keepdim=False, mode="min", )
[Prof] paddle.nanmedian 	 paddle.nanmedian(Tensor([508032],"float32"), keepdim=False, mode="min", ) 	 508032 	 2836 	 19.53684401512146 	 0.9135572910308838 	 0.006768226623535156 	 7.176399230957031e-05 	 0.1953878402709961 	 0.4190545082092285 	 3.457069396972656e-05 	 7.605552673339844e-05 	 combined
2025-08-04 13:35:34.110220 test begin: paddle.nanquantile(Tensor([4, 10584, 6],"float64"), q=0, axis=1, )
W0804 13:35:34.129328 130548 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([4, 10584, 6],"float64"), q=0, axis=1, ) 	 254016 	 24384 	 13.173147916793823 	 8.102028369903564 	 0.013482332229614258 	 0.0067195892333984375 	 5.011202335357666 	 5.549787759780884 	 7.033348083496094e-05 	 7.009506225585938e-05 	 
2025-08-04 13:36:06.376805 test begin: paddle.nanquantile(Tensor([4, 10584, 6],"float64"), q=0.35, axis=2, keepdim=True, )
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([4, 10584, 6],"float64"), q=0.35, axis=2, keepdim=True, ) 	 254016 	 24384 	 462.9128751754761 	 5.336311340332031 	 0.5884077548980713 	 0.00022935867309570312 	 4.210186004638672 	 5.143983602523804 	 6.794929504394531e-05 	 8.58306884765625e-05 	 
2025-08-04 13:44:04.043111 test begin: paddle.nanquantile(Tensor([4, 7, 9072],"float64"), q=0, axis=1, )
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([4, 7, 9072],"float64"), q=0, axis=1, ) 	 254016 	 24384 	 397.9772455692291 	 5.462303161621094 	 0.4569244384765625 	 0.00023365020751953125 	 5.005176305770874 	 5.499788761138916 	 6.842613220214844e-05 	 8.177757263183594e-05 	 
2025-08-04 13:50:58.028312 test begin: paddle.nanquantile(Tensor([4, 7, 9072],"float64"), q=0.35, axis=2, keepdim=True, )
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([4, 7, 9072],"float64"), q=0.35, axis=2, keepdim=True, ) 	 254016 	 24384 	 9.857925653457642 	 7.547070741653442 	 0.0035791397094726562 	 0.005969047546386719 	 4.197808504104614 	 5.159460783004761 	 5.3882598876953125e-05 	 7.414817810058594e-05 	 
2025-08-04 13:51:24.852576 test begin: paddle.nanquantile(Tensor([6048, 7, 6],"float64"), q=0, axis=1, )
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([6048, 7, 6],"float64"), q=0, axis=1, ) 	 254016 	 24384 	 398.2684121131897 	 5.5048768520355225 	 0.4573209285736084 	 0.00016880035400390625 	 4.976629972457886 	 5.488196849822998 	 6.151199340820312e-05 	 9.775161743164062e-05 	 
2025-08-04 13:58:19.166107 test begin: paddle.nanquantile(Tensor([6048, 7, 6],"float64"), q=0.35, axis=2, keepdim=True, )
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([6048, 7, 6],"float64"), q=0.35, axis=2, keepdim=True, ) 	 254016 	 24384 	 462.67398142814636 	 5.22367262840271 	 0.5880346298217773 	 9.846687316894531e-05 	 4.143238306045532 	 5.311891078948975 	 3.4809112548828125e-05 	 7.534027099609375e-05 	 
2025-08-04 14:06:16.559105 test begin: paddle.nansum(Tensor([2, 1270081, 4, 5],"float32"), axis=None, keepdim=False, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 1270081, 4, 5],"float32"), axis=None, keepdim=False, name=None, ) 	 50803240 	 10275 	 10.369885921478271 	 1.5707767009735107 	 0.20603251457214355 	 0.07808375358581543 	 5.684604167938232 	 6.0669639110565186 	 0.28270673751831055 	 0.20114922523498535 	 
2025-08-04 14:06:42.875540 test begin: paddle.nansum(Tensor([2, 1270081, 4, 5],"float32"), axis=None, keepdim=True, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 1270081, 4, 5],"float32"), axis=None, keepdim=True, name=None, ) 	 50803240 	 10275 	 10.376731872558594 	 1.570889949798584 	 0.20601415634155273 	 0.07816290855407715 	 5.684708595275879 	 6.066617488861084 	 0.28270554542541504 	 0.20113658905029297 	 
2025-08-04 14:07:10.356557 test begin: paddle.nansum(Tensor([2, 3, 1693441, 5],"float32"), axis=None, keepdim=False, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 3, 1693441, 5],"float32"), axis=None, keepdim=False, name=None, ) 	 50803230 	 10275 	 10.369797945022583 	 1.5707426071166992 	 0.20601558685302734 	 0.07813286781311035 	 5.68436861038208 	 6.066469430923462 	 0.28269481658935547 	 0.20111751556396484 	 
2025-08-04 14:07:35.061760 test begin: paddle.nansum(Tensor([2, 3, 1693441, 5],"float32"), axis=None, keepdim=True, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 3, 1693441, 5],"float32"), axis=None, keepdim=True, name=None, ) 	 50803230 	 10275 	 10.601405620574951 	 1.5706629753112793 	 0.20605111122131348 	 0.07806849479675293 	 5.6845293045043945 	 6.066044330596924 	 0.28298258781433105 	 0.20110273361206055 	 
2025-08-04 14:08:03.136246 test begin: paddle.nansum(Tensor([2, 3, 4, 2116801],"float32"), axis=None, keepdim=False, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 3, 4, 2116801],"float32"), axis=None, keepdim=False, name=None, ) 	 50803224 	 10275 	 10.369794607162476 	 1.570831060409546 	 0.20614910125732422 	 0.07813096046447754 	 5.683884620666504 	 6.065730094909668 	 0.28267455101013184 	 0.20109820365905762 	 
2025-08-04 14:08:27.689602 test begin: paddle.nansum(Tensor([2, 3, 4, 2116801],"float32"), axis=None, keepdim=True, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 3, 4, 2116801],"float32"), axis=None, keepdim=True, name=None, ) 	 50803224 	 10275 	 10.370540857315063 	 1.5708696842193604 	 0.20604300498962402 	 0.07807493209838867 	 5.684475898742676 	 6.06550145149231 	 0.2826993465423584 	 0.201066255569458 	 
2025-08-04 14:08:52.289711 test begin: paddle.nansum(Tensor([846721, 3, 4, 5],"float32"), axis=None, keepdim=False, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([846721, 3, 4, 5],"float32"), axis=None, keepdim=False, name=None, ) 	 50803260 	 10275 	 10.369926929473877 	 1.5708131790161133 	 0.20604419708251953 	 0.07810711860656738 	 5.684480905532837 	 6.064959287643433 	 0.28265857696533203 	 0.20106840133666992 	 
2025-08-04 14:09:18.025550 test begin: paddle.nansum(Tensor([846721, 3, 4, 5],"float32"), axis=None, keepdim=True, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([846721, 3, 4, 5],"float32"), axis=None, keepdim=True, name=None, ) 	 50803260 	 10275 	 10.369930267333984 	 1.5709936618804932 	 0.20605158805847168 	 0.07816171646118164 	 5.6840126514434814 	 6.065251588821411 	 0.28266215324401855 	 0.20107674598693848 	 
2025-08-04 14:09:44.795627 test begin: paddle.nansum(x=Tensor([105841, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([105841, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401840 	 10275 	 10.939970254898071 	 1.9456021785736084 	 0.27226734161376953 	 0.19350767135620117 	 5.445405721664429 	 4.537975072860718 	 0.2708127498626709 	 0.150390625 	 
2025-08-04 14:10:08.380546 test begin: paddle.nansum(x=Tensor([3, 2, 105841, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 2, 105841, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401840 	 10275 	 10.940735578536987 	 1.9455981254577637 	 0.27222681045532227 	 0.19348931312561035 	 5.446776390075684 	 4.5377418994903564 	 0.27082347869873047 	 0.15042328834533691 	 
2025-08-04 14:10:31.967169 test begin: paddle.nansum(x=Tensor([3, 2, 3, 141121, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 2, 3, 141121, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401780 	 10275 	 57.781508445739746 	 1.783036231994629 	 1.1520640850067139 	 0.0887155532836914 	 4.78299880027771 	 4.2901692390441895 	 0.23784852027893066 	 0.14216876029968262 	 
2025-08-04 14:11:42.857834 test begin: paddle.nansum(x=Tensor([3, 2, 3, 4, 176401, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 2, 3, 4, 176401, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401744 	 10275 	 10.017865180969238 	 1.9515182971954346 	 0.24913692474365234 	 0.19408631324768066 	 5.257086992263794 	 4.559664011001587 	 0.2613997459411621 	 0.15112829208374023 	 
2025-08-04 14:12:05.555478 test begin: paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 1, 70561],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 1, 70561],"float64"), axis=3, keepdim=True, ) 	 25401960 	 10275 	 10.017807722091675 	 1.9539530277252197 	 0.24920344352722168 	 0.19430327415466309 	 5.256870269775391 	 4.564283847808838 	 0.2614567279815674 	 0.15128660202026367 	 
2025-08-04 14:12:28.177035 test begin: paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 35281, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 35281, 2],"float64"), axis=3, keepdim=True, ) 	 25402320 	 10275 	 10.034440279006958 	 1.9517271518707275 	 0.2496623992919922 	 0.19434237480163574 	 5.2575976848602295 	 4.577831745147705 	 0.2614734172821045 	 0.1517469882965088 	 
2025-08-04 14:12:53.713734 test begin: paddle.nansum(x=Tensor([3, 70561, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 70561, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401960 	 10275 	 10.944409608840942 	 1.945671558380127 	 0.2723836898803711 	 0.1935434341430664 	 5.439203262329102 	 4.540865182876587 	 0.27051758766174316 	 0.15050530433654785 	 
2025-08-04 14:13:17.317453 test begin: paddle.neg(Tensor([3175201, 8],"float64"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([3175201, 8],"float64"), ) 	 25401608 	 33849 	 10.091999769210815 	 11.44720721244812 	 0.30472469329833984 	 0.30464887619018555 	 10.078497648239136 	 10.087677240371704 	 0.30440187454223633 	 0.30455946922302246 	 
2025-08-04 14:14:01.704447 test begin: paddle.neg(Tensor([32, 1587601],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([32, 1587601],"float32"), ) 	 50803232 	 33849 	 10.005775213241577 	 10.076555252075195 	 0.302112340927124 	 0.30434417724609375 	 10.02578091621399 	 10.075448989868164 	 0.3027069568634033 	 0.30417943000793457 	 
2025-08-04 14:14:43.848094 test begin: paddle.neg(Tensor([32, 793801],"float64"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([32, 793801],"float64"), ) 	 25401632 	 33849 	 10.095589876174927 	 10.090001583099365 	 0.3047313690185547 	 0.3046691417694092 	 10.078377723693848 	 10.087918519973755 	 0.30430173873901367 	 0.3046085834503174 	 
2025-08-04 14:15:25.494624 test begin: paddle.neg(Tensor([6350401, 8],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([6350401, 8],"float32"), ) 	 50803208 	 33849 	 10.0061514377594 	 10.080502271652222 	 0.3021676540374756 	 0.30425167083740234 	 10.026007890701294 	 10.075279712677002 	 0.30277037620544434 	 0.30423402786254883 	 
2025-08-04 14:16:09.470875 test begin: paddle.neg(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33849 	 10.011497259140015 	 10.076504945755005 	 0.30231475830078125 	 0.3042905330657959 	 10.0254807472229 	 10.075165510177612 	 0.30269432067871094 	 0.3042471408843994 	 
2025-08-04 14:16:51.521187 test begin: paddle.neg(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33849 	 10.010366201400757 	 10.084818124771118 	 0.3022282123565674 	 0.30425143241882324 	 10.025850534439087 	 10.075093746185303 	 0.30272746086120605 	 0.30420804023742676 	 
2025-08-04 14:17:34.045757 test begin: paddle.neg(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33849 	 10.010249137878418 	 10.076502561569214 	 0.3022727966308594 	 0.3042471408843994 	 10.0258948802948 	 10.07510256767273 	 0.30273866653442383 	 0.3041813373565674 	 
2025-08-04 14:18:16.092748 test begin: paddle.negative(Tensor([1693441, 3, 4, 5],"float16"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([1693441, 3, 4, 5],"float16"), ) 	 101606460 	 33834 	 10.102278232574463 	 10.041586637496948 	 0.3051729202270508 	 0.3024923801422119 	 10.090898513793945 	 10.012568950653076 	 0.3048062324523926 	 0.30242300033569336 	 
2025-08-04 14:19:01.964646 test begin: paddle.negative(Tensor([2, 1270081, 4, 5],"float32"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 1270081, 4, 5],"float32"), ) 	 50803240 	 33834 	 10.001487255096436 	 10.072481870651245 	 0.3021736145019531 	 0.3043656349182129 	 10.021069526672363 	 10.07083511352539 	 0.30270910263061523 	 0.30422234535217285 	 
2025-08-04 14:19:44.308815 test begin: paddle.negative(Tensor([2, 2540161, 4, 5],"float16"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 2540161, 4, 5],"float16"), ) 	 101606440 	 33834 	 10.103485107421875 	 10.014096736907959 	 0.3051590919494629 	 0.30251073837280273 	 10.09095549583435 	 10.01346492767334 	 0.30489063262939453 	 0.30242156982421875 	 
2025-08-04 14:20:28.581489 test begin: paddle.negative(Tensor([2, 3, 1693441, 5],"float32"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 1693441, 5],"float32"), ) 	 50803230 	 33834 	 10.001533508300781 	 10.10809063911438 	 0.30219364166259766 	 0.3043208122253418 	 10.021517038345337 	 10.071039199829102 	 0.3027207851409912 	 0.3042464256286621 	 
2025-08-04 14:21:14.234754 test begin: paddle.negative(Tensor([2, 3, 3386881, 5],"float16"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 3386881, 5],"float16"), ) 	 101606430 	 33834 	 10.102214336395264 	 10.01685619354248 	 0.3051474094390869 	 0.3025083541870117 	 10.0910964012146 	 10.012365102767944 	 0.30485057830810547 	 0.3024263381958008 	 
2025-08-04 14:21:59.220498 test begin: paddle.negative(Tensor([2, 3, 4, 1058401],"float64"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 4, 1058401],"float64"), ) 	 25401624 	 33834 	 11.096253871917725 	 10.085211038589478 	 0.3047196865081787 	 0.3047444820404053 	 10.073781251907349 	 10.083129405975342 	 0.3043026924133301 	 0.30452585220336914 	 
2025-08-04 14:22:42.693345 test begin: paddle.negative(Tensor([2, 3, 4, 2116801],"float32"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 4, 2116801],"float32"), ) 	 50803224 	 33834 	 10.01150918006897 	 10.611366033554077 	 0.30213236808776855 	 0.30426812171936035 	 10.021042108535767 	 10.070844650268555 	 0.3027808666229248 	 0.3042469024658203 	 
2025-08-04 14:23:26.793886 test begin: paddle.negative(Tensor([2, 3, 4, 4233601],"float16"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 4, 4233601],"float16"), ) 	 101606424 	 33834 	 10.102182626724243 	 10.016435384750366 	 0.3051319122314453 	 0.3025379180908203 	 10.090777397155762 	 10.013617277145386 	 0.30485081672668457 	 0.30243873596191406 	 
2025-08-04 14:24:11.310717 test begin: paddle.negative(Tensor([2, 3, 846721, 5],"float64"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 846721, 5],"float64"), ) 	 25401630 	 33834 	 10.087483882904053 	 10.085304737091064 	 0.30469775199890137 	 0.30464696884155273 	 10.073731422424316 	 10.082744121551514 	 0.3043069839477539 	 0.30454206466674805 	 
2025-08-04 14:24:52.954714 test begin: paddle.negative(Tensor([2, 635041, 4, 5],"float64"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 635041, 4, 5],"float64"), ) 	 25401640 	 33834 	 10.087438106536865 	 10.084935426712036 	 0.3047599792480469 	 0.3046836853027344 	 10.073848485946655 	 10.083251714706421 	 0.3042736053466797 	 0.30458641052246094 	 
2025-08-04 14:25:34.536923 test begin: paddle.negative(Tensor([423361, 3, 4, 5],"float64"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([423361, 3, 4, 5],"float64"), ) 	 25401660 	 33834 	 10.094936847686768 	 10.085318803787231 	 0.3046689033508301 	 0.304706335067749 	 10.073796510696411 	 10.083095788955688 	 0.3042876720428467 	 0.3045964241027832 	 
2025-08-04 14:26:16.158352 test begin: paddle.negative(Tensor([846721, 3, 4, 5],"float32"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([846721, 3, 4, 5],"float32"), ) 	 50803260 	 33834 	 10.003602504730225 	 10.084830522537231 	 0.3021397590637207 	 0.3042290210723877 	 10.021098613739014 	 10.07082223892212 	 0.3026771545410156 	 0.3041989803314209 	 
2025-08-04 14:26:58.294665 test begin: paddle.nextafter(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), ) 	 101606480 	 22191 	 10.001290321350098 	 9.962700605392456 	 0.4605381488800049 	 0.4586365222930908 	 None 	 None 	 None 	 None 	 
2025-08-04 14:27:19.993212 test begin: paddle.nextafter(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), ) 	 101606460 	 22191 	 10.004145622253418 	 9.95959210395813 	 0.460712194442749 	 0.4586820602416992 	 None 	 None 	 None 	 None 	 
2025-08-04 14:27:42.817296 test begin: paddle.nextafter(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), ) 	 101606448 	 22191 	 10.518909692764282 	 9.95922327041626 	 0.46070194244384766 	 0.45862460136413574 	 None 	 None 	 None 	 None 	 
2025-08-04 14:28:06.974497 test begin: paddle.nextafter(Tensor([4, 3, 2116801],"float32"), Tensor([4, 3, 2116801],"float64"), )
W0804 14:28:07.839176 143557 dygraph_functions.cc:57914] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3, 2116801],"float32"), Tensor([4, 3, 2116801],"float64"), ) 	 50803224 	 22191 	 15.194414138793945 	 8.603746891021729 	 0.34993672370910645 	 0.39624524116516113 	 None 	 None 	 None 	 None 	 
2025-08-04 14:28:31.798523 test begin: paddle.nextafter(Tensor([4, 3, 2116801],"float64"), Tensor([4, 3, 2116801],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3, 2116801],"float64"), Tensor([4, 3, 2116801],"float32"), ) 	 50803224 	 22191 	 15.198132514953613 	 8.514384984970093 	 0.3499577045440674 	 0.39214301109313965 	 None 	 None 	 None 	 None 	 
2025-08-04 14:28:59.739976 test begin: paddle.nextafter(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float64"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float64"), ) 	 101606424 	 22191 	 30.25407862663269 	 17.00795555114746 	 0.6968724727630615 	 0.7832741737365723 	 None 	 None 	 None 	 None 	 
2025-08-04 14:29:49.012203 test begin: paddle.nextafter(Tensor([4, 3, 4233601],"float64"), Tensor([4, 3, 4233601],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3, 4233601],"float64"), Tensor([4, 3, 4233601],"float32"), ) 	 101606424 	 22191 	 30.25693655014038 	 16.83522629737854 	 0.6967642307281494 	 0.7752680778503418 	 None 	 None 	 None 	 None 	 
2025-08-04 14:30:38.329433 test begin: paddle.nextafter(Tensor([4, 3175201, 2],"float32"), Tensor([4, 3175201, 2],"float64"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3175201, 2],"float32"), Tensor([4, 3175201, 2],"float64"), ) 	 50803216 	 22191 	 15.203283071517944 	 8.621205806732178 	 0.34989166259765625 	 0.3962888717651367 	 None 	 None 	 None 	 None 	 
2025-08-04 14:31:05.273525 test begin: paddle.nextafter(Tensor([4, 3175201, 2],"float64"), Tensor([4, 3175201, 2],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3175201, 2],"float64"), Tensor([4, 3175201, 2],"float32"), ) 	 50803216 	 22191 	 15.226465940475464 	 8.518190145492554 	 0.35041141510009766 	 0.3922724723815918 	 None 	 None 	 None 	 None 	 
2025-08-04 14:31:32.347454 test begin: paddle.nextafter(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float64"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float64"), ) 	 101606416 	 22191 	 30.259281635284424 	 17.010908603668213 	 0.6967244148254395 	 0.7832999229431152 	 None 	 None 	 None 	 None 	 
2025-08-04 14:32:22.828384 test begin: paddle.nextafter(Tensor([4, 6350401, 2],"float64"), Tensor([4, 6350401, 2],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 6350401, 2],"float64"), Tensor([4, 6350401, 2],"float32"), ) 	 101606416 	 22191 	 30.25668239593506 	 16.838143348693848 	 0.6966931819915771 	 0.7754807472229004 	 None 	 None 	 None 	 None 	 
2025-08-04 14:33:14.128537 test begin: paddle.nextafter(Tensor([4233601, 3, 2],"float32"), Tensor([4233601, 3, 2],"float64"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4233601, 3, 2],"float32"), Tensor([4233601, 3, 2],"float64"), ) 	 50803212 	 22191 	 15.208531856536865 	 8.609276294708252 	 0.35024237632751465 	 0.39621782302856445 	 None 	 None 	 None 	 None 	 
2025-08-04 14:33:42.925130 test begin: paddle.nextafter(Tensor([4233601, 3, 2],"float64"), Tensor([4233601, 3, 2],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4233601, 3, 2],"float64"), Tensor([4233601, 3, 2],"float32"), ) 	 50803212 	 22191 	 15.20549488067627 	 8.51446795463562 	 0.34990882873535156 	 0.39214181900024414 	 None 	 None 	 None 	 None 	 
2025-08-04 14:34:07.746058 test begin: paddle.nextafter(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float64"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float64"), ) 	 101606412 	 22191 	 30.25273084640503 	 17.81307029724121 	 0.6964926719665527 	 0.7834172248840332 	 None 	 None 	 None 	 None 	 
2025-08-04 14:34:59.383929 test begin: paddle.nextafter(Tensor([8467201, 3, 2],"float64"), Tensor([8467201, 3, 2],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([8467201, 3, 2],"float64"), Tensor([8467201, 3, 2],"float32"), ) 	 101606412 	 22191 	 30.254973888397217 	 16.83436107635498 	 0.6967430114746094 	 0.7752742767333984 	 None 	 None 	 None 	 None 	 
2025-08-04 14:35:48.557525 test begin: paddle.nextafter(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), ) 	 101606520 	 22191 	 10.010716915130615 	 9.976540327072144 	 0.4606337547302246 	 0.45862364768981934 	 None 	 None 	 None 	 None 	 
2025-08-04 14:36:11.209208 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([124, 1536, 267],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([124, 1536, 267],"float32"), 1, None, ) 	 50853888 	 52811 	 15.958463907241821 	 9.257255792617798 	 0.3088665008544922 	 0.17916059494018555 	 44.2491557598114 	 8.985637903213501 	 0.42808008193969727 	 0.17387604713439941 	 
2025-08-04 14:37:30.549413 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([124, 8362, 49],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([124, 8362, 49],"float32"), 1, None, ) 	 50807512 	 52811 	 18.746387720108032 	 19.917439937591553 	 0.3627583980560303 	 0.38542747497558594 	 45.67645478248596 	 9.205262184143066 	 0.44193577766418457 	 0.1780986785888672 	 
2025-08-04 14:39:05.838803 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([128, 1536, 259],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([128, 1536, 259],"float32"), 1, None, ) 	 50921472 	 52811 	 15.046534061431885 	 8.118088722229004 	 0.2911946773529053 	 0.15709185600280762 	 44.3381884098053 	 8.988818645477295 	 0.428943395614624 	 0.17390656471252441 	 
2025-08-04 14:40:26.297683 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([128, 8101, 49],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([128, 8101, 49],"float32"), 1, None, ) 	 50809472 	 52811 	 18.74345588684082 	 19.920719861984253 	 0.36266064643859863 	 0.38565826416015625 	 45.68082523345947 	 9.232686758041382 	 0.4419684410095215 	 0.17884111404418945 	 
2025-08-04 14:42:00.888840 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([345, 1024, 144],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([345, 1024, 144],"float32"), 1, None, ) 	 50872320 	 52811 	 24.22420382499695 	 11.598302602767944 	 0.46876072883605957 	 0.22453784942626953 	 44.754051208496094 	 8.99655532836914 	 0.43386316299438477 	 0.17413759231567383 	 
2025-08-04 14:43:31.352515 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([64, 1024, 776],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([64, 1024, 776],"float32"), 1, None, ) 	 50855936 	 52811 	 10.002037763595581 	 8.135316610336304 	 0.19353938102722168 	 0.15746474266052246 	 43.446826696395874 	 9.068824052810669 	 0.42057299613952637 	 0.17534899711608887 	 
2025-08-04 14:44:42.900547 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([64, 5513, 144],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([64, 5513, 144],"float32"), 1, None, ) 	 50807808 	 52811 	 24.195443391799927 	 11.571328163146973 	 0.4682290554046631 	 0.2239069938659668 	 44.700618267059326 	 9.032117366790771 	 0.4322841167449951 	 0.17461729049682617 	 
2025-08-04 14:46:14.182699 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([676, 1536, 49],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([676, 1536, 49],"float32"), 1, None, ) 	 50878464 	 52811 	 18.793918132781982 	 19.94012999534607 	 0.3635561466217041 	 0.3858788013458252 	 45.77893781661987 	 9.220376253128052 	 0.4429330825805664 	 0.17842507362365723 	 
2025-08-04 14:47:50.012701 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 2048, 2, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 2048, 2, 7],"float32"), output_size=1, ) 	 58404864 	 33312 	 9.993207931518555 	 13.614041805267334 	 0.3064584732055664 	 0.37332582473754883 	 34.25445580482483 	 7.4800660610198975 	 0.5262565612792969 	 0.2294471263885498 	 
2025-08-04 14:48:58.002125 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 2048, 7, 2],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 2048, 7, 2],"float32"), output_size=1, ) 	 58404864 	 33312 	 10.022681713104248 	 12.18154764175415 	 0.30751895904541016 	 0.3732478618621826 	 34.314797163009644 	 7.479497194290161 	 0.5264410972595215 	 0.22942352294921875 	 
2025-08-04 14:50:03.079483 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 509, 7, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 509, 7, 7],"float32"), output_size=1, ) 	 50804817 	 33312 	 11.841259479522705 	 12.58999252319336 	 0.36328792572021484 	 0.3862154483795166 	 28.838478088378906 	 5.81811785697937 	 0.4422287940979004 	 0.1784350872039795 	 
2025-08-04 14:51:04.496802 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 2048, 2, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 2048, 2, 7],"float32"), output_size=1, ) 	 58634240 	 33312 	 10.486902713775635 	 12.22887372970581 	 0.3083639144897461 	 0.37461161613464355 	 34.40872836112976 	 7.528006315231323 	 0.5278735160827637 	 0.23081445693969727 	 
2025-08-04 14:52:12.996554 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 2048, 7, 2],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 2048, 7, 2],"float32"), output_size=1, ) 	 58634240 	 33312 	 10.055739879608154 	 13.298227071762085 	 0.3084414005279541 	 0.3738422393798828 	 34.41510772705078 	 7.5255348682403564 	 0.5278372764587402 	 0.23089241981506348 	 
2025-08-04 14:53:21.251210 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 507, 7, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 507, 7, 7],"float32"), output_size=1, ) 	 50803935 	 33312 	 11.827550649642944 	 12.57681131362915 	 0.3627495765686035 	 0.3865036964416504 	 28.82771396636963 	 5.832480430603027 	 0.4425621032714844 	 0.17890548706054688 	 
2025-08-04 14:54:21.200641 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 2048, 2, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 2048, 2, 7],"float32"), output_size=1, ) 	 58720256 	 33312 	 10.07045578956604 	 12.224093675613403 	 0.30895137786865234 	 0.3750288486480713 	 34.46725583076477 	 7.521914482116699 	 0.5285825729370117 	 0.23073172569274902 	 
2025-08-04 14:55:27.821094 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 2048, 7, 2],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 2048, 7, 2],"float32"), output_size=1, ) 	 58720256 	 33312 	 10.06313443183899 	 12.221477746963501 	 0.3086884021759033 	 0.37491846084594727 	 34.45021080970764 	 7.520423173904419 	 0.5284626483917236 	 0.23071742057800293 	 
2025-08-04 14:56:33.355388 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 507, 7, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 507, 7, 7],"float32"), output_size=1, ) 	 50878464 	 33312 	 11.863404273986816 	 12.604908466339111 	 0.3639857769012451 	 0.38673949241638184 	 28.914912939071655 	 5.825712203979492 	 0.4435296058654785 	 0.17870688438415527 	 
2025-08-04 14:57:33.443665 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([507, 2048, 7, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([507, 2048, 7, 7],"float32"), output_size=1, ) 	 50878464 	 33312 	 11.857090950012207 	 12.606825113296509 	 0.36380696296691895 	 0.38689208030700684 	 28.911561012268066 	 5.829381704330444 	 0.44351816177368164 	 0.17880487442016602 	 
2025-08-04 14:58:33.565033 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 45361, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 45361, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50804320 	 19743 	 17.688148975372314 	 2.9416215419769287 	 0.9156920909881592 	 0.15228748321533203 	 44.80607080459595 	 3.415409564971924 	 1.8307056427001953 	 0.17673039436340332 	 
2025-08-04 14:59:45.694916 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 50401, 16, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 50401, 16, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50804208 	 19743 	 9.981710433959961 	 2.9955203533172607 	 0.5167410373687744 	 0.15510225296020508 	 44.78143620491028 	 3.4095711708068848 	 1.1574337482452393 	 0.17645740509033203 	 
2025-08-04 15:00:48.703141 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 50401, 16, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 50401, 16, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50804208 	 19743 	 10.765271425247192 	 2.9954991340637207 	 0.5572268962860107 	 0.15504074096679688 	 44.775657415390015 	 3.409496307373047 	 1.1591956615447998 	 0.17643952369689941 	 
2025-08-04 15:01:52.681021 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 1051, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 1051, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50851584 	 19743 	 314.3704409599304 	 2.980107307434082 	 16.274617671966553 	 0.15426039695739746 	 44.66869258880615 	 3.2634382247924805 	 1.1557002067565918 	 0.168928861618042 	 
2025-08-04 15:07:59.179944 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 1051, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 1051, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50851584 	 19743 	 381.18086338043213 	 2.9803266525268555 	 19.738102912902832 	 0.15428376197814941 	 44.722182750701904 	 3.253854751586914 	 1.1575109958648682 	 0.16834425926208496 	 
2025-08-04 15:15:12.219834 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 414, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f80a18569b0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 15:25:56.929830 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 460, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
W0804 15:25:57.907833   382 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4d4ecbeef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 15:36:01.761623 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 591, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
W0804 15:36:02.718883  3919 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 591, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50835456 	 19743 	 482.7619779109955 	 2.947136402130127 	 24.99660348892212 	 0.15258026123046875 	 43.865175008773804 	 3.2518227100372314 	 1.1352207660675049 	 0.16831064224243164 	 
2025-08-04 15:44:56.069362 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 591],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 591],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50835456 	 19743 	 458.583536863327 	 2.9555835723876953 	 23.736836671829224 	 0.15297794342041016 	 42.71599578857422 	 3.253535270690918 	 1.105656623840332 	 0.16838645935058594 	 
2025-08-04 15:53:25.122992 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 9, 460],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2a0bef28f0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:04:09.058855 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 946, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
W0804 16:04:10.074252 13398 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 946, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50856960 	 19743 	 343.8863627910614 	 2.956170082092285 	 17.800901889801025 	 0.15303659439086914 	 44.276294231414795 	 3.253516435623169 	 1.442195177078247 	 0.16840052604675293 	 
2025-08-04 16:10:48.163919 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([60, 768, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([60, 768, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 51609600 	 19743 	 17.813676118850708 	 2.9782891273498535 	 0.9210143089294434 	 0.15419220924377441 	 44.77024745941162 	 3.4579498767852783 	 1.1587169170379639 	 0.17894434928894043 	 
2025-08-04 16:11:58.713885 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([66, 768, 16, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([66, 768, 16, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 51093504 	 19743 	 10.050067901611328 	 3.002514362335205 	 0.5205731391906738 	 0.1554419994354248 	 45.33879327774048 	 3.4189767837524414 	 1.4968211650848389 	 0.17689776420593262 	 
2025-08-04 16:13:02.125874 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([66, 768, 16, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([66, 768, 16, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 51093504 	 19743 	 10.829135656356812 	 3.002448081970215 	 0.5605931282043457 	 0.15542173385620117 	 44.995452880859375 	 3.4189414978027344 	 1.164626121520996 	 0.17694830894470215 	 
2025-08-04 16:14:06.663265 test begin: paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([128, 16],"float32"), Tensor([128],"int64"), Tensor([16, 3175201],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, )
W0804 16:14:07.477988 16721 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.adaptive_log_softmax_with_loss 	 paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([128, 16],"float32"), Tensor([128],"int64"), Tensor([16, 3175201],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, ) 	 50805392 	 1469 	 17.912841081619263 	 11.999998092651367 	 0.009499311447143555 	 0.0066530704498291016 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:15:05.261144 test begin: paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([25401601, 16],"float32"), Tensor([25401601],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, )
[Prof] paddle.nn.functional.adaptive_log_softmax_with_loss 	 paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([25401601, 16],"float32"), Tensor([25401601],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, ) 	 431827345 	 1469 	 73.33670473098755 	 30.905883073806763 	 0.007273674011230469 	 0.006186723709106445 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:17:59.989549 test begin: paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([3175201, 16],"float32"), Tensor([3175201],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, )
[Prof] paddle.nn.functional.adaptive_log_softmax_with_loss 	 paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([3175201, 16],"float32"), Tensor([3175201],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, ) 	 53978545 	 1469 	 9.942286729812622 	 4.667385816574097 	 0.0008003711700439453 	 0.0006163120269775391 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:18:22.684719 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 4233601],"float64"), 8, False, None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f70f216fe20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:29:31.645180 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 8467201],"float32"), 16, False, None, )
W0804 16:29:32.604393 22585 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe59fb17010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:39:36.683015 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 8467201],"float32"), output_size=16, )
W0804 16:39:42.924520 26206 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0baf0f2d10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:49:44.246999 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 396901, 32],"float64"), 8, False, None, )
W0804 16:49:44.977494 30341 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 396901, 32],"float64"), 8, False, None, ) 	 25401664 	 48051 	 10.01092791557312 	 41.50491142272949 	 0.21290993690490723 	 0.8826491832733154 	 25.062761068344116 	 37.38402056694031 	 0.26653003692626953 	 0.397674560546875 	 
2025-08-04 16:51:41.615680 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 793801, 32],"float32"), 16, False, None, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 793801, 32],"float32"), 16, False, None, ) 	 50803264 	 48051 	 19.666744470596313 	 82.23113799095154 	 0.4180610179901123 	 1.7481858730316162 	 42.69587445259094 	 67.77808666229248 	 0.45410728454589844 	 0.7204899787902832 	 
2025-08-04 16:55:15.745884 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 793801, 32],"float32"), output_size=16, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 793801, 32],"float32"), output_size=16, ) 	 50803264 	 48051 	 19.648104190826416 	 82.12566208839417 	 0.41759634017944336 	 1.7467210292816162 	 42.67086315155029 	 67.76880502700806 	 0.4536857604980469 	 0.7208294868469238 	 
2025-08-04 16:58:50.350547 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([264601, 3, 32],"float64"), 8, False, None, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([264601, 3, 32],"float64"), 8, False, None, ) 	 25401696 	 48051 	 10.01092004776001 	 164.07001066207886 	 0.21286487579345703 	 3.488410234451294 	 25.063228368759155 	 159.86453342437744 	 0.2661478519439697 	 1.6999146938323975 	 
2025-08-04 17:04:50.081828 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([529201, 3, 32],"float32"), 16, False, None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff8cad28280>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:14:57.411847 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([529201, 3, 32],"float32"), output_size=16, )
W0804 17:14:58.439719 37287 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8d2cc52e00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:25:04.722929 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 1209601, 7],"float32"), output_size=5, return_mask=False, name=None, )
W0804 17:25:05.748951 40004 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8a5fb12ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:35:09.332055 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 1209601],"float32"), output_size=5, return_mask=False, name=None, )
W0804 17:35:10.330905 42507 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7d7f85f130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:45:13.924334 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 1209601],"float32"), output_size=list[2,5,], return_mask=False, name=None, )
W0804 17:45:15.072173 45004 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa8a27b70d0>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754301314 (unix time) try "date -d @1754301314" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xaf75) received by PID 44917 (TID 0x7fa89dfa8640) from PID 44917 ***]

2025-08-04 17:55:20.861048 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 1209601],"float32"), output_size=list[3,3,], return_mask=False, name=None, )
W0804 17:55:21.927165 47522 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f392c183130>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754301920 (unix time) try "date -d @1754301920" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xb947) received by PID 47431 (TID 0x7f39238f8640) from PID 47431 ***]

2025-08-04 18:05:28.249784 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )
W0804 18:05:29.382553 50022 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.adaptive_max_pool2d 	 paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803298 	 25828 	 15.036789655685425 	 120.99489188194275 	 0.5944933891296387 	 4.783113956451416 	 28.142449378967285 	 30.519717931747437 	 0.5568296909332275 	 0.6037735939025879 	 
2025-08-04 18:08:45.076418 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=list[2,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool2d 	 paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=list[2,5,], return_mask=False, name=None, ) 	 50803298 	 25828 	 11.298036098480225 	 68.02125263214111 	 0.44707751274108887 	 2.6914846897125244 	 26.122997999191284 	 25.00802230834961 	 0.5168797969818115 	 0.4947185516357422 	 
2025-08-04 18:10:57.861518 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=list[3,3,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool2d 	 paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=list[3,3,], return_mask=False, name=None, ) 	 50803298 	 25828 	 10.018609046936035 	 84.8605740070343 	 0.3964235782623291 	 3.3569440841674805 	 26.68925714492798 	 25.88654351234436 	 0.5277125835418701 	 0.5120899677276611 	 
2025-08-04 18:13:29.939996 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803690 	 6398 	 12.567519187927246 	 65.88349318504333 	 2.007330894470215 	 0.6577396392822266 	 8.064263582229614 	 9.54697561264038 	 0.6440954208374023 	 0.08967137336730957 	 
2025-08-04 18:15:07.377820 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, ) 	 50803690 	 6398 	 10.00534439086914 	 35.47347831726074 	 1.5980007648468018 	 0.8092062473297119 	 3.045567035675049 	 3.812126874923706 	 0.24324774742126465 	 0.07610702514648438 	 
2025-08-04 18:16:03.471527 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, ) 	 50803690 	 6398 	 12.255598545074463 	 43.95010304450989 	 1.9576356410980225 	 0.7018637657165527 	 3.698840856552124 	 5.200545072555542 	 0.29543590545654297 	 0.0754706859588623 	 
2025-08-04 18:17:09.645998 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 172801, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 172801, 7, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803494 	 6398 	 194.57844495773315 	 256.14568996429443 	 31.092936277389526 	 41.01658034324646 	 0.971062421798706 	 0.8799221515655518 	 0.07745909690856934 	 0.07017803192138672 	 
2025-08-04 18:24:46.622160 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 172801, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff6df4aa890>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 18:35:49.518984 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 172801, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
W0804 18:35:50.710414 57432 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5542ca70d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 18:47:26.859669 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 241921, 7],"float32"), output_size=5, return_mask=False, name=None, )
W0804 18:47:27.809937 59788 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 241921, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803410 	 6398 	 153.54064083099365 	 192.87445616722107 	 24.52981185913086 	 30.824878215789795 	 0.9637494087219238 	 0.8810040950775146 	 0.0769507884979248 	 0.0703134536743164 	 
2025-08-04 18:53:16.601426 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 241921, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdeb05fc280>,)) (kwargs={}) timed out after 600.000000 seconds.

W0804 19:04:36.663076 61202 backward.cc:462] While running Node (MaxPool3dWithIndexGradNode) raises an EnforceNotMet exception
terminate called without an active exception
2025-08-04 19:04:44.632826 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 241921, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
W0804 19:04:45.726063 64041 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6ecf47eef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 19:16:30.956597 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 241921],"float32"), output_size=5, return_mask=False, name=None, )
W0804 19:16:31.957602 64766 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 241921],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803410 	 6398 	 64.92518377304077 	 75.86924195289612 	 10.372385025024414 	 12.119559288024902 	 0.962836742401123 	 0.8793213367462158 	 0.07687592506408691 	 0.07018280029296875 	 
2025-08-04 19:18:58.221681 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 241921],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 241921],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, ) 	 50803410 	 6398 	 199.1211884021759 	 229.59939908981323 	 31.79998469352722 	 36.67392420768738 	 0.9571936130523682 	 0.8789060115814209 	 0.07648921012878418 	 0.07013344764709473 	 
2025-08-04 19:26:10.689182 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 241921],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f659fd67370>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 19:37:14.710402 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )
W0804 19:37:15.654310 65297 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803935 	 6398 	 12.574782848358154 	 66.63656210899353 	 2.0086638927459717 	 0.6576368808746338 	 8.049149990081787 	 9.537247657775879 	 0.6431658267974854 	 0.08956599235534668 	 
2025-08-04 19:38:57.511750 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, ) 	 50803935 	 6398 	 9.99769926071167 	 35.461634159088135 	 1.597015380859375 	 0.8092048168182373 	 3.049790620803833 	 3.8117830753326416 	 0.2436234951019287 	 0.0760495662689209 	 
2025-08-04 19:39:50.811893 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, ) 	 50803935 	 6398 	 12.2581787109375 	 43.94423770904541 	 1.9580719470977783 	 0.7016963958740234 	 3.7139506340026855 	 5.196396827697754 	 0.2964181900024414 	 0.07540106773376465 	 
2025-08-04 19:40:56.882309 test begin: paddle.nn.functional.avg_pool1d(Tensor([13, 1, 3907939],"float32"), 25, 1, 0, True, False, None, )
W0804 19:40:57.628644 65346 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.avg_pool1d 	 paddle.nn.functional.avg_pool1d(Tensor([13, 1, 3907939],"float32"), 25, 1, 0, True, False, None, ) 	 50803207 	 1335 	 49.063520193099976 	 2.112844705581665 	 37.55951118469238 	 1.6172635555267334 	 61.155277967453 	 4.983403444290161 	 46.81658720970154 	 3.814908027648926 	 
2025-08-04 19:42:56.025562 test begin: paddle.nn.functional.avg_pool1d(Tensor([13, 32567, 120],"float32"), 25, 1, 0, True, False, None, )
[Prof] paddle.nn.functional.avg_pool1d 	 paddle.nn.functional.avg_pool1d(Tensor([13, 32567, 120],"float32"), 25, 1, 0, True, False, None, ) 	 50804520 	 1335 	 10.008411645889282 	 1.8454558849334717 	 7.661500692367554 	 1.4125311374664307 	 15.235467672348022 	 5.142815351486206 	 11.662639141082764 	 3.9369585514068604 	 
2025-08-04 19:43:29.847993 test begin: paddle.nn.functional.avg_pool1d(Tensor([16, 1, 3175201],"float32"), 25, 1, 0, True, False, None, )
[Prof] paddle.nn.functional.avg_pool1d 	 paddle.nn.functional.avg_pool1d(Tensor([16, 1, 3175201],"float32"), 25, 1, 0, True, False, None, ) 	 50803216 	 1335 	 49.058737993240356 	 2.1129350662231445 	 37.556039571762085 	 1.6173324584960938 	 61.15287208557129 	 4.983922719955444 	 46.81488370895386 	 3.8152475357055664 	 
2025-08-04 19:45:28.941969 test begin: paddle.nn.functional.avg_pool1d(Tensor([16, 2, 1587601],"float32"), 25, 1, 0, True, False, None, )
[Prof] paddle.nn.functional.avg_pool1d 	 paddle.nn.functional.avg_pool1d(Tensor([16, 2, 1587601],"float32"), 25, 1, 0, True, False, None, ) 	 50803232 	 1335 	 24.644384145736694 	 3.830928325653076 	 18.86603546142578 	 1.6156721115112305 	 30.60931086540222 	 4.9845216274261475 	 23.43260908126831 	 3.8153116703033447 	 
2025-08-04 19:46:38.825126 test begin: paddle.nn.functional.avg_pool1d(Tensor([16, 26461, 120],"float32"), 25, 1, 0, True, False, None, )
[Prof] paddle.nn.functional.avg_pool1d 	 paddle.nn.functional.avg_pool1d(Tensor([16, 26461, 120],"float32"), 25, 1, 0, True, False, None, ) 	 50805120 	 1335 	 10.229912281036377 	 1.8453152179718018 	 7.661381959915161 	 1.4125137329101562 	 15.235870122909546 	 5.1425745487213135 	 11.663288593292236 	 3.9366977214813232 	 
2025-08-04 19:47:15.181068 test begin: paddle.nn.functional.avg_pool2d(Tensor([128, 127, 56, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([128, 127, 56, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 50978816 	 52538 	 10.2040114402771 	 21.878587007522583 	 0.19851350784301758 	 0.4262263774871826 	 18.540878295898438 	 82.43358159065247 	 0.360645055770874 	 1.603456735610962 	 
2025-08-04 19:49:29.309695 test begin: paddle.nn.functional.avg_pool2d(Tensor([128, 256, 28, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([128, 256, 28, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51380224 	 52538 	 10.446199893951416 	 22.061981678009033 	 0.20318865776062012 	 0.42951083183288574 	 17.494886875152588 	 82.84709572792053 	 0.3403005599975586 	 1.6115036010742188 	 
2025-08-04 19:51:44.486794 test begin: paddle.nn.functional.avg_pool2d(Tensor([128, 256, 56, 28],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([128, 256, 56, 28],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51380224 	 52538 	 10.495837211608887 	 22.07193350791931 	 0.2041785717010498 	 0.4290175437927246 	 17.56404948234558 	 82.88020539283752 	 0.34160757064819336 	 1.6120185852050781 	 
2025-08-04 19:54:01.067253 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 128, 256, 97],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([16, 128, 256, 97],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 50855936 	 52538 	 10.094513893127441 	 22.000227451324463 	 0.19637012481689453 	 0.4190335273742676 	 17.347036123275757 	 82.09615278244019 	 0.33736753463745117 	 1.5980870723724365 	 
2025-08-04 19:56:14.996875 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 128, 97, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([16, 128, 97, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 50855936 	 52538 	 9.992060422897339 	 21.337148427963257 	 0.19436144828796387 	 0.41491222381591797 	 17.28271222114563 	 81.46211338043213 	 0.3358302116394043 	 1.5854430198669434 	 
2025-08-04 19:58:26.138898 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 49, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([16, 49, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51380224 	 52538 	 10.317136287689209 	 21.80037832260132 	 0.20068573951721191 	 0.4241335391998291 	 18.56596803665161 	 82.39717984199524 	 0.36113429069519043 	 1.6027805805206299 	 
2025-08-04 20:00:42.708977 test begin: paddle.nn.functional.avg_pool2d(Tensor([4, 256, 240, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([4, 256, 240, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 58982400 	 52538 	 11.719713926315308 	 25.207682371139526 	 0.22788786888122559 	 0.4903585910797119 	 20.063036918640137 	 95.01376724243164 	 0.3902578353881836 	 1.8477463722229004 	 
2025-08-04 20:03:16.761565 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 256, 56, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([64, 256, 56, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51380224 	 52538 	 10.275359869003296 	 22.090481758117676 	 0.1999189853668213 	 0.42954587936401367 	 18.541218042373657 	 83.04847645759583 	 0.36064648628234863 	 1.6155238151550293 	 
2025-08-04 20:05:31.807471 test begin: paddle.nn.functional.avg_pool2d(Tensor([7, 128, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([7, 128, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 58720256 	 52538 	 11.613559007644653 	 25.843759059906006 	 0.2259359359741211 	 0.4828977584838867 	 19.993159294128418 	 94.00632786750793 	 0.38887596130371094 	 1.8287513256072998 	 
2025-08-04 20:08:06.005545 test begin: paddle.nn.functional.avg_pool2d(Tensor([8, 111, 240, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([8, 111, 240, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51148800 	 52538 	 10.215937376022339 	 23.247828006744385 	 0.19869494438171387 	 0.4266648292541504 	 17.58163809776306 	 82.50493621826172 	 0.3419973850250244 	 1.6044321060180664 	 
2025-08-04 20:10:23.068439 test begin: paddle.nn.functional.avg_pool2d(Tensor([8, 256, 104, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([8, 256, 104, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51118080 	 52538 	 10.20459771156311 	 21.90703821182251 	 0.1985616683959961 	 0.4261200428009033 	 17.517823934555054 	 82.29447531700134 	 0.3406970500946045 	 1.6007812023162842 	 
2025-08-04 20:12:39.734664 test begin: paddle.nn.functional.avg_pool2d(Tensor([8, 256, 240, 104],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([8, 256, 240, 104],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51118080 	 52538 	 10.465814113616943 	 21.868743181228638 	 0.19750142097473145 	 0.4253056049346924 	 17.55919909477234 	 82.69638180732727 	 0.34131288528442383 	 1.6062393188476562 	 
2025-08-04 20:14:54.288582 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 776, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(Tensor([2, 776, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", ) 	 50855936 	 56552 	 9.976337194442749 	 23.938422918319702 	 0.18021631240844727 	 0.4325582981109619 	 23.972667694091797 	 28.20618724822998 	 0.43315672874450684 	 0.2547636032104492 	 
2025-08-04 20:16:21.340833 test begin: paddle.nn.functional.avg_pool3d(Tensor([517, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(Tensor([517, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", ) 	 50823168 	 56552 	 10.059632062911987 	 23.92525291442871 	 0.18181109428405762 	 0.43232011795043945 	 31.52449345588684 	 28.20058536529541 	 0.5716087818145752 	 0.25475049018859863 	 
2025-08-04 20:17:57.328082 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([127, 2048, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([127, 2048, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 	 50978816 	 56552 	 13.570658445358276 	 127.22312712669373 	 0.24524521827697754 	 0.5750339031219482 	 177.50645184516907 	 163.7031810283661 	 3.2076950073242188 	 0.17397379875183105 	 
2025-08-04 20:26:01.006978 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([127, 256, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([127, 256, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 	 50978816 	 56552 	 14.898144006729126 	 112.6271800994873 	 0.2693333625793457 	 2.036569118499756 	 163.25639653205872 	 162.60272884368896 	 2.9543721675872803 	 0.17284250259399414 	 
2025-08-04 20:33:35.267864 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 111, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f041b023eb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 20:43:58.907641 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 7, 111],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
W0804 20:43:59.939487 67446 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0804 20:43:59.941854 67446 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 7, 111],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 	 50921472 	 56552 	 21.764603853225708 	 43.60450053215027 	 0.39322996139526367 	 0.7880280017852783 	 173.94787669181824 	 55.526121616363525 	 3.143062114715576 	 0.3344554901123047 	 
2025-08-04 20:48:59.117079 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 64, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f44ad1d7ac0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 20:59:11.610405 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 111, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
W0804 20:59:12.581364 68057 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0804 20:59:12.583237 68057 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4fb9cbafb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:09:26.647722 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 7, 111],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
W0804 21:09:27.626111 68374 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0804 21:09:27.627910 68374 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 7, 111],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 	 50921472 	 56552 	 23.465224027633667 	 56.04757618904114 	 0.4241979122161865 	 1.3298015594482422 	 173.46024703979492 	 54.3454864025116 	 3.1330041885375977 	 0.32752132415771484 	 
2025-08-04 21:14:40.421017 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 507, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7c9fe33ac0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:25:00.303100 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 32401, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
W0804 21:25:01.915982 68931 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0804 21:25:01.919428 68931 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 32401, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 	 50804768 	 56552 	 13.543716430664062 	 126.8407928943634 	 0.24468564987182617 	 0.5732097625732422 	 176.94486713409424 	 163.1462173461914 	 3.197755813598633 	 0.1735670566558838 	 
2025-08-04 21:33:05.424277 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 4051, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 4051, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 	 50815744 	 56552 	 15.077085971832275 	 112.47278714179993 	 0.27240419387817383 	 2.0325381755828857 	 162.85651850700378 	 162.1397430896759 	 2.941876173019409 	 0.17244863510131836 	 
2025-08-04 21:40:41.431866 test begin: paddle.nn.functional.batch_norm(Tensor([30, 40, 50, 847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), use_global_stats=True, data_format="NHWC", )
[Prof] paddle.nn.functional.batch_norm 	 paddle.nn.functional.batch_norm(Tensor([30, 40, 50, 847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), use_global_stats=True, data_format="NHWC", ) 	 50823388 	 29508 	 11.04342532157898 	 11.068467140197754 	 0.38227272033691406 	 0.3830411434173584 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([30, 40, 50, 847]) and output[0] has a shape of torch.Size([30, 847, 40, 50]).
2025-08-04 21:43:05.273351 test begin: paddle.nn.functional.batch_norm(Tensor([30, 40, 706, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", )
[Prof] paddle.nn.functional.batch_norm 	 paddle.nn.functional.batch_norm(Tensor([30, 40, 706, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", ) 	 50832240 	 29508 	 10.949158668518066 	 10.969571113586426 	 0.3791463375091553 	 0.37990546226501465 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([30, 40, 706, 60]) and output[0] has a shape of torch.Size([30, 60, 40, 706]).
2025-08-04 21:45:12.508148 test begin: paddle.nn.functional.batch_norm(Tensor([30, 565, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", )
[Prof] paddle.nn.functional.batch_norm 	 paddle.nn.functional.batch_norm(Tensor([30, 565, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", ) 	 50850240 	 29508 	 11.12999415397644 	 11.125312089920044 	 0.3854200839996338 	 0.3852403163909912 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([30, 565, 50, 60]) and output[0] has a shape of torch.Size([30, 60, 565, 50]).
2025-08-04 21:47:25.403887 test begin: paddle.nn.functional.batch_norm(Tensor([424, 40, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", )
[Prof] paddle.nn.functional.batch_norm 	 paddle.nn.functional.batch_norm(Tensor([424, 40, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", ) 	 50880240 	 29508 	 10.015598058700562 	 10.247869968414307 	 0.34678149223327637 	 0.35487842559814453 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([424, 40, 50, 60]) and output[0] has a shape of torch.Size([424, 60, 40, 50]).
2025-08-04 21:49:26.341815 test begin: paddle.nn.functional.bilinear(Tensor([25401601, 1],"float32"), Tensor([25401601, 2],"float32"), Tensor([4, 1, 2],"float32"), Tensor([1, 4],"float32"), None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fed7c8432b0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:59:31.016474 test begin: paddle.nn.functional.bilinear(Tensor([3, 1],"float32"), Tensor([3, 12700801],"float32"), Tensor([4, 1, 12700801],"float32"), Tensor([1, 4],"float32"), None, )
W0804 21:59:32.454874 70371 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.bilinear 	 paddle.nn.functional.bilinear(Tensor([3, 1],"float32"), Tensor([3, 12700801],"float32"), Tensor([4, 1, 12700801],"float32"), Tensor([1, 4],"float32"), None, ) 	 88905614 	 2209 	 7.502132892608643 	 101.91182112693787 	 0.0533604621887207 	 0.7602834701538086 	 14.718635082244873 	 109.88138127326965 	 0.09064292907714844 	 0.5514345169067383 	 
2025-08-04 22:03:29.792751 test begin: paddle.nn.functional.bilinear(Tensor([3, 1],"float32"), Tensor([3, 16934401],"float32"), Tensor([4, 1, 16934401],"float32"), Tensor([1, 4],"float32"), None, )
[Prof] paddle.nn.functional.bilinear 	 paddle.nn.functional.bilinear(Tensor([3, 1],"float32"), Tensor([3, 16934401],"float32"), Tensor([4, 1, 16934401],"float32"), Tensor([1, 4],"float32"), None, ) 	 118540814 	 2209 	 10.076382398605347 	 133.1033296585083 	 0.057586669921875 	 0.7977075576782227 	 19.40559196472168 	 146.24536752700806 	 0.09804248809814453 	 0.6032915115356445 	 
2025-08-04 22:08:41.942161 test begin: paddle.nn.functional.bilinear(Tensor([5, 5],"float32"), Tensor([5, 10161],"float32"), Tensor([1000, 5, 10161],"float32"), Tensor([1, 1000],"float32"), None, )
[Prof] paddle.nn.functional.bilinear 	 paddle.nn.functional.bilinear(Tensor([5, 5],"float32"), Tensor([5, 10161],"float32"), Tensor([1000, 5, 10161],"float32"), Tensor([1, 1000],"float32"), None, ) 	 50856830 	 2209 	 69.54972648620605 	 94.54020833969116 	 0.008028745651245117 	 0.0002498626708984375 	 87.24495267868042 	 228.52519536018372 	 0.006689786911010742 	 0.00021529197692871094 	 
2025-08-04 22:16:43.384231 test begin: paddle.nn.functional.bilinear(Tensor([50803201, 1],"float32"), Tensor([50803201, 2],"float32"), Tensor([4, 1, 2],"float32"), Tensor([1, 4],"float32"), None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f60a6e62bf0>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754317603 (unix time) try "date -d @1754317603" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x11289) received by PID 70281 (TID 0x7f609bfff640) from PID 70281 ***]

2025-08-04 22:26:53.718188 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([16, 10164, 313],"float32"), Tensor([16, 10164, 313],"float32"), weight=Tensor([16, 10164, 313],"float32"), reduction="sum", )
W0804 22:26:56.234241 71392 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([16, 10164, 313],"float32"), Tensor([16, 10164, 313],"float32"), weight=Tensor([16, 10164, 313],"float32"), reduction="sum", ) 	 152703936 	 9554 	 10.019105434417725 	 10.08493947982788 	 0.26761937141418457 	 0.21549057960510254 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 22:27:37.272524 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([16, 11109, 286],"float32"), Tensor([16, 11109, 286],"float32"), weight=Tensor([16, 11109, 286],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([16, 11109, 286],"float32"), Tensor([16, 11109, 286],"float32"), weight=Tensor([16, 11109, 286],"float32"), reduction="sum", ) 	 152504352 	 9554 	 10.011198282241821 	 10.077526807785034 	 0.26724672317504883 	 0.21535515785217285 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 22:28:19.673751 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([16, 12096, 263],"float32"), Tensor([16, 12096, 263],"float32"), weight=Tensor([16, 12096, 263],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([16, 12096, 263],"float32"), Tensor([16, 12096, 263],"float32"), weight=Tensor([16, 12096, 263],"float32"), reduction="sum", ) 	 152699904 	 9554 	 10.021984338760376 	 10.091540813446045 	 0.2676122188568115 	 0.21550369262695312 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 22:29:01.102546 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([16, 39691, 80],"float32"), Tensor([16, 39691, 80],"float32"), weight=Tensor([16, 39691, 80],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([16, 39691, 80],"float32"), Tensor([16, 39691, 80],"float32"), weight=Tensor([16, 39691, 80],"float32"), reduction="sum", ) 	 152413440 	 9554 	 10.001674890518188 	 10.071983814239502 	 0.26711106300354004 	 0.21518445014953613 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 22:29:42.951110 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([53, 12096, 80],"float32"), Tensor([53, 12096, 80],"float32"), weight=Tensor([53, 12096, 80],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([53, 12096, 80],"float32"), Tensor([53, 12096, 80],"float32"), weight=Tensor([53, 12096, 80],"float32"), reduction="sum", ) 	 153861120 	 9554 	 10.103346347808838 	 10.160886764526367 	 0.2697947025299072 	 0.21712470054626465 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 22:30:23.569480 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([58, 11109, 80],"float32"), Tensor([58, 11109, 80],"float32"), weight=Tensor([58, 11109, 80],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([58, 11109, 80],"float32"), Tensor([58, 11109, 80],"float32"), weight=Tensor([58, 11109, 80],"float32"), reduction="sum", ) 	 154637280 	 9554 	 10.146481037139893 	 10.210272312164307 	 0.27103686332702637 	 0.21815872192382812 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 22:31:07.224154 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([63, 10164, 80],"float32"), Tensor([63, 10164, 80],"float32"), weight=Tensor([63, 10164, 80],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([63, 10164, 80],"float32"), Tensor([63, 10164, 80],"float32"), weight=Tensor([63, 10164, 80],"float32"), reduction="sum", ) 	 153679680 	 9554 	 10.08625864982605 	 10.163809537887573 	 0.2693490982055664 	 0.21684551239013672 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 22:31:52.324459 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([16, 300, 10585],"float32"), Tensor([16, 300, 10585],"float32"), weight=Tensor([16, 300, 10585],"float32"), reduction="none", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([16, 300, 10585],"float32"), Tensor([16, 300, 10585],"float32"), weight=Tensor([16, 300, 10585],"float32"), reduction="none", ) 	 152424000 	 9646 	 10.020175218582153 	 21.358720779418945 	 0.3542354106903076 	 0.37728071212768555 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 22:32:45.445940 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([16, 39691, 80],"float32"), Tensor([16, 39691, 80],"float32"), weight=Tensor([16, 39691, 80],"float32"), reduction="none", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([16, 39691, 80],"float32"), Tensor([16, 39691, 80],"float32"), weight=Tensor([16, 39691, 80],"float32"), reduction="none", ) 	 152413440 	 9646 	 10.017473936080933 	 21.356963872909546 	 0.354184627532959 	 0.3772766590118408 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 22:33:39.607694 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([2117, 300, 80],"float32"), Tensor([2117, 300, 80],"float32"), weight=Tensor([2117, 300, 80],"float32"), reduction="none", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([2117, 300, 80],"float32"), Tensor([2117, 300, 80],"float32"), weight=Tensor([2117, 300, 80],"float32"), reduction="none", ) 	 152424000 	 9646 	 10.02209997177124 	 21.357282638549805 	 0.3542170524597168 	 0.377321720123291 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 22:34:37.034926 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([300, 169345],"float32"), Tensor([300, 169345],"float32"), weight=Tensor([300, 169345],"float32"), reduction="none", pos_weight=None, )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([300, 169345],"float32"), Tensor([300, 169345],"float32"), weight=Tensor([300, 169345],"float32"), reduction="none", pos_weight=None, ) 	 152410500 	 9646 	 10.66021180152893 	 21.355106115341187 	 0.35407423973083496 	 0.3773233890533447 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 22:35:34.327881 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([50804, 1000],"float32"), Tensor([50804, 1000],"float32"), weight=Tensor([50804, 1000],"float32"), reduction="none", pos_weight=None, )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([50804, 1000],"float32"), Tensor([50804, 1000],"float32"), weight=Tensor([50804, 1000],"float32"), reduction="none", pos_weight=None, ) 	 152412000 	 9646 	 10.022367238998413 	 21.355513334274292 	 0.35401010513305664 	 0.3773040771484375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 22:36:30.859809 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([512, 28, 3544],"float32"), Tensor([512, 28, 3544],"float32"), weight=Tensor([512, 1, 3544],"float32"), reduction="mean", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([512, 28, 3544],"float32"), Tensor([512, 28, 3544],"float32"), weight=Tensor([512, 1, 3544],"float32"), reduction="mean", ) 	 103428096 	 9646 	 10.063385009765625 	 21.623368740081787 	 0.21295928955078125 	 0.28658390045166016 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 22:37:22.931914 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([512, 3544, 28],"float32"), Tensor([512, 3544, 28],"float32"), weight=Tensor([512, 3544, 1],"float32"), reduction="mean", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([512, 3544, 28],"float32"), Tensor([512, 3544, 28],"float32"), weight=Tensor([512, 3544, 1],"float32"), reduction="mean", ) 	 103428096 	 9646 	 10.05834412574768 	 21.576826095581055 	 0.21289992332458496 	 0.28585338592529297 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 22:38:15.824226 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), weight=Tensor([64801, 1, 1],"float32"), reduction="mean", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), weight=Tensor([64801, 1, 1],"float32"), reduction="mean", ) 	 101672769 	 9646 	 10.004466772079468 	 21.517468452453613 	 0.21174192428588867 	 0.28509950637817383 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 22:39:05.447566 test begin: paddle.nn.functional.celu(Tensor([1587601, 4, 4],"float64"), 0.2, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([1587601, 4, 4],"float64"), 0.2, None, ) 	 25401616 	 32447 	 9.958898782730103 	 9.86298942565918 	 0.31369996070861816 	 0.31056666374206543 	 14.50597858428955 	 14.579816341400146 	 0.4569225311279297 	 0.4592149257659912 	 
2025-08-04 22:39:56.597369 test begin: paddle.nn.functional.celu(Tensor([1587601, 4, 4],"float64"), 1.0, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([1587601, 4, 4],"float64"), 1.0, None, ) 	 25401616 	 32447 	 9.961093187332153 	 9.858276844024658 	 0.31372594833374023 	 0.31046557426452637 	 14.513737916946411 	 14.580029964447021 	 0.45715904235839844 	 0.45918869972229004 	 
2025-08-04 22:40:46.680810 test begin: paddle.nn.functional.celu(Tensor([2, 3175201, 4],"float64"), 0.2, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([2, 3175201, 4],"float64"), 0.2, None, ) 	 25401608 	 32447 	 9.960828065872192 	 9.870925664901733 	 0.3137519359588623 	 0.310575008392334 	 14.513704061508179 	 14.579804182052612 	 0.45716190338134766 	 0.4592165946960449 	 
2025-08-04 22:41:38.150044 test begin: paddle.nn.functional.celu(Tensor([2, 3175201, 4],"float64"), 1.0, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([2, 3175201, 4],"float64"), 1.0, None, ) 	 25401608 	 32447 	 10.505194664001465 	 9.85787844657898 	 0.3137624263763428 	 0.31055378913879395 	 14.51348352432251 	 14.579712390899658 	 0.4571802616119385 	 0.4592018127441406 	 
2025-08-04 22:42:29.767391 test begin: paddle.nn.functional.celu(Tensor([2, 4, 3175201],"float64"), 0.2, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([2, 4, 3175201],"float64"), 0.2, None, ) 	 25401608 	 32447 	 9.960423707962036 	 9.86888337135315 	 0.31377291679382324 	 0.31058359146118164 	 14.513711929321289 	 14.579676866531372 	 0.45713353157043457 	 0.45926761627197266 	 
2025-08-04 22:43:20.167489 test begin: paddle.nn.functional.celu(Tensor([2, 4, 3175201],"float64"), 1.0, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([2, 4, 3175201],"float64"), 1.0, None, ) 	 25401608 	 32447 	 9.960500955581665 	 9.857730627059937 	 0.3137359619140625 	 0.3105452060699463 	 14.513661861419678 	 14.579884052276611 	 0.45715928077697754 	 0.45918798446655273 	 
2025-08-04 22:44:10.226735 test begin: paddle.nn.functional.celu(x=Tensor([1587601, 4, 4],"float64"), )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(x=Tensor([1587601, 4, 4],"float64"), ) 	 25401616 	 32447 	 9.960934162139893 	 10.575248718261719 	 0.31374621391296387 	 0.31050968170166016 	 14.513760805130005 	 14.579601764678955 	 0.4571259021759033 	 0.4592413902282715 	 
2025-08-04 22:45:03.267146 test begin: paddle.nn.functional.celu(x=Tensor([2, 3175201, 4],"float64"), )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(x=Tensor([2, 3175201, 4],"float64"), ) 	 25401608 	 32447 	 9.960937976837158 	 9.857818603515625 	 0.3137991428375244 	 0.3105275630950928 	 14.51492166519165 	 14.580795526504517 	 0.45712828636169434 	 0.4591948986053467 	 
2025-08-04 22:45:53.934308 test begin: paddle.nn.functional.celu(x=Tensor([2, 4, 3175201],"float64"), )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(x=Tensor([2, 4, 3175201],"float64"), ) 	 25401608 	 32447 	 9.966663360595703 	 9.861629009246826 	 0.31374263763427734 	 0.31055450439453125 	 14.513503074645996 	 14.579509258270264 	 0.45714664459228516 	 0.4591374397277832 	 
2025-08-04 22:46:45.173260 test begin: paddle.nn.functional.channel_shuffle(Tensor([176401, 4, 4, 9],"float64"), 3, "NHWC", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([176401, 4, 4, 9],"float64"), 3, "NHWC", ) 	 25401744 	 31913 	 10.065746784210205 	 9.489169597625732 	 0.32221412658691406 	 0.3039233684539795 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([176401, 4, 4, 9]) and output[0] has a shape of torch.Size([176401, 9, 4, 4]).
2025-08-04 22:47:16.081663 test begin: paddle.nn.functional.channel_shuffle(Tensor([176401, 4, 4, 9],"float64"), 3, "NHWC", None, )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([176401, 4, 4, 9],"float64"), 3, "NHWC", None, ) 	 25401744 	 31913 	 10.06234622001648 	 10.816983222961426 	 0.32227063179016113 	 0.30389976501464844 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([176401, 4, 4, 9]) and output[0] has a shape of torch.Size([176401, 9, 4, 4]).
2025-08-04 22:47:52.254635 test begin: paddle.nn.functional.channel_shuffle(Tensor([176401, 9, 4, 4],"float64"), 3, "NCHW", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([176401, 9, 4, 4],"float64"), 3, "NCHW", ) 	 25401744 	 31913 	 10.00960898399353 	 9.66079831123352 	 0.3204972743988037 	 0.3093752861022949 	 10.00634217262268 	 9.660423755645752 	 0.32042789459228516 	 0.3093907833099365 	 
2025-08-04 22:48:32.756612 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 352801, 4, 9],"float64"), 3, "NHWC", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 352801, 4, 9],"float64"), 3, "NHWC", ) 	 25401672 	 31913 	 10.044591188430786 	 41.61838889122009 	 0.32178783416748047 	 1.3324494361877441 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 352801, 4, 9]) and output[0] has a shape of torch.Size([2, 9, 352801, 4]).
2025-08-04 22:49:39.361438 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 352801, 4, 9],"float64"), 3, "NHWC", None, )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 352801, 4, 9],"float64"), 3, "NHWC", None, ) 	 25401672 	 31913 	 11.25634479522705 	 41.60707688331604 	 0.3216085433959961 	 1.3324601650238037 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 352801, 4, 9]) and output[0] has a shape of torch.Size([2, 9, 352801, 4]).
2025-08-04 22:50:45.564971 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 4, 352801, 9],"float64"), 3, "NHWC", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 4, 352801, 9],"float64"), 3, "NHWC", ) 	 25401672 	 31913 	 10.05112361907959 	 41.624576568603516 	 0.3216724395751953 	 1.332533836364746 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 4, 352801, 9]) and output[0] has a shape of torch.Size([2, 9, 4, 352801]).
2025-08-04 22:51:49.646366 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 4, 352801, 9],"float64"), 3, "NHWC", None, )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 4, 352801, 9],"float64"), 3, "NHWC", None, ) 	 25401672 	 31913 	 10.044472932815552 	 41.63610863685608 	 0.3216710090637207 	 1.3325107097625732 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 4, 352801, 9]) and output[0] has a shape of torch.Size([2, 9, 4, 352801]).
2025-08-04 22:52:54.645414 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 9, 352801, 4],"float64"), 3, "NCHW", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 9, 352801, 4],"float64"), 3, "NCHW", ) 	 25401672 	 31913 	 10.01804256439209 	 9.651591777801514 	 0.3207814693450928 	 0.3090982437133789 	 10.03474235534668 	 9.649264335632324 	 0.32139086723327637 	 0.30905795097351074 	 
2025-08-04 22:53:35.147102 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 9, 4, 352801],"float64"), 3, "NCHW", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 9, 4, 352801],"float64"), 3, "NCHW", ) 	 25401672 	 31913 	 11.219706058502197 	 9.651473045349121 	 0.3207895755767822 	 0.30904579162597656 	 10.034324884414673 	 9.649236679077148 	 0.3213667869567871 	 0.30902814865112305 	 
2025-08-04 22:54:17.725320 test begin: paddle.nn.functional.conv1d(Tensor([16, 125, 25500],"float32"), Tensor([1, 125, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
W0804 22:54:18.583585 72445 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([16, 125, 25500],"float32"), Tensor([1, 125, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 51000126 	 30827 	 10.00859022140503 	 5.009699106216431 	 0.11046314239501953 	 0.0830540657043457 	 25.548866748809814 	 10.352445840835571 	 0.1411435604095459 	 0.06848764419555664 	 
2025-08-04 22:55:12.513186 test begin: paddle.nn.functional.conv1d(Tensor([16, 64, 49613],"float32"), Tensor([1, 64, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([16, 64, 49613],"float32"), Tensor([1, 64, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50803777 	 30827 	 10.64595627784729 	 5.195512533187866 	 0.11508393287658691 	 0.08610844612121582 	 22.69589924812317 	 11.116037130355835 	 0.12540078163146973 	 0.07356476783752441 	 
2025-08-04 22:56:05.307975 test begin: paddle.nn.functional.conv1d(Tensor([28, 32, 58081],"float32"), Tensor([1, 32, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([28, 32, 58081],"float32"), Tensor([1, 32, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 52040609 	 30827 	 12.188712120056152 	 5.705360174179077 	 0.13465619087219238 	 0.09455156326293945 	 23.79758095741272 	 12.296825885772705 	 0.13156485557556152 	 0.0813298225402832 	 
2025-08-04 22:57:04.833238 test begin: paddle.nn.functional.conv1d(Tensor([28, 32, 58081],"float32"), Tensor([32, 32, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([28, 32, 58081],"float32"), Tensor([32, 32, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 52041632 	 30827 	 20.955086708068848 	 30.689706802368164 	 0.17394423484802246 	 0.5086061954498291 	 42.9073166847229 	 395.0918867588043 	 0.17789649963378906 	 2.6178524494171143 	 
2025-08-04 23:05:16.467096 test begin: paddle.nn.functional.conv1d(Tensor([32, 32, 49613],"float32"), Tensor([1, 32, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([32, 32, 49613],"float32"), Tensor([1, 32, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50803745 	 30827 	 11.855901956558228 	 5.5000410079956055 	 0.13089847564697266 	 0.09114551544189453 	 22.817293405532837 	 11.499089241027832 	 0.1260063648223877 	 0.07602214813232422 	 
2025-08-04 23:06:09.108287 test begin: paddle.nn.functional.conv1d(Tensor([32, 32, 49613],"float32"), Tensor([32, 32, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([32, 32, 49613],"float32"), Tensor([32, 32, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50804768 	 30827 	 20.286436557769775 	 29.15685772895813 	 0.1683976650238037 	 0.4832789897918701 	 40.86951661109924 	 341.927001953125 	 0.16943693161010742 	 2.2644641399383545 	 
2025-08-04 23:13:23.195858 test begin: paddle.nn.functional.conv1d(Tensor([32, 64, 25500],"float32"), Tensor([1, 64, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([32, 64, 25500],"float32"), Tensor([1, 64, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 52224065 	 30827 	 10.667990446090698 	 5.2723681926727295 	 0.11779093742370605 	 0.08740687370300293 	 19.726490020751953 	 10.794523239135742 	 0.08196878433227539 	 0.07145524024963379 	 
2025-08-04 23:14:10.626853 test begin: paddle.nn.functional.conv1d_transpose(Tensor([1, 24807, 7],"float32"), Tensor([24807, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([1, 24807, 7],"float32"), Tensor([24807, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50978641 	 14413 	 32.4561505317688 	 33.987212896347046 	 0.7678670883178711 	 0.804124116897583 	 19.121835947036743 	 15.448857069015503 	 0.27161455154418945 	 0.27382755279541016 	 
2025-08-04 23:15:54.016924 test begin: paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 7],"float32"), Tensor([512, 12404, 8],"float32"), bias=Tensor([12404],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 7],"float32"), Tensor([512, 12404, 8],"float32"), bias=Tensor([12404],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50822772 	 14413 	 3.8374366760253906 	 3.8780643939971924 	 0.09077262878417969 	 0.09175443649291992 	 303.1030375957489 	 138.52710390090942 	 4.2874977588653564 	 2.452775716781616 	 
2025-08-04 23:23:25.233758 test begin: paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 7],"float32"), Tensor([512, 256, 388],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa8cad63f70>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754321608 (unix time) try "date -d @1754321608" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1167b) received by PID 71291 (TID 0x7fa8cacf3640) from PID 71291 ***]

2025-08-04 23:33:35.903687 test begin: paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 99226],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0804 23:33:43.649616 73860 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0804 23:33:43.667143 73860 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb60985ac50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:43:49.341223 test begin: paddle.nn.functional.conv1d_transpose(Tensor([14176, 512, 7],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0804 23:43:50.404325 74333 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0804 23:43:50.427249 74333 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7feb5f4e2d40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:54:15.110129 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 24807, 7],"float32"), Tensor([24807, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0804 23:54:16.057725 74722 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0804 23:54:16.114152 74722 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 24807, 7],"float32"), Tensor([24807, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 51152290 	 14413 	 31.84847116470337 	 33.83083629608154 	 0.7535953521728516 	 0.8005640506744385 	 18.625340461730957 	 23.060081243515015 	 0.26892638206481934 	 0.40894317626953125 	 
2025-08-04 23:56:07.589594 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 256, 28],"float32"), Tensor([256, 128, 1551],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0cafb16b00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:06:22.466380 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 256, 28],"float32"), Tensor([256, 24807, 8],"float32"), bias=Tensor([24807],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0805 00:06:23.488874 75446 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 00:06:23.539994 75446 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f68ecaeecb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:16:34.044078 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 256, 99226],"float32"), Tensor([256, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0805 00:16:35.054561 76052 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 00:16:35.069378 76052 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f00b9f52cb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:26:44.307582 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 49613, 28],"float32"), Tensor([49613, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0805 00:26:45.268941 76775 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 00:26:45.324308 76775 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 49613, 28],"float32"), Tensor([49613, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 53582168 	 14413 	 63.849669456481934 	 66.85514044761658 	 1.5113112926483154 	 1.581969976425171 	 26.610101222991943 	 27.17432737350464 	 0.31261444091796875 	 0.4819817543029785 	 
2025-08-05 00:29:51.795917 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 49613],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa515b2add0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:40:06.355024 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 7],"float32"), Tensor([512, 12404, 8],"float32"), bias=Tensor([12404],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0805 00:40:07.278657 77430 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 00:40:07.327898 77430 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 7],"float32"), Tensor([512, 12404, 8],"float32"), bias=Tensor([12404],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50826356 	 14413 	 6.878159999847412 	 12.742573022842407 	 0.16262578964233398 	 0.30141353607177734 	 303.43072605133057 	 138.50087881088257 	 4.299078702926636 	 2.450360059738159 	 
2025-08-05 00:47:49.668096 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 7],"float32"), Tensor([512, 256, 388],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc397646950>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754326673 (unix time) try "date -d @1754326673" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x12e20) received by PID 77344 (TID 0x7fc392bf3640) from PID 77344 ***]

2025-08-05 00:58:00.824344 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 907201, 28],"float32"), Tensor([907201, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0805 00:58:15.031193 78208 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 00:58:15.877882 78208 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa2ffa0aa70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:08:32.398471 test begin: paddle.nn.functional.conv1d_transpose(Tensor([7088, 256, 28],"float32"), Tensor([256, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0805 01:08:33.380690 78789 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 01:08:33.391052 78789 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdf7261ed10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:18:43.855521 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 191, 260],"float32"), Tensor([1, 1, 191, 4],"float32"), )
W0805 01:18:44.805195 79117 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 01:18:44.809221 79117 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fda991aaf50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:29:12.958669 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 191, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), )
W0805 01:29:14.116364 79423 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 01:29:14.127190 79423 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 191, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50851856 	 9096 	 9.987793445587158 	 10.158072233200073 	 1.1219689846038818 	 1.235605001449585 	 107.40041637420654 	 119.17538619041443 	 4.017571926116943 	 4.466323375701904 	 
2025-08-05 01:33:24.261865 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 192, 259],"float32"), Tensor([1, 1, 192, 4],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5a95cf6950>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:43:52.465615 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 192, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), )
W0805 01:43:56.885061 79972 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 01:43:56.913648 79972 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 192, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50921488 	 9096 	 10.005531072616577 	 10.063967943191528 	 1.1237123012542725 	 1.130617380142212 	 107.52599334716797 	 119.37777805328369 	 4.0269694328308105 	 4.473050355911255 	 
2025-08-05 01:48:05.489755 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 193, 258],"float32"), Tensor([1, 1, 193, 4],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff26a7b6b30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:58:33.799324 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 193, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), )
W0805 01:58:37.241762 80481 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 01:58:37.261736 80481 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 193, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50989072 	 9096 	 10.764882326126099 	 10.091109275817871 	 1.1261944770812988 	 1.1329729557037354 	 107.8253059387207 	 119.462575674057 	 4.040907859802246 	 4.476385116577148 	 
2025-08-05 02:02:53.435399 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 193],"float32"), Tensor([1, 1, 4, 193],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa60b39ab30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 02:13:17.870507 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 193],"float32"), Tensor([1, 1, 4, 4],"float32"), )
W0805 02:13:20.440388 81095 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 02:13:20.456316 81095 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 193],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50989072 	 9096 	 10.07513689994812 	 10.14045763015747 	 1.1317667961120605 	 1.1393165588378906 	 108.83402490615845 	 119.9173686504364 	 4.0817766189575195 	 4.497473239898682 	 
2025-08-05 02:17:31.158238 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 68161552 	 9096 	 13.449508428573608 	 14.48238468170166 	 1.5122177600860596 	 1.520148754119873 	 146.06339025497437 	 160.57252168655396 	 5.452157258987427 	 6.001864433288574 	 
2025-08-05 02:23:10.143946 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 192],"float32"), Tensor([1, 1, 4, 192],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa64789be50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 02:33:34.233666 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 192],"float32"), Tensor([1, 1, 4, 4],"float32"), )
W0805 02:33:40.990149 81461 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 02:33:41.007517 81461 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 192],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50921488 	 9096 	 10.074352025985718 	 10.15200662612915 	 1.1306633949279785 	 1.1393938064575195 	 108.49548077583313 	 119.45974826812744 	 4.073535919189453 	 4.476266145706177 	 
2025-08-05 02:37:53.754343 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 68690960 	 9096 	 13.501535415649414 	 13.98172664642334 	 1.5168347358703613 	 1.5336635112762451 	 146.76339292526245 	 162.02617502212524 	 5.855536937713623 	 6.07419490814209 	 
2025-08-05 02:43:34.582268 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 191],"float32"), Tensor([1, 1, 4, 191],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7faa986da710>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 02:53:44.647426 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 191],"float32"), Tensor([1, 1, 4, 4],"float32"), )
W0805 02:53:45.605072 81882 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 02:53:45.614729 81882 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 191],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50851856 	 9096 	 10.031724452972412 	 10.095298528671265 	 1.1265714168548584 	 1.1342074871063232 	 108.40718650817871 	 119.31565713882446 	 4.0528576374053955 	 4.467868804931641 	 
2025-08-05 02:57:55.054617 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 69222416 	 9096 	 13.600705623626709 	 14.58882474899292 	 1.5279974937438965 	 1.5395936965942383 	 147.55762338638306 	 163.0208704471588 	 5.538044691085815 	 6.099958419799805 	 
2025-08-05 03:03:40.147011 test begin: paddle.nn.functional.conv2d(Tensor([752, 1, 260, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([752, 1, 260, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50835216 	 9096 	 11.024282932281494 	 10.097778558731079 	 1.1237401962280273 	 1.1335656642913818 	 115.07883858680725 	 115.79304790496826 	 4.551636457443237 	 4.341220140457153 	 
2025-08-05 03:07:55.784177 test begin: paddle.nn.functional.conv2d(Tensor([758, 1, 259, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([758, 1, 259, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50847414 	 9096 	 10.017725229263306 	 10.082289457321167 	 1.1255524158477783 	 1.1327059268951416 	 114.47644853591919 	 115.52137207984924 	 4.289712905883789 	 4.323792219161987 	 
2025-08-05 03:12:07.599293 test begin: paddle.nn.functional.conv2d(Tensor([764, 1, 258, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([764, 1, 258, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50854912 	 9096 	 10.012688636779785 	 10.085386991500854 	 1.124943733215332 	 1.133056640625 	 113.72814869880676 	 115.04938888549805 	 4.259451866149902 	 4.31244158744812 	 
2025-08-05 03:16:18.194318 test begin: paddle.nn.functional.conv2d_transpose(Tensor([16, 32, 320, 320],"float32"), Tensor([32, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([16, 32, 320, 320],"float32"), Tensor([32, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 52428929 	 19463 	 12.211320877075195 	 12.262558221817017 	 0.21390891075134277 	 0.21474146842956543 	 13.033395528793335 	 12.092102289199829 	 0.09769940376281738 	 0.10563254356384277 	 
2025-08-05 03:17:08.972507 test begin: paddle.nn.functional.conv2d_transpose(Tensor([16, 64, 156, 320],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([16, 64, 156, 320],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 51118337 	 19463 	 10.08132290840149 	 10.15433669090271 	 0.1765756607055664 	 0.17790603637695312 	 10.933754444122314 	 10.44967246055603 	 0.08196711540222168 	 0.09126710891723633 	 
2025-08-05 03:17:52.646870 test begin: paddle.nn.functional.conv2d_transpose(Tensor([16, 64, 320, 156],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([16, 64, 320, 156],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 51118337 	 19463 	 10.089114904403687 	 10.173828363418579 	 0.1767885684967041 	 0.17815256118774414 	 10.740156650543213 	 10.448636293411255 	 0.08052182197570801 	 0.09128236770629883 	 
2025-08-05 03:18:38.527741 test begin: paddle.nn.functional.conv2d_transpose(Tensor([4, 56, 480, 480],"float32"), Tensor([56, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([4, 56, 480, 480],"float32"), Tensor([56, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 51609825 	 19463 	 10.435921907424927 	 10.502666711807251 	 0.18281149864196777 	 0.1839885711669922 	 11.939703941345215 	 11.911313772201538 	 0.08953595161437988 	 0.1040046215057373 	 
2025-08-05 03:19:24.277347 test begin: paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 414, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 414, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 50872577 	 19463 	 10.046486616134644 	 10.114893913269043 	 0.17600345611572266 	 0.17722535133361816 	 11.34491777420044 	 11.158372640609741 	 0.08506488800048828 	 0.09746146202087402 	 
2025-08-05 03:20:07.847534 test begin: paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 414],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 414],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 50872577 	 19463 	 10.030200242996216 	 10.078166246414185 	 0.17519259452819824 	 0.17651963233947754 	 11.33033013343811 	 11.141532182693481 	 0.08490753173828125 	 0.09726309776306152 	 
2025-08-05 03:20:51.323385 test begin: paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 58982657 	 19463 	 11.56538200378418 	 11.63777494430542 	 0.2025773525238037 	 0.20450186729431152 	 12.930577993392944 	 12.832406997680664 	 0.09709334373474121 	 0.1120760440826416 	 
2025-08-05 03:21:42.469424 test begin: paddle.nn.functional.conv2d_transpose(Tensor([8, 28, 480, 480],"float32"), Tensor([28, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([8, 28, 480, 480],"float32"), Tensor([28, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 51609713 	 19463 	 13.686651706695557 	 13.761193752288818 	 0.2397315502166748 	 0.24105501174926758 	 13.565366268157959 	 12.903050661087036 	 0.10170197486877441 	 0.1127922534942627 	 
2025-08-05 03:22:37.840468 test begin: paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 207, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 207, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 50872577 	 19463 	 10.041746139526367 	 10.107646465301514 	 0.1758718490600586 	 0.17710614204406738 	 10.976783514022827 	 10.784766912460327 	 0.08226132392883301 	 0.09416675567626953 	 
2025-08-05 03:23:20.678581 test begin: paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 320, 320],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 320, 320],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 52429057 	 19463 	 10.345100164413452 	 10.405532598495483 	 0.1811833381652832 	 0.1822509765625 	 11.059568643569946 	 10.973129510879517 	 0.08293509483337402 	 0.09584641456604004 	 
2025-08-05 03:24:04.596181 test begin: paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 480, 207],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 480, 207],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 50872577 	 19463 	 10.046214818954468 	 10.114011287689209 	 0.17601323127746582 	 0.17712712287902832 	 10.88023328781128 	 10.78487777709961 	 0.08152914047241211 	 0.09417152404785156 	 
2025-08-05 03:24:48.601453 test begin: paddle.nn.functional.conv3d(Tensor([16538, 6, 8, 8, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([16538, 6, 8, 8, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", ) 	 50805060 	 1559 	 9.92516040802002 	 2.306971788406372 	 6.50624680519104 	 1.5119528770446777 	 68.60594606399536 	 28.54399871826172 	 22.48867392539978 	 9.357678174972534 	 
2025-08-05 03:26:40.889549 test begin: paddle.nn.functional.conv3d(Tensor([16538, 6, 8, 8, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([16538, 6, 8, 8, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50805392 	 1559 	 18.76127600669861 	 18.837153434753418 	 12.286547660827637 	 12.33575963973999 	 126.71060991287231 	 35.77318215370178 	 20.805588960647583 	 5.862380504608154 	 
2025-08-05 03:30:03.120453 test begin: paddle.nn.functional.conv3d(Tensor([33076, 3, 8, 8, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([33076, 3, 8, 8, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", ) 	 50805146 	 1559 	 29.60504651069641 	 41.07970976829529 	 9.704123497009277 	 13.464433431625366 	 375.62889790534973 	 73.67396926879883 	 35.1711163520813 	 6.90024471282959 	 
2025-08-05 03:38:47.484473 test begin: paddle.nn.functional.conv3d(Tensor([4, 3, 66151, 8, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2e8174fc40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 03:49:46.031755 test begin: paddle.nn.functional.conv3d(Tensor([4, 3, 8, 66151, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
W0805 03:49:46.988116 83354 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 03:49:46.995297 83354 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6167b26f20>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754337586 (unix time) try "date -d @1754337586" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14544) received by PID 83268 (TID 0x7f61630c1640) from PID 83268 ***]

2025-08-05 03:59:54.342278 test begin: paddle.nn.functional.conv3d(Tensor([4, 3, 8, 8, 66151],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
W0805 03:59:55.346383 83830 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 03:59:55.351059 83830 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe73fcaaf80>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754338194 (unix time) try "date -d @1754338194" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14719) received by PID 83737 (TID 0x7fe73b2ab640) from PID 83737 ***]

2025-08-05 04:10:02.308483 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 33076, 8, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
W0805 04:10:03.533686 92208 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 04:10:03.539568 92208 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 33076, 8, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", ) 	 50805060 	 1559 	 12.972105979919434 	 3.049009084701538 	 8.5030677318573 	 1.997755527496338 	 92.07448482513428 	 34.61810040473938 	 30.179388523101807 	 11.344078302383423 	 
2025-08-05 04:12:29.843928 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 33076, 8, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 33076, 8, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50805392 	 1559 	 19.026232957839966 	 19.104650259017944 	 12.460297107696533 	 12.512969493865967 	 139.02830481529236 	 136.24484062194824 	 18.28308606147766 	 22.29031538963318 	 
2025-08-05 04:17:47.399227 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 8, 33076, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 8, 33076, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", ) 	 50805060 	 1559 	 13.361693620681763 	 3.060779571533203 	 8.75845742225647 	 2.0044572353363037 	 91.18458318710327 	 19.037038564682007 	 29.888313055038452 	 6.2398459911346436 	 
2025-08-05 04:19:55.891532 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 8, 33076, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 8, 33076, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50805392 	 1559 	 18.75529384613037 	 18.82030463218689 	 12.282612800598145 	 12.325429201126099 	 137.88042569160461 	 143.67304229736328 	 18.135486841201782 	 31.32847499847412 	 
2025-08-05 04:25:17.242290 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 33076],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 33076],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", ) 	 50805060 	 1559 	 13.281840562820435 	 3.1386239528656006 	 8.70613169670105 	 2.057248592376709 	 93.33216762542725 	 20.671618938446045 	 30.593409061431885 	 6.774654388427734 	 
2025-08-05 04:27:29.518320 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 33076],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 33076],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50805392 	 1559 	 18.63574194908142 	 18.706244945526123 	 12.205458164215088 	 12.251270532608032 	 147.42195224761963 	 152.98327732086182 	 19.412002563476562 	 33.37429094314575 	 
2025-08-05 04:33:09.444909 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 2451, 2, 2, 2],"float32"), Tensor([2451, 12, 12, 12, 12],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d_transpose 	 paddle.nn.functional.conv3d_transpose(Tensor([2, 2451, 2, 2, 2],"float32"), Tensor([2451, 12, 12, 12, 12],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 	 50863164 	 3204 	 9.99558424949646 	 2.033705472946167 	 1.0633220672607422 	 0.21657347679138184 	 14.520332098007202 	 14.790208101272583 	 0.7715027332305908 	 1.45253324508667 	 
2025-08-05 04:33:52.999551 test begin: paddle.nn.functional.conv3d_transpose(Tensor([24807, 4, 8, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d_transpose 	 paddle.nn.functional.conv3d_transpose(Tensor([24807, 4, 8, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50805066 	 3204 	 50.35456705093384 	 86.3749840259552 	 7.99629807472229 	 13.772677659988403 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([24807, 6, 8, 6, 8]) and output[0] has a shape of torch.Size([24807, 6, 10, 10, 10]).
2025-08-05 04:42:41.808257 test begin: paddle.nn.functional.conv3d_transpose(Tensor([24807, 4, 8, 8, 8],"float32"), Tensor([4, 4, 3, 3, 3],"float32"), Tensor([4],"float32"), output_size=tuple(10,17,10,), padding="valid", stride=tuple(1,2,1,), dilation=1, groups=1, data_format="NCDHW", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f422a5629b0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 04:52:51.722576 test begin: paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 49613, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
W0805 04:52:53.420302 138004 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 04:52:53.433391 138004 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv3d_transpose 	 paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 49613, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50804042 	 3204 	 18.242056846618652 	 100.74029684066772 	 2.908780097961426 	 16.06952977180481 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([4, 6, 49613, 6, 8]) and output[0] has a shape of torch.Size([4, 6, 49615, 10, 10]).
2025-08-05 05:02:15.448154 test begin: paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 49613, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d_transpose 	 paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 49613, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50804042 	 3204 	 23.730890035629272 	 91.5548460483551 	 3.7827858924865723 	 14.604284524917603 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([4, 6, 8, 49611, 8]) and output[0] has a shape of torch.Size([4, 6, 10, 49615, 10]).
2025-08-05 05:11:44.302159 test begin: paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 49613],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d_transpose 	 paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 49613],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50804042 	 3204 	 17.98094868659973 	 98.05586838722229 	 2.8657901287078857 	 15.648468971252441 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([4, 6, 8, 6, 49613]) and output[0] has a shape of torch.Size([4, 6, 10, 10, 49615]).
2025-08-05 05:20:07.389180 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), Tensor([10],"int64"), margin=0.5, reduction="mean", name=None, )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), Tensor([10],"int64"), margin=0.5, reduction="mean", name=None, ) 	 101606430 	 5778 	 10.005931377410889 	 9.41789174079895 	 0.06738972663879395 	 0.06614947319030762 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:20:50.617562 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([16934401, 3],"float32"), Tensor([16934401, 3],"float32"), Tensor([16934401],"int64"), margin=0.5, reduction="mean", name=None, )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([16934401, 3],"float32"), Tensor([16934401, 3],"float32"), Tensor([16934401],"int64"), margin=0.5, reduction="mean", name=None, ) 	 118540807 	 5778 	 22.859909057617188 	 22.6156325340271 	 0.16774415969848633 	 0.1732332706451416 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:22:10.131333 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([25401601, 3],"float32"), Tensor([25401601, 3],"float32"), Tensor([25401601],"int64"), margin=0.5, reduction="mean", name=None, )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([25401601, 3],"float32"), Tensor([25401601, 3],"float32"), Tensor([25401601],"int64"), margin=0.5, reduction="mean", name=None, ) 	 177811207 	 5778 	 33.99717855453491 	 33.66477942466736 	 0.24945592880249023 	 0.2579021453857422 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:24:08.386975 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5],"int32"), margin=0.5, reduction="mean", )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5],"int32"), margin=0.5, reduction="mean", ) 	 50803215 	 5778 	 10.55057978630066 	 9.14044737815857 	 0.07105255126953125 	 0.06415319442749023 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:24:48.090789 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5],"int32"), margin=0.5, reduction="none", )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5],"int32"), margin=0.5, reduction="none", ) 	 50803215 	 5778 	 10.656402826309204 	 9.108322381973267 	 0.07490158081054688 	 0.06617021560668945 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:25:26.308594 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([50803201, 3],"float64"), Tensor([50803201, 3],"float64"), Tensor([50803201],"int32"), margin=0.5, reduction="mean", )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([50803201, 3],"float64"), Tensor([50803201, 3],"float64"), Tensor([50803201],"int32"), margin=0.5, reduction="mean", ) 	 355622407 	 5778 	 112.1220474243164 	 112.19796061515808 	 0.8215439319610596 	 0.8594861030578613 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:32:04.984300 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([50803201, 3],"float64"), Tensor([50803201, 3],"float64"), Tensor([50803201],"int32"), margin=0.5, reduction="none", )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([50803201, 3],"float64"), Tensor([50803201, 3],"float64"), Tensor([50803201],"int32"), margin=0.5, reduction="none", ) 	 355622407 	 5778 	 110.49364447593689 	 110.51869034767151 	 0.8860073089599609 	 0.9262826442718506 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:38:41.577468 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([8467201, 3],"float64"), Tensor([8467201, 3],"float64"), Tensor([8467201],"int32"), margin=0.5, reduction="mean", )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([8467201, 3],"float64"), Tensor([8467201, 3],"float64"), Tensor([8467201],"int32"), margin=0.5, reduction="mean", ) 	 59270407 	 5778 	 19.262782335281372 	 19.294925212860107 	 0.1410658359527588 	 0.14783334732055664 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:39:49.448881 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([8467201, 3],"float64"), Tensor([8467201, 3],"float64"), Tensor([8467201],"int32"), margin=0.5, reduction="none", )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([8467201, 3],"float64"), Tensor([8467201, 3],"float64"), Tensor([8467201],"int32"), margin=0.5, reduction="none", ) 	 59270407 	 5778 	 18.917211771011353 	 19.028781175613403 	 0.15149331092834473 	 0.15870928764343262 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:40:59.934672 test begin: paddle.nn.functional.cosine_similarity(Tensor([10, 12, 423361],"float32"), Tensor([10, 1, 423361],"float32"), axis=2, eps=1e-06, )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([10, 12, 423361],"float32"), Tensor([10, 1, 423361],"float32"), axis=2, eps=1e-06, ) 	 55036930 	 9454 	 10.0233633518219 	 13.303234815597534 	 0.08277654647827148 	 0.11085247993469238 	 29.426637172698975 	 66.74768328666687 	 0.1989600658416748 	 0.26692700386047363 	 
2025-08-05 05:43:03.909726 test begin: paddle.nn.functional.cosine_similarity(Tensor([10, 508033, 10],"float32"), Tensor([10, 1, 10],"float32"), axis=2, eps=1e-06, )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([10, 508033, 10],"float32"), Tensor([10, 1, 10],"float32"), axis=2, eps=1e-06, ) 	 50803400 	 9454 	 13.22990107536316 	 19.861153602600098 	 0.14269328117370605 	 0.17936396598815918 	 72.35886096954346 	 74.18206763267517 	 0.4350111484527588 	 0.3078939914703369 	 
2025-08-05 05:46:04.610315 test begin: paddle.nn.functional.cosine_similarity(Tensor([10, 508033, 10],"float32"), Tensor([10, 508033, 10],"float32"), axis=2, eps=1e-06, )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([10, 508033, 10],"float32"), Tensor([10, 508033, 10],"float32"), axis=2, eps=1e-06, ) 	 101606600 	 9454 	 20.85021162033081 	 24.18265652656555 	 0.22491097450256348 	 0.21820664405822754 	 53.785595417022705 	 73.71233773231506 	 0.4150426387786865 	 0.3328430652618408 	 
2025-08-05 05:48:58.994171 test begin: paddle.nn.functional.cosine_similarity(Tensor([210, 241921],"float32"), Tensor([210, 241921],"float32"), axis=-1, eps=1e-08, )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([210, 241921],"float32"), Tensor([210, 241921],"float32"), axis=-1, eps=1e-08, ) 	 101606820 	 9454 	 15.019650220870972 	 14.732320547103882 	 0.12433886528015137 	 0.12272405624389648 	 49.44882678985596 	 66.26751112937927 	 0.38151049613952637 	 0.27561283111572266 	 
2025-08-05 05:51:26.301832 test begin: paddle.nn.functional.cosine_similarity(Tensor([32, 1587601],"float32"), Tensor([32, 1587601],"float32"), )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([32, 1587601],"float32"), Tensor([32, 1587601],"float32"), ) 	 101606464 	 9454 	 14.487141847610474 	 15.29010272026062 	 0.11997294425964355 	 0.12735390663146973 	 49.52026724815369 	 66.64038324356079 	 0.3822331428527832 	 0.2771897315979004 	 
2025-08-05 05:53:55.081515 test begin: paddle.nn.functional.cosine_similarity(Tensor([396901, 128],"float32"), Tensor([396901, 128],"float32"), )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([396901, 128],"float32"), Tensor([396901, 128],"float32"), ) 	 101606656 	 9454 	 22.114930391311646 	 15.026658535003662 	 0.23849248886108398 	 0.13567399978637695 	 49.95289969444275 	 66.65053582191467 	 0.38543248176574707 	 0.299884557723999 	 
2025-08-05 05:56:37.197104 test begin: paddle.nn.functional.cosine_similarity(Tensor([423361, 12, 10],"float32"), Tensor([423361, 1, 10],"float32"), axis=2, eps=1e-06, )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([423361, 12, 10],"float32"), Tensor([423361, 1, 10],"float32"), axis=2, eps=1e-06, ) 	 55036930 	 9454 	 13.836182832717896 	 20.054202556610107 	 0.14918756484985352 	 0.18113136291503906 	 33.1675763130188 	 74.17684245109558 	 0.22418951988220215 	 0.3218860626220703 	 
2025-08-05 05:59:03.738935 test begin: paddle.nn.functional.cosine_similarity(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float32"), axis=-1, eps=1e-08, )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float32"), axis=-1, eps=1e-08, ) 	 101607424 	 9454 	 14.157464742660522 	 14.456907033920288 	 0.15269017219543457 	 0.15647673606872559 	 49.51647090911865 	 66.36172199249268 	 0.382174015045166 	 0.2999603748321533 	 
2025-08-05 06:01:29.998135 test begin: paddle.nn.functional.cross_entropy(Tensor([1, 1024, 50304],"float32"), Tensor([1, 1024, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7effbda22710>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 06:12:04.196789 test begin: paddle.nn.functional.cross_entropy(Tensor([1, 2048, 151936],"float32"), Tensor([1, 2048, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
W0805 06:12:09.099380 118346 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5c7b857040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 06:23:45.372482 test begin: paddle.nn.functional.cross_entropy(Tensor([1, 2048, 24807],"float32"), Tensor([1, 2048, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
W0805 06:23:49.464877 147645 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.cross_entropy 	 paddle.nn.functional.cross_entropy(Tensor([1, 2048, 24807],"float32"), Tensor([1, 2048, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, ) 	 50806784 	 8585 	 4.0663487911224365 	 368.81915187835693 	 0.09685945510864258 	 14.652089595794678 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 2048, 1]) and output[0] has a shape of torch.Size([1, 2048]).
2025-08-05 06:30:07.545955 test begin: paddle.nn.functional.cross_entropy(Tensor([1, 335, 151936],"float32"), Tensor([1, 335, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f94a3d6ebf0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 06:41:33.131740 test begin: paddle.nn.functional.cross_entropy(Tensor([1, 4096, 100352],"float32"), Tensor([1, 4096, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
W0805 06:41:42.599738 30167 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1a234a2e00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 06:52:47.031124 test begin: paddle.nn.functional.cross_entropy(Tensor([1, 4096, 12404],"float32"), Tensor([1, 4096, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
W0805 06:52:48.008863 57840 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.cross_entropy 	 paddle.nn.functional.cross_entropy(Tensor([1, 4096, 12404],"float32"), Tensor([1, 4096, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, ) 	 50810880 	 8585 	 3.3742616176605225 	 190.66095066070557 	 0.08040904998779297 	 7.57245135307312 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 4096, 1]) and output[0] has a shape of torch.Size([1, 4096]).
2025-08-05 06:56:07.159176 test begin: paddle.nn.functional.cross_entropy(Tensor([1, 507, 100352],"float32"), Tensor([1, 507, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa565c66c50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 07:07:06.777226 test begin: paddle.nn.functional.cross_entropy(Tensor([8, 1024, 6202],"float32"), Tensor([8, 1024, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
W0805 07:07:07.908751 94250 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.cross_entropy 	 paddle.nn.functional.cross_entropy(Tensor([8, 1024, 6202],"float32"), Tensor([8, 1024, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, ) 	 50814976 	 8585 	 3.746246576309204 	 99.69278025627136 	 0.08928990364074707 	 3.9596364498138428 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([8, 1024, 1]) and output[0] has a shape of torch.Size([8, 1024]).
2025-08-05 07:08:56.711075 test begin: paddle.nn.functional.cross_entropy(Tensor([8, 127, 50304],"float32"), Tensor([8, 127, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbced28ec50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 07:19:27.121965 test begin: paddle.nn.functional.dropout(Tensor([75760, 13412],"bfloat16"), 0.0, training=True, mode="upscale_in_train", )
W0805 07:19:49.017977 125926 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.dropout 	 paddle.nn.functional.dropout(Tensor([75760, 13412],"bfloat16"), 0.0, training=True, mode="upscale_in_train", ) 	 1016093120 	 66830 	 0.06493330001831055 	 0.690338134765625 	 1.8596649169921875e-05 	 0.00011014938354492188 	 2.1249282360076904 	 299.8269295692444 	 6.866455078125e-05 	 2.2951786518096924 	 combined
2025-08-05 07:25:13.216273 test begin: paddle.nn.functional.dropout(Tensor([77120, 13176],"bfloat16"), 0.0, training=True, mode="upscale_in_train", )
[Prof] paddle.nn.functional.dropout 	 paddle.nn.functional.dropout(Tensor([77120, 13176],"bfloat16"), 0.0, training=True, mode="upscale_in_train", ) 	 1016133120 	 66830 	 0.0680692195892334 	 0.46748781204223633 	 3.457069396972656e-05 	 7.891654968261719e-05 	 2.3726131916046143 	 300.0431411266327 	 7.2479248046875e-05 	 2.292666435241699 	 combined
2025-08-05 07:30:56.088064 test begin: paddle.nn.functional.dropout(Tensor([793810, 1280],"bfloat16"), 0.0, training=True, mode="upscale_in_train", )
[Prof] paddle.nn.functional.dropout 	 paddle.nn.functional.dropout(Tensor([793810, 1280],"bfloat16"), 0.0, training=True, mode="upscale_in_train", ) 	 1016076800 	 66830 	 0.06572294235229492 	 0.5169675350189209 	 1.52587890625e-05 	 8.797645568847656e-05 	 2.169731616973877 	 300.02870750427246 	 7.390975952148438e-05 	 2.2939069271087646 	 combined
2025-08-05 07:36:41.975046 test begin: paddle.nn.functional.dropout(Tensor([81680, 12440],"bfloat16"), 0.0, training=True, mode="upscale_in_train", )
[Prof] paddle.nn.functional.dropout 	 paddle.nn.functional.dropout(Tensor([81680, 12440],"bfloat16"), 0.0, training=True, mode="upscale_in_train", ) 	 1016099200 	 66830 	 0.07064199447631836 	 0.45783114433288574 	 5.125999450683594e-05 	 8.058547973632812e-05 	 2.2153162956237793 	 299.78369545936584 	 6.723403930664062e-05 	 2.2906179428100586 	 combined
2025-08-05 07:42:20.549896 test begin: paddle.nn.functional.elu(Tensor([1, 21504, 2363],"float32"), )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([1, 21504, 2363],"float32"), ) 	 50813952 	 33842 	 10.018949270248413 	 10.132682085037231 	 0.30236124992370605 	 0.30709266662597656 	 15.246926307678223 	 15.191232204437256 	 0.4601929187774658 	 0.4583265781402588 	 
2025-08-05 07:43:13.039150 test begin: paddle.nn.functional.elu(Tensor([1, 25401601, 2],"float32"), )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([1, 25401601, 2],"float32"), ) 	 50803202 	 33842 	 10.013047933578491 	 10.129236221313477 	 0.3021683692932129 	 0.3056955337524414 	 15.236407995223999 	 15.1884126663208 	 0.459836483001709 	 0.4582955837249756 	 
2025-08-05 07:44:07.254148 test begin: paddle.nn.functional.elu(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33842 	 10.017014026641846 	 11.937216520309448 	 0.3023655414581299 	 0.3069455623626709 	 15.249927520751953 	 15.189231634140015 	 0.4600687026977539 	 0.4582352638244629 	 
2025-08-05 07:45:02.340951 test begin: paddle.nn.functional.elu(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33842 	 10.013008117675781 	 10.126503229141235 	 0.3021664619445801 	 0.30571937561035156 	 15.243382453918457 	 15.178675889968872 	 0.45988965034484863 	 0.45826148986816406 	 
2025-08-05 07:45:54.797209 test begin: paddle.nn.functional.elu(Tensor([1182, 21504, 2],"float32"), )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([1182, 21504, 2],"float32"), ) 	 50835456 	 33842 	 10.020883083343506 	 10.135258674621582 	 0.30234718322753906 	 0.30590176582336426 	 15.259151935577393 	 15.192440271377563 	 0.4605128765106201 	 0.458538293838501 	 
2025-08-05 07:46:47.295188 test begin: paddle.nn.functional.elu(Tensor([15, 3386881],"float32"), 1.0, )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([15, 3386881],"float32"), 1.0, ) 	 50803215 	 33842 	 10.01175332069397 	 10.130778551101685 	 0.30225253105163574 	 0.305652379989624 	 15.240651607513428 	 15.183126211166382 	 0.46115636825561523 	 0.4582529067993164 	 
2025-08-05 07:47:41.615604 test begin: paddle.nn.functional.elu(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33842 	 10.015840768814087 	 10.150486707687378 	 0.30358004570007324 	 0.3069491386413574 	 15.2368483543396 	 15.190208673477173 	 0.45990610122680664 	 0.45830488204956055 	 
2025-08-05 07:48:41.068553 test begin: paddle.nn.functional.elu(Tensor([2540161, 20],"float32"), 1.0, )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([2540161, 20],"float32"), 1.0, ) 	 50803220 	 33842 	 10.013190984725952 	 10.130025625228882 	 0.30218505859375 	 0.30571603775024414 	 15.240913152694702 	 15.188211441040039 	 0.4599013328552246 	 0.4582486152648926 	 
2025-08-05 07:49:33.453787 test begin: paddle.nn.functional.embedding(Tensor([1, 4097],"int64"), weight=Tensor([12404, 8192],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, )
W0805 07:49:45.459297 37562 backward.cc:462] While running Node (EmbeddingGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.nn.functional.embedding 	 paddle.nn.functional.embedding(Tensor([1, 4097],"int64"), weight=Tensor([12404, 8192],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, ) 	 101617665 	 66306 	 10.106174230575562 	 26.427953243255615 	 0.15570402145385742 	 0.4070436954498291 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:50:12.549125 test begin: paddle.nn.functional.embedding(Tensor([101, 1024],"int64"), weight=Tensor([151936, 669],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, )
W0805 07:50:39.663942 39275 backward.cc:462] While running Node (EmbeddingGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.nn.functional.embedding 	 paddle.nn.functional.embedding(Tensor([101, 1024],"int64"), weight=Tensor([151936, 669],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, ) 	 101748608 	 66306 	 24.72132396697998 	 61.09048867225647 	 0.3808014392852783 	 0.9409646987915039 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:51:42.184072 test begin: paddle.nn.functional.embedding(Tensor([101, 1024],"int64"), weight=Tensor([24807, 4096],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, )
W0805 07:53:48.734045 42692 backward.cc:462] While running Node (EmbeddingGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.nn.functional.embedding 	 paddle.nn.functional.embedding(Tensor([101, 1024],"int64"), weight=Tensor([24807, 4096],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, ) 	 101712896 	 66306 	 118.35663199424744 	 365.80225348472595 	 1.8226633071899414 	 5.639646291732788 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:59:55.509726 test begin: paddle.nn.functional.embedding(Tensor([101, 4097],"int64"), weight=Tensor([100352, 1013],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, )
W0805 08:02:07.846639 63802 backward.cc:462] While running Node (EmbeddingGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.nn.functional.embedding 	 paddle.nn.functional.embedding(Tensor([101, 4097],"int64"), weight=Tensor([100352, 1013],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, ) 	 102070373 	 66306 	 124.91767001152039 	 367.6781542301178 	 1.9281284809112549 	 5.6672043800354 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 08:08:16.763076 test begin: paddle.nn.functional.embedding(Tensor([8, 1024],"int64"), weight=Tensor([24807, 4096],"float16"), padding_idx=None, sparse=False, name=None, )
[Prof] paddle.nn.functional.embedding 	 paddle.nn.functional.embedding(Tensor([8, 1024],"int64"), weight=Tensor([24807, 4096],"float16"), padding_idx=None, sparse=False, name=None, ) 	 101617664 	 66306 	 10.041775226593018 	 29.310329914093018 	 0.1546015739440918 	 0.4504098892211914 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 08:09:21.315359 test begin: paddle.nn.functional.embedding(Tensor([801, 1024],"int64"), weight=Tensor([50304, 2020],"float16"), padding_idx=None, sparse=False, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f66345ebe80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 08:19:30.281591 test begin: paddle.nn.functional.gather_tree(Tensor([100, 4, 8],"int64"), Tensor([100, 4, 8],"int64"), )
W0805 08:19:30.530433 140220 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([100, 4, 8],"int64"), Tensor([100, 4, 8],"int64"), ) 	 6400 	 1000 	 0.017339706420898438 	 217.77238845825195 	 4.9114227294921875e-05 	 0.00037789344787597656 	 None 	 None 	 None 	 None 	 combined
2025-08-05 08:23:08.835202 test begin: paddle.nn.functional.gather_tree(Tensor([100, 8, 4],"int64"), Tensor([100, 8, 4],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([100, 8, 4],"int64"), Tensor([100, 8, 4],"int64"), ) 	 6400 	 1000 	 0.010265588760375977 	 208.17148661613464 	 1.9550323486328125e-05 	 0.0002663135528564453 	 None 	 None 	 None 	 None 	 combined
2025-08-05 08:26:39.071192 test begin: paddle.nn.functional.gather_tree(Tensor([20, 28, 8],"int64"), Tensor([20, 28, 8],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([20, 28, 8],"int64"), Tensor([20, 28, 8],"int64"), ) 	 8960 	 1000 	 0.010330915451049805 	 286.766220331192 	 2.09808349609375e-05 	 0.0002415180206298828 	 None 	 None 	 None 	 None 	 combined
2025-08-05 08:31:26.183723 test begin: paddle.nn.functional.gather_tree(Tensor([20, 30, 4],"int64"), Tensor([20, 30, 4],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([20, 30, 4],"int64"), Tensor([20, 30, 4],"int64"), ) 	 4800 	 1000 	 0.010434389114379883 	 153.81847858428955 	 2.193450927734375e-05 	 0.0002319812774658203 	 None 	 None 	 None 	 None 	 combined
2025-08-05 08:34:00.226235 test begin: paddle.nn.functional.gather_tree(Tensor([20, 4, 57],"int64"), Tensor([20, 4, 57],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([20, 4, 57],"int64"), Tensor([20, 4, 57],"int64"), ) 	 9120 	 1000 	 0.010251998901367188 	 292.2252571582794 	 1.6927719116210938e-05 	 0.00022912025451660156 	 None 	 None 	 None 	 None 	 combined
2025-08-05 08:38:53.170930 test begin: paddle.nn.functional.gather_tree(Tensor([20, 57, 4],"int64"), Tensor([20, 57, 4],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([20, 57, 4],"int64"), Tensor([20, 57, 4],"int64"), ) 	 9120 	 1000 	 0.01025247573852539 	 292.77885389328003 	 1.0728836059570312e-05 	 0.0002238750457763672 	 None 	 None 	 None 	 None 	 combined
2025-08-05 08:43:46.272689 test begin: paddle.nn.functional.gather_tree(Tensor([20, 8, 15],"int64"), Tensor([20, 8, 15],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([20, 8, 15],"int64"), Tensor([20, 8, 15],"int64"), ) 	 4800 	 1000 	 0.01542973518371582 	 153.97558045387268 	 2.4080276489257812e-05 	 0.000240325927734375 	 None 	 None 	 None 	 None 	 combined
2025-08-05 08:46:20.458659 test begin: paddle.nn.functional.gather_tree(Tensor([200, 4, 4],"int64"), Tensor([200, 4, 4],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([200, 4, 4],"int64"), Tensor([200, 4, 4],"int64"), ) 	 6400 	 1000 	 0.011550664901733398 	 213.94399046897888 	 4.172325134277344e-05 	 0.0002224445343017578 	 None 	 None 	 None 	 None 	 combined
2025-08-05 08:49:54.729797 test begin: paddle.nn.functional.gelu(Tensor([11, 96, 96, 512],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([11, 96, 96, 512],"float32"), False, None, ) 	 51904512 	 29045 	 10.08811092376709 	 8.84818720817566 	 0.35451793670654297 	 0.3108656406402588 	 13.320151567459106 	 13.348244428634644 	 0.4682004451751709 	 0.4720475673675537 	 
2025-08-05 08:50:42.629011 test begin: paddle.nn.functional.gelu(Tensor([124, 9, 96, 512],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([124, 9, 96, 512],"float32"), False, None, ) 	 54853632 	 29045 	 10.638858556747437 	 9.338538646697998 	 0.3740572929382324 	 0.32958364486694336 	 14.061094284057617 	 14.10466742515564 	 0.49531078338623047 	 0.4982945919036865 	 
2025-08-05 08:51:32.819064 test begin: paddle.nn.functional.gelu(Tensor([124, 96, 9, 512],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([124, 96, 9, 512],"float32"), False, None, ) 	 54853632 	 29045 	 10.633928298950195 	 9.359071016311646 	 0.3738861083984375 	 0.3282003402709961 	 14.027738571166992 	 14.106457948684692 	 0.49294424057006836 	 0.49562835693359375 	 
2025-08-05 08:52:24.384010 test begin: paddle.nn.functional.gelu(Tensor([124, 96, 96, 45],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([124, 96, 96, 45],"float32"), False, None, ) 	 51425280 	 29045 	 10.018008470535278 	 8.768930673599243 	 0.3523285388946533 	 0.3080894947052002 	 13.187956809997559 	 13.22746753692627 	 0.46352672576904297 	 0.464918851852417 	 
2025-08-05 08:53:11.570443 test begin: paddle.nn.functional.gelu(Tensor([128, 6, 96, 768],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([128, 6, 96, 768],"float32"), False, None, ) 	 56623104 	 29045 	 10.999195098876953 	 9.644213914871216 	 0.3867497444152832 	 0.33863258361816406 	 14.509983539581299 	 14.556389093399048 	 0.5102026462554932 	 0.5125832557678223 	 
2025-08-05 08:54:04.203268 test begin: paddle.nn.functional.gelu(Tensor([128, 9, 96, 512],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([128, 9, 96, 512],"float32"), False, None, ) 	 56623104 	 29045 	 11.009779930114746 	 9.627735614776611 	 0.3869326114654541 	 0.3385319709777832 	 14.5205078125 	 14.555701494216919 	 0.510009765625 	 0.512747049331665 	 
2025-08-05 08:54:56.000199 test begin: paddle.nn.functional.gelu(Tensor([128, 96, 6, 768],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([128, 96, 6, 768],"float32"), False, None, ) 	 56623104 	 29045 	 11.010343074798584 	 9.635631322860718 	 0.38679957389831543 	 0.3386075496673584 	 14.51837944984436 	 14.544584035873413 	 0.5101709365844727 	 0.5138025283813477 	 
2025-08-05 08:55:47.636913 test begin: paddle.nn.functional.gelu(Tensor([128, 96, 9, 512],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([128, 96, 9, 512],"float32"), False, None, ) 	 56623104 	 29045 	 11.005352258682251 	 9.63327693939209 	 0.3866896629333496 	 0.33861470222473145 	 14.521317481994629 	 14.547010898590088 	 0.5113499164581299 	 0.5113739967346191 	 
2025-08-05 08:56:40.194903 test begin: paddle.nn.functional.gelu(Tensor([128, 96, 96, 44],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([128, 96, 96, 44],"float32"), False, None, ) 	 51904512 	 29045 	 10.109456777572632 	 8.846818447113037 	 0.3565340042114258 	 0.31212401390075684 	 13.320925235748291 	 13.352389812469482 	 0.4708364009857178 	 0.469271183013916 	 
2025-08-05 08:57:28.861312 test begin: paddle.nn.functional.gelu(Tensor([8, 96, 96, 768],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([8, 96, 96, 768],"float32"), False, None, ) 	 56623104 	 29045 	 11.463077783584595 	 9.635895252227783 	 0.38788914680480957 	 0.33855724334716797 	 14.520891904830933 	 14.555127382278442 	 0.5102441310882568 	 0.5113615989685059 	 
2025-08-05 08:58:21.859736 test begin: paddle.nn.functional.glu(Tensor([200, 498, 512],"float32"), -1, None, )
[Prof] paddle.nn.functional.glu 	 paddle.nn.functional.glu(Tensor([200, 498, 512],"float32"), -1, None, ) 	 50995200 	 13909 	 10.008054971694946 	 3.4307315349578857 	 0.24466347694396973 	 0.25098466873168945 	 14.960715293884277 	 5.2911036014556885 	 0.3672478199005127 	 0.3895595073699951 	 
2025-08-05 08:58:57.247159 test begin: paddle.nn.functional.glu(Tensor([209, 477, 512],"float32"), -1, None, )
[Prof] paddle.nn.functional.glu 	 paddle.nn.functional.glu(Tensor([209, 477, 512],"float32"), -1, None, ) 	 51042816 	 13909 	 10.021078109741211 	 3.4280343055725098 	 0.2460155487060547 	 0.2512023448944092 	 14.983638048171997 	 5.294765472412109 	 0.3663637638092041 	 0.38849639892578125 	 
2025-08-05 08:59:32.344123 test begin: paddle.nn.functional.glu(Tensor([218, 457, 512],"float32"), -1, None, )
[Prof] paddle.nn.functional.glu 	 paddle.nn.functional.glu(Tensor([218, 457, 512],"float32"), -1, None, ) 	 51008512 	 13909 	 9.990578174591064 	 3.423168420791626 	 0.24437642097473145 	 0.2510843276977539 	 14.963417291641235 	 5.291057586669922 	 0.36589670181274414 	 0.3882718086242676 	 
2025-08-05 09:00:08.149318 test begin: paddle.nn.functional.glu(Tensor([30, 3308, 512],"float32"), -1, None, )
[Prof] paddle.nn.functional.glu 	 paddle.nn.functional.glu(Tensor([30, 3308, 512],"float32"), -1, None, ) 	 50810880 	 13909 	 9.97612452507019 	 3.40897274017334 	 0.2438828945159912 	 0.2503700256347656 	 14.918837308883667 	 5.272511005401611 	 0.3645915985107422 	 0.3868718147277832 	 
2025-08-05 09:00:43.680220 test begin: paddle.nn.functional.glu(Tensor([30, 457, 3706],"float32"), -1, None, )
[Prof] paddle.nn.functional.glu 	 paddle.nn.functional.glu(Tensor([30, 457, 3706],"float32"), -1, None, ) 	 50809260 	 13909 	 10.36002492904663 	 3.4844346046447754 	 0.25319862365722656 	 0.2548234462738037 	 15.701241731643677 	 5.319988250732422 	 0.3840675354003906 	 0.39040493965148926 	 
2025-08-05 09:01:19.856799 test begin: paddle.nn.functional.grid_sample(Tensor([100, 1, 768, 768],"float32"), Tensor([100, 1, 254017, 2],"float32"), align_corners=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5a61d1aec0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 09:11:25.357971 test begin: paddle.nn.functional.grid_sample(Tensor([100, 1, 768, 768],"float32"), Tensor([100, 21, 12544, 2],"float32"), align_corners=False, )
W0805 09:11:27.253367 43205 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0f07c1f0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 09:21:30.962124 test begin: paddle.nn.functional.grid_sample(Tensor([100, 21, 768, 768],"float32"), Tensor([100, 21, 12544, 2],"float32"), align_corners=False, )
W0805 09:21:51.003136 87576 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f57cfa0ad10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 09:31:38.118704 test begin: paddle.nn.functional.grid_sample(Tensor([1000, 1, 662, 768],"float32"), Tensor([1000, 1, 12544, 2],"float32"), align_corners=False, )
W0805 09:31:46.767251 131306 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6fb01a30d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 09:41:45.501039 test begin: paddle.nn.functional.grid_sample(Tensor([1000, 1, 662, 768],"float32"), Tensor([1000, 1, 662, 2],"float32"), align_corners=False, )
W0805 09:41:53.437021 11115 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(Tensor([1000, 1, 662, 768],"float32"), Tensor([1000, 1, 662, 2],"float32"), align_corners=False, ) 	 509740000 	 97873 	 10.03143858909607 	 9.633846282958984 	 0.10461854934692383 	 0.10044550895690918 	 175.16832327842712 	 156.08837509155273 	 0.9136755466461182 	 0.8135743141174316 	 
2025-08-05 09:47:46.169580 test begin: paddle.nn.functional.grid_sample(Tensor([1000, 1, 768, 662],"float32"), Tensor([1000, 1, 12544, 2],"float32"), align_corners=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb8eaa36980>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 09:57:51.758391 test begin: paddle.nn.functional.grid_sample(Tensor([1000, 1, 768, 768],"float32"), Tensor([1000, 1, 12544, 2],"float32"), align_corners=False, )
W0805 09:58:03.735694 89665 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f743f9630d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 10:07:57.178074 test begin: paddle.nn.functional.grid_sample(Tensor([1720, 1, 544, 544],"float32"), Tensor([1720, 1, 12544, 2],"float32"), align_corners=False, )
W0805 10:08:07.681349 142466 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc54b7a70d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 10:18:06.230117 test begin: paddle.nn.functional.grid_sample(Tensor([200, 1, 544, 544],"float32"), Tensor([200, 1, 127009, 2],"float32"), align_corners=False, )
W0805 10:18:08.168710 32383 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f587247f0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 10:28:11.990254 test begin: paddle.nn.functional.grid_sample(Tensor([200, 1, 544, 544],"float32"), Tensor([200, 11, 12544, 2],"float32"), align_corners=False, )
W0805 10:28:14.207326 84995 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f253482f0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 10:38:17.602356 test begin: paddle.nn.functional.grid_sample(Tensor([200, 11, 544, 544],"float32"), Tensor([200, 11, 12544, 2],"float32"), align_corners=False, )
W0805 10:38:28.871152 137844 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2adda5ecb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 10:48:22.603685 test begin: paddle.nn.functional.grid_sample(Tensor([2000, 1, 467, 544],"float32"), Tensor([2000, 1, 12544, 2],"float32"), align_corners=False, )
W0805 10:48:31.422215 27683 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6f18926ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 10:58:28.848690 test begin: paddle.nn.functional.grid_sample(Tensor([2000, 1, 467, 544],"float32"), Tensor([2000, 1, 467, 2],"float32"), align_corners=False, )
W0805 10:58:44.655534 79805 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(Tensor([2000, 1, 467, 544],"float32"), Tensor([2000, 1, 467, 2],"float32"), align_corners=False, ) 	 509964000 	 97873 	 13.213939428329468 	 12.589664459228516 	 0.13767480850219727 	 0.1325972080230713 	 184.16774892807007 	 164.7744324207306 	 0.9643545150756836 	 0.8587520122528076 	 
2025-08-05 11:05:01.002869 test begin: paddle.nn.functional.grid_sample(Tensor([2000, 1, 544, 467],"float32"), Tensor([2000, 1, 12544, 2],"float32"), align_corners=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd635e4a860>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 11:15:07.327949 test begin: paddle.nn.functional.grid_sample(Tensor([2000, 1, 544, 544],"float32"), Tensor([2000, 1, 12544, 2],"float32"), align_corners=False, )
W0805 11:15:17.598337 163326 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f04546c2ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 11:25:12.354385 test begin: paddle.nn.functional.grid_sample(Tensor([2026, 1, 544, 544],"float32"), Tensor([2026, 1, 12544, 2],"float32"), align_corners=False, )
W0805 11:25:24.546193 52402 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc1dbcab130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 11:35:21.506713 test begin: paddle.nn.functional.grid_sample(Tensor([2026, 1, 768, 768],"float32"), Tensor([2026, 1, 12544, 2],"float32"), align_corners=False, )
W0805 11:35:42.029016 105156 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc535f930d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 11:45:26.662853 test begin: paddle.nn.functional.grid_sample(Tensor([870, 1, 768, 768],"float32"), Tensor([870, 1, 12544, 2],"float32"), align_corners=False, )
W0805 11:45:34.990506 159367 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f21cbef30d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 11:55:32.015171 test begin: paddle.nn.functional.grid_sample(x=Tensor([1, 64, 80, 94, 311],"float32"), grid=Tensor([1, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
W0805 11:55:34.971616 160839 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f473a27b340>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 12:05:37.693541 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 6, 80, 94, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
W0805 12:05:41.946779 53745 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4dedc3b130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 12:15:45.189963 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 6, 80, 94, 311],"float32"), grid=Tensor([4, 6, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
W0805 12:15:46.346771 108824 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(x=Tensor([4, 6, 80, 94, 311],"float32"), grid=Tensor([4, 6, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 	 56806080 	 97873 	 13.263489007949829 	 9.640562295913696 	 0.1382462978363037 	 0.10053277015686035 	 44.70232939720154 	 45.15443301200867 	 0.23279118537902832 	 0.23543977737426758 	 
2025-08-05 12:17:40.215787 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 7, 94, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff86c0ebe80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 12:28:20.449362 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 7, 94, 311],"float32"), grid=Tensor([4, 280, 7, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
W0805 12:28:21.523047 14388 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f57d70ef2e0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 12:38:25.615670 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 8, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
W0805 12:38:27.376930 75543 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8b5fffb2e0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 12:48:30.565067 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 8, 311],"float32"), grid=Tensor([4, 280, 376, 8, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
W0805 12:48:31.839861 140550 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8d4d64f2e0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 12:58:35.632268 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 27],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
W0805 12:58:39.762465 43313 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9ad85d33a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 13:08:44.246309 test begin: paddle.nn.functional.label_smooth(label=Tensor([48, 32, 33712],"float32"), epsilon=0.1, )
W0805 13:08:45.624997 110262 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.label_smooth 	 paddle.nn.functional.label_smooth(label=Tensor([48, 32, 33712],"float32"), epsilon=0.1, ) 	 51781632 	 33539 	 10.131428480148315 	 20.71090340614319 	 0.30837392807006836 	 0.20984435081481934 	 10.140161514282227 	 10.186749696731567 	 0.30971312522888184 	 0.3111908435821533 	 combined
2025-08-05 13:09:40.844074 test begin: paddle.nn.functional.label_smooth(label=Tensor([76, 20, 33712],"float32"), epsilon=0.1, )
[Prof] paddle.nn.functional.label_smooth 	 paddle.nn.functional.label_smooth(label=Tensor([76, 20, 33712],"float32"), epsilon=0.1, ) 	 51242240 	 33539 	 10.027235746383667 	 20.50931453704834 	 0.30501484870910645 	 0.20912957191467285 	 10.027701377868652 	 10.08931279182434 	 0.3051736354827881 	 0.3068523406982422 	 combined
2025-08-05 13:10:34.573408 test begin: paddle.nn.functional.layer_norm(Tensor([115, 435, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
[Prof] paddle.nn.functional.layer_norm 	 paddle.nn.functional.layer_norm(Tensor([115, 435, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, ) 	 51227648 	 33222 	 10.489371538162231 	 10.514126777648926 	 0.31011319160461426 	 0.32319164276123047 	 15.724268913269043 	 33.815940141677856 	 0.24135351181030273 	 0.5205631256103516 	 
2025-08-05 13:11:47.424470 test begin: paddle.nn.functional.layer_norm(Tensor([174, 286, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
[Prof] paddle.nn.functional.layer_norm 	 paddle.nn.functional.layer_norm(Tensor([174, 286, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, ) 	 50960384 	 33222 	 10.005365371704102 	 10.474770784378052 	 0.3072843551635742 	 0.32157087326049805 	 15.686823844909668 	 33.633034229278564 	 0.24207472801208496 	 0.5176551342010498 	 
2025-08-05 13:12:59.003529 test begin: paddle.nn.functional.layer_norm(Tensor([226, 220, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
[Prof] paddle.nn.functional.layer_norm 	 paddle.nn.functional.layer_norm(Tensor([226, 220, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, ) 	 50915328 	 33222 	 9.998156785964966 	 10.469018936157227 	 0.3070032596588135 	 0.32132387161254883 	 15.601341247558594 	 33.61885118484497 	 0.23963236808776855 	 0.5186316967010498 	 
2025-08-05 13:14:10.485310 test begin: paddle.nn.functional.layer_norm(Tensor([7, 7088, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
[Prof] paddle.nn.functional.layer_norm 	 paddle.nn.functional.layer_norm(Tensor([7, 7088, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, ) 	 50808832 	 33222 	 9.977487087249756 	 10.902127742767334 	 0.3063778877258301 	 0.320845365524292 	 15.574523210525513 	 33.5004780292511 	 0.23911213874816895 	 0.515526294708252 	 
2025-08-05 13:15:23.886891 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 26, 304, 544],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 26, 304, 544],"float32"), 0.1, ) 	 51597312 	 33695 	 10.145633935928345 	 10.203496217727661 	 0.3072488307952881 	 0.3089454174041748 	 15.428523302078247 	 15.31520676612854 	 0.4672245979309082 	 0.46363019943237305 	 
2025-08-05 13:16:16.826042 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 32, 122, 1088],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 32, 122, 1088],"float32"), 0.1, ) 	 50970624 	 33695 	 10.030343055725098 	 11.199512720108032 	 0.30478453636169434 	 0.30794668197631836 	 15.2406325340271 	 15.132508754730225 	 0.4615013599395752 	 0.45949363708496094 	 
2025-08-05 13:17:10.902906 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 32, 608, 218],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 32, 608, 218],"float32"), 0.1, ) 	 50896896 	 33695 	 10.011149406433105 	 10.07267451286316 	 0.30309247970581055 	 0.3060784339904785 	 15.220275640487671 	 15.106128454208374 	 0.46092796325683594 	 0.4600849151611328 	 
2025-08-05 13:18:03.124033 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 64, 122, 544],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 64, 122, 544],"float32"), 0.1, ) 	 50970624 	 33695 	 10.026202201843262 	 10.075555801391602 	 0.3035454750061035 	 0.30526041984558105 	 15.247492551803589 	 15.131099939346313 	 0.4640228748321533 	 0.45931267738342285 	 
2025-08-05 13:18:55.429126 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 64, 304, 218],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 64, 304, 218],"float32"), 0.1, ) 	 50896896 	 33695 	 10.6734139919281 	 10.069629430770874 	 0.30431318283081055 	 0.3060636520385742 	 15.2238290309906 	 15.10627007484436 	 0.46209716796875 	 0.45745396614074707 	 
2025-08-05 13:19:48.736805 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 7, 608, 1088],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 7, 608, 1088],"float32"), 0.1, ) 	 55566336 	 33695 	 10.91716718673706 	 10.990953207015991 	 0.3305227756500244 	 0.33238935470581055 	 16.611171007156372 	 16.468621015548706 	 0.5029511451721191 	 0.4999523162841797 	 
2025-08-05 13:20:45.676941 test begin: paddle.nn.functional.leaky_relu(Tensor([13, 64, 256, 256],"float32"), 0.1, None, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([13, 64, 256, 256],"float32"), 0.1, None, ) 	 54525952 	 33695 	 10.715823650360107 	 10.781404972076416 	 0.32555413246154785 	 0.3274683952331543 	 16.29180669784546 	 16.15859627723694 	 0.496295690536499 	 0.4895145893096924 	 
2025-08-05 13:21:41.498210 test begin: paddle.nn.functional.leaky_relu(Tensor([3, 32, 608, 1088],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([3, 32, 608, 1088],"float32"), 0.1, ) 	 63504384 	 33695 	 12.462762832641602 	 12.521580219268799 	 0.3772149085998535 	 0.38053250312805176 	 18.9614474773407 	 18.800811052322388 	 0.5753061771392822 	 0.5719354152679443 	 
2025-08-05 13:22:46.605739 test begin: paddle.nn.functional.leaky_relu(Tensor([5, 64, 304, 544],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([5, 64, 304, 544],"float32"), 0.1, ) 	 52920320 	 33695 	 10.402246952056885 	 10.466442346572876 	 0.3159935474395752 	 0.31672048568725586 	 15.827186584472656 	 15.68283462524414 	 0.4803602695465088 	 0.4779036045074463 	 
2025-08-05 13:23:40.887953 test begin: paddle.nn.functional.leaky_relu(Tensor([64, 13, 256, 256],"float32"), 0.1, None, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([64, 13, 256, 256],"float32"), 0.1, None, ) 	 54525952 	 33695 	 10.725159645080566 	 10.777968406677246 	 0.3268306255340576 	 0.3275125026702881 	 16.298828601837158 	 16.16167974472046 	 0.49343395233154297 	 0.4895176887512207 	 
2025-08-05 13:24:37.522969 test begin: paddle.nn.functional.leaky_relu(Tensor([64, 64, 256, 49],"float32"), 0.1, None, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([64, 64, 256, 49],"float32"), 0.1, None, ) 	 51380224 	 33695 	 10.556925296783447 	 10.162982940673828 	 0.30594611167907715 	 0.3077428340911865 	 15.374381065368652 	 15.252710580825806 	 0.4665796756744385 	 0.46169233322143555 	 
2025-08-05 13:25:31.126965 test begin: paddle.nn.functional.leaky_relu(Tensor([64, 64, 49, 256],"float32"), 0.1, None, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([64, 64, 49, 256],"float32"), 0.1, None, ) 	 51380224 	 33695 	 10.097314834594727 	 10.168974161148071 	 0.3059999942779541 	 0.30779027938842773 	 15.370133638381958 	 15.251943826675415 	 0.4668080806732178 	 0.46307873725891113 	 
2025-08-05 13:26:24.322153 test begin: paddle.nn.functional.linear(x=Tensor([1, 12404],"float32"), weight=Tensor([12404, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, )
Warning: The core code of paddle.nn.functional.linear is too complex.
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([1, 12404],"float32"), weight=Tensor([12404, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, ) 	 50823284 	 62826 	 10.033640623092651 	 9.797044038772583 	 0.05418252944946289 	 0.15964007377624512 	 19.771374940872192 	 19.86640429496765 	 0.08034634590148926 	 0.10867071151733398 	 
2025-08-05 13:27:25.340103 test begin: paddle.nn.functional.linear(x=Tensor([1, 25088],"float32"), weight=Tensor([25088, 2026],"float32"), bias=Tensor([2026],"float32"), name=None, )
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([1, 25088],"float32"), weight=Tensor([25088, 2026],"float32"), bias=Tensor([2026],"float32"), name=None, ) 	 50855402 	 62826 	 10.553418397903442 	 10.041430234909058 	 0.057038068771362305 	 0.1651620864868164 	 20.356793642044067 	 20.25969171524048 	 0.08271908760070801 	 0.1095285415649414 	 
2025-08-05 13:28:29.197479 test begin: paddle.nn.functional.linear(x=Tensor([2, 12404],"float32"), weight=Tensor([12404, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, )
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([2, 12404],"float32"), weight=Tensor([12404, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, ) 	 50835688 	 62826 	 13.519285678863525 	 15.410669326782227 	 0.07305192947387695 	 0.0626211166381836 	 26.90225386619568 	 26.50355100631714 	 0.0730288028717041 	 0.08597397804260254 	 
2025-08-05 13:29:52.439664 test begin: paddle.nn.functional.linear(x=Tensor([2, 25088],"float32"), weight=Tensor([25088, 2026],"float32"), bias=Tensor([2026],"float32"), name=None, )
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([2, 25088],"float32"), weight=Tensor([25088, 2026],"float32"), bias=Tensor([2026],"float32"), name=None, ) 	 50880490 	 62826 	 14.445396184921265 	 19.700701236724854 	 0.07791900634765625 	 0.08011150360107422 	 21.23454785346985 	 21.083113193511963 	 0.08631730079650879 	 0.11405777931213379 	 
2025-08-05 13:31:10.350606 test begin: paddle.nn.functional.linear(x=Tensor([2026, 25088],"float32"), weight=Tensor([25088, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd7f08fb520>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 13:41:26.827405 test begin: paddle.nn.functional.linear(x=Tensor([4051, 12544],"float32"), weight=Tensor([12544, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, )
Warning: The core code of paddle.nn.functional.linear is too complex.
W0805 13:41:28.034554 152894 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f64069aefe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 13:51:31.982250 test begin: paddle.nn.functional.linear(x=Tensor([4096, 12404],"float32"), weight=Tensor([12404, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, )
Warning: The core code of paddle.nn.functional.linear is too complex.
W0805 13:51:33.236285 54994 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f70de8defe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 14:01:37.645646 test begin: paddle.nn.functional.linear(x=Tensor([4096, 12544],"float32"), weight=Tensor([12544, 4051],"float32"), bias=Tensor([4051],"float32"), name=None, )
Warning: The core code of paddle.nn.functional.linear is too complex.
W0805 14:01:40.068360 120348 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f276bc16fe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 14:11:45.008156 test begin: paddle.nn.functional.linear(x=Tensor([4096, 49613],"float32"), weight=Tensor([49613, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, )
Warning: The core code of paddle.nn.functional.linear is too complex.
W0805 14:11:49.260783 22539 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe77cf2afe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 14:21:49.933937 test begin: paddle.nn.functional.local_response_norm(Tensor([10585, 3, 40, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, )
W0805 14:21:51.141372 87612 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 14:21:51.177717 87612 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(Tensor([10585, 3, 40, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, ) 	 50808000 	 2912 	 12.415673971176147 	 14.888216018676758 	 0.7271699905395508 	 0.6545631885528564 	 26.014824390411377 	 25.245230197906494 	 1.1093504428863525 	 0.5215029716491699 	 
2025-08-05 14:23:12.259671 test begin: paddle.nn.functional.local_response_norm(Tensor([3, 10585, 40, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(Tensor([3, 10585, 40, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, ) 	 50808000 	 2912 	 10.028220176696777 	 13.888800382614136 	 0.5862252712249756 	 0.6089127063751221 	 21.29224157333374 	 22.091621160507202 	 0.8291034698486328 	 0.4849245548248291 	 
2025-08-05 14:24:21.850514 test begin: paddle.nn.functional.local_response_norm(Tensor([3, 3, 141121, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(Tensor([3, 3, 141121, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, ) 	 50803560 	 2912 	 12.658946990966797 	 15.366717100143433 	 0.7383506298065186 	 0.672673225402832 	 26.545288562774658 	 25.83780574798584 	 1.0311741828918457 	 0.5659477710723877 	 
2025-08-05 14:25:45.074537 test begin: paddle.nn.functional.local_response_norm(Tensor([3, 3, 40, 141121],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(Tensor([3, 3, 40, 141121],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, ) 	 50803560 	 2912 	 12.645152807235718 	 12.562334299087524 	 0.7391209602355957 	 0.5504779815673828 	 26.47390341758728 	 23.136756896972656 	 1.0320518016815186 	 0.5065984725952148 	 
2025-08-05 14:27:02.286638 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3, 40, 47041],"float32"), size=5, data_format="NCDHW", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3, 40, 47041],"float32"), size=5, data_format="NCDHW", ) 	 50804280 	 2912 	 12.63784384727478 	 13.681125402450562 	 0.7370476722717285 	 0.5993432998657227 	 26.258625268936157 	 25.02316641807556 	 1.0206584930419922 	 0.548107385635376 	 
2025-08-05 14:28:21.683267 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3, 47041, 40],"float32"), size=5, data_format="NCDHW", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3, 47041, 40],"float32"), size=5, data_format="NCDHW", ) 	 50804280 	 2912 	 12.65064787864685 	 13.692189931869507 	 0.7411630153656006 	 0.6006219387054443 	 26.265700340270996 	 25.025481462478638 	 1.0205726623535156 	 0.5495967864990234 	 
2025-08-05 14:29:42.348824 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3529, 40, 40],"float32"), size=5, data_format="NCDHW", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3529, 40, 40],"float32"), size=5, data_format="NCDHW", ) 	 50817600 	 2912 	 12.624138593673706 	 12.410737991333008 	 0.7416267395019531 	 0.5439143180847168 	 26.48529577255249 	 22.86920142173767 	 1.0312485694885254 	 0.5007860660552979 	 
2025-08-05 14:30:58.893560 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 40, 40, 3529],"float32"), size=5, data_format="NDHWC", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 40, 40, 3529],"float32"), size=5, data_format="NDHWC", ) 	 50817600 	 2912 	 20.74790334701538 	 14.043370723724365 	 1.2138538360595703 	 0.6180331707000732 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 3, 40, 40, 3529]) and output[0] has a shape of torch.Size([3, 3529, 3, 40, 40]).
2025-08-05 14:32:15.233823 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 40, 47041, 3],"float32"), size=5, data_format="NDHWC", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 40, 47041, 3],"float32"), size=5, data_format="NDHWC", ) 	 50804280 	 2912 	 23.065042734146118 	 14.57291293144226 	 1.3452744483947754 	 0.6382777690887451 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 3, 40, 47041, 3]) and output[0] has a shape of torch.Size([3, 3, 3, 40, 47041]).
2025-08-05 14:34:09.382440 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 47041, 40, 3],"float32"), size=5, data_format="NDHWC", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 47041, 40, 3],"float32"), size=5, data_format="NDHWC", ) 	 50804280 	 2912 	 23.075804948806763 	 14.556951522827148 	 1.3480098247528076 	 0.6405518054962158 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 3, 47041, 40, 3]) and output[0] has a shape of torch.Size([3, 3, 3, 47041, 40]).
2025-08-05 14:36:04.478871 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3529, 3, 40, 40],"float32"), size=5, data_format="NCDHW", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3529, 3, 40, 40],"float32"), size=5, data_format="NCDHW", ) 	 50817600 	 2912 	 10.002660989761353 	 12.403173208236694 	 0.5836775302886963 	 0.54231858253479 	 21.187167167663574 	 21.4173903465271 	 0.8230783939361572 	 0.4702291488647461 	 
2025-08-05 14:37:12.239449 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3529, 40, 40, 3],"float32"), size=5, data_format="NDHWC", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3529, 40, 40, 3],"float32"), size=5, data_format="NDHWC", ) 	 50817600 	 2912 	 23.110023736953735 	 13.302886486053467 	 1.3471195697784424 	 0.5858535766601562 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 3529, 40, 40, 3]) and output[0] has a shape of torch.Size([3, 3, 3529, 40, 40]).
2025-08-05 14:39:06.437506 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3529, 3, 3, 40, 40],"float32"), size=5, data_format="NCDHW", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3529, 3, 3, 40, 40],"float32"), size=5, data_format="NCDHW", ) 	 50817600 	 2912 	 12.425427198410034 	 13.240337371826172 	 0.7241771221160889 	 0.5826356410980225 	 25.95083212852478 	 24.9763765335083 	 1.0120692253112793 	 0.5470075607299805 	 
2025-08-05 14:40:24.872116 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3529, 3, 40, 40, 3],"float32"), size=5, data_format="NDHWC", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3529, 3, 40, 40, 3],"float32"), size=5, data_format="NDHWC", ) 	 50817600 	 2912 	 23.126291513442993 	 13.25097370147705 	 1.3473563194274902 	 0.5813469886779785 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([3529, 3, 40, 40, 3]) and output[0] has a shape of torch.Size([3529, 3, 3, 40, 40]).
2025-08-05 14:42:17.751400 test begin: paddle.nn.functional.log_loss(Tensor([50803201, 1],"float32"), Tensor([50803201, 1],"float32"), )
[Prof] paddle.nn.functional.log_loss 	 paddle.nn.functional.log_loss(Tensor([50803201, 1],"float32"), Tensor([50803201, 1],"float32"), ) 	 101606402 	 20178 	 9.96570873260498 	 69.09271311759949 	 0.5063326358795166 	 0.35100340843200684 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-05 14:43:52.748994 test begin: paddle.nn.functional.log_loss(Tensor([50803201, 1],"float32"), Tensor([50803201, 1],"float32"), epsilon=1e-07, )
[Prof] paddle.nn.functional.log_loss 	 paddle.nn.functional.log_loss(Tensor([50803201, 1],"float32"), Tensor([50803201, 1],"float32"), epsilon=1e-07, ) 	 101606402 	 20178 	 9.972252607345581 	 69.09617209434509 	 0.5054192543029785 	 0.3508002758026123 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-05 14:45:27.264854 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 10, 254017],"float64"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([10, 10, 254017],"float64"), None, ) 	 25401700 	 33264 	 14.65153431892395 	 15.183897018432617 	 0.4496018886566162 	 0.46689867973327637 	 15.011084794998169 	 15.01519513130188 	 0.4613990783691406 	 0.460482120513916 	 
2025-08-05 14:46:28.338341 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 10, 508033],"float32"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([10, 10, 508033],"float32"), None, ) 	 50803300 	 33264 	 10.003089904785156 	 9.939084768295288 	 0.30692625045776367 	 0.30593061447143555 	 14.998141527175903 	 15.00004267692566 	 0.4613921642303467 	 0.46246933937072754 	 
2025-08-05 14:47:20.163278 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 254017, 10],"float64"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([10, 254017, 10],"float64"), None, ) 	 25401700 	 33264 	 14.661746263504028 	 15.20783519744873 	 0.45098280906677246 	 0.46727728843688965 	 15.012206554412842 	 15.015626907348633 	 0.46046948432922363 	 0.4605436325073242 	 
2025-08-05 14:48:22.290263 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 508033, 10],"float32"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([10, 508033, 10],"float32"), None, ) 	 50803300 	 33264 	 10.010461807250977 	 9.93120813369751 	 0.30693936347961426 	 0.30591797828674316 	 14.993599891662598 	 14.996213436126709 	 0.46022582054138184 	 0.45992016792297363 	 
2025-08-05 14:49:14.227975 test begin: paddle.nn.functional.log_sigmoid(Tensor([254017, 10, 10],"float64"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([254017, 10, 10],"float64"), None, ) 	 25401700 	 33264 	 14.671491384506226 	 15.192471027374268 	 0.4495406150817871 	 0.465435266494751 	 15.01064658164978 	 15.013077735900879 	 0.4604763984680176 	 0.4631807804107666 	 
2025-08-05 14:50:15.991495 test begin: paddle.nn.functional.log_sigmoid(Tensor([508033, 10, 10],"float32"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([508033, 10, 10],"float32"), None, ) 	 50803300 	 33264 	 10.007570266723633 	 9.949599742889404 	 0.30693960189819336 	 0.30603528022766113 	 15.000774145126343 	 14.994969367980957 	 0.4629695415496826 	 0.4599173069000244 	 
2025-08-05 14:51:08.559993 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([10, 10, 508033],"float32"), )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(x=Tensor([10, 10, 508033],"float32"), ) 	 50803300 	 33264 	 10.447780847549438 	 9.960708618164062 	 0.306926965713501 	 0.30725646018981934 	 14.997735738754272 	 14.998505115509033 	 0.46026182174682617 	 0.46002650260925293 	 
2025-08-05 14:52:02.172208 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([10, 508033, 10],"float32"), )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(x=Tensor([10, 508033, 10],"float32"), ) 	 50803300 	 33264 	 10.01478910446167 	 9.949408292770386 	 0.3081197738647461 	 0.30470895767211914 	 14.993503332138062 	 14.999984502792358 	 0.4613180160522461 	 0.4612116813659668 	 
2025-08-05 14:52:53.875259 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([508033, 10, 10],"float32"), )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(x=Tensor([508033, 10, 10],"float32"), ) 	 50803300 	 33264 	 10.009601831436157 	 9.941099882125854 	 0.30817556381225586 	 0.30590128898620605 	 15.004127502441406 	 14.990854978561401 	 0.4602179527282715 	 0.46137452125549316 	 
2025-08-05 14:53:45.543509 test begin: paddle.nn.functional.log_softmax(Tensor([128, 396901],"float32"), axis=-1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([128, 396901],"float32"), axis=-1, ) 	 50803328 	 32827 	 23.234863758087158 	 20.618774890899658 	 0.7232110500335693 	 0.6420409679412842 	 45.87954068183899 	 21.377314567565918 	 1.4300379753112793 	 0.6644923686981201 	 
2025-08-05 14:55:41.817680 test begin: paddle.nn.functional.log_softmax(Tensor([264, 192612],"float32"), axis=-1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([264, 192612],"float32"), axis=-1, ) 	 50849568 	 32827 	 20.405553817749023 	 21.186599016189575 	 0.6352903842926025 	 0.6599035263061523 	 28.3586905002594 	 21.767902374267578 	 0.8825192451477051 	 0.6775546073913574 	 
2025-08-05 14:57:17.226325 test begin: paddle.nn.functional.log_softmax(Tensor([2944, 17257],"float32"), axis=1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([2944, 17257],"float32"), axis=1, ) 	 50804608 	 32827 	 10.857500076293945 	 11.557569980621338 	 0.3386378288269043 	 0.34518933296203613 	 21.26347827911377 	 17.123793840408325 	 0.6637551784515381 	 0.5333991050720215 	 
2025-08-05 14:58:21.023765 test begin: paddle.nn.functional.log_softmax(Tensor([4224, 12028],"float32"), axis=1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([4224, 12028],"float32"), axis=1, ) 	 50806272 	 32827 	 10.032353639602661 	 10.110775709152222 	 0.31181883811950684 	 0.3153712749481201 	 20.71109628677368 	 14.953117370605469 	 0.6436765193939209 	 0.4659304618835449 	 
2025-08-05 14:59:18.613602 test begin: paddle.nn.functional.log_softmax(Tensor([7664, 6629],"float32"), axis=1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([7664, 6629],"float32"), axis=1, ) 	 50804656 	 32827 	 9.953941822052002 	 9.839083194732666 	 0.30941271781921387 	 0.3065011501312256 	 19.570662021636963 	 16.909493446350098 	 0.6082897186279297 	 0.5279884338378906 	 
2025-08-05 15:00:16.717990 test begin: paddle.nn.functional.lp_pool1d(Tensor([2, 3, 8467201],"float32"), 4.0, 3, 2, 1, False, "NCL", None, )
[Prof] paddle.nn.functional.lp_pool1d 	 paddle.nn.functional.lp_pool1d(Tensor([2, 3, 8467201],"float32"), 4.0, 3, 2, 1, False, "NCL", None, ) 	 50803206 	 10734 	 10.013091325759888 	 20.62350845336914 	 0.9564816951751709 	 0.24466919898986816 	 15.205908298492432 	 53.27801299095154 	 0.7227332592010498 	 0.3168213367462158 	 
2025-08-05 15:01:57.243832 test begin: paddle.nn.functional.lp_pool1d(Tensor([2, 793801, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, )
[Prof] paddle.nn.functional.lp_pool1d 	 paddle.nn.functional.lp_pool1d(Tensor([2, 793801, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, ) 	 50803264 	 10734 	 10.022651195526123 	 20.891746520996094 	 0.9536283016204834 	 0.2478322982788086 	 15.253737211227417 	 55.36539387702942 	 0.7274606227874756 	 0.3291442394256592 	 
2025-08-05 15:03:40.214050 test begin: paddle.nn.functional.lp_pool1d(Tensor([529201, 3, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, )
[Prof] paddle.nn.functional.lp_pool1d 	 paddle.nn.functional.lp_pool1d(Tensor([529201, 3, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, ) 	 50803296 	 10734 	 10.029186964035034 	 20.884530782699585 	 0.9554553031921387 	 0.24782609939575195 	 15.267358779907227 	 55.384103298187256 	 0.7291524410247803 	 0.3305954933166504 	 
2025-08-05 15:05:24.395175 test begin: paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, ) 	 50804736 	 18010 	 37.933122873306274 	 63.16461133956909 	 2.155611515045166 	 0.44910240173339844 	 40.57738947868347 	 122.93110728263855 	 1.1489191055297852 	 0.4640676975250244 	 
2025-08-05 15:09:50.974843 test begin: paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, ) 	 50804736 	 18010 	 10.036961317062378 	 21.793410778045654 	 0.5679616928100586 	 0.15379643440246582 	 18.007386445999146 	 66.04993748664856 	 0.5111861228942871 	 0.24972963333129883 	 
2025-08-05 15:11:48.411037 test begin: paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, ) 	 50804736 	 18010 	 16.95851469039917 	 14.261331796646118 	 0.9614341259002686 	 0.10053801536560059 	 34.695719480514526 	 59.504902839660645 	 0.988168478012085 	 0.22522187232971191 	 
2025-08-05 15:13:54.953754 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, ) 	 50804736 	 18010 	 37.93743586540222 	 63.12478065490723 	 2.1576926708221436 	 0.44744372367858887 	 40.56592679023743 	 122.8417718410492 	 1.1500940322875977 	 0.4650139808654785 	 
2025-08-05 15:18:21.286965 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, ) 	 50804736 	 18010 	 10.022202491760254 	 21.746253490447998 	 0.5688924789428711 	 0.15352630615234375 	 17.986488580703735 	 65.92771434783936 	 0.5120944976806641 	 0.25058650970458984 	 
2025-08-05 15:20:18.108768 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, ) 	 50804736 	 18010 	 16.942076921463013 	 14.283085584640503 	 0.960521936416626 	 0.10045909881591797 	 34.65952157974243 	 59.64354467391968 	 0.9829330444335938 	 0.22691655158996582 	 
2025-08-05 15:22:26.323841 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 3, 264601, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 3, 264601, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, ) 	 50803392 	 18010 	 39.12941312789917 	 63.46850252151489 	 2.22011399269104 	 0.44997096061706543 	 41.355412006378174 	 125.41214108467102 	 1.1710002422332764 	 0.44432783126831055 	 
2025-08-05 15:26:57.416035 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 3, 264601, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 3, 264601, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, ) 	 50803392 	 18010 	 10.02673625946045 	 21.79493021965027 	 0.5692412853240967 	 0.15398931503295898 	 18.018320560455322 	 66.09108591079712 	 0.51279616355896 	 0.23677754402160645 	 
2025-08-05 15:28:55.771040 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 264601],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 264601],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, ) 	 50803392 	 18010 	 38.155110120773315 	 64.72997951507568 	 2.1393561363220215 	 0.45730018615722656 	 41.34808397293091 	 125.1603889465332 	 1.173872709274292 	 0.4434244632720947 	 
2025-08-05 15:33:27.915002 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 264601],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 264601],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, ) 	 50803392 	 18010 	 10.010551452636719 	 21.87604308128357 	 0.5669713020324707 	 0.15566468238830566 	 17.933279514312744 	 65.23177313804626 	 0.5077059268951416 	 0.23250317573547363 	 
2025-08-05 15:35:24.541134 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([1373060, 37],"float32"), Tensor([1373060],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([1373060, 37],"float32"), Tensor([1373060],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, ) 	 52176280 	 3872 	 11.451876163482666 	 6.417226314544678 	 0.3344907760620117 	 0.11251568794250488 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 15:35:46.626594 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([25401601, 37],"float16"), Tensor([25401601],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([25401601, 37],"float16"), Tensor([25401601],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, ) 	 965260838 	 3872 	 203.51079535484314 	 75.52687168121338 	 5.9682910442352295 	 1.327977180480957 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 15:41:55.027863 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([25401601, 37],"float32"), Tensor([25401601],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([25401601, 37],"float32"), Tensor([25401601],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, ) 	 965260838 	 3872 	 213.5478060245514 	 117.05790567398071 	 6.262697219848633 	 1.814436674118042 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 15:48:25.364387 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([2746119, 37],"float16"), Tensor([2746119],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([2746119, 37],"float16"), Tensor([2746119],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, ) 	 104352522 	 3872 	 22.278942108154297 	 8.343464612960815 	 0.6521117687225342 	 0.14638972282409668 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 15:49:05.826273 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([5, 10160641],"float32"), Tensor([5],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([5, 10160641],"float32"), Tensor([5],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, ) 	 50803210 	 3872 	 271.8176338672638 	 23.98712158203125 	 6.526066303253174 	 0.4216926097869873 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 15:54:04.289683 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, return_softmax=True, reduction="mean", )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, return_softmax=True, reduction="mean", ) 	 25401610 	 3872 	 162.74895691871643 	 61.70126748085022 	 3.6159937381744385 	 1.0882258415222168 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 15:57:52.525348 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([686530, 37],"float64"), Tensor([686530],"int64"), margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, return_softmax=True, reduction="mean", )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([686530, 37],"float64"), Tensor([686530],"int64"), margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, return_softmax=True, reduction="mean", ) 	 26088140 	 3872 	 9.64798092842102 	 99.90659356117249 	 0.23091578483581543 	 1.7811987400054932 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 15:59:46.912008 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), 0.0, "mean", )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), 0.0, "mean", ) 	 76204830 	 7473 	 10.022338390350342 	 14.509732723236084 	 0.2745792865753174 	 0.2826998233795166 	 13.552674531936646 	 15.780158519744873 	 0.462740421295166 	 0.26993227005004883 	 
2025-08-05 16:00:44.417812 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), 0.0, "mean", None, )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), 0.0, "mean", None, ) 	 76204830 	 7473 	 10.02684473991394 	 14.528551816940308 	 0.2733640670776367 	 0.28272104263305664 	 13.550190687179565 	 15.783654689788818 	 0.4628641605377197 	 0.2697932720184326 	 
2025-08-05 16:01:42.435452 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), 0.0, "mean", )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), 0.0, "mean", ) 	 76204830 	 7473 	 10.021708011627197 	 14.500041723251343 	 0.27457666397094727 	 0.28403568267822266 	 13.553661584854126 	 15.786559820175171 	 0.46276307106018066 	 0.26989316940307617 	 
2025-08-05 16:02:39.999111 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), 0.0, "mean", None, )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), 0.0, "mean", None, ) 	 76204830 	 7473 	 10.024541139602661 	 14.4988534450531 	 0.2747342586517334 	 0.282759428024292 	 13.557251930236816 	 15.788169860839844 	 0.46282458305358887 	 0.26984405517578125 	 
2025-08-05 16:03:39.443909 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([50803201],"float32"), Tensor([50803201],"float32"), Tensor([50803201],"float32"), 0.5, "mean", None, )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([50803201],"float32"), Tensor([50803201],"float32"), Tensor([50803201],"float32"), 0.5, "mean", None, ) 	 152409603 	 7473 	 12.305643796920776 	 14.531242609024048 	 0.24139189720153809 	 0.28345632553100586 	 16.823899030685425 	 16.82952094078064 	 0.5760524272918701 	 0.28994131088256836 	 
2025-08-05 16:04:42.669205 test begin: paddle.nn.functional.max_pool1d(Tensor([2, 3, 8467201],"float32"), 2, None, 0, False, False, None, )
[Prof] paddle.nn.functional.max_pool1d 	 paddle.nn.functional.max_pool1d(Tensor([2, 3, 8467201],"float32"), 2, None, 0, False, False, None, ) 	 50803206 	 37061 	 9.947425603866577 	 15.42240571975708 	 0.27373743057250977 	 0.4241495132446289 	 28.94796061515808 	 49.04068350791931 	 0.39974117279052734 	 0.6761367321014404 	 
2025-08-05 16:06:27.372327 test begin: paddle.nn.functional.max_pool1d(Tensor([226801, 32, 7],"float32"), 7, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f59df5c5240>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 16:16:50.536505 test begin: paddle.nn.functional.max_pool1d(Tensor([91, 32, 17447],"float32"), 7, )
W0805 16:16:51.696329  2038 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 16:16:51.700836  2038 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.max_pool1d 	 paddle.nn.functional.max_pool1d(Tensor([91, 32, 17447],"float32"), 7, ) 	 50805664 	 37061 	 11.627231359481812 	 8.021074771881104 	 0.3213646411895752 	 0.22047114372253418 	 27.509177446365356 	 47.34182357788086 	 0.3785562515258789 	 0.6528418064117432 	 
2025-08-05 16:18:27.789715 test begin: paddle.nn.functional.max_pool2d(Tensor([10, 128, 480, 83],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([10, 128, 480, 83],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 50995200 	 50852 	 10.609904527664185 	 14.018401861190796 	 0.21303176879882812 	 0.28123950958251953 	 34.22875928878784 	 68.65267157554626 	 0.34313368797302246 	 0.6887352466583252 	 
2025-08-05 16:20:39.581857 test begin: paddle.nn.functional.max_pool2d(Tensor([10, 128, 83, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([10, 128, 83, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 50995200 	 50852 	 10.014902591705322 	 13.894675970077515 	 0.20083022117614746 	 0.2788088321685791 	 33.84002208709717 	 68.87073636054993 	 0.3393704891204834 	 0.6906414031982422 	 
2025-08-05 16:22:48.003707 test begin: paddle.nn.functional.max_pool2d(Tensor([10, 23, 480, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([10, 23, 480, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 52992000 	 50852 	 10.668617486953735 	 15.773774147033691 	 0.21387600898742676 	 0.2940690517425537 	 35.520870208740234 	 71.64840841293335 	 0.3576047420501709 	 0.7213442325592041 	 
2025-08-05 16:25:03.346420 test begin: paddle.nn.functional.max_pool2d(Tensor([1536, 24, 112, 13],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([1536, 24, 112, 13],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 53673984 	 50852 	 18.467588186264038 	 19.462231159210205 	 0.37033915519714355 	 0.39449381828308105 	 25.42007350921631 	 74.94108080863953 	 0.5100054740905762 	 0.7529058456420898 	 
2025-08-05 16:27:22.844806 test begin: paddle.nn.functional.max_pool2d(Tensor([1536, 24, 13, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([1536, 24, 13, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 53673984 	 50852 	 17.17378878593445 	 18.68144989013672 	 0.34576869010925293 	 0.3745567798614502 	 24.95544934272766 	 74.21216154098511 	 0.5005578994750977 	 0.7442989349365234 	 
2025-08-05 16:29:39.601263 test begin: paddle.nn.functional.max_pool2d(Tensor([1536, 3, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([1536, 3, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 57802752 	 50852 	 20.537964820861816 	 19.227359294891357 	 0.4119541645050049 	 0.38561463356018066 	 46.446420431137085 	 80.21888303756714 	 0.4657142162322998 	 0.8072247505187988 	 
2025-08-05 16:32:27.814323 test begin: paddle.nn.functional.max_pool2d(Tensor([169, 24, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([169, 24, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 50878464 	 50852 	 15.313750743865967 	 16.95488142967224 	 0.3069756031036377 	 0.340162992477417 	 41.02053928375244 	 70.77598786354065 	 0.4126758575439453 	 0.7112460136413574 	 
2025-08-05 16:34:53.018767 test begin: paddle.nn.functional.max_pool2d(Tensor([2, 128, 480, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([2, 128, 480, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 58982400 	 50852 	 11.694000244140625 	 16.326794385910034 	 0.23573517799377441 	 0.32836389541625977 	 39.41373896598816 	 79.70468711853027 	 0.3953115940093994 	 0.8034849166870117 	 
2025-08-05 16:37:22.753845 test begin: paddle.nn.functional.max_pool2d(Tensor([2, 64, 704, 704],"float32"), kernel_size=3, stride=2, padding=1, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([2, 64, 704, 704],"float32"), kernel_size=3, stride=2, padding=1, ) 	 63438848 	 50852 	 19.48909854888916 	 21.27310562133789 	 0.39100122451782227 	 0.4275846481323242 	 51.22460460662842 	 89.68656301498413 	 0.5164351463317871 	 0.9008443355560303 	 
2025-08-05 16:40:26.086039 test begin: paddle.nn.functional.max_pool2d(Tensor([8, 13, 704, 704],"float32"), kernel_size=3, stride=2, padding=1, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([8, 13, 704, 704],"float32"), kernel_size=3, stride=2, padding=1, ) 	 51544064 	 50852 	 17.936938285827637 	 17.322965621948242 	 0.36124420166015625 	 0.3475663661956787 	 41.85551619529724 	 72.95570492744446 	 0.4210362434387207 	 0.7315902709960938 	 
2025-08-05 16:42:57.393214 test begin: paddle.nn.functional.max_pool2d(Tensor([8, 64, 141, 704],"float32"), kernel_size=3, stride=2, padding=1, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([8, 64, 141, 704],"float32"), kernel_size=3, stride=2, padding=1, ) 	 50823168 	 50852 	 15.676147937774658 	 17.152069568634033 	 0.3158283233642578 	 0.34400224685668945 	 41.015570402145386 	 71.90790057182312 	 0.41428470611572266 	 0.7211544513702393 	 
2025-08-05 16:45:24.245141 test begin: paddle.nn.functional.max_pool2d(Tensor([8, 64, 704, 141],"float32"), kernel_size=3, stride=2, padding=1, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([8, 64, 704, 141],"float32"), kernel_size=3, stride=2, padding=1, ) 	 50823168 	 50852 	 15.728231191635132 	 17.197391986846924 	 0.31581544876098633 	 0.34615373611450195 	 41.109036684036255 	 71.00044631958008 	 0.4135763645172119 	 0.7121214866638184 	 
2025-08-05 16:47:51.185613 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 16934401],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc1007c3e80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 16:57:57.805899 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 16934401],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0805 16:58:00.396271 102956 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4bdabb2e60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 17:08:02.834554 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, )
W0805 17:08:04.345412  2061 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa20df82ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 17:18:07.694655 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0805 17:18:09.031944 67269 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0fdfd73040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 17:28:12.811307 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 8467201],"int32"), kernel_size=2, stride=2, )
W0805 17:28:14.220768 130695 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdac81af130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 17:38:17.816573 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 8467201],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0805 17:38:19.305321 30695 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6c5a9f70a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 17:48:23.101869 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, )
W0805 17:48:24.397246 93695 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f41cbc12ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 17:58:27.752806 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0805 17:58:29.089125 156719 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6baa5a6c50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 18:08:32.878149 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
W0805 18:08:33.112815 57049 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 2540162448 	 283012 	 12.43364930152893 	 12.260694980621338 	 8.416175842285156e-05 	 0.0002377033233642578 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 18:09:20.345369 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([211680101, 3, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([211680101, 3, 8],"int32"), kernel_size=2, stride=2, ) 	 5080322448 	 283012 	 10.340805053710938 	 11.204028606414795 	 8.654594421386719e-05 	 0.00020575523376464844 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 18:10:05.250211 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([211680101, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([211680101, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 5080322448 	 283012 	 10.563747882843018 	 11.11137342453003 	 8.559226989746094e-05 	 0.00020313262939453125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 18:10:49.929449 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc5a58fbd60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 18:20:55.066194 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0805 18:20:56.593411 135132 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd4b4392c50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 18:30:59.984849 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 3175201, 8],"int32"), kernel_size=2, stride=2, )
W0805 18:31:01.296698 35558 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3e7fe6f100>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 18:41:04.640847 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 3175201, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0805 18:41:07.720126 93611 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7ddd44b0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 18:51:14.296561 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, )
W0805 18:51:15.607007 150101 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3be70170d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 19:01:19.379144 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0805 19:01:20.875098 42412 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f30e97870a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 19:11:24.222682 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 6350401, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, )
W0805 19:11:26.471675 97849 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb96d3770d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 19:21:28.977446 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 6350401, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0805 19:21:31.247244 151564 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8ed3efb040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 11:30:52.835591 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 16934401],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
W0804 11:30:53.124552 85169 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 16934401],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 10262247006 	 283012 	 12.407544374465942 	 15.391239881515503 	 8.797645568847656e-05 	 0.00023794174194335938 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:31:47.606041 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 7696685406 	 283012 	 12.20718240737915 	 11.805490970611572 	 9.059906005859375e-05 	 0.0002033710479736328 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:32:36.970188 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5131125927 	 283012 	 12.187365055084229 	 11.616636276245117 	 8.606910705566406e-05 	 9.298324584960938e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:33:23.146503 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8467201],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8467201],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5131123806 	 283012 	 12.369959831237793 	 11.746346235275269 	 6.914138793945312e-05 	 0.00019168853759765625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:34:10.464910 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 2565564327 	 283012 	 18.109381914138794 	 13.859251976013184 	 9.298324584960938e-05 	 0.0001995563507080078 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:35:10.789432 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 2565564832 	 283012 	 18.013606309890747 	 12.343573808670044 	 9.512901306152344e-05 	 0.00021338462829589844 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:36:07.919253 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 25404048 	 283012 	 12.404555559158325 	 11.44411301612854 	 9.059906005859375e-05 	 0.0001983642578125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:36:55.102300 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 3, 16934401],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 3, 16934401],"int32"), kernel_size=2, stride=2, ) 	 5131125927 	 283012 	 10.771501541137695 	 11.426091432571411 	 5.984306335449219e-05 	 0.00021600723266601562 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:37:41.818123 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 3, 16934401],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 3, 16934401],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 5131125927 	 283012 	 11.604163646697998 	 11.572100400924683 	 9.250640869140625e-05 	 7.295608520507812e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:38:29.267880 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 6350401, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 6350401, 8],"int32"), kernel_size=2, stride=2, ) 	 5131126432 	 283012 	 10.444236516952515 	 11.19607949256897 	 0.000110626220703125 	 0.0005602836608886719 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:39:13.563356 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 6350401, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 6350401, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 5131126432 	 283012 	 10.79213809967041 	 11.001160860061646 	 9.250640869140625e-05 	 0.00019931793212890625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:39:58.096419 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, ) 	 50805648 	 283012 	 10.518373489379883 	 11.057208061218262 	 4.7206878662109375e-05 	 0.00016808509826660156 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:40:43.291630 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 50805648 	 283012 	 10.67927861213684 	 11.26702094078064 	 0.00012111663818359375 	 0.00017786026000976562 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:41:29.359327 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3175201, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3175201, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5131124816 	 283012 	 12.189847230911255 	 11.543889045715332 	 4.5299530029296875e-05 	 6.437301635742188e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:42:16.139440 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5131126432 	 283012 	 12.333332777023315 	 14.905622959136963 	 9.012222290039062e-05 	 0.00021004676818847656 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:43:08.669287 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 7696686416 	 283012 	 12.237716436386108 	 11.491912364959717 	 8.273124694824219e-05 	 0.00019979476928710938 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:43:55.346991 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 6350401, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 6350401, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 10262248016 	 283012 	 12.402210235595703 	 11.491487264633179 	 8.463859558105469e-05 	 0.0002052783966064453 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 11:44:42.453661 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcfb4737df0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 11:54:48.273896 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0804 11:54:49.524039 94225 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd35ee0abf0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 12:04:52.846410 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([1058401, 3, 8],"int32"), kernel_size=2, stride=2, )
W0804 12:04:54.364869 97865 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4eeb1ab0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 12:15:06.466576 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([1058401, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0804 12:15:07.662047 101627 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f649fa0b040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 12:25:10.990278 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, )
W0804 12:25:12.289479 105165 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2b25ff7130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 12:35:15.747046 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0804 12:35:17.123672 108967 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9f38d470a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 12:45:20.607744 test begin: paddle.nn.functional.max_unpool1d(Tensor([105840101, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
W0804 12:45:20.856041 112620 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([105840101, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5080324848 	 283012 	 13.207359552383423 	 11.856971740722656 	 0.0001304149627685547 	 0.0003848075866699219 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 12:46:07.644606 test begin: paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 50805648 	 283012 	 12.212786436080933 	 11.706604242324829 	 9.703636169433594e-05 	 0.00021028518676757812 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 12:46:55.183744 test begin: paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 2590965648 	 283012 	 12.109800338745117 	 11.545453786849976 	 0.00011181831359863281 	 0.00020647048950195312 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 12:47:42.410984 test begin: paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8ba68dbd90>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 12:57:48.430495 test begin: paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0804 12:57:50.850052 116982 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb3d0377040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 13:07:52.994234 test begin: paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([1, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
W0804 13:07:53.235114 120680 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([1, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5080322448 	 283012 	 13.102283954620361 	 12.00443148612976 	 8.797645568847656e-05 	 0.0002608299255371094 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:08:40.858414 test begin: paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5105724048 	 283012 	 12.515792608261108 	 11.663348197937012 	 8.082389831542969e-05 	 0.0002028942108154297 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:09:27.480024 test begin: paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([211680101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([211680101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 10160644848 	 283012 	 12.479747295379639 	 11.808141946792603 	 8.368492126464844e-05 	 0.00019788742065429688 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:10:14.280081 test begin: paddle.nn.functional.max_unpool2d(Tensor([1894, 8, 86, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([1894, 8, 86, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 222571440 	 470794 	 27.462133646011353 	 40.134546518325806 	 0.02972722053527832 	 0.04351973533630371 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:11:54.565949 test begin: paddle.nn.functional.max_unpool2d(Tensor([189401, 8, 86, 39],"float32"), Tensor([189401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([189401, 8, 86, 39],"float32"), Tensor([189401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 10164015264 	 470794 	 27.406635522842407 	 40.1290979385376 	 0.029742002487182617 	 0.04352450370788574 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:13:31.149322 test begin: paddle.nn.functional.max_unpool2d(Tensor([189401, 8, 86, 39],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([189401, 8, 86, 39],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5083724880 	 470794 	 27.41058373451233 	 39.98132848739624 	 0.029742717742919922 	 0.04336833953857422 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:15:13.439932 test begin: paddle.nn.functional.max_unpool2d(Tensor([3887, 16, 43, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([3887, 16, 43, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 134484736 	 470794 	 10.024656057357788 	 14.199471235275269 	 0.005007743835449219 	 0.000194549560546875 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:16:07.012449 test begin: paddle.nn.functional.max_unpool2d(Tensor([388701, 16, 43, 19],"float32"), Tensor([388701, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([388701, 16, 43, 19],"float32"), Tensor([388701, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 10162198944 	 470794 	 10.171349287033081 	 14.100944519042969 	 0.010992288589477539 	 0.00018930435180664062 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:17:02.307587 test begin: paddle.nn.functional.max_unpool2d(Tensor([388701, 16, 43, 19],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([388701, 16, 43, 19],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5081936080 	 470794 	 10.159831285476685 	 14.277201414108276 	 0.00043463706970214844 	 0.0001850128173828125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:17:57.895791 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([388701, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([388701, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5081936080 	 470794 	 14.349363565444946 	 17.17879033088684 	 9.274482727050781e-05 	 0.00016736984252929688 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:19:02.662335 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([840101, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([840101, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5081317920 	 470794 	 14.319380521774292 	 18.305620431900024 	 0.00010013580322265625 	 0.0001971721649169922 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:20:09.742245 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([189401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([189401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5083724880 	 470794 	 27.322121381759644 	 40.07888412475586 	 0.029650449752807617 	 0.04346179962158203 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:21:49.355097 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 2612, 19],"float32"), Tensor([6401, 16, 2612, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 2612, 19],"float32"), Tensor([6401, 16, 2612, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 10165402496 	 470794 	 14.322498559951782 	 18.053635358810425 	 8.606910705566406e-05 	 0.0001862049102783203 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:22:56.845565 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 2612, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 2612, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5166375120 	 470794 	 14.259019374847412 	 18.199227809906006 	 9.083747863769531e-05 	 0.00020003318786621094 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:24:02.878830 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 1154],"float32"), Tensor([6401, 16, 43, 1154],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 1154],"float32"), Tensor([6401, 16, 43, 1154],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 10164173504 	 470794 	 14.419746398925781 	 14.109036684036255 	 8.0108642578125e-05 	 0.00016951560974121094 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:25:01.466387 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 1154],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 1154],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5165760624 	 470794 	 14.505193948745728 	 15.04845404624939 	 8.797645568847656e-05 	 0.0001957416534423828 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:26:05.084625 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([3887, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([3887, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 134484736 	 470794 	 10.435624837875366 	 14.509185791015625 	 8.106231689453125e-05 	 0.0001983642578125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:27:00.494413 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 16, 2612, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 16, 2612, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5166375120 	 470794 	 10.242794036865234 	 17.51429772377014 	 0.000766754150390625 	 0.00022912025451660156 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:27:59.058618 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 16, 43, 1154],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 16, 43, 1154],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5165760624 	 470794 	 10.02884840965271 	 14.214421272277832 	 0.005882978439331055 	 0.0001862049102783203 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:28:52.172324 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 972, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 972, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5166861596 	 470794 	 10.000325441360474 	 14.466882944107056 	 0.0009696483612060547 	 0.00018405914306640625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:29:48.879702 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 237, 86, 39],"float32"), Tensor([6401, 237, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 237, 86, 39],"float32"), Tensor([6401, 237, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 10176284196 	 470794 	 27.48083186149597 	 40.16527438163757 	 0.029812097549438477 	 0.04357481002807617 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:31:25.272824 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 237, 86, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 237, 86, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5259893730 	 470794 	 27.412925243377686 	 39.954198598861694 	 0.02974867820739746 	 0.04337167739868164 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:33:01.272040 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 1182],"float32"), Tensor([6401, 32, 21, 1182],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 1182],"float32"), Tensor([6401, 32, 21, 1182],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 10168679808 	 470794 	 10.029428958892822 	 14.810693264007568 	 8.273124694824219e-05 	 0.00019407272338867188 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:33:54.908877 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 1182],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 1182],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5123053152 	 470794 	 14.199726343154907 	 18.491679906845093 	 9.298324584960938e-05 	 0.0001926422119140625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:35:00.557204 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 32, 21, 1182],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 32, 21, 1182],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5123053152 	 470794 	 10.008536338806152 	 14.396790266036987 	 8.630752563476562e-05 	 0.00021195411682128906 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:35:53.275920 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 32, 2757, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 32, 2757, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5121209664 	 470794 	 9.97545313835144 	 14.423198223114014 	 8.273124694824219e-05 	 0.0001862049102783203 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:36:46.739490 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 4201, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 4201, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5121036837 	 470794 	 10.014382600784302 	 14.32004427909851 	 7.62939453125e-05 	 0.0001862049102783203 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:37:41.972004 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([8401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([8401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 89522496 	 470794 	 10.22702145576477 	 14.399318933486938 	 9.560585021972656e-05 	 0.00019598007202148438 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:38:37.932148 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 2757, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 2757, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5121209664 	 470794 	 10.250693321228027 	 14.628145217895508 	 8.58306884765625e-05 	 0.00018596649169921875 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:39:31.676900 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 2757, 9],"float32"), Tensor([6401, 32, 2757, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 2757, 9],"float32"), Tensor([6401, 32, 2757, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 10164992832 	 470794 	 10.347246646881104 	 19.158892154693604 	 8.416175842285156e-05 	 0.00019621849060058594 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:40:30.295053 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 4201, 21, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 4201, 21, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5121036837 	 470794 	 10.314693212509155 	 15.02940034866333 	 9.012222290039062e-05 	 0.0001888275146484375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:41:26.373655 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 4201, 21, 9],"float32"), Tensor([6401, 4201, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 4201, 21, 9],"float32"), Tensor([6401, 4201, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 10164647178 	 470794 	 10.241524934768677 	 14.306096076965332 	 7.987022399902344e-05 	 0.00018668174743652344 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:42:24.670838 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 2545, 39],"float32"), Tensor([6401, 8, 2545, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 2545, 39],"float32"), Tensor([6401, 8, 2545, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 10165300080 	 470794 	 27.566776037216187 	 40.209477186203 	 0.029944419860839844 	 0.04362154006958008 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:44:01.458891 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 2545, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 2545, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5254401672 	 470794 	 27.70013999938965 	 40.20182466506958 	 0.03005051612854004 	 0.04364299774169922 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:45:38.602111 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 1154],"float32"), Tensor([6401, 8, 86, 1154],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 1154],"float32"), Tensor([6401, 8, 86, 1154],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 10164173504 	 470794 	 27.672297954559326 	 40.2260959148407 	 0.030039548873901367 	 0.04361557960510254 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:47:21.118709 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 1154],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 1154],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5253838384 	 470794 	 27.697689056396484 	 40.19881868362427 	 0.030049800872802734 	 0.04359078407287598 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:48:57.833711 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([1894, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([1894, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 222571440 	 470794 	 27.650951385498047 	 40.2012619972229 	 0.03001689910888672 	 0.04363751411437988 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:50:34.299817 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 237, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 237, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5259893730 	 470794 	 27.691380500793457 	 40.20481729507446 	 0.030048131942749023 	 0.04362773895263672 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:52:18.042210 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 8, 2545, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 8, 2545, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5254401672 	 470794 	 27.760749340057373 	 40.20976519584656 	 0.030140399932861328 	 0.0436248779296875 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:53:54.869464 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 8, 86, 1154],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 8, 86, 1154],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5253838384 	 470794 	 27.325294971466064 	 40.167354345321655 	 0.029650211334228516 	 0.04371476173400879 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:55:33.416002 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 972, 43, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 972, 43, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5166861596 	 470794 	 10.04517936706543 	 14.087884902954102 	 0.009103775024414062 	 0.00019168853759765625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:56:27.476049 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 972, 43, 19],"float32"), Tensor([6401, 972, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 972, 43, 19],"float32"), Tensor([6401, 972, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 10166375448 	 470794 	 10.225024938583374 	 14.198378324508667 	 8.249282836914062e-05 	 0.0001926422119140625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:57:21.642647 test begin: paddle.nn.functional.max_unpool2d(Tensor([8401, 32, 21, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([8401, 32, 21, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 89522496 	 470794 	 14.399308919906616 	 14.555174112319946 	 8.225440979003906e-05 	 0.00018262863159179688 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:58:19.936109 test begin: paddle.nn.functional.max_unpool2d(Tensor([840101, 32, 21, 9],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([840101, 32, 21, 9],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5081317920 	 470794 	 14.541631698608398 	 14.586690187454224 	 8.940696716308594e-05 	 0.0001888275146484375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 13:59:17.784360 test begin: paddle.nn.functional.max_unpool2d(Tensor([840101, 32, 21, 9],"float32"), Tensor([840101, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([840101, 32, 21, 9],"float32"), Tensor([840101, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 10161861696 	 470794 	 14.466386795043945 	 14.610172748565674 	 8.296966552734375e-05 	 0.0001823902130126953 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 14:00:20.308921 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 211681, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4cb86582b0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 14:10:25.401025 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 211681, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
W0804 14:10:29.642802 137581 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc8f010f0a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 14:20:29.911087 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 211681, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0804 14:20:32.935981 140974 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4c3ef6b010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 14:30:34.487045 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0804 14:30:42.106994 144343 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4086f0b400>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 14:40:47.092633 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
W0804 14:40:51.371397 148091 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f362b39f040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 14:50:53.199470 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0804 14:50:56.254153 151444 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f65c61bb400>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 15:00:57.897161 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0804 15:01:04.493664 155041 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3c4c28f010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 15:11:02.557524 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
W0804 15:11:10.420691 158493 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f50fe452e60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 15:21:07.160904 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([14112101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0804 15:21:08.829959 161924 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([14112101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5080356720 	 349895 	 13.215911626815796 	 15.573065996170044 	 0.000102996826171875 	 0.0002155303955078125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:22:00.367251 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([14112101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([14112101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5080356720 	 349895 	 10.09057903289795 	 11.785878896713257 	 7.653236389160156e-05 	 0.0001926422119140625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:22:45.064369 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([7056101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([7056101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2540196720 	 349895 	 11.974536895751953 	 11.761013269424438 	 7.176399230957031e-05 	 0.0001900196075439453 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:23:30.179264 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 423361, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3db2170250>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 15:33:35.876852 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 423361, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
W0804 15:33:44.093223  2717 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fea62587040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 15:43:44.848934 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0804 15:43:45.142596  6750 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131141380 	 349895 	 13.049326658248901 	 15.807312250137329 	 8.487701416015625e-05 	 0.00027298927307128906 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:44:40.074430 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131141380 	 349895 	 10.463905334472656 	 11.901822805404663 	 0.00010418891906738281 	 0.00020313262939453125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:45:24.867718 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131141380 	 349895 	 10.001498937606812 	 12.085727214813232 	 9.512901306152344e-05 	 0.00020313262939453125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:46:07.871840 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565607050 	 349895 	 10.062286853790283 	 11.778233766555786 	 7.82012939453125e-05 	 0.00020599365234375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:46:52.704694 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 2565607050 	 349895 	 9.934117317199707 	 12.032840013504028 	 8.034706115722656e-05 	 0.00019407272338867188 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:47:38.468660 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565607050 	 349895 	 10.991435050964355 	 11.895621299743652 	 8.416175842285156e-05 	 0.00020647048950195312 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:48:22.243534 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 7696702980 	 349895 	 13.35826563835144 	 11.975050210952759 	 8.797645568847656e-05 	 0.00020956993103027344 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:49:11.192243 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 7696702980 	 349895 	 10.324835538864136 	 12.68654179573059 	 8.273124694824219e-05 	 0.00020456314086914062 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:49:55.908848 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131137744 	 349895 	 12.493417501449585 	 13.31657075881958 	 8.749961853027344e-05 	 0.00021195411682128906 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:50:45.926412 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131137744 	 349895 	 13.422634601593018 	 11.95759391784668 	 8.463859558105469e-05 	 0.0002071857452392578 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:51:33.821450 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131137744 	 349895 	 9.88950228691101 	 13.653714656829834 	 7.724761962890625e-05 	 0.0002353191375732422 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:52:19.015716 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565605232 	 349895 	 9.997728824615479 	 12.00816559791565 	 7.82012939453125e-05 	 0.00020003318786621094 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:53:01.934659 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 2565605232 	 349895 	 9.992269515991211 	 15.773181200027466 	 8.821487426757812e-05 	 0.00020384788513183594 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:53:52.551496 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565605232 	 349895 	 9.964605331420898 	 12.086584329605103 	 7.605552673339844e-05 	 0.0001995563507080078 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:54:37.426667 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 7696699344 	 349895 	 9.820043325424194 	 11.992149114608765 	 8.821487426757812e-05 	 0.00020313262939453125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:55:20.326913 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 7696699344 	 349895 	 9.879947423934937 	 11.903818368911743 	 3.409385681152344e-05 	 0.00019884109497070312 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:56:03.141459 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131135320 	 349895 	 9.813666582107544 	 11.971190929412842 	 4.6253204345703125e-05 	 0.00021505355834960938 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:56:46.217128 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131135320 	 349895 	 9.82485032081604 	 11.956998109817505 	 4.673004150390625e-05 	 0.00019884109497070312 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:57:28.916326 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131135320 	 349895 	 10.125229835510254 	 11.915565967559814 	 0.00013017654418945312 	 0.00019931793212890625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:58:13.518394 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565604020 	 349895 	 13.338718175888062 	 14.937023639678955 	 8.392333984375e-05 	 0.00020194053649902344 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:59:08.425783 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 2565604020 	 349895 	 11.356919527053833 	 12.16422176361084 	 8.0108642578125e-05 	 0.00020599365234375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 15:59:54.226206 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565604020 	 349895 	 13.342554092407227 	 11.860651731491089 	 8.368492126464844e-05 	 0.0001823902130126953 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:00:41.886013 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 7696696920 	 349895 	 10.160660028457642 	 11.858047723770142 	 7.677078247070312e-05 	 0.00020051002502441406 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:01:24.940163 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 7696696920 	 349895 	 10.746095418930054 	 11.812022924423218 	 8.20159912109375e-05 	 0.00019979476928710938 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:02:08.463769 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 211681, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 211681, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565610080 	 349895 	 12.016239881515503 	 11.963101863861084 	 7.62939453125e-05 	 0.000202178955078125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:02:53.339164 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565607050 	 349895 	 13.492838144302368 	 11.930720567703247 	 8.749961853027344e-05 	 0.00020885467529296875 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:03:40.965943 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565605232 	 349895 	 12.498144626617432 	 11.894396305084229 	 7.939338684082031e-05 	 0.00020003318786621094 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:04:28.702457 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 423361],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 423361],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565604020 	 349895 	 11.70435881614685 	 12.17195463180542 	 7.772445678710938e-05 	 0.00020503997802734375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:05:13.465636 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131165620 	 349895 	 13.276052474975586 	 11.830170154571533 	 8.368492126464844e-05 	 0.00020122528076171875 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:06:02.398784 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131165620 	 349895 	 10.598313570022583 	 11.985120058059692 	 0.000133514404296875 	 0.00020360946655273438 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:06:47.060100 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131166832 	 349895 	 10.032981157302856 	 15.32424545288086 	 4.172325134277344e-05 	 0.00021600723266601562 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:07:34.775942 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131166832 	 349895 	 9.959620237350464 	 11.715841054916382 	 4.363059997558594e-05 	 0.00019359588623046875 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:08:18.265506 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131168650 	 349895 	 10.078203201293945 	 14.618562936782837 	 6.818771362304688e-05 	 0.00021719932556152344 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:09:06.982955 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131168650 	 349895 	 9.8431556224823 	 11.729140520095825 	 4.696846008300781e-05 	 0.00019979476928710938 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:09:50.342228 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131171680 	 349895 	 9.95301103591919 	 11.790964841842651 	 4.792213439941406e-05 	 0.00021147727966308594 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:10:34.275398 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131171680 	 349895 	 10.283288478851318 	 11.924128532409668 	 4.5299530029296875e-05 	 0.00020194053649902344 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:11:17.422531 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50839920 	 349895 	 9.89162826538086 	 11.871451616287231 	 4.410743713378906e-05 	 0.00022220611572265625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:11:59.981908 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 50839920 	 349895 	 9.93123173713684 	 11.563762664794922 	 4.57763671875e-05 	 0.00020074844360351562 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:12:43.690804 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25438320 	 349895 	 10.07414436340332 	 11.8968346118927 	 4.458427429199219e-05 	 0.000202178955078125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:13:26.642660 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 846721],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 846721],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 10262258520 	 349895 	 9.878255844116211 	 11.75984811782837 	 8.511543273925781e-05 	 0.0001952648162841797 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:14:08.868803 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 846721],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 846721],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 10262258520 	 349895 	 9.889688491821289 	 11.767288446426392 	 8.225440979003906e-05 	 0.00020647048950195312 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:14:53.180851 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 705601, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 705601, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 10262260944 	 349895 	 9.94965934753418 	 11.938966035842896 	 8.654594421386719e-05 	 0.000217437744140625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:15:37.925584 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 705601, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 705601, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 10262260944 	 349895 	 9.832119941711426 	 11.931861400604248 	 8.130073547363281e-05 	 0.0002014636993408203 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:16:23.968624 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 564481, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 564481, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 10262264580 	 349895 	 9.783955574035645 	 11.742347717285156 	 7.915496826171875e-05 	 0.0002067089080810547 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:17:06.986510 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 564481, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 564481, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 10262264580 	 349895 	 9.813865423202515 	 11.846587896347046 	 5.53131103515625e-05 	 0.0002071857452392578 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 16:17:50.641138 test begin: paddle.nn.functional.max_unpool3d(Tensor([141121, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6bee297f40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:27:56.993628 test begin: paddle.nn.functional.max_unpool3d(Tensor([141121, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
W0804 16:28:05.094565 22209 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fec65adb0a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:38:02.869815 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0804 16:38:05.899058 25935 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe02585f460>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:48:07.793571 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
W0804 16:48:14.707845 29665 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f479ac77040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:58:12.565964 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0804 16:58:15.655709 33074 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7eff704bf400>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:08:17.135852 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0804 17:08:20.174417 35685 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f489906f010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:18:21.759362 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
W0804 17:18:25.927353 38328 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd1f50d6e60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:28:26.376208 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0804 17:28:29.595063 40980 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0a2c797070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:38:31.088359 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
W0804 17:38:41.442970 43473 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa4b8e1b0a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:48:52.886500 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0804 17:48:55.895721 45979 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc767fff010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:58:57.610167 test begin: paddle.nn.functional.mish(Tensor([12, 10585, 20, 20],"float32"), )
W0804 17:58:58.558040 48512 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 10585, 20, 20],"float32"), ) 	 50808000 	 32808 	 9.99581265449524 	 9.840051889419556 	 0.3117103576660156 	 0.3064258098602295 	 14.869866609573364 	 14.900306463241577 	 0.4628434181213379 	 0.4640085697174072 	 
2025-08-04 17:59:50.944837 test begin: paddle.nn.functional.mish(Tensor([12, 128, 40, 827],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 128, 40, 827],"float32"), ) 	 50810880 	 32808 	 10.023658990859985 	 11.449564218521118 	 0.31209754943847656 	 0.3064994812011719 	 14.876677751541138 	 14.905021905899048 	 0.4635639190673828 	 0.46437907218933105 	 
2025-08-04 18:00:46.073812 test begin: paddle.nn.functional.mish(Tensor([12, 128, 827, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 128, 827, 40],"float32"), ) 	 50810880 	 32808 	 10.0042245388031 	 9.852025508880615 	 0.3116278648376465 	 0.3064231872558594 	 14.873819589614868 	 14.906955242156982 	 0.4635162353515625 	 0.46442723274230957 	 
2025-08-04 18:01:40.382853 test begin: paddle.nn.functional.mish(Tensor([12, 256, 40, 414],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 256, 40, 414],"float32"), ) 	 50872320 	 32808 	 10.001334428787231 	 9.850606441497803 	 0.3115580081939697 	 0.30684781074523926 	 14.893787384033203 	 14.922597885131836 	 0.46396660804748535 	 0.4648137092590332 	 
2025-08-04 18:02:34.355000 test begin: paddle.nn.functional.mish(Tensor([12, 256, 414, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 256, 414, 40],"float32"), ) 	 50872320 	 32808 	 10.002355098724365 	 9.85046124458313 	 0.3115658760070801 	 0.3068661689758301 	 14.893898248672485 	 14.922294855117798 	 0.4639101028442383 	 0.464846134185791 	 
2025-08-04 18:03:26.838323 test begin: paddle.nn.functional.mish(Tensor([12, 2647, 40, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 2647, 40, 40],"float32"), ) 	 50822400 	 32808 	 10.019627571105957 	 9.842739343643188 	 0.3121638298034668 	 0.30652427673339844 	 14.87977933883667 	 14.909736156463623 	 0.46378040313720703 	 0.4644484519958496 	 
2025-08-04 18:04:19.065450 test begin: paddle.nn.functional.mish(Tensor([12, 512, 20, 414],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 512, 20, 414],"float32"), ) 	 50872320 	 32808 	 10.003385305404663 	 9.852173566818237 	 0.3116321563720703 	 0.30681777000427246 	 14.894676208496094 	 14.922893285751343 	 0.46397924423217773 	 0.46484875679016113 	 
2025-08-04 18:05:13.340382 test begin: paddle.nn.functional.mish(Tensor([12, 512, 414, 20],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 512, 414, 20],"float32"), ) 	 50872320 	 32808 	 10.011123180389404 	 10.879228591918945 	 0.3115732669830322 	 0.3069288730621338 	 14.895596742630005 	 14.922122478485107 	 0.4640929698944092 	 0.4648280143737793 	 
2025-08-04 18:06:06.300203 test begin: paddle.nn.functional.mish(Tensor([125, 256, 40, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([125, 256, 40, 40],"float32"), ) 	 51200000 	 32808 	 10.074830770492554 	 9.909735202789307 	 0.3140566349029541 	 0.30868983268737793 	 14.98090672492981 	 15.017781257629395 	 0.46657323837280273 	 0.4677400588989258 	 
2025-08-04 18:06:58.019595 test begin: paddle.nn.functional.mish(Tensor([249, 128, 40, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([249, 128, 40, 40],"float32"), ) 	 50995200 	 32808 	 10.034505128860474 	 10.123974084854126 	 0.31261277198791504 	 0.3075225353240967 	 14.930420637130737 	 14.958604335784912 	 0.4651675224304199 	 0.4659268856048584 	 
2025-08-04 18:07:51.066247 test begin: paddle.nn.functional.mish(Tensor([249, 512, 20, 20],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([249, 512, 20, 20],"float32"), ) 	 50995200 	 32808 	 10.042418241500854 	 9.87211012840271 	 0.3125417232513428 	 0.30755138397216797 	 14.929417133331299 	 14.959140539169312 	 0.46498942375183105 	 0.4660038948059082 	 
2025-08-04 18:08:42.626976 test begin: paddle.nn.functional.mse_loss(Tensor([24904, 12, 170, 1],"float32"), Tensor([24904, 12, 170, 1],"float32"), "mean", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([24904, 12, 170, 1],"float32"), Tensor([24904, 12, 170, 1],"float32"), "mean", ) 	 101608320 	 13409 	 11.990717887878418 	 8.02584719657898 	 0.22818279266357422 	 0.20371198654174805 	 14.21464991569519 	 15.555290937423706 	 0.36127328872680664 	 0.2965524196624756 	 
2025-08-04 18:09:34.276375 test begin: paddle.nn.functional.mse_loss(Tensor([3548, 12, 1194, 1],"float32"), Tensor([3548, 12, 1194, 1],"float32"), "mean", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([3548, 12, 1194, 1],"float32"), Tensor([3548, 12, 1194, 1],"float32"), "mean", ) 	 101671488 	 13409 	 11.9960618019104 	 8.03087854385376 	 0.2282094955444336 	 0.20381784439086914 	 14.223313570022583 	 15.565048456192017 	 0.3615419864654541 	 0.29656267166137695 	 
2025-08-04 18:10:27.858397 test begin: paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 8],"float32"), "mean", )
/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py:104: UserWarning: Using a target size (torch.Size([3548, 12, 170, 8])) that is different to the input size (torch.Size([3548, 12, 170, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return func(*args, **kwargs)
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 8],"float32"), "mean", ) 	 65141280 	 13409 	 11.59468150138855 	 7.707120656967163 	 0.22065973281860352 	 0.19563007354736328 	 20.7243754863739 	 21.675135612487793 	 0.3949553966522217 	 0.3306593894958496 	 
2025-08-04 18:11:30.672730 test begin: paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 1],"float32"), "mean", )
/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py:104: UserWarning: Using a target size (torch.Size([3548, 12, 170, 1])) that is different to the input size (torch.Size([3548, 12, 170, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return func(*args, **kwargs)
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 1],"float32"), "mean", ) 	 65141280 	 13409 	 11.606338500976562 	 7.712488889694214 	 0.22086668014526367 	 0.19559192657470703 	 15.230851173400879 	 21.676127910614014 	 0.38715291023254395 	 0.3307151794433594 	 
2025-08-04 18:12:29.875174 test begin: paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 8],"float32"), "mean", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 8],"float32"), "mean", ) 	 115806720 	 13409 	 13.629113674163818 	 9.51111125946045 	 0.2593848705291748 	 0.2413311004638672 	 16.173175573349 	 17.688660860061646 	 0.41105055809020996 	 0.33693456649780273 	 
2025-08-04 18:13:28.822634 test begin: paddle.nn.functional.mse_loss(Tensor([3548, 85, 170, 1],"float32"), Tensor([3548, 85, 170, 1],"float32"), "mean", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([3548, 85, 170, 1],"float32"), Tensor([3548, 85, 170, 1],"float32"), "mean", ) 	 102537200 	 13409 	 12.093271493911743 	 8.093703746795654 	 0.23018169403076172 	 0.20541596412658691 	 14.33358359336853 	 15.693522930145264 	 0.3644077777862549 	 0.2989828586578369 	 
2025-08-04 18:14:20.803801 test begin: paddle.nn.functional.mse_loss(Tensor([517, 4, 3, 64, 128],"float32"), Tensor([517, 4, 3, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([517, 4, 3, 64, 128],"float32"), Tensor([517, 4, 3, 64, 128],"float32"), "none", ) 	 101646336 	 13409 	 10.003931999206543 	 5.990082740783691 	 0.3812906742095947 	 0.4565560817718506 	 12.397503137588501 	 19.390219926834106 	 0.4724009037017822 	 0.36937928199768066 	 
2025-08-04 18:15:11.130260 test begin: paddle.nn.functional.mse_loss(Tensor([64, 3, 3, 64, 1379],"float32"), Tensor([64, 3, 3, 64, 1379],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 3, 3, 64, 1379],"float32"), Tensor([64, 3, 3, 64, 1379],"float32"), "none", ) 	 101670912 	 13409 	 10.006654977798462 	 5.994359970092773 	 0.38139772415161133 	 0.45672082901000977 	 12.397876739501953 	 19.394669771194458 	 0.4725155830383301 	 0.3694446086883545 	 
2025-08-04 18:16:03.717336 test begin: paddle.nn.functional.mse_loss(Tensor([64, 3, 3, 690, 128],"float32"), Tensor([64, 3, 3, 690, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 3, 3, 690, 128],"float32"), Tensor([64, 3, 3, 690, 128],"float32"), "none", ) 	 101744640 	 13409 	 10.012746095657349 	 6.682863473892212 	 0.3815879821777344 	 0.45702695846557617 	 12.395329475402832 	 19.413769006729126 	 0.4723939895629883 	 0.36979222297668457 	 
2025-08-04 18:16:56.788298 test begin: paddle.nn.functional.mse_loss(Tensor([64, 3, 33, 64, 128],"float32"), Tensor([64, 3, 33, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 3, 33, 64, 128],"float32"), Tensor([64, 3, 33, 64, 128],"float32"), "none", ) 	 103809024 	 13409 	 10.213145732879639 	 6.117136001586914 	 0.3892245292663574 	 0.46622753143310547 	 12.666855335235596 	 19.79958748817444 	 0.4826927185058594 	 0.37711143493652344 	 
2025-08-04 18:17:50.575316 test begin: paddle.nn.functional.mse_loss(Tensor([64, 33, 3, 64, 128],"float32"), Tensor([64, 33, 3, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 33, 3, 64, 128],"float32"), Tensor([64, 33, 3, 64, 128],"float32"), "none", ) 	 103809024 	 13409 	 10.213404893875122 	 6.117245674133301 	 0.3892788887023926 	 0.46623873710632324 	 12.65041184425354 	 19.797761917114258 	 0.4820847511291504 	 0.37706780433654785 	 
2025-08-04 18:18:42.652437 test begin: paddle.nn.functional.mse_loss(Tensor([64, 4, 25, 64, 128],"float32"), Tensor([64, 4, 25, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 4, 25, 64, 128],"float32"), Tensor([64, 4, 25, 64, 128],"float32"), "none", ) 	 104857600 	 13409 	 10.314183235168457 	 6.17691445350647 	 0.392946720123291 	 0.4707322120666504 	 12.776902914047241 	 19.987087965011597 	 0.48685646057128906 	 0.38088464736938477 	 
2025-08-04 18:19:34.578353 test begin: paddle.nn.functional.mse_loss(Tensor([64, 4, 3, 517, 128],"float32"), Tensor([64, 4, 3, 517, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 4, 3, 517, 128],"float32"), Tensor([64, 4, 3, 517, 128],"float32"), "none", ) 	 101646336 	 13409 	 10.0218665599823 	 5.991485595703125 	 0.38118743896484375 	 0.4565412998199463 	 12.396810531616211 	 19.38947319984436 	 0.4725008010864258 	 0.3693554401397705 	 
2025-08-04 18:20:28.830908 test begin: paddle.nn.functional.mse_loss(Tensor([64, 4, 3, 64, 1034],"float32"), Tensor([64, 4, 3, 64, 1034],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 4, 3, 64, 1034],"float32"), Tensor([64, 4, 3, 64, 1034],"float32"), "none", ) 	 101646336 	 13409 	 10.003564834594727 	 5.990110158920288 	 0.3812437057495117 	 0.4565713405609131 	 12.39705777168274 	 19.393451690673828 	 0.4724726676940918 	 0.36933016777038574 	 
2025-08-04 18:21:20.550479 test begin: paddle.nn.functional.mse_loss(Tensor([690, 3, 3, 64, 128],"float32"), Tensor([690, 3, 3, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([690, 3, 3, 64, 128],"float32"), Tensor([690, 3, 3, 64, 128],"float32"), "none", ) 	 101744640 	 13409 	 10.009727478027344 	 5.995833873748779 	 0.3814718723297119 	 0.4569671154022217 	 12.394786834716797 	 19.410687685012817 	 0.4723856449127197 	 0.37012481689453125 	 
2025-08-04 18:22:10.978406 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), reduction="mean", weight=None, )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), reduction="mean", weight=None, ) 	 50803210 	 2957 	 9.978456735610962 	 9.833459377288818 	 0.3134176731109619 	 0.28261399269104004 	 13.694819927215576 	 12.719245195388794 	 0.36513543128967285 	 0.3387606143951416 	 
2025-08-04 18:23:05.104199 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), reduction="mean", weight=Tensor([5, 5080321],"float64"), )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), reduction="mean", weight=Tensor([5, 5080321],"float64"), ) 	 76204815 	 2957 	 11.28626012802124 	 11.149619817733765 	 0.32455945014953613 	 0.2956259250640869 	 15.90010142326355 	 14.973862886428833 	 0.3925158977508545 	 0.3452789783477783 	 
2025-08-04 18:24:02.266779 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), weight=Tensor([5, 5080321],"float64"), reduction="mean", name=None, )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), weight=Tensor([5, 5080321],"float64"), reduction="mean", name=None, ) 	 76204815 	 2957 	 11.28798794746399 	 11.133514404296875 	 0.32460618019104004 	 0.2955152988433838 	 15.9021737575531 	 14.974111795425415 	 0.39260244369506836 	 0.34520387649536133 	 
2025-08-04 18:24:57.768881 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), reduction="mean", weight=None, )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), reduction="mean", weight=None, ) 	 50803210 	 2957 	 10.024648189544678 	 10.306170225143433 	 0.314835786819458 	 0.2962625026702881 	 13.943397283554077 	 13.180345058441162 	 0.371657133102417 	 0.35096025466918945 	 
2025-08-04 18:25:46.409887 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), reduction="mean", weight=Tensor([5080321, 5],"float64"), )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), reduction="mean", weight=Tensor([5080321, 5],"float64"), ) 	 76204815 	 2957 	 11.3453049659729 	 11.623179912567139 	 0.3261449337005615 	 0.30866026878356934 	 16.155454397201538 	 15.411864757537842 	 0.3988008499145508 	 0.35530614852905273 	 
2025-08-04 18:26:43.196094 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), weight=Tensor([5080321, 5],"float64"), reduction="mean", name=None, )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), weight=Tensor([5080321, 5],"float64"), reduction="mean", name=None, ) 	 76204815 	 2957 	 11.344727277755737 	 11.62521243095398 	 0.3261442184448242 	 0.3085813522338867 	 16.155662059783936 	 15.411941289901733 	 0.3988006114959717 	 0.35527586936950684 	 
2025-08-04 18:27:41.344885 test begin: paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, )
W0804 18:27:42.002624 55160 dygraph_functions.cc:93089] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, ) 	 38102403 	 7942 	 30.718842267990112 	 128.88239860534668 	 0.0001995563507080078 	 5.517855405807495 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:30:56.209760 test begin: paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, ) 	 38102403 	 7942 	 30.08356022834778 	 128.2668845653534 	 0.00013208389282226562 	 16.506081342697144 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:34:09.603359 test begin: paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, ) 	 38102403 	 7942 	 30.736369371414185 	 128.92063808441162 	 0.0001919269561767578 	 5.519817590713501 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:37:23.151777 test begin: paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, ) 	 76204803 	 7942 	 59.76359677314758 	 257.64920377731323 	 0.00041675567626953125 	 11.027150392532349 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:43:46.799768 test begin: paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, ) 	 76204803 	 7942 	 58.596086740493774 	 256.3906829357147 	 0.0002808570861816406 	 32.99126172065735 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:50:10.019602 test begin: paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, ) 	 76204803 	 7942 	 59.73799180984497 	 257.662682056427 	 0.0004169940948486328 	 11.03195071220398 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:56:33.695257 test begin: paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, ) 	 25401610 	 7942 	 10.141571044921875 	 47.84183955192566 	 4.458427429199219e-05 	 3.0785417556762695 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:57:50.777917 test begin: paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, ) 	 25401610 	 7942 	 10.006280899047852 	 47.79670310020447 	 3.790855407714844e-05 	 6.149807453155518 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:59:07.723231 test begin: paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, ) 	 25401610 	 7942 	 10.320998191833496 	 47.847912311553955 	 3.266334533691406e-05 	 3.0782310962677 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 19:00:25.785364 test begin: paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="mean", name=None, )
[Prof] paddle.nn.functional.nll_loss 	 paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="mean", name=None, ) 	 25443143 	 366849 	 10.02325177192688 	 13.79043173789978 	 6.556510925292969e-05 	 0.00019478797912597656 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 19:01:43.297306 test begin: paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="none", name=None, )
[Prof] paddle.nn.functional.nll_loss 	 paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="none", name=None, ) 	 25443143 	 366849 	 10.332947969436646 	 8.800454139709473 	 4.601478576660156e-05 	 8.845329284667969e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 19:02:55.451483 test begin: paddle.nn.functional.normalize(Tensor([2009, 25288],"float32"), )
[Prof] paddle.nn.functional.normalize 	 paddle.nn.functional.normalize(Tensor([2009, 25288],"float32"), ) 	 50803592 	 21309 	 10.107748746871948 	 9.997573137283325 	 0.0969855785369873 	 0.15983104705810547 	 57.17158913612366 	 68.2412474155426 	 0.5492479801177979 	 0.23375868797302246 	 
2025-08-04 19:05:24.720579 test begin: paddle.nn.functional.normalize(Tensor([2081, 24413],"float32"), )
[Prof] paddle.nn.functional.normalize 	 paddle.nn.functional.normalize(Tensor([2081, 24413],"float32"), ) 	 50803453 	 21309 	 10.126994132995605 	 10.035992622375488 	 0.09722471237182617 	 0.16019058227539062 	 57.15383577346802 	 68.27912616729736 	 0.5490586757659912 	 0.23391056060791016 	 
2025-08-04 19:07:52.882923 test begin: paddle.nn.functional.normalize(Tensor([2331, 21795],"float32"), )
[Prof] paddle.nn.functional.normalize 	 paddle.nn.functional.normalize(Tensor([2331, 21795],"float32"), ) 	 50804145 	 21309 	 10.044779300689697 	 10.020526885986328 	 0.096435546875 	 0.16019105911254883 	 57.34521675109863 	 68.26381278038025 	 0.5509276390075684 	 0.23378658294677734 	 
2025-08-04 19:10:20.267417 test begin: paddle.nn.functional.normalize(Tensor([99226, 512],"float32"), )
[Prof] paddle.nn.functional.normalize 	 paddle.nn.functional.normalize(Tensor([99226, 512],"float32"), ) 	 50803712 	 21309 	 10.418927431106567 	 9.954398155212402 	 0.10000348091125488 	 0.1590886116027832 	 57.7185640335083 	 68.4850664138794 	 0.5545265674591064 	 0.23458170890808105 	 
2025-08-04 19:12:50.677165 test begin: paddle.nn.functional.npair_loss(Tensor([18, 2822401],"float32"), positive=Tensor([18, 2822401],"float32"), labels=Tensor([18],"float32"), l2_reg=0.002, )
[Prof] paddle.nn.functional.npair_loss 	 paddle.nn.functional.npair_loss(Tensor([18, 2822401],"float32"), positive=Tensor([18, 2822401],"float32"), labels=Tensor([18],"float32"), l2_reg=0.002, ) 	 101606454 	 6743 	 9.930652141571045 	 9.54452133178711 	 0.06278228759765625 	 0.07211971282958984 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 19:13:32.438560 test begin: paddle.nn.functional.pad(Tensor([7573, 11, 1280],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([7573, 11, 1280],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, ) 	 106627840 	 8486 	 10.473563432693481 	 3.9869801998138428 	 1.2611100673675537 	 0.15966391563415527 	 8.365553855895996 	 6.8098156452178955 	 1.0073492527008057 	 0.2731051445007324 	 
2025-08-04 19:14:06.578313 test begin: paddle.nn.functional.pad(Tensor([7573, 8, 1678],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([7573, 8, 1678],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, ) 	 101659952 	 8486 	 9.991158246994019 	 3.7976808547973633 	 1.2031710147857666 	 0.15246367454528809 	 7.978042840957642 	 6.496983051300049 	 0.9608469009399414 	 0.26061439514160156 	 
2025-08-04 19:14:39.955398 test begin: paddle.nn.functional.pad(Tensor([7710, 11, 1280],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([7710, 11, 1280],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, ) 	 108556800 	 8486 	 10.658961534500122 	 4.048016786575317 	 1.283663272857666 	 0.16248726844787598 	 8.515865564346313 	 6.928299427032471 	 1.025578498840332 	 0.2778801918029785 	 
2025-08-04 19:15:13.723328 test begin: paddle.nn.functional.pad(Tensor([7710, 8, 1648],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([7710, 8, 1648],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, ) 	 101648640 	 8486 	 9.991177082061768 	 3.7980988025665283 	 1.2032339572906494 	 0.15240931510925293 	 7.978508234024048 	 6.493208646774292 	 0.9609756469726562 	 0.26047205924987793 	 
2025-08-04 19:15:45.956634 test begin: paddle.nn.functional.pad(Tensor([8162, 10, 1280],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([8162, 10, 1280],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, ) 	 104473600 	 8486 	 10.275064945220947 	 3.900054454803467 	 1.2373175621032715 	 0.15658068656921387 	 8.198264837265015 	 6.677258491516113 	 0.9873049259185791 	 0.26782846450805664 	 
2025-08-04 19:16:18.461613 test begin: paddle.nn.functional.pad(Tensor([8162, 8, 1557],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([8162, 8, 1557],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, ) 	 101665872 	 8486 	 9.996654272079468 	 3.804738759994507 	 1.203725814819336 	 0.15260004997253418 	 7.983154535293579 	 6.498901128768921 	 0.9614415168762207 	 0.26068592071533203 	 
2025-08-04 19:16:50.516754 test begin: paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, ) 	 101611520 	 8486 	 9.986460447311401 	 3.7801928520202637 	 1.202453851699829 	 0.22710227966308594 	 7.975701808929443 	 6.466933727264404 	 0.960623025894165 	 0.3894052505493164 	 
2025-08-04 19:17:22.718895 test begin: paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, ) 	 101611520 	 8486 	 9.986703157424927 	 3.772953510284424 	 1.2026054859161377 	 0.22715044021606445 	 7.975769758224487 	 6.467027187347412 	 0.960503339767456 	 0.3894217014312744 	 
2025-08-04 19:17:55.692133 test begin: paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, ) 	 101611520 	 8486 	 9.986943006515503 	 3.7806105613708496 	 1.2026119232177734 	 0.22717881202697754 	 7.975898027420044 	 6.468228101730347 	 0.9606022834777832 	 0.389540433883667 	 
2025-08-04 19:18:28.980129 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 508033],"float32"), Tensor([100, 508033],"float32"), -1, 1e-06, False, None, )
[Prof] paddle.nn.functional.pairwise_distance 	 paddle.nn.functional.pairwise_distance(Tensor([100, 508033],"float32"), Tensor([100, 508033],"float32"), -1, 1e-06, False, None, ) 	 101606600 	 8769 	 10.001007795333862 	 9.318761587142944 	 0.19388079643249512 	 0.27127933502197266 	 16.601678133010864 	 24.340484857559204 	 0.9675357341766357 	 0.28366875648498535 	 
2025-08-04 19:19:30.999248 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 508033],"float32"), Tensor([100, 508033],"float32"), -1, 1e-06, True, None, )
[Prof] paddle.nn.functional.pairwise_distance 	 paddle.nn.functional.pairwise_distance(Tensor([100, 508033],"float32"), Tensor([100, 508033],"float32"), -1, 1e-06, True, None, ) 	 101606600 	 8769 	 10.003135681152344 	 9.31936001777649 	 0.19390082359313965 	 0.2713441848754883 	 16.614146947860718 	 24.33992052078247 	 0.9682509899139404 	 0.28369593620300293 	 
2025-08-04 19:20:33.399704 test begin: paddle.nn.functional.pairwise_distance(Tensor([508033, 100],"float32"), Tensor([508033, 100],"float32"), -1, 1e-06, False, None, )
[Prof] paddle.nn.functional.pairwise_distance 	 paddle.nn.functional.pairwise_distance(Tensor([508033, 100],"float32"), Tensor([508033, 100],"float32"), -1, 1e-06, False, None, ) 	 101606600 	 8769 	 13.369604349136353 	 14.419677972793579 	 0.3117992877960205 	 0.5601687431335449 	 17.34937834739685 	 24.440235376358032 	 1.0110249519348145 	 0.28482842445373535 	 
2025-08-04 19:21:46.717287 test begin: paddle.nn.functional.pairwise_distance(Tensor([508033, 100],"float32"), Tensor([508033, 100],"float32"), -1, 1e-06, True, None, )
[Prof] paddle.nn.functional.pairwise_distance 	 paddle.nn.functional.pairwise_distance(Tensor([508033, 100],"float32"), Tensor([508033, 100],"float32"), -1, 1e-06, True, None, ) 	 101606600 	 8769 	 13.369610071182251 	 14.416501760482788 	 0.3118007183074951 	 0.5602076053619385 	 17.349525928497314 	 24.440450429916382 	 1.0110125541687012 	 0.28482961654663086 	 
2025-08-04 19:22:58.029987 test begin: paddle.nn.functional.pixel_shuffle(Tensor([13, 256, 128, 128],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([13, 256, 128, 128],"float32"), 2, "NCHW", None, ) 	 54525952 	 26616 	 10.80813717842102 	 9.355107307434082 	 0.41503262519836426 	 0.35923171043395996 	 10.487836599349976 	 9.125917434692383 	 0.4027719497680664 	 0.3503999710083008 	 
2025-08-04 19:23:41.199532 test begin: paddle.nn.functional.pixel_shuffle(Tensor([4, 256, 128, 388],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([4, 256, 128, 388],"float32"), 2, "NCHW", None, ) 	 50855936 	 26616 	 10.012276649475098 	 8.865553140640259 	 0.3842785358428955 	 0.3404223918914795 	 10.706741571426392 	 8.648256778717041 	 0.4111514091491699 	 0.33205509185791016 	 
2025-08-04 19:24:21.961241 test begin: paddle.nn.functional.pixel_shuffle(Tensor([4, 256, 388, 128],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([4, 256, 388, 128],"float32"), 2, "NCHW", None, ) 	 50855936 	 26616 	 10.233392715454102 	 8.881924390792847 	 0.39293646812438965 	 0.34103846549987793 	 10.632435321807861 	 8.578133583068848 	 0.40825867652893066 	 0.32938456535339355 	 
2025-08-04 19:25:03.791947 test begin: paddle.nn.functional.pixel_shuffle(Tensor([4, 776, 128, 128],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([4, 776, 128, 128],"float32"), 2, "NCHW", None, ) 	 50855936 	 26616 	 10.065454721450806 	 8.730669260025024 	 0.38649892807006836 	 0.3352344036102295 	 9.787829160690308 	 8.525289297103882 	 0.3757927417755127 	 0.32735419273376465 	 
2025-08-04 19:25:42.618976 test begin: paddle.nn.functional.pixel_shuffle(Tensor([49, 256, 64, 64],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([49, 256, 64, 64],"float32"), 2, "NCHW", None, ) 	 51380224 	 26616 	 10.304046154022217 	 9.22122073173523 	 0.3956305980682373 	 0.33683300018310547 	 9.742909669876099 	 8.719619274139404 	 0.37407350540161133 	 0.3348045349121094 	 
2025-08-04 19:26:24.304620 test begin: paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 128, 25],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 128, 25],"float32"), 2, "NCHW", None, ) 	 52428800 	 26616 	 10.26216435432434 	 9.050090312957764 	 0.39403223991394043 	 0.347454309463501 	 10.396931648254395 	 9.10334849357605 	 0.39920806884765625 	 0.34951257705688477 	 
2025-08-04 19:27:05.348175 test begin: paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 25, 128],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 25, 128],"float32"), 2, "NCHW", None, ) 	 52428800 	 26616 	 10.47797441482544 	 9.009933948516846 	 0.40233922004699707 	 0.345994234085083 	 10.100693225860596 	 8.573800086975098 	 0.3878347873687744 	 0.3292112350463867 	 
2025-08-04 19:27:45.355960 test begin: paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 49, 64],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 49, 64],"float32"), 2, "NCHW", None, ) 	 51380224 	 26616 	 10.296665668487549 	 8.952473878860474 	 0.3952610492706299 	 0.34365344047546387 	 9.779370546340942 	 8.481757879257202 	 0.3754394054412842 	 0.32565832138061523 	 
2025-08-04 19:28:24.599203 test begin: paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 64, 49],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 64, 49],"float32"), 2, "NCHW", None, ) 	 51380224 	 26616 	 10.097594976425171 	 8.80866026878357 	 0.3878140449523926 	 0.3378458023071289 	 10.04565954208374 	 8.86734652519226 	 0.38573265075683594 	 0.3404848575592041 	 
2025-08-04 19:29:04.760841 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([176401, 1, 12, 12],"float64"), 3, "NCHW", )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([176401, 1, 12, 12],"float64"), 3, "NCHW", ) 	 25401744 	 31500 	 9.984102487564087 	 9.507735013961792 	 0.3239607810974121 	 0.30842137336730957 	 9.966903924942017 	 9.516558647155762 	 0.3233909606933594 	 0.30876994132995605 	 
2025-08-04 19:29:45.635749 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([176401, 1, 12, 12],"float64"), 3, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([176401, 1, 12, 12],"float64"), 3, "NCHW", None, ) 	 25401744 	 31500 	 9.984096765518188 	 9.506550073623657 	 0.3239450454711914 	 0.3084290027618408 	 9.967466354370117 	 9.51652216911316 	 0.32346510887145996 	 0.3087449073791504 	 
2025-08-04 19:30:25.723338 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([2, 176401, 12, 12],"float32"), 3, "NCHW", )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([2, 176401, 12, 12],"float32"), 3, "NCHW", ) 	 50803488 	 31500 	 11.561860799789429 	 10.28737187385559 	 0.37522292137145996 	 0.3337886333465576 	 11.367899894714355 	 10.174623489379883 	 0.3688952922821045 	 0.33008694648742676 	 
2025-08-04 19:31:11.059184 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([2, 88201, 12, 12],"float64"), 3, "NCHW", )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([2, 88201, 12, 12],"float64"), 3, "NCHW", ) 	 25401888 	 31500 	 9.982661008834839 	 9.506576776504517 	 0.3238716125488281 	 0.3084545135498047 	 9.967178344726562 	 9.516652822494507 	 0.32332539558410645 	 0.30875158309936523 	 
2025-08-04 19:31:51.196012 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([2, 88201, 12, 12],"float64"), 3, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([2, 88201, 12, 12],"float64"), 3, "NCHW", None, ) 	 25401888 	 31500 	 9.983318090438843 	 9.506640672683716 	 0.3238961696624756 	 0.3084559440612793 	 9.967139720916748 	 9.51667332649231 	 0.32341432571411133 	 0.30878591537475586 	 
2025-08-04 19:32:31.299860 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([352801, 1, 12, 12],"float32"), 3, "NCHW", )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([352801, 1, 12, 12],"float32"), 3, "NCHW", ) 	 50803344 	 31500 	 11.544301986694336 	 10.309213399887085 	 0.3745248317718506 	 0.33377623558044434 	 11.367602348327637 	 10.174686193466187 	 0.36881279945373535 	 0.3301112651824951 	 
2025-08-04 19:33:17.154578 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([16934401, 3, 2],"float32"), Tensor([16934401, 3, 2],"bfloat16"), )
W0804 19:33:20.208818 65165 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([16934401, 3, 2],"float32"), Tensor([16934401, 3, 2],"bfloat16"), ) 	 203212812 	 6305 	 19.857751846313477 	 16.349787950515747 	 0.5356574058532715 	 0.529320478439331 	 35.609124183654785 	 30.333276510238647 	 0.9612555503845215 	 0.7024164199829102 	 
2025-08-04 19:35:03.511856 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([16934401, 3, 2],"float32"), Tensor([16934401, 3, 2],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([16934401, 3, 2],"float32"), Tensor([16934401, 3, 2],"float16"), ) 	 203212812 	 6305 	 19.857922554016113 	 16.321164846420288 	 0.535740852355957 	 0.5284290313720703 	 35.59073567390442 	 30.31227946281433 	 0.9610645771026611 	 0.7018775939941406 	 
2025-08-04 19:36:49.208080 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 12700801, 2],"float32"), Tensor([4, 12700801, 2],"bfloat16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 12700801, 2],"float32"), Tensor([4, 12700801, 2],"bfloat16"), ) 	 203212816 	 6305 	 19.85754418373108 	 16.348442316055298 	 0.5356791019439697 	 0.5290136337280273 	 35.606316328048706 	 30.333282709121704 	 0.9611837863922119 	 0.7024002075195312 	 
2025-08-04 19:38:34.819322 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 12700801, 2],"float32"), Tensor([4, 12700801, 2],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 12700801, 2],"float32"), Tensor([4, 12700801, 2],"float16"), ) 	 203212816 	 6305 	 19.866429805755615 	 16.325150728225708 	 0.5356676578521729 	 0.5284202098846436 	 35.59071230888367 	 30.312226057052612 	 0.9608902931213379 	 0.7018911838531494 	 
2025-08-04 19:40:24.121162 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 2116801],"float32"), Tensor([4, 3, 2116801],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 2116801],"float32"), Tensor([4, 3, 2116801],"float64"), ) 	 50803224 	 6305 	 10.51907753944397 	 6.741282224655151 	 0.24358725547790527 	 0.2181258201599121 	 14.334170579910278 	 13.68722653388977 	 0.3318517208099365 	 0.27739787101745605 	 
2025-08-04 19:41:11.617424 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"bfloat16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"bfloat16"), ) 	 101606424 	 6305 	 10.004618406295776 	 8.281928539276123 	 0.269838809967041 	 0.2679286003112793 	 17.911282300949097 	 15.263386964797974 	 0.48387789726257324 	 0.3534054756164551 	 
2025-08-04 19:42:07.376329 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float16"), ) 	 101606424 	 6305 	 10.005467891693115 	 8.284212827682495 	 0.2698526382446289 	 0.26775074005126953 	 17.90384817123413 	 15.251797676086426 	 0.4833800792694092 	 0.3531768321990967 	 
2025-08-04 19:43:01.906762 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float64"), ) 	 101606424 	 6305 	 20.866694450378418 	 13.25186824798584 	 0.4829270839691162 	 0.4290931224822998 	 28.466124534606934 	 27.10761332511902 	 0.65920090675354 	 0.5494611263275146 	 
2025-08-04 19:44:38.136665 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 8467201],"float32"), Tensor([4, 3, 8467201],"bfloat16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 8467201],"float32"), Tensor([4, 3, 8467201],"bfloat16"), ) 	 203212824 	 6305 	 19.865996599197388 	 16.34980845451355 	 0.5356121063232422 	 0.5291914939880371 	 35.60049867630005 	 30.3311824798584 	 0.9612374305725098 	 0.7023327350616455 	 
2025-08-04 19:46:28.873686 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 8467201],"float32"), Tensor([4, 3, 8467201],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 8467201],"float32"), Tensor([4, 3, 8467201],"float16"), ) 	 203212824 	 6305 	 19.85756230354309 	 16.321927547454834 	 0.53570556640625 	 0.5284707546234131 	 35.59063458442688 	 30.312463760375977 	 0.9606781005859375 	 0.7018532752990723 	 
2025-08-04 19:48:14.679590 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3175201, 2],"float32"), Tensor([4, 3175201, 2],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3175201, 2],"float32"), Tensor([4, 3175201, 2],"float64"), ) 	 50803216 	 6305 	 10.519526481628418 	 6.740211009979248 	 0.24353408813476562 	 0.21816205978393555 	 14.333301067352295 	 13.687441349029541 	 0.33200597763061523 	 0.2774651050567627 	 
2025-08-04 19:49:02.370685 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"bfloat16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"bfloat16"), ) 	 101606416 	 6305 	 10.00501799583435 	 8.298706531524658 	 0.2699730396270752 	 0.26817941665649414 	 17.904074668884277 	 15.262911319732666 	 0.48346447944641113 	 0.3534107208251953 	 
2025-08-04 19:49:57.837395 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float16"), ) 	 101606416 	 6305 	 10.003058195114136 	 8.263776540756226 	 0.26984596252441406 	 0.2675323486328125 	 17.903162717819214 	 15.251700401306152 	 0.48347949981689453 	 0.3531656265258789 	 
2025-08-04 19:50:52.381257 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float64"), ) 	 101606416 	 6305 	 20.85460090637207 	 13.261299133300781 	 0.48288965225219727 	 0.42943429946899414 	 28.476399660110474 	 27.108022212982178 	 0.6594774723052979 	 0.5494091510772705 	 
2025-08-04 19:52:24.130136 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4233601, 3, 2],"float32"), Tensor([4233601, 3, 2],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4233601, 3, 2],"float32"), Tensor([4233601, 3, 2],"float64"), ) 	 50803212 	 6305 	 10.520069122314453 	 6.745248079299927 	 0.24361753463745117 	 0.21840929985046387 	 14.326677560806274 	 13.686502695083618 	 0.3317451477050781 	 0.27739787101745605 	 
2025-08-04 19:53:10.402826 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"bfloat16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"bfloat16"), ) 	 101606412 	 6305 	 10.004533767700195 	 8.276842594146729 	 0.26990199089050293 	 0.2679619789123535 	 17.91531801223755 	 15.264717102050781 	 0.4838709831237793 	 0.353435754776001 	 
2025-08-04 19:54:05.822000 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float16"), ) 	 101606412 	 6305 	 10.004575252532959 	 8.274657487869263 	 0.26987266540527344 	 0.2675809860229492 	 17.90984559059143 	 15.254552364349365 	 0.4834117889404297 	 0.35318446159362793 	 
2025-08-04 19:55:02.141782 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float64"), ) 	 101606412 	 6305 	 20.855427742004395 	 13.25376296043396 	 0.482973575592041 	 0.4292020797729492 	 28.475885152816772 	 27.105802297592163 	 0.6593589782714844 	 0.54939866065979 	 
2025-08-04 19:56:33.849678 test begin: paddle.nn.functional.prelu(Tensor([104, 128, 56, 69],"float32"), Tensor([128],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([104, 128, 56, 69],"float32"), Tensor([128],"float32"), data_format="NCHW", ) 	 51437696 	 33188 	 10.10671591758728 	 10.368440389633179 	 0.3111553192138672 	 0.31929540634155273 	 36.54262566566467 	 26.448731184005737 	 0.3745448589324951 	 0.27116870880126953 	 
2025-08-04 19:57:59.331329 test begin: paddle.nn.functional.prelu(Tensor([104, 128, 69, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([104, 128, 69, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", ) 	 51437696 	 33188 	 10.645032167434692 	 10.366284370422363 	 0.31113266944885254 	 0.31922364234924316 	 36.526994705200195 	 26.426767110824585 	 0.37432312965393066 	 0.27094149589538574 	 
2025-08-04 19:59:25.590742 test begin: paddle.nn.functional.prelu(Tensor([104, 156, 56, 56],"float32"), Tensor([156],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([104, 156, 56, 56],"float32"), Tensor([156],"float32"), data_format="NCHW", ) 	 50878620 	 33188 	 9.991546869277954 	 10.271245241165161 	 0.30776214599609375 	 0.31582117080688477 	 35.975651264190674 	 26.334662437438965 	 0.3688066005706787 	 0.2700841426849365 	 
2025-08-04 20:00:52.777639 test begin: paddle.nn.functional.prelu(Tensor([127, 128, 56, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([127, 128, 56, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", ) 	 50978944 	 33188 	 10.013017177581787 	 10.285680294036865 	 0.3083174228668213 	 0.31636571884155273 	 36.12630534172058 	 26.144254684448242 	 0.37029194831848145 	 0.26802778244018555 	 
2025-08-04 20:02:18.737831 test begin: paddle.nn.functional.prelu(Tensor([128, 128, 56, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([128, 128, 56, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", ) 	 51380352 	 33188 	 10.094661235809326 	 10.35626220703125 	 0.31082701683044434 	 0.3189113140106201 	 36.347251892089844 	 26.350539445877075 	 0.3725893497467041 	 0.27014780044555664 	 
2025-08-04 20:03:45.929878 test begin: paddle.nn.functional.prelu(Tensor([128, 256, 28, 56],"float32"), Tensor([256],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([128, 256, 28, 56],"float32"), Tensor([256],"float32"), data_format="NCHW", ) 	 51380480 	 33188 	 10.098902702331543 	 10.359328746795654 	 0.310809850692749 	 0.31892943382263184 	 36.33602952957153 	 26.348081350326538 	 0.37242937088012695 	 0.2700643539428711 	 
2025-08-04 20:05:10.813156 test begin: paddle.nn.functional.prelu(Tensor([128, 256, 56, 28],"float32"), Tensor([256],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([128, 256, 56, 28],"float32"), Tensor([256],"float32"), data_format="NCHW", ) 	 51380480 	 33188 	 10.092966079711914 	 10.355537414550781 	 0.3107728958129883 	 0.31887292861938477 	 36.320608615875244 	 26.36886692047119 	 0.37229084968566895 	 0.27033352851867676 	 
2025-08-04 20:06:39.277828 test begin: paddle.nn.functional.prelu(Tensor([128, 507, 28, 28],"float32"), Tensor([507],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([128, 507, 28, 28],"float32"), Tensor([507],"float32"), data_format="NCHW", ) 	 50878971 	 33188 	 10.623919486999512 	 10.256999492645264 	 0.3077833652496338 	 0.3159060478210449 	 35.71007466316223 	 25.909921646118164 	 0.3660166263580322 	 0.39889001846313477 	 
2025-08-04 20:08:05.001841 test begin: paddle.nn.functional.prelu(Tensor([254, 256, 28, 28],"float32"), Tensor([256],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([254, 256, 28, 28],"float32"), Tensor([256],"float32"), data_format="NCHW", ) 	 50979072 	 33188 	 10.013720035552979 	 10.273915529251099 	 0.3083634376525879 	 0.3163642883300781 	 36.12413311004639 	 26.210426092147827 	 0.37026286125183105 	 0.2686760425567627 	 
2025-08-04 20:09:33.004023 test begin: paddle.nn.functional.relu(Tensor([10, 128, 480, 83],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([10, 128, 480, 83],"float32"), None, ) 	 50995200 	 33682 	 9.996869087219238 	 10.064356327056885 	 0.30333542823791504 	 0.3053724765777588 	 15.223453760147095 	 15.098793029785156 	 0.4619622230529785 	 0.45810556411743164 	 
2025-08-04 20:10:25.130742 test begin: paddle.nn.functional.relu(Tensor([10, 128, 83, 480],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([10, 128, 83, 480],"float32"), None, ) 	 50995200 	 33682 	 9.996662616729736 	 10.06424880027771 	 0.3033599853515625 	 0.30539536476135254 	 15.223874807357788 	 15.097558736801147 	 0.4619271755218506 	 0.45809102058410645 	 
2025-08-04 20:11:17.632912 test begin: paddle.nn.functional.relu(Tensor([10, 23, 480, 480],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([10, 23, 480, 480],"float32"), None, ) 	 52992000 	 33682 	 10.379612684249878 	 10.455876588821411 	 0.31496548652648926 	 0.3171546459197998 	 15.808632373809814 	 15.682764053344727 	 0.479672908782959 	 0.4758641719818115 	 
2025-08-04 20:12:11.734081 test begin: paddle.nn.functional.relu(Tensor([2, 128, 480, 480],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([2, 128, 480, 480],"float32"), None, ) 	 58982400 	 33682 	 11.541412353515625 	 11.633610486984253 	 0.3502373695373535 	 0.35233306884765625 	 17.57896113395691 	 17.439693689346313 	 0.5334186553955078 	 0.5291152000427246 	 
2025-08-04 20:13:13.069293 test begin: paddle.nn.functional.relu(Tensor([2, 256, 352, 352],"float32"), )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([2, 256, 352, 352],"float32"), ) 	 63438848 	 33682 	 12.406019449234009 	 12.477400302886963 	 0.37642908096313477 	 0.37859034538269043 	 18.894386768341064 	 18.74065852165222 	 0.5732316970825195 	 0.5686569213867188 	 
2025-08-04 20:14:17.679301 test begin: paddle.nn.functional.relu(Tensor([64, 64, 112, 112],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([64, 64, 112, 112],"float32"), None, ) 	 51380224 	 33682 	 10.073073625564575 	 10.143478155136108 	 0.30561399459838867 	 0.30768799781799316 	 15.33534026145935 	 15.211745738983154 	 0.465350866317749 	 0.4615452289581299 	 
2025-08-04 20:15:10.151899 test begin: paddle.nn.functional.relu(Tensor([640, 64, 112, 12],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([640, 64, 112, 12],"float32"), None, ) 	 55050240 	 33682 	 10.778488397598267 	 11.983668088912964 	 0.32705235481262207 	 0.3291802406311035 	 16.4191792011261 	 16.28129506111145 	 0.49817705154418945 	 0.4939746856689453 	 
2025-08-04 20:16:10.228822 test begin: paddle.nn.functional.relu(Tensor([640, 64, 12, 112],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([640, 64, 12, 112],"float32"), None, ) 	 55050240 	 33682 	 10.779905080795288 	 10.853294849395752 	 0.32706665992736816 	 0.3291168212890625 	 16.419102668762207 	 16.281267166137695 	 0.4982285499572754 	 0.49399638175964355 	 
2025-08-04 20:17:07.276679 test begin: paddle.nn.functional.relu(Tensor([640, 7, 112, 112],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([640, 7, 112, 112],"float32"), None, ) 	 56197120 	 33682 	 11.002457618713379 	 11.09244680404663 	 0.3338284492492676 	 0.33595824241638184 	 16.761691331863403 	 16.623008489608765 	 0.5085961818695068 	 0.5043313503265381 	 
2025-08-04 20:18:06.770577 test begin: paddle.nn.functional.relu(Tensor([8, 256, 352, 71],"float32"), )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([8, 256, 352, 71],"float32"), ) 	 51183616 	 33682 	 10.032088279724121 	 10.105406284332275 	 0.3044114112854004 	 0.30647969245910645 	 15.276946544647217 	 15.151807069778442 	 0.46349048614501953 	 0.459716796875 	 
2025-08-04 20:18:59.530183 test begin: paddle.nn.functional.relu(Tensor([8, 256, 71, 352],"float32"), )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([8, 256, 71, 352],"float32"), ) 	 51183616 	 33682 	 10.032297372817993 	 10.101034164428711 	 0.30443739891052246 	 0.3064899444580078 	 15.277338027954102 	 15.152249336242676 	 0.46360015869140625 	 0.4597642421722412 	 
2025-08-04 20:19:51.892656 test begin: paddle.nn.functional.relu(Tensor([8, 52, 352, 352],"float32"), )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([8, 52, 352, 352],"float32"), ) 	 51544064 	 33682 	 10.102579832077026 	 10.170682668685913 	 0.3065450191497803 	 0.3085606098175049 	 15.379913568496704 	 15.255014419555664 	 0.4666447639465332 	 0.4628317356109619 	 
2025-08-04 20:20:45.239291 test begin: paddle.nn.functional.relu6(Tensor([128, 144, 112, 25],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([128, 144, 112, 25],"float32"), ) 	 51609600 	 33340 	 10.014948844909668 	 11.442825317382812 	 0.30689239501953125 	 0.3089761734008789 	 15.241678476333618 	 15.122029542922974 	 0.4672694206237793 	 0.46354079246520996 	 
2025-08-04 20:21:42.721722 test begin: paddle.nn.functional.relu6(Tensor([128, 144, 25, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([128, 144, 25, 112],"float32"), ) 	 51609600 	 33340 	 10.014991044998169 	 10.079368829727173 	 0.3068668842315674 	 0.3089735507965088 	 15.241714239120483 	 15.121900796890259 	 0.46723031997680664 	 0.4635484218597412 	 
2025-08-04 20:22:34.927807 test begin: paddle.nn.functional.relu6(Tensor([128, 192, 112, 19],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([128, 192, 112, 19],"float32"), ) 	 52297728 	 33340 	 10.658658981323242 	 10.212718963623047 	 0.3109152317047119 	 0.3130481243133545 	 15.445529699325562 	 15.317196369171143 	 0.47354602813720703 	 0.4695289134979248 	 
2025-08-04 20:23:29.133962 test begin: paddle.nn.functional.relu6(Tensor([128, 192, 19, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([128, 192, 19, 112],"float32"), ) 	 52297728 	 33340 	 10.142647504806519 	 10.212580442428589 	 0.3108839988708496 	 0.31301283836364746 	 15.447393894195557 	 15.31881856918335 	 0.4735720157623291 	 0.4695909023284912 	 
2025-08-04 20:24:22.206796 test begin: paddle.nn.functional.relu6(Tensor([128, 32, 112, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([128, 32, 112, 112],"float32"), ) 	 51380224 	 33340 	 9.970744132995605 	 10.062035322189331 	 0.30555200576782227 	 0.3076660633087158 	 15.180023431777954 	 15.060200452804565 	 0.4652876853942871 	 0.46161985397338867 	 
2025-08-04 20:25:15.466448 test begin: paddle.nn.functional.relu6(Tensor([22, 192, 112, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([22, 192, 112, 112],"float32"), ) 	 52985856 	 33340 	 10.272095918655396 	 10.360533714294434 	 0.3148372173309326 	 0.3171093463897705 	 15.644929885864258 	 15.521106481552124 	 0.4795057773590088 	 0.4757671356201172 	 
2025-08-04 20:26:09.844686 test begin: paddle.nn.functional.relu6(Tensor([256, 16, 112, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([256, 16, 112, 112],"float32"), ) 	 51380224 	 33340 	 9.969754457473755 	 11.427196741104126 	 0.3056221008300781 	 0.3077123165130615 	 15.177759170532227 	 15.058230876922607 	 0.46518874168395996 	 0.4616236686706543 	 
2025-08-04 20:27:05.024233 test begin: paddle.nn.functional.relu6(Tensor([256, 96, 112, 19],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([256, 96, 112, 19],"float32"), ) 	 52297728 	 33340 	 11.234110116958618 	 10.212759494781494 	 0.31090474128723145 	 0.31308531761169434 	 15.44630241394043 	 15.317874193191528 	 0.4736158847808838 	 0.4695281982421875 	 
2025-08-04 20:28:00.566155 test begin: paddle.nn.functional.relu6(Tensor([256, 96, 19, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([256, 96, 19, 112],"float32"), ) 	 52297728 	 33340 	 10.142590045928955 	 10.212702989578247 	 0.31090283393859863 	 0.3130652904510498 	 15.44666576385498 	 15.318214654922485 	 0.47334861755371094 	 0.469515323638916 	 
2025-08-04 20:28:53.480547 test begin: paddle.nn.functional.relu6(Tensor([29, 144, 112, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([29, 144, 112, 112],"float32"), ) 	 52383744 	 33340 	 10.159294128417969 	 10.228883504867554 	 0.3113529682159424 	 0.3135714530944824 	 15.47199273109436 	 15.342360258102417 	 0.4742922782897949 	 0.47032690048217773 	 
2025-08-04 20:29:46.471319 test begin: paddle.nn.functional.relu6(Tensor([43, 96, 112, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([43, 96, 112, 112],"float32"), ) 	 51781632 	 33340 	 10.044603109359741 	 10.111895561218262 	 0.3079080581665039 	 0.3099508285522461 	 15.293486833572388 	 15.172313690185547 	 0.4688270092010498 	 0.4650893211364746 	 
2025-08-04 20:30:39.399824 test begin: paddle.nn.functional.rrelu(Tensor([1, 2, 3, 4233601],"float64"), 0.05, 0.25, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([1, 2, 3, 4233601],"float64"), 0.05, 0.25, training=False, ) 	 25401606 	 20818 	 10.014090776443481 	 6.21466326713562 	 0.490891695022583 	 0.30510449409484863 	 12.168323040008545 	 9.225441932678223 	 0.5973906517028809 	 0.4528672695159912 	 
2025-08-04 20:31:18.375996 test begin: paddle.nn.functional.rrelu(Tensor([1, 2, 3175201, 4],"float64"), 0.05, 0.25, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([1, 2, 3175201, 4],"float64"), 0.05, 0.25, training=False, ) 	 25401608 	 20818 	 9.999305725097656 	 6.463433265686035 	 0.4908866882324219 	 0.3050711154937744 	 12.1681809425354 	 9.225486993789673 	 0.5973591804504395 	 0.45290660858154297 	 
2025-08-04 20:31:59.556691 test begin: paddle.nn.functional.rrelu(Tensor([1, 2116801, 3, 4],"float64"), 0.05, 0.25, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([1, 2116801, 3, 4],"float64"), 0.05, 0.25, training=False, ) 	 25401612 	 20818 	 10.001926183700562 	 6.214811086654663 	 0.49086761474609375 	 0.30516552925109863 	 12.168062210083008 	 9.225946187973022 	 0.5973546504974365 	 0.45285820960998535 	 
2025-08-04 20:32:39.616033 test begin: paddle.nn.functional.rrelu(Tensor([1058401, 2, 3, 4],"float64"), 0.05, 0.25, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([1058401, 2, 3, 4],"float64"), 0.05, 0.25, training=False, ) 	 25401624 	 20818 	 11.261256456375122 	 6.226308822631836 	 0.49083709716796875 	 0.30517101287841797 	 12.168769121170044 	 9.22642469406128 	 0.5973637104034424 	 0.45293235778808594 	 
2025-08-04 20:33:23.259579 test begin: paddle.nn.functional.rrelu(Tensor([2, 1270081, 4, 5],"float32"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 1270081, 4, 5],"float32"), 0.1, 0.3, training=False, ) 	 50803240 	 20818 	 10.06957745552063 	 6.1979310512542725 	 0.49419498443603516 	 0.30423784255981445 	 12.5711510181427 	 9.298341274261475 	 0.6171050071716309 	 0.45647168159484863 	 
2025-08-04 20:34:03.255577 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 1693441, 5],"float32"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 3, 1693441, 5],"float32"), 0.1, 0.3, training=False, ) 	 50803230 	 20818 	 10.066875219345093 	 6.197835445404053 	 0.49417543411254883 	 0.30428099632263184 	 12.569738864898682 	 9.29833459854126 	 0.6170926094055176 	 0.4565300941467285 	 
2025-08-04 20:34:44.818511 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 1058401],"float64"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 3, 4, 1058401],"float64"), 0.1, 0.3, training=False, ) 	 25401624 	 20818 	 10.008373737335205 	 6.214868783950806 	 0.4908912181854248 	 0.3050577640533447 	 12.168547868728638 	 9.226369857788086 	 0.5974061489105225 	 0.4529416561126709 	 
2025-08-04 20:35:26.145545 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 2116801],"float32"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 3, 4, 2116801],"float32"), 0.1, 0.3, training=False, ) 	 50803224 	 20818 	 10.06689453125 	 6.210675001144409 	 0.4942028522491455 	 0.3042612075805664 	 12.570068836212158 	 9.298022031784058 	 0.6170523166656494 	 0.4564673900604248 	 
2025-08-04 20:36:06.033124 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 846721, 5],"float64"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 3, 846721, 5],"float64"), 0.1, 0.3, training=False, ) 	 25401630 	 20818 	 9.99921441078186 	 6.232849836349487 	 0.49094271659851074 	 0.30512380599975586 	 12.168696165084839 	 9.22640323638916 	 0.5973930358886719 	 0.4529447555541992 	 
2025-08-04 20:36:46.147255 test begin: paddle.nn.functional.rrelu(Tensor([2, 635041, 4, 5],"float64"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 635041, 4, 5],"float64"), 0.1, 0.3, training=False, ) 	 25401640 	 20818 	 9.996706008911133 	 6.215001821517944 	 0.49056458473205566 	 0.30503129959106445 	 12.168152570724487 	 9.226462364196777 	 0.597365140914917 	 0.45293402671813965 	 
2025-08-04 20:37:24.889248 test begin: paddle.nn.functional.rrelu(Tensor([423361, 3, 4, 5],"float64"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([423361, 3, 4, 5],"float64"), 0.1, 0.3, training=False, ) 	 25401660 	 20818 	 9.992839574813843 	 6.235917091369629 	 0.49062156677246094 	 0.3050549030303955 	 12.168232202529907 	 9.22648286819458 	 0.5974059104919434 	 0.45296716690063477 	 
2025-08-04 20:38:04.809380 test begin: paddle.nn.functional.rrelu(Tensor([846721, 3, 4, 5],"float32"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([846721, 3, 4, 5],"float32"), 0.1, 0.3, training=False, ) 	 50803260 	 20818 	 10.068798065185547 	 6.198337078094482 	 0.4942512512207031 	 0.3042490482330322 	 12.570011138916016 	 9.298209190368652 	 0.6171407699584961 	 0.4564526081085205 	 
2025-08-04 20:38:44.709200 test begin: paddle.nn.functional.selu(Tensor([101607, 5, 5, 10],"float64"), 1.5, 2.0, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([101607, 5, 5, 10],"float64"), 1.5, 2.0, ) 	 25401750 	 33360 	 9.999478340148926 	 60.918039083480835 	 0.30619311332702637 	 0.31114792823791504 	 14.932961463928223 	 70.86466979980469 	 0.4575772285461426 	 0.2716183662414551 	 
2025-08-04 20:41:25.958867 test begin: paddle.nn.functional.selu(Tensor([101607, 5, 5, 10],"float64"), 1.5, 2.0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([101607, 5, 5, 10],"float64"), 1.5, 2.0, None, ) 	 25401750 	 33360 	 9.995286464691162 	 60.92412328720093 	 0.30622053146362305 	 0.311229944229126 	 14.934194087982178 	 70.86621809005737 	 0.4574432373046875 	 0.27158141136169434 	 
2025-08-04 20:44:04.162974 test begin: paddle.nn.functional.selu(Tensor([2822401, 3, 3],"float64"), 1.0507009873554805, 0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([2822401, 3, 3],"float64"), 1.0507009873554805, 0, None, ) 	 25401609 	 33360 	 9.997344493865967 	 60.91591286659241 	 0.3062856197357178 	 0.31113719940185547 	 14.933132648468018 	 70.8642520904541 	 0.45764708518981934 	 0.27158331871032715 	 
2025-08-04 20:46:42.893870 test begin: paddle.nn.functional.selu(Tensor([3, 169345, 5, 10],"float64"), 1.5, 2.0, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 169345, 5, 10],"float64"), 1.5, 2.0, ) 	 25401750 	 33360 	 9.997694253921509 	 60.91675305366516 	 0.30622076988220215 	 0.31118345260620117 	 14.933823823928833 	 70.86492037773132 	 0.4576289653778076 	 0.27162933349609375 	 
2025-08-04 20:49:20.772104 test begin: paddle.nn.functional.selu(Tensor([3, 169345, 5, 10],"float64"), 1.5, 2.0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 169345, 5, 10],"float64"), 1.5, 2.0, None, ) 	 25401750 	 33360 	 9.995206117630005 	 60.91807007789612 	 0.3062279224395752 	 0.31127047538757324 	 14.93230938911438 	 70.86433577537537 	 0.45740413665771484 	 0.27158141136169434 	 
2025-08-04 20:52:02.674562 test begin: paddle.nn.functional.selu(Tensor([3, 2822401, 3],"float64"), 1.0507009873554805, 0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 2822401, 3],"float64"), 1.0507009873554805, 0, None, ) 	 25401609 	 33360 	 10.00101113319397 	 60.91570782661438 	 0.3062772750854492 	 0.31121397018432617 	 14.93325161933899 	 70.86445307731628 	 0.45755505561828613 	 0.2715473175048828 	 
2025-08-04 20:54:43.351589 test begin: paddle.nn.functional.selu(Tensor([3, 3, 2822401],"float64"), 1.0507009873554805, 0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 3, 2822401],"float64"), 1.0507009873554805, 0, None, ) 	 25401609 	 33360 	 10.00165319442749 	 60.914660930633545 	 0.30628323554992676 	 0.3112008571624756 	 14.934204578399658 	 70.86262965202332 	 0.45754003524780273 	 0.2715158462524414 	 
2025-08-04 20:57:21.238083 test begin: paddle.nn.functional.selu(Tensor([3, 5, 169345, 10],"float64"), 1.5, 2.0, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 5, 169345, 10],"float64"), 1.5, 2.0, ) 	 25401750 	 33360 	 9.995331525802612 	 60.91913390159607 	 0.30620312690734863 	 0.3111860752105713 	 14.933639526367188 	 70.86512398719788 	 0.45733642578125 	 0.27159905433654785 	 
2025-08-04 20:59:59.105637 test begin: paddle.nn.functional.selu(Tensor([3, 5, 169345, 10],"float64"), 1.5, 2.0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 5, 169345, 10],"float64"), 1.5, 2.0, None, ) 	 25401750 	 33360 	 9.995377540588379 	 60.91752886772156 	 0.30620360374450684 	 0.3110792636871338 	 14.933024406433105 	 70.86430311203003 	 0.4573988914489746 	 0.2716362476348877 	 
2025-08-04 21:02:37.394059 test begin: paddle.nn.functional.selu(Tensor([3, 5, 5, 338689],"float64"), 1.5, 2.0, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 5, 5, 338689],"float64"), 1.5, 2.0, ) 	 25401675 	 33360 	 10.843332529067993 	 60.9151291847229 	 0.30684351921081543 	 0.3111698627471924 	 14.933568954467773 	 70.86332416534424 	 0.45743799209594727 	 0.2715747356414795 	 
2025-08-04 21:05:17.272165 test begin: paddle.nn.functional.selu(Tensor([3, 5, 5, 338689],"float64"), 1.5, 2.0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 5, 5, 338689],"float64"), 1.5, 2.0, None, ) 	 25401675 	 33360 	 10.015934944152832 	 60.932326793670654 	 0.30684328079223633 	 0.3112165927886963 	 14.932662725448608 	 70.86108589172363 	 0.4575660228729248 	 0.27155399322509766 	 
2025-08-04 21:07:56.716807 test begin: paddle.nn.functional.sequence_mask(Tensor([2, 2, 3, 3, 705601],"float64"), maxlen=5, dtype=type(numpy.int32), )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([2, 2, 3, 3, 705601],"float64"), maxlen=5, dtype=type(numpy.int32), ) 	 25401636 	 12839 	 9.986391305923462 	 19.015942573547363 	 0.7949526309967041 	 0.5052700042724609 	 None 	 None 	 None 	 None 	 combined
2025-08-04 21:08:26.292294 test begin: paddle.nn.functional.sequence_mask(Tensor([2, 2, 3, 705601, 3],"float64"), maxlen=5, dtype=type(numpy.int32), )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([2, 2, 3, 705601, 3],"float64"), maxlen=5, dtype=type(numpy.int32), ) 	 25401636 	 12839 	 9.986272811889648 	 19.050684452056885 	 0.7949166297912598 	 0.5050818920135498 	 None 	 None 	 None 	 None 	 combined
2025-08-04 21:08:57.140233 test begin: paddle.nn.functional.sequence_mask(Tensor([2, 2, 705601, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([2, 2, 705601, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), ) 	 25401636 	 12839 	 9.986445903778076 	 19.028576135635376 	 0.7949404716491699 	 0.5050694942474365 	 None 	 None 	 None 	 None 	 combined
2025-08-04 21:09:26.735827 test begin: paddle.nn.functional.sequence_mask(Tensor([2, 470401, 3, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([2, 470401, 3, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), ) 	 25401654 	 12839 	 9.986843347549438 	 19.035094022750854 	 0.7949626445770264 	 0.5050795078277588 	 None 	 None 	 None 	 None 	 combined
2025-08-04 21:09:58.482068 test begin: paddle.nn.functional.sequence_mask(Tensor([470401, 2, 3, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([470401, 2, 3, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), ) 	 25401654 	 12839 	 9.993895053863525 	 19.018633127212524 	 0.7950112819671631 	 0.5051040649414062 	 None 	 None 	 None 	 None 	 combined
2025-08-04 21:10:28.085777 test begin: paddle.nn.functional.sequence_mask(Tensor([50803201],"int32"), maxlen=4, dtype="float32", )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([50803201],"int32"), maxlen=4, dtype="float32", ) 	 50803201 	 12839 	 15.533571481704712 	 31.127597093582153 	 1.2365622520446777 	 0.8264501094818115 	 None 	 None 	 None 	 None 	 combined
2025-08-04 21:11:19.017073 test begin: paddle.nn.functional.sigmoid(Tensor([10, 32, 400, 400],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([10, 32, 400, 400],"float32"), ) 	 51200000 	 33850 	 10.058599710464478 	 10.181199550628662 	 0.30367612838745117 	 0.3071568012237549 	 15.357499837875366 	 15.232159614562988 	 0.46367430686950684 	 0.4598250389099121 	 
2025-08-04 21:12:11.844416 test begin: paddle.nn.functional.sigmoid(Tensor([364, 304, 460],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([364, 304, 460],"float32"), ) 	 50901760 	 33850 	 10.538354635238647 	 11.941446304321289 	 0.30205345153808594 	 0.30538058280944824 	 15.269552230834961 	 15.144538164138794 	 0.461017370223999 	 0.45722126960754395 	 
2025-08-04 21:13:08.382232 test begin: paddle.nn.functional.sigmoid(Tensor([364, 416, 336],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([364, 416, 336],"float32"), ) 	 50878464 	 33850 	 9.996106624603271 	 10.12551760673523 	 0.30180883407592773 	 0.30525827407836914 	 15.26193618774414 	 15.135931015014648 	 0.46071672439575195 	 0.4569988250732422 	 
2025-08-04 21:14:04.333870 test begin: paddle.nn.functional.sigmoid(Tensor([372, 304, 450],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([372, 304, 450],"float32"), ) 	 50889600 	 33850 	 10.001565933227539 	 10.116383075714111 	 0.30196619033813477 	 0.30530786514282227 	 15.265968799591064 	 15.140815734863281 	 0.4608571529388428 	 0.45710110664367676 	 
2025-08-04 21:14:56.582886 test begin: paddle.nn.functional.sigmoid(Tensor([372, 407, 336],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([372, 407, 336],"float32"), ) 	 50871744 	 33850 	 9.995580434799194 	 10.10802674293518 	 0.30178117752075195 	 0.30518436431884766 	 15.261442422866821 	 15.135738849639893 	 0.46081042289733887 	 0.45692014694213867 	 
2025-08-04 21:15:48.830829 test begin: paddle.nn.functional.sigmoid(Tensor([498, 304, 336],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([498, 304, 336],"float32"), ) 	 50867712 	 33850 	 10.53424596786499 	 10.107462406158447 	 0.30184507369995117 	 0.30521059036254883 	 15.259810209274292 	 15.13449478149414 	 0.4607369899749756 	 0.45696139335632324 	 
2025-08-04 21:16:42.750451 test begin: paddle.nn.functional.sigmoid(Tensor([8, 32, 400, 497],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([8, 32, 400, 497],"float32"), ) 	 50892800 	 33850 	 10.000144720077515 	 10.112196207046509 	 0.30185461044311523 	 0.30532360076904297 	 15.26685905456543 	 15.141563177108765 	 0.46100306510925293 	 0.45713186264038086 	 
2025-08-04 21:17:35.029959 test begin: paddle.nn.functional.sigmoid(Tensor([8, 32, 497, 400],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([8, 32, 497, 400],"float32"), ) 	 50892800 	 33850 	 10.939878463745117 	 10.112191200256348 	 0.3018612861633301 	 0.30533742904663086 	 15.266547203063965 	 15.141633033752441 	 0.46097874641418457 	 0.45712995529174805 	 
2025-08-04 21:18:28.674854 test begin: paddle.nn.functional.sigmoid(Tensor([8, 40, 400, 400],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([8, 40, 400, 400],"float32"), ) 	 51200000 	 33850 	 10.057301998138428 	 10.171228408813477 	 0.30362868309020996 	 0.30707526206970215 	 15.356866359710693 	 15.231656551361084 	 0.46367812156677246 	 0.45993781089782715 	 
2025-08-04 21:19:22.542311 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803248 	 1450 	 10.005029439926147 	 8.179195880889893 	 0.0006973743438720703 	 0.3033750057220459 	 13.425255298614502 	 13.293832540512085 	 0.411670446395874 	 0.37473535537719727 	 combined
2025-08-04 21:20:08.510098 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803249 	 1450 	 10.439464807510376 	 8.616100311279297 	 0.0009765625 	 0.3032341003417969 	 14.523880004882812 	 15.89959979057312 	 0.39380478858947754 	 0.3504188060760498 	 combined
2025-08-04 21:20:59.126354 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", ) 	 50803249 	 1450 	 10.43847131729126 	 8.615532636642456 	 0.0009737014770507812 	 0.30324721336364746 	 14.51646113395691 	 15.902065515518188 	 0.39355945587158203 	 0.3504600524902344 	 combined
2025-08-04 21:21:50.643930 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803320 	 1450 	 10.00592589378357 	 8.17774486541748 	 0.0006923675537109375 	 0.3033428192138672 	 13.426635503768921 	 13.293874502182007 	 0.41170763969421387 	 0.3747825622558594 	 combined
2025-08-04 21:22:38.308851 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803321 	 1450 	 10.449084758758545 	 8.61501145362854 	 0.0009543895721435547 	 0.3031589984893799 	 14.519855499267578 	 15.900279521942139 	 0.39363741874694824 	 0.35050058364868164 	 combined
2025-08-04 21:23:31.166827 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", ) 	 50803321 	 1450 	 10.435341596603394 	 8.634187459945679 	 0.000978708267211914 	 0.30319833755493164 	 14.521191596984863 	 15.89963412284851 	 0.39368462562561035 	 0.35042548179626465 	 combined
2025-08-04 21:24:22.738537 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803360 	 1450 	 10.007324695587158 	 8.178982496261597 	 0.0006909370422363281 	 0.3033897876739502 	 13.423929214477539 	 13.295804023742676 	 0.41162586212158203 	 0.37481069564819336 	 combined
2025-08-04 21:25:08.704449 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803361 	 1450 	 10.435580492019653 	 8.615990400314331 	 0.0009760856628417969 	 0.30316615104675293 	 14.52227520942688 	 15.900029420852661 	 0.3937492370605469 	 0.35036730766296387 	 combined
2025-08-04 21:25:59.230970 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", ) 	 50803361 	 1450 	 10.43632459640503 	 8.615262985229492 	 0.0009763240814208984 	 0.3031930923461914 	 14.51872181892395 	 15.90074110031128 	 0.39362049102783203 	 0.35047483444213867 	 combined
2025-08-04 21:26:50.447481 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803440 	 1450 	 10.001018285751343 	 8.184364795684814 	 0.0006864070892333984 	 0.3033573627471924 	 13.424593448638916 	 13.29398775100708 	 0.4116854667663574 	 0.37486982345581055 	 combined
2025-08-04 21:27:39.260402 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803441 	 1450 	 10.439528465270996 	 8.615259170532227 	 0.0009469985961914062 	 0.3032217025756836 	 14.508074522018433 	 15.900320768356323 	 0.39332127571105957 	 0.3504362106323242 	 combined
2025-08-04 21:28:31.585634 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", ) 	 50803441 	 1450 	 10.430774927139282 	 8.615230083465576 	 0.0009756088256835938 	 0.30319714546203613 	 14.508849382400513 	 15.899943351745605 	 0.39331936836242676 	 0.35047197341918945 	 combined
2025-08-04 21:29:22.081204 test begin: paddle.nn.functional.silu(Tensor([128, 128, 128, 25],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 128, 128, 25],"float32"), None, ) 	 52428800 	 33834 	 10.30046820640564 	 10.39936113357544 	 0.31109118461608887 	 0.31414222717285156 	 15.719498872756958 	 15.709763765335083 	 0.4749271869659424 	 0.47448253631591797 	 
2025-08-04 21:30:16.386159 test begin: paddle.nn.functional.silu(Tensor([128, 128, 25, 128],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 128, 25, 128],"float32"), None, ) 	 52428800 	 33834 	 10.29717469215393 	 10.405783891677856 	 0.31098341941833496 	 0.31414031982421875 	 15.71947979927063 	 15.709819555282593 	 0.47482800483703613 	 0.47446465492248535 	 
2025-08-04 21:31:13.205236 test begin: paddle.nn.functional.silu(Tensor([128, 25, 128, 128],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 25, 128, 128],"float32"), None, ) 	 52428800 	 33834 	 10.299762725830078 	 10.42011046409607 	 0.3110077381134033 	 0.31413936614990234 	 15.72082257270813 	 15.709558486938477 	 0.474855899810791 	 0.4745333194732666 	 
2025-08-04 21:32:08.829530 test begin: paddle.nn.functional.silu(Tensor([128, 256, 25, 64],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 256, 25, 64],"float32"), None, ) 	 52428800 	 33834 	 10.296517133712769 	 10.419497013092041 	 0.31102991104125977 	 0.31413960456848145 	 15.718945026397705 	 15.709349393844604 	 0.4748067855834961 	 0.4745011329650879 	 
2025-08-04 21:33:05.446830 test begin: paddle.nn.functional.silu(Tensor([128, 256, 64, 25],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 256, 64, 25],"float32"), None, ) 	 52428800 	 33834 	 10.296111583709717 	 10.3995361328125 	 0.3110368251800537 	 0.31413865089416504 	 15.719177484512329 	 15.709612369537354 	 0.47476840019226074 	 0.474581241607666 	 
2025-08-04 21:33:59.334203 test begin: paddle.nn.functional.silu(Tensor([128, 64, 128, 49],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 64, 128, 49],"float32"), None, ) 	 51380224 	 33834 	 10.101043224334717 	 10.196953535079956 	 0.3050720691680908 	 0.30803418159484863 	 15.409791946411133 	 15.401829242706299 	 0.46550655364990234 	 0.4651646614074707 	 
2025-08-04 21:34:52.938374 test begin: paddle.nn.functional.silu(Tensor([128, 64, 49, 128],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 64, 49, 128],"float32"), None, ) 	 51380224 	 33834 	 10.102185487747192 	 10.202072381973267 	 0.3051004409790039 	 0.3080291748046875 	 15.40897011756897 	 15.400644063949585 	 0.4654049873352051 	 0.46512413024902344 	 
2025-08-04 21:35:45.867578 test begin: paddle.nn.functional.silu(Tensor([128, 97, 64, 64],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 97, 64, 64],"float32"), None, ) 	 50855936 	 33834 	 9.996572732925415 	 10.09489631652832 	 0.30196142196655273 	 0.30492115020751953 	 15.252359390258789 	 15.246590375900269 	 0.4607243537902832 	 0.4605555534362793 	 
2025-08-04 21:36:38.598427 test begin: paddle.nn.functional.silu(Tensor([25, 128, 128, 128],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([25, 128, 128, 128],"float32"), None, ) 	 52428800 	 33834 	 10.596504211425781 	 10.399428367614746 	 0.31223058700561523 	 0.31412601470947266 	 15.717674493789673 	 15.709401607513428 	 0.4748067855834961 	 0.4744119644165039 	 
2025-08-04 21:37:36.409081 test begin: paddle.nn.functional.silu(Tensor([49, 256, 64, 64],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([49, 256, 64, 64],"float32"), None, ) 	 51380224 	 33834 	 10.921493291854858 	 10.197072744369507 	 0.30507969856262207 	 0.30800533294677734 	 15.408805131912231 	 15.400173902511597 	 0.46545839309692383 	 0.46514892578125 	 
2025-08-04 21:38:30.586034 test begin: paddle.nn.functional.silu(Tensor([49, 64, 128, 128],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([49, 64, 128, 128],"float32"), None, ) 	 51380224 	 33834 	 10.101164817810059 	 10.19698166847229 	 0.30509114265441895 	 0.3080143928527832 	 15.408977746963501 	 15.401630640029907 	 0.46547532081604004 	 0.46515965461730957 	 
2025-08-04 21:39:24.059612 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([1016065, 50],"float32"), Tensor([1016065, 50],"float32"), reduction="none", )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([1016065, 50],"float32"), Tensor([1016065, 50],"float32"), reduction="none", ) 	 101606500 	 12325 	 10.015764713287354 	 5.510123014450073 	 0.4152553081512451 	 0.45644640922546387 	 20.042577266693115 	 17.829901933670044 	 0.41547679901123047 	 0.3695082664489746 	 
2025-08-04 21:40:22.017339 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([1017, 50000],"float32"), Tensor([1017, 50000],"float32"), reduction="mean", delta=1.0, name=None, )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([1017, 50000],"float32"), Tensor([1017, 50000],"float32"), reduction="mean", delta=1.0, name=None, ) 	 101700000 	 12325 	 11.90886402130127 	 7.385460615158081 	 0.246551513671875 	 0.20386242866516113 	 21.7466778755188 	 14.31884479522705 	 0.3606991767883301 	 0.29670119285583496 	 
2025-08-04 21:41:20.662745 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([1914, 26543],"float32"), Tensor([1914, 26543],"float32"), reduction="none", )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([1914, 26543],"float32"), Tensor([1914, 26543],"float32"), reduction="none", ) 	 101606604 	 12325 	 10.000268697738647 	 5.509064674377441 	 0.4144783020019531 	 0.45654296875 	 20.029374599456787 	 17.834219694137573 	 0.415219783782959 	 0.3695945739746094 	 
2025-08-04 21:42:16.613487 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([33960, 187, 8],"float32"), Tensor([33960, 187, 8],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([33960, 187, 8],"float32"), Tensor([33960, 187, 8],"float32"), reduction="sum", ) 	 101608320 	 12325 	 11.901491165161133 	 7.379770040512085 	 0.2463512420654297 	 0.20380496978759766 	 21.708280563354492 	 14.30776071548462 	 0.36004137992858887 	 0.29654574394226074 	 
2025-08-04 21:43:13.612149 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([64, 187, 4245],"float32"), Tensor([64, 187, 4245],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([64, 187, 4245],"float32"), Tensor([64, 187, 4245],"float32"), reduction="sum", ) 	 101608320 	 12325 	 11.901418924331665 	 7.379120588302612 	 0.24645495414733887 	 0.20373201370239258 	 21.703752756118774 	 14.30793809890747 	 0.3601114749908447 	 0.2965250015258789 	 
2025-08-04 21:44:10.863854 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([64, 99226, 8],"float32"), Tensor([64, 99226, 8],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([64, 99226, 8],"float32"), Tensor([64, 99226, 8],"float32"), reduction="sum", ) 	 101607424 	 12325 	 11.887228965759277 	 7.3793675899505615 	 0.24613642692565918 	 0.20370745658874512 	 21.714551210403442 	 14.307579755783081 	 0.3601837158203125 	 0.2965092658996582 	 
2025-08-04 21:45:08.788681 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([7, 7257601],"float32"), Tensor([7, 7257601],"float32"), reduction="mean", delta=1.0, name=None, )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([7, 7257601],"float32"), Tensor([7, 7257601],"float32"), reduction="mean", delta=1.0, name=None, ) 	 101606414 	 12325 	 11.893858671188354 	 7.379498243331909 	 0.2462904453277588 	 0.20368432998657227 	 21.709362506866455 	 14.308966159820557 	 0.3603348731994629 	 0.2965877056121826 	 
2025-08-04 21:46:07.032996 test begin: paddle.nn.functional.softmax(Tensor([10, 2304, 2304],"float32"), axis=-1, )
W0804 21:46:07.856637 69624 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([10, 2304, 2304],"float32"), axis=-1, ) 	 53084160 	 33703 	 10.618062019348145 	 18.486889123916626 	 0.3219180107116699 	 0.5405049324035645 	 16.4917471408844 	 31.373594760894775 	 0.5000462532043457 	 0.4756908416748047 	 
2025-08-04 21:47:27.487230 test begin: paddle.nn.functional.softmax(Tensor([3840, 1, 144, 144],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([3840, 1, 144, 144],"float32"), -1, name=None, ) 	 79626240 	 33703 	 15.569864273071289 	 16.919984340667725 	 0.4721500873565674 	 0.5130753517150879 	 23.6201069355011 	 46.93905997276306 	 0.716285228729248 	 0.711681604385376 	 
2025-08-04 21:49:13.333964 test begin: paddle.nn.functional.softmax(Tensor([3840, 4, 144, 23],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([3840, 4, 144, 23],"float32"), -1, name=None, ) 	 50872320 	 33703 	 12.334688663482666 	 16.942208528518677 	 0.37398195266723633 	 0.5135929584503174 	 15.18135404586792 	 30.17914128303528 	 0.46034693717956543 	 0.4576075077056885 	 
2025-08-04 21:50:30.201147 test begin: paddle.nn.functional.softmax(Tensor([3840, 4, 23, 144],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([3840, 4, 23, 144],"float32"), -1, name=None, ) 	 50872320 	 33703 	 9.999757289886475 	 10.902299404144287 	 0.3032197952270508 	 0.3306121826171875 	 15.155444860458374 	 30.079424619674683 	 0.4595785140991211 	 0.456035852432251 	 
2025-08-04 21:51:41.628298 test begin: paddle.nn.functional.softmax(Tensor([4096, 1, 144, 144],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([4096, 1, 144, 144],"float32"), -1, name=None, ) 	 84934656 	 33703 	 16.5947425365448 	 18.027586936950684 	 0.503103494644165 	 0.5465953350067139 	 25.184033393859863 	 50.04264187812805 	 0.7636823654174805 	 0.7588009834289551 	 
2025-08-04 21:53:36.096951 test begin: paddle.nn.functional.softmax(Tensor([4096, 4, 144, 22],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([4096, 4, 144, 22],"float32"), -1, name=None, ) 	 51904512 	 33703 	 16.211740970611572 	 18.040766716003418 	 0.47055768966674805 	 0.5470046997070312 	 15.600611448287964 	 30.80426049232483 	 0.4730851650238037 	 0.4669759273529053 	 
2025-08-04 21:54:59.176431 test begin: paddle.nn.functional.softmax(Tensor([4096, 4, 22, 144],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([4096, 4, 22, 144],"float32"), -1, name=None, ) 	 51904512 	 33703 	 10.202110528945923 	 11.115444660186768 	 0.309293270111084 	 0.3370487689971924 	 15.459362983703613 	 30.690012454986572 	 0.4687650203704834 	 0.4653000831604004 	 
2025-08-04 21:56:09.330095 test begin: paddle.nn.functional.softmax(Tensor([60, 2304, 368],"float32"), axis=-1, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([60, 2304, 368],"float32"), axis=-1, ) 	 50872320 	 33703 	 10.130681276321411 	 10.21517539024353 	 0.3072376251220703 	 0.309023380279541 	 15.176631689071655 	 30.098285913467407 	 0.4602351188659668 	 0.45621466636657715 	 
2025-08-04 21:57:18.944973 test begin: paddle.nn.functional.softmax(Tensor([60, 368, 2304],"float32"), axis=-1, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([60, 368, 2304],"float32"), axis=-1, ) 	 50872320 	 33703 	 10.190516710281372 	 17.095396280288696 	 0.30899643898010254 	 0.5184450149536133 	 15.829557180404663 	 30.075443983078003 	 0.4800221920013428 	 0.45598578453063965 	 
2025-08-04 21:58:33.883539 test begin: paddle.nn.functional.softmax(Tensor([613, 4, 144, 144],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([613, 4, 144, 144],"float32"), -1, name=None, ) 	 50844672 	 33703 	 9.99262022972107 	 10.893304347991943 	 0.3042471408843994 	 0.33025169372558594 	 15.146635055541992 	 30.06392765045166 	 0.45932435989379883 	 0.4558126926422119 	 
2025-08-04 21:59:42.052641 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([1551, 16, 32, 64],"float32"), Tensor([1551, 16, 32, 1],"int64"), axis=3, )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([1551, 16, 32, 64],"float32"), Tensor([1551, 16, 32, 1],"int64"), axis=3, ) 	 51617280 	 29087 	 9.988953828811646 	 37.09173512458801 	 0.35082483291625977 	 0.260178804397583 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 22:00:46.121395 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 1, 64],"int64"), axis=2, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 1, 64],"int64"), axis=2, ) 	 52394496 	 29087 	 36.90237474441528 	 65.26263570785522 	 0.6482753753662109 	 0.381333589553833 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 22:02:50.778557 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 32, 1],"int64"), axis=-1, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 32, 1],"int64"), axis=-1, ) 	 51600640 	 29087 	 10.631706953048706 	 37.08405780792236 	 0.3509976863861084 	 0.26009225845336914 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 22:03:58.234098 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 32, 1],"int64"), axis=3, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 32, 1],"int64"), axis=3, ) 	 51600640 	 29087 	 10.887558221817017 	 37.07582974433899 	 0.351029634475708 	 0.2600715160369873 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 22:05:04.741284 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 24807, 64],"float32"), Tensor([2, 16, 1, 64],"int64"), axis=2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f47226402b0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:15:22.178493 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 24807, 64],"float32"), Tensor([2, 16, 24807, 1],"int64"), axis=-1, )
W0804 22:15:23.215648 70924 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 24807, 64],"float32"), Tensor([2, 16, 24807, 1],"int64"), axis=-1, ) 	 51598560 	 29087 	 10.015244483947754 	 37.0685658454895 	 0.3518648147583008 	 0.2599954605102539 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 22:16:26.925522 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 24807, 64],"float32"), Tensor([2, 16, 24807, 1],"int64"), axis=3, )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 24807, 64],"float32"), Tensor([2, 16, 24807, 1],"int64"), axis=3, ) 	 51598560 	 29087 	 10.011345148086548 	 37.08601975440979 	 0.3515152931213379 	 0.26006197929382324 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 22:17:30.996149 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 1, 49613],"int64"), axis=2, )
W0804 22:17:31.760066 70950 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 1, 49613],"int64"), axis=2, ) 	 52391328 	 29087 	 44.72951674461365 	 64.45889520645142 	 0.7857704162597656 	 0.37672996520996094 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 22:19:38.632385 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 32, 1],"int64"), axis=-1, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 32, 1],"int64"), axis=-1, ) 	 50804736 	 29087 	 19.09083604812622 	 31.298464059829712 	 0.6548314094543457 	 0.21957874298095703 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 22:20:44.660706 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 32, 1],"int64"), axis=3, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 32, 1],"int64"), axis=3, ) 	 50804736 	 29087 	 18.6387882232666 	 31.29856777191162 	 0.6549367904663086 	 0.2194805145263672 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 22:21:49.170470 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 793801],"float32"), Tensor([2, 16, 1, 793801],"int64"), axis=2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8790d77f40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:32:05.695942 test begin: paddle.nn.functional.softplus(Tensor([113401, 7, 64],"float32"), )
W0804 22:32:06.655768 71649 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([113401, 7, 64],"float32"), ) 	 50803648 	 33404 	 10.00200629234314 	 10.013758420944214 	 0.3059837818145752 	 0.30634307861328125 	 15.045579195022583 	 15.044135093688965 	 0.46019554138183594 	 0.4602193832397461 	 
2025-08-04 22:32:58.928311 test begin: paddle.nn.functional.softplus(Tensor([13, 10, 390794],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([13, 10, 390794],"float32"), ) 	 50803220 	 33404 	 10.007479190826416 	 10.442320346832275 	 0.30620527267456055 	 0.3062605857849121 	 15.047067403793335 	 15.047881603240967 	 0.460360050201416 	 0.4603290557861328 	 
2025-08-04 22:33:52.964052 test begin: paddle.nn.functional.softplus(Tensor([13, 1007, 3881],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([13, 1007, 3881],"float32"), ) 	 50806171 	 33404 	 10.004814386367798 	 10.012063980102539 	 0.30605483055114746 	 0.3062152862548828 	 15.047077894210815 	 15.048098087310791 	 0.460376501083374 	 0.4604156017303467 	 
2025-08-04 22:34:47.779844 test begin: paddle.nn.functional.softplus(Tensor([13, 61062, 64],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([13, 61062, 64],"float32"), ) 	 50803584 	 33404 	 10.061881065368652 	 10.00975227355957 	 0.30777835845947266 	 0.30626893043518066 	 15.047411680221558 	 15.047284603118896 	 0.46038246154785156 	 0.460315465927124 	 
2025-08-04 22:35:43.859405 test begin: paddle.nn.functional.softplus(Tensor([14, 56701, 64],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([14, 56701, 64],"float32"), ) 	 50804096 	 33404 	 10.05652117729187 	 10.009751081466675 	 0.3073403835296631 	 0.30620384216308594 	 15.04623031616211 	 15.047672748565674 	 0.460385799407959 	 0.4604456424713135 	 
2025-08-04 22:36:39.204597 test begin: paddle.nn.functional.softplus(Tensor([14, 7, 518401],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([14, 7, 518401],"float32"), ) 	 50803298 	 33404 	 11.644567251205444 	 10.009629249572754 	 0.30782413482666016 	 0.30623960494995117 	 15.0470130443573 	 15.046786546707153 	 0.4603235721588135 	 0.4603080749511719 	 
2025-08-04 22:37:33.314844 test begin: paddle.nn.functional.softplus(Tensor([789, 1007, 64],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([789, 1007, 64],"float32"), ) 	 50849472 	 33404 	 10.014180660247803 	 10.019476175308228 	 0.3062889575958252 	 0.30650997161865234 	 15.064523935317993 	 15.059634923934937 	 0.4608621597290039 	 0.4607510566711426 	 
2025-08-04 22:38:25.205681 test begin: paddle.nn.functional.softplus(Tensor([79381, 10, 64],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([79381, 10, 64],"float32"), ) 	 50803840 	 33404 	 10.0592041015625 	 10.010105609893799 	 0.3077719211578369 	 0.3062319755554199 	 15.048907995223999 	 15.047447919845581 	 0.46038222312927246 	 0.46039414405822754 	 
2025-08-04 22:39:19.439886 test begin: paddle.nn.functional.softshrink(Tensor([2822401, 3, 3],"float64"), 0, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([2822401, 3, 3],"float64"), 0, None, ) 	 25401609 	 33823 	 10.083930015563965 	 10.114219665527344 	 0.30469417572021484 	 0.3052711486816406 	 15.150392532348633 	 15.075926542282104 	 0.45775294303894043 	 0.4555535316467285 	 
2025-08-04 22:40:11.033962 test begin: paddle.nn.functional.softshrink(Tensor([2822401, 3, 3],"float64"), 5, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([2822401, 3, 3],"float64"), 5, None, ) 	 25401609 	 33823 	 10.083120822906494 	 10.530953168869019 	 0.30472517013549805 	 0.305112361907959 	 15.15416955947876 	 15.076815843582153 	 0.458054780960083 	 0.45559191703796387 	 
2025-08-04 22:41:04.416135 test begin: paddle.nn.functional.softshrink(Tensor([3, 2822401, 3],"float64"), 0, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([3, 2822401, 3],"float64"), 0, None, ) 	 25401609 	 33823 	 10.083105325698853 	 10.102588176727295 	 0.3047010898590088 	 0.30521488189697266 	 15.152573823928833 	 15.07609224319458 	 0.4581489562988281 	 0.45554184913635254 	 
2025-08-04 22:41:55.963136 test begin: paddle.nn.functional.softshrink(Tensor([3, 2822401, 3],"float64"), 5, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([3, 2822401, 3],"float64"), 5, None, ) 	 25401609 	 33823 	 10.082942247390747 	 10.09762692451477 	 0.3046889305114746 	 0.30507850646972656 	 15.153446674346924 	 15.078234434127808 	 0.45812129974365234 	 0.45557689666748047 	 
2025-08-04 22:42:50.750753 test begin: paddle.nn.functional.softshrink(Tensor([3, 3, 2822401],"float64"), 0, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([3, 3, 2822401],"float64"), 0, None, ) 	 25401609 	 33823 	 10.084035634994507 	 10.106404066085815 	 0.304659366607666 	 0.30528759956359863 	 15.152138233184814 	 15.075898170471191 	 0.45815253257751465 	 0.4555847644805908 	 
2025-08-04 22:43:43.630397 test begin: paddle.nn.functional.softshrink(Tensor([3, 3, 2822401],"float64"), 5, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([3, 3, 2822401],"float64"), 5, None, ) 	 25401609 	 33823 	 10.08691668510437 	 10.097308874130249 	 0.30463242530822754 	 0.30506181716918945 	 15.152230501174927 	 15.078222513198853 	 0.4576079845428467 	 0.4556136131286621 	 
2025-08-04 22:44:35.247925 test begin: paddle.nn.functional.softshrink(Tensor([32, 15, 207, 8, 32, 2],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([32, 15, 207, 8, 32, 2],"float32"), threshold=0.01, ) 	 50872320 	 33823 	 10.03400993347168 	 10.09960126876831 	 0.3027644157409668 	 0.3052177429199219 	 15.248443126678467 	 15.1254243850708 	 0.4606606960296631 	 0.4569733142852783 	 
2025-08-04 22:45:30.222058 test begin: paddle.nn.functional.softshrink(Tensor([32, 15, 8, 207, 32, 2],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([32, 15, 8, 207, 32, 2],"float32"), threshold=0.01, ) 	 50872320 	 33823 	 10.021973133087158 	 10.099526166915894 	 0.3027961254119873 	 0.305248498916626 	 15.250091791152954 	 15.124275922775269 	 0.46070241928100586 	 0.456956148147583 	 
2025-08-04 22:46:25.029096 test begin: paddle.nn.functional.softshrink(Tensor([32, 15, 8, 8, 32, 52],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([32, 15, 8, 8, 32, 52],"float32"), threshold=0.01, ) 	 51118080 	 33823 	 10.060885906219482 	 10.15060305595398 	 0.30403637886047363 	 0.3066709041595459 	 15.31576418876648 	 15.197723388671875 	 0.46275949478149414 	 0.45921969413757324 	 
2025-08-04 22:47:18.602094 test begin: paddle.nn.functional.softshrink(Tensor([32, 15, 8, 8, 827, 2],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([32, 15, 8, 8, 827, 2],"float32"), threshold=0.01, ) 	 50810880 	 33823 	 10.003559112548828 	 10.096829891204834 	 0.3022487163543701 	 0.3047199249267578 	 15.227582693099976 	 15.107241868972778 	 0.4600698947906494 	 0.45645904541015625 	 
2025-08-04 22:48:10.773911 test begin: paddle.nn.functional.softshrink(Tensor([32, 388, 8, 8, 32, 2],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([32, 388, 8, 8, 32, 2],"float32"), threshold=0.01, ) 	 50855936 	 33823 	 10.011855363845825 	 10.130958795547485 	 0.3025245666503906 	 0.30507731437683105 	 15.239399433135986 	 15.11992073059082 	 0.46046948432922363 	 0.4569094181060791 	 
2025-08-04 22:49:04.728328 test begin: paddle.nn.functional.softshrink(Tensor([827, 15, 8, 8, 32, 2],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([827, 15, 8, 8, 32, 2],"float32"), threshold=0.01, ) 	 50810880 	 33823 	 10.003647804260254 	 10.086985349655151 	 0.3022482395172119 	 0.3047940731048584 	 15.22697901725769 	 15.10743522644043 	 0.46007347106933594 	 0.4565098285675049 	 
2025-08-04 22:49:58.398076 test begin: paddle.nn.functional.softsign(Tensor([12404, 4096],"float32"), )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([12404, 4096],"float32"), ) 	 50806784 	 33892 	 10.014244079589844 	 35.349427461624146 	 0.3018069267272949 	 0.3553304672241211 	 15.259172439575195 	 111.13564276695251 	 0.4600536823272705 	 0.4187924861907959 	 
2025-08-04 22:52:53.828677 test begin: paddle.nn.functional.softsign(Tensor([2822401, 3, 3],"float64"), None, )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([2822401, 3, 3],"float64"), None, ) 	 25401609 	 33892 	 10.140120506286621 	 35.369229316711426 	 0.3056933879852295 	 0.355288028717041 	 15.177710056304932 	 110.80383205413818 	 0.4577326774597168 	 0.4175126552581787 	 
2025-08-04 22:55:49.019090 test begin: paddle.nn.functional.softsign(Tensor([3, 2822401, 3],"float64"), None, )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([3, 2822401, 3],"float64"), None, ) 	 25401609 	 33892 	 10.14192271232605 	 35.33946251869202 	 0.30569982528686523 	 0.35515260696411133 	 15.178190231323242 	 110.80162072181702 	 0.45783185958862305 	 0.41759562492370605 	 
2025-08-04 22:58:42.747412 test begin: paddle.nn.functional.softsign(Tensor([3, 3, 2822401],"float64"), None, )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([3, 3, 2822401],"float64"), None, ) 	 25401609 	 33892 	 10.14421796798706 	 35.344003200531006 	 0.30570507049560547 	 0.35532379150390625 	 15.178537845611572 	 110.81337332725525 	 0.45757508277893066 	 0.41757798194885254 	 
2025-08-04 23:01:35.773323 test begin: paddle.nn.functional.softsign(Tensor([300, 169345],"float32"), )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([300, 169345],"float32"), ) 	 50803500 	 33892 	 10.51939845085144 	 35.346479177474976 	 0.3014645576477051 	 0.35533571243286133 	 15.259055852890015 	 111.13337111473083 	 0.46015381813049316 	 0.41877293586730957 	 
2025-08-04 23:04:32.480158 test begin: paddle.nn.functional.softsign(Tensor([32, 1587601],"float32"), )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([32, 1587601],"float32"), ) 	 50803232 	 33892 	 9.999232530593872 	 35.34613108634949 	 0.30148863792419434 	 0.3553345203399658 	 15.258028745651245 	 111.13303136825562 	 0.4602231979370117 	 0.418806791305542 	 
2025-08-04 23:07:25.963313 test begin: paddle.nn.functional.softsign(Tensor([396901, 128],"float32"), )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([396901, 128],"float32"), ) 	 50803328 	 33892 	 11.585040807723999 	 35.34550857543945 	 0.3013927936553955 	 0.3553283214569092 	 15.254917621612549 	 111.1302399635315 	 0.45999765396118164 	 0.41878509521484375 	 
2025-08-04 23:10:24.213319 test begin: paddle.nn.functional.square_error_cost(Tensor([10161, 100, 100],"float16"), Tensor([10161, 100, 100],"float32"), )
W0804 23:10:27.519780 73053 dygraph_functions.cc:93089] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([10161, 100, 100],"float16"), Tensor([10161, 100, 100],"float32"), ) 	 203220000 	 16765 	 32.91058421134949 	 23.528568744659424 	 0.6689338684082031 	 0.7165069580078125 	 38.42005205154419 	 52.59735870361328 	 0.7805547714233398 	 0.5343718528747559 	 combined
2025-08-04 23:12:58.713076 test begin: paddle.nn.functional.square_error_cost(Tensor([3, 2, 1, 2],"float64"), label=Tensor([3, 2, 2116801, 2],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([3, 2, 1, 2],"float64"), label=Tensor([3, 2, 2116801, 2],"float64"), ) 	 25401624 	 16765 	 10.035608768463135 	 10.042912006378174 	 0.3059060573577881 	 0.30590176582336426 	 69.57062673568726 	 25.42514395713806 	 1.1732604503631592 	 0.2213582992553711 	 combined
2025-08-04 23:14:55.934898 test begin: paddle.nn.functional.square_error_cost(Tensor([3, 2, 1, 4233601],"float64"), label=Tensor([3, 2, 1, 4233601],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([3, 2, 1, 4233601],"float64"), label=Tensor([3, 2, 1, 4233601],"float64"), ) 	 50803212 	 16765 	 12.460762023925781 	 12.44120717048645 	 0.3798243999481201 	 0.3787055015563965 	 15.52756381034851 	 22.670143842697144 	 0.4732646942138672 	 0.2765367031097412 	 combined
2025-08-04 23:16:03.957401 test begin: paddle.nn.functional.square_error_cost(Tensor([3, 2, 2116801, 2],"float64"), label=Tensor([3, 2, 1, 2],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([3, 2, 2116801, 2],"float64"), label=Tensor([3, 2, 1, 2],"float64"), ) 	 25401624 	 16765 	 10.007442474365234 	 10.035157918930054 	 0.3049492835998535 	 0.30585765838623047 	 64.33616328239441 	 25.419950008392334 	 1.3085739612579346 	 0.22141385078430176 	 combined
2025-08-04 23:17:54.967566 test begin: paddle.nn.functional.square_error_cost(Tensor([3, 2, 2116801, 2],"float64"), label=Tensor([3, 2, 2116801, 2],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([3, 2, 2116801, 2],"float64"), label=Tensor([3, 2, 2116801, 2],"float64"), ) 	 50803224 	 16765 	 12.46904993057251 	 12.425357818603516 	 0.3799881935119629 	 0.37871503829956055 	 15.527754783630371 	 22.67032027244568 	 0.47331690788269043 	 0.2765960693359375 	 combined
2025-08-04 23:19:00.083664 test begin: paddle.nn.functional.square_error_cost(Tensor([3, 4233601, 1, 2],"float64"), label=Tensor([3, 4233601, 1, 2],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([3, 4233601, 1, 2],"float64"), label=Tensor([3, 4233601, 1, 2],"float64"), ) 	 50803212 	 16765 	 12.47072696685791 	 12.42539358139038 	 0.38011884689331055 	 0.37871360778808594 	 15.528377532958984 	 22.670276403427124 	 0.47325778007507324 	 0.276599645614624 	 combined
2025-08-04 23:20:05.004246 test begin: paddle.nn.functional.square_error_cost(Tensor([5081, 100, 100],"float16"), Tensor([5081, 100, 100],"float32"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([5081, 100, 100],"float16"), Tensor([5081, 100, 100],"float32"), ) 	 101620000 	 16765 	 16.53816246986389 	 11.83932113647461 	 0.33622288703918457 	 0.3608858585357666 	 19.281325578689575 	 26.485356330871582 	 0.3918616771697998 	 0.2690706253051758 	 combined
2025-08-04 23:21:21.939249 test begin: paddle.nn.functional.square_error_cost(Tensor([5081, 100, 100],"float32"), Tensor([5081, 100, 100],"float32"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([5081, 100, 100],"float32"), Tensor([5081, 100, 100],"float32"), ) 	 101620000 	 16765 	 12.49968695640564 	 12.458256721496582 	 0.38100242614746094 	 0.3796093463897705 	 15.493535041809082 	 22.689043760299683 	 0.4722466468811035 	 0.27680492401123047 	 combined
2025-08-04 23:22:27.958560 test begin: paddle.nn.functional.square_error_cost(Tensor([6350401, 2, 1, 2],"float64"), label=Tensor([6350401, 2, 1, 2],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([6350401, 2, 1, 2],"float64"), label=Tensor([6350401, 2, 1, 2],"float64"), ) 	 50803208 	 16765 	 12.468483209609985 	 12.42525863647461 	 0.3800523281097412 	 0.37876033782958984 	 15.528676748275757 	 22.6718533039093 	 0.4733242988586426 	 0.27655816078186035 	 combined
2025-08-04 23:23:34.495010 test begin: paddle.nn.functional.square_error_cost(Tensor([8, 100, 63505],"float32"), Tensor([8, 100, 63505],"float32"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([8, 100, 63505],"float32"), Tensor([8, 100, 63505],"float32"), ) 	 101608000 	 16765 	 12.526163816452026 	 12.455798625946045 	 0.38094329833984375 	 0.37961578369140625 	 15.4922194480896 	 22.687416076660156 	 0.472156286239624 	 0.27675342559814453 	 combined
2025-08-04 23:24:42.962238 test begin: paddle.nn.functional.square_error_cost(Tensor([8, 63505, 100],"float32"), Tensor([8, 63505, 100],"float32"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([8, 63505, 100],"float32"), Tensor([8, 63505, 100],"float32"), ) 	 101608000 	 16765 	 12.512073040008545 	 12.45418930053711 	 0.3810126781463623 	 0.3795959949493408 	 15.49124026298523 	 22.68756628036499 	 0.47215890884399414 	 0.27677416801452637 	 combined
2025-08-04 23:25:50.697374 test begin: paddle.nn.functional.tanh(Tensor([1016065, 50],"float32"), None, )
[Prof] paddle.nn.functional.tanh 	 paddle.nn.functional.tanh(Tensor([1016065, 50],"float32"), None, ) 	 50803250 	 33853 	 10.730704545974731 	 10.083534955978394 	 0.3020141124725342 	 0.30438947677612305 	 15.238847255706787 	 15.113523721694946 	 0.4600560665130615 	 0.4562828540802002 	 
2025-08-04 23:26:44.894006 test begin: paddle.nn.functional.tanh(Tensor([147015, 346],"float32"), None, )
[Prof] paddle.nn.functional.tanh 	 paddle.nn.functional.tanh(Tensor([147015, 346],"float32"), None, ) 	 50867190 	 33853 	 10.028366565704346 	 10.097459077835083 	 0.3027067184448242 	 0.30478930473327637 	 15.254662990570068 	 15.132497310638428 	 0.4605581760406494 	 0.45682239532470703 	 
2025-08-04 23:27:38.766272 test begin: paddle.nn.functional.tanh(Tensor([282600, 180],"float32"), None, )
[Prof] paddle.nn.functional.tanh 	 paddle.nn.functional.tanh(Tensor([282600, 180],"float32"), None, ) 	 50868000 	 33853 	 10.48332142829895 	 10.745154619216919 	 0.302720308303833 	 0.3047785758972168 	 15.25610899925232 	 15.133513450622559 	 0.4605519771575928 	 0.4569251537322998 	 
2025-08-04 23:28:34.861246 test begin: paddle.nn.functional.tanh(Tensor([564481, 90],"float32"), None, )
[Prof] paddle.nn.functional.tanh 	 paddle.nn.functional.tanh(Tensor([564481, 90],"float32"), None, ) 	 50803290 	 33853 	 10.63772439956665 	 10.083666801452637 	 0.30179524421691895 	 0.30443334579467773 	 15.237736225128174 	 15.11350417137146 	 0.4600095748901367 	 0.456240177154541 	 
2025-08-04 23:29:28.791545 test begin: paddle.nn.functional.tanh(Tensor([93401, 544],"float32"), None, )
[Prof] paddle.nn.functional.tanh 	 paddle.nn.functional.tanh(Tensor([93401, 544],"float32"), None, ) 	 50810144 	 33853 	 10.006152629852295 	 10.09085988998413 	 0.3021063804626465 	 0.30445075035095215 	 15.24057674407959 	 15.115248918533325 	 0.46008944511413574 	 0.4562869071960449 	 
2025-08-04 23:30:21.954435 test begin: paddle.nn.functional.tanhshrink(Tensor([2822401, 3, 3],"float64"), None, )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(Tensor([2822401, 3, 3],"float64"), None, ) 	 25401609 	 33845 	 10.110339164733887 	 25.142091512680054 	 0.3052654266357422 	 0.3795771598815918 	 15.160731554031372 	 40.10105895996094 	 0.4578676223754883 	 0.40369510650634766 	 
2025-08-04 23:31:53.645159 test begin: paddle.nn.functional.tanhshrink(Tensor([3, 2822401, 3],"float64"), None, )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(Tensor([3, 2822401, 3],"float64"), None, ) 	 25401609 	 33845 	 10.110929012298584 	 25.146260499954224 	 0.30527687072753906 	 0.3796346187591553 	 15.161290884017944 	 40.09995889663696 	 0.4578530788421631 	 0.4037320613861084 	 
2025-08-04 23:33:25.425550 test begin: paddle.nn.functional.tanhshrink(Tensor([3, 3, 2822401],"float64"), None, )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(Tensor([3, 3, 2822401],"float64"), None, ) 	 25401609 	 33845 	 10.111310005187988 	 25.1419198513031 	 0.305250883102417 	 0.3796210289001465 	 15.15855622291565 	 40.09900403022766 	 0.4576301574707031 	 0.4037613868713379 	 
2025-08-04 23:35:01.462363 test begin: paddle.nn.functional.tanhshrink(Tensor([50803201],"float32"), None, )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(Tensor([50803201],"float32"), None, ) 	 50803201 	 33845 	 10.52913498878479 	 25.147573709487915 	 0.30185437202453613 	 0.37965917587280273 	 15.232948303222656 	 40.23938488960266 	 0.4598863124847412 	 0.4050934314727783 	 
2025-08-04 23:36:38.443503 test begin: paddle.nn.functional.tanhshrink(x=Tensor([2822401, 3, 3],"float64"), )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(x=Tensor([2822401, 3, 3],"float64"), ) 	 25401609 	 33845 	 10.633278369903564 	 25.142295122146606 	 0.30525922775268555 	 0.37962818145751953 	 15.160248041152954 	 40.10141086578369 	 0.4578874111175537 	 0.40370917320251465 	 
2025-08-04 23:38:11.160516 test begin: paddle.nn.functional.tanhshrink(x=Tensor([3, 2822401, 3],"float64"), )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(x=Tensor([3, 2822401, 3],"float64"), ) 	 25401609 	 33845 	 10.109556198120117 	 25.157968044281006 	 0.30527353286743164 	 0.3795289993286133 	 15.159208536148071 	 40.09956455230713 	 0.4574873447418213 	 0.403759241104126 	 
2025-08-04 23:39:44.030177 test begin: paddle.nn.functional.tanhshrink(x=Tensor([3, 3, 2822401],"float64"), )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(x=Tensor([3, 3, 2822401],"float64"), ) 	 25401609 	 33845 	 11.041762113571167 	 25.141223907470703 	 0.3052957057952881 	 0.3796076774597168 	 15.157593488693237 	 40.101420402526855 	 0.45765137672424316 	 0.4036998748779297 	 
2025-08-04 23:41:16.720326 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 28225, 3, 3],"float64"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 28225, 3, 3],"float64"), 1.0, 0.0, None, ) 	 25402500 	 33826 	 10.063915014266968 	 10.08370327949524 	 0.3040482997894287 	 0.30454182624816895 	 15.158313512802124 	 15.078991889953613 	 0.457836389541626 	 0.45556163787841797 	 
2025-08-04 23:42:08.278762 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4, 21169, 3],"float64"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 4, 21169, 3],"float64"), 1.0, 0.0, None, ) 	 25402800 	 33826 	 10.078998565673828 	 10.08642292022705 	 0.30450987815856934 	 0.304567813873291 	 15.155413627624512 	 15.079795122146606 	 0.4579658508300781 	 0.4550666809082031 	 
2025-08-04 23:42:59.856090 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3, 21169],"float64"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3, 21169],"float64"), 1.0, 0.0, None, ) 	 25402800 	 33826 	 10.078961372375488 	 10.081719160079956 	 0.30454325675964355 	 0.3045687675476074 	 15.15481948852539 	 15.08168363571167 	 0.4580404758453369 	 0.4556119441986084 	 
2025-08-04 23:43:52.371325 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3, 42337],"float32"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3, 42337],"float32"), 1.0, 0.0, None, ) 	 50804400 	 33826 	 10.00045919418335 	 10.07561206817627 	 0.3021864891052246 	 0.3043022155761719 	 15.220438957214355 	 15.106335639953613 	 0.4598877429962158 	 0.45638608932495117 	 
2025-08-04 23:44:45.269971 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4, 42337, 3],"float32"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 4, 42337, 3],"float32"), 1.0, 0.0, None, ) 	 50804400 	 33826 	 10.00999665260315 	 10.073943614959717 	 0.3021070957183838 	 0.30431365966796875 	 15.220303773880005 	 15.106279134750366 	 0.45987391471862793 	 0.45640063285827637 	 
2025-08-04 23:45:40.508013 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 56449, 3, 3],"float32"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 56449, 3, 3],"float32"), 1.0, 0.0, None, ) 	 50804100 	 33826 	 11.0344398021698 	 10.073722839355469 	 0.30207109451293945 	 0.3043673038482666 	 15.220882654190063 	 15.106107234954834 	 0.4597780704498291 	 0.45638561248779297 	 
2025-08-04 23:46:34.665472 test begin: paddle.nn.functional.thresholded_relu(Tensor([1411201, 4, 3, 3],"float32"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([1411201, 4, 3, 3],"float32"), 1.0, 0.0, None, ) 	 50803236 	 33826 	 10.827533721923828 	 10.073911666870117 	 0.30224180221557617 	 0.3043649196624756 	 15.226739406585693 	 15.105820655822754 	 0.46004796028137207 	 0.4564340114593506 	 
2025-08-04 23:47:29.435073 test begin: paddle.nn.functional.thresholded_relu(Tensor([705601, 4, 3, 3],"float64"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([705601, 4, 3, 3],"float64"), 1.0, 0.0, None, ) 	 25401636 	 33826 	 10.077039241790771 	 10.327224016189575 	 0.30445241928100586 	 0.30456113815307617 	 15.155517578125 	 15.079357385635376 	 0.45788049697875977 	 0.4555647373199463 	 
2025-08-04 23:48:24.644296 test begin: paddle.nn.functional.thresholded_relu(x=Tensor([100, 4, 3, 42337],"float32"), )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(x=Tensor([100, 4, 3, 42337],"float32"), ) 	 50804400 	 33826 	 10.001375913619995 	 10.074134826660156 	 0.3021552562713623 	 0.3043828010559082 	 15.220347166061401 	 15.106260299682617 	 0.45979762077331543 	 0.4563474655151367 	 
2025-08-04 23:49:17.506141 test begin: paddle.nn.functional.thresholded_relu(x=Tensor([100, 4, 42337, 3],"float32"), )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(x=Tensor([100, 4, 42337, 3],"float32"), ) 	 50804400 	 33826 	 10.000144004821777 	 10.080254316329956 	 0.30214881896972656 	 0.30432963371276855 	 15.22067904472351 	 15.106409311294556 	 0.4598851203918457 	 0.45640134811401367 	 
2025-08-04 23:50:09.642888 test begin: paddle.nn.functional.thresholded_relu(x=Tensor([100, 56449, 3, 3],"float32"), )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(x=Tensor([100, 56449, 3, 3],"float32"), ) 	 50804100 	 33826 	 9.998337507247925 	 10.081055879592896 	 0.3020758628845215 	 0.3043246269226074 	 15.221132278442383 	 15.10624384880066 	 0.4598114490509033 	 0.45642876625061035 	 
2025-08-04 23:51:04.650542 test begin: paddle.nn.functional.thresholded_relu(x=Tensor([1411201, 4, 3, 3],"float32"), )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(x=Tensor([1411201, 4, 3, 3],"float32"), ) 	 50803236 	 33826 	 10.505712032318115 	 10.07394790649414 	 0.30216550827026367 	 0.3043496608734131 	 15.225924968719482 	 15.1055748462677 	 0.45999646186828613 	 0.45638227462768555 	 
2025-08-04 23:51:58.645522 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="mean", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="mean", name=None, ) 	 76204815 	 4341 	 10.31852912902832 	 7.9040772914886475 	 4.1484832763671875e-05 	 0.15475845336914062 	 17.662225484848022 	 12.446125984191895 	 0.46370863914489746 	 0.1834430694580078 	 
2025-08-04 23:52:49.688051 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="none", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="none", name=None, ) 	 76204815 	 4341 	 10.04172134399414 	 7.883798599243164 	 2.956390380859375e-05 	 0.1684584617614746 	 17.655327320098877 	 12.430556297302246 	 0.5205626487731934 	 0.19540047645568848 	 
2025-08-04 23:53:42.080352 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="sum", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="sum", name=None, ) 	 76204815 	 4341 	 10.078357934951782 	 7.902162075042725 	 2.574920654296875e-05 	 0.15472626686096191 	 17.667192697525024 	 12.430156230926514 	 0.4638350009918213 	 0.19542264938354492 	 
2025-08-04 23:54:32.317691 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="mean", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="mean", name=None, ) 	 76204815 	 4341 	 12.031562566757202 	 9.695671796798706 	 0.00018525123596191406 	 0.20725560188293457 	 19.177169799804688 	 14.135530948638916 	 0.503326416015625 	 0.20837020874023438 	 
2025-08-04 23:55:29.099330 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="none", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="none", name=None, ) 	 76204815 	 4341 	 11.885350942611694 	 9.535153865814209 	 0.0001499652862548828 	 0.24892783164978027 	 19.038341760635376 	 13.993611574172974 	 0.5611855983734131 	 0.2198774814605713 	 
2025-08-04 23:56:26.147858 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="sum", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="sum", name=None, ) 	 76204815 	 4341 	 12.052512407302856 	 9.694705486297607 	 0.0001800060272216797 	 0.20724225044250488 	 19.177308797836304 	 13.892332077026367 	 0.5033295154571533 	 0.2183363437652588 	 
2025-08-04 23:57:24.366757 test begin: paddle.nn.functional.unfold(Tensor([10, 1241, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 1241, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), ) 	 50831360 	 5718 	 9.996273040771484 	 15.089768648147583 	 0.17864727973937988 	 0.24509644508361816 	 21.348819732666016 	 40.9804265499115 	 0.34708642959594727 	 7.324284315109253 	 
2025-08-04 23:59:00.366719 test begin: paddle.nn.functional.unfold(Tensor([10, 1241, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 1241, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, ) 	 50831360 	 5718 	 9.995667934417725 	 15.096271514892578 	 0.17864274978637695 	 0.24510455131530762 	 21.347941637039185 	 40.97787308692932 	 0.347048282623291 	 7.32407546043396 	 
2025-08-05 00:00:40.047965 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 26461, 64],"float32"), 3, 1, 1, tuple(1,1,), )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 3, 26461, 64],"float32"), 3, 1, 1, tuple(1,1,), ) 	 50805120 	 5718 	 11.392696142196655 	 15.132406234741211 	 0.18693852424621582 	 0.22539663314819336 	 21.572720289230347 	 41.29029083251953 	 0.35070300102233887 	 7.380061388015747 	 
2025-08-05 00:02:18.383062 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 26461, 64],"float32"), 3, 1, tuple(1,1,), 1, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 3, 26461, 64],"float32"), 3, 1, tuple(1,1,), 1, ) 	 50805120 	 5718 	 10.460181474685669 	 15.133039712905884 	 0.18689846992492676 	 0.22544336318969727 	 21.585139751434326 	 41.24730134010315 	 0.35103392601013184 	 7.372358560562134 	 
2025-08-05 00:03:56.448420 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 64, 26461],"float32"), 3, 1, 1, tuple(1,1,), )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 3, 64, 26461],"float32"), 3, 1, 1, tuple(1,1,), ) 	 50805120 	 5718 	 12.341148376464844 	 17.064257383346558 	 0.2205793857574463 	 0.25405311584472656 	 21.189934492111206 	 40.525447607040405 	 0.3444831371307373 	 7.243211030960083 	 
2025-08-05 00:05:38.065734 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 64, 26461],"float32"), 3, 1, tuple(1,1,), 1, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 3, 64, 26461],"float32"), 3, 1, tuple(1,1,), 1, ) 	 50805120 	 5718 	 13.338608264923096 	 17.065013647079468 	 0.22063899040222168 	 0.2541985511779785 	 21.204878330230713 	 40.56938076019287 	 0.34481215476989746 	 7.251018285751343 	 
2025-08-05 00:07:19.320255 test begin: paddle.nn.functional.unfold(Tensor([338, 3, 224, 224],"float32"), 16, 16, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([338, 3, 224, 224],"float32"), 16, 16, ) 	 50878464 	 5718 	 138.7628276348114 	 137.5844841003418 	 0.07325863838195801 	 0.07273507118225098 	 18.86453366279602 	 12.052250862121582 	 0.00994110107421875 	 2.155001401901245 	 
2025-08-05 00:12:30.778467 test begin: paddle.nn.functional.unfold(Tensor([4135, 3, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9d22cc2b00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:22:35.706384 test begin: paddle.nn.functional.unfold(Tensor([4135, 3, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, )
W0805 00:22:40.734097 76458 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f777bac6e30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:32:44.124516 test begin: paddle.nn.functional.unfold(Tensor([64, 16, 224, 224],"float32"), 16, 16, )
W0805 00:32:45.234666 77004 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([64, 16, 224, 224],"float32"), 16, 16, ) 	 51380224 	 5718 	 87.90282678604126 	 45.11279916763306 	 0.2454674243927002 	 0.1259632110595703 	 10.68138074874878 	 12.1670241355896 	 0.02926802635192871 	 2.174618721008301 	 
2025-08-05 00:35:24.562485 test begin: paddle.nn.functional.unfold(Tensor([64, 3, 1182, 224],"float32"), 16, 16, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([64, 3, 1182, 224],"float32"), 16, 16, ) 	 50835456 	 5718 	 88.2370707988739 	 44.926334857940674 	 0.2449040412902832 	 0.12543034553527832 	 10.615423679351807 	 13.754254341125488 	 0.029051542282104492 	 1.22916841506958 	 
2025-08-05 00:38:04.764744 test begin: paddle.nn.functional.unfold(Tensor([64, 3, 224, 1182],"float32"), 16, 16, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([64, 3, 224, 1182],"float32"), 16, 16, ) 	 50835456 	 5718 	 90.0979118347168 	 45.51129698753357 	 0.2515742778778076 	 0.12707805633544922 	 10.633867502212524 	 13.769988536834717 	 0.029133319854736328 	 1.2305293083190918 	 
2025-08-05 00:40:47.468226 test begin: paddle.nn.functional.zeropad2d(Tensor([338, 3, 224, 224],"float32"), list[2,2,2,2,], )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.common.zeropad2d" is deprecated since 3.0.0, and will be removed in future versions. Please use "paddle.nn.ZeroPad2D" instead.
    Reason: Please use class ZeroPad2D [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.common.zeropad2d" is deprecated since 3.0.0, and will be removed in future versions. Please use "paddle.nn.ZeroPad2D" instead.
    Reason: Please use class ZeroPad2D [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([338, 3, 224, 224],"float32"), list[2,2,2,2,], ) 	 50878464 	 31313 	 19.68964385986328 	 14.657431364059448 	 0.642462968826294 	 0.23916029930114746 	 23.49692440032959 	 9.753901958465576 	 0.38337063789367676 	 0.31830930709838867 	 combined
2025-08-05 00:41:57.538253 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 127, 224, 224],"float64"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 127, 224, 224],"float64"), list[2,2,2,2,], ) 	 25489408 	 31313 	 10.135114192962646 	 14.365907430648804 	 0.33078908920288086 	 0.2343897819519043 	 14.393185377120972 	 9.506889343261719 	 0.2346973419189453 	 0.3102610111236572 	 combined
2025-08-05 00:42:47.168006 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 127, 224, 224],"int64"), Tensor([4],"int32"), )
W0805 00:42:59.664980 77550 backward.cc:462] While running Node (Pad3dGradNode) raises an EnforceNotMet exception
[Error] (NotFound) The kernel with key (GPU, Undefined(AnyLayout), int64) of kernel `pad3d_grad` is not registered and fail to fallback to CPU one. Selected wrong DataType `int64`. Paddle support following DataTypes: float64, complex128, float16, float32, complex64, bfloat16.
  [Hint: Expected kernel_iter != iter->second.end(), but received kernel_iter == iter->second.end().] (at ../paddle/phi/core/kernel_factory.cc:380)

[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 127, 224, 224],"int64"), Tensor([4],"int32"), ) 	 25489412 	 31313 	 11.764252424240112 	 16.35788321495056 	 0.00031185150146484375 	 0.00042939186096191406 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 00:43:16.226575 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 254, 224, 224],"float32"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 254, 224, 224],"float32"), list[2,2,2,2,], ) 	 50978816 	 31313 	 19.7200448513031 	 14.680454015731812 	 0.6436126232147217 	 0.2395339012145996 	 23.526320934295654 	 9.771951913833618 	 0.38397932052612305 	 0.31897926330566406 	 combined
2025-08-05 00:44:26.199961 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 3, 18901, 224],"float32"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 3, 18901, 224],"float32"), list[2,2,2,2,], ) 	 50805888 	 31313 	 19.337111949920654 	 14.549351215362549 	 0.631096363067627 	 0.23742413520812988 	 23.433462381362915 	 9.731137752532959 	 0.38237953186035156 	 0.31760597229003906 	 combined
2025-08-05 00:45:35.015128 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 18901],"float32"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 18901],"float32"), list[2,2,2,2,], ) 	 50805888 	 31313 	 20.023275136947632 	 13.911295652389526 	 0.6296992301940918 	 0.2270069122314453 	 23.460286378860474 	 9.65484070777893 	 0.38280701637268066 	 0.315108060836792 	 combined
2025-08-05 00:46:45.994549 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 9451],"float64"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 9451],"float64"), list[2,2,2,2,], ) 	 25404288 	 31313 	 9.993377447128296 	 13.773759365081787 	 0.3261899948120117 	 0.22480297088623047 	 14.29217791557312 	 9.386603832244873 	 0.23300671577453613 	 0.30634236335754395 	 combined
2025-08-05 00:47:34.717443 test begin: paddle.nonzero(Tensor([510, 128, 28, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([510, 128, 28, 28],"float32"), ) 	 51179520 	 1360 	 10.523521423339844 	 3.171814203262329 	 0.00554347038269043 	 0.002128124237060547 	 None 	 None 	 None 	 None 	 
2025-08-05 00:47:52.352004 test begin: paddle.nonzero(Tensor([510, 80, 28, 45],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([510, 80, 28, 45],"float32"), ) 	 51408000 	 1360 	 10.211507320404053 	 3.2033450603485107 	 0.005540609359741211 	 0.0021200180053710938 	 None 	 None 	 None 	 None 	 
2025-08-05 00:48:10.303794 test begin: paddle.nonzero(Tensor([510, 80, 45, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([510, 80, 45, 28],"float32"), ) 	 51408000 	 1360 	 10.204697847366333 	 3.182401418685913 	 0.005555152893066406 	 0.002135753631591797 	 None 	 None 	 None 	 None 	 
2025-08-05 00:48:24.488734 test begin: paddle.nonzero(Tensor([511, 127, 28, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([511, 127, 28, 28],"float32"), ) 	 50879248 	 1360 	 10.178776264190674 	 3.8919172286987305 	 0.005514383316040039 	 0.002127408981323242 	 None 	 None 	 None 	 None 	 
2025-08-05 00:48:43.020348 test begin: paddle.nonzero(Tensor([511, 80, 28, 45],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([511, 80, 28, 45],"float32"), ) 	 51508800 	 1360 	 10.237229585647583 	 3.2049190998077393 	 0.005527973175048828 	 0.002173900604248047 	 None 	 None 	 None 	 None 	 
2025-08-05 00:48:57.451482 test begin: paddle.nonzero(Tensor([511, 80, 45, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([511, 80, 45, 28],"float32"), ) 	 51508800 	 1360 	 10.231177806854248 	 3.183819532394409 	 0.005557060241699219 	 0.002147674560546875 	 None 	 None 	 None 	 None 	 
2025-08-05 00:49:14.593973 test begin: paddle.nonzero(Tensor([512, 127, 28, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([512, 127, 28, 28],"float32"), ) 	 50978816 	 1360 	 11.368132829666138 	 3.148193597793579 	 0.00552678108215332 	 0.002119302749633789 	 None 	 None 	 None 	 None 	 
2025-08-05 00:49:30.087056 test begin: paddle.nonzero(Tensor([512, 80, 28, 45],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([512, 80, 28, 45],"float32"), ) 	 51609600 	 1360 	 10.238536596298218 	 3.2118003368377686 	 0.005562305450439453 	 0.0021708011627197266 	 None 	 None 	 None 	 None 	 
2025-08-05 00:49:45.796943 test begin: paddle.nonzero(Tensor([512, 80, 45, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([512, 80, 45, 28],"float32"), ) 	 51609600 	 1360 	 10.250486135482788 	 3.193086624145508 	 0.005563497543334961 	 0.002153158187866211 	 None 	 None 	 None 	 None 	 
2025-08-05 00:50:00.217644 test begin: paddle.nonzero(Tensor([811, 80, 28, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([811, 80, 28, 28],"float32"), ) 	 50865920 	 1360 	 10.117482423782349 	 3.151228904724121 	 0.005480051040649414 	 0.0021047592163085938 	 None 	 None 	 None 	 None 	 
2025-08-05 00:50:14.515777 test begin: paddle.not_equal(Tensor([13, 15266, 16, 4, 1],"int64"), Tensor([13, 15266, 16, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 15266, 16, 4, 1],"int64"), Tensor([13, 15266, 16, 1, 8],"int64"), ) 	 38103936 	 46945 	 25.88679051399231 	 24.176265954971313 	 0.5635254383087158 	 0.5260934829711914 	 None 	 None 	 None 	 None 	 
2025-08-05 00:51:08.620573 test begin: paddle.not_equal(Tensor([13, 2, 122124, 4, 1],"int64"), Tensor([13, 2, 122124, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 122124, 4, 1],"int64"), Tensor([13, 2, 122124, 1, 8],"int64"), ) 	 38102688 	 46945 	 26.768041610717773 	 24.182399034500122 	 0.5682418346405029 	 0.5259449481964111 	 None 	 None 	 None 	 None 	 
2025-08-05 00:52:03.123697 test begin: paddle.not_equal(Tensor([13, 2, 16, 4, 15266],"int64"), Tensor([13, 2, 16, 1, 15266],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 16, 4, 15266],"int64"), Tensor([13, 2, 16, 1, 15266],"int64"), ) 	 31753280 	 46945 	 9.99994945526123 	 10.292794466018677 	 0.21771860122680664 	 0.22410297393798828 	 None 	 None 	 None 	 None 	 
2025-08-05 00:52:23.936563 test begin: paddle.not_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 61062],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 61062],"int64"), ) 	 25403456 	 46945 	 26.39967131614685 	 22.16198968887329 	 0.5747404098510742 	 0.48172640800476074 	 None 	 None 	 None 	 None 	 
2025-08-05 00:53:14.651195 test begin: paddle.not_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), ) 	 25405120 	 46945 	 48.92596483230591 	 44.49734616279602 	 1.0650441646575928 	 0.9687032699584961 	 None 	 None 	 None 	 None 	 
2025-08-05 00:54:48.599178 test begin: paddle.not_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 61062, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 61062, 8],"int64"), ) 	 228616128 	 46945 	 83.80624485015869 	 73.74100399017334 	 1.8244049549102783 	 1.6050853729248047 	 None 	 None 	 None 	 None 	 
2025-08-05 00:57:32.287644 test begin: paddle.not_equal(Tensor([13, 2, 244247, 4, 1],"int64"), Tensor([13, 2, 244247, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 244247, 4, 1],"int64"), Tensor([13, 2, 244247, 1, 8],"int64"), ) 	 76205064 	 46945 	 51.865899324417114 	 48.04120707511902 	 1.129164457321167 	 1.0458605289459229 	 None 	 None 	 None 	 None 	 
2025-08-05 00:59:13.607555 test begin: paddle.not_equal(Tensor([13, 30531, 16, 4, 1],"int64"), Tensor([13, 30531, 16, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 30531, 16, 4, 1],"int64"), Tensor([13, 30531, 16, 1, 8],"int64"), ) 	 76205376 	 46945 	 51.45756196975708 	 48.04273533821106 	 1.1202564239501953 	 1.0459086894989014 	 None 	 None 	 None 	 None 	 
2025-08-05 01:00:54.529343 test begin: paddle.not_equal(Tensor([198451, 2, 16, 4, 1],"int64"), Tensor([198451, 2, 16, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([198451, 2, 16, 4, 1],"int64"), Tensor([198451, 2, 16, 1, 8],"int64"), ) 	 76205184 	 46945 	 51.45700168609619 	 48.09681415557861 	 1.1200456619262695 	 1.0996456146240234 	 None 	 None 	 None 	 None 	 
2025-08-05 01:02:38.747358 test begin: paddle.not_equal(Tensor([25401601],"int64"), Tensor([25401601],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([25401601],"int64"), Tensor([25401601],"int64"), ) 	 50803202 	 46945 	 14.562541484832764 	 14.691752433776855 	 0.31664276123046875 	 0.31986117362976074 	 None 	 None 	 None 	 None 	 
2025-08-05 01:03:12.337496 test begin: paddle.not_equal(Tensor([99226, 2, 16, 4, 1],"int64"), Tensor([99226, 2, 16, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([99226, 2, 16, 4, 1],"int64"), Tensor([99226, 2, 16, 1, 8],"int64"), ) 	 38102784 	 46945 	 26.11940860748291 	 25.600111722946167 	 0.5686166286468506 	 0.5260879993438721 	 None 	 None 	 None 	 None 	 
2025-08-05 01:04:06.540205 test begin: paddle.numel(Tensor([508032010],"float32"), )
[Prof] paddle.numel 	 paddle.numel(Tensor([508032010],"float32"), ) 	 508032010 	 1155041 	 10.86623239517212 	 32.12418460845947 	 0.0001571178436279297 	 0.00022554397583007812 	 None 	 None 	 None 	 None 	 
2025-08-05 01:04:58.668013 test begin: paddle.ones_like(Tensor([144, 392, 901],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([144, 392, 901],"float32"), ) 	 50859648 	 74688 	 10.009101629257202 	 10.017731666564941 	 0.1369326114654541 	 0.13700151443481445 	 None 	 None 	 None 	 None 	 
2025-08-05 01:05:19.686632 test begin: paddle.ones_like(Tensor([144, 901, 392],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([144, 901, 392],"float32"), ) 	 50859648 	 74688 	 10.012058973312378 	 10.025993824005127 	 0.13692975044250488 	 0.13702082633972168 	 None 	 None 	 None 	 None 	 
2025-08-05 01:05:41.362147 test begin: paddle.ones_like(Tensor([160, 392, 811],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([160, 392, 811],"float32"), ) 	 50865920 	 74688 	 10.011142015457153 	 10.020390033721924 	 0.13698053359985352 	 0.13703393936157227 	 None 	 None 	 None 	 None 	 
2025-08-05 01:06:02.405312 test begin: paddle.ones_like(Tensor([160, 811, 392],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([160, 811, 392],"float32"), ) 	 50865920 	 74688 	 10.011102676391602 	 10.018208503723145 	 0.13695812225341797 	 0.1370394229888916 	 None 	 None 	 None 	 None 	 
2025-08-05 01:06:23.300330 test begin: paddle.ones_like(Tensor([176, 392, 737],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([176, 392, 737],"float32"), ) 	 50847104 	 74688 	 10.003856420516968 	 10.024357795715332 	 0.13686609268188477 	 0.13701272010803223 	 None 	 None 	 None 	 None 	 
2025-08-05 01:06:44.212797 test begin: paddle.ones_like(Tensor([176, 737, 392],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([176, 737, 392],"float32"), ) 	 50847104 	 74688 	 10.01236343383789 	 10.019479036331177 	 0.13688325881958008 	 0.13695645332336426 	 None 	 None 	 None 	 None 	 
2025-08-05 01:07:05.072915 test begin: paddle.ones_like(Tensor([331, 392, 392],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([331, 392, 392],"float32"), ) 	 50862784 	 74688 	 10.004583358764648 	 10.016695022583008 	 0.13681292533874512 	 0.13700175285339355 	 None 	 None 	 None 	 None 	 
2025-08-05 01:07:25.897016 test begin: paddle.outer(Tensor([50803201],"float32"), Tensor([2],"float32"), )
[Prof] paddle.outer 	 paddle.outer(Tensor([50803201],"float32"), Tensor([2],"float32"), ) 	 50803203 	 1897 	 10.011300802230835 	 0.9879701137542725 	 0.10995268821716309 	 0.5290324687957764 	 7.090595483779907 	 4.954932928085327 	 1.2712411880493164 	 0.5330555438995361 	 
2025-08-05 01:07:51.893494 test begin: paddle.outer(Tensor([50803201],"float32"), Tensor([3],"float32"), )
[Prof] paddle.outer 	 paddle.outer(Tensor([50803201],"float32"), Tensor([3],"float32"), ) 	 50803204 	 1897 	 10.024179935455322 	 1.3939697742462158 	 0.11021924018859863 	 0.7409305572509766 	 8.703919649124146 	 6.551191806793213 	 1.5608031749725342 	 0.7049560546875 	 
2025-08-05 01:08:21.935798 test begin: paddle.outer(Tensor([50803201],"float32"), Tensor([4],"float32"), )
[Prof] paddle.outer 	 paddle.outer(Tensor([50803201],"float32"), Tensor([4],"float32"), ) 	 50803205 	 1897 	 10.033005475997925 	 1.7936773300170898 	 0.11028742790222168 	 0.9652135372161865 	 8.363638877868652 	 9.599733591079712 	 1.499847173690796 	 1.032850980758667 	 
2025-08-05 01:08:57.160643 test begin: paddle.pdist(Tensor([10, 5080321],"float32"), 0, )
[Prof] paddle.pdist 	 paddle.pdist(Tensor([10, 5080321],"float32"), 0, ) 	 50803210 	 1686 	 9.9979727268219 	 15.193037271499634 	 3.504753112792969e-05 	 9.208964109420776 	 9.4873685836792 	 0.22589826583862305 	 0.005463838577270508 	 0.11394381523132324 	 
2025-08-05 01:09:33.000422 test begin: paddle.pdist(Tensor([10, 5080321],"float32"), 1.0, )
[Prof] paddle.pdist 	 paddle.pdist(Tensor([10, 5080321],"float32"), 1.0, ) 	 50803210 	 1686 	 9.971309423446655 	 14.11426568031311 	 3.147125244140625e-05 	 8.555938720703125 	 38.44164752960205 	 22.823981285095215 	 0.02268052101135254 	 6.9190590381622314 	 
2025-08-05 01:11:01.912681 test begin: paddle.pdist(Tensor([50, 508033],"float64"), 2.0, )
[Prof] paddle.pdist 	 paddle.pdist(Tensor([50, 508033],"float64"), 2.0, ) 	 25401650 	 1686 	 38.70570969581604 	 3.6187546253204346 	 5.054473876953125e-05 	 2.193223237991333 	 148.22783160209656 	 66.27949357032776 	 0.08772587776184082 	 4.448171138763428 	 
2025-08-05 01:15:19.601181 test begin: paddle.polar(Tensor([1, 793801, 64],"float32"), Tensor([1, 793801, 64],"float32"), )
[Prof] paddle.polar 	 paddle.polar(Tensor([1, 793801, 64],"float32"), Tensor([1, 793801, 64],"float32"), ) 	 101606528 	 8265 	 17.202122688293457 	 17.13818049430847 	 0.42536497116088867 	 0.42398953437805176 	 38.96217322349548 	 41.61622142791748 	 0.6883683204650879 	 0.46764540672302246 	 combined
2025-08-05 01:17:19.196174 test begin: paddle.polar(Tensor([1, 8192, 6202],"float32"), Tensor([1, 8192, 6202],"float32"), )
[Prof] paddle.polar 	 paddle.polar(Tensor([1, 8192, 6202],"float32"), Tensor([1, 8192, 6202],"float32"), ) 	 101613568 	 8265 	 20.265568494796753 	 17.183431386947632 	 0.42563772201538086 	 0.4240438938140869 	 39.013771772384644 	 41.62254858016968 	 0.6893174648284912 	 0.4677388668060303 	 combined
2025-08-05 01:19:28.493447 test begin: paddle.polar(Tensor([1, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), )
[Prof] paddle.polar 	 paddle.polar(Tensor([1, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), ) 	 51380224 	 8265 	 14.824942827224731 	 14.906185626983643 	 0.3667776584625244 	 0.36859726905822754 	 29.87622904777527 	 38.43473505973816 	 0.3970489501953125 	 0.3653903007507324 	 combined
2025-08-05 01:21:11.209483 test begin: paddle.polar(Tensor([97, 8192, 64],"float32"), Tensor([1, 8192, 64],"float32"), )
[Prof] paddle.polar 	 paddle.polar(Tensor([97, 8192, 64],"float32"), Tensor([1, 8192, 64],"float32"), ) 	 51380224 	 8265 	 9.99320387840271 	 10.065341472625732 	 0.2475261688232422 	 0.24842333793640137 	 21.574690341949463 	 23.911505222320557 	 0.2956123352050781 	 0.22730040550231934 	 combined
2025-08-05 01:22:18.842370 test begin: paddle.polar(Tensor([97, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), )
[Prof] paddle.polar 	 paddle.polar(Tensor([97, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), ) 	 101711872 	 8265 	 17.200603485107422 	 17.186111211776733 	 0.42560696601867676 	 0.42449116706848145 	 39.05626463890076 	 41.66146278381348 	 0.6899275779724121 	 0.46822214126586914 	 combined
2025-08-05 01:24:19.259252 test begin: paddle.polygamma(Tensor([10, 20, 254017],"float32"), 1, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([10, 20, 254017],"float32"), 1, ) 	 50803400 	 1793 	 9.99633526802063 	 1.1325418949127197 	 5.697709560394287 	 0.6403236389160156 	 16.279085397720337 	 18.38855218887329 	 9.278825283050537 	 5.2407965660095215 	 
2025-08-05 01:25:06.860240 test begin: paddle.polygamma(Tensor([10, 5080321, 1],"float32"), 1, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([10, 5080321, 1],"float32"), 1, ) 	 50803210 	 1793 	 9.995641231536865 	 1.869507074356079 	 5.6971354484558105 	 0.6404047012329102 	 16.27438521385193 	 18.387356758117676 	 9.276340246200562 	 5.240780353546143 	 
2025-08-05 01:25:57.510814 test begin: paddle.polygamma(Tensor([2, 12700801],"float64"), 1, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([2, 12700801],"float64"), 1, ) 	 25401602 	 1793 	 16.576516151428223 	 1.9958174228668213 	 9.449194431304932 	 0.6544528007507324 	 20.479035139083862 	 38.53281092643738 	 11.674380540847778 	 10.98145079612732 	 
2025-08-05 01:27:20.707927 test begin: paddle.polygamma(Tensor([2, 2, 6350401],"float64"), 2, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([2, 2, 6350401],"float64"), 2, ) 	 25401604 	 1793 	 20.34970784187317 	 38.52938914299011 	 11.598924398422241 	 21.51896858215332 	 16.494867086410522 	 16.63991928100586 	 9.468101263046265 	 4.737516641616821 	 
2025-08-05 01:28:55.724578 test begin: paddle.polygamma(Tensor([2, 2116801, 6],"float64"), 2, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([2, 2116801, 6],"float64"), 2, ) 	 25401612 	 1793 	 20.361844301223755 	 37.725990295410156 	 11.607284307479858 	 21.503493547439575 	 16.353839635849 	 16.631376028060913 	 9.321701526641846 	 4.73905611038208 	 
2025-08-05 01:30:28.427691 test begin: paddle.polygamma(Tensor([2116801, 2, 6],"float64"), 2, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([2116801, 2, 6],"float64"), 2, ) 	 25401612 	 1793 	 20.36294722557068 	 37.74262857437134 	 11.606711149215698 	 21.511348009109497 	 16.353120803833008 	 16.63784384727478 	 9.320963382720947 	 4.743152141571045 	 
2025-08-05 01:32:03.316036 test begin: paddle.polygamma(Tensor([2540161, 20, 1],"float32"), 1, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([2540161, 20, 1],"float32"), 1, ) 	 50803220 	 1793 	 9.994978189468384 	 1.499208688735962 	 5.696364641189575 	 0.640310525894165 	 16.274691581726074 	 18.3977108001709 	 9.27629041671753 	 5.243351221084595 	 
2025-08-05 01:32:52.236947 test begin: paddle.polygamma(Tensor([4233601, 6],"float64"), 1, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([4233601, 6],"float64"), 1, ) 	 25401606 	 1793 	 16.576474905014038 	 1.1492223739624023 	 9.448646306991577 	 0.6540746688842773 	 20.47741723060608 	 38.52803087234497 	 11.671597003936768 	 10.980311155319214 	 
2025-08-05 01:34:10.231780 test begin: paddle.positive(Tensor([100, 5080321],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([100, 5080321],"float32"), ) 	 508032100 	 5479885 	 10.27191424369812 	 1.0298969745635986 	 0.0001583099365234375 	 0.00014019012451171875 	 182.42467498779297 	 308.884859085083 	 0.00035500526428222656 	 0.0006568431854248047 	 combined
2025-08-05 01:42:53.125138 test begin: paddle.positive(Tensor([16934410, 3, 4, 5],"float16"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([16934410, 3, 4, 5],"float16"), ) 	 1016064600 	 5479885 	 12.27768850326538 	 1.0323739051818848 	 0.0004286766052246094 	 0.00010395050048828125 	 159.43282175064087 	 266.3716208934784 	 0.00014352798461914062 	 0.0002617835998535156 	 combined
2025-08-05 01:50:55.583860 test begin: paddle.positive(Tensor([20, 1270081, 4, 5],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 1270081, 4, 5],"float32"), ) 	 508032400 	 5479885 	 9.605868101119995 	 1.0253667831420898 	 0.00014829635620117188 	 6.890296936035156e-05 	 158.3304626941681 	 260.2590937614441 	 0.00011682510375976562 	 0.00022864341735839844 	 combined
2025-08-05 01:58:21.392241 test begin: paddle.positive(Tensor([20, 2540161, 4, 5],"float16"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 2540161, 4, 5],"float16"), ) 	 1016064400 	 5479885 	 9.58934497833252 	 1.0245118141174316 	 0.00011730194091796875 	 4.7206878662109375e-05 	 160.20214295387268 	 259.7173681259155 	 0.00011515617370605469 	 0.00023603439331054688 	 combined
2025-08-05 02:06:09.268002 test begin: paddle.positive(Tensor([20, 3, 1693441, 5],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 3, 1693441, 5],"float32"), ) 	 508032300 	 5479885 	 9.90481948852539 	 1.0294806957244873 	 0.0001659393310546875 	 9.703636169433594e-05 	 158.0482783317566 	 260.255571603775 	 0.00011730194091796875 	 0.00022268295288085938 	 combined
2025-08-05 02:13:38.851802 test begin: paddle.positive(Tensor([20, 3, 3386881, 5],"float16"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 3, 3386881, 5],"float16"), ) 	 1016064300 	 5479885 	 9.643074989318848 	 1.0314874649047852 	 0.00013327598571777344 	 0.00023484230041503906 	 158.25432801246643 	 260.4007833003998 	 0.00011730194091796875 	 0.00022411346435546875 	 combined
2025-08-05 02:21:26.258763 test begin: paddle.positive(Tensor([20, 3, 4, 2116801],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 3, 4, 2116801],"float32"), ) 	 508032240 	 5479885 	 9.581484079360962 	 1.024874210357666 	 6.794929504394531e-05 	 8.988380432128906e-05 	 158.61107659339905 	 261.0218183994293 	 0.00012302398681640625 	 0.00022792816162109375 	 combined
2025-08-05 02:28:53.089685 test begin: paddle.positive(Tensor([20, 3, 4, 4233601],"float16"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 3, 4, 4233601],"float16"), ) 	 1016064240 	 5479885 	 9.535363674163818 	 1.0235786437988281 	 0.00012612342834472656 	 5.030632019042969e-05 	 157.68063139915466 	 259.400963306427 	 0.00011444091796875 	 0.0002257823944091797 	 combined
2025-08-05 02:36:38.830270 test begin: paddle.positive(Tensor([496130, 1024],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([496130, 1024],"float32"), ) 	 508037120 	 5479885 	 9.552762746810913 	 1.023751974105835 	 6.508827209472656e-05 	 6.127357482910156e-05 	 158.88025331497192 	 254.43957424163818 	 0.00013399124145507812 	 0.0002315044403076172 	 combined
2025-08-05 02:43:59.405400 test begin: paddle.positive(Tensor([8467210, 3, 4, 5],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([8467210, 3, 4, 5],"float32"), ) 	 508032600 	 5479885 	 9.5470871925354 	 1.030517816543579 	 0.00010037422180175781 	 0.0002453327178955078 	 158.73024702072144 	 260.75225138664246 	 0.00011730194091796875 	 0.00022554397583007812 	 combined
2025-08-05 02:51:26.714281 test begin: paddle.pow(Tensor([1024, 1024, 25],"float64"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([1024, 1024, 25],"float64"), 2, ) 	 26214400 	 27143 	 16.006128549575806 	 8.34530258178711 	 0.6026937961578369 	 0.31413888931274414 	 16.909759998321533 	 29.429006814956665 	 0.6366627216339111 	 0.36932992935180664 	 
2025-08-05 02:52:38.928319 test begin: paddle.pow(Tensor([1024, 1024, 49],"float32"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([1024, 1024, 49],"float32"), 2, ) 	 51380224 	 27143 	 10.111443519592285 	 8.16652250289917 	 0.3807487487792969 	 0.30751514434814453 	 12.414543390274048 	 28.882837295532227 	 0.4675416946411133 	 0.36253952980041504 	 
2025-08-05 02:53:41.553893 test begin: paddle.pow(Tensor([1024, 3101, 8],"float64"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([1024, 3101, 8],"float64"), 2, ) 	 25403392 	 27143 	 15.526939868927002 	 8.092845916748047 	 0.5845949649810791 	 0.30469441413879395 	 16.380916357040405 	 28.53176259994507 	 0.6167619228363037 	 0.35808348655700684 	 
2025-08-05 02:54:51.289932 test begin: paddle.pow(Tensor([1024, 6202, 8],"float32"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([1024, 6202, 8],"float32"), 2, ) 	 50806784 	 27143 	 9.995384931564331 	 8.078665018081665 	 0.3763725757598877 	 0.3041543960571289 	 12.276917219161987 	 28.566125631332397 	 0.4624001979827881 	 0.35858893394470215 	 
2025-08-05 02:55:53.287495 test begin: paddle.pow(Tensor([22, 81, 94, 311],"float32"), 2.0, )
[Prof] paddle.pow 	 paddle.pow(Tensor([22, 81, 94, 311],"float32"), 2.0, ) 	 52094988 	 27143 	 10.255388975143433 	 8.278329372406006 	 0.3861503601074219 	 0.3116309642791748 	 12.574605226516724 	 29.3617947101593 	 0.47343015670776367 	 0.2765672206878662 	 
2025-08-05 02:56:56.400172 test begin: paddle.pow(Tensor([3101, 1024, 8],"float64"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([3101, 1024, 8],"float64"), 2, ) 	 25403392 	 27143 	 15.521044492721558 	 8.092893600463867 	 0.584416389465332 	 0.30469775199890137 	 16.377272129058838 	 28.532002449035645 	 0.6166481971740723 	 0.3581538200378418 	 
2025-08-05 02:58:06.973778 test begin: paddle.pow(Tensor([4, 435, 94, 311],"float32"), 2.0, )
[Prof] paddle.pow 	 paddle.pow(Tensor([4, 435, 94, 311],"float32"), 2.0, ) 	 50867160 	 27143 	 10.00618577003479 	 8.088240385055542 	 0.3767681121826172 	 0.304476261138916 	 12.287630081176758 	 28.679356813430786 	 0.4624812602996826 	 0.27013516426086426 	 
2025-08-05 02:59:09.650585 test begin: paddle.pow(Tensor([4, 81, 505, 311],"float32"), 2.0, )
[Prof] paddle.pow 	 paddle.pow(Tensor([4, 81, 505, 311],"float32"), 2.0, ) 	 50885820 	 27143 	 10.019892454147339 	 8.09106159210205 	 0.37729907035827637 	 0.30469226837158203 	 12.28854489326477 	 28.683155059814453 	 0.4627554416656494 	 0.27019238471984863 	 
2025-08-05 03:00:10.470258 test begin: paddle.pow(Tensor([4, 81, 94, 1669],"float32"), 2.0, )
[Prof] paddle.pow 	 paddle.pow(Tensor([4, 81, 94, 1669],"float32"), 2.0, ) 	 50831064 	 27143 	 9.999866008758545 	 8.087292194366455 	 0.3765265941619873 	 0.30432939529418945 	 12.279696464538574 	 28.664427518844604 	 0.46242451667785645 	 0.27007055282592773 	 
2025-08-05 03:01:12.776901 test begin: paddle.pow(Tensor([6202, 1024, 8],"float32"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([6202, 1024, 8],"float32"), 2, ) 	 50806784 	 27143 	 9.994486570358276 	 8.711864471435547 	 0.3763296604156494 	 0.304210901260376 	 12.276688814163208 	 28.566683530807495 	 0.46216630935668945 	 0.3585548400878906 	 
2025-08-05 03:02:14.725748 test begin: paddle.prod(Tensor([10, 10, 28225, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
Warning: The core code of paddle.prod is too complex.
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 10, 28225, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402502 	 68328 	 21.125643014907837 	 2.4422338008880615 	 0.000202178955078125 	 0.00022673606872558594 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([10]) and output[0] has a shape of torch.Size([1, 10, 1, 1]).
2025-08-05 03:03:37.518754 test begin: paddle.prod(Tensor([10, 10, 9, 28225],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 10, 9, 28225],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402502 	 68328 	 22.396135091781616 	 2.4812424182891846 	 0.00020265579223632812 	 0.0001125335693359375 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([9]) and output[0] has a shape of torch.Size([1, 1, 9, 1]).
2025-08-05 03:05:01.150224 test begin: paddle.prod(Tensor([10, 31361, 9, 9],"float64"), Tensor([2],"int64"), )
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 31361, 9, 9],"float64"), Tensor([2],"int64"), ) 	 25402412 	 68328 	 14.690432071685791 	 1.6394751071929932 	 0.00016999244689941406 	 7.772445678710938e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([10, 9]) and output[0] has a shape of torch.Size([10, 1, 9, 1]).
2025-08-05 03:06:16.073169 test begin: paddle.prod(Tensor([10, 31361, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 31361, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402412 	 68328 	 379.6266222000122 	 2.4267849922180176 	 0.005438804626464844 	 7.677078247070312e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([9]) and output[0] has a shape of torch.Size([1, 1, 1, 9]).
2025-08-05 03:13:38.598106 test begin: paddle.prod(Tensor([10, 5, 56449, 9],"float64"), Tensor([2],"int64"), )
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 5, 56449, 9],"float64"), Tensor([2],"int64"), ) 	 25402052 	 68328 	 13.740103721618652 	 1.6617119312286377 	 0.00014829635620117188 	 7.677078247070312e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([56449, 9]) and output[0] has a shape of torch.Size([1, 1, 56449, 9]).
2025-08-05 03:14:52.722331 test begin: paddle.prod(Tensor([10, 5, 9, 56449],"float64"), Tensor([2],"int64"), )
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 5, 9, 56449],"float64"), Tensor([2],"int64"), ) 	 25402052 	 68328 	 13.654380559921265 	 2.57429838180542 	 0.0001671314239501953 	 0.0005640983581542969 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([9, 56449]) and output[0] has a shape of torch.Size([1, 1, 9, 56449]).
2025-08-05 03:16:10.504065 test begin: paddle.prod(Tensor([16, 3175201],"float32"), -1, )
[Prof] paddle.prod 	 paddle.prod(Tensor([16, 3175201],"float32"), -1, ) 	 50803216 	 68328 	 12.190561294555664 	 10.41983675956726 	 0.0911705493927002 	 0.07790017127990723 	 54.06626534461975 	 107.97843337059021 	 0.8086814880371094 	 0.0006184577941894531 	 
2025-08-05 03:19:16.258011 test begin: paddle.prod(Tensor([31361, 10, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.prod 	 paddle.prod(Tensor([31361, 10, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402412 	 68328 	 379.7954914569855 	 2.4583117961883545 	 0.00543665885925293 	 0.00011706352233886719 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([9]) and output[0] has a shape of torch.Size([1, 1, 1, 9]).
2025-08-05 03:26:37.863518 test begin: paddle.prod(Tensor([49613, 1024],"float32"), -1, )
[Prof] paddle.prod 	 paddle.prod(Tensor([49613, 1024],"float32"), -1, ) 	 50803712 	 68328 	 9.993772268295288 	 10.054569482803345 	 0.1494579315185547 	 0.15044260025024414 	 54.07608652114868 	 108.9839882850647 	 0.8087584972381592 	 0.0006289482116699219 	 
2025-08-05 03:29:41.939405 test begin: paddle.prod(Tensor([62721, 5, 9, 9],"float64"), Tensor([2],"int64"), )
[Prof] paddle.prod 	 paddle.prod(Tensor([62721, 5, 9, 9],"float64"), Tensor([2],"int64"), ) 	 25402007 	 68328 	 38.40588569641113 	 1.6832983493804932 	 0.0005171298980712891 	 9.417533874511719e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([62721, 9]) and output[0] has a shape of torch.Size([62721, 1, 1, 9]).
2025-08-05 03:31:20.768474 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 1016065, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6b788b3e80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 03:41:25.694866 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([2032129, 5, 5],"float32"), 1, "mul", True, False, )
W0805 03:41:25.892338 83077 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([2032129, 5, 5],"float32"), 1, "mul", True, False, ) 	 50804350 	 77440 	 12.162827491760254 	 2.9666759967803955 	 5.5789947509765625e-05 	 7.414817810058594e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 03:41:51.414072 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 2032129, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 2032129, 5],"float32"), 1, "mul", True, False, ) 	 50804350 	 77440 	 11.489330530166626 	 2.8852999210357666 	 6.0558319091796875e-05 	 7.700920104980469e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 03:42:13.560678 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 2032129],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 2032129],"float32"), 1, "mul", True, False, ) 	 50804350 	 77440 	 9.655601501464844 	 2.00236177444458 	 5.412101745605469e-05 	 7.367134094238281e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 03:42:31.703895 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 2032129, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd477517460>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 03:52:36.566833 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([2032129, 5, 5],"int32"), 1, "mul", True, False, )
W0805 03:52:42.666543 83469 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([2032129, 5, 5],"int32"), 1, "mul", True, False, ) 	 50804350 	 77440 	 10.116374969482422 	 2.0890238285064697 	 5.340576171875e-05 	 0.00010442733764648438 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:53:02.197461 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 2032129, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 2032129, 5],"int32"), 1, "mul", True, False, ) 	 50804350 	 77440 	 12.760804653167725 	 2.0085349082946777 	 6.389617919921875e-05 	 8.916854858398438e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:53:25.158294 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 2032129],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 2032129],"int32"), 1, "mul", True, False, ) 	 50804350 	 77440 	 10.170382022857666 	 1.9993798732757568 	 5.1021575927734375e-05 	 0.00010538101196289062 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:53:44.313322 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 1016065, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6eac2728c0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 04:03:48.976672 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([1016065, 5, 5],"int64"), 1, "mul", True, False, )
W0805 04:03:49.220860 86417 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([1016065, 5, 5],"int64"), 1, "mul", True, False, ) 	 25402750 	 77440 	 12.589717149734497 	 2.2371842861175537 	 7.43865966796875e-05 	 0.00023126602172851562 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:04:12.386238 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 1016065, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 1016065, 5],"int64"), 1, "mul", True, False, ) 	 25402750 	 77440 	 13.146268606185913 	 2.1259994506835938 	 8.845329284667969e-05 	 0.00011491775512695312 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:04:35.871136 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 1016065],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 1016065],"int64"), 1, "mul", True, False, ) 	 25402750 	 77440 	 10.846001625061035 	 2.1284046173095703 	 6.67572021484375e-05 	 0.00011682510375976562 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:04:55.891353 test begin: paddle.put_along_axis(Tensor([10, 10, 254017],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 254017],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, ) 	 25401950 	 77440 	 29.221304893493652 	 48.86660623550415 	 0.0002942085266113281 	 0.1286473274230957 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:06:45.681106 test begin: paddle.put_along_axis(Tensor([10, 10, 508033],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 508033],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, ) 	 50803550 	 77440 	 29.087523221969604 	 48.85477304458618 	 0.00029397010803222656 	 0.1286942958831787 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:08:41.429370 test begin: paddle.put_along_axis(Tensor([10, 10, 508033],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 508033],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, ) 	 50803550 	 77440 	 28.991561889648438 	 48.84329700469971 	 0.0002875328063964844 	 0.12860727310180664 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:10:37.805770 test begin: paddle.put_along_axis(Tensor([10, 254017, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 254017, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, ) 	 25401950 	 77440 	 29.18209457397461 	 48.79859900474548 	 0.00029206275939941406 	 0.12853026390075684 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:12:32.260266 test begin: paddle.put_along_axis(Tensor([10, 508033, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 508033, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, ) 	 50803550 	 77440 	 29.169676780700684 	 48.80391597747803 	 0.0002918243408203125 	 0.12852239608764648 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:14:30.014384 test begin: paddle.put_along_axis(Tensor([10, 508033, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 508033, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, ) 	 50803550 	 77440 	 29.08421778678894 	 48.80275821685791 	 0.0002913475036621094 	 0.12850570678710938 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:16:26.938581 test begin: paddle.put_along_axis(Tensor([254017, 10, 10],"int64"), Tensor([254017, 5, 5],"int64"), Tensor([254017, 5, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([254017, 10, 10],"int64"), Tensor([254017, 5, 5],"int64"), Tensor([254017, 5, 5],"int64"), 1, "mul", True, False, ) 	 38102550 	 77440 	 64.3961546421051 	 79.74957585334778 	 0.0006105899810791016 	 0.21043109893798828 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:21:02.771442 test begin: paddle.put_along_axis(Tensor([254017, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([254017, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, ) 	 25401950 	 77440 	 29.49022150039673 	 48.845738887786865 	 0.0002923011779785156 	 0.12860846519470215 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:22:53.650346 test begin: paddle.put_along_axis(Tensor([508033, 10, 10],"float32"), Tensor([254017, 5, 5],"int64"), Tensor([508033, 5, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([508033, 10, 10],"float32"), Tensor([254017, 5, 5],"int64"), Tensor([508033, 5, 5],"float32"), 1, "mul", True, False, ) 	 69854550 	 77440 	 60.19938254356384 	 73.97086930274963 	 0.0005669593811035156 	 0.19510698318481445 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:26:55.382726 test begin: paddle.put_along_axis(Tensor([508033, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([508033, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, ) 	 50803550 	 77440 	 29.069905281066895 	 48.865153074264526 	 0.00029277801513671875 	 0.12870025634765625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:28:51.059058 test begin: paddle.put_along_axis(Tensor([508033, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([508033, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, ) 	 50803550 	 77440 	 28.99182105064392 	 48.824146032333374 	 0.00028777122497558594 	 0.1285722255706787 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:30:46.144631 test begin: paddle.put_along_axis(Tensor([508033, 10, 10],"int32"), Tensor([508033, 5, 5],"int32"), Tensor([508033, 5, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([508033, 10, 10],"int32"), Tensor([508033, 5, 5],"int32"), Tensor([508033, 5, 5],"int32"), 1, "mul", True, False, ) 	 76204950 	 77440 	 81.09910011291504 	 97.50342535972595 	 0.0008242130279541016 	 0.2575252056121826 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:36:31.061856 test begin: paddle.rad2deg(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33829 	 10.00433874130249 	 10.070066928863525 	 0.3021047115325928 	 0.30414819717407227 	 10.014358043670654 	 10.069969654083252 	 0.3024919033050537 	 0.30416440963745117 	 
2025-08-05 04:37:12.890865 test begin: paddle.rad2deg(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33829 	 10.015461444854736 	 10.068893432617188 	 0.3025188446044922 	 0.30416059494018555 	 10.014382123947144 	 10.071213960647583 	 0.3024940490722656 	 0.30541372299194336 	 
2025-08-05 04:37:54.744483 test begin: paddle.rad2deg(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33829 	 10.018924474716187 	 10.072125911712646 	 0.3025496006011963 	 0.30411791801452637 	 10.015795230865479 	 10.070251941680908 	 0.30251240730285645 	 0.3041977882385254 	 
2025-08-05 04:38:38.033535 test begin: paddle.rad2deg(x=Tensor([1587601, 4, 4],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([1587601, 4, 4],"float64"), ) 	 25401616 	 33829 	 10.806124687194824 	 10.08927059173584 	 0.30445170402526855 	 0.30467748641967773 	 10.076850175857544 	 10.092032194137573 	 0.3043186664581299 	 0.30476951599121094 	 
2025-08-05 04:39:22.598999 test begin: paddle.rad2deg(x=Tensor([396901, 4, 4, 4],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 33829 	 10.080418348312378 	 10.088557004928589 	 0.3044435977935791 	 0.30466485023498535 	 10.0743727684021 	 10.094314336776733 	 0.3042945861816406 	 0.30623650550842285 	 
2025-08-05 04:40:04.047928 test begin: paddle.rad2deg(x=Tensor([4, 1587601, 4],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([4, 1587601, 4],"float64"), ) 	 25401616 	 33829 	 10.08048963546753 	 10.089006662368774 	 0.30443763732910156 	 0.3047780990600586 	 10.075232028961182 	 10.090981483459473 	 0.3042795658111572 	 0.30483055114746094 	 
2025-08-05 04:40:45.477374 test begin: paddle.rad2deg(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 33829 	 10.081189393997192 	 10.104154586791992 	 0.3044278621673584 	 0.30477428436279297 	 10.075669765472412 	 10.092014074325562 	 0.3043220043182373 	 0.30475807189941406 	 
2025-08-05 04:41:28.242575 test begin: paddle.rad2deg(x=Tensor([4, 4, 1587601],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([4, 4, 1587601],"float64"), ) 	 25401616 	 33829 	 10.079779863357544 	 10.090101480484009 	 0.3044898509979248 	 0.30474376678466797 	 10.074337005615234 	 10.091918706893921 	 0.3043050765991211 	 0.3047621250152588 	 
2025-08-05 04:42:10.613886 test begin: paddle.rad2deg(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 33829 	 10.079600095748901 	 10.086549282073975 	 0.30446767807006836 	 0.3047311305999756 	 10.07676076889038 	 10.092287302017212 	 0.3042769432067871 	 0.30482053756713867 	 
2025-08-05 04:42:53.615444 test begin: paddle.rad2deg(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 33829 	 10.081906795501709 	 10.088706493377686 	 0.3044157028198242 	 0.3046441078186035 	 10.075453042984009 	 10.093660831451416 	 0.304293155670166 	 0.3060910701751709 	 
2025-08-05 04:43:35.093922 test begin: paddle.rank(input=Tensor([1270080101, 2],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([1270080101, 2],"float64"), ) 	 2540160202 	 247008 	 10.21635627746582 	 6.963770627975464 	 4.9591064453125e-05 	 6.103515625e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 11:30:53.365875 test begin: paddle.Tensor.__abs__(Tensor([10, 5080321],"float32"), )
W0804 11:30:54.696291 85198 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.__abs__ 	 paddle.Tensor.__abs__(Tensor([10, 5080321],"float32"), ) 	 50803210 	 33766 	 9.997885704040527 	 10.052088975906372 	 0.3025226593017578 	 0.30418968200683594 	 15.202509880065918 	 25.081366539001465 	 0.4600543975830078 	 0.3796241283416748 	 
2025-08-04 11:31:56.642193 test begin: paddle.Tensor.__abs__(Tensor([49613, 1024],"float32"), )
[Prof] paddle.Tensor.__abs__ 	 paddle.Tensor.__abs__(Tensor([49613, 1024],"float32"), ) 	 50803712 	 33766 	 9.999738693237305 	 10.0496187210083 	 0.3026707172393799 	 0.30415987968444824 	 15.202900648117065 	 25.080904006958008 	 0.4602041244506836 	 0.37953925132751465 	 
2025-08-04 11:32:59.313597 test begin: paddle.Tensor.__abs__(Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.__abs__ 	 paddle.Tensor.__abs__(Tensor([50803201],"float32"), ) 	 50803201 	 33766 	 9.993688106536865 	 10.049637794494629 	 0.3024609088897705 	 0.304185152053833 	 15.20241641998291 	 25.08090329170227 	 0.46019816398620605 	 0.3795607089996338 	 
2025-08-04 11:34:01.823400 test begin: paddle.Tensor.__add__(Tensor([1, 32, 388, 4096],"float32"), Tensor([1, 1, 388, 4096],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([1, 32, 388, 4096],"float32"), Tensor([1, 1, 388, 4096],"float32"), ) 	 52445184 	 29759 	 10.012148141860962 	 9.636305093765259 	 0.3436589241027832 	 0.330935001373291 	 14.186323881149292 	 4.576326370239258 	 0.2435765266418457 	 0.15714812278747559 	 
2025-08-04 11:34:45.215880 test begin: paddle.Tensor.__add__(Tensor([1, 32, 4096, 388],"float32"), Tensor([1, 1, 4096, 388],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([1, 32, 4096, 388],"float32"), Tensor([1, 1, 4096, 388],"float32"), ) 	 52445184 	 29759 	 10.007161378860474 	 9.650649785995483 	 0.3430900573730469 	 0.331026554107666 	 14.179848670959473 	 4.572794198989868 	 0.24347829818725586 	 0.15697860717773438 	 
2025-08-04 11:35:25.997671 test begin: paddle.Tensor.__add__(Tensor([1, 32, 4096, 4096],"float32"), Tensor([1, 1, 4096, 4096],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([1, 32, 4096, 4096],"float32"), Tensor([1, 1, 4096, 4096],"float32"), ) 	 553648128 	 29759 	 139.6595299243927 	 137.63259267807007 	 4.796314239501953 	 4.72664999961853 	 147.7813937664032 	 46.421629190444946 	 1.6901891231536865 	 1.5942270755767822 	 
2025-08-04 11:43:38.094378 test begin: paddle.Tensor.__add__(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 1, 4096, 4096],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 1, 4096, 4096],"float32"), ) 	 83886080 	 29759 	 17.651190519332886 	 17.349842309951782 	 0.6067707538604736 	 0.5958266258239746 	 19.742506742477417 	 7.2607338428497314 	 0.3390023708343506 	 0.24935674667358398 	 
2025-08-04 11:44:42.815215 test begin: paddle.Tensor.__add__(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 4, 4096, 4096],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 4, 4096, 4096],"float32"), ) 	 134217728 	 29759 	 17.66202449798584 	 17.501050233840942 	 0.6065356731414795 	 0.6010630130767822 	 18.939417839050293 	 1.6632261276245117 	 0.6503520011901855 	 8.463859558105469e-05 	 
2025-08-04 11:45:43.215881 test begin: paddle.Tensor.__add__(Tensor([2, 256, 336, 336],"float16"), Tensor([2, 256, 336, 336],"float32"), )
W0804 11:45:45.202653 90785 dygraph_functions.cc:87088] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([2, 256, 336, 336],"float16"), Tensor([2, 256, 336, 336],"float32"), ) 	 115605504 	 29759 	 23.323215007781982 	 13.877972602844238 	 0.4005260467529297 	 0.4766080379486084 	 23.8905029296875 	 7.743072986602783 	 0.4102206230163574 	 0.26593613624572754 	 
2025-08-04 11:46:58.317938 test begin: paddle.Tensor.__add__(Tensor([2, 256, 336, 336],"float32"), Tensor([2, 256, 336, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([2, 256, 336, 336],"float32"), Tensor([2, 256, 336, 336],"float32"), ) 	 115605504 	 29759 	 15.23154902458191 	 15.10327410697937 	 0.5231003761291504 	 0.5186402797698975 	 16.331377506256104 	 1.7134284973144531 	 0.5609610080718994 	 7.891654968261719e-05 	 
2025-08-04 11:47:49.629429 test begin: paddle.Tensor.__add__(Tensor([4, 256, 336, 336],"float16"), Tensor([4, 256, 336, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([4, 256, 336, 336],"float16"), Tensor([4, 256, 336, 336],"float32"), ) 	 231211008 	 29759 	 46.490678787231445 	 27.56571912765503 	 0.7982995510101318 	 0.9466996192932129 	 47.63545632362366 	 15.33718991279602 	 0.8177545070648193 	 0.5267026424407959 	 
2025-08-04 11:50:14.845744 test begin: paddle.Tensor.__add__(Tensor([8, 113, 336, 336],"float16"), Tensor([8, 113, 336, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([8, 113, 336, 336],"float16"), Tensor([8, 113, 336, 336],"float32"), ) 	 204115968 	 29759 	 41.067548990249634 	 25.50827717781067 	 0.7051551342010498 	 0.8365163803100586 	 42.121455669403076 	 13.556985855102539 	 0.723344087600708 	 0.4655888080596924 	 
2025-08-04 11:52:24.384493 test begin: paddle.Tensor.__add__(Tensor([8, 256, 336, 74],"float32"), Tensor([8, 256, 336, 74],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([8, 256, 336, 74],"float32"), Tensor([8, 256, 336, 74],"float32"), ) 	 101842944 	 29759 	 13.432875633239746 	 13.329421997070312 	 0.4613528251647949 	 0.4573688507080078 	 14.400444507598877 	 2.10331130027771 	 0.4945046901702881 	 0.00020051002502441406 	 
2025-08-04 11:53:10.276293 test begin: paddle.Tensor.__add__(Tensor([8, 256, 74, 336],"float32"), Tensor([8, 256, 74, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([8, 256, 74, 336],"float32"), Tensor([8, 256, 74, 336],"float32"), ) 	 101842944 	 29759 	 13.432835340499878 	 13.676774263381958 	 0.461378812789917 	 0.4573662281036377 	 14.39986801147461 	 2.143319845199585 	 0.49454569816589355 	 0.0002288818359375 	 
2025-08-04 11:53:59.969157 test begin: paddle.Tensor.__add__(Tensor([8, 57, 336, 336],"float16"), Tensor([8, 57, 336, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([8, 57, 336, 336],"float16"), Tensor([8, 57, 336, 336],"float32"), ) 	 102961152 	 29759 	 20.80055570602417 	 12.395700454711914 	 0.35712099075317383 	 0.42517757415771484 	 21.3214168548584 	 6.914128541946411 	 0.36610841751098633 	 0.23737621307373047 	 
2025-08-04 11:55:04.173832 test begin: paddle.Tensor.__add__(Tensor([8, 57, 336, 336],"float32"), Tensor([8, 57, 336, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([8, 57, 336, 336],"float32"), Tensor([8, 57, 336, 336],"float32"), ) 	 102961152 	 29759 	 13.583678007125854 	 13.469763278961182 	 0.46658873558044434 	 0.46245431900024414 	 14.555469274520874 	 1.6476061344146729 	 0.4998610019683838 	 7.867813110351562e-05 	 
2025-08-04 11:55:50.161928 test begin: paddle.Tensor.__and__(Tensor([1, 1, 2048, 2048],"bool"), Tensor([1, 13, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 1, 2048, 2048],"bool"), Tensor([1, 13, 2048, 2048],"bool"), ) 	 58720256 	 84955 	 13.690421342849731 	 19.246761798858643 	 0.1647031307220459 	 0.2315351963043213 	 None 	 None 	 None 	 None 	 
2025-08-04 11:56:24.009142 test begin: paddle.Tensor.__and__(Tensor([1, 1, 2048, 2048],"bool"), Tensor([13, 1, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 1, 2048, 2048],"bool"), Tensor([13, 1, 2048, 2048],"bool"), ) 	 58720256 	 84955 	 13.689643383026123 	 19.262221574783325 	 0.16469287872314453 	 0.23154902458190918 	 None 	 None 	 None 	 None 	 
2025-08-04 11:56:58.127555 test begin: paddle.Tensor.__and__(Tensor([1, 1, 2048, 24807],"bool"), Tensor([1, 1, 2048, 24807],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 1, 2048, 24807],"bool"), Tensor([1, 1, 2048, 24807],"bool"), ) 	 101609472 	 84955 	 10.008087873458862 	 9.796862840652466 	 0.12037181854248047 	 0.11788010597229004 	 None 	 None 	 None 	 None 	 
2025-08-04 11:57:19.456131 test begin: paddle.Tensor.__and__(Tensor([1, 1, 24807, 2048],"bool"), Tensor([1, 1, 24807, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 1, 24807, 2048],"bool"), Tensor([1, 1, 24807, 2048],"bool"), ) 	 101609472 	 84955 	 10.008050441741943 	 9.802570581436157 	 0.12039017677307129 	 0.1178433895111084 	 None 	 None 	 None 	 None 	 
2025-08-04 11:57:42.710474 test begin: paddle.Tensor.__and__(Tensor([1, 13, 2048, 2048],"bool"), Tensor([1, 1, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 13, 2048, 2048],"bool"), Tensor([1, 1, 2048, 2048],"bool"), ) 	 58720256 	 84955 	 15.337889432907104 	 19.244199752807617 	 0.18452072143554688 	 0.23148322105407715 	 None 	 None 	 None 	 None 	 
2025-08-04 11:58:18.120641 test begin: paddle.Tensor.__and__(Tensor([1, 13, 2048, 2048],"bool"), Tensor([1, 13, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 13, 2048, 2048],"bool"), Tensor([1, 13, 2048, 2048],"bool"), ) 	 109051904 	 84955 	 10.677420854568481 	 10.470916032791138 	 0.1284351348876953 	 0.12595009803771973 	 None 	 None 	 None 	 None 	 
2025-08-04 11:58:42.115949 test begin: paddle.Tensor.__and__(Tensor([13, 1, 1007, 1007],"bool"), Tensor([13, 4, 1007, 1007],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 1, 1007, 1007],"bool"), Tensor([13, 4, 1007, 1007],"bool"), ) 	 65913185 	 84955 	 15.687059164047241 	 20.283568143844604 	 0.18875908851623535 	 0.24399256706237793 	 None 	 None 	 None 	 None 	 
2025-08-04 11:59:19.024823 test begin: paddle.Tensor.__and__(Tensor([13, 1, 1007, 3881],"bool"), Tensor([13, 1, 1007, 3881],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 1, 1007, 3881],"bool"), Tensor([13, 1, 1007, 3881],"bool"), ) 	 101612342 	 84955 	 10.033244132995605 	 9.834962844848633 	 0.12072873115539551 	 0.11828923225402832 	 None 	 None 	 None 	 None 	 
2025-08-04 11:59:42.333308 test begin: paddle.Tensor.__and__(Tensor([13, 1, 2048, 2048],"bool"), Tensor([1, 1, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 1, 2048, 2048],"bool"), Tensor([1, 1, 2048, 2048],"bool"), ) 	 58720256 	 84955 	 15.337754011154175 	 19.241760730743408 	 0.18450331687927246 	 0.23145174980163574 	 None 	 None 	 None 	 None 	 
2025-08-04 12:00:17.775894 test begin: paddle.Tensor.__and__(Tensor([13, 1, 2048, 2048],"bool"), Tensor([13, 1, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 1, 2048, 2048],"bool"), Tensor([13, 1, 2048, 2048],"bool"), ) 	 109051904 	 84955 	 10.677321434020996 	 10.474185705184937 	 0.12846636772155762 	 0.12594938278198242 	 None 	 None 	 None 	 None 	 
2025-08-04 12:00:42.244037 test begin: paddle.Tensor.__and__(Tensor([13, 1, 3881, 1007],"bool"), Tensor([13, 1, 3881, 1007],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 1, 3881, 1007],"bool"), Tensor([13, 1, 3881, 1007],"bool"), ) 	 101612342 	 84955 	 10.033780813217163 	 9.8401198387146 	 0.12068986892700195 	 0.11831450462341309 	 None 	 None 	 None 	 None 	 
2025-08-04 12:01:03.588626 test begin: paddle.Tensor.__and__(Tensor([13, 4, 1007, 1007],"bool"), Tensor([13, 1, 1007, 1007],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 4, 1007, 1007],"bool"), Tensor([13, 1, 1007, 1007],"bool"), ) 	 65913185 	 84955 	 16.80671191215515 	 20.27645182609558 	 0.20218610763549805 	 0.24392914772033691 	 None 	 None 	 None 	 None 	 
2025-08-04 12:01:42.657815 test begin: paddle.Tensor.__and__(Tensor([13, 4, 1007, 1007],"bool"), Tensor([13, 4, 1007, 1007],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 4, 1007, 1007],"bool"), Tensor([13, 4, 1007, 1007],"bool"), ) 	 105461096 	 84955 	 10.432169198989868 	 10.245308876037598 	 0.1254863739013672 	 0.1232156753540039 	 None 	 None 	 None 	 None 	 
2025-08-04 12:02:06.663484 test begin: paddle.Tensor.__and__(Tensor([194, 1, 512, 512],"bool"), Tensor([194, 1, 512, 512],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([194, 1, 512, 512],"bool"), Tensor([194, 1, 512, 512],"bool"), ) 	 101711872 	 84955 	 10.013725519180298 	 9.806872844696045 	 0.12046933174133301 	 0.1179511547088623 	 None 	 None 	 None 	 None 	 
2025-08-04 12:02:27.924479 test begin: paddle.Tensor.__and__(Tensor([51, 1, 1007, 1007],"bool"), Tensor([51, 1, 1007, 1007],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([51, 1, 1007, 1007],"bool"), Tensor([51, 1, 1007, 1007],"bool"), ) 	 103432998 	 84955 	 10.153464555740356 	 10.009013652801514 	 0.12215662002563477 	 0.12028145790100098 	 None 	 None 	 None 	 None 	 
2025-08-04 12:02:51.914920 test begin: paddle.Tensor.__and__(Tensor([8, 1, 12404, 512],"bool"), Tensor([8, 1, 12404, 512],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([8, 1, 12404, 512],"bool"), Tensor([8, 1, 12404, 512],"bool"), ) 	 101613568 	 84955 	 10.00726580619812 	 9.796979188919067 	 0.12037539482116699 	 0.1178436279296875 	 None 	 None 	 None 	 None 	 
2025-08-04 12:03:14.390373 test begin: paddle.Tensor.__and__(Tensor([8, 1, 512, 12404],"bool"), Tensor([8, 1, 512, 12404],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([8, 1, 512, 12404],"bool"), Tensor([8, 1, 512, 12404],"bool"), ) 	 101613568 	 84955 	 10.006898403167725 	 9.797070264816284 	 0.12036418914794922 	 0.11785888671875 	 None 	 None 	 None 	 None 	 
2025-08-04 12:03:38.417121 test begin: paddle.Tensor.__and__(Tensor([8, 1, 512, 512],"bool"), Tensor([8, 25, 512, 512],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([8, 1, 512, 512],"bool"), Tensor([8, 25, 512, 512],"bool"), ) 	 54525952 	 84955 	 15.595510244369507 	 20.00256109237671 	 0.18764853477478027 	 0.24061918258666992 	 None 	 None 	 None 	 None 	 
2025-08-04 12:04:14.771770 test begin: paddle.Tensor.__and__(Tensor([8, 25, 512, 512],"bool"), Tensor([8, 1, 512, 512],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([8, 25, 512, 512],"bool"), Tensor([8, 1, 512, 512],"bool"), ) 	 54525952 	 84955 	 16.43922233581543 	 20.005139112472534 	 0.19778919219970703 	 0.24064302444458008 	 None 	 None 	 None 	 None 	 
2025-08-04 12:04:52.022744 test begin: paddle.Tensor.__and__(Tensor([8, 25, 512, 512],"bool"), Tensor([8, 25, 512, 512],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([8, 25, 512, 512],"bool"), Tensor([8, 25, 512, 512],"bool"), ) 	 104857600 	 84955 	 10.280590057373047 	 10.094644546508789 	 0.12368321418762207 	 0.12142419815063477 	 None 	 None 	 None 	 None 	 
2025-08-04 12:05:13.920712 test begin: paddle.Tensor.__div__(Tensor([8, 16, 396901],"float32"), 2, )
[Prof] paddle.Tensor.__div__ 	 paddle.Tensor.__div__(Tensor([8, 16, 396901],"float32"), 2, ) 	 50803328 	 33824 	 10.000028133392334 	 10.080093383789062 	 0.30213332176208496 	 0.3043081760406494 	 9.998990058898926 	 10.06827688217163 	 0.3021576404571533 	 0.30417943000793457 	 
2025-08-04 12:05:56.786543 test begin: paddle.Tensor.__div__(Tensor([8, 198451, 32],"float32"), 2, )
[Prof] paddle.Tensor.__div__ 	 paddle.Tensor.__div__(Tensor([8, 198451, 32],"float32"), 2, ) 	 50803456 	 33824 	 10.006171703338623 	 10.069734573364258 	 0.30233120918273926 	 0.30423879623413086 	 10.008395910263062 	 10.068539142608643 	 0.30239176750183105 	 0.3042259216308594 	 
2025-08-04 12:06:39.559161 test begin: paddle.Tensor.__div__(Tensor([99226, 16, 32],"float32"), 2, )
[Prof] paddle.Tensor.__div__ 	 paddle.Tensor.__div__(Tensor([99226, 16, 32],"float32"), 2, ) 	 50803712 	 33824 	 10.004992961883545 	 10.069824695587158 	 0.30230021476745605 	 0.3042874336242676 	 10.00324010848999 	 10.068294525146484 	 0.3022027015686035 	 0.30423450469970703 	 
2025-08-04 12:07:21.427172 test begin: paddle.Tensor.__eq__(Tensor([138, 369303],"float32"), Tensor([138, 1],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([138, 369303],"float32"), Tensor([138, 1],"float32"), ) 	 50963952 	 52174 	 10.02034330368042 	 12.624077081680298 	 0.19631600379943848 	 0.24730730056762695 	 None 	 None 	 None 	 None 	 
2025-08-04 12:07:45.035140 test begin: paddle.Tensor.__eq__(Tensor([146, 349866],"float32"), Tensor([146, 1],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([146, 349866],"float32"), Tensor([146, 1],"float32"), ) 	 51080582 	 52174 	 10.037965059280396 	 13.554434537887573 	 0.19661188125610352 	 0.24786639213562012 	 None 	 None 	 None 	 None 	 
2025-08-04 12:08:11.556006 test begin: paddle.Tensor.__eq__(Tensor([49, 1036801],"float32"), Tensor([49, 1036801],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([49, 1036801],"float32"), Tensor([49, 1036801],"float32"), ) 	 101606498 	 52174 	 17.05614185333252 	 17.095641136169434 	 0.33414387702941895 	 0.33489513397216797 	 None 	 None 	 None 	 None 	 
2025-08-04 12:08:47.438322 test begin: paddle.Tensor.__eq__(Tensor([49, 1036801],"float32"), Tensor([49, 1],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([49, 1036801],"float32"), Tensor([49, 1],"float32"), ) 	 50803298 	 52174 	 10.001995086669922 	 12.587594032287598 	 0.19591951370239258 	 0.2465512752532959 	 None 	 None 	 None 	 None 	 
2025-08-04 12:09:11.291410 test begin: paddle.Tensor.__eq__(Tensor([53, 958551],"float32"), Tensor([53, 1],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([53, 958551],"float32"), Tensor([53, 1],"float32"), ) 	 50803256 	 52174 	 10.006922006607056 	 12.588282585144043 	 0.19598722457885742 	 0.24658942222595215 	 None 	 None 	 None 	 None 	 
2025-08-04 12:09:34.765315 test begin: paddle.Tensor.__eq__(Tensor([53, 958551],"float32"), Tensor([53, 958551],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([53, 958551],"float32"), Tensor([53, 958551],"float32"), ) 	 101606406 	 52174 	 17.056262493133545 	 17.095686435699463 	 0.3340795040130615 	 0.33490633964538574 	 None 	 None 	 None 	 None 	 
2025-08-04 12:10:10.654717 test begin: paddle.Tensor.__eq__(Tensor([55, 923695],"float32"), Tensor([55, 1],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([55, 923695],"float32"), Tensor([55, 1],"float32"), ) 	 50803280 	 52174 	 10.002048254013062 	 12.61208438873291 	 0.19596076011657715 	 0.24659466743469238 	 None 	 None 	 None 	 None 	 
2025-08-04 12:10:34.498862 test begin: paddle.Tensor.__eq__(Tensor([55, 923695],"float32"), Tensor([55, 923695],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([55, 923695],"float32"), Tensor([55, 923695],"float32"), ) 	 101606450 	 52174 	 17.056335926055908 	 17.095489740371704 	 0.33409953117370605 	 0.3348965644836426 	 None 	 None 	 None 	 None 	 
2025-08-04 12:11:10.458259 test begin: paddle.Tensor.__floordiv__(Tensor([10, 10160641],"float32"), Tensor([10, 10160641],"float16"), )
W0804 12:11:13.885509 100169 dygraph_functions.cc:89596] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([10, 10160641],"float32"), Tensor([10, 10160641],"float16"), ) 	 203212820 	 32836 	 45.054147481918335 	 39.25809979438782 	 0.7010226249694824 	 1.3712069988250732 	 None 	 None 	 None 	 None 	 
2025-08-04 12:12:40.108587 test begin: paddle.Tensor.__floordiv__(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), ) 	 50803220 	 32836 	 14.695315837860107 	 14.699033498764038 	 0.45744919776916504 	 0.45743846893310547 	 None 	 None 	 None 	 None 	 
2025-08-04 12:13:10.646057 test begin: paddle.Tensor.__floordiv__(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float16"), )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float16"), ) 	 101606420 	 32836 	 22.64594340324402 	 19.70173192024231 	 0.35242486000061035 	 0.6130447387695312 	 None 	 None 	 None 	 None 	 
2025-08-04 12:13:54.885887 test begin: paddle.Tensor.__floordiv__(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), ) 	 50804736 	 32836 	 14.710162878036499 	 14.69693112373352 	 0.45785021781921387 	 0.4574427604675293 	 None 	 None 	 None 	 None 	 
2025-08-04 12:14:25.213278 test begin: paddle.Tensor.__floordiv__(Tensor([4, 6350401],"int64"), 4, )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([4, 6350401],"int64"), 4, ) 	 25401604 	 32836 	 10.006819486618042 	 10.291355848312378 	 0.15574145317077637 	 0.30901479721069336 	 None 	 None 	 None 	 None 	 
2025-08-04 12:14:49.016899 test begin: paddle.Tensor.__floordiv__(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float16"), )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float16"), ) 	 101607424 	 32836 	 22.674574851989746 	 19.696845769882202 	 0.35283756256103516 	 0.6130750179290771 	 None 	 None 	 None 	 None 	 
2025-08-04 12:15:33.321349 test begin: paddle.Tensor.__floordiv__(Tensor([84673, 300],"int64"), 4, )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([84673, 300],"int64"), 4, ) 	 25401900 	 32836 	 10.011492729187012 	 9.950003862380981 	 0.15581464767456055 	 0.3090527057647705 	 None 	 None 	 None 	 None 	 
2025-08-04 12:15:54.359406 test begin: paddle.Tensor.__floordiv__(Tensor([99226, 1024],"float32"), Tensor([99226, 1024],"float16"), )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([99226, 1024],"float32"), Tensor([99226, 1024],"float16"), ) 	 203214848 	 32836 	 45.12848997116089 	 39.121875047683716 	 0.7022755146026611 	 1.2172608375549316 	 None 	 None 	 None 	 None 	 
2025-08-04 12:17:22.320606 test begin: paddle.Tensor.__ge__(Tensor([50803201],"int32"), 0, )
[Prof] paddle.Tensor.__ge__ 	 paddle.Tensor.__ge__(Tensor([50803201],"int32"), 0, ) 	 50803201 	 21328 	 10.004528045654297 	 3.9693918228149414 	 0.239701509475708 	 0.19005370140075684 	 None 	 None 	 None 	 None 	 
2025-08-04 12:17:39.496200 test begin: paddle.Tensor.__getitem__(Tensor([10, 7576, 12800],"bfloat16"), slice(None,-3,None), )
W0804 12:18:15.087766 102400 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 2715238400, memory's size is 1939456000.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):2715238400 > memory_size():1939456000.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3cbb50b070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 12:27:53.901351 test begin: paddle.Tensor.__getitem__(Tensor([10, 7576, 16770],"bfloat16"), slice(None,-3,None), )
W0804 12:28:13.265000 106049 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0804 12:28:48.511689 106049 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 3557386560, memory's size is 2540990464.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):3557386560 > memory_size():2540990464.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f44a7752e00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 12:37:59.316152 test begin: paddle.Tensor.__getitem__(Tensor([10, 7712, 12800],"bfloat16"), slice(None,-2,None), )
W0804 12:38:19.359525 109852 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0804 12:38:51.397383 109852 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 3158835200, memory's size is 1974272000.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):3158835200 > memory_size():1974272000.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9e50f86e00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 12:48:04.889706 test begin: paddle.Tensor.__getitem__(Tensor([10, 7712, 16470],"bfloat16"), slice(None,-2,None), )
W0804 12:48:24.025949 113562 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0804 12:48:49.021446 113562 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 4064532480, memory's size is 2540332800.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):4064532480 > memory_size():2540332800.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fecb7ffee00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 12:58:10.660088 test begin: paddle.Tensor.__getitem__(Tensor([10, 8168, 12800],"bfloat16"), slice(None,-6,None), )
W0804 12:58:26.676988 117119 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0804 12:58:43.273008 117119 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc807b86e60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 13:08:16.373453 test begin: paddle.Tensor.__getitem__(Tensor([10, 8168, 15550],"bfloat16"), slice(None,-6,None), )
W0804 13:08:45.066009 120817 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0804 13:09:12.462786 120817 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc9db1aac20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 13:18:21.730166 test begin: paddle.Tensor.__getitem__(Tensor([10, 9923, 12800],"bfloat16"), slice(None,-2,None), )
W0804 13:18:46.078246 124387 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0804 13:19:14.953358 124387 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 4064460800, memory's size is 2540288000.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):4064460800 > memory_size():2540288000.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7efb0185ee00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 13:28:27.468573 test begin: paddle.Tensor.__getitem__(Tensor([10, 9923, 12800],"bfloat16"), slice(None,-3,None), )
W0804 13:28:46.765627 127958 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0804 13:29:09.850860 127958 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 3556403200, memory's size is 2540288000.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):3556403200 > memory_size():2540288000.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7efb5b962e00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 13:38:32.863476 test begin: paddle.Tensor.__getitem__(Tensor([10, 9923, 12800],"bfloat16"), slice(None,-6,None), )
W0804 13:38:52.650068 131440 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0804 13:39:10.506855 131440 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff3ac0eee60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 13:48:44.658302 test begin: paddle.Tensor.__gt__(Tensor([1, 400, 127009],"float32"), 0, )
W0804 13:48:45.694360 134235 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([1, 400, 127009],"float32"), 0, ) 	 50803600 	 21267 	 9.979917049407959 	 3.9552130699157715 	 0.23970246315002441 	 0.1898510456085205 	 None 	 None 	 None 	 None 	 
2025-08-04 13:48:59.903199 test begin: paddle.Tensor.__gt__(Tensor([1, 400, 127009],"float32"), 1e-09, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([1, 400, 127009],"float32"), 1e-09, ) 	 50803600 	 21267 	 9.98570728302002 	 3.9509239196777344 	 0.2398967742919922 	 0.18992853164672852 	 None 	 None 	 None 	 None 	 
2025-08-04 13:49:14.833732 test begin: paddle.Tensor.__gt__(Tensor([1, 772, 65856],"float32"), 0, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([1, 772, 65856],"float32"), 0, ) 	 50840832 	 21267 	 9.993502855300903 	 3.9641363620758057 	 0.24003028869628906 	 0.1899864673614502 	 None 	 None 	 None 	 None 	 
2025-08-04 13:49:29.750328 test begin: paddle.Tensor.__gt__(Tensor([1, 772, 65856],"float32"), 1e-09, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([1, 772, 65856],"float32"), 1e-09, ) 	 50840832 	 21267 	 9.988428115844727 	 3.9682016372680664 	 0.239943265914917 	 0.18996119499206543 	 None 	 None 	 None 	 None 	 
2025-08-04 13:49:46.430518 test begin: paddle.Tensor.__gt__(Tensor([2, 400, 65856],"float32"), 0, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([2, 400, 65856],"float32"), 0, ) 	 52684800 	 21267 	 10.337510347366333 	 4.092259407043457 	 0.24827837944030762 	 0.19662189483642578 	 None 	 None 	 None 	 None 	 
2025-08-04 13:50:01.870478 test begin: paddle.Tensor.__gt__(Tensor([2, 400, 65856],"float32"), 1e-09, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([2, 400, 65856],"float32"), 1e-09, ) 	 52684800 	 21267 	 10.336504936218262 	 4.091938257217407 	 0.24837565422058105 	 0.1966397762298584 	 None 	 None 	 None 	 None 	 
2025-08-04 13:50:17.157802 test begin: paddle.Tensor.__gt__(Tensor([324000, 157],"float32"), 0.0, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([324000, 157],"float32"), 0.0, ) 	 50868000 	 21267 	 9.98998475074768 	 3.9667704105377197 	 0.2403721809387207 	 0.19007587432861328 	 None 	 None 	 None 	 None 	 
2025-08-04 13:50:33.999626 test begin: paddle.Tensor.__gt__(Tensor([635041, 80],"float32"), 0.0, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([635041, 80],"float32"), 0.0, ) 	 50803280 	 21267 	 9.984867334365845 	 3.9537770748138428 	 0.23967576026916504 	 0.18992066383361816 	 None 	 None 	 None 	 None 	 
2025-08-04 13:50:48.771024 test begin: paddle.Tensor.__le__(Tensor([243360, 209],"float32"), 0.0, )
[Prof] paddle.Tensor.__le__ 	 paddle.Tensor.__le__(Tensor([243360, 209],"float32"), 0.0, ) 	 50862240 	 21231 	 9.972355842590332 	 3.9484341144561768 	 0.23985981941223145 	 0.19007420539855957 	 None 	 None 	 None 	 None 	 
2025-08-04 13:51:03.512600 test begin: paddle.Tensor.__le__(Tensor([282240, 181],"float32"), 0.0, )
[Prof] paddle.Tensor.__le__ 	 paddle.Tensor.__le__(Tensor([282240, 181],"float32"), 0.0, ) 	 51085440 	 21231 	 10.011303186416626 	 3.9680488109588623 	 0.24093389511108398 	 0.19088363647460938 	 None 	 None 	 None 	 None 	 
2025-08-04 13:51:18.308546 test begin: paddle.Tensor.__le__(Tensor([324000, 157],"float32"), 0.0, )
[Prof] paddle.Tensor.__le__ 	 paddle.Tensor.__le__(Tensor([324000, 157],"float32"), 0.0, ) 	 50868000 	 21231 	 9.972035884857178 	 3.953594923019409 	 0.2400050163269043 	 0.19008731842041016 	 None 	 None 	 None 	 None 	 
2025-08-04 13:51:33.048225 test begin: paddle.Tensor.__le__(Tensor([635041, 80],"float32"), 0.0, )
[Prof] paddle.Tensor.__le__ 	 paddle.Tensor.__le__(Tensor([635041, 80],"float32"), 0.0, ) 	 50803280 	 21231 	 9.965238094329834 	 3.967261552810669 	 0.2397022247314453 	 0.18990802764892578 	 None 	 None 	 None 	 None 	 
2025-08-04 13:51:50.433782 test begin: paddle.Tensor.__len__(Tensor([1000, 1352, 376],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([1000, 1352, 376],"float32"), ) 	 508352000 	 2154792 	 9.858379364013672 	 10.452195167541504 	 0.0001239776611328125 	 0.0002684593200683594 	 None 	 None 	 None 	 None 	 
2025-08-04 13:52:18.857795 test begin: paddle.Tensor.__len__(Tensor([1000, 376, 1352],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([1000, 376, 1352],"float32"), ) 	 508352000 	 2154792 	 10.186784505844116 	 10.620549201965332 	 0.00011420249938964844 	 0.0007851123809814453 	 None 	 None 	 None 	 None 	 
2025-08-04 13:52:54.677532 test begin: paddle.Tensor.__len__(Tensor([1000000, 509],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([1000000, 509],"float32"), ) 	 509000000 	 2154792 	 9.67752456665039 	 10.357877969741821 	 9.1552734375e-05 	 0.0007512569427490234 	 None 	 None 	 None 	 None 	 
2025-08-04 13:53:23.039298 test begin: paddle.Tensor.__len__(Tensor([230, 1501, 1501],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([230, 1501, 1501],"float32"), ) 	 518190230 	 2154792 	 11.772779703140259 	 10.50230622291565 	 0.00013709068298339844 	 0.00011587142944335938 	 None 	 None 	 None 	 None 	 
2025-08-04 13:53:53.591329 test begin: paddle.Tensor.__len__(Tensor([3600, 376, 376],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([3600, 376, 376],"float32"), ) 	 508953600 	 2154792 	 10.176493883132935 	 10.441855669021606 	 0.0001246929168701172 	 0.000263214111328125 	 None 	 None 	 None 	 None 	 
2025-08-04 13:54:22.482891 test begin: paddle.Tensor.__len__(Tensor([500, 1501, 677],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([500, 1501, 677],"float32"), ) 	 508088500 	 2154792 	 10.38679552078247 	 14.084843635559082 	 0.0001163482666015625 	 0.0002665519714355469 	 None 	 None 	 None 	 None 	 
2025-08-04 13:54:57.167490 test begin: paddle.Tensor.__len__(Tensor([500, 677, 1501],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([500, 677, 1501],"float32"), ) 	 508088500 	 2154792 	 9.990858793258667 	 10.516578435897827 	 6.628036499023438e-05 	 0.00025534629821777344 	 None 	 None 	 None 	 None 	 
2025-08-04 13:55:26.006083 test begin: paddle.Tensor.__len__(Tensor([5080330, 100],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([5080330, 100],"float32"), ) 	 508033000 	 2154792 	 10.03918719291687 	 10.448328733444214 	 5.7697296142578125e-05 	 0.0002675056457519531 	 None 	 None 	 None 	 None 	 
2025-08-04 13:55:54.844863 test begin: paddle.Tensor.__lshift__(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), ) 	 101607000 	 22338 	 10.047983169555664 	 9.973065376281738 	 0.4597468376159668 	 0.45627307891845703 	 None 	 None 	 None 	 None 	 
2025-08-04 13:56:17.689792 test begin: paddle.Tensor.__lshift__(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), ) 	 101606800 	 22338 	 10.05052137374878 	 9.978007793426514 	 0.4598548412322998 	 0.45628976821899414 	 None 	 None 	 None 	 None 	 
2025-08-04 13:56:39.294265 test begin: paddle.Tensor.__lshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), ) 	 203213200 	 22338 	 9.999335050582886 	 10.049353122711182 	 0.4576091766357422 	 0.45987558364868164 	 None 	 None 	 None 	 None 	 
2025-08-04 13:57:01.357153 test begin: paddle.Tensor.__lshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, ) 	 203213200 	 22338 	 9.998350381851196 	 10.048685312271118 	 0.4575157165527344 	 0.4598209857940674 	 None 	 None 	 None 	 None 	 
2025-08-04 13:57:24.607913 test begin: paddle.Tensor.__lshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), ) 	 203213400 	 22338 	 10.002348899841309 	 10.056565999984741 	 0.4576418399810791 	 0.4597506523132324 	 None 	 None 	 None 	 None 	 
2025-08-04 13:57:48.495435 test begin: paddle.Tensor.__lshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, ) 	 203213400 	 22338 	 10.003185033798218 	 10.049250841140747 	 0.4577615261077881 	 0.4598100185394287 	 None 	 None 	 None 	 None 	 
2025-08-04 13:58:10.554939 test begin: paddle.Tensor.__lt__(Tensor([1034, 3, 64, 128],"float64"), 1, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([1034, 3, 64, 128],"float64"), 1, ) 	 25411584 	 22019 	 9.999080896377563 	 3.70202374458313 	 0.23205780982971191 	 0.1718432903289795 	 None 	 None 	 None 	 None 	 
2025-08-04 13:58:24.826723 test begin: paddle.Tensor.__lt__(Tensor([256, 13, 64, 128],"float64"), 1, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([256, 13, 64, 128],"float64"), 1, ) 	 27262976 	 22019 	 10.694475412368774 	 3.960442304611206 	 0.2480635643005371 	 0.1837911605834961 	 None 	 None 	 None 	 None 	 
2025-08-04 13:58:42.918561 test begin: paddle.Tensor.__lt__(Tensor([256, 3, 259, 128],"float64"), 1, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([256, 3, 259, 128],"float64"), 1, ) 	 25460736 	 22019 	 10.01042652130127 	 3.708292007446289 	 0.23226523399353027 	 0.1721193790435791 	 None 	 None 	 None 	 None 	 
2025-08-04 13:58:57.209136 test begin: paddle.Tensor.__lt__(Tensor([256, 3, 64, 517],"float64"), 1, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([256, 3, 64, 517],"float64"), 1, ) 	 25411584 	 22019 	 9.9991614818573 	 3.7019801139831543 	 0.23197197914123535 	 0.1718299388885498 	 None 	 None 	 None 	 None 	 
2025-08-04 13:59:11.461973 test begin: paddle.Tensor.__lt__(Tensor([4, 157920, 81],"float32"), 0.1111111111111111, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([4, 157920, 81],"float32"), 0.1111111111111111, ) 	 51166080 	 22019 	 10.397318124771118 	 4.118742942810059 	 0.241286039352417 	 0.19116473197937012 	 None 	 None 	 None 	 None 	 
2025-08-04 13:59:26.825953 test begin: paddle.Tensor.__lt__(Tensor([4, 1814401, 7],"float32"), 0.1111111111111111, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([4, 1814401, 7],"float32"), 0.1111111111111111, ) 	 50803228 	 22019 	 10.322622537612915 	 4.106978416442871 	 0.2394871711730957 	 0.18987441062927246 	 None 	 None 	 None 	 None 	 
2025-08-04 13:59:43.436291 test begin: paddle.Tensor.__lt__(Tensor([46, 157920, 7],"float32"), 0.1111111111111111, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([46, 157920, 7],"float32"), 0.1111111111111111, ) 	 50850240 	 22019 	 10.338333129882812 	 4.119017601013184 	 0.239793062210083 	 0.1901085376739502 	 None 	 None 	 None 	 None 	 
2025-08-04 13:59:58.777135 test begin: paddle.Tensor.__lt__(Tensor([50803201],"float32"), 0.7, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([50803201],"float32"), 0.7, ) 	 50803201 	 22019 	 10.321072578430176 	 4.095533609390259 	 0.23951172828674316 	 0.1898937225341797 	 None 	 None 	 None 	 None 	 
2025-08-04 14:00:14.202153 test begin: paddle.Tensor.__matmul__(Tensor([10, 2304, 2304],"float32"), Tensor([10, 2304, 64],"float32"), )
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([10, 2304, 2304],"float32"), Tensor([10, 2304, 64],"float32"), ) 	 54558720 	 12494 	 11.613172054290771 	 11.611801385879517 	 0.9500184059143066 	 0.9497931003570557 	 17.18611717224121 	 17.18416118621826 	 0.7028298377990723 	 0.7027711868286133 	 
2025-08-04 14:01:15.617947 test begin: paddle.Tensor.__matmul__(Tensor([111, 3, 392, 392],"float32"), Tensor([111, 3, 392, 32],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([111, 3, 392, 392],"float32"), Tensor([111, 3, 392, 32],"float32"), ) 	 55347264 	 12494 	 12.915535688400269 	 12.915602684020996 	 1.0564956665039062 	 1.056513786315918 	 18.117777585983276 	 18.11490035057068 	 0.741041898727417 	 0.740847110748291 	 
2025-08-04 14:02:18.837145 test begin: paddle.Tensor.__matmul__(Tensor([1351, 3, 392, 392],"float32"), Tensor([1351, 3, 392, 32],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd04009c520>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 14:12:35.116757 test begin: paddle.Tensor.__matmul__(Tensor([176, 2, 392, 392],"float32"), Tensor([176, 2, 392, 32],"float32"), )
W0804 14:12:36.978951 138350 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([176, 2, 392, 392],"float32"), Tensor([176, 2, 392, 32],"float32"), ) 	 58505216 	 12494 	 14.875670909881592 	 13.882946729660034 	 1.1355650424957275 	 1.1356005668640137 	 19.379831075668335 	 19.380495309829712 	 0.7925965785980225 	 0.7926232814788818 	 
2025-08-04 14:13:51.066452 test begin: paddle.Tensor.__matmul__(Tensor([176, 24, 392, 392],"float32"), Tensor([176, 24, 392, 32],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3d3637ee00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 14:24:04.206083 test begin: paddle.Tensor.__matmul__(Tensor([176, 3, 246, 392],"float32"), Tensor([176, 3, 392, 32],"float32"), )
W0804 14:24:05.297329 142274 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([176, 3, 246, 392],"float32"), Tensor([176, 3, 392, 32],"float32"), ) 	 57539328 	 12494 	 9.980209589004517 	 9.981290578842163 	 0.8163821697235107 	 0.8164639472961426 	 16.85492968559265 	 16.85621953010559 	 0.6893563270568848 	 0.6894552707672119 	 
2025-08-04 14:24:59.991624 test begin: paddle.Tensor.__matmul__(Tensor([176, 3, 392, 392],"float32"), Tensor([176, 3, 392, 246],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([176, 3, 392, 392],"float32"), Tensor([176, 3, 392, 246],"float32"), ) 	 132050688 	 12494 	 39.54799222946167 	 39.534850120544434 	 3.2450411319732666 	 3.2339110374450684 	 89.50872778892517 	 89.50629711151123 	 3.6608455181121826 	 3.6607189178466797 	 
2025-08-04 14:29:21.252094 test begin: paddle.Tensor.__matmul__(Tensor([345, 2304, 2304],"float32"), Tensor([345, 2304, 64],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f16c37a6b60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 14:39:46.982616 test begin: paddle.Tensor.__matmul__(Tensor([49, 1024, 1024],"float32"), Tensor([49, 1024, 64],"float32"), )
W0804 14:39:48.048926 147796 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([49, 1024, 1024],"float32"), Tensor([49, 1024, 64],"float32"), ) 	 54591488 	 12494 	 10.360260963439941 	 10.37116813659668 	 0.8473858833312988 	 0.8477427959442139 	 15.777500867843628 	 15.776405811309814 	 0.6451947689056396 	 0.6451647281646729 	 
2025-08-04 14:40:43.091427 test begin: paddle.Tensor.__matmul__(Tensor([60, 2304, 2304],"float32"), Tensor([60, 2304, 368],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3c43e66c80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 14:51:01.168459 test begin: paddle.Tensor.__matmul__(Tensor([60, 368, 2304],"float32"), Tensor([60, 2304, 64],"float32"), )
W0804 14:51:02.294459 151555 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([60, 368, 2304],"float32"), Tensor([60, 2304, 64],"float32"), ) 	 59719680 	 12494 	 11.603925228118896 	 11.605137348175049 	 0.94921875 	 0.9493720531463623 	 14.966736793518066 	 14.967542171478271 	 0.6121141910552979 	 0.6122152805328369 	 
2025-08-04 14:51:57.432834 test begin: paddle.Tensor.__matmul__(Tensor([776, 1024, 1024],"float32"), Tensor([776, 1024, 64],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f319b6bab30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 15:02:10.964483 test begin: paddle.Tensor.__matmul__(Tensor([96, 1024, 1024],"float32"), Tensor([96, 1024, 517],"float32"), )
W0804 15:02:16.794312 155682 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([96, 1024, 1024],"float32"), Tensor([96, 1024, 517],"float32"), ) 	 151486464 	 12494 	 92.14000725746155 	 93.37284088134766 	 7.5361151695251465 	 7.535343885421753 	 166.78090620040894 	 166.76795268058777 	 6.820597410202026 	 6.822125673294067 	 
2025-08-04 15:11:04.277393 test begin: paddle.Tensor.__matmul__(Tensor([96, 517, 1024],"float32"), Tensor([96, 1024, 64],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([96, 517, 1024],"float32"), Tensor([96, 1024, 64],"float32"), ) 	 57114624 	 12494 	 13.097489833831787 	 12.911307573318481 	 1.2597548961639404 	 1.0545580387115479 	 17.136977434158325 	 17.139026880264282 	 0.7008609771728516 	 0.7009692192077637 	 
2025-08-04 15:12:08.243935 test begin: paddle.Tensor.__mod__(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), ) 	 50803220 	 22349 	 10.001110315322876 	 10.133086681365967 	 0.45717763900756836 	 0.4573485851287842 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:12:48.693279 test begin: paddle.Tensor.__mod__(Tensor([13, 2, 976985],"int64"), 16, )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([13, 2, 976985],"int64"), 16, ) 	 25401610 	 22349 	 13.01390528678894 	 6.6791040897369385 	 0.2973933219909668 	 0.3054392337799072 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:13:16.073740 test begin: paddle.Tensor.__mod__(Tensor([13, 30531, 64],"int64"), 16, )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([13, 30531, 64],"int64"), 16, ) 	 25401792 	 22349 	 13.009007215499878 	 6.701876401901245 	 0.2973935604095459 	 0.30556464195251465 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:13:43.531711 test begin: paddle.Tensor.__mod__(Tensor([198451, 2, 64],"int64"), 16, )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([198451, 2, 64],"int64"), 16, ) 	 25401728 	 22349 	 12.999516248703003 	 6.685546636581421 	 0.2971799373626709 	 0.305492639541626 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:14:10.710626 test begin: paddle.Tensor.__mod__(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), ) 	 50804736 	 22349 	 10.015916585922241 	 10.014303207397461 	 0.45814037322998047 	 0.45728182792663574 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:14:50.987807 test begin: paddle.Tensor.__mod__(Tensor([26, 976985],"int64"), 64, )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([26, 976985],"int64"), 64, ) 	 25401610 	 22349 	 13.01809310913086 	 6.6807990074157715 	 0.29726433753967285 	 0.30547499656677246 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:15:18.224530 test begin: paddle.Tensor.__mod__(Tensor([396901, 64],"int64"), 64, )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([396901, 64],"int64"), 64, ) 	 25401664 	 22349 	 12.998170614242554 	 7.420201539993286 	 0.29711246490478516 	 0.3054955005645752 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:15:47.520095 test begin: paddle.Tensor.__mul__(Tensor([1, 1, 32768, 32768],"float16"), 10000.0, )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([1, 1, 32768, 32768],"float16"), 10000.0, ) 	 1073741824 	 33522 	 104.10151958465576 	 103.62471008300781 	 3.1738505363464355 	 3.1590449810028076 	 104.10832405090332 	 103.61917090415955 	 3.1740310192108154 	 3.1592915058135986 	 
2025-08-04 15:23:29.620419 test begin: paddle.Tensor.__mul__(Tensor([108544, 469],"float32"), Tensor([108544, 469],"float32"), )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([108544, 469],"float32"), Tensor([108544, 469],"float32"), ) 	 101814272 	 33522 	 15.12570858001709 	 14.994720697402954 	 0.461181640625 	 0.45711374282836914 	 39.092490434646606 	 29.98828434944153 	 1.1918740272521973 	 0.4571065902709961 	 
2025-08-04 15:25:11.836554 test begin: paddle.Tensor.__mul__(Tensor([111616, 456],"float32"), Tensor([111616, 456],"float32"), )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([111616, 456],"float32"), Tensor([111616, 456],"float32"), ) 	 101793792 	 33522 	 15.121137142181396 	 14.999988555908203 	 0.46100687980651855 	 0.45709657669067383 	 39.12228727340698 	 29.982113361358643 	 1.2390632629394531 	 0.4570608139038086 	 
2025-08-04 15:26:53.649661 test begin: paddle.Tensor.__mul__(Tensor([14176, 3584],"float32"), Tensor([14176, 3584],"float32"), )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([14176, 3584],"float32"), Tensor([14176, 3584],"float32"), ) 	 101613568 	 33522 	 15.095016717910767 	 14.964014291763306 	 0.4601905345916748 	 0.45621180534362793 	 39.01123571395874 	 29.932743787765503 	 1.1893680095672607 	 0.45626306533813477 	 
2025-08-04 15:28:35.194633 test begin: paddle.Tensor.__mul__(Tensor([2, 1, 1551, 32768],"float16"), 10000.0, )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([2, 1, 1551, 32768],"float16"), 10000.0, ) 	 101646336 	 33522 	 9.997732400894165 	 9.926867961883545 	 0.3047921657562256 	 0.30266261100769043 	 9.996647119522095 	 9.925480365753174 	 0.30476927757263184 	 0.30257654190063477 	 
2025-08-04 15:29:18.979160 test begin: paddle.Tensor.__mul__(Tensor([2, 1, 32768, 1551],"float16"), 10000.0, )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([2, 1, 32768, 1551],"float16"), 10000.0, ) 	 101646336 	 33522 	 9.997661352157593 	 9.945883750915527 	 0.30480360984802246 	 0.30261969566345215 	 9.996994972229004 	 9.925287246704102 	 0.3047776222229004 	 0.30258846282958984 	 
2025-08-04 15:30:03.028446 test begin: paddle.Tensor.__mul__(Tensor([2, 1, 32768, 32768],"float16"), 10000.0, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f02fea23940>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 15:40:10.443355 test begin: paddle.Tensor.__ne__(Tensor([144, 392, 901],"float32"), 0, )
W0804 15:40:11.412632  5349 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([144, 392, 901],"float32"), 0, ) 	 50859648 	 21223 	 9.978129386901855 	 3.9505345821380615 	 0.24019861221313477 	 0.19014811515808105 	 None 	 None 	 None 	 None 	 
2025-08-04 15:40:25.596308 test begin: paddle.Tensor.__ne__(Tensor([144, 901, 392],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([144, 901, 392],"float32"), 0, ) 	 50859648 	 21223 	 9.980496168136597 	 3.954538345336914 	 0.2402808666229248 	 0.1901874542236328 	 None 	 None 	 None 	 None 	 
2025-08-04 15:40:41.805009 test begin: paddle.Tensor.__ne__(Tensor([160, 392, 811],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([160, 392, 811],"float32"), 0, ) 	 50865920 	 21223 	 9.982154130935669 	 3.9501564502716064 	 0.24024629592895508 	 0.1901998519897461 	 None 	 None 	 None 	 None 	 
2025-08-04 15:40:56.642871 test begin: paddle.Tensor.__ne__(Tensor([160, 811, 392],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([160, 811, 392],"float32"), 0, ) 	 50865920 	 21223 	 9.979564905166626 	 3.9676403999328613 	 0.24023771286010742 	 0.1901850700378418 	 None 	 None 	 None 	 None 	 
2025-08-04 15:41:13.384933 test begin: paddle.Tensor.__ne__(Tensor([176, 392, 737],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([176, 392, 737],"float32"), 0, ) 	 50847104 	 21223 	 9.977113723754883 	 3.9484364986419678 	 0.2399730682373047 	 0.19012761116027832 	 None 	 None 	 None 	 None 	 
2025-08-04 15:41:28.618203 test begin: paddle.Tensor.__ne__(Tensor([176, 737, 392],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([176, 737, 392],"float32"), 0, ) 	 50847104 	 21223 	 9.971133470535278 	 4.182601451873779 	 0.23996591567993164 	 0.19011259078979492 	 None 	 None 	 None 	 None 	 
2025-08-04 15:41:45.151823 test begin: paddle.Tensor.__ne__(Tensor([331, 392, 392],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([331, 392, 392],"float32"), 0, ) 	 50862784 	 21223 	 9.983919382095337 	 3.9487144947052 	 0.24013662338256836 	 0.19014954566955566 	 None 	 None 	 None 	 None 	 
2025-08-04 15:41:59.956497 test begin: paddle.Tensor.__neg__(Tensor([128, 396901],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([128, 396901],"float32"), ) 	 50803328 	 33840 	 10.00155758857727 	 10.08704137802124 	 0.3020758628845215 	 0.3041505813598633 	 10.01761269569397 	 10.072970390319824 	 0.3025932312011719 	 0.30419301986694336 	 
2025-08-04 15:42:42.849600 test begin: paddle.Tensor.__neg__(Tensor([128, 793801],"float16"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([128, 793801],"float16"), ) 	 101606528 	 33840 	 10.102041482925415 	 10.019599199295044 	 0.305098295211792 	 0.3025822639465332 	 10.084463119506836 	 10.015253782272339 	 0.30453038215637207 	 0.30249595642089844 	 
2025-08-04 15:43:26.931180 test begin: paddle.Tensor.__neg__(Tensor([22, 81, 94, 311],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([22, 81, 94, 311],"float32"), ) 	 52094988 	 33840 	 10.275466442108154 	 10.32957649230957 	 0.31011486053466797 	 0.31174397468566895 	 10.268942832946777 	 10.320788621902466 	 0.31011176109313965 	 0.31169962882995605 	 
2025-08-04 15:44:12.328416 test begin: paddle.Tensor.__neg__(Tensor([264, 192612],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([264, 192612],"float32"), ) 	 50849568 	 33840 	 10.02342414855957 	 10.692365169525146 	 0.30277061462402344 	 0.3044729232788086 	 10.022809743881226 	 10.081663846969604 	 0.30271315574645996 	 0.30448150634765625 	 
2025-08-04 15:44:56.255475 test begin: paddle.Tensor.__neg__(Tensor([4, 435, 94, 311],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([4, 435, 94, 311],"float32"), ) 	 50867160 	 33840 	 10.019697189331055 	 10.087688684463501 	 0.3025851249694824 	 0.30460286140441895 	 10.025836944580078 	 10.087073802947998 	 0.3028254508972168 	 0.3046448230743408 	 
2025-08-04 15:45:38.218123 test begin: paddle.Tensor.__neg__(Tensor([4, 81, 505, 311],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([4, 81, 505, 311],"float32"), ) 	 50885820 	 33840 	 10.557538270950317 	 10.08895206451416 	 0.30252814292907715 	 0.30472588539123535 	 10.02910590171814 	 10.090174198150635 	 0.3028745651245117 	 0.3047165870666504 	 
2025-08-04 15:46:21.761039 test begin: paddle.Tensor.__neg__(Tensor([4, 81, 94, 1669],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([4, 81, 94, 1669],"float32"), ) 	 50831064 	 33840 	 10.014991044998169 	 10.083492517471313 	 0.30248475074768066 	 0.30436253547668457 	 10.019287109375 	 10.078532457351685 	 0.3025825023651123 	 0.3043949604034424 	 
2025-08-04 15:47:03.691049 test begin: paddle.Tensor.__neg__(Tensor([528, 192612],"float16"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([528, 192612],"float16"), ) 	 101699136 	 33840 	 10.092907190322876 	 10.029293298721313 	 0.304826021194458 	 0.3028857707977295 	 10.098991394042969 	 10.02355146408081 	 0.3050041198730469 	 0.3027207851409912 	 
2025-08-04 15:47:47.728845 test begin: paddle.Tensor.__or__(Tensor([1, 210, 241921],"bool"), Tensor([1, 210, 241921],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 210, 241921],"bool"), Tensor([1, 210, 241921],"bool"), ) 	 101606820 	 85484 	 10.09259033203125 	 9.963886022567749 	 0.1206822395324707 	 0.11907553672790527 	 None 	 None 	 None 	 None 	 
2025-08-04 15:48:10.010281 test begin: paddle.Tensor.__or__(Tensor([1, 210, 75600],"bool"), Tensor([4, 210, 75600],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 210, 75600],"bool"), Tensor([4, 210, 75600],"bool"), ) 	 79380000 	 85484 	 13.837248802185059 	 23.756834030151367 	 0.16543078422546387 	 0.28398990631103516 	 None 	 None 	 None 	 None 	 
2025-08-04 15:48:48.717067 test begin: paddle.Tensor.__or__(Tensor([1, 218, 233043],"bool"), Tensor([1, 218, 233043],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 218, 233043],"bool"), Tensor([1, 218, 233043],"bool"), ) 	 101606748 	 85484 	 10.092503786087036 	 9.955565690994263 	 0.12067174911499023 	 0.11904692649841309 	 None 	 None 	 None 	 None 	 
2025-08-04 15:49:10.746007 test begin: paddle.Tensor.__or__(Tensor([1, 218, 70644],"bool"), Tensor([4, 218, 70644],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 218, 70644],"bool"), Tensor([4, 218, 70644],"bool"), ) 	 77001960 	 85484 	 13.444010019302368 	 23.011279344558716 	 0.16072297096252441 	 0.2751150131225586 	 None 	 None 	 None 	 None 	 
2025-08-04 15:49:48.278254 test begin: paddle.Tensor.__or__(Tensor([1, 400, 127009],"bool"), Tensor([1, 400, 127009],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 400, 127009],"bool"), Tensor([1, 400, 127009],"bool"), ) 	 101607200 	 85484 	 10.096647500991821 	 10.00922703742981 	 0.12073516845703125 	 0.11967921257019043 	 None 	 None 	 None 	 None 	 
2025-08-04 15:50:09.807093 test begin: paddle.Tensor.__or__(Tensor([1, 400, 65856],"bool"), Tensor([2, 400, 65856],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 400, 65856],"bool"), Tensor([2, 400, 65856],"bool"), ) 	 79027200 	 85484 	 11.551344633102417 	 19.81116008758545 	 0.13813328742980957 	 0.236863374710083 	 None 	 None 	 None 	 None 	 
2025-08-04 15:50:42.317485 test begin: paddle.Tensor.__or__(Tensor([1, 673, 75600],"bool"), Tensor([1, 673, 75600],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 673, 75600],"bool"), Tensor([1, 673, 75600],"bool"), ) 	 101757600 	 85484 	 10.090962171554565 	 9.854288339614868 	 0.12065720558166504 	 0.11778664588928223 	 None 	 None 	 None 	 None 	 
2025-08-04 15:51:03.677977 test begin: paddle.Tensor.__or__(Tensor([1, 720, 70644],"bool"), Tensor([1, 720, 70644],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 720, 70644],"bool"), Tensor([1, 720, 70644],"bool"), ) 	 101727360 	 85484 	 10.087860107421875 	 9.919296264648438 	 0.1205897331237793 	 0.1185598373413086 	 None 	 None 	 None 	 None 	 
2025-08-04 15:51:25.418131 test begin: paddle.Tensor.__or__(Tensor([1, 772, 65856],"bool"), Tensor([1, 772, 65856],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 772, 65856],"bool"), Tensor([1, 772, 65856],"bool"), ) 	 101681664 	 85484 	 10.064905405044556 	 9.832786798477173 	 0.12033796310424805 	 0.11745786666870117 	 None 	 None 	 None 	 None 	 
2025-08-04 15:51:47.504262 test begin: paddle.Tensor.__or__(Tensor([2, 400, 65856],"bool"), Tensor([1, 400, 65856],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([2, 400, 65856],"bool"), Tensor([1, 400, 65856],"bool"), ) 	 79027200 	 85484 	 15.100309133529663 	 19.8232159614563 	 0.1805558204650879 	 0.23672175407409668 	 None 	 None 	 None 	 None 	 
2025-08-04 15:52:23.568035 test begin: paddle.Tensor.__or__(Tensor([2, 400, 65856],"bool"), Tensor([2, 400, 65856],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([2, 400, 65856],"bool"), Tensor([2, 400, 65856],"bool"), ) 	 105369600 	 85484 	 10.442752122879028 	 10.828903675079346 	 0.12485885620117188 	 0.12143540382385254 	 None 	 None 	 None 	 None 	 
2025-08-04 15:52:47.758479 test begin: paddle.rank(input=Tensor([201, 12700801],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([201, 12700801],"float64"), ) 	 2552861001 	 247008 	 9.680538654327393 	 7.07461953163147 	 3.695487976074219e-05 	 6.747245788574219e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:54:07.203717 test begin: paddle.rank(input=Tensor([301, 2, 2, 2116801],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([301, 2, 2, 2116801],"float64"), ) 	 2548628404 	 247008 	 9.71582293510437 	 6.907843351364136 	 4.00543212890625e-05 	 6.556510925292969e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:55:33.610507 test begin: paddle.rank(input=Tensor([301, 2, 2116801, 2],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([301, 2, 2116801, 2],"float64"), ) 	 2548628404 	 247008 	 9.902296304702759 	 6.952672243118286 	 4.315376281738281e-05 	 7.176399230957031e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:56:56.227260 test begin: paddle.rank(input=Tensor([301, 2116801, 2, 2],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([301, 2116801, 2, 2],"float64"), ) 	 2548628404 	 247008 	 9.614738464355469 	 6.89194917678833 	 4.2438507080078125e-05 	 7.367134094238281e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:58:13.408020 test begin: paddle.rank(input=Tensor([317520101, 2, 2, 2],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([317520101, 2, 2, 2],"float64"), ) 	 2540160808 	 247008 	 9.679452419281006 	 6.953213930130005 	 4.57763671875e-05 	 6.818771362304688e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 15:59:29.906608 test begin: paddle.reciprocal(Tensor([125, 1, 640, 640],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([125, 1, 640, 640],"float32"), ) 	 51200000 	 33850 	 10.068788290023804 	 10.16348147392273 	 0.30397629737854004 	 0.30686140060424805 	 15.330393314361572 	 35.482306718826294 	 0.4628880023956299 	 0.3571135997772217 	 
2025-08-04 16:00:42.780519 test begin: paddle.reciprocal(Tensor([16, 1, 4962, 640],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([16, 1, 4962, 640],"float32"), ) 	 50810880 	 33850 	 9.997663259506226 	 10.557888984680176 	 0.3018162250518799 	 0.3046092987060547 	 15.216460466384888 	 35.21907877922058 	 0.45948338508605957 	 0.35448265075683594 	 
2025-08-04 16:01:58.687552 test begin: paddle.reciprocal(Tensor([16, 1, 640, 4962],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([16, 1, 640, 4962],"float32"), ) 	 50810880 	 33850 	 9.99801754951477 	 10.088470220565796 	 0.30190181732177734 	 0.304563045501709 	 15.216542959213257 	 35.21884775161743 	 0.4594600200653076 	 0.35446834564208984 	 
2025-08-04 16:03:11.635351 test begin: paddle.reciprocal(Tensor([16, 8, 640, 640],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([16, 8, 640, 640],"float32"), ) 	 52428800 	 33850 	 10.313899755477905 	 10.417798280715942 	 0.3113832473754883 	 0.31420302391052246 	 15.698874950408936 	 36.32262682914734 	 0.4739694595336914 	 0.3656346797943115 	 
2025-08-04 16:04:27.355345 test begin: paddle.reciprocal(Tensor([4, 1, 13231, 960],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([4, 1, 13231, 960],"float32"), ) 	 50807040 	 33850 	 10.002031326293945 	 10.360032320022583 	 0.30202198028564453 	 0.3046300411224365 	 15.212469816207886 	 35.217052698135376 	 0.45932793617248535 	 0.35443925857543945 	 
2025-08-04 16:05:41.161506 test begin: paddle.reciprocal(Tensor([4, 1, 960, 13231],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([4, 1, 960, 13231],"float32"), ) 	 50807040 	 33850 	 10.001872301101685 	 10.087639808654785 	 0.30196666717529297 	 0.3045480251312256 	 15.211630821228027 	 35.21568179130554 	 0.45923423767089844 	 0.3544657230377197 	 
2025-08-04 16:06:53.426384 test begin: paddle.reciprocal(Tensor([4, 14, 960, 960],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([4, 14, 960, 960],"float32"), ) 	 51609600 	 33850 	 10.148037195205688 	 10.242980718612671 	 0.30642175674438477 	 0.30925583839416504 	 15.454308271408081 	 35.76312708854675 	 0.46654701232910156 	 0.3600289821624756 	 
2025-08-04 16:08:06.801410 test begin: paddle.reciprocal(Tensor([56, 1, 960, 960],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([56, 1, 960, 960],"float32"), ) 	 51609600 	 33850 	 10.147895336151123 	 10.254764556884766 	 0.3063778877258301 	 0.3092343807220459 	 15.453207731246948 	 35.761887550354004 	 0.4666304588317871 	 0.35992860794067383 	 
2025-08-04 16:09:21.221310 test begin: paddle.reciprocal(Tensor([8, 1, 6616, 960],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([8, 1, 6616, 960],"float32"), ) 	 50810880 	 33850 	 9.998099327087402 	 10.090593814849854 	 0.30188679695129395 	 0.3046557903289795 	 15.215583562850952 	 35.21752452850342 	 0.4594235420227051 	 0.35450077056884766 	 
2025-08-04 16:10:34.302542 test begin: paddle.reciprocal(Tensor([8, 1, 960, 6616],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([8, 1, 960, 6616],"float32"), ) 	 50810880 	 33850 	 9.997963905334473 	 10.088549375534058 	 0.30190610885620117 	 0.3046095371246338 	 15.215810775756836 	 35.21920156478882 	 0.4593322277069092 	 0.3544890880584717 	 
2025-08-04 16:11:48.132258 test begin: paddle.reciprocal(Tensor([8, 7, 960, 960],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([8, 7, 960, 960],"float32"), ) 	 51609600 	 33850 	 10.147928714752197 	 10.242419481277466 	 0.30635547637939453 	 0.30927181243896484 	 15.454505681991577 	 35.7636992931366 	 0.4665679931640625 	 0.35997557640075684 	 
2025-08-04 16:13:01.513521 test begin: paddle.reduce_as(Tensor([30, 1270081, 40],"float32"), Tensor([1270081, 40],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1c7d71e980>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:23:14.660550 test begin: paddle.reduce_as(Tensor([30, 200, 254017],"float32"), Tensor([200, 254017],"float32"), )
W0804 16:23:45.332170 19726 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f225bf02fb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:33:19.789922 test begin: paddle.reduce_as(Tensor([30, 200, 8468],"float32"), Tensor([200, 8468],"float32"), )
W0804 16:33:20.804001 23986 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.reduce_as 	 paddle.reduce_as(Tensor([30, 200, 8468],"float32"), Tensor([200, 8468],"float32"), ) 	 52501600 	 56022 	 9.995075702667236 	 8.7265305519104 	 0.18232226371765137 	 0.15907692909240723 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 16:33:50.847340 test begin: paddle.reduce_as(Tensor([30, 42337, 40],"float32"), Tensor([42337, 40],"float32"), )
[Prof] paddle.reduce_as 	 paddle.reduce_as(Tensor([30, 42337, 40],"float32"), Tensor([42337, 40],"float32"), ) 	 52497880 	 56022 	 10.044360399246216 	 9.905015707015991 	 0.18321800231933594 	 0.1597309112548828 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 16:34:24.719412 test begin: paddle.reduce_as(Tensor([6351, 200, 40],"float32"), Tensor([200, 40],"float32"), )
[Prof] paddle.reduce_as 	 paddle.reduce_as(Tensor([6351, 200, 40],"float32"), Tensor([200, 40],"float32"), ) 	 50816000 	 56022 	 14.518906593322754 	 8.703885316848755 	 0.1325361728668213 	 0.07937192916870117 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 16:34:58.199044 test begin: paddle.remainder(Tensor([1, 2, 1270081, 4, 5],"float32"), Tensor([1, 2, 1270081, 4, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 1270081, 4, 5],"float32"), Tensor([1, 2, 1270081, 4, 5],"float32"), ) 	 101606480 	 33680 	 15.183262348175049 	 15.123332023620605 	 0.4607210159301758 	 0.4588744640350342 	 None 	 None 	 None 	 None 	 
2025-08-04 16:35:33.602459 test begin: paddle.remainder(Tensor([1, 2, 1270081, 4, 5],"int32"), Tensor([1, 2, 1270081, 4, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 1270081, 4, 5],"int32"), Tensor([1, 2, 1270081, 4, 5],"int32"), ) 	 101606480 	 33680 	 15.19300889968872 	 15.136270999908447 	 0.4606466293334961 	 0.45931434631347656 	 None 	 None 	 None 	 None 	 
2025-08-04 16:36:05.205547 test begin: paddle.remainder(Tensor([1, 2, 3, 1693441, 5],"float32"), Tensor([1, 2, 3, 1693441, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 1693441, 5],"float32"), Tensor([1, 2, 3, 1693441, 5],"float32"), ) 	 101606460 	 33680 	 15.18654203414917 	 15.129041910171509 	 0.46085095405578613 	 0.45897936820983887 	 None 	 None 	 None 	 None 	 
2025-08-04 16:36:38.207785 test begin: paddle.remainder(Tensor([1, 2, 3, 1693441, 5],"int32"), Tensor([1, 2, 3, 1693441, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 1693441, 5],"int32"), Tensor([1, 2, 3, 1693441, 5],"int32"), ) 	 101606460 	 33680 	 15.187187671661377 	 15.138849258422852 	 0.46079444885253906 	 0.45925259590148926 	 None 	 None 	 None 	 None 	 
2025-08-04 16:37:09.812311 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 1058401],"float64"), Tensor([1, 2, 3, 4, 1058401],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 1058401],"float64"), Tensor([1, 2, 3, 4, 1058401],"float64"), ) 	 50803248 	 33680 	 15.037501573562622 	 15.36806869506836 	 0.4563143253326416 	 0.46643877029418945 	 None 	 None 	 None 	 None 	 
2025-08-04 16:37:43.339314 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 2116801],"float32"), Tensor([1, 2, 3, 4, 2116801],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 2116801],"float32"), Tensor([1, 2, 3, 4, 2116801],"float32"), ) 	 101606448 	 33680 	 15.18595576286316 	 15.129675388336182 	 0.4607973098754883 	 0.45896339416503906 	 None 	 None 	 None 	 None 	 
2025-08-04 16:38:15.375973 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 2116801],"int32"), Tensor([1, 2, 3, 4, 2116801],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 2116801],"int32"), Tensor([1, 2, 3, 4, 2116801],"int32"), ) 	 101606448 	 33680 	 15.184808492660522 	 15.135154247283936 	 0.46075940132141113 	 0.45925426483154297 	 None 	 None 	 None 	 None 	 
2025-08-04 16:38:46.936044 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 5],"float32"), Tensor([423361, 2, 3, 4, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 5],"float32"), Tensor([423361, 2, 3, 4, 5],"float32"), ) 	 50803440 	 33680 	 10.018409252166748 	 11.155330181121826 	 0.3039824962615967 	 0.3384847640991211 	 None 	 None 	 None 	 None 	 
2025-08-04 16:39:08.974302 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 5],"float64"), Tensor([211681, 2, 3, 4, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 5],"float64"), Tensor([211681, 2, 3, 4, 5],"float64"), ) 	 25401840 	 33680 	 14.650084257125854 	 12.436339855194092 	 0.4445509910583496 	 0.3773336410522461 	 None 	 None 	 None 	 None 	 
2025-08-04 16:39:38.543419 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 5],"int32"), Tensor([423361, 2, 3, 4, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 5],"int32"), Tensor([423361, 2, 3, 4, 5],"int32"), ) 	 50803440 	 33680 	 10.11838436126709 	 11.54910922050476 	 0.30667901039123535 	 0.35044384002685547 	 None 	 None 	 None 	 None 	 
2025-08-04 16:40:03.773817 test begin: paddle.remainder(Tensor([1, 2, 3, 846721, 5],"float64"), Tensor([1, 2, 3, 846721, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 846721, 5],"float64"), Tensor([1, 2, 3, 846721, 5],"float64"), ) 	 50803260 	 33680 	 15.03805923461914 	 15.369999647140503 	 0.4563565254211426 	 0.466322660446167 	 None 	 None 	 None 	 None 	 
2025-08-04 16:40:38.770945 test begin: paddle.remainder(Tensor([1, 2, 635041, 4, 5],"float64"), Tensor([1, 2, 635041, 4, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 635041, 4, 5],"float64"), Tensor([1, 2, 635041, 4, 5],"float64"), ) 	 50803280 	 33680 	 15.048968315124512 	 15.367205142974854 	 0.4562101364135742 	 0.46631383895874023 	 None 	 None 	 None 	 None 	 
2025-08-04 16:41:14.548959 test begin: paddle.remainder(Tensor([1, 423361, 3, 4, 5],"float64"), Tensor([1, 423361, 3, 4, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 423361, 3, 4, 5],"float64"), Tensor([1, 423361, 3, 4, 5],"float64"), ) 	 50803320 	 33680 	 15.049547910690308 	 15.368220329284668 	 0.4563472270965576 	 0.4662771224975586 	 None 	 None 	 None 	 None 	 
2025-08-04 16:41:46.143081 test begin: paddle.remainder(Tensor([1, 846721, 3, 4, 5],"float32"), Tensor([1, 846721, 3, 4, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 846721, 3, 4, 5],"float32"), Tensor([1, 846721, 3, 4, 5],"float32"), ) 	 101606520 	 33680 	 15.184202671051025 	 15.125803470611572 	 0.46064257621765137 	 0.45893287658691406 	 None 	 None 	 None 	 None 	 
2025-08-04 16:42:18.185947 test begin: paddle.remainder(Tensor([1, 846721, 3, 4, 5],"int32"), Tensor([1, 846721, 3, 4, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 846721, 3, 4, 5],"int32"), Tensor([1, 846721, 3, 4, 5],"int32"), ) 	 101606520 	 33680 	 15.182273387908936 	 15.145143270492554 	 0.4608147144317627 	 0.45921826362609863 	 None 	 None 	 None 	 None 	 
2025-08-04 16:42:50.025839 test begin: paddle.remainder(Tensor([211681, 2, 3, 4, 5],"float64"), Tensor([1, 2, 3, 4, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([211681, 2, 3, 4, 5],"float64"), Tensor([1, 2, 3, 4, 5],"float64"), ) 	 25401840 	 33680 	 14.22870659828186 	 12.879937648773193 	 0.43178510665893555 	 0.39084362983703613 	 None 	 None 	 None 	 None 	 
2025-08-04 16:43:17.709949 test begin: paddle.remainder(Tensor([211681, 2, 3, 4, 5],"float64"), Tensor([211681, 2, 3, 4, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([211681, 2, 3, 4, 5],"float64"), Tensor([211681, 2, 3, 4, 5],"float64"), ) 	 50803440 	 33680 	 15.038593053817749 	 15.385813474655151 	 0.4563467502593994 	 0.46626734733581543 	 None 	 None 	 None 	 None 	 
2025-08-04 16:43:50.251023 test begin: paddle.remainder(Tensor([423361, 2, 3, 4, 5],"float32"), Tensor([1, 2, 3, 4, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([423361, 2, 3, 4, 5],"float32"), Tensor([1, 2, 3, 4, 5],"float32"), ) 	 50803440 	 33680 	 9.99855661392212 	 11.165372371673584 	 0.3032264709472656 	 0.3388044834136963 	 None 	 None 	 None 	 None 	 
2025-08-04 16:44:12.279732 test begin: paddle.remainder(Tensor([423361, 2, 3, 4, 5],"float32"), Tensor([423361, 2, 3, 4, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([423361, 2, 3, 4, 5],"float32"), Tensor([423361, 2, 3, 4, 5],"float32"), ) 	 101606640 	 33680 	 15.186887741088867 	 15.131093502044678 	 0.4607994556427002 	 0.4588780403137207 	 None 	 None 	 None 	 None 	 
2025-08-04 16:44:44.332460 test begin: paddle.remainder(Tensor([423361, 2, 3, 4, 5],"int32"), Tensor([1, 2, 3, 4, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([423361, 2, 3, 4, 5],"int32"), Tensor([1, 2, 3, 4, 5],"int32"), ) 	 50803440 	 33680 	 10.042833089828491 	 12.797430515289307 	 0.30461621284484863 	 0.34528279304504395 	 None 	 None 	 None 	 None 	 
2025-08-04 16:45:09.249414 test begin: paddle.remainder(Tensor([423361, 2, 3, 4, 5],"int32"), Tensor([423361, 2, 3, 4, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([423361, 2, 3, 4, 5],"int32"), Tensor([423361, 2, 3, 4, 5],"int32"), ) 	 101606640 	 33680 	 15.18559718132019 	 15.135244369506836 	 0.4608113765716553 	 0.45923280715942383 	 None 	 None 	 None 	 None 	 
2025-08-04 16:45:41.486895 test begin: paddle.renorm(Tensor([10, 20, 254017],"float32"), 1.0, -1, 2.05, )
[Prof] paddle.renorm 	 paddle.renorm(Tensor([10, 20, 254017],"float32"), 1.0, -1, 2.05, ) 	 50803400 	 3503 	 9.981213092803955 	 1.6852288246154785 	 0.7263021469116211 	 0.16384434700012207 	 19.435415267944336 	 10.225928783416748 	 1.4175572395324707 	 0.22916078567504883 	 
2025-08-04 16:46:24.696191 test begin: paddle.repeat_interleave(Tensor([1, 1500, 33869],"float32"), 5, axis=0, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([1, 1500, 33869],"float32"), 5, axis=0, ) 	 50803500 	 9813 	 18.19942545890808 	 14.802550792694092 	 0.9477357864379883 	 1.5415308475494385 	 23.666933298110962 	 8.581153392791748 	 0.8222897052764893 	 0.8937315940856934 	 
2025-08-04 16:47:35.032249 test begin: paddle.repeat_interleave(Tensor([1, 39691, 1280],"float32"), 5, axis=0, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([1, 39691, 1280],"float32"), 5, axis=0, ) 	 50804480 	 9813 	 18.12514328956604 	 14.644658327102661 	 0.9437036514282227 	 1.5252466201782227 	 23.57076597213745 	 8.467427968978882 	 0.81888747215271 	 0.8818356990814209 	 
2025-08-04 16:48:46.954611 test begin: paddle.repeat_interleave(Tensor([14, 1, 384, 9451],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([14, 1, 384, 9451],"float32"), repeats=3, axis=1, ) 	 50808576 	 9813 	 10.746982097625732 	 8.329337120056152 	 0.5596165657043457 	 0.8672366142272949 	 11.983425378799438 	 5.756941795349121 	 0.41628193855285645 	 0.5995335578918457 	 
2025-08-04 16:49:27.282679 test begin: paddle.repeat_interleave(Tensor([14, 1, 9451, 384],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([14, 1, 9451, 384],"float32"), repeats=3, axis=1, ) 	 50808576 	 9813 	 10.746396780014038 	 8.327102899551392 	 0.559593677520752 	 0.8672399520874023 	 11.988633394241333 	 5.757033824920654 	 0.4164719581604004 	 0.5995688438415527 	 
2025-08-04 16:50:07.601743 test begin: paddle.repeat_interleave(Tensor([14, 25, 384, 384],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([14, 25, 384, 384],"float32"), repeats=3, axis=1, ) 	 51609600 	 9813 	 10.175509452819824 	 7.166893482208252 	 0.5298426151275635 	 0.7464182376861572 	 11.771745443344116 	 5.912274599075317 	 0.4089176654815674 	 0.6157679557800293 	 
2025-08-04 16:50:46.186869 test begin: paddle.repeat_interleave(Tensor([27, 1500, 1280],"float32"), 5, axis=0, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([27, 1500, 1280],"float32"), 5, axis=0, ) 	 51840000 	 9813 	 16.44721269607544 	 10.940696954727173 	 0.8564660549163818 	 1.2028295993804932 	 18.255844831466675 	 8.640029430389404 	 0.6341879367828369 	 0.8997962474822998 	 
2025-08-04 16:51:46.219889 test begin: paddle.repeat_interleave(Tensor([345, 1, 384, 384],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([345, 1, 384, 384],"float32"), repeats=3, axis=1, ) 	 50872320 	 9813 	 10.031798601150513 	 7.067681789398193 	 0.5222713947296143 	 0.7360873222351074 	 11.60501480102539 	 5.829187870025635 	 0.4031672477722168 	 0.6071188449859619 	 
2025-08-04 16:52:24.377740 test begin: paddle.repeat_interleave(Tensor([5, 1, 13231, 768],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([5, 1, 13231, 768],"float32"), repeats=3, axis=1, ) 	 50807040 	 9813 	 10.90657377243042 	 8.866572856903076 	 0.5679421424865723 	 0.9234058856964111 	 14.6957266330719 	 5.756670236587524 	 0.5105435848236084 	 0.5995640754699707 	 
2025-08-04 16:53:08.019462 test begin: paddle.repeat_interleave(Tensor([5, 1, 768, 13231],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([5, 1, 768, 13231],"float32"), repeats=3, axis=1, ) 	 50807040 	 9813 	 10.906498432159424 	 8.928008794784546 	 0.5679306983947754 	 0.923464298248291 	 14.695001363754272 	 5.756488084793091 	 0.5105037689208984 	 0.5995142459869385 	 
2025-08-04 16:53:53.918825 test begin: paddle.repeat_interleave(Tensor([5, 18, 768, 768],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([5, 18, 768, 768],"float32"), repeats=3, axis=1, ) 	 53084160 	 9813 	 10.272234439849854 	 6.8945183753967285 	 0.534888505935669 	 0.717944860458374 	 12.168493747711182 	 6.134427309036255 	 0.42272377014160156 	 0.6388275623321533 	 
2025-08-04 16:54:33.052940 test begin: paddle.repeat_interleave(Tensor([87, 1, 768, 768],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([87, 1, 768, 768],"float32"), repeats=3, axis=1, ) 	 51314688 	 9813 	 9.941285133361816 	 6.6604084968566895 	 0.5172655582427979 	 0.6936728954315186 	 11.767608880996704 	 5.931760787963867 	 0.4087705612182617 	 0.617729902267456 	 
2025-08-04 16:55:10.946325 test begin: paddle.reshape(Tensor([141760, 7168],"bfloat16"), list[-1,7168,], )
[Prof] paddle.reshape 	 paddle.reshape(Tensor([141760, 7168],"bfloat16"), list[-1,7168,], ) 	 1016135680 	 66714 	 0.3463606834411621 	 0.2657504081726074 	 2.956390380859375e-05 	 2.86102294921875e-05 	 3.3216311931610107 	 299.9702410697937 	 5.2928924560546875e-05 	 2.2978692054748535 	 
2025-08-04 17:00:48.293066 test begin: paddle.reverse(Tensor([12, 132301, 16],"float64"), axis=list[0,], )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([12, 132301, 16],"float64"), axis=list[0,], ) 	 25401792 	 19922 	 10.064478635787964 	 6.057135343551636 	 0.5163557529449463 	 0.31035542488098145 	 10.072486162185669 	 6.049286842346191 	 0.5166914463043213 	 0.31034374237060547 	 
2025-08-04 17:01:22.776143 test begin: paddle.reverse(Tensor([12, 264601, 8],"float64"), axis=0, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([12, 264601, 8],"float64"), axis=0, ) 	 25401696 	 19922 	 10.065921783447266 	 6.050760507583618 	 0.5164046287536621 	 0.310413122177124 	 10.09173583984375 	 6.050004005432129 	 0.5176830291748047 	 0.31029844284057617 	 
2025-08-04 17:01:56.179671 test begin: paddle.reverse(Tensor([12, 4, 529201],"float64"), axis=0, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([12, 4, 529201],"float64"), axis=0, ) 	 25401648 	 19922 	 10.070350885391235 	 6.050250291824341 	 0.5166704654693604 	 0.31035304069519043 	 10.09070873260498 	 6.0495874881744385 	 0.5176749229431152 	 0.3103330135345459 	 
2025-08-04 17:02:29.541612 test begin: paddle.reverse(Tensor([12, 4, 529201],"float64"), axis=list[0,], )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([12, 4, 529201],"float64"), axis=list[0,], ) 	 25401648 	 19922 	 10.07028317451477 	 6.050325393676758 	 0.5166134834289551 	 0.3103957176208496 	 10.090373754501343 	 6.049635648727417 	 0.5176239013671875 	 0.310344934463501 	 
2025-08-04 17:03:02.937050 test begin: paddle.reverse(Tensor([396901, 4, 16],"float64"), axis=list[0,], )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([396901, 4, 16],"float64"), axis=list[0,], ) 	 25401664 	 19922 	 9.988604545593262 	 6.025733470916748 	 0.5124578475952148 	 0.30915331840515137 	 10.015509128570557 	 6.024571895599365 	 0.5137598514556885 	 0.3090519905090332 	 
2025-08-04 17:03:38.474016 test begin: paddle.reverse(Tensor([4, 12, 529201],"float64"), axis=1, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([4, 12, 529201],"float64"), axis=1, ) 	 25401648 	 19922 	 10.0659761428833 	 6.082752227783203 	 0.5164110660552979 	 0.31203484535217285 	 10.089499950408936 	 6.085995674133301 	 0.5176401138305664 	 0.3121938705444336 	 
2025-08-04 17:04:11.900307 test begin: paddle.reverse(Tensor([4, 198451, 32],"float64"), axis=1, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([4, 198451, 32],"float64"), axis=1, ) 	 25401728 	 19922 	 10.018154382705688 	 6.036778211593628 	 0.5139031410217285 	 0.3096954822540283 	 9.988448858261108 	 6.039890289306641 	 0.5123672485351562 	 0.30975985527038574 	 
2025-08-04 17:04:45.097335 test begin: paddle.reverse(Tensor([66151, 12, 32],"float64"), axis=1, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([66151, 12, 32],"float64"), axis=1, ) 	 25401984 	 19922 	 10.018966913223267 	 6.039655685424805 	 0.5139138698577881 	 0.30985498428344727 	 9.994310855865479 	 6.040743589401245 	 0.5126426219940186 	 0.309891939163208 	 
2025-08-04 17:05:18.680370 test begin: paddle.reverse(Tensor([793801, 4, 8],"float64"), axis=0, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([793801, 4, 8],"float64"), axis=0, ) 	 25401632 	 19922 	 10.004757642745972 	 6.044097185134888 	 0.5132629871368408 	 0.30933380126953125 	 10.011229515075684 	 6.029857397079468 	 0.5136640071868896 	 0.30930566787719727 	 
2025-08-04 17:05:53.097952 test begin: paddle.roll(Tensor([128, 37, 56, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 37, 56, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 	 50921472 	 18327 	 9.963457584381104 	 14.415222406387329 	 0.5556418895721436 	 0.4022257328033447 	 9.963414907455444 	 14.367194890975952 	 0.5556168556213379 	 0.4005413055419922 	 
2025-08-04 17:06:43.622443 test begin: paddle.roll(Tensor([128, 37, 56, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 37, 56, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 	 50921472 	 18327 	 9.96392273902893 	 14.372124433517456 	 0.5556473731994629 	 0.40068793296813965 	 9.961601972579956 	 14.399692296981812 	 0.5555078983306885 	 0.40148210525512695 	 
2025-08-04 17:07:34.048006 test begin: paddle.roll(Tensor([128, 56, 37, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 56, 37, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 	 50921472 	 18327 	 9.962477684020996 	 14.457443237304688 	 0.5555689334869385 	 0.40312647819519043 	 9.963088035583496 	 14.42951488494873 	 0.5555846691131592 	 0.40230679512023926 	 
2025-08-04 17:08:24.586265 test begin: paddle.roll(Tensor([128, 56, 37, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 56, 37, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 	 50921472 	 18327 	 9.963197946548462 	 14.426493883132935 	 0.5556225776672363 	 0.4021158218383789 	 9.960195779800415 	 14.453317403793335 	 0.555413007736206 	 0.4030005931854248 	 
2025-08-04 17:09:15.115777 test begin: paddle.roll(Tensor([128, 56, 56, 127],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 56, 56, 127],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 	 50978816 	 18327 	 9.97106122970581 	 14.472558736801147 	 0.5560696125030518 	 0.40300512313842773 	 9.97078013420105 	 14.407220363616943 	 0.5560336112976074 	 0.4016883373260498 	 
2025-08-04 17:10:06.508549 test begin: paddle.roll(Tensor([128, 56, 56, 127],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 56, 56, 127],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 	 50978816 	 18327 	 9.972117185592651 	 14.408713817596436 	 0.5560932159423828 	 0.40177297592163086 	 9.971975564956665 	 14.45197606086731 	 0.556060791015625 	 0.4029364585876465 	 
2025-08-04 17:10:57.417329 test begin: paddle.roll(Tensor([44, 96, 96, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([44, 96, 96, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 	 51904512 	 18327 	 10.141986608505249 	 14.613527059555054 	 0.5655758380889893 	 0.40749096870422363 	 10.141735792160034 	 14.555608987808228 	 0.5655410289764404 	 0.4058370590209961 	 
2025-08-04 17:11:48.652994 test begin: paddle.roll(Tensor([64, 65, 96, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([64, 65, 96, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 	 51118080 	 18327 	 9.998311996459961 	 14.396563053131104 	 0.5575706958770752 	 0.4014246463775635 	 9.990545988082886 	 14.349509477615356 	 0.557131290435791 	 0.4000859260559082 	 
2025-08-04 17:12:40.208920 test begin: paddle.roll(Tensor([64, 96, 65, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([64, 96, 65, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 	 51118080 	 18327 	 9.995706558227539 	 14.413365602493286 	 0.5574460029602051 	 0.4019138813018799 	 9.991804361343384 	 14.366248846054077 	 0.5571718215942383 	 0.40057921409606934 	 
2025-08-04 17:13:30.730850 test begin: paddle.roll(Tensor([64, 96, 96, 87],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([64, 96, 96, 87],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 	 51314688 	 18327 	 10.03966736793518 	 14.51074743270874 	 0.5599162578582764 	 0.4045844078063965 	 10.037520170211792 	 14.45172119140625 	 0.5597193241119385 	 0.4029419422149658 	 
2025-08-04 17:14:21.520729 test begin: paddle.roll(Tensor([85, 56, 56, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([85, 56, 56, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 	 51179520 	 18327 	 10.012474060058594 	 14.471261262893677 	 0.5583760738372803 	 0.4033026695251465 	 10.014345407485962 	 14.425816774368286 	 0.5584614276885986 	 0.40220069885253906 	 
2025-08-04 17:15:12.189309 test begin: paddle.roll(Tensor([85, 56, 56, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([85, 56, 56, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 	 51179520 	 18327 	 10.013534784317017 	 14.425283193588257 	 0.5584275722503662 	 0.4022226333618164 	 10.013009548187256 	 14.46194314956665 	 0.5583696365356445 	 0.4003031253814697 	 
2025-08-04 17:16:02.796872 test begin: paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 19442 	 10.09378433227539 	 5.902161598205566 	 0.5305843353271484 	 0.3098609447479248 	 16.106769800186157 	 5.902524948120117 	 0.423382043838501 	 0.3102574348449707 	 
2025-08-04 17:16:41.994378 test begin: paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 19442 	 16.02161431312561 	 5.8944175243377686 	 0.42109060287475586 	 0.30988526344299316 	 10.10223388671875 	 5.8928399085998535 	 0.5309929847717285 	 0.3097703456878662 	 
2025-08-04 17:17:21.039112 test begin: paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 19442 	 15.943227291107178 	 5.89442253112793 	 0.4190685749053955 	 0.30986452102661133 	 10.102437257766724 	 5.889951467514038 	 0.5310392379760742 	 0.30962181091308594 	 
2025-08-04 17:18:00.003623 test begin: paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 19442 	 10.111093521118164 	 5.908915042877197 	 0.531548261642456 	 0.3105781078338623 	 16.101933240890503 	 5.8819544315338135 	 0.42314863204956055 	 0.30918002128601074 	 
2025-08-04 17:18:40.639759 test begin: paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 19442 	 18.53206753730774 	 5.908797979354858 	 0.48711228370666504 	 0.31058740615844727 	 10.13614559173584 	 5.914078712463379 	 0.5328295230865479 	 0.31085777282714844 	 
2025-08-04 17:19:22.267156 test begin: paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 19442 	 15.942039489746094 	 5.897010803222656 	 0.41901111602783203 	 0.3098151683807373 	 10.101486921310425 	 5.8897974491119385 	 0.5310046672821045 	 0.30964183807373047 	 
2025-08-04 17:20:01.811853 test begin: paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 19442 	 10.128567457199097 	 5.910384654998779 	 0.5324015617370605 	 0.31072402000427246 	 16.125014543533325 	 5.905955791473389 	 0.42385292053222656 	 0.310438871383667 	 
2025-08-04 17:20:41.042928 test begin: paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 19442 	 16.253769636154175 	 6.652196407318115 	 0.4272153377532959 	 0.3106870651245117 	 10.101223468780518 	 5.897012948989868 	 0.5310232639312744 	 0.31000351905822754 	 
2025-08-04 17:21:22.440206 test begin: paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 19442 	 15.983815431594849 	 5.9117772579193115 	 0.4201538562774658 	 0.3107898235321045 	 10.12885046005249 	 5.941809415817261 	 0.5324184894561768 	 0.3123745918273926 	 
2025-08-04 17:22:02.185968 test begin: paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 19442 	 10.127538681030273 	 5.910682678222656 	 0.5323436260223389 	 0.31067562103271484 	 16.124635457992554 	 5.9059553146362305 	 0.42381763458251953 	 0.3104431629180908 	 
2025-08-04 17:22:41.400707 test begin: paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 19442 	 16.15364384651184 	 7.434400796890259 	 0.4246091842651367 	 0.3107328414916992 	 10.131390810012817 	 5.951582670211792 	 0.5325756072998047 	 0.31282806396484375 	 
2025-08-04 17:23:23.203754 test begin: paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 19442 	 15.851595640182495 	 5.9486188888549805 	 0.41665124893188477 	 0.31270408630371094 	 10.100214958190918 	 5.889829158782959 	 0.5309407711029053 	 0.3095889091491699 	 
2025-08-04 17:24:02.118143 test begin: paddle.round(Tensor([128, 396901],"float32"), )
[Prof] paddle.round 	 paddle.round(Tensor([128, 396901],"float32"), ) 	 50803328 	 33846 	 9.999370336532593 	 10.076884269714355 	 0.30191993713378906 	 0.3042433261871338 	 4.541284799575806 	 4.533937692642212 	 0.1368236541748047 	 0.13687467575073242 	 
2025-08-04 17:24:33.117609 test begin: paddle.round(Tensor([16, 1587601],"float64"), )
[Prof] paddle.round 	 paddle.round(Tensor([16, 1587601],"float64"), ) 	 25401616 	 33846 	 10.317243814468384 	 10.093296527862549 	 0.311539888381958 	 0.3047060966491699 	 4.530759334564209 	 4.55139946937561 	 0.13674068450927734 	 0.13730335235595703 	 
2025-08-04 17:25:03.752300 test begin: paddle.round(Tensor([396901, 128],"float32"), )
[Prof] paddle.round 	 paddle.round(Tensor([396901, 128],"float32"), ) 	 50803328 	 33846 	 10.000753402709961 	 10.084866046905518 	 0.30196285247802734 	 0.3041987419128418 	 4.533531188964844 	 4.534895658493042 	 0.13681364059448242 	 0.13683319091796875 	 
2025-08-04 17:25:38.525807 test begin: paddle.round(Tensor([99226, 256],"float64"), )
[Prof] paddle.round 	 paddle.round(Tensor([99226, 256],"float64"), ) 	 25401856 	 33846 	 10.282701253890991 	 10.093343496322632 	 0.31050944328308105 	 0.30473828315734863 	 4.528863430023193 	 4.548380613327026 	 0.13668012619018555 	 0.13732314109802246 	 
2025-08-04 17:26:09.107121 test begin: paddle.round(x=Tensor([3, 3, 5644801],"float32"), )
[Prof] paddle.round 	 paddle.round(x=Tensor([3, 3, 5644801],"float32"), ) 	 50803209 	 33846 	 10.003559350967407 	 10.07325291633606 	 0.3020939826965332 	 0.3041973114013672 	 4.534165859222412 	 4.538253545761108 	 0.13685154914855957 	 0.1368393898010254 	 
2025-08-04 17:26:42.053544 test begin: paddle.round(x=Tensor([3, 5644801, 3],"float32"), )
[Prof] paddle.round 	 paddle.round(x=Tensor([3, 5644801, 3],"float32"), ) 	 50803209 	 33846 	 10.003556966781616 	 10.073089838027954 	 0.3020906448364258 	 0.3042018413543701 	 4.535179853439331 	 4.534520626068115 	 0.13686156272888184 	 0.13683414459228516 	 
2025-08-04 17:27:12.916069 test begin: paddle.round(x=Tensor([5644801, 3, 3],"float32"), )
[Prof] paddle.round 	 paddle.round(x=Tensor([5644801, 3, 3],"float32"), ) 	 50803209 	 33846 	 10.003057718276978 	 10.072929859161377 	 0.3020322322845459 	 0.30420446395874023 	 4.533871412277222 	 4.538274526596069 	 0.13689470291137695 	 0.13695240020751953 	 
2025-08-04 17:27:43.902127 test begin: paddle.row_stack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], ) 	 76204872 	 32497 	 30.585554361343384 	 29.96992063522339 	 0.16034293174743652 	 0.9422614574432373 	 30.90065312385559 	 2.2370593547821045 	 0.16189265251159668 	 0.00010538101196289062 	 
2025-08-04 17:29:23.928300 test begin: paddle.row_stack(list[Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2, 1058401],"float64"),], ) 	 25401624 	 32497 	 10.229164838790894 	 10.181768655776978 	 0.16086173057556152 	 0.1600947380065918 	 10.257797002792358 	 1.7985484600067139 	 0.16129565238952637 	 7.414817810058594e-05 	 
2025-08-04 17:29:59.502932 test begin: paddle.row_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401880 	 32497 	 10.19192385673523 	 10.499716997146606 	 0.08026504516601562 	 0.33020949363708496 	 10.463300466537476 	 2.430117607116699 	 0.08238840103149414 	 0.00010204315185546875 	 
2025-08-04 17:30:34.244787 test begin: paddle.row_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401880 	 32497 	 10.249408483505249 	 10.406957626342773 	 0.08041238784790039 	 0.32744717597961426 	 10.561108827590942 	 2.3378822803497314 	 0.08284783363342285 	 8.20159912109375e-05 	 
2025-08-04 17:31:11.263646 test begin: paddle.row_stack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], ) 	 76204836 	 32497 	 30.574872255325317 	 30.611587285995483 	 0.16027379035949707 	 0.9347436428070068 	 30.487733840942383 	 2.2427399158477783 	 0.15979242324829102 	 8.440017700195312e-05 	 
2025-08-04 17:32:51.020375 test begin: paddle.row_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], ) 	 25401656 	 32497 	 10.43580412864685 	 10.456908941268921 	 0.08218884468078613 	 0.32900524139404297 	 10.265784740447998 	 2.3357677459716797 	 0.08083653450012207 	 0.0001232624053955078 	 
2025-08-04 17:33:27.036020 test begin: paddle.row_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401656 	 32497 	 10.221429347991943 	 10.349412441253662 	 0.0802147388458252 	 0.3254685401916504 	 10.231295347213745 	 2.2837324142456055 	 0.08027482032775879 	 0.0001685619354248047 	 
2025-08-04 17:34:01.253791 test begin: paddle.row_stack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 76204980 	 32497 	 30.490073919296265 	 29.849039316177368 	 0.1598198413848877 	 0.9388015270233154 	 30.428975582122803 	 2.3787975311279297 	 0.1594829559326172 	 8.0108642578125e-05 	 
2025-08-04 17:35:37.927601 test begin: paddle.row_stack(list[Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401660 	 32497 	 9.968011617660522 	 10.181649208068848 	 0.1567072868347168 	 0.16007494926452637 	 9.98975157737732 	 1.7807214260101318 	 0.15707707405090332 	 7.581710815429688e-05 	 
2025-08-04 17:36:10.976212 test begin: paddle.row_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 76204818 	 32497 	 30.694968461990356 	 29.897523880004883 	 0.16089797019958496 	 0.9402470588684082 	 30.958688020706177 	 2.2145755290985107 	 0.1622614860534668 	 9.202957153320312e-05 	 
2025-08-04 17:37:48.131489 test begin: paddle.row_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 76204890 	 32497 	 30.61921715736389 	 30.19147491455078 	 0.1605076789855957 	 0.9495489597320557 	 30.7864773273468 	 2.3069186210632324 	 0.16133856773376465 	 8.654594421386719e-05 	 
2025-08-04 17:39:25.380388 test begin: paddle.row_stack(list[Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401630 	 32497 	 10.228896379470825 	 10.181674242019653 	 0.16083574295043945 	 0.1600632667541504 	 10.257777452468872 	 1.811715841293335 	 0.16127490997314453 	 7.510185241699219e-05 	 
2025-08-04 17:39:58.997577 test begin: paddle.row_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401656 	 32497 	 10.390104532241821 	 10.041656255722046 	 0.08151388168334961 	 0.3149690628051758 	 10.385569334030151 	 2.2294278144836426 	 0.08147931098937988 	 8.511543273925781e-05 	 
2025-08-04 17:40:39.164201 test begin: paddle.row_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], ) 	 76204824 	 32497 	 30.602768182754517 	 29.761955499649048 	 0.16042804718017578 	 0.9354972839355469 	 30.91908550262451 	 2.2903647422790527 	 0.1620335578918457 	 8.654594421386719e-05 	 
2025-08-04 17:42:16.694368 test begin: paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401880 	 32497 	 10.164887428283691 	 10.137886762619019 	 0.07974386215209961 	 0.31784558296203613 	 10.383246421813965 	 2.268279552459717 	 0.08145642280578613 	 0.00010466575622558594 	 
2025-08-04 17:42:52.830968 test begin: paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 76204920 	 32497 	 30.475589752197266 	 29.998875856399536 	 0.15973973274230957 	 0.9425265789031982 	 30.653205156326294 	 2.29079008102417 	 0.16068577766418457 	 0.00019168853759765625 	 
2025-08-04 17:44:32.914250 test begin: paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401640 	 32497 	 9.968233108520508 	 10.185741424560547 	 0.15674138069152832 	 0.16007614135742188 	 9.99022889137268 	 1.867863655090332 	 0.15706348419189453 	 7.367134094238281e-05 	 
2025-08-04 17:45:08.093337 test begin: paddle.rsqrt(Tensor([10000, 1694, 3],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([10000, 1694, 3],"float32"), ) 	 50820000 	 33838 	 10.005879163742065 	 10.079981327056885 	 0.3021857738494873 	 0.3043193817138672 	 15.228173971176147 	 35.203861236572266 	 0.45995068550109863 	 0.35451221466064453 	 
2025-08-04 17:46:20.500567 test begin: paddle.rsqrt(Tensor([10000, 2, 1271],"float64"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([10000, 2, 1271],"float64"), ) 	 25420000 	 33838 	 10.104994773864746 	 10.117900848388672 	 0.30521154403686523 	 0.3055422306060791 	 15.173523187637329 	 35.18480181694031 	 0.4582536220550537 	 0.35427284240722656 	 
2025-08-04 17:47:32.226052 test begin: paddle.rsqrt(Tensor([10000, 2, 2541],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([10000, 2, 2541],"float32"), ) 	 50820000 	 33838 	 10.004934787750244 	 10.07497262954712 	 0.30220961570739746 	 0.30426740646362305 	 15.226473331451416 	 35.202409744262695 	 0.45992136001586914 	 0.3544654846191406 	 
2025-08-04 17:48:44.940721 test begin: paddle.rsqrt(Tensor([10000, 847, 3],"float64"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([10000, 847, 3],"float64"), ) 	 25410000 	 33838 	 10.104598999023438 	 10.113550662994385 	 0.3051471710205078 	 0.3054494857788086 	 15.166670322418213 	 35.17063283920288 	 0.45813918113708496 	 0.3541250228881836 	 
2025-08-04 17:49:58.869799 test begin: paddle.rsqrt(Tensor([13, 1007, 3881],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([13, 1007, 3881],"float32"), ) 	 50806171 	 33838 	 10.004439353942871 	 10.072288274765015 	 0.30218005180358887 	 0.30417442321777344 	 15.222735404968262 	 35.1945903301239 	 0.45974087715148926 	 0.35437488555908203 	 
2025-08-04 17:51:15.301298 test begin: paddle.rsqrt(Tensor([13, 3907939, 1],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([13, 3907939, 1],"float32"), ) 	 50803207 	 33838 	 10.003236770629883 	 10.093396186828613 	 0.30210375785827637 	 0.3042013645172119 	 15.221132278442383 	 35.19080305099487 	 0.4596569538116455 	 0.3543095588684082 	 
2025-08-04 17:52:28.726183 test begin: paddle.rsqrt(Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([4233601, 2, 3],"float64"), ) 	 25401606 	 33838 	 10.098963260650635 	 10.115508317947388 	 0.3050682544708252 	 0.3053255081176758 	 15.15891432762146 	 35.15884828567505 	 0.45798349380493164 	 0.3539724349975586 	 
2025-08-04 17:53:42.633958 test begin: paddle.rsqrt(Tensor([50451, 1007, 1],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([50451, 1007, 1],"float32"), ) 	 50804157 	 33838 	 10.004056930541992 	 10.072036743164062 	 0.30217838287353516 	 0.30418872833251953 	 15.221944332122803 	 35.19204354286194 	 0.4596834182739258 	 0.35437846183776855 	 
2025-08-04 17:54:54.906059 test begin: paddle.rsqrt(Tensor([8467201, 2, 3],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([8467201, 2, 3],"float32"), ) 	 50803206 	 33838 	 10.00340747833252 	 10.07236933708191 	 0.30219364166259766 	 0.3041987419128418 	 15.222740173339844 	 35.19069838523865 	 0.459730863571167 	 0.35430097579956055 	 
2025-08-04 17:56:07.219700 test begin: paddle.scale(Tensor([2, 256, 256, 388],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([2, 256, 256, 388],"float32"), scale=1.1111111111111112, ) 	 50855936 	 33791 	 10.01250147819519 	 20.131818056106567 	 0.3028573989868164 	 0.3044767379760742 	 10.007944107055664 	 10.066001176834106 	 0.3027462959289551 	 0.3044440746307373 	 combined
2025-08-04 17:56:59.153091 test begin: paddle.scale(Tensor([2, 256, 388, 256],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([2, 256, 388, 256],"float32"), scale=1.1111111111111112, ) 	 50855936 	 33791 	 10.012699127197266 	 20.131845712661743 	 0.3028707504272461 	 0.3045027256011963 	 10.008090734481812 	 10.065871477127075 	 0.30266261100769043 	 0.3044612407684326 	 combined
2025-08-04 17:57:51.087118 test begin: paddle.scale(Tensor([2, 388, 256, 256],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([2, 388, 256, 256],"float32"), scale=1.1111111111111112, ) 	 50855936 	 33791 	 10.012763977050781 	 20.131590843200684 	 0.3028392791748047 	 0.30446434020996094 	 10.007685661315918 	 10.065853595733643 	 0.3026599884033203 	 0.3044466972351074 	 combined
2025-08-04 17:58:44.248868 test begin: paddle.scale(Tensor([4, 194, 256, 256],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 194, 256, 256],"float32"), scale=1.1111111111111112, ) 	 50855936 	 33791 	 10.012874126434326 	 20.13179087638855 	 0.3028712272644043 	 0.3044126033782959 	 10.007852554321289 	 10.065807580947876 	 0.30266761779785156 	 0.30442237854003906 	 combined
2025-08-04 17:59:39.120076 test begin: paddle.scale(Tensor([4, 256, 194, 256],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 256, 194, 256],"float32"), scale=1.1111111111111112, ) 	 50855936 	 33791 	 10.012697696685791 	 20.131638288497925 	 0.3028578758239746 	 0.3044168949127197 	 10.008094072341919 	 10.065837144851685 	 0.3027019500732422 	 0.3044266700744629 	 combined
2025-08-04 18:00:31.090674 test begin: paddle.scale(Tensor([4, 256, 256, 194],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 256, 256, 194],"float32"), scale=1.1111111111111112, ) 	 50855936 	 33791 	 10.012682676315308 	 20.131962537765503 	 0.30280590057373047 	 0.3044610023498535 	 10.007763862609863 	 10.065798044204712 	 0.3026607036590576 	 0.3044466972351074 	 combined
2025-08-04 18:01:23.010282 test begin: paddle.scale(Tensor([4, 256, 256, 256],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 256, 256, 256],"float32"), scale=1.1111111111111112, ) 	 67108864 	 33791 	 13.170067548751831 	 26.455190181732178 	 0.39835119247436523 	 0.40004539489746094 	 13.171716451644897 	 13.226562738418579 	 0.398453950881958 	 0.400036096572876 	 combined
2025-08-04 18:02:31.314331 test begin: paddle.scale(Tensor([4, 256, 256, 388],"float16"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 256, 256, 388],"float16"), scale=1.1111111111111112, ) 	 101711872 	 33791 	 10.087785959243774 	 20.047155141830444 	 0.3051326274871826 	 0.30288004875183105 	 10.082038640975952 	 10.012459754943848 	 0.3049437999725342 	 0.30281853675842285 	 combined
2025-08-04 18:03:25.408732 test begin: paddle.scale(Tensor([4, 256, 388, 256],"float16"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 256, 388, 256],"float16"), scale=1.1111111111111112, ) 	 101711872 	 33791 	 10.087775230407715 	 20.025177717208862 	 0.3051130771636963 	 0.3028392791748047 	 10.081891059875488 	 10.012385845184326 	 0.30492663383483887 	 0.3027987480163574 	 combined
2025-08-04 18:04:19.417481 test begin: paddle.scale(Tensor([4, 388, 256, 256],"float16"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 388, 256, 256],"float16"), scale=1.1111111111111112, ) 	 101711872 	 33791 	 10.087623357772827 	 20.03179907798767 	 0.30510497093200684 	 0.30283260345458984 	 10.08202075958252 	 10.012524843215942 	 0.30492711067199707 	 0.30280351638793945 	 combined
2025-08-04 18:05:13.487806 test begin: paddle.scale(Tensor([7, 256, 256, 256],"float16"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([7, 256, 256, 256],"float16"), scale=1.1111111111111112, ) 	 117440512 	 33791 	 11.613043069839478 	 23.102643728256226 	 0.35120677947998047 	 0.3490133285522461 	 11.614267349243164 	 11.542979717254639 	 0.3512732982635498 	 0.3491494655609131 	 combined
2025-08-04 18:06:17.934007 test begin: paddle.scatter(Tensor([262144, 194],"float32"), Tensor([197],"int32"), Tensor([197, 194],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 194],"float32"), Tensor([197],"int32"), Tensor([197, 194],"float32"), overwrite=True, ) 	 50894351 	 14665 	 4.64722204208374 	 99.2618772983551 	 0.16200590133666992 	 0.0002548694610595703 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:08:08.212744 test begin: paddle.scatter(Tensor([262144, 194],"float32"), Tensor([205],"int32"), Tensor([205, 194],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 194],"float32"), Tensor([205],"int32"), Tensor([205, 194],"float32"), overwrite=True, ) 	 50895911 	 14665 	 4.641800165176392 	 125.98770332336426 	 0.16175556182861328 	 0.0002429485321044922 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:10:25.185805 test begin: paddle.scatter(Tensor([262144, 194],"float32"), Tensor([219],"int32"), Tensor([219, 194],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 194],"float32"), Tensor([219],"int32"), Tensor([219, 194],"float32"), overwrite=True, ) 	 50898641 	 14665 	 4.649230718612671 	 107.02038884162903 	 0.16204166412353516 	 0.00024127960205078125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:12:26.488544 test begin: paddle.scatter(Tensor([262144, 2314],"float32"), Tensor([219],"int32"), Tensor([219, 2314],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 2314],"float32"), Tensor([219],"int32"), Tensor([219, 2314],"float32"), overwrite=True, ) 	 607108201 	 14665 	 53.736759424209595 	 135.67745351791382 	 1.245558261871338 	 0.00023603439331054688 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:16:55.276600 test begin: paddle.scatter(Tensor([262144, 2476],"float32"), Tensor([205],"int32"), Tensor([205, 2476],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 2476],"float32"), Tensor([205],"int32"), Tensor([205, 2476],"float32"), overwrite=True, ) 	 649576329 	 14665 	 57.50232195854187 	 99.7439169883728 	 1.3330862522125244 	 0.00023484230041503906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:20:51.817099 test begin: paddle.scatter(Tensor([262144, 2569],"float32"), Tensor([197],"int32"), Tensor([197, 2569],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 2569],"float32"), Tensor([197],"int32"), Tensor([197, 2569],"float32"), overwrite=True, ) 	 673954226 	 14665 	 59.62564516067505 	 96.1656129360199 	 1.3823649883270264 	 0.00023984909057617188 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:24:51.711484 test begin: paddle.scatter(Tensor([262144, 64],"float32"), Tensor([197],"int32"), Tensor([7938, 64],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 64],"float32"), Tensor([197],"int32"), Tensor([7938, 64],"float32"), overwrite=True, ) 	 17285445 	 14665 	 1.645331621170044 	 113.61429333686829 	 0.05729222297668457 	 0.00025463104248046875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:26:49.268105 test begin: paddle.scatter(Tensor([262144, 64],"float32"), Tensor([205],"int32"), Tensor([7938, 64],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 64],"float32"), Tensor([205],"int32"), Tensor([7938, 64],"float32"), overwrite=True, ) 	 17285453 	 14665 	 1.6439123153686523 	 104.09442663192749 	 0.057282447814941406 	 0.00023865699768066406 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:28:38.468250 test begin: paddle.scatter(Tensor([262144, 64],"float32"), Tensor([219],"int32"), Tensor([7938, 64],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 64],"float32"), Tensor([219],"int32"), Tensor([7938, 64],"float32"), overwrite=True, ) 	 17285467 	 14665 	 1.6447105407714844 	 106.56413388252258 	 0.05733227729797363 	 0.0002434253692626953 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:30:28.952013 test begin: paddle.scatter(Tensor([793801, 64],"float32"), Tensor([197],"int32"), Tensor([197, 64],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([793801, 64],"float32"), Tensor([197],"int32"), Tensor([197, 64],"float32"), overwrite=True, ) 	 50816069 	 14665 	 4.7200400829315186 	 116.83861589431763 	 0.1094520092010498 	 0.00023818016052246094 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:32:39.933934 test begin: paddle.scatter(Tensor([793801, 64],"float32"), Tensor([205],"int32"), Tensor([205, 64],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([793801, 64],"float32"), Tensor([205],"int32"), Tensor([205, 64],"float32"), overwrite=True, ) 	 50816589 	 14665 	 4.71821403503418 	 99.71865844726562 	 0.10938596725463867 	 0.00023031234741210938 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:34:30.931466 test begin: paddle.scatter(Tensor([793801, 64],"float32"), Tensor([219],"int32"), Tensor([219, 64],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([793801, 64],"float32"), Tensor([219],"int32"), Tensor([219, 64],"float32"), overwrite=True, ) 	 50817499 	 14665 	 4.710460424423218 	 106.27376580238342 	 0.10921859741210938 	 0.0002422332763671875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:36:28.290633 test begin: paddle.scatter_nd(Tensor([1280, 2],"int64"), Tensor([1280, 9, 10],"float32"), list[3,5,9,10,], )
[Prof] paddle.scatter_nd 	 paddle.scatter_nd(Tensor([1280, 2],"int64"), Tensor([1280, 9, 10],"float32"), list[3,5,9,10,], ) 	 117760 	 1000 	 0.036618709564208984 	 152.36015796661377 	 1.5020370483398438e-05 	 0.00020313262939453125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:39:00.987393 test begin: paddle.scatter_nd_add(Tensor([1, 14176, 7168],"bfloat16"), Tensor([585, 2],"int64"), Tensor([585, 7168],"bfloat16"), )
[Prof] paddle.scatter_nd_add 	 paddle.scatter_nd_add(Tensor([1, 14176, 7168],"bfloat16"), Tensor([585, 2],"int64"), Tensor([585, 7168],"bfloat16"), ) 	 105808018 	 1437 	 0.5760600566864014 	 98.59699177742004 	 0.20483636856079102 	 0.0001995563507080078 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:40:44.609043 test begin: paddle.scatter_nd_add(Tensor([1, 14176, 7168],"bfloat16"), Tensor([595, 2],"int64"), Tensor([595, 7168],"bfloat16"), )
[Prof] paddle.scatter_nd_add 	 paddle.scatter_nd_add(Tensor([1, 14176, 7168],"bfloat16"), Tensor([595, 2],"int64"), Tensor([595, 7168],"bfloat16"), ) 	 105879718 	 1437 	 0.5768465995788574 	 99.44835114479065 	 0.20509767532348633 	 0.0002090930938720703 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:42:29.040850 test begin: paddle.scatter_nd_add(Tensor([1, 8192, 12404],"bfloat16"), Tensor([587, 2],"int64"), Tensor([587, 12404],"bfloat16"), )
[Prof] paddle.scatter_nd_add 	 paddle.scatter_nd_add(Tensor([1, 8192, 12404],"bfloat16"), Tensor([587, 2],"int64"), Tensor([587, 12404],"bfloat16"), ) 	 108895890 	 1437 	 0.7743308544158936 	 98.04609370231628 	 0.2753725051879883 	 0.0002052783966064453 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:44:12.409982 test begin: paddle.scatter_nd_add(Tensor([1, 8192, 17069],"bfloat16"), Tensor([595, 2],"int64"), Tensor([595, 17069],"bfloat16"), )
[Prof] paddle.scatter_nd_add 	 paddle.scatter_nd_add(Tensor([1, 8192, 17069],"bfloat16"), Tensor([595, 2],"int64"), Tensor([595, 17069],"bfloat16"), ) 	 149986493 	 1437 	 1.2992520332336426 	 99.36319231987 	 0.4619777202606201 	 0.000209808349609375 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:45:59.650654 test begin: paddle.scatter_nd_add(Tensor([2, 8192, 7168],"bfloat16"), Tensor([585, 2],"int64"), Tensor([585, 7168],"bfloat16"), )
[Prof] paddle.scatter_nd_add 	 paddle.scatter_nd_add(Tensor([2, 8192, 7168],"bfloat16"), Tensor([585, 2],"int64"), Tensor([585, 7168],"bfloat16"), ) 	 121634962 	 1437 	 0.6437280178070068 	 98.54414629936218 	 0.22890210151672363 	 0.00020623207092285156 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:47:44.195584 test begin: paddle.scatter_nd_add(Tensor([2, 8192, 7168],"bfloat16"), Tensor([587, 2],"int64"), Tensor([587, 7168],"bfloat16"), )
[Prof] paddle.scatter_nd_add 	 paddle.scatter_nd_add(Tensor([2, 8192, 7168],"bfloat16"), Tensor([587, 2],"int64"), Tensor([587, 7168],"bfloat16"), ) 	 121649302 	 1437 	 0.6439294815063477 	 97.52630949020386 	 0.22897958755493164 	 0.0002186298370361328 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:49:28.577480 test begin: paddle.scatter_nd_add(Tensor([2, 8192, 7168],"bfloat16"), Tensor([595, 2],"int64"), Tensor([595, 7168],"bfloat16"), )
[Prof] paddle.scatter_nd_add 	 paddle.scatter_nd_add(Tensor([2, 8192, 7168],"bfloat16"), Tensor([595, 2],"int64"), Tensor([595, 7168],"bfloat16"), ) 	 121706662 	 1437 	 0.6442527770996094 	 99.35695195198059 	 0.22908639907836914 	 0.00019979476928710938 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 18:51:13.720832 test begin: paddle.searchsorted(Tensor([1024],"float32"), Tensor([50803201],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa3dd62d390>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 19:01:19.126625 test begin: paddle.searchsorted(Tensor([1024],"float64"), Tensor([25401601],"float64"), )
W0804 19:01:19.801616 63065 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2c93962f50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 19:11:23.608483 test begin: paddle.searchsorted(Tensor([1024],"int32"), Tensor([50803201],"int32"), )
W0804 19:11:24.317991 64613 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f70dbd6ef50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 19:21:28.414318 test begin: paddle.searchsorted(Tensor([2540160101],"float64"), Tensor([512],"float64"), )
W0804 19:22:28.753927 64916 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([2540160101],"float64"), Tensor([512],"float64"), ) 	 2540160613 	 998667 	 9.744139432907104 	 11.21450138092041 	 0.009890079498291016 	 0.0002689361572265625 	 None 	 None 	 None 	 None 	 
2025-08-04 19:23:03.310283 test begin: paddle.searchsorted(Tensor([25401601],"float64"), Tensor([25401601],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe93750ac20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 19:33:08.778893 test begin: paddle.searchsorted(Tensor([25401601],"float64"), Tensor([51201],"float64"), )
W0804 19:33:09.437753 65159 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([25401601],"float64"), Tensor([51201],"float64"), ) 	 25452802 	 998667 	 14.18270206451416 	 17.392622709274292 	 0.0001227855682373047 	 0.0002689361572265625 	 None 	 None 	 None 	 None 	 
2025-08-04 19:33:41.842628 test begin: paddle.searchsorted(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f47f27169e0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 19:43:49.144198 test begin: paddle.searchsorted(Tensor([50803201],"float32"), Tensor([51201],"float32"), )
W0804 19:43:50.318617 65470 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([50803201],"float32"), Tensor([51201],"float32"), ) 	 50854402 	 998667 	 11.116278409957886 	 12.141089677810669 	 0.00014328956604003906 	 0.0003292560577392578 	 None 	 None 	 None 	 None 	 
2025-08-04 19:44:13.804750 test begin: paddle.searchsorted(Tensor([50803201],"int32"), Tensor([50803201],"int32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5a23d6e9e0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 19:54:20.893377 test begin: paddle.searchsorted(Tensor([50803201],"int32"), Tensor([51201],"int32"), )
W0804 19:54:21.659091 65891 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([50803201],"int32"), Tensor([51201],"int32"), ) 	 50854402 	 998667 	 13.843806266784668 	 11.474211931228638 	 0.00011706352233886719 	 0.0002655982971191406 	 None 	 None 	 None 	 None 	 
2025-08-04 19:54:54.443063 test begin: paddle.select_scatter(Tensor([12700801, 3, 4],"float32"), Tensor([12700801, 4],"float32"), 1, 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5dd36beaa0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 20:05:02.937361 test begin: paddle.select_scatter(Tensor([1693441, 3, 4, 5],"float64"), Tensor([1693441, 3, 5],"float64"), 2, 1, )
W0804 20:05:05.397711 66306 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f02bd5870d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 20:15:07.787384 test begin: paddle.select_scatter(Tensor([2, 211681, 4, 5, 6],"int32"), Tensor([2, 211681, 5, 6],"int32"), 2, 1, )
W0804 20:15:08.663403 66615 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 211681, 4, 5, 6],"int32"), Tensor([2, 211681, 5, 6],"int32"), 2, 1, ) 	 63504300 	 315923 	 56.28701066970825 	 155.4593801498413 	 0.18208742141723633 	 0.16747450828552246 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 20:23:41.057093 test begin: paddle.select_scatter(Tensor([2, 2540161, 4, 5],"float64"), Tensor([2, 2540161, 5],"float64"), 2, 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe8b7932860>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 20:33:48.155525 test begin: paddle.select_scatter(Tensor([2, 3, 25401601],"float32"), Tensor([2, 25401601],"float32"), 1, 1, )
W0804 20:33:51.675875 67056 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9eea57f0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 20:43:58.251995 test begin: paddle.select_scatter(Tensor([2, 3, 4, 1411201, 6],"int32"), Tensor([2, 3, 1411201, 6],"int32"), 2, 1, )
W0804 20:44:01.002815 67437 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2e69bcf040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 20:54:03.086364 test begin: paddle.select_scatter(Tensor([2, 3, 4, 352801, 6],"int32"), Tensor([2, 3, 352801, 6],"int32"), 2, 1, )
W0804 20:54:03.924912 67872 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 3, 4, 352801, 6],"int32"), Tensor([2, 3, 352801, 6],"int32"), 2, 1, ) 	 63504180 	 315923 	 43.689712047576904 	 123.94449806213379 	 0.14132189750671387 	 0.13345837593078613 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 21:01:26.329717 test begin: paddle.select_scatter(Tensor([2, 3, 4, 4233601],"float64"), Tensor([2, 3, 4233601],"float64"), 2, 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f23f4df6b60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:11:31.234890 test begin: paddle.select_scatter(Tensor([2, 3, 4, 5, 1693441],"int32"), Tensor([2, 3, 5, 1693441],"int32"), 2, 1, )
W0804 21:11:34.261011 68488 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7effc4582ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:21:46.907909 test begin: paddle.select_scatter(Tensor([2, 3, 4, 5, 423361],"int32"), Tensor([2, 3, 5, 423361],"int32"), 2, 1, )
W0804 21:21:47.790432 68711 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 3, 4, 5, 423361],"int32"), Tensor([2, 3, 5, 423361],"int32"), 2, 1, ) 	 63504150 	 315923 	 43.64969515800476 	 123.85044574737549 	 0.14119815826416016 	 0.13335585594177246 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 21:29:07.936890 test begin: paddle.select_scatter(Tensor([2, 3, 8467201],"float32"), Tensor([2, 8467201],"float32"), 1, 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5ae7016b60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:39:12.813241 test begin: paddle.select_scatter(Tensor([2, 635041, 4, 5],"float64"), Tensor([2, 635041, 5],"float64"), 2, 1, )
W0804 21:39:13.586294 69453 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa45da5b130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:49:17.766235 test begin: paddle.select_scatter(Tensor([2, 846721, 4, 5, 6],"int32"), Tensor([2, 846721, 5, 6],"int32"), 2, 1, )
W0804 21:49:20.554701 69838 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8a3cd96e60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:59:27.879516 test begin: paddle.select_scatter(Tensor([20, 3, 282241, 5, 6],"int32"), Tensor([20, 3, 5, 6],"int32"), 2, 1, )
W0804 21:59:33.842711 70356 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f88acd42d70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:09:37.314161 test begin: paddle.select_scatter(Tensor([20, 3, 4, 1058401],"float64"), Tensor([20, 3, 1058401],"float64"), 2, 1, )
W0804 22:09:43.220243 70693 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f506969b130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:19:48.124947 test begin: paddle.select_scatter(Tensor([20, 3, 846721, 5],"float64"), Tensor([20, 3, 5],"float64"), 2, 1, )
W0804 22:19:53.564078 71057 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe83361efb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:29:53.131475 test begin: paddle.select_scatter(Tensor([20, 635040, 4],"float32"), Tensor([20, 4],"float32"), 1, 1, )
W0804 22:29:54.298955 71512 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([20, 635040, 4],"float32"), Tensor([20, 4],"float32"), 1, 1, ) 	 50803280 	 315923 	 8.98461389541626 	 99.83806920051575 	 6.890296936035156e-05 	 0.10744833946228027 	 103.7466344833374 	 100.58090686798096 	 0.041704416275024414 	 0.08113884925842285 	 
2025-08-04 22:35:08.885065 test begin: paddle.select_scatter(Tensor([4233601, 3, 4],"float32"), Tensor([4233601, 4],"float32"), 1, 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fadadd46aa0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:45:18.003363 test begin: paddle.select_scatter(Tensor([423361, 3, 4, 5],"float64"), Tensor([423361, 3, 5],"float64"), 2, 1, )
W0804 22:45:18.771345 72067 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fba6a2b70d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 22:55:22.889134 test begin: paddle.sgn(Tensor([12, 1058401, 2],"float64"), )
W0804 22:55:23.537102 72537 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.sgn 	 paddle.sgn(Tensor([12, 1058401, 2],"float64"), ) 	 25401624 	 32402 	 10.015607357025146 	 9.671992778778076 	 0.3159036636352539 	 0.3047614097595215 	 9.643043518066406 	 1.852062463760376 	 0.3041222095489502 	 6.914138793945312e-05 	 
2025-08-04 22:55:56.262056 test begin: paddle.sgn(Tensor([12, 20, 105841],"float64"), )
[Prof] paddle.sgn 	 paddle.sgn(Tensor([12, 20, 105841],"float64"), ) 	 25401840 	 32402 	 10.018395900726318 	 9.659935712814331 	 0.31590962409973145 	 0.3045923709869385 	 9.65119743347168 	 2.2623517513275146 	 0.30440616607666016 	 7.867813110351562e-05 	 
2025-08-04 22:56:29.257112 test begin: paddle.sgn(Tensor([12, 20, 211681],"float32"), )
[Prof] paddle.sgn 	 paddle.sgn(Tensor([12, 20, 211681],"float32"), ) 	 50803440 	 32402 	 11.176293134689331 	 9.641441583633423 	 0.35251331329345703 	 0.3041038513183594 	 9.588447093963623 	 1.7909729480743408 	 0.3024590015411377 	 8.249282836914062e-05 	 
2025-08-04 22:57:04.839306 test begin: paddle.sgn(Tensor([12, 2116801, 2],"float32"), )
[Prof] paddle.sgn 	 paddle.sgn(Tensor([12, 2116801, 2],"float32"), ) 	 50803224 	 32402 	 11.17340350151062 	 9.647072315216064 	 0.35245180130004883 	 0.30402636528015137 	 9.588397979736328 	 1.9299125671386719 	 0.3023707866668701 	 8.7738037109375e-05 	 
2025-08-04 22:57:39.390859 test begin: paddle.sgn(Tensor([1270081, 20, 2],"float32"), )
[Prof] paddle.sgn 	 paddle.sgn(Tensor([1270081, 20, 2],"float32"), ) 	 50803240 	 32402 	 12.41417384147644 	 9.643314123153687 	 0.3524322509765625 	 0.30413293838500977 	 9.588130474090576 	 1.7291195392608643 	 0.3024258613586426 	 7.510185241699219e-05 	 
2025-08-04 22:58:15.554171 test begin: paddle.sgn(Tensor([635041, 20, 2],"float64"), )
[Prof] paddle.sgn 	 paddle.sgn(Tensor([635041, 20, 2],"float64"), ) 	 25401640 	 32402 	 9.994928359985352 	 10.112789869308472 	 0.3152306079864502 	 0.304645299911499 	 9.636774778366089 	 2.248167037963867 	 0.303896427154541 	 7.462501525878906e-05 	 
2025-08-04 22:58:50.575865 test begin: paddle.shape(Tensor([10, 1600, 376, 280],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([10, 1600, 376, 280],"float32"), ) 	 1684480000 	 2368860 	 10.707624912261963 	 71.41785168647766 	 8.535385131835938e-05 	 0.0002143383026123047 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:00:41.932957 test begin: paddle.shape(Tensor([130, 128, 256, 256],"float16"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([130, 128, 256, 256],"float16"), ) 	 1090519040 	 2368860 	 22.144144535064697 	 74.96750807762146 	 0.00012755393981933594 	 0.00021982192993164062 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:02:41.550973 test begin: paddle.shape(Tensor([40, 121, 376, 280],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 121, 376, 280],"float32"), ) 	 509555200 	 2368860 	 15.850330114364624 	 75.57221555709839 	 0.00010991096496582031 	 0.00021576881408691406 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:04:21.578943 test begin: paddle.shape(Tensor([40, 128, 256, 388],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 128, 256, 388],"float32"), ) 	 508559360 	 2368860 	 22.397685766220093 	 74.41155576705933 	 0.00013065338134765625 	 0.0002086162567138672 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:06:07.147898 test begin: paddle.shape(Tensor([40, 128, 256, 776],"float16"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 128, 256, 776],"float16"), ) 	 1017118720 	 2368860 	 22.301219940185547 	 93.80833721160889 	 0.00012993812561035156 	 0.00022077560424804688 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:08:22.327133 test begin: paddle.shape(Tensor([40, 128, 388, 256],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 128, 388, 256],"float32"), ) 	 508559360 	 2368860 	 22.31775712966919 	 92.94156312942505 	 0.00012564659118652344 	 0.000209808349609375 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:10:26.040472 test begin: paddle.shape(Tensor([40, 128, 776, 256],"float16"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 128, 776, 256],"float16"), ) 	 1017118720 	 2368860 	 22.298313856124878 	 83.07937669754028 	 0.00013518333435058594 	 0.00021195411682128906 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:12:36.851584 test begin: paddle.shape(Tensor([40, 1600, 29, 280],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 1600, 29, 280],"float32"), ) 	 519680000 	 2368860 	 22.328769207000732 	 83.14632797241211 	 0.00013947486877441406 	 0.0002067089080810547 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:14:32.672370 test begin: paddle.shape(Tensor([40, 1600, 376, 22],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 1600, 376, 22],"float32"), ) 	 529408000 	 2368860 	 11.838856935501099 	 74.00876069068909 	 0.0003955364227294922 	 0.0005373954772949219 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:16:08.186972 test begin: paddle.shape(Tensor([40, 194, 256, 256],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 194, 256, 256],"float32"), ) 	 508559360 	 2368860 	 10.747272729873657 	 71.26412439346313 	 0.0002448558807373047 	 0.0006477832794189453 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:17:38.772053 test begin: paddle.shape(Tensor([40, 388, 256, 256],"float16"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 388, 256, 256],"float16"), ) 	 1017118720 	 2368860 	 10.722992897033691 	 78.50314140319824 	 0.00015616416931152344 	 0.00023794174194335938 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:19:28.965081 test begin: paddle.shape(Tensor([70, 128, 256, 256],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([70, 128, 256, 256],"float32"), ) 	 587202560 	 2368860 	 22.426613330841064 	 80.09052896499634 	 0.00015854835510253906 	 0.0006120204925537109 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:21:25.169203 test begin: paddle.shard_index(input=Tensor([12700801, 2, 1],"int64"), index_num=20, nshards=4, shard_id=1, )
[Prof] paddle.shard_index 	 paddle.shard_index(input=Tensor([12700801, 2, 1],"int64"), index_num=20, nshards=4, shard_id=1, ) 	 25401602 	 32350 	 10.0128014087677 	 65.96856617927551 	 0.316328763961792 	 0.0006518363952636719 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:22:43.988620 test begin: paddle.shard_index(input=Tensor([12700801, 2, 1],"int64"), index_num=20, nshards=4, shard_id=1, ignore_value=16, )
[Prof] paddle.shard_index 	 paddle.shard_index(input=Tensor([12700801, 2, 1],"int64"), index_num=20, nshards=4, shard_id=1, ignore_value=16, ) 	 25401602 	 32350 	 10.01231598854065 	 65.92324447631836 	 0.3162956237792969 	 0.0006535053253173828 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:24:00.628259 test begin: paddle.shard_index(input=Tensor([25401601, 1],"int64"), index_num=13, nshards=3, shard_id=0, )
[Prof] paddle.shard_index 	 paddle.shard_index(input=Tensor([25401601, 1],"int64"), index_num=13, nshards=3, shard_id=0, ) 	 25401601 	 32350 	 10.028666257858276 	 72.24244022369385 	 0.3168151378631592 	 0.0007574558258056641 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:25:23.462651 test begin: paddle.shard_index(input=Tensor([4, 6350401, 1],"int64"), index_num=20, nshards=4, shard_id=1, )
[Prof] paddle.shard_index 	 paddle.shard_index(input=Tensor([4, 6350401, 1],"int64"), index_num=20, nshards=4, shard_id=1, ) 	 25401604 	 32350 	 10.01217007637024 	 65.88448190689087 	 0.31629085540771484 	 0.0006530284881591797 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:26:42.770306 test begin: paddle.shard_index(input=Tensor([4, 6350401, 1],"int64"), index_num=20, nshards=4, shard_id=1, ignore_value=16, )
[Prof] paddle.shard_index 	 paddle.shard_index(input=Tensor([4, 6350401, 1],"int64"), index_num=20, nshards=4, shard_id=1, ignore_value=16, ) 	 25401604 	 32350 	 10.012025356292725 	 65.91895246505737 	 0.31629252433776855 	 0.0006520748138427734 	 None 	 None 	 None 	 None 	 combined
2025-08-04 23:27:59.407455 test begin: paddle.sign(Tensor([12404, 32, 128],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([12404, 32, 128],"float32"), ) 	 50806784 	 32506 	 11.243617057800293 	 9.674375057220459 	 0.35352396965026855 	 0.30412960052490234 	 9.620335578918457 	 2.2496097087860107 	 0.30249500274658203 	 6.580352783203125e-05 	 
2025-08-04 23:28:33.924066 test begin: paddle.sign(Tensor([32, 12404, 128],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([32, 12404, 128],"float32"), ) 	 50806784 	 32506 	 11.244729280471802 	 9.674601078033447 	 0.3536112308502197 	 0.3041868209838867 	 9.620414733886719 	 2.4318015575408936 	 0.3024742603302002 	 0.00011610984802246094 	 
2025-08-04 23:29:08.745989 test begin: paddle.sign(Tensor([32, 32, 49613],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([32, 32, 49613],"float32"), ) 	 50803712 	 32506 	 11.212119579315186 	 9.673790216445923 	 0.35253238677978516 	 0.30416059494018555 	 9.612315893173218 	 2.4472098350524902 	 0.30226874351501465 	 0.0002181529998779297 	 
2025-08-04 23:29:43.443233 test begin: paddle.sign(Tensor([64, 1, 28, 28351],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([64, 1, 28, 28351],"float32"), ) 	 50804992 	 32506 	 11.21576714515686 	 9.674337387084961 	 0.3526017665863037 	 0.3042726516723633 	 9.614677906036377 	 2.2869880199432373 	 0.3022632598876953 	 0.0005314350128173828 	 
2025-08-04 23:30:18.066312 test begin: paddle.sign(Tensor([64, 1, 28351, 28],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([64, 1, 28351, 28],"float32"), ) 	 50804992 	 32506 	 11.216371774673462 	 10.106858253479004 	 0.35261011123657227 	 0.30419111251831055 	 9.614262104034424 	 1.8369102478027344 	 0.3022322654724121 	 7.343292236328125e-05 	 
2025-08-04 23:30:54.180306 test begin: paddle.sign(Tensor([64, 1013, 28, 28],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([64, 1013, 28, 28],"float32"), ) 	 50828288 	 32506 	 11.214294195175171 	 9.67872142791748 	 0.35260796546936035 	 0.3042476177215576 	 9.618592262268066 	 2.2808587551116943 	 0.30240964889526367 	 9.989738464355469e-05 	 
2025-08-04 23:31:30.185269 test begin: paddle.sign(Tensor([64801, 1, 28, 28],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([64801, 1, 28, 28],"float32"), ) 	 50803984 	 32506 	 11.197818756103516 	 9.673960208892822 	 0.35216593742370605 	 0.3041348457336426 	 9.614414691925049 	 2.3366713523864746 	 0.30228400230407715 	 7.295608520507812e-05 	 
2025-08-04 23:32:04.730432 test begin: paddle.sign(Tensor([66151, 1, 384],"int64"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([66151, 1, 384],"int64"), ) 	 25401984 	 32506 	 10.018870830535889 	 9.698681354522705 	 0.31503796577453613 	 0.30492734909057617 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 23:32:38.073136 test begin: paddle.sign(Tensor([7, 1, 3628801],"int64"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([7, 1, 3628801],"int64"), ) 	 25401607 	 32506 	 10.007437229156494 	 9.698475360870361 	 0.3146250247955322 	 0.3048992156982422 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 23:33:08.390340 test begin: paddle.sign(Tensor([7, 9451, 384],"int64"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([7, 9451, 384],"int64"), ) 	 25404288 	 32506 	 10.01560378074646 	 9.701076984405518 	 0.3150029182434082 	 0.3049640655517578 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 23:33:39.494007 test begin: paddle.signal.stft(Tensor([16, 3175201],"float32"), 1024, 120, 600, window=Tensor([600],"float32"), center=True, pad_mode="reflect", )
[Prof] paddle.signal.stft 	 paddle.signal.stft(Tensor([16, 3175201],"float32"), 1024, 120, 600, window=Tensor([600],"float32"), center=True, pad_mode="reflect", ) 	 50803816 	 1000 	 19.411707639694214 	 4.760666847229004 	 2.4840071201324463 	 0.9755070209503174 	 43.31218123435974 	 33.87786793708801 	 2.94588303565979 	 1.72816801071167 	 
2025-08-04 23:35:29.961921 test begin: paddle.signal.stft(Tensor([16, 3175201],"float32"), 2048, 240, 1200, window=Tensor([1200],"float32"), center=True, pad_mode="reflect", )
[Prof] paddle.signal.stft 	 paddle.signal.stft(Tensor([16, 3175201],"float32"), 2048, 240, 1200, window=Tensor([1200],"float32"), center=True, pad_mode="reflect", ) 	 50804416 	 1000 	 19.795164585113525 	 5.922512054443359 	 2.532989978790283 	 1.0207045078277588 	 43.158979177474976 	 32.351104974746704 	 2.935594320297241 	 1.6504032611846924 	 
2025-08-04 23:37:19.685332 test begin: paddle.signal.stft(Tensor([1993, 25500],"float32"), 1024, 120, 600, window=Tensor([600],"float32"), center=True, pad_mode="reflect", )
[Prof] paddle.signal.stft 	 paddle.signal.stft(Tensor([1993, 25500],"float32"), 1024, 120, 600, window=Tensor([600],"float32"), center=True, pad_mode="reflect", ) 	 50822100 	 1000 	 17.683055639266968 	 4.789425373077393 	 2.2624025344848633 	 0.9806680679321289 	 42.47003388404846 	 31.10696792602539 	 2.889007329940796 	 1.586852788925171 	 
2025-08-04 23:39:04.978196 test begin: paddle.signal.stft(Tensor([1993, 25500],"float32"), 2048, 240, 1200, window=Tensor([1200],"float32"), center=True, pad_mode="reflect", )
[Prof] paddle.signal.stft 	 paddle.signal.stft(Tensor([1993, 25500],"float32"), 2048, 240, 1200, window=Tensor([1200],"float32"), center=True, pad_mode="reflect", ) 	 50822700 	 1000 	 18.146315574645996 	 5.058881759643555 	 2.3217782974243164 	 1.036658525466919 	 42.795974254608154 	 31.487319469451904 	 2.910820960998535 	 1.6063506603240967 	 
2025-08-04 23:40:51.657503 test begin: paddle.signbit(Tensor([11, 17, 271],"int32"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([11, 17, 271],"int32"), ) 	 50677 	 4599 	 10.010467529296875 	 0.0460817813873291 	 4.124641418457031e-05 	 4.4345855712890625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 23:41:01.871716 test begin: paddle.signbit(Tensor([11, 17, 543],"int16"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([11, 17, 543],"int16"), ) 	 101541 	 4599 	 18.731166124343872 	 0.046114444732666016 	 4.100799560546875e-05 	 4.410743713378906e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 23:41:20.781577 test begin: paddle.signbit(Tensor([11, 461, 10],"int32"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([11, 461, 10],"int32"), ) 	 50710 	 4599 	 10.054762601852417 	 0.0467984676361084 	 4.5299530029296875e-05 	 5.841255187988281e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 23:41:31.013222 test begin: paddle.signbit(Tensor([11, 923, 10],"int16"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([11, 923, 10],"int16"), ) 	 101530 	 4599 	 18.898931741714478 	 0.04570341110229492 	 4.553794860839844e-05 	 3.361701965332031e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 23:41:50.097302 test begin: paddle.signbit(Tensor([12, 20, 211],"float32"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([12, 20, 211],"float32"), ) 	 50640 	 4599 	 9.904066801071167 	 0.04581785202026367 	 3.0994415283203125e-05 	 4.291534423828125e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 23:42:00.514414 test begin: paddle.signbit(Tensor([12, 2116, 2],"float32"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([12, 2116, 2],"float32"), ) 	 50784 	 4599 	 9.936776876449585 	 0.045949459075927734 	 4.124641418457031e-05 	 3.337860107421875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 23:42:10.626244 test begin: paddle.signbit(Tensor([1270, 20, 2],"float32"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([1270, 20, 2],"float32"), ) 	 50800 	 4599 	 9.966099500656128 	 0.045655250549316406 	 3.9577484130859375e-05 	 3.552436828613281e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 23:42:20.766847 test begin: paddle.signbit(Tensor([298, 17, 10],"int32"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([298, 17, 10],"int32"), ) 	 50660 	 4599 	 9.954236030578613 	 0.0488436222076416 	 3.838539123535156e-05 	 6.103515625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 23:42:30.898924 test begin: paddle.signbit(Tensor([597, 17, 10],"int16"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([597, 17, 10],"int16"), ) 	 101490 	 4599 	 18.73076319694519 	 0.046324729919433594 	 4.00543212890625e-05 	 3.5762786865234375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 23:42:49.814159 test begin: paddle.sin(Tensor([128512, 396],"float32"), )
[Prof] paddle.sin 	 paddle.sin(Tensor([128512, 396],"float32"), ) 	 50890752 	 33844 	 10.01125717163086 	 10.101267099380493 	 0.3023107051849365 	 0.30506181716918945 	 15.265235900878906 	 25.20383882522583 	 0.4609565734863281 	 0.38051557540893555 	 
2025-08-04 23:43:52.182018 test begin: paddle.sin(Tensor([254017, 200],"float32"), )
[Prof] paddle.sin 	 paddle.sin(Tensor([254017, 200],"float32"), ) 	 50803400 	 33844 	 10.002761602401733 	 10.090474128723145 	 0.30205297470092773 	 0.30452585220336914 	 15.246469020843506 	 25.15295171737671 	 0.4603996276855469 	 0.3797271251678467 	 
2025-08-04 23:44:54.555707 test begin: paddle.sin(Tensor([50000, 1017],"float32"), )
[Prof] paddle.sin 	 paddle.sin(Tensor([50000, 1017],"float32"), ) 	 50850000 	 33844 	 10.006498575210571 	 10.094231367111206 	 0.3021872043609619 	 0.3047831058502197 	 15.257516622543335 	 25.176743745803833 	 0.4607541561126709 	 0.3801138401031494 	 
2025-08-04 23:45:57.224323 test begin: paddle.sin(Tensor([508033, 100],"float32"), )
[Prof] paddle.sin 	 paddle.sin(Tensor([508033, 100],"float32"), ) 	 50803300 	 33844 	 9.999132871627808 	 10.087232828140259 	 0.30194807052612305 	 0.30452775955200195 	 15.233450174331665 	 25.152852058410645 	 0.46006083488464355 	 0.378201961517334 	 
2025-08-04 23:46:59.426933 test begin: paddle.sin(Tensor([68608, 741],"float32"), )
[Prof] paddle.sin 	 paddle.sin(Tensor([68608, 741],"float32"), ) 	 50838528 	 33844 	 10.00813341140747 	 10.092136144638062 	 0.30221104621887207 	 0.3046760559082031 	 15.254234075546265 	 25.172118663787842 	 0.46057987213134766 	 0.38005900382995605 	 
2025-08-04 23:48:03.800852 test begin: paddle.sinc(Tensor([16, 1587601],"float64"), )
[Prof] paddle.sinc 	 paddle.sinc(Tensor([16, 1587601],"float64"), ) 	 25401616 	 3389 	 9.977790355682373 	 1.0213336944580078 	 0.2508721351623535 	 0.30790066719055176 	 8.779877424240112 	 12.758841514587402 	 0.44100046157836914 	 0.3204076290130615 	 
2025-08-04 23:48:39.008958 test begin: paddle.sinc(Tensor([396901, 64],"float64"), )
[Prof] paddle.sinc 	 paddle.sinc(Tensor([396901, 64],"float64"), ) 	 25401664 	 3389 	 9.984177589416504 	 1.0213072299957275 	 0.2509622573852539 	 0.30793166160583496 	 8.783280611038208 	 12.757039070129395 	 0.44124412536621094 	 0.32044053077697754 	 
2025-08-04 23:49:13.203630 test begin: paddle.sinh(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.sinh 	 paddle.sinh(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33851 	 9.994259357452393 	 12.07118821144104 	 0.30170249938964844 	 0.3045797348022461 	 15.230848550796509 	 25.1663236618042 	 0.45979928970336914 	 0.37990832328796387 	 
2025-08-04 23:50:18.616519 test begin: paddle.sinh(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.sinh 	 paddle.sinh(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33851 	 10.000567197799683 	 10.098640441894531 	 0.30190086364746094 	 0.30455613136291504 	 15.241568326950073 	 25.165335416793823 	 0.46007442474365234 	 0.3798809051513672 	 
2025-08-04 23:51:21.092643 test begin: paddle.sinh(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.sinh 	 paddle.sinh(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33851 	 9.997387409210205 	 10.098492622375488 	 0.3018639087677002 	 0.3045220375061035 	 15.23980164527893 	 25.165538787841797 	 0.46004438400268555 	 0.37990260124206543 	 
2025-08-04 23:52:23.328103 test begin: paddle.slice(Tensor([653440, 1555],"bfloat16"), axes=list[0,], starts=list[0,], ends=list[8168,], )
W0804 23:52:51.173563 74614 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdac307ab30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:02:28.199730 test begin: paddle.slice(Tensor([653440, 1555],"bfloat16"), axes=list[0,], starts=list[16336,], ends=list[24504,], )
W0805 00:02:43.495757 75231 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 00:02:55.850658 75231 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc40d83b010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:12:32.889759 test begin: paddle.slice(Tensor([653440, 1555],"bfloat16"), axes=list[0,], starts=list[24504,], ends=list[32672,], )
W0805 00:12:48.523195 75763 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 00:12:58.802086 75763 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5adeb26e30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:22:43.446007 test begin: paddle.slice(Tensor([793810, 1280],"bfloat16"), axes=list[0,], starts=list[0,], ends=list[8168,], )
W0805 00:22:58.695935 76469 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 00:23:09.501734 76469 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa215b9b010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:32:48.203352 test begin: paddle.slice(Tensor([793810, 1280],"bfloat16"), axes=list[0,], starts=list[16336,], ends=list[24504,], )
W0805 00:33:03.766288 77087 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 00:33:15.153893 77087 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f89fe1bb010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:42:53.418646 test begin: paddle.slice(Tensor([793810, 1280],"bfloat16"), axes=list[0,], starts=list[24504,], ends=list[32672,], )
W0805 00:43:09.072436 77633 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 00:43:19.709165 77633 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbbc1bcf070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:52:58.331973 test begin: paddle.slice_scatter(Tensor([8, 1058401, 3, 9],"float32"), Tensor([8, 1058401, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
W0805 00:53:02.644814 77957 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f54f6cb7100>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:03:03.079990 test begin: paddle.slice_scatter(Tensor([8, 117601, 3, 9],"float64"), Tensor([8, 117601, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
W0805 01:03:03.833523 78444 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 117601, 3, 9],"float64"), Tensor([8, 117601, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 31046664 	 54545 	 12.751398801803589 	 30.02279281616211 	 0.23904657363891602 	 0.1873793601989746 	 45.15608859062195 	 42.22702741622925 	 0.1409587860107422 	 0.1581273078918457 	 combined
2025-08-05 01:05:15.478563 test begin: paddle.slice_scatter(Tensor([8, 235201, 3, 9],"float32"), Tensor([8, 235201, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 235201, 3, 9],"float32"), Tensor([8, 235201, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 62093064 	 54545 	 17.906785011291504 	 34.9759418964386 	 0.3355271816253662 	 0.2183237075805664 	 50.61828923225403 	 47.21000385284424 	 0.15801572799682617 	 0.17684268951416016 	 combined
2025-08-05 01:07:48.749261 test begin: paddle.slice_scatter(Tensor([8, 529201, 3, 9],"float64"), Tensor([8, 529201, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 529201, 3, 9],"float64"), Tensor([8, 529201, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 139709064 	 54545 	 57.70104098320007 	 133.87982416152954 	 1.0810871124267578 	 0.8355517387390137 	 198.175528049469 	 184.92525172233582 	 0.6185581684112549 	 0.6928224563598633 	 combined
2025-08-05 01:17:29.104115 test begin: paddle.slice_scatter(Tensor([8, 6, 117601, 9],"float32"), Tensor([8, 6, 117601, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 117601, 9],"float32"), Tensor([8, 6, 117601, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 62093328 	 54545 	 17.903213262557983 	 34.99293231964111 	 0.3354196548461914 	 0.2183213233947754 	 50.799718379974365 	 47.193392515182495 	 0.15856623649597168 	 0.17679619789123535 	 combined
2025-08-05 01:20:02.642178 test begin: paddle.slice_scatter(Tensor([8, 6, 211681, 5],"float32"), Tensor([8, 2, 211681, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 211681, 5],"float32"), Tensor([8, 2, 211681, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 67737920 	 54545 	 9.995332479476929 	 22.813924551010132 	 0.18727564811706543 	 0.14223814010620117 	 42.162975549697876 	 31.215842723846436 	 0.1316390037536621 	 0.11684775352478027 	 combined
2025-08-05 01:21:52.576759 test begin: paddle.slice_scatter(Tensor([8, 6, 264601, 9],"float64"), Tensor([8, 6, 264601, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 264601, 9],"float64"), Tensor([8, 6, 264601, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 139709328 	 54545 	 57.71721124649048 	 133.8815939426422 	 1.081517219543457 	 0.8356566429138184 	 197.823655128479 	 184.92778992652893 	 0.6173834800720215 	 0.6927447319030762 	 combined
2025-08-05 01:31:32.505685 test begin: paddle.slice_scatter(Tensor([8, 6, 3, 1058401],"float32"), Tensor([8, 2, 3, 1058401],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 3, 1058401],"float32"), Tensor([8, 2, 3, 1058401],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 203212992 	 54545 	 29.492034435272217 	 67.04864883422852 	 0.5525646209716797 	 0.41828131675720215 	 124.24533414840698 	 91.2462260723114 	 0.3878941535949707 	 0.34168052673339844 	 combined
2025-08-05 01:36:52.040797 test begin: paddle.slice_scatter(Tensor([8, 6, 3, 352801],"float32"), Tensor([8, 2, 3, 352801],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 3, 352801],"float32"), Tensor([8, 2, 3, 352801],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 67737792 	 54545 	 9.991623401641846 	 22.802582263946533 	 0.18723607063293457 	 0.14223122596740723 	 42.257112979888916 	 31.20902919769287 	 0.13190174102783203 	 0.11685013771057129 	 combined
2025-08-05 01:38:41.761530 test begin: paddle.slice_scatter(Tensor([8, 6, 529201, 9],"float32"), Tensor([8, 6, 529201, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f47cab06aa0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:48:46.755158 test begin: paddle.slice_scatter(Tensor([8, 6, 58801, 9],"float64"), Tensor([8, 6, 58801, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
W0805 01:48:47.511468 80193 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 58801, 9],"float64"), Tensor([8, 6, 58801, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 31046928 	 54545 	 12.771184206008911 	 30.03474187850952 	 0.23913216590881348 	 0.18742585182189941 	 45.16145300865173 	 42.27230381965637 	 0.14103102684020996 	 0.15835332870483398 	 combined
2025-08-05 01:50:59.029162 test begin: paddle.slice_scatter(Tensor([8, 6, 635041, 5],"float32"), Tensor([8, 2, 635041, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 635041, 5],"float32"), Tensor([8, 2, 635041, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 203213120 	 54545 	 29.502554178237915 	 67.42108988761902 	 0.5527591705322266 	 0.41814088821411133 	 124.32564425468445 	 91.24098873138428 	 0.3882637023925781 	 0.3416478633880615 	 combined
2025-08-05 01:56:19.233800 test begin: paddle.slice_scatter(Tensor([80, 423361, 3, 5],"float32"), Tensor([80, 2, 3, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([80, 423361, 3, 5],"float32"), Tensor([80, 2, 3, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 508035600 	 54545 	 0.8292937278747559 	 167.34303426742554 	 3.0040740966796875e-05 	 1.0430614948272705 	 167.67435216903687 	 167.67544412612915 	 0.5225858688354492 	 0.6270432472229004 	 combined
2025-08-05 02:04:59.534848 test begin: paddle.slice_scatter(Tensor([80, 6, 3, 176401],"float64"), Tensor([80, 6, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([80, 6, 3, 176401],"float64"), Tensor([80, 6, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 254020320 	 54545 	 0.8504054546356201 	 167.36919474601746 	 2.7894973754882812e-05 	 1.0432682037353516 	 167.75329279899597 	 167.70083951950073 	 0.5228655338287354 	 0.6271741390228271 	 combined
2025-08-05 02:13:34.305147 test begin: paddle.slice_scatter(Tensor([80, 6, 3, 352801],"float32"), Tensor([80, 6, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([80, 6, 3, 352801],"float32"), Tensor([80, 6, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 508036320 	 54545 	 0.8747310638427734 	 167.3618712425232 	 4.410743713378906e-05 	 1.0431699752807617 	 167.74670219421387 	 167.70634722709656 	 0.5228285789489746 	 0.6271791458129883 	 combined
2025-08-05 02:22:18.182265 test begin: paddle.sqrt(Tensor([128, 396901],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([128, 396901],"float32"), ) 	 50803328 	 33937 	 9.995143175125122 	 10.142912149429321 	 0.30098557472229004 	 0.30522799491882324 	 15.287468910217285 	 25.34908628463745 	 0.460407018661499 	 0.3817121982574463 	 
2025-08-05 02:23:20.753182 test begin: paddle.sqrt(Tensor([18, 15, 3, 256, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([18, 15, 3, 256, 256],"float32"), ) 	 53084160 	 33937 	 10.437453269958496 	 10.582662582397461 	 0.31435394287109375 	 0.31868934631347656 	 15.964051723480225 	 26.46687078475952 	 0.48068785667419434 	 0.39825439453125 	 
2025-08-05 02:24:25.999740 test begin: paddle.sqrt(Tensor([259, 3, 256, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([259, 3, 256, 256],"float32"), ) 	 50921472 	 33937 	 10.015939474105835 	 10.158726692199707 	 0.30162692070007324 	 0.30589938163757324 	 15.317685842514038 	 25.40782070159912 	 0.4612619876861572 	 0.3825554847717285 	 
2025-08-05 02:25:29.201576 test begin: paddle.sqrt(Tensor([4, 15, 13, 256, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([4, 15, 13, 256, 256],"float32"), ) 	 51118080 	 33937 	 10.060107469558716 	 10.209206581115723 	 0.30298638343811035 	 0.30693888664245605 	 15.37535810470581 	 25.505513429641724 	 0.4630463123321533 	 0.3840656280517578 	 
2025-08-05 02:26:32.825868 test begin: paddle.sqrt(Tensor([4, 15, 3, 1103, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([4, 15, 3, 1103, 256],"float32"), ) 	 50826240 	 33937 	 9.995643138885498 	 10.140707969665527 	 0.3010103702545166 	 0.30541300773620605 	 15.292654037475586 	 25.360966444015503 	 0.4606192111968994 	 0.3818082809448242 	 
2025-08-05 02:27:38.473327 test begin: paddle.sqrt(Tensor([4, 15, 3, 256, 1103],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([4, 15, 3, 256, 1103],"float32"), ) 	 50826240 	 33937 	 9.995764970779419 	 10.140599250793457 	 0.301011323928833 	 0.3054051399230957 	 15.29251217842102 	 25.361918210983276 	 0.46053481101989746 	 0.38187718391418457 	 
2025-08-05 02:28:42.416766 test begin: paddle.sqrt(Tensor([4, 65, 3, 256, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([4, 65, 3, 256, 256],"float32"), ) 	 51118080 	 33937 	 10.06019139289856 	 10.193295001983643 	 0.3029940128326416 	 0.30692005157470703 	 15.37651801109314 	 25.506062269210815 	 0.46308350563049316 	 0.38399243354797363 	 
2025-08-05 02:29:46.174299 test begin: paddle.sqrt(Tensor([544, 93431],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([544, 93431],"float32"), ) 	 50826464 	 33937 	 9.999884843826294 	 10.14050579071045 	 0.301145076751709 	 0.3053624629974365 	 15.295533180236816 	 25.36196208000183 	 0.46062254905700684 	 0.38183140754699707 	 
2025-08-05 02:30:48.701153 test begin: paddle.sqrt(Tensor([64, 13, 256, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([64, 13, 256, 256],"float32"), ) 	 54525952 	 33937 	 10.723530769348145 	 10.864665508270264 	 0.32291340827941895 	 0.32720446586608887 	 16.389132261276245 	 27.18122386932373 	 0.49349164962768555 	 0.4092671871185303 	 
2025-08-05 02:31:55.716979 test begin: paddle.sqrt(Tensor([64, 3, 1034, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([64, 3, 1034, 256],"float32"), ) 	 50823168 	 33937 	 10.000191926956177 	 10.140121698379517 	 0.30115795135498047 	 0.30535030364990234 	 15.289689779281616 	 25.359809398651123 	 0.46044063568115234 	 0.3818049430847168 	 
2025-08-05 02:32:59.413332 test begin: paddle.sqrt(Tensor([64, 3, 256, 1034],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([64, 3, 256, 1034],"float32"), ) 	 50823168 	 33937 	 10.000181913375854 	 10.141422033309937 	 0.30115747451782227 	 0.30541253089904785 	 15.289658784866333 	 25.3604679107666 	 0.4604151248931885 	 0.38190746307373047 	 
2025-08-05 02:34:02.940671 test begin: paddle.square(Tensor([104, 488493],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([104, 488493],"float32"), ) 	 50803272 	 33810 	 9.996559381484985 	 10.061520099639893 	 0.30219221115112305 	 0.30416297912597656 	 15.208701372146606 	 35.684998512268066 	 0.45964550971984863 	 0.26984429359436035 	 
2025-08-05 02:35:16.587427 test begin: paddle.square(Tensor([128, 396901],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([128, 396901],"float32"), ) 	 50803328 	 33810 	 9.996479749679565 	 10.06716251373291 	 0.30209994316101074 	 0.30412936210632324 	 15.20816946029663 	 35.68492889404297 	 0.45970702171325684 	 0.269878625869751 	 
2025-08-05 02:36:29.297777 test begin: paddle.square(Tensor([24904, 12, 170, 1],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([24904, 12, 170, 1],"float32"), ) 	 50804160 	 33810 	 9.992295503616333 	 10.061801433563232 	 0.3020167350769043 	 0.3041188716888428 	 15.20556354522705 	 35.68478226661682 	 0.4595506191253662 	 0.26987195014953613 	 
2025-08-05 02:37:42.793825 test begin: paddle.square(Tensor([3548, 12, 1194, 1],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([3548, 12, 1194, 1],"float32"), ) 	 50835744 	 33810 	 10.008234977722168 	 10.067761898040771 	 0.30252814292907715 	 0.30431532859802246 	 15.217729330062866 	 35.70707845687866 	 0.4599907398223877 	 0.2700233459472656 	 
2025-08-05 02:38:55.519242 test begin: paddle.square(Tensor([3548, 12, 170, 8],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([3548, 12, 170, 8],"float32"), ) 	 57903360 	 33810 	 11.375133752822876 	 11.442664384841919 	 0.3437819480895996 	 0.3458681106567383 	 17.310490608215332 	 40.592047691345215 	 0.5231916904449463 	 0.3069753646850586 	 
2025-08-05 02:40:18.980386 test begin: paddle.square(Tensor([3548, 85, 170, 1],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([3548, 85, 170, 1],"float32"), ) 	 51268600 	 33810 	 10.085440397262573 	 10.149644374847412 	 0.30482983589172363 	 0.30681872367858887 	 15.343894004821777 	 35.993804931640625 	 0.46382832527160645 	 0.2721986770629883 	 
2025-08-05 02:41:34.348981 test begin: paddle.square(Tensor([544, 93431],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([544, 93431],"float32"), ) 	 50826464 	 33810 	 10.003320693969727 	 10.070255994796753 	 0.3023695945739746 	 0.3042716979980469 	 15.214764595031738 	 35.698203563690186 	 0.4599771499633789 	 0.26996493339538574 	 
2025-08-05 02:42:47.851038 test begin: paddle.squeeze(Tensor([100, 512, 1, 100, 100],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([100, 512, 1, 100, 100],"float32"), axis=list[2,], ) 	 512000000 	 2700427 	 12.964352130889893 	 13.28630781173706 	 0.00012874603271484375 	 0.0002639293670654297 	 110.4383192062378 	 142.91696548461914 	 0.00010561943054199219 	 0.0002143383026123047 	 
2025-08-05 02:47:44.995464 test begin: paddle.squeeze(Tensor([1053440, 483],"float32"), )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([1053440, 483],"float32"), ) 	 508811520 	 2700427 	 10.438230514526367 	 10.154928922653198 	 0.00012636184692382812 	 0.00026607513427734375 	 116.86908626556396 	 130.74595260620117 	 0.000102996826171875 	 0.00022482872009277344 	 
2025-08-05 02:52:30.193263 test begin: paddle.squeeze(Tensor([3969010, 128],"float32"), )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([3969010, 128],"float32"), ) 	 508033280 	 2700427 	 10.342897415161133 	 10.162456750869751 	 0.00015115737915039062 	 8.678436279296875e-05 	 108.17791199684143 	 128.45173001289368 	 0.00012922286987304688 	 0.00021958351135253906 	 
2025-08-05 02:57:05.829169 test begin: paddle.squeeze(Tensor([4211200, 25, 5],"float32"), axis=-1, )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([4211200, 25, 5],"float32"), axis=-1, ) 	 526400000 	 2700427 	 12.732344150543213 	 10.374387979507446 	 0.00012230873107910156 	 0.00022864341735839844 	 110.48785448074341 	 130.7956268787384 	 0.00010395050048828125 	 0.0002338886260986328 	 
2025-08-05 03:01:49.226619 test begin: paddle.squeeze(Tensor([4211200, 31, 4],"float32"), axis=-1, )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([4211200, 31, 4],"float32"), axis=-1, ) 	 522188800 	 2700427 	 12.539736270904541 	 10.412482023239136 	 0.0001227855682373047 	 0.0002522468566894531 	 109.49892830848694 	 130.68282079696655 	 0.0001232624053955078 	 0.0002162456512451172 	 
2025-08-05 03:06:29.925033 test begin: paddle.squeeze(Tensor([5080330, 25, 4],"float32"), axis=-1, )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([5080330, 25, 4],"float32"), axis=-1, ) 	 508033000 	 2700427 	 12.639255285263062 	 10.42873477935791 	 8.7738037109375e-05 	 0.0001227855682373047 	 110.44397497177124 	 130.78277158737183 	 0.0001227855682373047 	 0.00020885467529296875 	 
2025-08-05 03:11:13.058785 test begin: paddle.squeeze(Tensor([80, 512, 1, 100, 125],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([80, 512, 1, 100, 125],"float32"), axis=list[2,], ) 	 512000000 	 2700427 	 12.753536224365234 	 13.22481632232666 	 9.083747863769531e-05 	 0.00013065338134765625 	 110.43058919906616 	 141.7506821155548 	 0.00010347366333007812 	 0.00021409988403320312 	 
2025-08-05 03:16:10.619127 test begin: paddle.squeeze(Tensor([80, 512, 1, 125, 100],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([80, 512, 1, 125, 100],"float32"), axis=list[2,], ) 	 512000000 	 2700427 	 25.26462721824646 	 13.12686538696289 	 0.0001404285430908203 	 0.0002644062042236328 	 116.83652567863464 	 142.61920833587646 	 0.00010657310485839844 	 0.0002040863037109375 	 
2025-08-05 03:21:25.560279 test begin: paddle.squeeze(Tensor([80, 512, 2, 100, 100],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([80, 512, 2, 100, 100],"float32"), axis=list[2,], ) 	 819200000 	 2700427 	 25.48627233505249 	 13.436807632446289 	 0.0003573894500732422 	 0.0007686614990234375 	 111.33735489845276 	 159.22295475006104 	 0.00015163421630859375 	 0.0002391338348388672 	 
2025-08-05 03:27:03.017597 test begin: paddle.squeeze(Tensor([80, 636, 1, 100, 100],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([80, 636, 1, 100, 100],"float32"), axis=list[2,], ) 	 508800000 	 2700427 	 12.794940948486328 	 13.26574182510376 	 0.00026679039001464844 	 0.0002028942108154297 	 109.8531243801117 	 180.52965211868286 	 0.00010395050048828125 	 0.0002658367156982422 	 
2025-08-05 03:32:38.508430 test begin: paddle.stack(list[Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),], axis=-2, ) 	 259269120 	 5521 	 10.022468328475952 	 39.309654235839844 	 1.8557851314544678 	 7.276608943939209 	 11.195032596588135 	 0.6834595203399658 	 2.0722553730010986 	 0.00017189979553222656 	 
2025-08-05 03:33:50.145281 test begin: paddle.stack(list[Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),], axis=-2, ) 	 270103680 	 5521 	 10.520779132843018 	 40.969868183135986 	 1.9475228786468506 	 7.585240125656128 	 11.741667747497559 	 0.6161439418792725 	 2.1734189987182617 	 6.580352783203125e-05 	 
2025-08-05 03:35:03.153069 test begin: paddle.stack(list[Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),], axis=0, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),], axis=0, ) 	 609681408 	 5521 	 16.302042961120605 	 14.301231861114502 	 3.016984701156616 	 2.6471731662750244 	 25.221890449523926 	 14.914528846740723 	 4.6688807010650635 	 1.3803963661193848 	 
2025-08-05 03:36:38.377127 test begin: paddle.stack(list[Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),], axis=0, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),], axis=0, ) 	 609681408 	 5521 	 16.29882550239563 	 14.303116083145142 	 3.016775369644165 	 2.6473519802093506 	 25.2214674949646 	 14.913560152053833 	 4.668790340423584 	 1.380321741104126 	 
2025-08-05 03:38:11.485804 test begin: paddle.stack(list[Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),], axis=-2, ) 	 282839040 	 5521 	 11.043466091156006 	 44.999860763549805 	 2.0443215370178223 	 8.329814195632935 	 12.230222940444946 	 0.6166362762451172 	 2.263947010040283 	 9.560585021972656e-05 	 
2025-08-05 03:39:30.191221 test begin: paddle.stack(list[Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),], axis=-2, ) 	 294658560 	 5521 	 11.82299017906189 	 46.870057582855225 	 2.132490396499634 	 8.676252126693726 	 12.747511625289917 	 0.6074388027191162 	 2.3596277236938477 	 6.270408630371094e-05 	 
2025-08-05 03:40:52.179505 test begin: paddle.stack(list[Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),], axis=-2, ) 	 254018560 	 5521 	 9.769899129867554 	 38.49924635887146 	 1.8087654113769531 	 7.126380443572998 	 10.953179121017456 	 0.5948426723480225 	 2.0274744033813477 	 5.984306335449219e-05 	 
2025-08-05 03:42:01.902707 test begin: paddle.stack(list[Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),], axis=-2, ) 	 257826240 	 5521 	 9.98549747467041 	 39.09536004066467 	 1.8475372791290283 	 7.237060785293579 	 11.201756715774536 	 0.6081578731536865 	 2.0735278129577637 	 7.486343383789062e-05 	 
2025-08-05 03:43:11.377183 test begin: paddle.stack(list[Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),], axis=-2, ) 	 259269120 	 5521 	 10.019290208816528 	 39.31322002410889 	 1.8546578884124756 	 7.276555299758911 	 11.181610584259033 	 0.5951638221740723 	 2.069817066192627 	 6.413459777832031e-05 	 
2025-08-05 03:44:22.428805 test begin: paddle.stanh(x=Tensor([12700801, 2],"float64"), scale_a=6.42, scale_b=3.58, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([12700801, 2],"float64"), scale_a=6.42, scale_b=3.58, ) 	 25401602 	 33926 	 10.372272968292236 	 10.422172546386719 	 0.31244492530822754 	 0.3133995532989502 	 15.135265588760376 	 25.141584873199463 	 0.4559330940246582 	 0.3786487579345703 	 
2025-08-05 03:45:24.652767 test begin: paddle.stanh(x=Tensor([2, 12700801],"float64"), scale_a=6.42, scale_b=3.58, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2, 12700801],"float64"), scale_a=6.42, scale_b=3.58, ) 	 25401602 	 33926 	 10.37453556060791 	 10.403932571411133 	 0.3124721050262451 	 0.31343507766723633 	 15.133833885192871 	 25.139878273010254 	 0.4557971954345703 	 0.3786282539367676 	 
2025-08-05 03:46:26.864442 test begin: paddle.stanh(x=Tensor([2, 25401601],"float32"), scale_a=6.42, scale_b=3.58, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2, 25401601],"float32"), scale_a=6.42, scale_b=3.58, ) 	 50803202 	 33926 	 10.000641822814941 	 10.134355306625366 	 0.30134105682373047 	 0.30521607398986816 	 15.282113790512085 	 25.200031518936157 	 0.46030592918395996 	 0.37955212593078613 	 
2025-08-05 03:47:30.324295 test begin: paddle.stanh(x=Tensor([2, 3, 2, 2116801],"float64"), scale_a=0.67, scale_b=1.72, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2, 3, 2, 2116801],"float64"), scale_a=0.67, scale_b=1.72, ) 	 25401612 	 33926 	 10.154268026351929 	 10.181660890579224 	 0.30589962005615234 	 0.30676937103271484 	 15.192349910736084 	 25.14007043838501 	 0.4575803279876709 	 0.3786885738372803 	 
2025-08-05 03:48:32.134192 test begin: paddle.stanh(x=Tensor([2, 3, 2116801, 2],"float64"), scale_a=0.67, scale_b=1.72, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2, 3, 2116801, 2],"float64"), scale_a=0.67, scale_b=1.72, ) 	 25401612 	 33926 	 10.154271841049194 	 10.18208360671997 	 0.3059260845184326 	 0.30675482749938965 	 15.193358182907104 	 25.139291286468506 	 0.457674503326416 	 0.37868332862854004 	 
2025-08-05 03:49:34.088815 test begin: paddle.stanh(x=Tensor([2, 3175201, 2, 2],"float64"), scale_a=0.67, scale_b=1.72, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2, 3175201, 2, 2],"float64"), scale_a=0.67, scale_b=1.72, ) 	 25401608 	 33926 	 10.155379295349121 	 10.182393789291382 	 0.3058607578277588 	 0.30669164657592773 	 15.192763566970825 	 25.13910937309265 	 0.457517147064209 	 0.37865352630615234 	 
2025-08-05 03:50:38.092787 test begin: paddle.stanh(x=Tensor([2116801, 3, 2, 2],"float64"), scale_a=0.67, scale_b=1.72, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2116801, 3, 2, 2],"float64"), scale_a=0.67, scale_b=1.72, ) 	 25401612 	 33926 	 10.153698682785034 	 10.202038288116455 	 0.305863618850708 	 0.30675601959228516 	 15.195013284683228 	 25.139942169189453 	 0.45789146423339844 	 0.37860631942749023 	 
2025-08-05 03:51:41.988420 test begin: paddle.stanh(x=Tensor([25401601, 2],"float32"), scale_a=6.42, scale_b=3.58, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([25401601, 2],"float32"), scale_a=6.42, scale_b=3.58, ) 	 50803202 	 33926 	 10.000393390655518 	 10.134848356246948 	 0.3012230396270752 	 0.3052055835723877 	 15.282015323638916 	 25.201088666915894 	 0.4603271484375 	 0.3796043395996094 	 
2025-08-05 03:52:45.007662 test begin: paddle.std(Tensor([1, 1270081, 4, 10],"float32"), list[1,3,], True, False, )
W0805 03:52:45.784284 83476 dygraph_functions.cc:88394] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.std 	 paddle.std(Tensor([1, 1270081, 4, 10],"float32"), list[1,3,], True, False, ) 	 50803240 	 9126 	 11.693593978881836 	 2.107546091079712 	 3.886222839355469e-05 	 0.11799955368041992 	 12.998899698257446 	 7.317203760147095 	 0.18230438232421875 	 0.09136009216308594 	 
2025-08-05 03:53:20.662609 test begin: paddle.std(Tensor([1, 3, 1693441, 10],"float32"), list[1,3,], True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([1, 3, 1693441, 10],"float32"), list[1,3,], True, False, ) 	 50803230 	 9126 	 14.175114631652832 	 7.239928483963013 	 4.673004150390625e-05 	 0.8107879161834717 	 15.04775857925415 	 9.797675371170044 	 0.24070191383361816 	 0.1373603343963623 	 
2025-08-05 03:54:08.047662 test begin: paddle.std(Tensor([1, 3, 4, 2116801],"float64"), 2, True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([1, 3, 4, 2116801],"float64"), 2, True, False, ) 	 25401612 	 9126 	 15.018444776535034 	 2.041710376739502 	 0.00010132789611816406 	 0.23239827156066895 	 18.446312427520752 	 13.558515310287476 	 0.29508066177368164 	 0.18997979164123535 	 
2025-08-05 03:54:57.949151 test begin: paddle.std(Tensor([1, 3, 4, 4233601],"float32"), list[1,3,], True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([1, 3, 4, 4233601],"float32"), list[1,3,], True, False, ) 	 50803212 	 9126 	 11.050220966339111 	 2.1447031497955322 	 3.504753112792969e-05 	 0.12011241912841797 	 12.826563358306885 	 7.286229372024536 	 0.17985796928405762 	 0.09096646308898926 	 
2025-08-05 03:55:33.293458 test begin: paddle.std(Tensor([1, 3, 846721, 10],"float64"), 2, True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([1, 3, 846721, 10],"float64"), 2, True, False, ) 	 25401630 	 9126 	 72.80827522277832 	 1.6919386386871338 	 4.9114227294921875e-05 	 0.09474349021911621 	 43.97180724143982 	 7.100943565368652 	 0.6166872978210449 	 0.08862638473510742 	 
2025-08-05 03:57:40.600279 test begin: paddle.std(Tensor([1, 635041, 4, 10],"float64"), 2, True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([1, 635041, 4, 10],"float64"), 2, True, False, ) 	 25401640 	 9126 	 15.98570704460144 	 1.8145852088928223 	 0.00011372566223144531 	 0.20273447036743164 	 17.700178861618042 	 11.561314105987549 	 0.2831401824951172 	 0.1619417667388916 	 
2025-08-05 03:58:28.403119 test begin: paddle.std(Tensor([1587601, 32],"float32"), )
[Prof] paddle.std 	 paddle.std(Tensor([1587601, 32],"float32"), ) 	 50803232 	 9126 	 9.937634468078613 	 1.5191450119018555 	 2.574920654296875e-05 	 0.08510327339172363 	 12.207518577575684 	 7.0775251388549805 	 0.17117023468017578 	 0.08832168579101562 	 
2025-08-05 03:59:01.064042 test begin: paddle.std(Tensor([211681, 3, 4, 10],"float64"), 2, True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([211681, 3, 4, 10],"float64"), 2, True, False, ) 	 25401720 	 9126 	 15.977406978607178 	 3.566917657852173 	 0.00011539459228515625 	 0.20275425910949707 	 17.70006513595581 	 11.563514471054077 	 0.2831425666809082 	 0.16191411018371582 	 
2025-08-05 03:59:53.284474 test begin: paddle.std(Tensor([32, 1587601],"float32"), )
[Prof] paddle.std 	 paddle.std(Tensor([32, 1587601],"float32"), ) 	 50803232 	 9126 	 9.942854881286621 	 1.5191140174865723 	 3.6716461181640625e-05 	 0.0851130485534668 	 12.20766806602478 	 7.074507236480713 	 0.1711282730102539 	 0.0883321762084961 	 
2025-08-05 04:00:24.931067 test begin: paddle.std(Tensor([423361, 3, 4, 10],"float32"), list[1,3,], True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([423361, 3, 4, 10],"float32"), list[1,3,], True, False, ) 	 50803320 	 9126 	 13.79017424583435 	 7.535376787185669 	 4.863739013671875e-05 	 0.8435697555541992 	 14.917413473129272 	 10.053325891494751 	 0.23862457275390625 	 0.14092087745666504 	 
2025-08-05 04:01:14.210298 test begin: paddle.strided_slice(x=Tensor([301, 4, 3528, 6],"float64"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
[Prof] paddle.strided_slice 	 paddle.strided_slice(x=Tensor([301, 4, 3528, 6],"float64"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], ) 	 25486272 	 476161 	 2.713719367980957 	 101.5515079498291 	 5.14984130859375e-05 	 0.000202178955078125 	 70.98492789268494 	 119.0732524394989 	 0.07588315010070801 	 0.009276628494262695 	 combined
2025-08-05 04:06:09.222954 test begin: paddle.subtract(Tensor([24904, 12, 170, 1],"float32"), Tensor([24904, 12, 170, 1],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([24904, 12, 170, 1],"float32"), Tensor([24904, 12, 170, 1],"float32"), ) 	 101608320 	 28013 	 12.617210149765015 	 12.510772466659546 	 0.46148228645324707 	 0.4562649726867676 	 13.33476996421814 	 8.338023900985718 	 0.4865131378173828 	 0.3041238784790039 	 
2025-08-05 04:07:01.026993 test begin: paddle.subtract(Tensor([3548, 12, 1194, 1],"float32"), Tensor([3548, 12, 1194, 1],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([3548, 12, 1194, 1],"float32"), Tensor([3548, 12, 1194, 1],"float32"), ) 	 101671488 	 28013 	 12.622798442840576 	 12.517990589141846 	 0.4604380130767822 	 0.4565391540527344 	 13.3569815158844 	 8.343974828720093 	 0.48703908920288086 	 0.30428004264831543 	 
2025-08-05 04:07:50.470984 test begin: paddle.subtract(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 8],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 8],"float32"), ) 	 65141280 	 28013 	 10.017165422439575 	 10.354897737503052 	 0.3653371334075928 	 0.3776724338531494 	 24.85409927368164 	 25.166539192199707 	 0.4531874656677246 	 0.4589102268218994 	 
2025-08-05 04:09:03.314023 test begin: paddle.subtract(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 1],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 1],"float32"), ) 	 65141280 	 28013 	 10.004966735839844 	 11.77393913269043 	 0.36489033699035645 	 0.3776242733001709 	 23.008235216140747 	 25.168339490890503 	 0.27967023849487305 	 0.4588806629180908 	 
2025-08-05 04:10:19.507339 test begin: paddle.subtract(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 8],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 8],"float32"), ) 	 115806720 	 28013 	 14.365641355514526 	 14.242343425750732 	 0.5239651203155518 	 0.5194571018218994 	 15.14664340019226 	 9.482823133468628 	 0.5524029731750488 	 0.34588146209716797 	 
2025-08-05 04:11:15.739271 test begin: paddle.subtract(Tensor([3548, 85, 170, 1],"float32"), Tensor([3548, 85, 170, 1],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([3548, 85, 170, 1],"float32"), Tensor([3548, 85, 170, 1],"float32"), ) 	 102537200 	 28013 	 12.732198238372803 	 12.62312388420105 	 0.4644765853881836 	 0.4603891372680664 	 13.406533241271973 	 8.413052558898926 	 0.4890303611755371 	 0.3068513870239258 	 
2025-08-05 04:12:05.506893 test begin: paddle.subtract(Tensor([517, 4, 3, 64, 128],"float32"), Tensor([517, 4, 3, 64, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([517, 4, 3, 64, 128],"float32"), Tensor([517, 4, 3, 64, 128],"float32"), ) 	 101646336 	 28013 	 12.62527847290039 	 12.513903856277466 	 0.46045851707458496 	 0.4577305316925049 	 13.300050973892212 	 8.342380046844482 	 0.4850184917449951 	 0.30570197105407715 	 
2025-08-05 04:12:55.937672 test begin: paddle.subtract(Tensor([64, 3, 3, 64, 1379],"float32"), Tensor([64, 3, 3, 64, 1379],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 3, 3, 64, 1379],"float32"), Tensor([64, 3, 3, 64, 1379],"float32"), ) 	 101670912 	 28013 	 12.62676191329956 	 12.515933513641357 	 0.46054768562316895 	 0.4565255641937256 	 13.310938358306885 	 8.342873334884644 	 0.48549461364746094 	 0.30559754371643066 	 
2025-08-05 04:13:47.241424 test begin: paddle.subtract(Tensor([64, 3, 3, 690, 128],"float32"), Tensor([64, 3, 3, 690, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 3, 3, 690, 128],"float32"), Tensor([64, 3, 3, 690, 128],"float32"), ) 	 101744640 	 28013 	 12.634454727172852 	 12.527182579040527 	 0.46090245246887207 	 0.45687389373779297 	 13.322606563568115 	 8.34919261932373 	 0.48584771156311035 	 0.30460429191589355 	 
2025-08-05 04:14:37.507798 test begin: paddle.subtract(Tensor([64, 3, 33, 64, 128],"float32"), Tensor([64, 3, 33, 64, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 3, 33, 64, 128],"float32"), Tensor([64, 3, 33, 64, 128],"float32"), ) 	 103809024 	 28013 	 12.889643669128418 	 12.778485298156738 	 0.4701850414276123 	 0.46604490280151367 	 13.620996713638306 	 8.513662815093994 	 0.4968416690826416 	 0.3104982376098633 	 
2025-08-05 04:15:28.207051 test begin: paddle.subtract(Tensor([64, 33, 3, 64, 128],"float32"), Tensor([64, 33, 3, 64, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 33, 3, 64, 128],"float32"), Tensor([64, 33, 3, 64, 128],"float32"), ) 	 103809024 	 28013 	 12.89062213897705 	 12.784830331802368 	 0.47013187408447266 	 0.46600341796875 	 13.619787693023682 	 8.513744831085205 	 0.49676990509033203 	 0.31049561500549316 	 
2025-08-05 04:16:20.687043 test begin: paddle.subtract(Tensor([64, 4, 25, 64, 128],"float32"), Tensor([64, 4, 25, 64, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 4, 25, 64, 128],"float32"), Tensor([64, 4, 25, 64, 128],"float32"), ) 	 104857600 	 28013 	 13.01865839958191 	 12.907365083694458 	 0.47479915618896484 	 0.47073888778686523 	 13.685128211975098 	 8.59968614578247 	 0.5004551410675049 	 0.3136627674102783 	 
2025-08-05 04:17:11.656323 test begin: paddle.subtract(Tensor([64, 4, 3, 517, 128],"float32"), Tensor([64, 4, 3, 517, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 4, 3, 517, 128],"float32"), Tensor([64, 4, 3, 517, 128],"float32"), ) 	 101646336 	 28013 	 12.624190330505371 	 12.518738269805908 	 0.4603574275970459 	 0.4564673900604248 	 13.299006223678589 	 8.341909408569336 	 0.48511767387390137 	 0.3042612075805664 	 
2025-08-05 04:18:01.744393 test begin: paddle.subtract(Tensor([64, 4, 3, 64, 1034],"float32"), Tensor([64, 4, 3, 64, 1034],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 4, 3, 64, 1034],"float32"), Tensor([64, 4, 3, 64, 1034],"float32"), ) 	 101646336 	 28013 	 12.622052192687988 	 12.5127592086792 	 0.4605116844177246 	 0.4564247131347656 	 13.300575733184814 	 8.341896057128906 	 0.4851858615875244 	 0.30424046516418457 	 
2025-08-05 04:18:51.103081 test begin: paddle.subtract(Tensor([690, 3, 3, 64, 128],"float32"), Tensor([690, 3, 3, 64, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([690, 3, 3, 64, 128],"float32"), Tensor([690, 3, 3, 64, 128],"float32"), ) 	 101744640 	 28013 	 12.63556170463562 	 12.99228310585022 	 0.460784912109375 	 0.45686769485473633 	 13.322332620620728 	 8.349098920822144 	 0.48585939407348633 	 0.3060166835784912 	 
2025-08-05 04:19:43.381881 test begin: paddle.sum(Tensor([3544, 32, 896],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([3544, 32, 896],"bfloat16"), axis=1, keepdim=False, ) 	 101613568 	 58698 	 10.023168802261353 	 9.379515171051025 	 0.17445850372314453 	 0.16326379776000977 	 15.74105954170227 	 4.572350740432739 	 0.2739598751068115 	 7.05718994140625e-05 	 
2025-08-05 04:20:25.302250 test begin: paddle.sum(Tensor([6017, 19, 896],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6017, 19, 896],"bfloat16"), axis=1, keepdim=False, ) 	 102433408 	 58698 	 10.25939154624939 	 10.293370485305786 	 0.1786046028137207 	 0.17917132377624512 	 15.895625591278076 	 4.7300944328308105 	 0.2766869068145752 	 0.0001728534698486328 	 
2025-08-05 04:21:08.291630 test begin: paddle.sum(Tensor([6017, 32, 528],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6017, 32, 528],"bfloat16"), axis=1, keepdim=False, ) 	 101663232 	 58698 	 10.736129760742188 	 9.531989574432373 	 0.18691086769104004 	 0.16590666770935059 	 15.757607221603394 	 4.592584848403931 	 0.2742626667022705 	 0.00014734268188476562 	 
2025-08-05 04:21:51.539969 test begin: paddle.sum(Tensor([6036, 19, 896],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6036, 19, 896],"bfloat16"), axis=1, keepdim=False, ) 	 102756864 	 58698 	 10.269602298736572 	 10.326982975006104 	 0.17873167991638184 	 0.17977309226989746 	 15.930117845535278 	 4.548358917236328 	 0.27725672721862793 	 7.176399230957031e-05 	 
2025-08-05 04:22:34.470253 test begin: paddle.sum(Tensor([6036, 32, 527],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6036, 32, 527],"bfloat16"), axis=1, keepdim=False, ) 	 101791104 	 58698 	 10.878257751464844 	 10.548693895339966 	 0.18937277793884277 	 0.18359041213989258 	 15.777221918106079 	 4.728923559188843 	 0.2746312618255615 	 0.0001494884490966797 	 
2025-08-05 04:23:18.165071 test begin: paddle.sum(Tensor([6078, 19, 896],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6078, 19, 896],"bfloat16"), axis=1, keepdim=False, ) 	 103471872 	 58698 	 10.350893020629883 	 11.644064664840698 	 0.18159198760986328 	 0.18105673789978027 	 16.03535294532776 	 4.609145879745483 	 0.2791330814361572 	 6.937980651855469e-05 	 
2025-08-05 04:24:04.635391 test begin: paddle.sum(Tensor([6078, 32, 523],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6078, 32, 523],"bfloat16"), axis=1, keepdim=False, ) 	 101721408 	 58698 	 10.647618532180786 	 10.332221031188965 	 0.18532490730285645 	 0.17987632751464844 	 15.769108772277832 	 4.7457275390625 	 0.2744567394256592 	 7.367134094238281e-05 	 
2025-08-05 04:24:47.894755 test begin: paddle.t(Tensor([100, 5080321],"float32"), )
[Prof] paddle.t 	 paddle.t(Tensor([100, 5080321],"float32"), ) 	 508032100 	 2515777 	 10.653074026107788 	 9.062868118286133 	 0.000133514404296875 	 9.465217590332031e-05 	 98.78652453422546 	 131.28275966644287 	 0.00010919570922851562 	 0.0002205371856689453 	 
2025-08-05 04:29:14.419229 test begin: paddle.t(Tensor([200, 2540161],"float32"), )
[Prof] paddle.t 	 paddle.t(Tensor([200, 2540161],"float32"), ) 	 508032200 	 2515777 	 10.675142526626587 	 9.290317296981812 	 8.130073547363281e-05 	 8.96453857421875e-05 	 98.9144446849823 	 130.65650725364685 	 0.00012993812561035156 	 0.00022220611572265625 	 
2025-08-05 04:33:41.013673 test begin: paddle.t(Tensor([25401610, 20],"float32"), )
[Prof] paddle.t 	 paddle.t(Tensor([25401610, 20],"float32"), ) 	 508032200 	 2515777 	 10.702191829681396 	 9.252698183059692 	 7.43865966796875e-05 	 0.0001347064971923828 	 101.99762797355652 	 130.78137755393982 	 0.00010895729064941406 	 0.00023889541625976562 	 
2025-08-05 04:38:10.869459 test begin: paddle.t(Tensor([496130, 512],"int64"), )
[Prof] paddle.t 	 paddle.t(Tensor([496130, 512],"int64"), ) 	 254018560 	 2515777 	 10.747519493103027 	 9.245360612869263 	 6.246566772460938e-05 	 0.00013113021850585938 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:40:18.953038 test begin: paddle.t(Tensor([50803210, 10],"float32"), )
[Prof] paddle.t 	 paddle.t(Tensor([50803210, 10],"float32"), ) 	 508032100 	 2515777 	 10.624655723571777 	 9.132542848587036 	 7.939338684082031e-05 	 0.000240325927734375 	 98.5533185005188 	 130.4725124835968 	 0.00010919570922851562 	 0.00021409988403320312 	 
2025-08-05 04:44:44.589153 test begin: paddle.t(Tensor([5120, 49613],"int64"), )
[Prof] paddle.t 	 paddle.t(Tensor([5120, 49613],"int64"), ) 	 254018560 	 2515777 	 10.816371202468872 	 9.138912677764893 	 0.00014281272888183594 	 0.0002510547637939453 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:46:55.128228 test begin: paddle.take(Tensor([12700801, 4],"float32"), Tensor([2, 12700801],"int64"), mode="raise", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe69e17ec50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 04:57:01.613455 test begin: paddle.take(Tensor([12700801, 4],"float32"), Tensor([201, 3],"int64"), mode="raise", )
W0805 04:57:02.622414 145452 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.take 	 paddle.take(Tensor([12700801, 4],"float32"), Tensor([201, 3],"int64"), mode="raise", ) 	 50803807 	 172659 	 14.234357833862305 	 21.335394859313965 	 5.7697296142578125e-05 	 0.0002551078796386719 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 04:58:06.526900 test begin: paddle.take(Tensor([12700801, 4],"float32"), Tensor([6350401, 3],"int64"), mode="raise", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8e71d76aa0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 05:08:12.434843 test begin: paddle.take(Tensor([3, 16934401],"float32"), Tensor([2, 3],"int64"), mode="raise", )
W0805 05:08:13.472312  1561 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.take 	 paddle.take(Tensor([3, 16934401],"float32"), Tensor([2, 3],"int64"), mode="raise", ) 	 50803209 	 172659 	 14.519422054290771 	 22.43071222305298 	 5.8650970458984375e-05 	 0.00027108192443847656 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:09:17.113862 test begin: paddle.take(Tensor([3, 16934401],"float32"), Tensor([2, 8467201],"int64"), mode="raise", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6ffea228c0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 05:19:22.870065 test begin: paddle.take(Tensor([3, 16934401],"float32"), Tensor([201, 3],"int64"), mode="raise", )
W0805 05:19:24.102250 21491 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.take 	 paddle.take(Tensor([3, 16934401],"float32"), Tensor([201, 3],"int64"), mode="raise", ) 	 50803806 	 172659 	 14.344954490661621 	 21.025342226028442 	 5.793571472167969e-05 	 0.00020885467529296875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:20:27.625340 test begin: paddle.take(Tensor([3, 4],"float32"), Tensor([2, 12700801],"int64"), mode="raise", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4d95f92b60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 05:30:37.607292 test begin: paddle.take(Tensor([3, 4],"float32"), Tensor([8467201, 3],"int64"), mode="raise", )
W0805 05:30:42.557485 40935 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f32b830afb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 05:40:45.058477 test begin: paddle.take(Tensor([3, 4],"float64"), Tensor([3175201, 8],"int64"), mode="clip", )
W0805 05:40:45.732156 57310 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f56f134edd0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 05:50:49.748218 test begin: paddle.take(Tensor([3, 4],"float64"), Tensor([3175201, 8],"int64"), mode="wrap", )
W0805 05:50:53.265293 74471 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdaa0463010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 06:00:54.522687 test begin: paddle.take(Tensor([3, 4],"float64"), Tensor([5, 5080321],"int64"), mode="clip", )
W0805 06:00:55.269709 91761 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f18ecf9afb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 06:10:59.372318 test begin: paddle.take(Tensor([3, 4],"float64"), Tensor([5, 5080321],"int64"), mode="wrap", )
W0805 06:11:00.326232 115940 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fec2d962dd0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 06:21:04.094982 test begin: paddle.take(Tensor([3, 8467201],"float64"), Tensor([5, 8467201],"int64"), mode="clip", )
W0805 06:21:05.521878 141391 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0674d4eef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 06:31:14.132584 test begin: paddle.take(Tensor([3, 8467201],"float64"), Tensor([5, 8467201],"int64"), mode="wrap", )
W0805 06:31:15.518788  2944 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa13d446ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 06:41:18.626422 test begin: paddle.take(Tensor([3, 8467201],"float64"), Tensor([5, 8],"int64"), mode="wrap", )
W0805 06:41:19.446125 29371 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.take 	 paddle.take(Tensor([3, 8467201],"float64"), Tensor([5, 8],"int64"), mode="wrap", ) 	 25401643 	 172659 	 25.60795569419861 	 13.221322059631348 	 5.745887756347656e-05 	 0.00011038780212402344 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 06:42:24.241273 test begin: paddle.take(Tensor([3, 8467201],"float64"), Tensor([501, 8],"int64"), mode="clip", )
[Prof] paddle.take 	 paddle.take(Tensor([3, 8467201],"float64"), Tensor([501, 8],"int64"), mode="clip", ) 	 25405611 	 172659 	 10.073009252548218 	 7.515832185745239 	 5.984306335449219e-05 	 0.00018143653869628906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 06:43:09.008615 test begin: paddle.take(Tensor([6350401, 4],"float64"), Tensor([5, 8],"int64"), mode="wrap", )
[Prof] paddle.take 	 paddle.take(Tensor([6350401, 4],"float64"), Tensor([5, 8],"int64"), mode="wrap", ) 	 25401644 	 172659 	 25.68848752975464 	 12.652960300445557 	 5.5789947509765625e-05 	 0.00024175643920898438 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 06:44:14.923977 test begin: paddle.take(Tensor([6350401, 4],"float64"), Tensor([6350401, 8],"int64"), mode="clip", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7acc966d10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 06:54:21.677688 test begin: paddle.take(Tensor([6350401, 4],"float64"), Tensor([6350401, 8],"int64"), mode="wrap", )
W0805 06:54:23.294781 62183 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdf35c4f130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 07:04:26.462383 test begin: paddle.take_along_axis(Tensor([1024, 384],"float32"), Tensor([1024, 24807],"int64"), axis=-1, )
W0805 07:04:28.581262 87107 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([1024, 384],"float32"), Tensor([1024, 24807],"int64"), axis=-1, ) 	 25795584 	 32853 	 19.06327199935913 	 7.935849905014038 	 0.1974782943725586 	 0.24651145935058594 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:05:48.330318 test begin: paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 24807],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 24807],"int64"), axis=-1, ) 	 76206080 	 32853 	 32.991657733917236 	 14.474685192108154 	 0.341785192489624 	 0.44985485076904297 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:07:17.516665 test begin: paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 7],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 7],"int64"), axis=-1, ) 	 50810880 	 32853 	 10.015039682388306 	 0.5333356857299805 	 0.1038370132446289 	 7.62939453125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:07:43.988931 test begin: paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 8],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 8],"int64"), axis=-1, ) 	 50811904 	 32853 	 10.01612401008606 	 0.5569686889648438 	 0.10383009910583496 	 8.392333984375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:08:10.556140 test begin: paddle.take_along_axis(Tensor([1051, 63, 768],"float32"), axis=1, indices=Tensor([1051, 7, 768],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([1051, 63, 768],"float32"), axis=1, indices=Tensor([1051, 7, 768],"int64"), ) 	 56501760 	 32853 	 18.163692474365234 	 10.083970308303833 	 0.1882927417755127 	 0.31334662437438965 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:09:04.498180 test begin: paddle.take_along_axis(Tensor([132301, 384],"float32"), Tensor([132301, 7],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([132301, 384],"float32"), Tensor([132301, 7],"int64"), axis=-1, ) 	 51729691 	 32853 	 11.972866296768188 	 1.9055967330932617 	 0.12550711631774902 	 0.05923795700073242 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:09:40.719941 test begin: paddle.take_along_axis(Tensor([132301, 384],"float32"), Tensor([132301, 8],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([132301, 384],"float32"), Tensor([132301, 8],"int64"), axis=-1, ) 	 51861992 	 32853 	 12.215418100357056 	 2.064912796020508 	 0.12662816047668457 	 0.06418728828430176 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:10:15.105207 test begin: paddle.take_along_axis(Tensor([3175201, 384],"float32"), Tensor([3175201, 8],"int64"), axis=-1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6d4a8129e0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 07:20:22.709959 test begin: paddle.take_along_axis(Tensor([3628801, 384],"float32"), Tensor([3628801, 7],"int64"), axis=-1, )
W0805 07:20:45.851893 128685 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5537462c20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 07:30:27.724157 test begin: paddle.take_along_axis(Tensor([4726, 63, 768],"float32"), axis=1, indices=Tensor([4726, 7, 768],"int64"), )
W0805 07:30:33.313836 153091 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([4726, 63, 768],"float32"), axis=1, indices=Tensor([4726, 7, 768],"int64"), ) 	 254069760 	 32853 	 79.90674352645874 	 45.41426730155945 	 0.8315653800964355 	 1.4113163948059082 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:34:27.950315 test begin: paddle.take_along_axis(Tensor([8, 63, 100801],"float32"), axis=1, indices=Tensor([8, 7, 100801],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([8, 63, 100801],"float32"), axis=1, indices=Tensor([8, 7, 100801],"int64"), ) 	 56448560 	 32853 	 18.793985843658447 	 10.116841554641724 	 0.19610881805419922 	 0.3159358501434326 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:35:28.029485 test begin: paddle.take_along_axis(Tensor([8, 63, 453601],"float32"), axis=1, indices=Tensor([8, 7, 453601],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([8, 63, 453601],"float32"), axis=1, indices=Tensor([8, 7, 453601],"int64"), ) 	 254016560 	 32853 	 99.78458166122437 	 52.15270495414734 	 1.0360982418060303 	 1.6211814880371094 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:41:18.189463 test begin: paddle.take_along_axis(Tensor([8, 63, 768],"float32"), axis=1, indices=Tensor([8, 4135, 768],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([8, 63, 768],"float32"), axis=1, indices=Tensor([8, 4135, 768],"int64"), ) 	 25792512 	 32853 	 19.740503072738647 	 9.36130428314209 	 0.20441079139709473 	 0.2909717559814453 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:42:11.298797 test begin: paddle.take_along_axis(Tensor([8, 8269, 768],"float32"), axis=1, indices=Tensor([8, 4135, 768],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([8, 8269, 768],"float32"), axis=1, indices=Tensor([8, 4135, 768],"int64"), ) 	 76210176 	 32853 	 44.39775323867798 	 31.25399684906006 	 0.46030545234680176 	 0.9714162349700928 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:44:22.429917 test begin: paddle.take_along_axis(Tensor([8, 8269, 768],"float32"), axis=1, indices=Tensor([8, 7, 768],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([8, 8269, 768],"float32"), axis=1, indices=Tensor([8, 7, 768],"int64"), ) 	 50847744 	 32853 	 10.120612621307373 	 0.5554988384246826 	 0.10494494438171387 	 7.486343383789062e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:44:49.181939 test begin: paddle.tan(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.tan 	 paddle.tan(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33898 	 10.00621747970581 	 10.11019778251648 	 0.30140089988708496 	 0.30442047119140625 	 15.318073511123657 	 35.28042125701904 	 0.46277642250061035 	 0.3556673526763916 	 
2025-08-05 07:46:01.733100 test begin: paddle.tan(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.tan 	 paddle.tan(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33898 	 10.005752086639404 	 10.10257887840271 	 0.30144262313842773 	 0.30446720123291016 	 15.322614908218384 	 35.2844660282135 	 0.4630441665649414 	 0.35443615913391113 	 
2025-08-05 07:47:14.207899 test begin: paddle.tan(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.tan 	 paddle.tan(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33898 	 10.006412029266357 	 10.115153789520264 	 0.30146145820617676 	 0.30443310737609863 	 15.312333822250366 	 35.286457538604736 	 0.4614877700805664 	 0.3543863296508789 	 
2025-08-05 07:48:27.206468 test begin: paddle.tanh(Tensor([16, 125, 25500],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([16, 125, 25500],"float32"), ) 	 51000000 	 33846 	 10.050455331802368 	 10.13663911819458 	 0.30320191383361816 	 0.30562424659729004 	 15.305711507797241 	 15.176289319992065 	 0.4617645740509033 	 0.459378719329834 	 
2025-08-05 07:49:20.577778 test begin: paddle.tanh(Tensor([16, 64, 49613],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([16, 64, 49613],"float32"), ) 	 50803712 	 33846 	 10.00666618347168 	 10.0922532081604 	 0.30187010765075684 	 0.3043632507324219 	 15.236952543258667 	 15.123203754425049 	 0.45974278450012207 	 0.4576237201690674 	 
2025-08-05 07:50:12.761906 test begin: paddle.tanh(Tensor([28, 32, 241, 241],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([28, 32, 241, 241],"float32"), ) 	 52040576 	 33846 	 10.25208330154419 	 10.340969800949097 	 0.3093297481536865 	 0.3117349147796631 	 15.607721328735352 	 15.486842155456543 	 0.47108936309814453 	 0.46718668937683105 	 
2025-08-05 07:51:07.608458 test begin: paddle.tanh(Tensor([32, 64, 25500],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([32, 64, 25500],"float32"), ) 	 52224000 	 33846 	 10.292683601379395 	 10.833845615386963 	 0.3104078769683838 	 0.31276965141296387 	 15.662578105926514 	 15.54252552986145 	 0.47272801399230957 	 0.4701657295227051 	 
2025-08-05 07:52:04.504959 test begin: paddle.tanh(Tensor([64, 26, 512, 1, 60],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([64, 26, 512, 1, 60],"float32"), ) 	 51118080 	 33846 	 10.077468395233154 	 10.14685606956482 	 0.3053271770477295 	 0.30629849433898926 	 15.34062671661377 	 15.212141990661621 	 0.46271443367004395 	 0.45896077156066895 	 
2025-08-05 07:52:57.309909 test begin: paddle.tanh(Tensor([64, 26, 512, 2, 40],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([64, 26, 512, 2, 40],"float32"), ) 	 68157440 	 33846 	 13.390459775924683 	 13.466664791107178 	 0.40393829345703125 	 0.40645670890808105 	 20.397859573364258 	 20.22929096221924 	 0.6166877746582031 	 0.6116952896118164 	 
2025-08-05 07:54:07.391059 test begin: paddle.tanh(Tensor([64, 26, 764, 1, 40],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([64, 26, 764, 1, 40],"float32"), ) 	 50851840 	 33846 	 10.028961658477783 	 10.108982563018799 	 0.3024594783782959 	 0.30473947525024414 	 15.258942604064941 	 15.131356716156006 	 0.4604356288909912 	 0.45661115646362305 	 
2025-08-05 07:55:00.326920 test begin: paddle.tanh(Tensor([64, 39, 512, 1, 40],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([64, 39, 512, 1, 40],"float32"), ) 	 51118080 	 33846 	 10.075045824050903 	 10.153105735778809 	 0.30385470390319824 	 0.3063368797302246 	 15.338037967681885 	 15.21324872970581 	 0.4627208709716797 	 0.4590139389038086 	 
2025-08-05 07:55:52.822150 test begin: paddle.tanh(Tensor([8, 110, 241, 241],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([8, 110, 241, 241],"float32"), ) 	 51111280 	 33846 	 10.057024955749512 	 10.15378999710083 	 0.30351948738098145 	 0.30629539489746094 	 15.33691954612732 	 15.206447124481201 	 0.46275877952575684 	 0.4589202404022217 	 
2025-08-05 07:56:45.929358 test begin: paddle.tanh(Tensor([8, 32, 241, 824],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([8, 32, 241, 824],"float32"), ) 	 50837504 	 33846 	 10.024281978607178 	 10.110513925552368 	 0.3037447929382324 	 0.305997371673584 	 15.252417087554932 	 15.129337310791016 	 0.4603271484375 	 0.4564814567565918 	 
2025-08-05 07:57:41.364103 test begin: paddle.tanh(Tensor([8, 32, 824, 241],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([8, 32, 824, 241],"float32"), ) 	 50837504 	 33846 	 10.02575135231018 	 10.102891683578491 	 0.3024253845214844 	 0.30617237091064453 	 15.254651546478271 	 15.131028652191162 	 0.46019792556762695 	 0.4565093517303467 	 
2025-08-05 07:58:33.753869 test begin: paddle.tanh(Tensor([96, 26, 512, 1, 40],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([96, 26, 512, 1, 40],"float32"), ) 	 51118080 	 33846 	 10.074060678482056 	 10.154744625091553 	 0.30389904975891113 	 0.3076903820037842 	 15.341092348098755 	 15.210427045822144 	 0.4640624523162842 	 0.45900464057922363 	 
2025-08-05 07:59:27.660176 test begin: paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), list[2,3,], axis=3, )
W0805 07:59:46.709877 62601 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), list[2,3,], axis=3, ) 	 254017120 	 428549 	 10.520148277282715 	 3.4159154891967773 	 0.000278472900390625 	 0.00012731552124023438 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 07:59:51.553819 test begin: paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), list[2,4,6,], axis=3, )
W0805 08:00:13.164242 63575 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), list[2,4,6,], axis=3, ) 	 254017120 	 428549 	 14.16600489616394 	 3.9886598587036133 	 0.0001666545867919922 	 0.00038933753967285156 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 08:00:19.085130 test begin: paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), tuple(2,6,), axis=3, )
W0805 08:00:39.793581 64918 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), tuple(2,6,), axis=3, ) 	 254017120 	 428549 	 11.319396734237671 	 5.39308762550354 	 0.0001327991485595703 	 0.0003325939178466797 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 08:00:47.640037 test begin: paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), list[2,3,], axis=3, )
W0805 08:01:05.177896 66053 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), list[2,3,], axis=3, ) 	 254017120 	 428549 	 10.47061038017273 	 3.4124600887298584 	 0.00017881393432617188 	 8.106231689453125e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 08:01:10.566589 test begin: paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), list[2,4,6,], axis=3, )
W0805 08:01:35.530248 67168 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), list[2,4,6,], axis=3, ) 	 254017120 	 428549 	 16.866744995117188 	 4.650738716125488 	 0.00013947486877441406 	 8.130073547363281e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 08:01:43.021304 test begin: paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), tuple(2,6,), axis=3, )
W0805 08:02:08.167709 68504 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), tuple(2,6,), axis=3, ) 	 254017120 	 428549 	 18.32733678817749 	 5.644747972488403 	 0.00012087821960449219 	 8.249282836914062e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 08:02:16.151430 test begin: paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), list[2,3,], axis=3, )
W0805 08:02:37.436295 70089 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), list[2,3,], axis=3, ) 	 254017120 	 428549 	 12.96323037147522 	 5.613811731338501 	 0.00011992454528808594 	 0.0002071857452392578 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 08:02:45.801344 test begin: paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), list[2,4,6,], axis=3, )
W0805 08:03:06.329156 71414 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), list[2,4,6,], axis=3, ) 	 254017120 	 428549 	 13.65040111541748 	 3.8929762840270996 	 0.00010824203491210938 	 6.842613220214844e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 08:03:11.863398 test begin: paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), tuple(2,6,), axis=3, )
W0805 08:03:29.634938 72546 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), tuple(2,6,), axis=3, ) 	 254017120 	 428549 	 10.823131799697876 	 3.3534786701202393 	 0.00010800361633300781 	 0.0002193450927734375 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 08:03:34.683153 test begin: paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), list[2,3,], axis=3, )
W0805 08:03:58.949352 73671 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), list[2,3,], axis=3, ) 	 254016640 	 428549 	 17.57215166091919 	 3.3776655197143555 	 0.00021028518676757812 	 0.0007691383361816406 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 08:04:03.718167 test begin: paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), list[2,4,6,], axis=3, )
W0805 08:04:24.166411 74792 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), list[2,4,6,], axis=3, ) 	 254016640 	 428549 	 13.566572189331055 	 4.020775079727173 	 0.0002391338348388672 	 0.0002300739288330078 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 08:04:29.902943 test begin: paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), tuple(2,6,), axis=3, )
W0805 08:04:47.013586 76135 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), tuple(2,6,), axis=3, ) 	 254016640 	 428549 	 10.482543230056763 	 3.398252248764038 	 0.00022673606872558594 	 0.0008096694946289062 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 9, in <module>
    from tester import (APIConfig, APITestAccuracy, APITestCINNVSDygraph,
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/__init__.py", line 50, in __getattr__
    from .accuracy import APITestAccuracy
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/accuracy.py", line 9, in <module>
    from .api_config.log_writer import write_to_log
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/log_writer.py", line 7, in <module>
    import pandas as pd
  File "/usr/local/lib/python3.10/dist-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/usr/local/lib/python3.10/dist-packages/pandas/core/api.py", line 28, in <module>
    from pandas.core.arrays import Categorical
  File "/usr/local/lib/python3.10/dist-packages/pandas/core/arrays/__init__.py", line 1, in <module>
    from pandas.core.arrays.arrow import ArrowExtensionArray
  File "/usr/local/lib/python3.10/dist-packages/pandas/core/arrays/arrow/__init__.py", line 5, in <module>
    from pandas.core.arrays.arrow.array import ArrowExtensionArray
  File "/usr/local/lib/python3.10/dist-packages/pandas/core/arrays/arrow/array.py", line 68, in <module>
    from pandas.core.arrays.string_ import StringDtype
  File "/usr/local/lib/python3.10/dist-packages/pandas/core/arrays/string_.py", line 30, in <module>
    from pandas.compat.numpy import function as nv
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
KeyboardInterrupt
2025-08-04 11:30:56.806159 test begin: paddle.Tensor.__or__(Tensor([4, 210, 75600],"bool"), Tensor([1, 210, 75600],"bool"), )
W0804 11:30:58.225415 85326 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([4, 210, 75600],"bool"), Tensor([1, 210, 75600],"bool"), ) 	 79380000 	 85484 	 17.97809600830078 	 23.76120138168335 	 0.21490883827209473 	 0.2840299606323242 	 None 	 None 	 None 	 None 	 
2025-08-04 11:31:44.228629 test begin: paddle.Tensor.__or__(Tensor([4, 210, 75600],"bool"), Tensor([4, 210, 75600],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([4, 210, 75600],"bool"), Tensor([4, 210, 75600],"bool"), ) 	 127008000 	 85484 	 12.394388437271118 	 12.245262145996094 	 0.14803504943847656 	 0.14642000198364258 	 None 	 None 	 None 	 None 	 
2025-08-04 11:32:10.656903 test begin: paddle.Tensor.__or__(Tensor([4, 218, 70644],"bool"), Tensor([1, 218, 70644],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([4, 218, 70644],"bool"), Tensor([1, 218, 70644],"bool"), ) 	 77001960 	 85484 	 17.51173710823059 	 23.006417989730835 	 0.2093360424041748 	 0.2750117778778076 	 None 	 None 	 None 	 None 	 
2025-08-04 11:32:52.377350 test begin: paddle.Tensor.__or__(Tensor([4, 218, 70644],"bool"), Tensor([4, 218, 70644],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([4, 218, 70644],"bool"), Tensor([4, 218, 70644],"bool"), ) 	 123203136 	 85484 	 12.068405151367188 	 11.981454133987427 	 0.14424920082092285 	 0.14325213432312012 	 None 	 None 	 None 	 None 	 
2025-08-04 11:33:18.215827 test begin: paddle.Tensor.__pow__(Tensor([23, 17, 256, 256],"float64"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([23, 17, 256, 256],"float64"), 2, ) 	 25624576 	 27052 	 15.634259700775146 	 9.590675115585327 	 0.5906016826629639 	 0.3074312210083008 	 16.49522566795349 	 28.677576065063477 	 0.6231393814086914 	 0.3611297607421875 	 
2025-08-04 11:34:31.970623 test begin: paddle.Tensor.__pow__(Tensor([24, 17, 244, 256],"float64"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([24, 17, 244, 256],"float64"), 2, ) 	 25485312 	 27052 	 15.549811601638794 	 8.104655981063843 	 0.5874605178833008 	 0.3057520389556885 	 16.54897117614746 	 28.522332668304443 	 0.6265010833740234 	 0.3592655658721924 	 
2025-08-04 11:35:44.870348 test begin: paddle.Tensor.__pow__(Tensor([24, 17, 256, 244],"float64"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([24, 17, 256, 244],"float64"), 2, ) 	 25485312 	 27052 	 15.558159589767456 	 8.094943046569824 	 0.5870468616485596 	 0.30568408966064453 	 16.579816341400146 	 28.52259588241577 	 0.6269149780273438 	 0.35921239852905273 	 
2025-08-04 11:36:54.828569 test begin: paddle.Tensor.__pow__(Tensor([24, 17, 256, 256],"float64"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([24, 17, 256, 256],"float64"), 2, ) 	 26738688 	 27052 	 16.301222324371338 	 10.143472909927368 	 0.6156370639801025 	 0.32053637504577637 	 17.366944551467896 	 29.92384171485901 	 0.6565330028533936 	 0.37687110900878906 	 
2025-08-04 11:38:12.507766 test begin: paddle.Tensor.__pow__(Tensor([259, 3, 256, 256],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([259, 3, 256, 256],"float32"), 2, ) 	 50921472 	 27052 	 10.001186609268188 	 8.615623235702515 	 0.3778114318847656 	 0.3049290180206299 	 12.26502799987793 	 28.536020755767822 	 0.46331238746643066 	 0.3594202995300293 	 
2025-08-04 11:39:15.856190 test begin: paddle.Tensor.__pow__(Tensor([28, 32, 241, 241],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([28, 32, 241, 241],"float32"), 2, ) 	 52040576 	 27052 	 10.22927975654602 	 8.865631580352783 	 0.38642454147338867 	 0.3115243911743164 	 12.521935224533081 	 29.239465475082397 	 0.47304320335388184 	 0.2763190269470215 	 
2025-08-04 11:40:21.031475 test begin: paddle.Tensor.__pow__(Tensor([64, 13, 256, 256],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([64, 13, 256, 256],"float32"), 2, ) 	 54525952 	 27052 	 10.70923376083374 	 8.63267707824707 	 0.4045259952545166 	 0.3261833190917969 	 13.116003274917603 	 30.54000735282898 	 0.4954667091369629 	 0.38462042808532715 	 
2025-08-04 11:41:26.092133 test begin: paddle.Tensor.__pow__(Tensor([64, 3, 1034, 256],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([64, 3, 1034, 256],"float32"), 2, ) 	 50823168 	 27052 	 9.98762059211731 	 8.055056095123291 	 0.3772928714752197 	 0.30436086654663086 	 12.240030288696289 	 28.475289583206177 	 0.4623243808746338 	 0.3586716651916504 	 
2025-08-04 11:42:27.364884 test begin: paddle.Tensor.__pow__(Tensor([64, 3, 256, 1034],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([64, 3, 256, 1034],"float32"), 2, ) 	 50823168 	 27052 	 9.987309694290161 	 8.055006742477417 	 0.3774447441101074 	 0.30437588691711426 	 12.239977598190308 	 28.475229740142822 	 0.4624152183532715 	 0.35862040519714355 	 
2025-08-04 11:43:29.512126 test begin: paddle.Tensor.__pow__(Tensor([8, 110, 241, 241],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([8, 110, 241, 241],"float32"), 2, ) 	 51111280 	 27052 	 10.041657209396362 	 8.112842559814453 	 0.3794248104095459 	 0.30611324310302734 	 12.30703330039978 	 28.724364519119263 	 0.4650099277496338 	 0.2715275287628174 	 
2025-08-04 11:44:31.020817 test begin: paddle.Tensor.__pow__(Tensor([8, 32, 241, 824],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([8, 32, 241, 824],"float32"), 2, ) 	 50837504 	 27052 	 10.735406637191772 	 8.057509422302246 	 0.3772609233856201 	 0.3043704032897949 	 12.246307849884033 	 28.48313593864441 	 0.4623420238494873 	 0.3587052822113037 	 
2025-08-04 11:45:34.242556 test begin: paddle.Tensor.__pow__(Tensor([8, 32, 824, 241],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([8, 32, 824, 241],"float32"), 2, ) 	 50837504 	 27052 	 10.005162477493286 	 8.057517290115356 	 0.3787407875061035 	 0.30440545082092285 	 12.237918853759766 	 28.483845949172974 	 0.462191104888916 	 0.3587319850921631 	 
2025-08-04 11:46:38.826084 test begin: paddle.Tensor.__radd__(Tensor([192, 104, 32, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 104, 32, 160],"float16"), 0, ) 	 102236160 	 33511 	 11.150534629821777 	 9.996813774108887 	 0.3066565990447998 	 0.30452489852905273 	 10.04672122001648 	 1.6426000595092773 	 0.30640196800231934 	 7.605552673339844e-05 	 
2025-08-04 11:47:16.416162 test begin: paddle.Tensor.__radd__(Tensor([192, 128, 16, 259],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 128, 16, 259],"float16"), 0, ) 	 101842944 	 33511 	 10.012806415557861 	 12.233330965042114 	 0.30532050132751465 	 0.3032646179199219 	 10.013027906417847 	 2.3187971115112305 	 0.3053617477416992 	 0.00018787384033203125 	 
2025-08-04 11:47:56.126685 test begin: paddle.Tensor.__radd__(Tensor([192, 128, 26, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 128, 26, 160],"float16"), 0, ) 	 102236160 	 33511 	 10.054432153701782 	 9.983733177185059 	 0.30666112899780273 	 0.30446624755859375 	 10.046621084213257 	 2.122290849685669 	 0.3063817024230957 	 0.0001633167266845703 	 
2025-08-04 11:48:33.773331 test begin: paddle.Tensor.__radd__(Tensor([192, 207, 16, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 207, 16, 160],"float16"), 0, ) 	 101744640 	 33511 	 10.551411867141724 	 9.936670780181885 	 0.30507659912109375 	 0.30297112464904785 	 10.000872135162354 	 1.6565263271331787 	 0.30499958992004395 	 7.104873657226562e-05 	 
2025-08-04 11:49:10.645063 test begin: paddle.Tensor.__radd__(Tensor([192, 240, 16, 138],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 240, 16, 138],"float16"), 0, ) 	 101744640 	 33511 	 10.00339674949646 	 9.948346614837646 	 0.305178165435791 	 0.3029766082763672 	 10.00063443183899 	 2.3108158111572266 	 0.30500125885009766 	 7.963180541992188e-05 	 
2025-08-04 11:49:46.879055 test begin: paddle.Tensor.__radd__(Tensor([192, 240, 28, 80],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 240, 28, 80],"float16"), 0, ) 	 103219200 	 33511 	 10.143288612365723 	 10.074298858642578 	 0.30933380126953125 	 0.3073127269744873 	 10.142423152923584 	 1.8345236778259277 	 0.30933308601379395 	 8.702278137207031e-05 	 
2025-08-04 11:50:23.102305 test begin: paddle.Tensor.__radd__(Tensor([192, 414, 16, 80],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 414, 16, 80],"float16"), 0, ) 	 101744640 	 33511 	 11.210534572601318 	 9.934375286102295 	 0.3050966262817383 	 0.3029665946960449 	 10.00033187866211 	 1.6745095252990723 	 0.3049917221069336 	 9.012222290039062e-05 	 
2025-08-04 11:51:00.893930 test begin: paddle.Tensor.__radd__(Tensor([192, 64, 32, 259],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 64, 32, 259],"float16"), 0, ) 	 101842944 	 33511 	 10.011905431747437 	 9.943992376327515 	 0.3053562641143799 	 0.3032190799713135 	 10.012612581253052 	 1.7877869606018066 	 0.3053419589996338 	 0.0004150867462158203 	 
2025-08-04 11:51:37.024713 test begin: paddle.Tensor.__radd__(Tensor([192, 64, 52, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 64, 52, 160],"float16"), 0, ) 	 102236160 	 33511 	 11.16050386428833 	 9.983300685882568 	 0.3066079616546631 	 0.3044395446777344 	 10.045929193496704 	 1.6190521717071533 	 0.30637168884277344 	 0.00016880035400390625 	 
2025-08-04 11:52:14.214718 test begin: paddle.Tensor.__radd__(Tensor([311, 128, 16, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([311, 128, 16, 160],"float16"), 0, ) 	 101908480 	 33511 	 10.018047332763672 	 9.959954977035522 	 0.30550575256347656 	 0.3034634590148926 	 10.01792573928833 	 2.133742570877075 	 0.3054850101470947 	 0.0002474784851074219 	 
2025-08-04 11:52:54.267229 test begin: paddle.Tensor.__radd__(Tensor([311, 64, 32, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([311, 64, 32, 160],"float16"), 0, ) 	 101908480 	 33511 	 10.018053770065308 	 9.952805042266846 	 0.3055412769317627 	 0.3034353256225586 	 10.01772165298462 	 1.7733180522918701 	 0.3054850101470947 	 7.462501525878906e-05 	 
2025-08-04 11:53:29.926354 test begin: paddle.Tensor.__radd__(Tensor([331, 240, 16, 80],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([331, 240, 16, 80],"float16"), 0, ) 	 101683200 	 33511 	 9.996386528015137 	 9.927708148956299 	 0.30486536026000977 	 0.3027920722961426 	 9.994282722473145 	 1.7246651649475098 	 0.30480074882507324 	 6.866455078125e-05 	 
2025-08-04 11:54:05.389178 test begin: paddle.Tensor.__rlshift__(Tensor([169345, 300],"int32"), -223, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([169345, 300],"int32"), -223, ) 	 50803500 	 33460 	 9.995761394500732 	 9.960502862930298 	 0.15262389183044434 	 0.30420351028442383 	 None 	 None 	 None 	 None 	 
2025-08-04 11:54:25.991665 test begin: paddle.Tensor.__rlshift__(Tensor([200, 254017],"int32"), -223, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([200, 254017],"int32"), -223, ) 	 50803400 	 33460 	 9.998491048812866 	 10.582195520401001 	 0.15271234512329102 	 0.30428147315979004 	 None 	 None 	 None 	 None 	 
2025-08-04 11:54:48.988065 test begin: paddle.Tensor.__rlshift__(Tensor([200, 508033],"int16"), -212, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([200, 508033],"int16"), -212, ) 	 101606600 	 33460 	 11.26152491569519 	 9.864206314086914 	 0.00029206275939941406 	 0.3013124465942383 	 None 	 None 	 None 	 None 	 
2025-08-04 11:55:14.705808 test begin: paddle.Tensor.__rlshift__(Tensor([200, 508033],"int16"), 63, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([200, 508033],"int16"), 63, ) 	 101606600 	 33460 	 11.27079153060913 	 9.86427092552185 	 0.0002961158752441406 	 0.30132055282592773 	 None 	 None 	 None 	 None 	 
2025-08-04 11:55:39.362266 test begin: paddle.Tensor.__rlshift__(Tensor([338689, 300],"int16"), -212, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([338689, 300],"int16"), -212, ) 	 101606700 	 33460 	 12.24988603591919 	 9.86414885520935 	 0.0002856254577636719 	 0.30132555961608887 	 None 	 None 	 None 	 None 	 
2025-08-04 11:56:03.385343 test begin: paddle.Tensor.__rlshift__(Tensor([338689, 300],"int16"), 63, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([338689, 300],"int16"), 63, ) 	 101606700 	 33460 	 11.250309467315674 	 10.704218864440918 	 0.0002989768981933594 	 0.3012690544128418 	 None 	 None 	 None 	 None 	 
2025-08-04 11:56:28.547870 test begin: paddle.Tensor.__rmatmul__(Tensor([10160641, 5],"float32"), Tensor([2, 10160641],"float32"), )
[Prof] paddle.Tensor.__rmatmul__ 	 paddle.Tensor.__rmatmul__(Tensor([10160641, 5],"float32"), Tensor([2, 10160641],"float32"), ) 	 71124487 	 13088 	 18.149167776107788 	 18.11073589324951 	 0.7086212635040283 	 0.7071163654327393 	 27.436131954193115 	 27.42170476913452 	 0.10704708099365234 	 0.10699152946472168 	 
2025-08-04 11:58:01.094094 test begin: paddle.Tensor.__rmatmul__(Tensor([25401601, 5],"float32"), Tensor([2, 25401601],"float32"), )
[Prof] paddle.Tensor.__rmatmul__ 	 paddle.Tensor.__rmatmul__(Tensor([25401601, 5],"float32"), Tensor([2, 25401601],"float32"), ) 	 177811207 	 13088 	 44.51379871368408 	 44.422144174575806 	 1.7380433082580566 	 1.7342612743377686 	 68.57516765594482 	 68.56413984298706 	 0.10706591606140137 	 0.10702776908874512 	 
2025-08-04 12:01:50.840936 test begin: paddle.Tensor.__rmatmul__(Tensor([3, 16934401],"float32"), Tensor([2, 3],"float32"), )
[Prof] paddle.Tensor.__rmatmul__ 	 paddle.Tensor.__rmatmul__(Tensor([3, 16934401],"float32"), Tensor([2, 3],"float32"), ) 	 50803209 	 13088 	 10.001237392425537 	 10.025527000427246 	 0.0458989143371582 	 0.0460202693939209 	 53.67740845680237 	 53.63619017601013 	 0.21886396408081055 	 0.22085046768188477 	 
2025-08-04 12:03:59.746024 test begin: paddle.Tensor.__rmatmul__(Tensor([3, 5],"float32"), Tensor([16934401, 3],"float32"), )
[Prof] paddle.Tensor.__rmatmul__ 	 paddle.Tensor.__rmatmul__(Tensor([3, 5],"float32"), Tensor([16934401, 3],"float32"), ) 	 50803218 	 13088 	 23.37462615966797 	 23.377808094024658 	 0.10727548599243164 	 0.1073145866394043 	 54.79450845718384 	 54.76093029975891 	 0.22690081596374512 	 0.22333240509033203 	 
2025-08-04 12:06:39.603302 test begin: paddle.Tensor.__rmod__(Tensor([2, 3, 8467201],"float32"), Tensor([2, 3, 8467201],"float32"), )
[Prof] paddle.Tensor.__rmod__ 	 paddle.Tensor.__rmod__(Tensor([2, 3, 8467201],"float32"), Tensor([2, 3, 8467201],"float32"), ) 	 101606412 	 22183 	 10.658523559570312 	 9.961631059646606 	 0.46045398712158203 	 0.4589669704437256 	 24.750608205795288 	 26.552156925201416 	 1.140167236328125 	 0.4077448844909668 	 
2025-08-04 12:07:58.932008 test begin: paddle.Tensor.__rmod__(Tensor([2, 6350401, 4],"float32"), Tensor([2, 6350401, 4],"float32"), )
[Prof] paddle.Tensor.__rmod__ 	 paddle.Tensor.__rmod__(Tensor([2, 6350401, 4],"float32"), Tensor([2, 6350401, 4],"float32"), ) 	 101606416 	 22183 	 10.01633358001709 	 10.440329551696777 	 0.46080946922302246 	 0.4589517116546631 	 24.749202728271484 	 26.552486419677734 	 1.1401734352111816 	 0.4076852798461914 	 
2025-08-04 12:09:16.572210 test begin: paddle.Tensor.__rmod__(Tensor([4233601, 3, 4],"float32"), Tensor([4233601, 3, 4],"float32"), )
[Prof] paddle.Tensor.__rmod__ 	 paddle.Tensor.__rmod__(Tensor([4233601, 3, 4],"float32"), Tensor([4233601, 3, 4],"float32"), ) 	 101606424 	 22183 	 9.997641324996948 	 9.97044038772583 	 0.46060705184936523 	 0.45899152755737305 	 24.748343467712402 	 26.55220317840576 	 1.1401758193969727 	 0.4077122211456299 	 
2025-08-04 12:10:30.501675 test begin: paddle.Tensor.__rmul__(Tensor([176, 392, 737],"float32"), -100.0, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([176, 392, 737],"float32"), -100.0, ) 	 50847104 	 33795 	 10.013201713562012 	 10.072680473327637 	 0.3027501106262207 	 0.30455684661865234 	 10.012508392333984 	 10.065853834152222 	 0.302753210067749 	 0.3044397830963135 	 
2025-08-04 12:11:14.822081 test begin: paddle.Tensor.__rmul__(Tensor([176, 737, 392],"float32"), -100.0, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([176, 737, 392],"float32"), -100.0, ) 	 50847104 	 33795 	 10.014461517333984 	 11.264791488647461 	 0.30283188819885254 	 0.30447959899902344 	 10.015608072280884 	 10.065736770629883 	 0.30283236503601074 	 0.30441832542419434 	 
2025-08-04 12:11:58.959490 test begin: paddle.Tensor.__rmul__(Tensor([324000, 157],"float32"), 0.75, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([324000, 157],"float32"), 0.75, ) 	 50868000 	 33795 	 10.02006483078003 	 10.07478666305542 	 0.30300402641296387 	 0.3046543598175049 	 10.02031683921814 	 10.07146954536438 	 0.30304884910583496 	 0.3045535087585449 	 
2025-08-04 12:12:43.709726 test begin: paddle.Tensor.__rmul__(Tensor([324000, 157],"float32"), 1.0, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([324000, 157],"float32"), 1.0, ) 	 50868000 	 33795 	 10.034776449203491 	 10.074883460998535 	 0.3029909133911133 	 0.30465149879455566 	 10.020063638687134 	 10.071552753448486 	 0.3030283451080322 	 0.30457019805908203 	 
2025-08-04 12:13:25.669642 test begin: paddle.Tensor.__rmul__(Tensor([331, 392, 392],"float32"), -100.0, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([331, 392, 392],"float32"), -100.0, ) 	 50862784 	 33795 	 10.001438617706299 	 10.097712278366089 	 0.30242133140563965 	 0.30460667610168457 	 10.018975973129272 	 10.070476770401001 	 0.302936315536499 	 0.304520845413208 	 
2025-08-04 12:14:08.420116 test begin: paddle.Tensor.__rmul__(Tensor([635041, 80],"float32"), 0.75, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([635041, 80],"float32"), 0.75, ) 	 50803280 	 33795 	 9.993648052215576 	 10.060538053512573 	 0.3022315502166748 	 0.30422377586364746 	 10.00364875793457 	 10.05740737915039 	 0.30251622200012207 	 0.30419182777404785 	 
2025-08-04 12:14:50.275585 test begin: paddle.Tensor.__rmul__(Tensor([635041, 80],"float32"), 1.0, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([635041, 80],"float32"), 1.0, ) 	 50803280 	 33795 	 9.993382453918457 	 10.076933145523071 	 0.3022439479827881 	 0.30423593521118164 	 10.005424499511719 	 10.05738091468811 	 0.3025822639465332 	 0.3041684627532959 	 
2025-08-04 12:15:33.095108 test begin: paddle.Tensor.__ror__(Tensor([2, 3, 8467201],"int32"), 5, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([2, 3, 8467201],"int32"), 5, ) 	 50803206 	 33465 	 9.994232177734375 	 9.979275941848755 	 0.15262198448181152 	 0.30416226387023926 	 None 	 None 	 None 	 None 	 
2025-08-04 12:15:54.390080 test begin: paddle.Tensor.__ror__(Tensor([2, 3, 8467201],"int32"), True, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([2, 3, 8467201],"int32"), True, ) 	 50803206 	 33465 	 9.997715950012207 	 9.972788095474243 	 0.1526036262512207 	 0.3041374683380127 	 None 	 None 	 None 	 None 	 
2025-08-04 12:16:16.721512 test begin: paddle.Tensor.__ror__(Tensor([2, 5080321, 5],"int32"), 5, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([2, 5080321, 5],"int32"), 5, ) 	 50803210 	 33465 	 9.995066165924072 	 9.960566997528076 	 0.15262818336486816 	 0.30418872833251953 	 None 	 None 	 None 	 None 	 
2025-08-04 12:16:38.581237 test begin: paddle.Tensor.__ror__(Tensor([2, 5080321, 5],"int32"), True, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([2, 5080321, 5],"int32"), True, ) 	 50803210 	 33465 	 11.303565263748169 	 9.960467338562012 	 0.1525876522064209 	 0.30416440963745117 	 None 	 None 	 None 	 None 	 
2025-08-04 12:17:01.491185 test begin: paddle.Tensor.__ror__(Tensor([3386881, 3, 5],"int32"), 5, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([3386881, 3, 5],"int32"), 5, ) 	 50803215 	 33465 	 9.994896411895752 	 9.968975067138672 	 0.15262436866760254 	 0.30420351028442383 	 None 	 None 	 None 	 None 	 
2025-08-04 12:17:22.098228 test begin: paddle.Tensor.__ror__(Tensor([3386881, 3, 5],"int32"), True, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([3386881, 3, 5],"int32"), True, ) 	 50803215 	 33465 	 9.994740962982178 	 9.960520505905151 	 0.15258049964904785 	 0.304166316986084 	 None 	 None 	 None 	 None 	 
2025-08-04 12:17:43.461083 test begin: paddle.Tensor.__rpow__(Tensor([50803201],"float32"), 10000, )
[Prof] paddle.Tensor.__rpow__ 	 paddle.Tensor.__rpow__(Tensor([50803201],"float32"), 10000, ) 	 50803201 	 17155 	 10.018654346466064 	 10.815157651901245 	 0.29785609245300293 	 0.3221616744995117 	 12.020089149475098 	 12.745299816131592 	 0.7160840034484863 	 0.37968897819519043 	 
2025-08-04 12:18:30.914874 test begin: paddle.Tensor.__rpow__(Tensor([50803201],"float32"), 10000.0, )
[Prof] paddle.Tensor.__rpow__ 	 paddle.Tensor.__rpow__(Tensor([50803201],"float32"), 10000.0, ) 	 50803201 	 17155 	 10.00428557395935 	 11.057324647903442 	 0.2979259490966797 	 0.32917094230651855 	 12.014480829238892 	 12.745028257369995 	 0.7157421112060547 	 0.37961769104003906 	 
2025-08-04 12:19:18.554709 test begin: paddle.Tensor.__rrshift__(Tensor([169345, 300],"int32"), 232, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([169345, 300],"int32"), 232, ) 	 50803500 	 33481 	 9.995172023773193 	 9.96470832824707 	 0.15252423286437988 	 0.3041868209838867 	 None 	 None 	 None 	 None 	 
2025-08-04 12:19:39.551738 test begin: paddle.Tensor.__rrshift__(Tensor([200, 254017],"int32"), 232, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([200, 254017],"int32"), 232, ) 	 50803400 	 33481 	 10.001842737197876 	 9.964850425720215 	 0.1524665355682373 	 0.30419921875 	 None 	 None 	 None 	 None 	 
2025-08-04 12:20:00.169545 test begin: paddle.Tensor.__rrshift__(Tensor([200, 508033],"int16"), -255, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([200, 508033],"int16"), -255, ) 	 101606600 	 33481 	 11.408441066741943 	 9.905740976333618 	 0.0002994537353515625 	 0.3023707866668701 	 None 	 None 	 None 	 None 	 
2025-08-04 12:20:22.685170 test begin: paddle.Tensor.__rrshift__(Tensor([200, 508033],"int16"), 11, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([200, 508033],"int16"), 11, ) 	 101606600 	 33481 	 11.264836072921753 	 9.911409854888916 	 0.0002956390380859375 	 0.30234479904174805 	 None 	 None 	 None 	 None 	 
2025-08-04 12:20:47.387856 test begin: paddle.Tensor.__rrshift__(Tensor([338689, 300],"int16"), -255, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([338689, 300],"int16"), -255, ) 	 101606700 	 33481 	 11.246416568756104 	 9.905986070632935 	 0.0002989768981933594 	 0.30238771438598633 	 None 	 None 	 None 	 None 	 
2025-08-04 12:21:09.628590 test begin: paddle.Tensor.__rrshift__(Tensor([338689, 300],"int16"), 11, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([338689, 300],"int16"), 11, ) 	 101606700 	 33481 	 12.588147640228271 	 9.905761957168579 	 0.0002987384796142578 	 0.3023819923400879 	 None 	 None 	 None 	 None 	 
2025-08-04 12:21:33.758836 test begin: paddle.Tensor.__rshift__(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), ) 	 101607000 	 22342 	 10.064938068389893 	 9.981225490570068 	 0.46048951148986816 	 0.45638275146484375 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:21:55.529825 test begin: paddle.Tensor.__rshift__(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), ) 	 101606800 	 22342 	 11.804809093475342 	 9.977865219116211 	 0.46025919914245605 	 0.4564237594604492 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:22:19.639519 test begin: paddle.Tensor.__rshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), ) 	 203213200 	 22342 	 9.998040914535522 	 10.05402421951294 	 0.4573550224304199 	 0.4599273204803467 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:22:42.530489 test begin: paddle.Tensor.__rshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, ) 	 203213200 	 22342 	 10.012317657470703 	 69.56282925605774 	 0.4579145908355713 	 0.35364222526550293 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:24:04.309182 test begin: paddle.Tensor.__rshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), ) 	 203213400 	 22342 	 10.000705480575562 	 10.05785322189331 	 0.4574851989746094 	 0.45987939834594727 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:24:27.631107 test begin: paddle.Tensor.__rshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, ) 	 203213400 	 22342 	 10.023316383361816 	 69.58539128303528 	 0.4586200714111328 	 0.3536417484283447 	 None 	 None 	 None 	 None 	 combined
2025-08-04 12:25:52.226318 test begin: paddle.Tensor.__rsub__(Tensor([2, 1, 12404, 4096],"float16"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([2, 1, 12404, 4096],"float16"), 1, ) 	 101613568 	 33801 	 10.081378698348999 	 10.009693145751953 	 0.3047335147857666 	 0.3026084899902344 	 10.077795505523682 	 10.006856918334961 	 0.3046853542327881 	 0.302548885345459 	 
2025-08-04 12:26:36.721546 test begin: paddle.Tensor.__rsub__(Tensor([2, 1, 4096, 12404],"float16"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([2, 1, 4096, 12404],"float16"), 1, ) 	 101613568 	 33801 	 11.583946228027344 	 10.009875059127808 	 0.304553747177124 	 0.30266642570495605 	 10.080845355987549 	 10.006696939468384 	 0.3047962188720703 	 0.30257272720336914 	 
2025-08-04 12:27:22.899662 test begin: paddle.Tensor.__rsub__(Tensor([2, 4, 4096, 4096],"float16"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([2, 4, 4096, 4096],"float16"), 1, ) 	 134217728 	 33801 	 13.2610182762146 	 13.181516885757446 	 0.40099143981933594 	 0.3984215259552002 	 13.261441707611084 	 13.181072473526001 	 0.4009582996368408 	 0.3984544277191162 	 
2025-08-04 12:28:21.274298 test begin: paddle.Tensor.__rsub__(Tensor([2944, 17257],"float32"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([2944, 17257],"float32"), 1, ) 	 50804608 	 33801 	 9.996896028518677 	 10.081048727035522 	 0.3021845817565918 	 0.3042421340942383 	 10.00597333908081 	 10.059610366821289 	 0.30253052711486816 	 0.30415844917297363 	 
2025-08-04 12:29:03.166447 test begin: paddle.Tensor.__rsub__(Tensor([4224, 12028],"float32"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([4224, 12028],"float32"), 1, ) 	 50806272 	 33801 	 10.00660252571106 	 10.063009023666382 	 0.30243682861328125 	 0.30423784255981445 	 10.005828380584717 	 10.059754610061646 	 0.30251288414001465 	 0.30419111251831055 	 
2025-08-04 12:29:45.030546 test begin: paddle.Tensor.__rsub__(Tensor([7, 1, 4096, 4096],"float16"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([7, 1, 4096, 4096],"float16"), 1, ) 	 117440512 	 33801 	 11.614923000335693 	 11.543700695037842 	 0.35120296478271484 	 0.3490633964538574 	 11.616168737411499 	 11.542223930358887 	 0.3511967658996582 	 0.34900426864624023 	 
2025-08-04 12:30:38.092385 test begin: paddle.Tensor.__rsub__(Tensor([7664, 6629],"float32"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([7664, 6629],"float32"), 1, ) 	 50804656 	 33801 	 11.568089485168457 	 10.065671443939209 	 0.3023393154144287 	 0.3042612075805664 	 10.00585150718689 	 10.059513330459595 	 0.30252933502197266 	 0.3041684627532959 	 
2025-08-04 12:31:22.126204 test begin: paddle.Tensor.__rtruediv__(Tensor([15548, 3268],"float32"), 1.0, )
[Prof] paddle.Tensor.__rtruediv__ 	 paddle.Tensor.__rtruediv__(Tensor([15548, 3268],"float32"), 1.0, ) 	 50810864 	 17130 	 9.999288558959961 	 10.228131294250488 	 0.2983272075653076 	 0.30449867248535156 	 10.033777952194214 	 22.926506280899048 	 0.5986306667327881 	 0.34204602241516113 	 
2025-08-04 12:32:17.157634 test begin: paddle.Tensor.__rtruediv__(Tensor([16773, 3029],"float32"), 1.0, )
[Prof] paddle.Tensor.__rtruediv__ 	 paddle.Tensor.__rtruediv__(Tensor([16773, 3029],"float32"), 1.0, ) 	 50805417 	 17130 	 10.006612062454224 	 10.215435266494751 	 0.2983415126800537 	 0.30452609062194824 	 10.032763004302979 	 22.92434787750244 	 0.598602294921875 	 0.34200310707092285 	 
2025-08-04 12:33:12.346346 test begin: paddle.Tensor.__rtruediv__(Tensor([26736, 1901],"float32"), 1.0, )
[Prof] paddle.Tensor.__rtruediv__ 	 paddle.Tensor.__rtruediv__(Tensor([26736, 1901],"float32"), 1.0, ) 	 50825136 	 17130 	 10.00508713722229 	 10.225632905960083 	 0.2984611988067627 	 0.3045649528503418 	 10.036279439926147 	 22.9325430393219 	 0.598783016204834 	 0.3420841693878174 	 
2025-08-04 12:34:08.338099 test begin: paddle.Tensor.__rtruediv__(Tensor([37411, 1358],"float32"), 1.0, )
[Prof] paddle.Tensor.__rtruediv__ 	 paddle.Tensor.__rtruediv__(Tensor([37411, 1358],"float32"), 1.0, ) 	 50804138 	 17130 	 10.00133228302002 	 10.205111026763916 	 0.29831957817077637 	 0.30444836616516113 	 10.032275915145874 	 22.924875497817993 	 0.5984823703765869 	 0.34197306632995605 	 
2025-08-04 12:35:04.482152 test begin: paddle.Tensor.__rtruediv__(Tensor([6684, 7601],"float32"), 1.0, )
[Prof] paddle.Tensor.__rtruediv__ 	 paddle.Tensor.__rtruediv__(Tensor([6684, 7601],"float32"), 1.0, ) 	 50805084 	 17130 	 10.001912593841553 	 10.205479860305786 	 0.2982935905456543 	 0.30439114570617676 	 10.03048062324524 	 22.924732208251953 	 0.598419189453125 	 0.34198665618896484 	 
2025-08-04 12:36:02.777097 test begin: paddle.Tensor.__rxor__(Tensor([2, 3, 8467201],"int32"), 5, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([2, 3, 8467201],"int32"), 5, ) 	 50803206 	 33457 	 10.0006263256073 	 9.958155155181885 	 0.15256023406982422 	 0.3041403293609619 	 None 	 None 	 None 	 None 	 
2025-08-04 12:36:23.440040 test begin: paddle.Tensor.__rxor__(Tensor([2, 3, 8467201],"int32"), True, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([2, 3, 8467201],"int32"), True, ) 	 50803206 	 33457 	 9.99339771270752 	 9.973361492156982 	 0.15260648727416992 	 0.304201602935791 	 None 	 None 	 None 	 None 	 
2025-08-04 12:36:44.397411 test begin: paddle.Tensor.__rxor__(Tensor([2, 5080321, 5],"int32"), 5, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([2, 5080321, 5],"int32"), 5, ) 	 50803210 	 33457 	 9.998796939849854 	 9.958233118057251 	 0.15265917778015137 	 0.3041970729827881 	 None 	 None 	 None 	 None 	 
2025-08-04 12:37:06.760725 test begin: paddle.Tensor.__rxor__(Tensor([2, 5080321, 5],"int32"), True, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([2, 5080321, 5],"int32"), True, ) 	 50803210 	 33457 	 9.999162435531616 	 9.95821475982666 	 0.15262937545776367 	 0.30420899391174316 	 None 	 None 	 None 	 None 	 
2025-08-04 12:37:27.347685 test begin: paddle.Tensor.__rxor__(Tensor([3386881, 3, 5],"int32"), 5, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([3386881, 3, 5],"int32"), 5, ) 	 50803215 	 33457 	 9.993042469024658 	 9.964223861694336 	 0.1526656150817871 	 0.30418872833251953 	 None 	 None 	 None 	 None 	 
2025-08-04 12:37:48.356594 test begin: paddle.Tensor.__rxor__(Tensor([3386881, 3, 5],"int32"), True, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([3386881, 3, 5],"int32"), True, ) 	 50803215 	 33457 	 9.995980024337769 	 9.958218574523926 	 0.15259385108947754 	 0.3041543960571289 	 None 	 None 	 None 	 None 	 
2025-08-04 12:38:08.977397 test begin: paddle.Tensor.__sub__(Tensor([1, 1, 32768, 32768],"float16"), 1, )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([1, 1, 32768, 32768],"float16"), 1, ) 	 1073741824 	 33533 	 104.11320948600769 	 103.64527797698975 	 3.1730525493621826 	 3.1590349674224854 	 104.14380717277527 	 1.8804066181182861 	 3.174149990081787 	 7.43865966796875e-05 	 
2025-08-04 12:44:05.120408 test begin: paddle.Tensor.__sub__(Tensor([128, 192, 22, 96],"float32"), Tensor([128, 1, 22, 96],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([128, 192, 22, 96],"float32"), Tensor([128, 1, 22, 96],"float32"), ) 	 52174848 	 33533 	 10.190470695495605 	 10.589761734008789 	 0.3105647563934326 	 0.3226604461669922 	 17.963634252548218 	 15.274448871612549 	 0.2737431526184082 	 0.2327747344970703 	 
2025-08-04 12:45:00.917774 test begin: paddle.Tensor.__sub__(Tensor([128, 192, 96, 22],"float32"), Tensor([128, 1, 96, 22],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([128, 192, 96, 22],"float32"), Tensor([128, 1, 96, 22],"float32"), ) 	 52174848 	 33533 	 10.190653562545776 	 10.600332260131836 	 0.3105745315551758 	 0.32265520095825195 	 17.96484065055847 	 15.274257898330688 	 0.2737574577331543 	 0.23272466659545898 	 
2025-08-04 12:45:56.714418 test begin: paddle.Tensor.__sub__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 1, 96, 96],"float32"), ) 	 53084160 	 33533 	 10.250393867492676 	 10.699332475662231 	 0.3124251365661621 	 0.3260831832885742 	 16.290698528289795 	 15.497071743011475 	 0.24825382232666016 	 0.2360825538635254 	 
2025-08-04 12:46:51.305896 test begin: paddle.Tensor.__sub__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 44, 96, 96],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 44, 96, 96],"float32"), ) 	 103809024 	 33533 	 15.426634550094604 	 15.291664600372314 	 0.47029590606689453 	 0.46617794036865234 	 16.246933698654175 	 10.19426965713501 	 0.49515604972839355 	 0.3106961250305176 	 
2025-08-04 12:47:51.484563 test begin: paddle.Tensor.__sub__(Tensor([2, 1, 1551, 32768],"float16"), 1, )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([2, 1, 1551, 32768],"float16"), 1, ) 	 101646336 	 33533 	 10.000560760498047 	 9.930310726165771 	 0.3047311305999756 	 0.302645206451416 	 9.999214887619019 	 2.3987138271331787 	 0.3047506809234619 	 0.00014662742614746094 	 
2025-08-04 12:48:27.714526 test begin: paddle.Tensor.__sub__(Tensor([2, 1, 32768, 1551],"float16"), 1, )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([2, 1, 32768, 1551],"float16"), 1, ) 	 101646336 	 33533 	 9.999172449111938 	 9.930209159851074 	 0.3047454357147217 	 0.30261659622192383 	 9.99939751625061 	 1.8504223823547363 	 0.3047471046447754 	 6.890296936035156e-05 	 
2025-08-04 12:49:04.570645 test begin: paddle.Tensor.__sub__(Tensor([2, 1, 32768, 32768],"float16"), 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe59038bf10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 12:59:11.783497 test begin: paddle.Tensor.__sub__(Tensor([26736, 3029, 1],"float32"), Tensor([26736, 3029, 1],"float32"), )
W0804 12:59:14.686333 117739 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([26736, 3029, 1],"float32"), Tensor([26736, 3029, 1],"float32"), ) 	 161966688 	 33533 	 23.98344111442566 	 23.774462938308716 	 0.7308673858642578 	 0.7246503829956055 	 25.29086470603943 	 15.808464050292969 	 0.770806074142456 	 0.4817967414855957 	 
2025-08-04 13:00:48.711441 test begin: paddle.Tensor.__sub__(Tensor([26736, 3029, 1],"float32"), Tensor([26736, 3029, 2],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([26736, 3029, 1],"float32"), Tensor([26736, 3029, 2],"float32"), ) 	 242950032 	 33533 	 39.422385931015015 	 40.13677215576172 	 1.2015020847320557 	 1.2232699394226074 	 79.75887942314148 	 82.72047162055969 	 1.2153620719909668 	 1.2606604099273682 	 
2025-08-04 13:04:57.738434 test begin: paddle.Tensor.__sub__(Tensor([26736, 3029, 2],"float32"), Tensor([26736, 3029, 1],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([26736, 3029, 2],"float32"), Tensor([26736, 3029, 1],"float32"), ) 	 242950032 	 33533 	 39.403055906295776 	 40.13219857215881 	 1.2007896900177002 	 1.223123550415039 	 74.30331945419312 	 82.72666430473328 	 0.754652738571167 	 1.260479211807251 	 
2025-08-04 13:09:01.863708 test begin: paddle.Tensor.__sub__(Tensor([26736, 951, 2],"float32"), Tensor([26736, 951, 2],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([26736, 951, 2],"float32"), Tensor([26736, 951, 2],"float32"), ) 	 101703744 	 33533 	 15.118759632110596 	 15.019151449203491 	 0.4608449935913086 	 0.45674967765808105 	 15.915472745895386 	 9.994408369064331 	 0.48505640029907227 	 0.30460071563720703 	 
2025-08-04 13:10:02.888381 test begin: paddle.Tensor.__sub__(Tensor([29, 192, 96, 96],"float32"), Tensor([29, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([29, 192, 96, 96],"float32"), Tensor([29, 1, 96, 96],"float32"), ) 	 51581952 	 33533 	 10.059057235717773 	 10.457117319107056 	 0.30655527114868164 	 0.3186795711517334 	 16.045648336410522 	 15.09160041809082 	 0.24451518058776855 	 0.22999000549316406 	 
2025-08-04 13:10:56.303491 test begin: paddle.Tensor.__sub__(Tensor([8387, 3029, 2],"float32"), Tensor([8387, 3029, 2],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([8387, 3029, 2],"float32"), Tensor([8387, 3029, 2],"float32"), ) 	 101616892 	 33533 	 15.10535740852356 	 14.973984718322754 	 0.4603300094604492 	 0.4563310146331787 	 15.901124238967896 	 9.986125946044922 	 0.4847407341003418 	 0.3043344020843506 	 
2025-08-04 13:11:55.295240 test begin: paddle.Tensor.__truediv__(Tensor([124, 128, 34, 96],"float32"), Tensor([124, 1, 34, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([124, 128, 34, 96],"float32"), Tensor([124, 1, 34, 96],"float32"), ) 	 52210944 	 33318 	 10.14596939086914 	 10.79348611831665 	 0.3112308979034424 	 0.3308682441711426 	 27.065415143966675 	 62.84714889526367 	 0.41492724418640137 	 0.32110142707824707 	 
2025-08-04 13:13:48.008829 test begin: paddle.Tensor.__truediv__(Tensor([124, 128, 96, 34],"float32"), Tensor([124, 1, 96, 34],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([124, 128, 96, 34],"float32"), Tensor([124, 1, 96, 34],"float32"), ) 	 52210944 	 33318 	 10.128454685211182 	 10.792976379394531 	 0.3106541633605957 	 0.33109569549560547 	 27.039010286331177 	 62.87176990509033 	 0.4147045612335205 	 0.3213207721710205 	 
2025-08-04 13:15:42.397350 test begin: paddle.Tensor.__truediv__(Tensor([124, 45, 96, 96],"float32"), Tensor([124, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([124, 45, 96, 96],"float32"), Tensor([124, 1, 96, 96],"float32"), ) 	 52568064 	 33318 	 10.113085269927979 	 10.79421591758728 	 0.30999302864074707 	 0.33115410804748535 	 25.688700914382935 	 62.83516716957092 	 0.393979549407959 	 0.32109785079956055 	 
2025-08-04 13:17:33.591940 test begin: paddle.Tensor.__truediv__(Tensor([124, 45, 96, 96],"float32"), Tensor([124, 45, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([124, 45, 96, 96],"float32"), Tensor([124, 45, 96, 96],"float32"), ) 	 102850560 	 33318 	 15.967997789382935 	 15.157298803329468 	 0.46617579460144043 	 0.4649786949157715 	 38.38854765892029 	 70.53891730308533 	 1.1775200366973877 	 0.4328012466430664 	 
2025-08-04 13:19:57.598544 test begin: paddle.Tensor.__truediv__(Tensor([128, 128, 33, 96],"float32"), Tensor([128, 1, 33, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 128, 33, 96],"float32"), Tensor([128, 1, 33, 96],"float32"), ) 	 52310016 	 33318 	 10.156920909881592 	 10.830335140228271 	 0.31152772903442383 	 0.33158278465270996 	 27.087587356567383 	 62.998796701431274 	 0.41544175148010254 	 0.3219137191772461 	 
2025-08-04 13:21:52.610898 test begin: paddle.Tensor.__truediv__(Tensor([128, 128, 96, 33],"float32"), Tensor([128, 1, 96, 33],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 128, 96, 33],"float32"), Tensor([128, 1, 96, 33],"float32"), ) 	 52310016 	 33318 	 10.144137144088745 	 10.817105054855347 	 0.31114983558654785 	 0.3318057060241699 	 27.0985267162323 	 62.99404501914978 	 0.41559696197509766 	 0.32189440727233887 	 
2025-08-04 13:23:45.691852 test begin: paddle.Tensor.__truediv__(Tensor([128, 192, 22, 96],"float32"), Tensor([128, 1, 22, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 192, 22, 96],"float32"), Tensor([128, 1, 22, 96],"float32"), ) 	 52174848 	 33318 	 10.131118535995483 	 10.79423189163208 	 0.31077146530151367 	 0.3309195041656494 	 27.492429494857788 	 62.889055252075195 	 0.4216444492340088 	 0.32132482528686523 	 
2025-08-04 13:25:43.057021 test begin: paddle.Tensor.__truediv__(Tensor([128, 192, 96, 22],"float32"), Tensor([128, 1, 96, 22],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 192, 96, 22],"float32"), Tensor([128, 1, 96, 22],"float32"), ) 	 52174848 	 33318 	 10.142843246459961 	 10.797118902206421 	 0.3107757568359375 	 0.3309977054595947 	 27.49149227142334 	 62.88465762138367 	 0.4216475486755371 	 0.32135820388793945 	 
2025-08-04 13:27:38.684408 test begin: paddle.Tensor.__truediv__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 1, 96, 96],"float32"), ) 	 53084160 	 33318 	 11.598171710968018 	 10.89808440208435 	 0.31290507316589355 	 0.3339989185333252 	 25.985035181045532 	 63.34571838378906 	 0.3985159397125244 	 0.3237135410308838 	 
2025-08-04 13:29:32.698961 test begin: paddle.Tensor.__truediv__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 44, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 44, 96, 96],"float32"), ) 	 103809024 	 33318 	 15.335824489593506 	 15.297391891479492 	 0.4704310894012451 	 0.4692971706390381 	 38.74535012245178 	 71.19164609909058 	 1.1883652210235596 	 0.4368157386779785 	 
2025-08-04 13:31:57.352396 test begin: paddle.Tensor.__truediv__(Tensor([29, 192, 96, 96],"float32"), Tensor([29, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([29, 192, 96, 96],"float32"), Tensor([29, 1, 96, 96],"float32"), ) 	 51581952 	 33318 	 9.996351957321167 	 10.672587633132935 	 0.3065969944000244 	 0.32738375663757324 	 25.479394912719727 	 62.15625858306885 	 0.3907179832458496 	 0.3175668716430664 	 
2025-08-04 13:33:47.768354 test begin: paddle.Tensor.__truediv__(Tensor([44, 128, 96, 96],"float32"), Tensor([44, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([44, 128, 96, 96],"float32"), Tensor([44, 1, 96, 96],"float32"), ) 	 52310016 	 33318 	 10.12454605102539 	 10.80850601196289 	 0.31058359146118164 	 0.3313560485839844 	 25.847644567489624 	 62.9285888671875 	 0.39643049240112305 	 0.3215641975402832 	 
2025-08-04 13:35:43.241764 test begin: paddle.Tensor.__xor__(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 85302 	 38.26175665855408 	 38.39537334442139 	 0.45867371559143066 	 0.4600512981414795 	 None 	 None 	 None 	 None 	 
2025-08-04 13:37:01.966997 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 85302 	 38.28911352157593 	 38.40981698036194 	 0.45813560485839844 	 0.4600677490234375 	 None 	 None 	 None 	 None 	 
2025-08-04 13:38:20.957376 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), ) 	 203214240 	 85302 	 38.247615814208984 	 38.396971702575684 	 0.4581184387207031 	 0.4599575996398926 	 None 	 None 	 None 	 None 	 
2025-08-04 13:39:41.537890 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), ) 	 203213880 	 85302 	 38.279648780822754 	 38.39508008956909 	 0.4579904079437256 	 0.4600105285644531 	 None 	 None 	 None 	 None 	 
2025-08-04 13:41:00.308809 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), ) 	 101607264 	 85302 	 10.069021224975586 	 9.993911981582642 	 0.12060689926147461 	 0.11960244178771973 	 None 	 None 	 None 	 None 	 
2025-08-04 13:41:21.946644 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), ) 	 101607264 	 85302 	 38.42469763755798 	 38.09208965301514 	 0.46035003662109375 	 0.4564077854156494 	 None 	 None 	 None 	 None 	 
2025-08-04 13:42:41.755886 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), ) 	 203213664 	 85302 	 38.26268935203552 	 38.39670133590698 	 0.45856785774230957 	 0.46002793312072754 	 None 	 None 	 None 	 None 	 
2025-08-04 13:44:00.498120 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 50807520 	 85302 	 14.355289220809937 	 19.380308151245117 	 0.17197775840759277 	 0.23217368125915527 	 None 	 None 	 None 	 None 	 
2025-08-04 13:44:35.080852 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 101610720 	 85302 	 27.054916381835938 	 40.87677073478699 	 0.3214290142059326 	 0.48972010612487793 	 None 	 None 	 None 	 None 	 
2025-08-04 13:45:45.591334 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 50807520 	 85302 	 25.290307760238647 	 26.279605865478516 	 0.3030245304107666 	 0.31481146812438965 	 None 	 None 	 None 	 None 	 
2025-08-04 13:46:38.034493 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), ) 	 101608560 	 85302 	 10.075954675674438 	 9.892405986785889 	 0.1206200122833252 	 0.11850380897521973 	 None 	 None 	 None 	 None 	 
2025-08-04 13:47:02.927809 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), ) 	 101608560 	 85302 	 38.42454171180725 	 38.09021067619324 	 0.4602839946746826 	 0.4563283920288086 	 None 	 None 	 None 	 None 	 
2025-08-04 13:48:23.544663 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), ) 	 203214960 	 85302 	 38.297425985336304 	 38.400081396102905 	 0.4580562114715576 	 0.4600701332092285 	 None 	 None 	 None 	 None 	 
2025-08-04 13:49:42.470805 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 50807520 	 85302 	 15.096784830093384 	 19.370473384857178 	 0.18088841438293457 	 0.23209691047668457 	 None 	 None 	 None 	 None 	 
2025-08-04 13:50:17.652137 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 101610720 	 85302 	 10.051105976104736 	 9.920219421386719 	 0.12041497230529785 	 0.11857342720031738 	 None 	 None 	 None 	 None 	 
2025-08-04 13:50:39.398151 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50807520 	 85302 	 25.256848096847534 	 26.23033905029297 	 0.30251431465148926 	 0.31423139572143555 	 None 	 None 	 None 	 None 	 
2025-08-04 13:51:37.833518 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 101610720 	 85302 	 39.20384216308594 	 38.08896446228027 	 0.46035122871398926 	 0.4563577175140381 	 None 	 None 	 None 	 None 	 
2025-08-04 13:52:59.226096 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101610720 	 85302 	 29.430186986923218 	 40.76527190208435 	 0.3525826930999756 	 0.4884061813354492 	 None 	 None 	 None 	 None 	 
2025-08-04 13:54:10.464868 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 203217120 	 85302 	 38.268800020217896 	 38.397632360458374 	 0.4586207866668701 	 0.46004295349121094 	 None 	 None 	 None 	 None 	 
2025-08-04 13:55:29.183851 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), ) 	 101607480 	 85302 	 10.055484056472778 	 9.839309930801392 	 0.12046694755554199 	 0.11778807640075684 	 None 	 None 	 None 	 None 	 
2025-08-04 13:55:50.723726 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), ) 	 101607480 	 85302 	 38.42354226112366 	 38.08905482292175 	 0.46039915084838867 	 0.4562854766845703 	 None 	 None 	 None 	 None 	 
2025-08-04 13:57:08.751052 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), ) 	 101607840 	 85302 	 10.03705096244812 	 9.83911681175232 	 0.12030696868896484 	 0.1177833080291748 	 None 	 None 	 None 	 None 	 
2025-08-04 13:57:31.084005 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), ) 	 101607840 	 85302 	 38.42138934135437 	 38.08753538131714 	 0.46023058891296387 	 0.4562711715698242 	 None 	 None 	 None 	 None 	 
2025-08-04 13:58:48.887705 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 85302 	 10.049814701080322 	 9.838549375534058 	 0.12041521072387695 	 0.11788082122802734 	 None 	 None 	 None 	 None 	 
2025-08-04 13:59:10.186099 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 85302 	 38.42064619064331 	 38.091087102890015 	 0.46013641357421875 	 0.4563894271850586 	 None 	 None 	 None 	 None 	 
2025-08-04 14:00:27.953685 test begin: paddle.Tensor.__xor__(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 85302 	 10.050307989120483 	 10.599989175796509 	 0.12038397789001465 	 0.11787223815917969 	 None 	 None 	 None 	 None 	 
2025-08-04 14:00:51.590981 test begin: paddle.Tensor.__xor__(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 85302 	 38.42010545730591 	 38.087995529174805 	 0.4603283405303955 	 0.4563181400299072 	 None 	 None 	 None 	 None 	 
2025-08-04 14:02:10.612264 test begin: paddle.Tensor.__xor__(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101608560 	 85302 	 10.051466464996338 	 9.891648769378662 	 0.12041091918945312 	 0.11848068237304688 	 None 	 None 	 None 	 None 	 
2025-08-04 14:02:32.061311 test begin: paddle.Tensor.__xor__(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101608560 	 85302 	 38.42314291000366 	 38.09772610664368 	 0.4603574275970459 	 0.45633840560913086 	 None 	 None 	 None 	 None 	 
2025-08-04 14:03:51.766262 test begin: paddle.Tensor.__xor__(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214960 	 85302 	 38.285027503967285 	 38.39747953414917 	 0.4590017795562744 	 0.46010732650756836 	 None 	 None 	 None 	 None 	 
2025-08-04 14:05:10.494932 test begin: paddle.Tensor.abs(Tensor([243360, 209],"float32"), )
[Prof] paddle.Tensor.abs 	 paddle.Tensor.abs(Tensor([243360, 209],"float32"), ) 	 50862240 	 33826 	 10.025699853897095 	 10.094278812408447 	 0.30290722846984863 	 0.30454325675964355 	 15.246658086776733 	 25.156994819641113 	 0.4606771469116211 	 0.3800535202026367 	 
2025-08-04 14:06:14.374265 test begin: paddle.Tensor.abs(Tensor([282240, 181],"float32"), )
[Prof] paddle.Tensor.abs 	 paddle.Tensor.abs(Tensor([282240, 181],"float32"), ) 	 51085440 	 33826 	 10.069350957870483 	 10.130455017089844 	 0.30425047874450684 	 0.3058278560638428 	 15.308499336242676 	 25.266515970230103 	 0.4626486301422119 	 0.38167285919189453 	 
2025-08-04 14:07:18.332275 test begin: paddle.Tensor.abs(Tensor([324000, 157],"float32"), )
[Prof] paddle.Tensor.abs 	 paddle.Tensor.abs(Tensor([324000, 157],"float32"), ) 	 50868000 	 33826 	 10.017380714416504 	 10.0812087059021 	 0.30264997482299805 	 0.30454158782958984 	 15.25162410736084 	 25.160783767700195 	 0.460679292678833 	 0.38010573387145996 	 
2025-08-04 14:08:20.579330 test begin: paddle.Tensor.abs(Tensor([635041, 80],"float32"), )
[Prof] paddle.Tensor.abs 	 paddle.Tensor.abs(Tensor([635041, 80],"float32"), ) 	 50803280 	 33826 	 10.008760690689087 	 10.072479009628296 	 0.30241847038269043 	 0.304140567779541 	 15.22948956489563 	 25.126629114151 	 0.45986008644104004 	 0.3796114921569824 	 
2025-08-04 14:09:22.757859 test begin: paddle.Tensor.add(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.add 	 paddle.Tensor.add(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 22210 	 10.003230333328247 	 9.916230201721191 	 0.4603114128112793 	 0.4562671184539795 	 10.723815202713013 	 1.50984525680542 	 0.49337220191955566 	 8.845329284667969e-05 	 
2025-08-04 14:09:57.606469 test begin: paddle.Tensor.all(Tensor([10, 1, 2048, 24807],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([10, 1, 2048, 24807],"bool"), ) 	 508047360 	 21540 	 10.013031482696533 	 10.940950155258179 	 0.23753952980041504 	 0.25953078269958496 	 None 	 None 	 None 	 None 	 
2025-08-04 14:10:27.124522 test begin: paddle.Tensor.all(Tensor([10, 1, 24807, 2048],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([10, 1, 24807, 2048],"bool"), ) 	 508047360 	 21540 	 10.01200246810913 	 10.945538520812988 	 0.23751497268676758 	 0.25963568687438965 	 None 	 None 	 None 	 None 	 
2025-08-04 14:10:55.300891 test begin: paddle.Tensor.all(Tensor([10, 13, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([10, 13, 2048, 2048],"bool"), ) 	 545259520 	 21540 	 10.732803583145142 	 11.745560646057129 	 0.25463080406188965 	 0.27860355377197266 	 None 	 None 	 None 	 None 	 
2025-08-04 14:11:27.332111 test begin: paddle.Tensor.all(Tensor([130, 1, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([130, 1, 2048, 2048],"bool"), ) 	 545259520 	 21540 	 10.73276400566101 	 11.746014833450317 	 0.25461792945861816 	 0.2786440849304199 	 None 	 None 	 None 	 None 	 
2025-08-04 14:12:03.127190 test begin: paddle.Tensor.all(Tensor([1590, 10, 32000],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([1590, 10, 32000],"bool"), ) 	 508800000 	 21540 	 10.0548996925354 	 10.956321954727173 	 0.23851227760314941 	 0.25989460945129395 	 None 	 None 	 None 	 None 	 
2025-08-04 14:12:31.521279 test begin: paddle.Tensor.all(Tensor([20, 10, 2540161],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([20, 10, 2540161],"bool"), ) 	 508032200 	 21540 	 10.082818508148193 	 10.935272216796875 	 0.2391667366027832 	 0.2594153881072998 	 None 	 None 	 None 	 None 	 
2025-08-04 14:12:59.830265 test begin: paddle.Tensor.all(Tensor([20, 100, 256000],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([20, 100, 256000],"bool"), ) 	 512000000 	 21540 	 10.08336877822876 	 11.16126537322998 	 0.23945260047912598 	 0.2647881507873535 	 None 	 None 	 None 	 None 	 
2025-08-04 14:13:28.863667 test begin: paddle.Tensor.all(Tensor([20, 794, 32000],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([20, 794, 32000],"bool"), ) 	 508160000 	 21540 	 10.034665822982788 	 10.90522313117981 	 0.2381577491760254 	 0.2587120532989502 	 None 	 None 	 None 	 None 	 
2025-08-04 14:13:57.392156 test begin: paddle.Tensor.all(Tensor([200, 10, 256000],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([200, 10, 256000],"bool"), ) 	 512000000 	 21540 	 10.083089828491211 	 11.160829544067383 	 0.23921799659729004 	 0.2647702693939209 	 None 	 None 	 None 	 None 	 
2025-08-04 14:14:25.898070 test begin: paddle.Tensor.amax(Tensor([1270081, 2, 4, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([1270081, 2, 4, 5],"float32"), axis=-1, keepdim=True, ) 	 50803240 	 65883 	 29.885955572128296 	 31.074867486953735 	 0.46364665031433105 	 0.48208189010620117 	 86.62868571281433 	 106.54579162597656 	 0.33603811264038086 	 0.33020973205566406 	 
2025-08-04 14:18:41.846455 test begin: paddle.Tensor.amax(Tensor([1270081, 2, 5, 4],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([1270081, 2, 5, 4],"float32"), axis=2, keepdim=True, ) 	 50803240 	 65883 	 33.81854057312012 	 12.11230754852295 	 0.5246627330780029 	 0.18770718574523926 	 91.29171061515808 	 100.74890446662903 	 0.3540325164794922 	 0.31244802474975586 	 
2025-08-04 14:22:42.656183 test begin: paddle.Tensor.amax(Tensor([1270081, 2, 5, 4],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([1270081, 2, 5, 4],"float32"), axis=None, keepdim=False, ) 	 50803240 	 65883 	 10.002896547317505 	 10.069550514221191 	 0.07756924629211426 	 0.0780949592590332 	 68.78122329711914 	 82.19676542282104 	 0.2135159969329834 	 0.21236896514892578 	 
2025-08-04 14:25:38.512119 test begin: paddle.Tensor.amax(Tensor([3, 2, 1693441, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 2, 1693441, 5],"float32"), axis=-1, keepdim=True, ) 	 50803230 	 65883 	 29.87338638305664 	 31.790529012680054 	 0.4633488655090332 	 0.4816930294036865 	 86.61463069915771 	 106.48213577270508 	 0.33587026596069336 	 0.33022570610046387 	 
2025-08-04 14:29:56.430382 test begin: paddle.Tensor.amax(Tensor([3, 2, 2116801, 4],"float32"), axis=2, keepdim=True, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2fd867ab30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 14:40:01.660232 test begin: paddle.Tensor.amax(Tensor([3, 2, 2116801, 4],"float32"), axis=None, keepdim=False, )
W0804 14:40:02.606451 147909 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 2, 2116801, 4],"float32"), axis=None, keepdim=False, ) 	 50803224 	 65883 	 9.985072135925293 	 10.063770294189453 	 0.07748889923095703 	 0.07805681228637695 	 68.77505159378052 	 82.18302607536316 	 0.2134852409362793 	 0.2123401165008545 	 
2025-08-04 14:42:56.004627 test begin: paddle.Tensor.amax(Tensor([3, 2, 4, 2116801],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 2, 4, 2116801],"float32"), axis=-1, keepdim=True, ) 	 50803224 	 65883 	 11.5335214138031 	 10.176245212554932 	 0.08946418762207031 	 0.07892274856567383 	 70.17994141578674 	 83.92886471748352 	 0.21785759925842285 	 0.21686029434204102 	 
2025-08-04 14:45:52.693669 test begin: paddle.Tensor.amax(Tensor([3, 2, 5, 1693441],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 2, 5, 1693441],"float32"), axis=2, keepdim=True, ) 	 50803230 	 65883 	 13.702616453170776 	 14.283456802368164 	 0.21255016326904297 	 0.22152972221374512 	 83.25350069999695 	 100.49275493621826 	 0.3229997158050537 	 0.3116912841796875 	 
2025-08-04 14:49:29.459395 test begin: paddle.Tensor.amax(Tensor([3, 2, 5, 1693441],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 2, 5, 1693441],"float32"), axis=None, keepdim=False, ) 	 50803230 	 65883 	 9.99202585220337 	 10.067584753036499 	 0.07749724388122559 	 0.0781097412109375 	 68.76182961463928 	 82.17476630210876 	 0.21349787712097168 	 0.2123572826385498 	 
2025-08-04 14:52:23.610968 test begin: paddle.Tensor.amax(Tensor([3, 846721, 4, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 846721, 4, 5],"float32"), axis=-1, keepdim=True, ) 	 50803260 	 65883 	 29.86940574645996 	 31.053539514541626 	 0.46332669258117676 	 0.48173093795776367 	 86.59255528450012 	 106.47600793838501 	 0.33579015731811523 	 0.33011865615844727 	 
2025-08-04 14:56:40.572188 test begin: paddle.Tensor.amax(Tensor([3, 846721, 5, 4],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 846721, 5, 4],"float32"), axis=2, keepdim=True, ) 	 50803260 	 65883 	 33.823150634765625 	 12.12690806388855 	 0.5246615409851074 	 0.18750262260437012 	 91.25152659416199 	 100.75967860221863 	 0.3538632392883301 	 0.3124682903289795 	 
2025-08-04 15:00:41.023279 test begin: paddle.Tensor.amax(Tensor([3, 846721, 5, 4],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 846721, 5, 4],"float32"), axis=None, keepdim=False, ) 	 50803260 	 65883 	 9.991630554199219 	 10.067403078079224 	 0.07748532295227051 	 0.07804346084594727 	 68.76341533660889 	 82.16656398773193 	 0.2134997844696045 	 0.21227645874023438 	 
2025-08-04 15:03:33.807083 test begin: paddle.Tensor.amin(Tensor([1270081, 2, 4, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([1270081, 2, 4, 5],"float32"), axis=-1, keepdim=True, ) 	 50803240 	 65865 	 29.85322117805481 	 31.04784870147705 	 0.4630863666534424 	 0.4817318916320801 	 86.58177781105042 	 106.47278308868408 	 0.336134672164917 	 0.33010244369506836 	 
2025-08-04 15:07:48.850847 test begin: paddle.Tensor.amin(Tensor([1270081, 2, 5, 4],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([1270081, 2, 5, 4],"float32"), axis=2, keepdim=True, ) 	 50803240 	 65865 	 34.60976576805115 	 12.084324598312378 	 0.5243337154388428 	 0.18749666213989258 	 91.21328592300415 	 100.7159686088562 	 0.3537008762359619 	 0.31242823600769043 	 
2025-08-04 15:11:50.118322 test begin: paddle.Tensor.amin(Tensor([1270081, 2, 5, 4],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([1270081, 2, 5, 4],"float32"), axis=None, keepdim=False, ) 	 50803240 	 65865 	 9.98945927619934 	 10.063940525054932 	 0.07750868797302246 	 0.07807779312133789 	 68.74155235290527 	 82.15240335464478 	 0.21344304084777832 	 0.21242427825927734 	 
2025-08-04 15:14:42.304929 test begin: paddle.Tensor.amin(Tensor([3, 2, 1693441, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 2, 1693441, 5],"float32"), axis=-1, keepdim=True, ) 	 50803230 	 65865 	 29.875271320343018 	 31.27761149406433 	 0.4633290767669678 	 0.48157310485839844 	 86.56884980201721 	 106.43522262573242 	 0.3357696533203125 	 0.33011341094970703 	 
2025-08-04 15:18:59.369832 test begin: paddle.Tensor.amin(Tensor([3, 2, 2116801, 4],"float32"), axis=2, keepdim=True, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8599242a70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 15:29:04.641342 test begin: paddle.Tensor.amin(Tensor([3, 2, 2116801, 4],"float32"), axis=None, keepdim=False, )
W0804 15:29:05.650951  1245 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 2, 2116801, 4],"float32"), axis=None, keepdim=False, ) 	 50803224 	 65865 	 9.982919454574585 	 10.059957027435303 	 0.07746052742004395 	 0.07809638977050781 	 68.7626805305481 	 82.15661025047302 	 0.2135472297668457 	 0.21236205101013184 	 
2025-08-04 15:31:57.406470 test begin: paddle.Tensor.amin(Tensor([3, 2, 4, 2116801],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 2, 4, 2116801],"float32"), axis=-1, keepdim=True, ) 	 50803224 	 65865 	 11.531991243362427 	 10.170258045196533 	 0.08946466445922852 	 0.07887387275695801 	 70.16827011108398 	 83.91548347473145 	 0.21790409088134766 	 0.21692180633544922 	 
2025-08-04 15:34:54.074850 test begin: paddle.Tensor.amin(Tensor([3, 2, 5, 1693441],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 2, 5, 1693441],"float32"), axis=2, keepdim=True, ) 	 50803230 	 65865 	 13.699583530426025 	 14.276419639587402 	 0.2125704288482666 	 0.22142696380615234 	 83.24076271057129 	 100.46755337715149 	 0.3229949474334717 	 0.3116443157196045 	 
2025-08-04 15:38:26.814774 test begin: paddle.Tensor.amin(Tensor([3, 2, 5, 1693441],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 2, 5, 1693441],"float32"), axis=None, keepdim=False, ) 	 50803230 	 65865 	 9.994730949401855 	 10.061824560165405 	 0.07753610610961914 	 0.07809138298034668 	 68.75402021408081 	 82.15635275840759 	 0.21353554725646973 	 0.21239328384399414 	 
2025-08-04 15:41:20.066699 test begin: paddle.Tensor.amin(Tensor([3, 846721, 4, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 846721, 4, 5],"float32"), axis=-1, keepdim=True, ) 	 50803260 	 65865 	 29.868685960769653 	 31.043102741241455 	 0.46349334716796875 	 0.48169779777526855 	 86.58118414878845 	 106.44437837600708 	 0.33584165573120117 	 0.33014440536499023 	 
2025-08-04 15:45:35.093099 test begin: paddle.Tensor.amin(Tensor([3, 846721, 5, 4],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 846721, 5, 4],"float32"), axis=2, keepdim=True, ) 	 50803260 	 65865 	 34.981985330581665 	 12.106732606887817 	 0.5246906280517578 	 0.1875007152557373 	 91.23066234588623 	 100.72741341590881 	 0.3538212776184082 	 0.31250762939453125 	 
2025-08-04 15:49:37.388197 test begin: paddle.Tensor.amin(Tensor([3, 846721, 5, 4],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 846721, 5, 4],"float32"), axis=None, keepdim=False, ) 	 50803260 	 65865 	 9.995301485061646 	 10.061272859573364 	 0.07755374908447266 	 0.07806062698364258 	 68.75353765487671 	 82.1583526134491 	 0.21350312232971191 	 0.21234726905822754 	 
2025-08-04 15:52:31.608538 test begin: paddle.Tensor.any(Tensor([10, 1379, 192, 192],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 1379, 192, 192],"bool"), axis=list[2,3,], ) 	 508354560 	 20191 	 10.004284858703613 	 11.134557008743286 	 0.2532045841217041 	 0.563603401184082 	 None 	 None 	 None 	 None 	 
2025-08-04 15:53:01.872249 test begin: paddle.Tensor.any(Tensor([10, 1501, 184, 184],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 1501, 184, 184],"bool"), axis=list[2,3,], ) 	 508178560 	 20191 	 10.222114086151123 	 11.487938165664673 	 0.25870370864868164 	 0.5814929008483887 	 None 	 None 	 None 	 None 	 
2025-08-04 15:53:30.955209 test begin: paddle.Tensor.any(Tensor([10, 300, 184, 921],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 300, 184, 921],"bool"), axis=list[2,3,], ) 	 508392000 	 20191 	 12.887858867645264 	 10.640975952148438 	 0.3261699676513672 	 0.5386583805084229 	 None 	 None 	 None 	 None 	 
2025-08-04 15:54:02.996765 test begin: paddle.Tensor.any(Tensor([10, 300, 192, 883],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 300, 192, 883],"bool"), axis=list[2,3,], ) 	 508608000 	 20191 	 10.949578285217285 	 10.610610961914062 	 0.27713894844055176 	 0.5371274948120117 	 None 	 None 	 None 	 None 	 
2025-08-04 15:54:32.730921 test begin: paddle.Tensor.any(Tensor([10, 300, 883, 192],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 300, 883, 192],"bool"), axis=list[2,3,], ) 	 508608000 	 20191 	 10.94900107383728 	 10.60413646697998 	 0.2771031856536865 	 0.5368044376373291 	 None 	 None 	 None 	 None 	 
2025-08-04 15:55:01.636597 test begin: paddle.Tensor.any(Tensor([10, 300, 921, 184],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 300, 921, 184],"bool"), axis=list[2,3,], ) 	 508392000 	 20191 	 12.881229877471924 	 10.646430015563965 	 0.3260619640350342 	 0.538947582244873 	 None 	 None 	 None 	 None 	 
2025-08-04 15:55:32.410708 test begin: paddle.Tensor.any(Tensor([100, 300, 136, 136],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([100, 300, 136, 136],"bool"), axis=list[2,3,], ) 	 554880000 	 20191 	 10.85970950126648 	 14.409558057785034 	 0.5497119426727295 	 0.7294120788574219 	 None 	 None 	 None 	 None 	 
2025-08-04 15:56:05.537154 test begin: paddle.Tensor.any(Tensor([20, 1374, 136, 136],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([20, 1374, 136, 136],"bool"), axis=list[2,3,], ) 	 508270080 	 20191 	 9.99114990234375 	 13.215140581130981 	 0.505744218826294 	 0.668917179107666 	 None 	 None 	 None 	 None 	 
2025-08-04 15:56:37.077398 test begin: paddle.Tensor.any(Tensor([20, 300, 136, 623],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([20, 300, 136, 623],"bool"), axis=list[2,3,], ) 	 508368000 	 20191 	 12.52427339553833 	 10.795291185379028 	 0.31705260276794434 	 0.5464472770690918 	 None 	 None 	 None 	 None 	 
2025-08-04 15:57:07.819412 test begin: paddle.Tensor.any(Tensor([20, 300, 623, 136],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([20, 300, 623, 136],"bool"), axis=list[2,3,], ) 	 508368000 	 20191 	 12.523848056793213 	 10.794937372207642 	 0.3170299530029297 	 0.5464704036712646 	 None 	 None 	 None 	 None 	 
2025-08-04 15:57:39.842929 test begin: paddle.Tensor.any(Tensor([50, 300, 192, 192],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([50, 300, 192, 192],"bool"), axis=list[2,3,], ) 	 552960000 	 20191 	 10.83332633972168 	 12.09149169921875 	 0.27419495582580566 	 0.6121151447296143 	 None 	 None 	 None 	 None 	 
2025-08-04 15:58:10.872075 test begin: paddle.Tensor.any(Tensor([60, 300, 184, 184],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([60, 300, 184, 184],"bool"), axis=list[2,3,], ) 	 609408000 	 20191 	 12.188230514526367 	 13.724418640136719 	 0.3084585666656494 	 0.6947112083435059 	 None 	 None 	 None 	 None 	 
2025-08-04 15:58:45.508935 test begin: paddle.Tensor.argmax(Tensor([13, 498, 8000],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([13, 498, 8000],"float32"), axis=2, ) 	 51792000 	 36546 	 10.163870811462402 	 6.199314117431641 	 0.28423118591308594 	 0.17334985733032227 	 None 	 None 	 None 	 None 	 
2025-08-04 15:59:02.816266 test begin: paddle.Tensor.argmax(Tensor([14, 457, 8000],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([14, 457, 8000],"float32"), axis=2, ) 	 51184000 	 36546 	 10.047434329986572 	 6.136519908905029 	 0.2809727191925049 	 0.17158269882202148 	 None 	 None 	 None 	 None 	 
2025-08-04 15:59:19.885653 test begin: paddle.Tensor.argmax(Tensor([14, 477, 8000],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([14, 477, 8000],"float32"), axis=2, ) 	 53424000 	 36546 	 10.477545738220215 	 6.341048955917358 	 0.2930116653442383 	 0.17729663848876953 	 None 	 None 	 None 	 None 	 
2025-08-04 15:59:38.894121 test begin: paddle.Tensor.argmax(Tensor([30, 212, 8000],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([30, 212, 8000],"float32"), axis=2, ) 	 50880000 	 36546 	 9.994681596755981 	 6.111987829208374 	 0.2795107364654541 	 0.17094659805297852 	 None 	 None 	 None 	 None 	 
2025-08-04 15:59:55.879002 test begin: paddle.Tensor.argmax(Tensor([30, 457, 3706],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([30, 457, 3706],"float32"), axis=2, ) 	 50809260 	 36546 	 12.724850416183472 	 5.9452965259552 	 0.3558616638183594 	 0.1662917137145996 	 None 	 None 	 None 	 None 	 
2025-08-04 16:00:16.571291 test begin: paddle.Tensor.argmax(Tensor([30, 477, 3551],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([30, 477, 3551],"float32"), axis=2, ) 	 50814810 	 36546 	 13.100270509719849 	 6.114067077636719 	 0.3663787841796875 	 0.17097997665405273 	 None 	 None 	 None 	 None 	 
2025-08-04 16:00:38.118038 test begin: paddle.Tensor.argmax(Tensor([30, 498, 3401],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([30, 498, 3401],"float32"), axis=2, ) 	 50810940 	 36546 	 13.515705823898315 	 6.033945560455322 	 0.377988338470459 	 0.1687459945678711 	 None 	 None 	 None 	 None 	 
2025-08-04 16:00:58.549377 test begin: paddle.Tensor.astype(Tensor([10, 32, 388, 4096],"float32"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([10, 32, 388, 4096],"float32"), "float32", ) 	 508559360 	 3208126 	 16.827919960021973 	 7.347541809082031 	 0.0001373291015625 	 9.179115295410156e-05 	 100.834801197052 	 144.93031454086304 	 0.00011467933654785156 	 0.00021767616271972656 	 
2025-08-04 16:05:47.530274 test begin: paddle.Tensor.astype(Tensor([10, 32, 4096, 388],"float32"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([10, 32, 4096, 388],"float32"), "float32", ) 	 508559360 	 3208126 	 10.465630292892456 	 7.236462831497192 	 0.00010848045349121094 	 0.0001399517059326172 	 100.80923271179199 	 145.34845232963562 	 0.0001163482666015625 	 0.0002224445343017578 	 
2025-08-04 16:10:29.597361 test begin: paddle.Tensor.astype(Tensor([10, 4, 4096, 4096],"float32"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([10, 4, 4096, 4096],"float32"), "float32", ) 	 671088640 	 3208126 	 10.502194881439209 	 7.6320037841796875 	 0.00011277198791503906 	 0.00010061264038085938 	 101.33389163017273 	 146.48251056671143 	 0.00011992454528808594 	 0.00022792816162109375 	 
2025-08-04 16:15:20.270318 test begin: paddle.Tensor.astype(Tensor([100352, 1013],"bfloat16"), "float32", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f10db822ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:25:25.043590 test begin: paddle.Tensor.astype(Tensor([1013, 100352],"bfloat16"), "float32", )
W0804 16:25:26.755896 20799 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fedf5a5f070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:35:38.645799 test begin: paddle.Tensor.astype(Tensor([12404, 8192],"bfloat16"), "float32", )
W0804 16:35:45.526445 24776 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f32f58ff010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:45:45.632323 test begin: paddle.Tensor.astype(Tensor([8192, 12404],"bfloat16"), "float32", )
W0804 16:45:47.468382 28693 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fea40a1b010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:55:50.422585 test begin: paddle.Tensor.atanh(Tensor([1, 16934401, 3],"float32"), )
W0804 16:55:51.479137 32540 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([1, 16934401, 3],"float32"), ) 	 50803203 	 33658 	 9.988479614257812 	 10.031590938568115 	 0.3033008575439453 	 0.3045170307159424 	 15.14979076385498 	 54.610535621643066 	 0.460003137588501 	 0.3308677673339844 	 
2025-08-04 16:57:22.738055 test begin: paddle.Tensor.atanh(Tensor([1, 2, 12700801],"float64"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([1, 2, 12700801],"float64"), ) 	 25401602 	 33658 	 14.926902294158936 	 13.703953742980957 	 0.4532756805419922 	 0.41601085662841797 	 15.092296600341797 	 54.53698253631592 	 0.4584016799926758 	 0.33121657371520996 	 
2025-08-04 16:59:02.753831 test begin: paddle.Tensor.atanh(Tensor([1, 2, 25401601],"float32"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([1, 2, 25401601],"float32"), ) 	 50803202 	 33658 	 9.98701524734497 	 10.028924703598022 	 0.30325961112976074 	 0.30454111099243164 	 15.150548934936523 	 54.60776996612549 	 0.45998716354370117 	 0.33181095123291016 	 
2025-08-04 17:00:34.602324 test begin: paddle.Tensor.atanh(Tensor([1, 8467201, 3],"float64"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([1, 8467201, 3],"float64"), ) 	 25401603 	 33658 	 14.926310062408447 	 14.511175155639648 	 0.45322275161743164 	 0.4160118103027344 	 15.091907262802124 	 54.53701949119568 	 0.4584462642669678 	 0.3312718868255615 	 
2025-08-04 17:02:16.079715 test begin: paddle.Tensor.atanh(Tensor([2, 12700801],"float64"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([2, 12700801],"float64"), ) 	 25401602 	 33658 	 14.923160552978516 	 13.69969630241394 	 0.45311570167541504 	 0.41600584983825684 	 15.08943247795105 	 54.533918619155884 	 0.45845627784729004 	 0.3312697410583496 	 
2025-08-04 17:03:56.832751 test begin: paddle.Tensor.atanh(Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([4233601, 2, 3],"float64"), ) 	 25401606 	 33658 	 14.92304801940918 	 13.69908881187439 	 0.453110933303833 	 0.41595458984375 	 15.090788841247559 	 54.53863024711609 	 0.4576733112335205 	 0.3311922550201416 	 
2025-08-04 17:05:38.062955 test begin: paddle.Tensor.atanh(Tensor([6350401, 4],"float64"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([6350401, 4],"float64"), ) 	 25401604 	 33658 	 15.839174032211304 	 13.699488639831543 	 0.45311760902404785 	 0.4159970283508301 	 15.088406085968018 	 54.53285026550293 	 0.45856165885925293 	 0.3312184810638428 	 
2025-08-04 17:07:19.612374 test begin: paddle.Tensor.atanh(Tensor([8467201, 2, 3],"float32"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([8467201, 2, 3],"float32"), ) 	 50803206 	 33658 	 9.987841129302979 	 10.029002904891968 	 0.30321407318115234 	 0.30451297760009766 	 15.151229619979858 	 54.61020278930664 	 0.4601175785064697 	 0.33171606063842773 	 
2025-08-04 17:08:51.283517 test begin: paddle.Tensor.bmm(Tensor([1, 16934401, 3],"float32"), Tensor([1, 3, 2],"float32"), )
[Prof] paddle.Tensor.bmm 	 paddle.Tensor.bmm(Tensor([1, 16934401, 3],"float32"), Tensor([1, 3, 2],"float32"), ) 	 50803209 	 42736 	 76.04238533973694 	 76.07084822654724 	 0.10688376426696777 	 0.1069333553314209 	 176.9672725200653 	 176.84244894981384 	 0.2231426239013672 	 0.22081708908081055 	 
2025-08-04 17:17:19.375640 test begin: paddle.Tensor.bmm(Tensor([1, 170476, 299],"float32"), Tensor([1, 299, 2],"float32"), )
[Prof] paddle.Tensor.bmm 	 paddle.Tensor.bmm(Tensor([1, 170476, 299],"float32"), Tensor([1, 299, 2],"float32"), ) 	 50972922 	 42736 	 10.227259874343872 	 10.223367691040039 	 0.24457645416259766 	 0.24453043937683105 	 18.096493244171143 	 18.062610149383545 	 0.14394617080688477 	 0.1439824104309082 	 
2025-08-04 17:18:17.292032 test begin: paddle.Tensor.bmm(Tensor([1, 179876, 283],"float32"), Tensor([1, 283, 2],"float32"), )
[Prof] paddle.Tensor.bmm 	 paddle.Tensor.bmm(Tensor([1, 179876, 283],"float32"), Tensor([1, 283, 2],"float32"), ) 	 50905474 	 42736 	 10.31518816947937 	 10.29865288734436 	 0.24640202522277832 	 0.24628639221191406 	 17.820247650146484 	 17.797719717025757 	 0.14198088645935059 	 0.1416478157043457 	 
2025-08-04 17:19:15.084861 test begin: paddle.Tensor.bmm(Tensor([1, 191277, 266],"float32"), Tensor([1, 266, 2],"float32"), )
[Prof] paddle.Tensor.bmm 	 paddle.Tensor.bmm(Tensor([1, 191277, 266],"float32"), Tensor([1, 266, 2],"float32"), ) 	 50880214 	 42736 	 9.98945927619934 	 9.9898202419281 	 0.23889803886413574 	 0.23886919021606445 	 17.972774028778076 	 17.965148210525513 	 0.1433088779449463 	 0.1430051326751709 	 
2025-08-04 17:20:11.873809 test begin: paddle.Tensor.bmm(Tensor([100, 170476, 3],"float32"), Tensor([100, 3, 2],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe8cb0daaa0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:30:35.596486 test begin: paddle.Tensor.bmm(Tensor([89, 191277, 3],"float32"), Tensor([89, 3, 2],"float32"), )
W0804 17:30:42.015475 41520 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f57a7b22d70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:40:46.372966 test begin: paddle.Tensor.bmm(Tensor([95, 179876, 3],"float32"), Tensor([95, 3, 2],"float32"), )
W0804 17:40:47.418776 44013 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9402676f50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:50:51.263063 test begin: paddle.Tensor.cast(Tensor([128256, 793],"float16"), Dtype(float16), )
W0804 17:50:53.524907 46511 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([128256, 793],"float16"), Dtype(float16), ) 	 101707008 	 32559 	 10.19999885559082 	 0.07033729553222656 	 0.1600940227508545 	 2.5272369384765625e-05 	 10.197651386260986 	 1.520925521850586 	 0.16005253791809082 	 6.556510925292969e-05 	 combined
2025-08-04 17:51:21.879406 test begin: paddle.Tensor.cast(Tensor([152064, 669],"float16"), Dtype(float16), )
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([152064, 669],"float16"), Dtype(float16), ) 	 101730816 	 32559 	 10.198628664016724 	 0.06420087814331055 	 0.16005229949951172 	 2.0742416381835938e-05 	 10.200282335281372 	 1.4688830375671387 	 0.16007089614868164 	 6.723403930664062e-05 	 combined
2025-08-04 17:51:49.354278 test begin: paddle.Tensor.cast(Tensor([24807, 4096],"float16"), Dtype(float16), )
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([24807, 4096],"float16"), Dtype(float16), ) 	 101609472 	 32559 	 10.13172197341919 	 0.0624539852142334 	 0.31803441047668457 	 1.9788742065429688e-05 	 10.090246200561523 	 1.4340095520019531 	 0.3167293071746826 	 7.009506225585938e-05 	 combined
2025-08-04 17:52:17.090832 test begin: paddle.Tensor.cast(Tensor([28351, 3584],"float16"), Dtype(float16), )
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([28351, 3584],"float16"), Dtype(float16), ) 	 101609984 	 32559 	 10.183883666992188 	 0.06233549118041992 	 0.15983057022094727 	 5.91278076171875e-05 	 10.187639474868774 	 1.4643683433532715 	 0.159898042678833 	 8.296966552734375e-05 	 combined
2025-08-04 17:52:43.790958 test begin: paddle.Tensor.cast(Tensor([3584, 28351],"float16"), Dtype(float16), )
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([3584, 28351],"float16"), Dtype(float16), ) 	 101609984 	 32559 	 10.183940172195435 	 0.10532951354980469 	 0.15981078147888184 	 3.0279159545898438e-05 	 10.187482357025146 	 1.6582233905792236 	 0.15986251831054688 	 0.00016736984252929688 	 combined
2025-08-04 17:53:09.765496 test begin: paddle.Tensor.cast(Tensor([669, 152064],"float16"), Dtype(float16), )
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([669, 152064],"float16"), Dtype(float16), ) 	 101730816 	 32559 	 10.198359489440918 	 0.10584259033203125 	 0.16005754470825195 	 5.459785461425781e-05 	 10.200252056121826 	 2.123152017593384 	 0.16005420684814453 	 0.0005834102630615234 	 combined
2025-08-04 17:53:42.634962 test begin: paddle.Tensor.ceil(Tensor([1, 50803201],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([1, 50803201],"float32"), ) 	 50803201 	 33828 	 10.007485151290894 	 10.07266879081726 	 0.3022923469543457 	 0.30425453186035156 	 4.5373194217681885 	 4.534451246261597 	 0.13700175285339355 	 0.13694357872009277 	 
2025-08-04 17:54:13.604258 test begin: paddle.Tensor.ceil(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33828 	 10.010600328445435 	 10.079217910766602 	 0.30236077308654785 	 0.3042104244232178 	 4.530417442321777 	 4.541254281997681 	 0.13682794570922852 	 0.13717031478881836 	 
2025-08-04 17:54:44.942148 test begin: paddle.Tensor.ceil(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33828 	 10.003946542739868 	 10.070349216461182 	 0.30225706100463867 	 0.30426979064941406 	 4.5313661098480225 	 4.536140203475952 	 0.13686251640319824 	 0.13695049285888672 	 
2025-08-04 17:55:15.814015 test begin: paddle.Tensor.ceil(Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([10, 5080321],"float32"), ) 	 50803210 	 33828 	 10.004990816116333 	 10.070203065872192 	 0.3022184371948242 	 0.304302453994751 	 4.531294345855713 	 4.539997577667236 	 0.13683438301086426 	 0.13697528839111328 	 
2025-08-04 17:55:49.179715 test begin: paddle.Tensor.ceil(Tensor([25401601, 2],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([25401601, 2],"float32"), ) 	 50803202 	 33828 	 10.006415128707886 	 10.070158004760742 	 0.3022933006286621 	 0.3042473793029785 	 4.53137469291687 	 4.535435199737549 	 0.1368410587310791 	 0.1369459629058838 	 
2025-08-04 17:56:20.067801 test begin: paddle.Tensor.ceil(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33828 	 10.004212379455566 	 10.304258346557617 	 0.3021965026855469 	 0.30423712730407715 	 4.53220534324646 	 4.539858818054199 	 0.13682317733764648 	 0.13692688941955566 	 
2025-08-04 17:56:53.888975 test begin: paddle.Tensor.ceil(Tensor([2540161, 20],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([2540161, 20],"float32"), ) 	 50803220 	 33828 	 10.0084867477417 	 10.075588703155518 	 0.302295446395874 	 0.3043332099914551 	 4.53134822845459 	 4.535333871841431 	 0.13686490058898926 	 0.13695144653320312 	 
2025-08-04 17:57:24.858369 test begin: paddle.Tensor.chunk(Tensor([1034, 32, 64, 48],"float16"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([1034, 32, 64, 48],"float16"), 2, axis=1, ) 	 101646336 	 29186 	 13.38267970085144 	 0.19998717308044434 	 0.4685800075531006 	 4.458427429199219e-05 	 8.994330883026123 	 13.195592403411865 	 0.314988374710083 	 0.46206164360046387 	 
2025-08-04 17:58:07.059071 test begin: paddle.Tensor.chunk(Tensor([128, 2068, 192],"float32"), 3, axis=-1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([128, 2068, 192],"float32"), 3, axis=-1, ) 	 50823168 	 29186 	 10.057124376296997 	 0.22966742515563965 	 0.35225510597229004 	 2.9087066650390625e-05 	 9.004433631896973 	 9.014443635940552 	 0.3153400421142578 	 0.31567835807800293 	 
2025-08-04 17:58:39.807456 test begin: paddle.Tensor.chunk(Tensor([512, 32, 130, 48],"float16"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([512, 32, 130, 48],"float16"), 2, axis=1, ) 	 102236160 	 29186 	 13.30180549621582 	 0.3462975025177002 	 0.46535491943359375 	 6.175041198730469e-05 	 9.13270092010498 	 13.273826837539673 	 0.31980419158935547 	 0.4647648334503174 	 
2025-08-04 17:59:24.202279 test begin: paddle.Tensor.chunk(Tensor([512, 32, 64, 49],"float32"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([512, 32, 64, 49],"float32"), 2, axis=1, ) 	 51380224 	 29186 	 10.249362230300903 	 0.3458559513092041 	 0.35894107818603516 	 5.793571472167969e-05 	 9.18043565750122 	 9.140145778656006 	 0.3214411735534668 	 0.3201277256011963 	 
2025-08-04 17:59:56.350864 test begin: paddle.Tensor.chunk(Tensor([512, 32, 64, 97],"float16"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([512, 32, 64, 97],"float16"), 2, axis=1, ) 	 101711872 	 29186 	 13.262042760848999 	 0.20267176628112793 	 0.4642002582550049 	 3.981590270996094e-05 	 9.098791360855103 	 13.192087888717651 	 0.3185439109802246 	 0.46193385124206543 	 
2025-08-04 18:00:39.488625 test begin: paddle.Tensor.chunk(Tensor([512, 32, 65, 48],"float32"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([512, 32, 65, 48],"float32"), 2, axis=1, ) 	 51118080 	 29186 	 11.456251859664917 	 0.20210576057434082 	 0.3583073616027832 	 3.0517578125e-05 	 9.13487720489502 	 9.087486743927002 	 0.3197619915008545 	 0.3182699680328369 	 
2025-08-04 18:01:14.486038 test begin: paddle.Tensor.chunk(Tensor([517, 32, 64, 48],"float32"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([517, 32, 64, 48],"float32"), 2, axis=1, ) 	 50823168 	 29186 	 10.205732345581055 	 0.35085415840148926 	 0.3572399616241455 	 8.988380432128906e-05 	 9.006198167800903 	 9.087260246276855 	 0.315248966217041 	 0.3181464672088623 	 
2025-08-04 18:01:46.898584 test begin: paddle.Tensor.chunk(Tensor([85, 3136, 192],"float32"), 3, axis=-1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([85, 3136, 192],"float32"), 3, axis=-1, ) 	 51179520 	 29186 	 10.087958812713623 	 0.23346209526062012 	 0.3532700538635254 	 5.054473876953125e-05 	 9.065542221069336 	 9.074175596237183 	 0.3173682689666748 	 0.31780457496643066 	 
2025-08-04 18:02:17.259742 test begin: paddle.Tensor.clip(Tensor([1, 386, 65856, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([1, 386, 65856, 2],"float32"), 0, ) 	 50840832 	 33863 	 10.013666152954102 	 10.090116500854492 	 0.3022639751434326 	 0.3044240474700928 	 15.260048627853394 	 20.175384521484375 	 0.4605746269226074 	 0.20312190055847168 	 
2025-08-04 18:03:15.271948 test begin: paddle.Tensor.clip(Tensor([1, 400, 63505, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([1, 400, 63505, 2],"float32"), 0, ) 	 50804000 	 33863 	 10.008585453033447 	 10.086286783218384 	 0.30190324783325195 	 0.3042783737182617 	 15.247328519821167 	 20.159762144088745 	 0.4602053165435791 	 0.2030336856842041 	 
2025-08-04 18:04:13.654694 test begin: paddle.Tensor.clip(Tensor([1, 400, 65856, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([1, 400, 65856, 2],"float32"), 0, ) 	 52684800 	 33863 	 10.38145923614502 	 10.455518245697021 	 0.3131875991821289 	 0.3152484893798828 	 15.80215311050415 	 20.873741388320923 	 0.47687816619873047 	 0.21016693115234375 	 
2025-08-04 18:05:13.976830 test begin: paddle.Tensor.clip(Tensor([2100, 12096, 3],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([2100, 12096, 3],"float32"), 0, ) 	 76204800 	 33863 	 14.962165832519531 	 15.073965311050415 	 0.45138096809387207 	 0.45362043380737305 	 22.785033702850342 	 29.953821659088135 	 0.6875922679901123 	 0.30161166191101074 	 
2025-08-04 18:06:40.950996 test begin: paddle.Tensor.clip(Tensor([2100, 12097, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([2100, 12097, 2],"float32"), 0, ) 	 50807400 	 33863 	 10.010720014572144 	 10.081602096557617 	 0.3020205497741699 	 0.30425167083740234 	 15.241380453109741 	 20.154432773590088 	 0.45993804931640625 	 0.20293474197387695 	 
2025-08-04 18:07:40.782531 test begin: paddle.Tensor.clip(Tensor([2101, 12096, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([2101, 12096, 2],"float32"), 0, ) 	 50827392 	 33863 	 10.012542486190796 	 10.085050106048584 	 0.30203962326049805 	 0.3043229579925537 	 15.247663259506226 	 20.16136884689331 	 0.4601593017578125 	 0.20298266410827637 	 
2025-08-04 18:08:40.552921 test begin: paddle.Tensor.clip(Tensor([4, 525, 12096, 3],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([4, 525, 12096, 3],"float32"), 0, ) 	 76204800 	 33863 	 14.952858924865723 	 15.031273126602173 	 0.45131731033325195 	 0.45365333557128906 	 22.78145694732666 	 29.953701972961426 	 0.6875948905944824 	 0.3016026020050049 	 
2025-08-04 18:10:06.957455 test begin: paddle.Tensor.clip(Tensor([4, 525, 12097, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([4, 525, 12097, 2],"float32"), 0, ) 	 50807400 	 33863 	 10.006864309310913 	 10.084737777709961 	 0.30203986167907715 	 0.30422139167785645 	 15.240599155426025 	 20.153650999069214 	 0.45993566513061523 	 0.2028958797454834 	 
2025-08-04 18:11:04.251588 test begin: paddle.Tensor.clip(Tensor([4, 526, 12096, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([4, 526, 12096, 2],"float32"), 0, ) 	 50899968 	 33863 	 10.035446882247925 	 10.09946084022522 	 0.30277538299560547 	 0.30478596687316895 	 15.272448539733887 	 20.191567182540894 	 0.4609034061431885 	 0.203322172164917 	 
2025-08-04 18:12:04.171313 test begin: paddle.Tensor.clip(Tensor([5, 525, 12096, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([5, 525, 12096, 2],"float32"), 0, ) 	 63504000 	 33863 	 12.46407151222229 	 14.653191566467285 	 0.37615036964416504 	 0.378978967666626 	 19.00703001022339 	 25.040600776672363 	 0.5737109184265137 	 0.2522439956665039 	 
2025-08-04 18:13:19.071598 test begin: paddle.Tensor.clone(Tensor([3544, 32, 896],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([3544, 32, 896],"bfloat16"), ) 	 101613568 	 32273 	 10.056372165679932 	 10.001534938812256 	 0.31844043731689453 	 0.3166930675506592 	 19.84595513343811 	 14.633745670318604 	 0.6284890174865723 	 0.46338701248168945 	 
2025-08-04 18:14:17.062256 test begin: paddle.Tensor.clone(Tensor([6017, 19, 896],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6017, 19, 896],"bfloat16"), ) 	 102433408 	 32273 	 10.2616708278656 	 10.178555965423584 	 0.1623227596282959 	 0.1611461639404297 	 20.102660179138184 	 14.754064798355103 	 0.31830716133117676 	 0.4671742916107178 	 
2025-08-04 18:15:17.791943 test begin: paddle.Tensor.clone(Tensor([6017, 32, 528],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6017, 32, 528],"bfloat16"), ) 	 101663232 	 32273 	 10.103179216384888 	 10.096174716949463 	 0.15993404388427734 	 0.15985369682312012 	 19.952499628067017 	 14.640907287597656 	 0.3159158229827881 	 0.46370363235473633 	 
2025-08-04 18:16:16.006572 test begin: paddle.Tensor.clone(Tensor([6036, 19, 896],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6036, 19, 896],"bfloat16"), ) 	 102756864 	 32273 	 10.207183122634888 	 10.207216262817383 	 0.16160058975219727 	 0.16161108016967773 	 20.1713547706604 	 14.798443794250488 	 0.3193676471710205 	 0.4685807228088379 	 
2025-08-04 18:17:16.384427 test begin: paddle.Tensor.clone(Tensor([6036, 32, 527],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6036, 32, 527],"bfloat16"), ) 	 101791104 	 32273 	 10.17736268043518 	 10.129319667816162 	 0.1611323356628418 	 0.16020774841308594 	 19.983837604522705 	 14.661690711975098 	 0.31641244888305664 	 0.4643373489379883 	 
2025-08-04 18:18:15.538964 test begin: paddle.Tensor.clone(Tensor([6078, 19, 896],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6078, 19, 896],"bfloat16"), ) 	 103471872 	 32273 	 10.098524570465088 	 10.278709888458252 	 0.15988779067993164 	 0.16274499893188477 	 20.309808492660522 	 14.901555061340332 	 0.32157468795776367 	 0.47185349464416504 	 
2025-08-04 18:19:14.596861 test begin: paddle.Tensor.clone(Tensor([6078, 32, 523],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6078, 32, 523],"bfloat16"), ) 	 101721408 	 32273 	 10.169931173324585 	 10.113069534301758 	 0.16101479530334473 	 0.16011404991149902 	 19.970022916793823 	 14.652230024337769 	 0.3161945343017578 	 0.4640488624572754 	 
2025-08-04 18:20:12.926401 test begin: paddle.Tensor.conj(Tensor([10, 2540161],"float64"), )
[Prof] paddle.Tensor.conj 	 paddle.Tensor.conj(Tensor([10, 2540161],"float64"), ) 	 25401610 	 33610 	 9.991896390914917 	 0.05489945411682129 	 0.30382823944091797 	 7.081031799316406e-05 	 9.971427917480469 	 1.4946317672729492 	 0.3032052516937256 	 6.628036499023438e-05 	 
2025-08-04 18:20:38.872455 test begin: paddle.Tensor.conj(Tensor([1270081, 20],"float64"), )
[Prof] paddle.Tensor.conj 	 paddle.Tensor.conj(Tensor([1270081, 20],"float64"), ) 	 25401620 	 33610 	 10.476368188858032 	 0.054563283920288086 	 0.303851842880249 	 2.4557113647460938e-05 	 9.97143816947937 	 1.5088295936584473 	 0.3031954765319824 	 8.988380432128906e-05 	 
2025-08-04 18:21:02.825602 test begin: paddle.Tensor.cos(Tensor([131072, 388],"float32"), )
[Prof] paddle.Tensor.cos 	 paddle.Tensor.cos(Tensor([131072, 388],"float32"), ) 	 50855936 	 33870 	 10.017038345336914 	 10.103209733963013 	 0.30224156379699707 	 0.30492687225341797 	 15.26942229270935 	 35.292789459228516 	 0.46078014373779297 	 0.3550608158111572 	 
2025-08-04 18:22:17.959381 test begin: paddle.Tensor.cos(Tensor([3175201, 16],"float32"), )
[Prof] paddle.Tensor.cos 	 paddle.Tensor.cos(Tensor([3175201, 16],"float32"), ) 	 50803216 	 33870 	 10.008936166763306 	 10.093259572982788 	 0.30202651023864746 	 0.30457043647766113 	 15.251524209976196 	 35.25704288482666 	 0.4602234363555908 	 0.3546755313873291 	 
2025-08-04 18:23:32.298582 test begin: paddle.Tensor.cos(Tensor([32768, 1551],"float32"), )
[Prof] paddle.Tensor.cos 	 paddle.Tensor.cos(Tensor([32768, 1551],"float32"), ) 	 50823168 	 33870 	 10.237350702285767 	 10.097127199172974 	 0.3021199703216553 	 0.3046998977661133 	 15.257371664047241 	 35.26907134056091 	 0.4603583812713623 	 0.35477685928344727 	 
2025-08-04 18:24:46.670926 test begin: paddle.Tensor.cos(Tensor([396901, 128],"float32"), )
[Prof] paddle.Tensor.cos 	 paddle.Tensor.cos(Tensor([396901, 128],"float32"), ) 	 50803328 	 33870 	 10.011437892913818 	 10.093617916107178 	 0.3018825054168701 	 0.30452513694763184 	 15.247334957122803 	 35.25591564178467 	 0.46012043952941895 	 0.35463619232177734 	 
2025-08-04 18:25:59.604916 test begin: paddle.Tensor.cumprod(Tensor([25401601],"float64"), -1, )
[Prof] paddle.Tensor.cumprod 	 paddle.Tensor.cumprod(Tensor([25401601],"float64"), -1, ) 	 25401601 	 30751 	 9.992900848388672 	 10.07319974899292 	 0.16609406471252441 	 0.16725754737854004 	 86.02205562591553 	 63.13303589820862 	 0.00030422210693359375 	 0.0013053417205810547 	 
2025-08-04 18:28:51.748496 test begin: paddle.Tensor.cumprod(Tensor([50803201],"float32"), -1, )
[Prof] paddle.Tensor.cumprod 	 paddle.Tensor.cumprod(Tensor([50803201],"float32"), -1, ) 	 50803201 	 30751 	 10.01979923248291 	 10.150577306747437 	 0.16654419898986816 	 0.1683347225189209 	 95.95441508293152 	 65.04850935935974 	 0.0002999305725097656 	 0.0013036727905273438 	 
2025-08-04 18:31:55.968134 test begin: paddle.Tensor.cumsum(Tensor([1, 144, 352801],"float32"), 1, )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([1, 144, 352801],"float32"), 1, ) 	 50803344 	 28938 	 36.254496812820435 	 10.248471021652222 	 0.4268927574157715 	 0.36117029190063477 	 185.10419607162476 	 28.276668787002563 	 1.4291465282440186 	 0.3327951431274414 	 
2025-08-04 18:36:18.970258 test begin: paddle.Tensor.cumsum(Tensor([1, 144, 352801],"float32"), 2, )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([1, 144, 352801],"float32"), 2, ) 	 50803344 	 28938 	 25.68814754486084 	 29.413064002990723 	 0.9072425365447998 	 1.0386552810668945 	 48.714739084243774 	 47.48715114593506 	 0.5737559795379639 	 0.5592451095581055 	 
2025-08-04 18:38:52.021543 test begin: paddle.Tensor.cumsum(Tensor([1, 254017, 200],"float32"), 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4ec7e0aa40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 18:51:03.953509 test begin: paddle.Tensor.cumsum(Tensor([1, 254017, 200],"float32"), 2, )
W0804 18:51:05.031380 60752 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([1, 254017, 200],"float32"), 2, ) 	 50803400 	 28938 	 11.710891246795654 	 81.58810544013977 	 0.41358447074890137 	 2.8814845085144043 	 120.36121773719788 	 99.63148617744446 	 1.416349172592163 	 1.173661470413208 	 
2025-08-04 18:56:20.772138 test begin: paddle.Tensor.cumsum(Tensor([1765, 144, 200],"float32"), 1, )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([1765, 144, 200],"float32"), 1, ) 	 50832000 	 28938 	 37.171720027923584 	 10.47886848449707 	 0.43773841857910156 	 0.3700690269470215 	 186.3154890537262 	 28.75370764732361 	 1.3164987564086914 	 0.33854174613952637 	 
2025-08-04 19:00:45.304068 test begin: paddle.Tensor.cumsum(Tensor([1765, 144, 200],"float32"), 2, )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([1765, 144, 200],"float32"), 2, ) 	 50832000 	 28938 	 11.949888706207275 	 81.64358139038086 	 0.4140286445617676 	 2.8836467266082764 	 120.26506400108337 	 99.74606919288635 	 1.415374517440796 	 1.1750280857086182 	 
2025-08-04 19:06:01.921837 test begin: paddle.Tensor.cumsum(Tensor([211681, 120],"int64"), )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([211681, 120],"int64"), ) 	 25401720 	 28938 	 10.14371919631958 	 9.467840433120728 	 0.00013899803161621094 	 0.1671741008758545 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:06:34.844141 test begin: paddle.Tensor.cumsum(Tensor([300, 84673],"int64"), )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([300, 84673],"int64"), ) 	 25401900 	 28938 	 10.548406839370728 	 9.467212677001953 	 0.0001404285430908203 	 0.16715502738952637 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:07:08.625262 test begin: paddle.Tensor.detach(Tensor([1003520, 1013],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([1003520, 1013],"bfloat16"), ) 	 1016565760 	 12614448 	 9.809434175491333 	 35.61604857444763 	 9.036064147949219e-05 	 0.0002980232238769531 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:13:57.127370 test begin: paddle.Tensor.detach(Tensor([10130, 100352],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([10130, 100352],"bfloat16"), ) 	 1016565760 	 12614448 	 9.76898741722107 	 35.999985694885254 	 0.00017309188842773438 	 0.0003116130828857422 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:20:48.667258 test begin: paddle.Tensor.detach(Tensor([124040, 8192],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([124040, 8192],"bfloat16"), ) 	 1016135680 	 12614448 	 9.770357131958008 	 35.43205547332764 	 8.106231689453125e-05 	 0.0003001689910888672 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:27:37.685857 test begin: paddle.Tensor.detach(Tensor([17720, 57344],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([17720, 57344],"bfloat16"), ) 	 1016135680 	 12614448 	 9.745985269546509 	 35.9515814781189 	 7.104873657226562e-05 	 0.0003044605255126953 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:34:31.218742 test begin: paddle.Tensor.detach(Tensor([81920, 12404],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([81920, 12404],"bfloat16"), ) 	 1016135680 	 12614448 	 10.244669198989868 	 37.89304709434509 	 0.00016379356384277344 	 0.0002968311309814453 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 19:41:28.226277 test begin: paddle.Tensor.diag_embed(Tensor([1, 25401601, 2],"float32"), )
[Prof] paddle.Tensor.diag_embed 	 paddle.Tensor.diag_embed(Tensor([1, 25401601, 2],"float32"), ) 	 50803202 	 5658 	 20.27901339530945 	 5.7404985427856445 	 0.00010371208190917969 	 0.5177934169769287 	 None 	 None 	 None 	 None 	 
2025-08-04 19:41:57.829627 test begin: paddle.Tensor.diag_embed(Tensor([25401601, 1, 2],"float32"), )
[Prof] paddle.Tensor.diag_embed 	 paddle.Tensor.diag_embed(Tensor([25401601, 1, 2],"float32"), ) 	 50803202 	 5658 	 12.002401113510132 	 5.7331702709198 	 0.00010418891906738281 	 0.517829418182373 	 None 	 None 	 None 	 None 	 
2025-08-04 19:42:16.531961 test begin: paddle.Tensor.diagonal(Tensor([301, 84672],"float64"), axis1=-2, axis2=-1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f28e3c12980>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 19:52:21.292859 test begin: paddle.Tensor.diagonal(Tensor([8467201, 3],"float64"), axis1=-2, axis2=-1, )
W0804 19:52:21.952875 65782 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f203ee66e30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 20:02:25.714105 test begin: paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), )
W0804 20:02:26.444286 66181 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 12395 	 11.70126461982727 	 3.247556209564209 	 0.3215951919555664 	 0.26702451705932617 	 None 	 None 	 None 	 None 	 
2025-08-04 20:02:43.884346 test begin: paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=-2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=-2, ) 	 25401664 	 12395 	 11.697810173034668 	 3.242841958999634 	 0.3215019702911377 	 0.2671337127685547 	 None 	 None 	 None 	 None 	 
2025-08-04 20:02:59.428701 test begin: paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=2, ) 	 25401664 	 12395 	 11.698006629943848 	 3.239942789077759 	 0.32151174545288086 	 0.26714515686035156 	 None 	 None 	 None 	 None 	 
2025-08-04 20:03:14.931810 test begin: paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 12395 	 10.795236825942993 	 3.2399375438690186 	 0.29669809341430664 	 0.2671184539794922 	 None 	 None 	 None 	 None 	 
2025-08-04 20:03:29.535736 test begin: paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=-2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=-2, ) 	 25401664 	 12395 	 10.795641422271729 	 3.251068115234375 	 0.2967112064361572 	 0.2671942710876465 	 None 	 None 	 None 	 None 	 
2025-08-04 20:03:45.919837 test begin: paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=2, ) 	 25401664 	 12395 	 10.797163248062134 	 3.2398581504821777 	 0.2967667579650879 	 0.26709794998168945 	 None 	 None 	 None 	 None 	 
2025-08-04 20:04:00.530084 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 12395 	 10.79473090171814 	 3.239992380142212 	 0.29673075675964355 	 0.26717519760131836 	 None 	 None 	 None 	 None 	 
2025-08-04 20:04:15.086700 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=-2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=-2, ) 	 25401664 	 12395 	 13.2734956741333 	 3.7087512016296387 	 0.36487865447998047 	 0.3057851791381836 	 None 	 None 	 None 	 None 	 
2025-08-04 20:04:32.630983 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=2, ) 	 25401664 	 12395 	 13.275575637817383 	 3.708730697631836 	 0.3648700714111328 	 0.3057732582092285 	 None 	 None 	 None 	 None 	 
2025-08-04 20:04:50.151857 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 12395 	 13.274226665496826 	 3.7106831073760986 	 0.3647904396057129 	 0.3057842254638672 	 None 	 None 	 None 	 None 	 
2025-08-04 20:05:07.714857 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=-2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=-2, ) 	 25401664 	 12395 	 10.00325870513916 	 3.255751371383667 	 0.27489495277404785 	 0.2684140205383301 	 None 	 None 	 None 	 None 	 
2025-08-04 20:05:21.499608 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=2, ) 	 25401664 	 12395 	 10.011174201965332 	 3.255662202835083 	 0.2748997211456299 	 0.2684473991394043 	 None 	 None 	 None 	 None 	 
2025-08-04 20:05:38.784905 test begin: paddle.Tensor.digamma(Tensor([4, 6350401],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([4, 6350401],"float64"), ) 	 25401604 	 8544 	 10.863595247268677 	 9.763557434082031 	 1.1955218315124512 	 1.1678507328033447 	 73.10811066627502 	 9.270407676696777 	 8.746111392974854 	 0.5543897151947021 	 
2025-08-04 20:07:24.827344 test begin: paddle.Tensor.digamma(Tensor([453601, 7, 8],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([453601, 7, 8],"float64"), ) 	 25401656 	 8544 	 9.994187116622925 	 9.761831521987915 	 1.1953530311584473 	 1.1675775051116943 	 73.08812117576599 	 9.269684791564941 	 8.74300503730774 	 0.5544242858886719 	 
2025-08-04 20:09:09.561623 test begin: paddle.Tensor.digamma(Tensor([45361, 7, 8, 10],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([45361, 7, 8, 10],"float64"), ) 	 25402160 	 8544 	 9.995569229125977 	 10.212544202804565 	 1.1953463554382324 	 1.284752368927002 	 73.09616184234619 	 9.273108005523682 	 8.74269413948059 	 0.5545368194580078 	 
2025-08-04 20:10:57.489646 test begin: paddle.Tensor.digamma(Tensor([5, 635041, 8],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5, 635041, 8],"float64"), ) 	 25401640 	 8544 	 10.755147933959961 	 9.755406379699707 	 1.9555320739746094 	 1.1669228076934814 	 73.08906435966492 	 9.270468473434448 	 8.742164850234985 	 0.5543715953826904 	 
2025-08-04 20:12:43.811705 test begin: paddle.Tensor.digamma(Tensor([5, 63505, 8, 10],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5, 63505, 8, 10],"float64"), ) 	 25402000 	 8544 	 9.996670484542847 	 9.757556200027466 	 1.1953814029693604 	 1.1671175956726074 	 73.1096122264862 	 9.27242136001587 	 8.744385004043579 	 0.5545778274536133 	 
2025-08-04 20:14:27.227421 test begin: paddle.Tensor.digamma(Tensor([5, 7, 725761],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5, 7, 725761],"float64"), ) 	 25401635 	 8544 	 9.991035461425781 	 9.785080671310425 	 1.1949989795684814 	 1.1677789688110352 	 73.10338854789734 	 9.275243520736694 	 8.744341850280762 	 0.5547709465026855 	 
2025-08-04 20:16:11.367269 test begin: paddle.Tensor.digamma(Tensor([5, 7, 72577, 10],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5, 7, 72577, 10],"float64"), ) 	 25401950 	 8544 	 9.990821838378906 	 11.96487283706665 	 1.1950054168701172 	 1.6861701011657715 	 73.08797812461853 	 9.271550416946411 	 8.742584466934204 	 0.5546190738677979 	 
2025-08-04 20:17:59.229750 test begin: paddle.Tensor.digamma(Tensor([5, 7, 8, 90721],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5, 7, 8, 90721],"float64"), ) 	 25401880 	 8544 	 9.989408493041992 	 9.754684925079346 	 1.1948397159576416 	 1.1667559146881104 	 73.10927534103394 	 9.269542217254639 	 8.744930267333984 	 0.5543968677520752 	 
2025-08-04 20:19:44.551072 test begin: paddle.Tensor.digamma(Tensor([5080321, 5],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5080321, 5],"float64"), ) 	 25401605 	 8544 	 9.992591142654419 	 9.922277927398682 	 1.1943490505218506 	 1.3209905624389648 	 73.12915182113647 	 9.271565675735474 	 8.747330665588379 	 0.55454421043396 	 
2025-08-04 20:21:29.901212 test begin: paddle.Tensor.dim(Tensor([1116160, 911],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([1116160, 911],"bfloat16"), ) 	 1016821760 	 14433255 	 9.136836528778076 	 22.19797945022583 	 0.00013589859008789062 	 0.0002968311309814453 	 None 	 None 	 None 	 None 	 
2025-08-04 20:22:18.051596 test begin: paddle.Tensor.dim(Tensor([124040, 8192],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([124040, 8192],"bfloat16"), ) 	 1016135680 	 14433255 	 9.116042375564575 	 21.658397436141968 	 6.914138793945312e-05 	 0.0002796649932861328 	 None 	 None 	 None 	 None 	 
2025-08-04 20:23:05.960736 test begin: paddle.Tensor.dim(Tensor([141760, 7168],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([141760, 7168],"bfloat16"), ) 	 1016135680 	 14433255 	 9.102921962738037 	 25.877389907836914 	 5.91278076171875e-05 	 0.00028514862060546875 	 None 	 None 	 None 	 None 	 
2025-08-04 20:23:57.424291 test begin: paddle.Tensor.dim(Tensor([71680, 14176],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([71680, 14176],"bfloat16"), ) 	 1016135680 	 14433255 	 9.097473382949829 	 22.038501262664795 	 5.841255187988281e-05 	 0.0002841949462890625 	 None 	 None 	 None 	 None 	 
2025-08-04 20:24:45.341066 test begin: paddle.Tensor.dim(Tensor([9110, 111616],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([9110, 111616],"bfloat16"), ) 	 1016821760 	 14433255 	 9.159164428710938 	 21.624719381332397 	 6.389617919921875e-05 	 0.0002856254577636719 	 None 	 None 	 None 	 None 	 
2025-08-04 20:25:32.668266 test begin: paddle.Tensor.dim(Tensor([958720, 1060],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([958720, 1060],"bfloat16"), ) 	 1016243200 	 14433255 	 8.972805738449097 	 21.902204990386963 	 6.389617919921875e-05 	 0.0002982616424560547 	 None 	 None 	 None 	 None 	 
2025-08-04 20:26:22.849570 test begin: paddle.Tensor.dot(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
Warning: The core code of paddle.Tensor.dot is too complex.
[Prof] paddle.Tensor.dot 	 paddle.Tensor.dot(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 33757 	 9.985553741455078 	 9.9049072265625 	 0.3023076057434082 	 0.1499934196472168 	 23.95381450653076 	 20.37982487678528 	 0.3626244068145752 	 0.3084707260131836 	 
2025-08-04 20:27:29.762596 test begin: paddle.Tensor.equal(Tensor([128, 198451],"int64"), Tensor([128, 198451],"int64"), )
[Prof] paddle.Tensor.equal 	 paddle.Tensor.equal(Tensor([128, 198451],"int64"), Tensor([128, 198451],"int64"), ) 	 50803456 	 56162 	 17.362593412399292 	 17.586302042007446 	 0.31592345237731934 	 0.31981897354125977 	 None 	 None 	 None 	 None 	 
2025-08-04 20:28:05.676342 test begin: paddle.Tensor.equal(Tensor([198451, 128],"int64"), Tensor([198451, 128],"int64"), )
[Prof] paddle.Tensor.equal 	 paddle.Tensor.equal(Tensor([198451, 128],"int64"), Tensor([198451, 128],"int64"), ) 	 50803456 	 56162 	 17.362316846847534 	 17.574592351913452 	 0.31594300270080566 	 0.3197972774505615 	 None 	 None 	 None 	 None 	 
2025-08-04 20:28:44.660966 test begin: paddle.Tensor.equal(Tensor([2, 12700801],"int64"), 3, )
[Prof] paddle.Tensor.equal 	 paddle.Tensor.equal(Tensor([2, 12700801],"int64"), 3, ) 	 25401602 	 56162 	 9.98857307434082 	 9.44550108909607 	 0.09088492393493652 	 0.17175602912902832 	 None 	 None 	 None 	 None 	 
2025-08-04 20:29:04.515096 test begin: paddle.Tensor.equal(Tensor([2540161, 10],"int64"), 3, )
[Prof] paddle.Tensor.equal 	 paddle.Tensor.equal(Tensor([2540161, 10],"int64"), 3, ) 	 25401610 	 56162 	 9.988392114639282 	 9.438624620437622 	 0.09088373184204102 	 0.17174077033996582 	 None 	 None 	 None 	 None 	 
2025-08-04 20:29:24.398983 test begin: paddle.Tensor.equal_all(Tensor([25401601],"int64"), Tensor([25401601],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([25401601],"int64"), Tensor([25401601],"int64"), ) 	 50803202 	 612074 	 209.46936416625977 	 232.32488560676575 	 0.11639595031738281 	 0.00024580955505371094 	 None 	 None 	 None 	 None 	 
2025-08-04 20:36:47.162111 test begin: paddle.Tensor.equal_all(Tensor([25401601],"int64"), Tensor([801],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([25401601],"int64"), Tensor([801],"int64"), ) 	 25402402 	 612074 	 13.898285388946533 	 2.999401569366455 	 0.00010919570922851562 	 0.0001361370086669922 	 None 	 None 	 None 	 None 	 
2025-08-04 20:37:04.532456 test begin: paddle.Tensor.equal_all(Tensor([8, 3175201],"int64"), Tensor([8, 3175201],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8, 3175201],"int64"), Tensor([8, 3175201],"int64"), ) 	 50803216 	 612074 	 209.47172713279724 	 231.1414885520935 	 0.11637639999389648 	 0.0002541542053222656 	 None 	 None 	 None 	 None 	 
2025-08-04 20:44:26.100033 test begin: paddle.Tensor.equal_all(Tensor([801, 3175201],"int64"), Tensor([801, 3],"int64"), )
[Error] CUDA out of memory. Tried to allocate 18.95 GiB. GPU 0 has a total capacity of 39.39 GiB of which 18.03 GiB is free. Process 57659 has 21.35 GiB memory in use. Of the allocated memory 8.12 MiB is allocated by PyTorch, and 11.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-04 20:45:17.752975 test begin: paddle.Tensor.equal_all(Tensor([801, 3],"int64"), Tensor([801, 3175201],"int64"), )
W0804 20:45:53.233132 67547 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([801, 3],"int64"), Tensor([801, 3175201],"int64"), ) 	 2543338404 	 612074 	 9.949933052062988 	 2.7670066356658936 	 4.506111145019531e-05 	 0.0003180503845214844 	 None 	 None 	 None 	 None 	 
2025-08-04 20:46:22.830139 test begin: paddle.Tensor.equal_all(Tensor([801, 3],"int64"), Tensor([8467201, 3],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([801, 3],"int64"), Tensor([8467201, 3],"int64"), ) 	 25404006 	 612074 	 12.502694606781006 	 1.6242506504058838 	 0.00010943412780761719 	 0.00011944770812988281 	 None 	 None 	 None 	 None 	 
2025-08-04 20:46:41.117617 test begin: paddle.Tensor.equal_all(Tensor([801],"int64"), Tensor([25401601],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([801],"int64"), Tensor([25401601],"int64"), ) 	 25402402 	 612074 	 13.227545022964478 	 2.544004201889038 	 0.00021314620971679688 	 0.00020265579223632812 	 None 	 None 	 None 	 None 	 
2025-08-04 20:46:58.586612 test begin: paddle.Tensor.equal_all(Tensor([8467201, 3],"int64"), Tensor([801, 3],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8467201, 3],"int64"), Tensor([801, 3],"int64"), ) 	 25404006 	 612074 	 12.959275960922241 	 2.919656991958618 	 0.0001552104949951172 	 0.00012874603271484375 	 None 	 None 	 None 	 None 	 
2025-08-04 20:47:18.446778 test begin: paddle.Tensor.equal_all(Tensor([8467201, 3],"int64"), Tensor([8467201, 3],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8467201, 3],"int64"), Tensor([8467201, 3],"int64"), ) 	 50803206 	 612074 	 208.63551878929138 	 231.080335855484 	 0.1159369945526123 	 0.00024366378784179688 	 None 	 None 	 None 	 None 	 
2025-08-04 20:54:43.358492 test begin: paddle.Tensor.equal_all(Tensor([846720101, 3],"int64"), Tensor([8, 3],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([846720101, 3],"int64"), Tensor([8, 3],"int64"), ) 	 2540160327 	 612074 	 10.279846906661987 	 1.6231939792633057 	 4.863739013671875e-05 	 0.00011348724365234375 	 None 	 None 	 None 	 None 	 
2025-08-04 20:55:33.905462 test begin: paddle.Tensor.equal_all(Tensor([8],"int64"), Tensor([2540160101],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8],"int64"), Tensor([2540160101],"int64"), ) 	 2540160109 	 612074 	 10.292116403579712 	 1.6238484382629395 	 4.363059997558594e-05 	 9.1552734375e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 20:56:23.067598 test begin: paddle.Tensor.erfinv(x=Tensor([211681, 2, 3, 5, 4],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([211681, 2, 3, 5, 4],"float64"), ) 	 25401720 	 30679 	 10.175945043563843 	 9.487234830856323 	 0.34221696853637695 	 0.3155252933502197 	 13.746259689331055 	 50.38287091255188 	 0.4579157829284668 	 0.33570027351379395 	 
2025-08-04 20:57:52.685721 test begin: paddle.Tensor.erfinv(x=Tensor([4, 105841, 3, 5, 4],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 105841, 3, 5, 4],"float64"), ) 	 25401840 	 30679 	 10.251892328262329 	 9.522991180419922 	 0.3430962562561035 	 0.3173651695251465 	 13.745055913925171 	 50.381874799728394 	 0.4579036235809326 	 0.335768461227417 	 
2025-08-04 20:59:17.679870 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 158761, 5, 4],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 158761, 5, 4],"float64"), ) 	 25401760 	 30679 	 10.322073459625244 	 9.574812173843384 	 0.3444328308105469 	 0.3175320625305176 	 13.72840142250061 	 50.37972378730774 	 0.4573190212249756 	 0.33565735816955566 	 
2025-08-04 21:00:44.976271 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 1058401],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 1058401],"float64"), ) 	 25401624 	 30679 	 10.366409063339233 	 9.62285852432251 	 0.3463783264160156 	 0.3204939365386963 	 13.730532169342041 	 50.37860989570618 	 0.45737409591674805 	 0.335782527923584 	 
2025-08-04 21:02:10.225909 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 264601, 4],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 264601, 4],"float64"), ) 	 25401696 	 30679 	 10.383347511291504 	 9.653673648834229 	 0.34769201278686523 	 0.3208479881286621 	 13.727617025375366 	 50.377487897872925 	 0.4573345184326172 	 0.33571505546569824 	 
2025-08-04 21:03:40.096106 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 5, 211681],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 5, 211681],"float64"), ) 	 25401720 	 30679 	 10.428012132644653 	 9.67310881614685 	 0.3480668067932129 	 0.3229868412017822 	 13.745528936386108 	 50.38201451301575 	 0.4578392505645752 	 0.3357577323913574 	 
2025-08-04 21:05:06.038439 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 3175201],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 3175201],"float64"), ) 	 25401608 	 30679 	 10.419281721115112 	 9.651856660842896 	 0.3481600284576416 	 0.32053637504577637 	 13.73046588897705 	 50.381516218185425 	 0.45752882957458496 	 0.3357970714569092 	 
2025-08-04 21:06:35.177351 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 635041, 5],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 635041, 5],"float64"), ) 	 25401640 	 30679 	 10.42430830001831 	 9.674273490905762 	 0.34708714485168457 	 0.3233821392059326 	 13.732670545578003 	 50.420886516571045 	 0.4575340747833252 	 0.33600354194641113 	 
2025-08-04 21:08:00.534476 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2116801, 3],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2116801, 3],"float64"), ) 	 25401612 	 30679 	 10.507557153701782 	 9.716540813446045 	 0.35114431381225586 	 0.323671817779541 	 13.730588436126709 	 50.42021083831787 	 0.45738911628723145 	 0.3360164165496826 	 
2025-08-04 21:09:28.498806 test begin: paddle.Tensor.erfinv(x=Tensor([4, 423361, 3, 5],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 423361, 3, 5],"float64"), ) 	 25401660 	 30679 	 10.368669033050537 	 9.613757848739624 	 0.34679102897644043 	 0.3204312324523926 	 13.73311972618103 	 50.38208985328674 	 0.457521915435791 	 0.3356595039367676 	 
2025-08-04 21:10:53.712676 test begin: paddle.Tensor.erfinv(x=Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4233601, 2, 3],"float64"), ) 	 25401606 	 30679 	 10.355449676513672 	 9.614747047424316 	 0.34404850006103516 	 0.32057929039001465 	 13.730517625808716 	 50.39346694946289 	 0.45739316940307617 	 0.33568572998046875 	 
2025-08-04 21:12:19.222037 test begin: paddle.Tensor.erfinv(x=Tensor([846721, 2, 3, 5],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([846721, 2, 3, 5],"float64"), ) 	 25401630 	 30679 	 10.428333044052124 	 9.691573858261108 	 0.348224401473999 	 0.3229541778564453 	 13.730708122253418 	 50.40955710411072 	 0.4573941230773926 	 0.33606982231140137 	 
2025-08-04 21:13:44.656942 test begin: paddle.Tensor.exp(Tensor([1000000, 26],"float64"), )
[Prof] paddle.Tensor.exp 	 paddle.Tensor.exp(Tensor([1000000, 26],"float64"), ) 	 26000000 	 33844 	 10.355185508728027 	 10.396952867507935 	 0.3127157688140869 	 0.3139009475708008 	 15.531487226486206 	 15.384960889816284 	 0.46909475326538086 	 0.4645512104034424 	 
2025-08-04 21:14:39.629390 test begin: paddle.Tensor.exp(Tensor([2540161, 20],"float32"), )
[Prof] paddle.Tensor.exp 	 paddle.Tensor.exp(Tensor([2540161, 20],"float32"), ) 	 50803220 	 33844 	 9.995615243911743 	 10.069960594177246 	 0.30181217193603516 	 0.30416083335876465 	 15.212418794631958 	 15.107914924621582 	 0.45931315422058105 	 0.4561593532562256 	 
2025-08-04 21:15:31.686151 test begin: paddle.Tensor.exp(Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.exp 	 paddle.Tensor.exp(Tensor([50803201],"float32"), ) 	 50803201 	 33844 	 9.995209217071533 	 10.07570505142212 	 0.3017723560333252 	 0.30408740043640137 	 15.214119911193848 	 15.108931303024292 	 0.4593989849090576 	 0.45624542236328125 	 
2025-08-04 21:16:23.742964 test begin: paddle.Tensor.exp(Tensor([6350401, 4],"float64"), )
[Prof] paddle.Tensor.exp 	 paddle.Tensor.exp(Tensor([6350401, 4],"float64"), ) 	 25401604 	 33844 	 10.129184246063232 	 10.162841558456421 	 0.30590152740478516 	 0.30687570571899414 	 15.16275954246521 	 15.038380861282349 	 0.45784497261047363 	 0.45414018630981445 	 
2025-08-04 21:17:15.325992 test begin: paddle.Tensor.exp(Tensor([64, 793801],"float32"), )
[Prof] paddle.Tensor.exp 	 paddle.Tensor.exp(Tensor([64, 793801],"float32"), ) 	 50803264 	 33844 	 9.995399951934814 	 10.076313257217407 	 0.3018791675567627 	 0.3040766716003418 	 15.214513063430786 	 15.109291553497314 	 0.45943498611450195 	 0.4562866687774658 	 
2025-08-04 21:18:09.134373 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 266, 477, 401],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 266, 477, 401],"float32"), ) 	 50879683 	 74165 	 10.019342422485352 	 0.31394100189208984 	 0.13794708251953125 	 9.679794311523438e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:18:32.206526 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 283, 466, 386],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 283, 466, 386],"float32"), ) 	 50904909 	 74165 	 10.02795147895813 	 0.3141772747039795 	 0.13806390762329102 	 6.532669067382812e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:18:55.251605 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 299, 391, 436],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 299, 391, 436],"float32"), ) 	 50972325 	 74165 	 10.024235010147095 	 0.3199634552001953 	 0.13811182975769043 	 9.274482727050781e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:19:18.672623 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 38841, 436],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 38841, 436],"float32"), ) 	 50804029 	 74165 	 9.996242046356201 	 0.3296480178833008 	 0.13767695426940918 	 8.344650268554688e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:19:42.865738 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 391, 43311],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 391, 43311],"float32"), ) 	 50803804 	 74165 	 9.993080854415894 	 0.3150033950805664 	 0.13765287399291992 	 4.291534423828125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:20:05.829619 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 42231, 401],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 42231, 401],"float32"), ) 	 50803894 	 74165 	 9.992814779281616 	 0.3183286190032959 	 0.1377120018005371 	 8.749961853027344e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:20:30.968741 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 43872, 386],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 43872, 386],"float32"), ) 	 50803777 	 74165 	 9.989434957504272 	 0.31159210205078125 	 0.13832926750183105 	 3.838539123535156e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:20:53.940451 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 466, 36340],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 466, 36340],"float32"), ) 	 50803321 	 74165 	 9.994577884674072 	 0.3123791217803955 	 0.13767623901367188 	 8.535385131835938e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:21:16.937312 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 477, 35502],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 477, 35502],"float32"), ) 	 50803363 	 74165 	 9.987713098526001 	 0.3115415573120117 	 0.13760590553283691 	 8.606910705566406e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:21:41.311829 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([100, 3, 391, 436],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([100, 3, 391, 436],"float32"), ) 	 51142801 	 74165 	 10.063507795333862 	 0.31078648567199707 	 0.1385939121246338 	 3.457069396972656e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:22:04.450798 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([89, 3, 477, 401],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([89, 3, 477, 401],"float32"), ) 	 51070960 	 74165 	 10.0472092628479 	 0.6128158569335938 	 0.13842129707336426 	 0.00013566017150878906 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:22:28.085179 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([95, 3, 466, 386],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([95, 3, 466, 386],"float32"), ) 	 51264661 	 74165 	 10.086580038070679 	 0.3100550174713135 	 0.1389002799987793 	 9.012222290039062e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-04 21:22:51.289953 test begin: paddle.Tensor.fill_(Tensor([50803201],"float32"), 0, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(Tensor([50803201],"float32"), 0, ) 	 50803201 	 68510 	 9.917528629302979 	 9.17869520187378 	 0.14801669120788574 	 0.13688373565673828 	 None 	 None 	 None 	 None 	 
2025-08-04 21:23:14.247919 test begin: paddle.Tensor.fill_(Tensor([659782, 77],"float32"), value=-math.inf, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(Tensor([659782, 77],"float32"), value=-math.inf, ) 	 50803214 	 68510 	 9.91504955291748 	 9.179583311080933 	 0.14795780181884766 	 0.1368722915649414 	 None 	 None 	 None 	 None 	 
2025-08-04 21:23:38.467635 test begin: paddle.Tensor.fill_(Tensor([77, 659782],"float32"), value=-math.inf, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(Tensor([77, 659782],"float32"), value=-math.inf, ) 	 50803214 	 68510 	 9.92379093170166 	 9.186107397079468 	 0.14786028861999512 	 0.13685822486877441 	 None 	 None 	 None 	 None 	 
2025-08-04 21:24:02.695901 test begin: paddle.Tensor.fill_(x=Tensor([10, 158761, 16],"float64"), value=41.2, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(x=Tensor([10, 158761, 16],"float64"), value=41.2, ) 	 25401760 	 68510 	 10.368738412857056 	 9.20542860031128 	 0.15443754196166992 	 0.13727569580078125 	 None 	 None 	 None 	 None 	 
2025-08-04 21:24:25.551772 test begin: paddle.Tensor.fill_(x=Tensor([10, 16, 158761],"float64"), value=41.2, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(x=Tensor([10, 16, 158761],"float64"), value=41.2, ) 	 25401760 	 68510 	 10.37004804611206 	 9.212915658950806 	 0.15452814102172852 	 0.1374049186706543 	 None 	 None 	 None 	 None 	 
2025-08-04 21:24:50.222051 test begin: paddle.Tensor.fill_(x=Tensor([99226, 16, 16],"float64"), value=41.2, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(x=Tensor([99226, 16, 16],"float64"), value=41.2, ) 	 25401856 	 68510 	 10.18239951133728 	 9.206865787506104 	 0.15163397789001465 	 0.13732385635375977 	 None 	 None 	 None 	 None 	 
2025-08-04 21:25:15.106643 test begin: paddle.Tensor.fill_diagonal_(Tensor([1280, 396901],"float32"), 0, wrap=False, )
[Prof] paddle.Tensor.fill_diagonal_ 	 paddle.Tensor.fill_diagonal_(Tensor([1280, 396901],"float32"), 0, wrap=False, ) 	 508033280 	 432242 	 9.990555047988892 	 4.916680812835693 	 9.822845458984375e-05 	 7.724761962890625e-05 	 13.791363716125488 	 18.983219146728516 	 9.655952453613281e-05 	 0.00020265579223632812 	 combined
2025-08-04 21:26:19.068610 test begin: paddle.Tensor.fill_diagonal_(Tensor([3969010, 128],"float32"), 0, wrap=False, )
[Prof] paddle.Tensor.fill_diagonal_ 	 paddle.Tensor.fill_diagonal_(Tensor([3969010, 128],"float32"), 0, wrap=False, ) 	 508033280 	 432242 	 9.969987869262695 	 4.853016376495361 	 0.00012063980102539062 	 7.939338684082031e-05 	 13.828822612762451 	 18.936558485031128 	 0.00011086463928222656 	 0.00023221969604492188 	 combined
2025-08-04 21:27:23.191105 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([12700801, 4, 7],"int32"), Tensor([12700801, 4],"int32"), 0, 1, 2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f69fcd43580>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:37:28.191786 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([1814401, 4, 7],"int32"), Tensor([1814401, 4],"int32"), 0, 1, 2, )
W0804 21:37:32.741089 69350 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([1814401, 4, 7],"int32"), Tensor([1814401, 4],"int32"), 0, 1, 2, ) 	 58060832 	 31092 	 142.58572936058044 	 19.585439682006836 	 0.0015151500701904297 	 0.21449780464172363 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 21:42:41.998753 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4, 3175201],"int64"), Tensor([2, 4],"int64"), 0, 1, 2, )
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4, 3175201],"int64"), Tensor([2, 4],"int64"), 0, 1, 2, ) 	 25401616 	 31092 	 9.91675066947937 	 9.82835841178894 	 0.08128738403320312 	 0.10749673843383789 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 21:43:12.568817 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4, 6350401],"int32"), Tensor([2, 4],"int32"), 0, 1, 2, )
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4, 6350401],"int32"), Tensor([2, 4],"int32"), 0, 1, 2, ) 	 50803216 	 31092 	 9.909700870513916 	 9.828313827514648 	 0.08129739761352539 	 0.10747218132019043 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 21:43:43.560171 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4233601, 3, 2],"int32"), Tensor([2, 2, 3],"int32"), offset=0, dim1=1, dim2=2, )
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4233601, 3, 2],"int32"), Tensor([2, 2, 3],"int32"), offset=0, dim1=1, dim2=2, ) 	 50803224 	 31092 	 9.906738758087158 	 9.831222534179688 	 0.08121919631958008 	 0.10750222206115723 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 21:44:14.567084 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([6350401, 4, 7],"int64"), Tensor([6350401, 4],"int64"), 0, 1, 2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9b08927100>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 21:54:19.208960 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([907201, 4, 7],"int64"), Tensor([907201, 4],"int64"), 0, 1, 2, )
W0804 21:54:19.840330 69979 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([907201, 4, 7],"int64"), Tensor([907201, 4],"int64"), 0, 1, 2, ) 	 29030432 	 31092 	 72.07537508010864 	 16.098741054534912 	 0.00080108642578125 	 0.17622733116149902 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 21:57:01.347738 test begin: paddle.Tensor.flatten(Tensor([10, 64, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([10, 64, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, ) 	 1684480000 	 1816345 	 10.6417715549469 	 14.91294240951538 	 6.890296936035156e-05 	 0.0003116130828857422 	 77.91632437705994 	 97.00340175628662 	 9.1552734375e-05 	 0.00022292137145996094 	 
2025-08-04 22:01:18.955924 test begin: paddle.Tensor.flatten(Tensor([1280, 127, 56, 56],"float32"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 127, 56, 56],"float32"), 2, ) 	 509788160 	 1816345 	 10.033040523529053 	 7.590354681015015 	 7.581710815429688e-05 	 0.00013256072998046875 	 77.88248038291931 	 95.34798216819763 	 0.000102996826171875 	 0.00023412704467773438 	 
2025-08-04 22:04:47.158908 test begin: paddle.Tensor.flatten(Tensor([1280, 254, 56, 56],"float16"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 254, 56, 56],"float16"), 2, ) 	 1019576320 	 1816345 	 14.301995277404785 	 7.767235994338989 	 0.00012159347534179688 	 8.368492126464844e-05 	 78.08988094329834 	 96.70204067230225 	 0.00010585784912109375 	 0.00020956993103027344 	 
2025-08-04 22:08:44.936661 test begin: paddle.Tensor.flatten(Tensor([1280, 512, 14, 56],"float32"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 512, 14, 56],"float32"), 2, ) 	 513802240 	 1816345 	 9.98871898651123 	 7.684209585189819 	 8.845329284667969e-05 	 9.012222290039062e-05 	 78.0239052772522 	 96.50615334510803 	 0.00011563301086425781 	 0.00022077560424804688 	 
2025-08-04 22:12:14.875288 test begin: paddle.Tensor.flatten(Tensor([1280, 512, 28, 56],"float16"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 512, 28, 56],"float16"), 2, ) 	 1027604480 	 1816345 	 10.006029844284058 	 7.710620641708374 	 8.487701416015625e-05 	 0.00011873245239257812 	 78.58516263961792 	 96.70925545692444 	 0.00012874603271484375 	 0.0002200603485107422 	 
2025-08-04 22:16:07.185212 test begin: paddle.Tensor.flatten(Tensor([1280, 512, 56, 14],"float32"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 512, 56, 14],"float32"), 2, ) 	 513802240 	 1816345 	 9.967946529388428 	 7.850285768508911 	 0.00012373924255371094 	 8.702278137207031e-05 	 77.54870533943176 	 96.93711185455322 	 0.0001163482666015625 	 0.00021982192993164062 	 
2025-08-04 22:19:37.702171 test begin: paddle.Tensor.flatten(Tensor([1280, 512, 56, 28],"float16"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 512, 56, 28],"float16"), 2, ) 	 1027604480 	 1816345 	 9.98744535446167 	 7.896674871444702 	 8.630752563476562e-05 	 0.00012755393981933594 	 78.00468611717224 	 96.00296521186829 	 0.00010776519775390625 	 0.00021529197692871094 	 
2025-08-04 22:23:28.777311 test begin: paddle.Tensor.flatten(Tensor([320, 512, 56, 56],"float32"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([320, 512, 56, 56],"float32"), 2, ) 	 513802240 	 1816345 	 10.063078880310059 	 7.775607585906982 	 7.224082946777344e-05 	 7.891654968261719e-05 	 77.95325541496277 	 96.72269678115845 	 0.00010585784912109375 	 0.0002193450927734375 	 
2025-08-04 22:26:59.079087 test begin: paddle.Tensor.flatten(Tensor([40, 5, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([40, 5, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, ) 	 526400000 	 1816345 	 10.676262140274048 	 7.938368797302246 	 6.771087646484375e-05 	 0.00012874603271484375 	 78.19102644920349 	 96.73920679092407 	 9.179115295410156e-05 	 0.00021791458129882812 	 
2025-08-04 22:30:31.748013 test begin: paddle.Tensor.flatten(Tensor([40, 64, 2, 376, 280],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([40, 64, 2, 376, 280],"float32"), start_axis=1, stop_axis=2, ) 	 539033600 	 1816345 	 10.590710639953613 	 7.845052003860474 	 0.00012421607971191406 	 8.58306884765625e-05 	 77.78551578521729 	 98.64806699752808 	 0.00011515617370605469 	 0.00020241737365722656 	 
2025-08-04 22:34:05.979525 test begin: paddle.Tensor.flatten(Tensor([40, 64, 25, 29, 280],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([40, 64, 25, 29, 280],"float32"), start_axis=1, stop_axis=2, ) 	 519680000 	 1816345 	 10.469976425170898 	 8.02718210220337 	 7.390975952148438e-05 	 8.7738037109375e-05 	 78.22278118133545 	 96.07451033592224 	 0.00011301040649414062 	 0.00021266937255859375 	 
2025-08-04 22:37:39.447758 test begin: paddle.Tensor.flatten(Tensor([40, 64, 25, 376, 22],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([40, 64, 25, 376, 22],"float32"), start_axis=1, stop_axis=2, ) 	 529408000 	 1816345 	 10.573553323745728 	 8.09749698638916 	 6.151199340820312e-05 	 0.00013327598571777344 	 78.51350331306458 	 97.48117446899414 	 9.298324584960938e-05 	 0.00022649765014648438 	 
2025-08-04 22:41:13.654458 test begin: paddle.Tensor.flatten(Tensor([640, 512, 56, 56],"float16"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([640, 512, 56, 56],"float16"), 2, ) 	 1027604480 	 1816345 	 18.961533308029175 	 7.6445088386535645 	 0.00014829635620117188 	 8.845329284667969e-05 	 77.19702744483948 	 96.18127799034119 	 0.00010752677917480469 	 0.00021457672119140625 	 
2025-08-04 22:45:14.130728 test begin: paddle.Tensor.flip(Tensor([16, 3, 224, 4726],"float32"), 0, )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([16, 3, 224, 4726],"float32"), 0, ) 	 50813952 	 11097 	 10.70448088645935 	 3.482675075531006 	 0.9857015609741211 	 0.31807398796081543 	 10.680969953536987 	 3.4525914192199707 	 0.9835789203643799 	 0.31795406341552734 	 
2025-08-04 22:45:47.009368 test begin: paddle.Tensor.flip(Tensor([16, 3, 4726, 224],"float32"), 0, )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([16, 3, 4726, 224],"float32"), 0, ) 	 50813952 	 11097 	 10.703441619873047 	 3.455232858657837 	 0.9855458736419678 	 0.318068265914917 	 10.679461479187012 	 3.4525935649871826 	 0.9835288524627686 	 0.3179759979248047 	 
2025-08-04 22:46:18.678231 test begin: paddle.Tensor.flip(Tensor([16, 64, 224, 224],"float32"), 0, )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([16, 64, 224, 224],"float32"), 0, ) 	 51380224 	 11097 	 10.807016372680664 	 3.5125718116760254 	 0.9953176975250244 	 0.3217732906341553 	 10.783147811889648 	 3.493849039077759 	 0.9930922985076904 	 0.3217494487762451 	 
2025-08-04 22:46:52.252497 test begin: paddle.Tensor.flip(Tensor([3, 400, 42337],"float32"), axis=list[-1,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([3, 400, 42337],"float32"), axis=list[-1,], ) 	 50804400 	 11097 	 10.054561376571655 	 3.471364974975586 	 0.9260444641113281 	 0.31917619705200195 	 10.00046420097351 	 3.464871644973755 	 0.9210305213928223 	 0.3190944194793701 	 
2025-08-04 22:47:20.965932 test begin: paddle.Tensor.flip(Tensor([3, 400, 42337],"float32"), axis=list[-2,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([3, 400, 42337],"float32"), axis=list[-2,], ) 	 50804400 	 11097 	 10.054520606994629 	 3.5096335411071777 	 0.9260196685791016 	 0.3227102756500244 	 10.00192642211914 	 3.5033950805664062 	 0.921154260635376 	 0.3226468563079834 	 
2025-08-04 22:47:51.261939 test begin: paddle.Tensor.flip(Tensor([3, 56449, 300],"float32"), axis=list[-1,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([3, 56449, 300],"float32"), axis=list[-1,], ) 	 50804100 	 11097 	 10.057058572769165 	 3.4659252166748047 	 0.9262523651123047 	 0.3191800117492676 	 10.00212550163269 	 3.4651498794555664 	 0.9211888313293457 	 0.31924891471862793 	 
2025-08-04 22:48:19.967296 test begin: paddle.Tensor.flip(Tensor([3, 56449, 300],"float32"), axis=list[-2,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([3, 56449, 300],"float32"), axis=list[-2,], ) 	 50804100 	 11097 	 10.057071685791016 	 3.759115695953369 	 0.926262378692627 	 0.32251620292663574 	 10.001538515090942 	 3.499950885772705 	 0.9211366176605225 	 0.3222999572753906 	 
2025-08-04 22:48:51.269780 test begin: paddle.Tensor.flip(Tensor([338, 3, 224, 224],"float32"), 0, )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([338, 3, 224, 224],"float32"), 0, ) 	 50878464 	 11097 	 10.704556226730347 	 3.4705300331115723 	 0.9859111309051514 	 0.3188183307647705 	 10.687458038330078 	 3.4613802433013916 	 0.984288215637207 	 0.3187224864959717 	 
2025-08-04 22:49:21.333655 test begin: paddle.Tensor.flip(Tensor([424, 400, 300],"float32"), axis=list[-1,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([424, 400, 300],"float32"), axis=list[-1,], ) 	 50880000 	 11097 	 10.077569246292114 	 3.4814441204071045 	 0.928149938583374 	 0.3196842670440674 	 10.01077675819397 	 3.4706690311431885 	 0.9219608306884766 	 0.3196451663970947 	 
2025-08-04 22:49:53.176264 test begin: paddle.Tensor.flip(Tensor([424, 400, 300],"float32"), axis=list[-2,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([424, 400, 300],"float32"), axis=list[-2,], ) 	 50880000 	 11097 	 10.078877210617065 	 3.510063409805298 	 0.9282674789428711 	 0.3232553005218506 	 10.012402772903442 	 3.509943962097168 	 0.9221150875091553 	 0.3232383728027344 	 
2025-08-04 22:50:22.025721 test begin: paddle.Tensor.floor(Tensor([12700801, 4],"float32"), )
[Prof] paddle.Tensor.floor 	 paddle.Tensor.floor(Tensor([12700801, 4],"float32"), ) 	 50803204 	 33839 	 10.002997875213623 	 10.08872127532959 	 0.3020751476287842 	 0.30426931381225586 	 4.533941745758057 	 4.537033319473267 	 0.13687491416931152 	 0.13774991035461426 	 
2025-08-04 22:50:54.761585 test begin: paddle.Tensor.floor(Tensor([1857, 27358],"float32"), )
[Prof] paddle.Tensor.floor 	 paddle.Tensor.floor(Tensor([1857, 27358],"float32"), ) 	 50803806 	 33839 	 9.995416402816772 	 10.510861873626709 	 0.3018510341644287 	 0.30429863929748535 	 4.536729335784912 	 4.536118984222412 	 0.1368732452392578 	 0.13697314262390137 	 
2025-08-04 22:51:28.104968 test begin: paddle.Tensor.floor(Tensor([1872, 27139],"float32"), )
[Prof] paddle.Tensor.floor 	 paddle.Tensor.floor(Tensor([1872, 27139],"float32"), ) 	 50804208 	 33839 	 10.008199214935303 	 10.086832284927368 	 0.3022150993347168 	 0.3043053150177002 	 4.533695697784424 	 4.535461902618408 	 0.13680672645568848 	 0.13692641258239746 	 
2025-08-04 22:51:59.791703 test begin: paddle.Tensor.floor(Tensor([1915, 26530],"float32"), )
[Prof] paddle.Tensor.floor 	 paddle.Tensor.floor(Tensor([1915, 26530],"float32"), ) 	 50804950 	 33839 	 10.008800268173218 	 10.540231227874756 	 0.3022763729095459 	 0.3042941093444824 	 4.533527851104736 	 4.536370038986206 	 0.13678884506225586 	 0.13692831993103027 	 
2025-08-04 22:52:33.246288 test begin: paddle.Tensor.gather(Tensor([40, 12700801],"float32"), Tensor([40, 1],"int64"), 1, )
[Prof] paddle.Tensor.gather 	 paddle.Tensor.gather(Tensor([40, 12700801],"float32"), Tensor([40, 1],"int64"), 1, ) 	 508032080 	 72474 	 0.6805815696716309 	 94.36890387535095 	 3.3855438232421875e-05 	 0.00021696090698242188 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 22:56:11.401967 test begin: paddle.Tensor.gather(Tensor([400, 1270080],"float32"), Tensor([400, 1],"int64"), 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f29405b0eb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:06:24.177839 test begin: paddle.Tensor.gather(Tensor([4000, 127008],"float32"), Tensor([4000, 1],"int64"), 1, )
W0804 23:06:32.161814 73006 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f51f887f0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:16:29.257372 test begin: paddle.Tensor.gather_nd(Tensor([11, 53, 8],"float32"), Tensor([40, 50, 2],"int64"), )
W0804 23:16:31.610478 73380 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([11, 53, 8],"float32"), Tensor([40, 50, 2],"int64"), ) 	 8664 	 1730 	 0.0356135368347168 	 285.07445430755615 	 0.00014209747314453125 	 0.0003497600555419922 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 23:21:20.596572 test begin: paddle.Tensor.gather_nd(Tensor([16, 3, 15, 80, 8],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([16, 3, 15, 80, 8],"float32"), Tensor([516, 4],"int64"), ) 	 462864 	 1730 	 0.01849055290222168 	 132.54514908790588 	 1.2159347534179688e-05 	 0.00019288063049316406 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 23:23:34.447138 test begin: paddle.Tensor.gather_nd(Tensor([16, 3, 80, 156, 85],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([16, 3, 80, 156, 85],"float32"), Tensor([516, 4],"int64"), ) 	 50920464 	 1730 	 0.018580913543701172 	 141.0186312198639 	 1.5974044799804688e-05 	 0.00024271011352539062 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 23:25:58.236933 test begin: paddle.Tensor.gather_nd(Tensor([16, 3, 80, 80, 166],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([16, 3, 80, 80, 166],"float32"), Tensor([516, 4],"int64"), ) 	 50997264 	 1730 	 0.018439769744873047 	 135.3007674217224 	 1.1682510375976562e-05 	 0.00022411346435546875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 23:28:14.725690 test begin: paddle.Tensor.gather_nd(Tensor([16, 6, 80, 80, 85],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([16, 6, 80, 80, 85],"float32"), Tensor([516, 4],"int64"), ) 	 52226064 	 1730 	 0.018590927124023438 	 140.89243006706238 	 1.1920928955078125e-05 	 0.0002334117889404297 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 23:30:39.192956 test begin: paddle.Tensor.gather_nd(Tensor([32, 3, 80, 80, 85],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([32, 3, 80, 80, 85],"float32"), Tensor([385, 4],"int64"), ) 	 52225540 	 1730 	 0.018540143966674805 	 116.26044821739197 	 1.2874603271484375e-05 	 0.0002162456512451172 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 23:32:40.219274 test begin: paddle.Tensor.gather_nd(Tensor([32, 3, 80, 80, 85],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([32, 3, 80, 80, 85],"float32"), Tensor([516, 4],"int64"), ) 	 52226064 	 1730 	 0.018846511840820312 	 131.52164697647095 	 1.3828277587890625e-05 	 0.00022077560424804688 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 23:34:55.916816 test begin: paddle.Tensor.gather_nd(Tensor([8, 12, 80, 80, 85],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([8, 12, 80, 80, 85],"float32"), Tensor([385, 4],"int64"), ) 	 52225540 	 1730 	 0.037244319915771484 	 99.40042400360107 	 2.193450927734375e-05 	 0.00021195411682128906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 23:36:38.402592 test begin: paddle.Tensor.gather_nd(Tensor([8, 3, 312, 80, 85],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([8, 3, 312, 80, 85],"float32"), Tensor([385, 4],"int64"), ) 	 50919940 	 1730 	 0.018840789794921875 	 112.35899424552917 	 1.4066696166992188e-05 	 9.202957153320312e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 23:38:34.775778 test begin: paddle.Tensor.gather_nd(Tensor([8, 3, 80, 312, 85],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([8, 3, 80, 312, 85],"float32"), Tensor([385, 4],"int64"), ) 	 50919940 	 1730 	 0.018695831298828125 	 97.66152572631836 	 1.1682510375976562e-05 	 0.00022459030151367188 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 23:40:15.905598 test begin: paddle.Tensor.gather_nd(Tensor([8, 3, 80, 80, 331],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([8, 3, 80, 80, 331],"float32"), Tensor([385, 4],"int64"), ) 	 50843140 	 1730 	 0.01853156089782715 	 97.43893957138062 	 1.33514404296875e-05 	 0.00021648406982421875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-04 23:41:54.474669 test begin: paddle.Tensor.gcd(x=Tensor([127008, 2, 4, 5],"int32"), y=Tensor([127008, 2, 4, 5],"int32"), )
W0804 23:42:06.681622 74217 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.Tensor.gcd 	 paddle.Tensor.gcd(x=Tensor([127008, 2, 4, 5],"int32"), y=Tensor([127008, 2, 4, 5],"int32"), ) 	 10160640 	 1000 	 11.907224893569946 	 0.15894508361816406 	 4.5299530029296875e-05 	 0.14737224578857422 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 23:42:06.872661 test begin: paddle.Tensor.gcd(x=Tensor([2, 4, 635040],"int32"), y=Tensor([2, 4, 635040],"int32"), )
W0804 23:42:18.996814 74223 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.Tensor.gcd 	 paddle.Tensor.gcd(x=Tensor([2, 4, 635040],"int32"), y=Tensor([2, 4, 635040],"int32"), ) 	 10160640 	 1000 	 11.925342082977295 	 0.15880346298217773 	 4.76837158203125e-05 	 0.1475358009338379 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 23:42:19.186403 test begin: paddle.Tensor.gcd(x=Tensor([2, 508032, 5],"int32"), y=Tensor([2, 508032, 5],"int32"), )
W0804 23:42:31.250331 74226 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.Tensor.gcd 	 paddle.Tensor.gcd(x=Tensor([2, 508032, 5],"int32"), y=Tensor([2, 508032, 5],"int32"), ) 	 10160640 	 1000 	 11.936280012130737 	 0.15876460075378418 	 3.409385681152344e-05 	 0.14773297309875488 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 23:42:31.435811 test begin: paddle.Tensor.gcd(x=Tensor([254016, 1, 4, 5],"int32"), y=Tensor([2, 1, 5],"int32"), )
W0804 23:42:53.498574 74234 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.Tensor.gcd 	 paddle.Tensor.gcd(x=Tensor([254016, 1, 4, 5],"int32"), y=Tensor([2, 1, 5],"int32"), ) 	 5080330 	 1000 	 21.89055585861206 	 0.42661237716674805 	 4.601478576660156e-05 	 0.41405344009399414 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 23:42:54.006777 test begin: paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([13001],"int64"), )
[Prof] paddle.Tensor.index_select 	 paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([13001],"int64"), ) 	 50816225 	 1057032 	 10.060079336166382 	 13.890958309173584 	 0.00011682510375976562 	 0.0001671314239501953 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 23:45:58.832681 test begin: paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([18201],"int64"), )
[Prof] paddle.Tensor.index_select 	 paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([18201],"int64"), ) 	 50821425 	 1057032 	 10.807416439056396 	 14.132381439208984 	 0.00010323524475097656 	 0.00011849403381347656 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 23:49:08.329977 test begin: paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([9101],"int64"), )
[Prof] paddle.Tensor.index_select 	 paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([9101],"int64"), ) 	 50812325 	 1057032 	 9.794588565826416 	 13.996029138565063 	 0.00010371208190917969 	 0.00020742416381835938 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 23:52:15.561686 test begin: paddle.Tensor.index_select(Tensor([4004, 12689],"float32"), axis=0, index=Tensor([18201],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe1647a7a90>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:02:22.876580 test begin: paddle.Tensor.index_select(Tensor([4004, 24],"float32"), axis=0, index=Tensor([25401601],"int64"), )
W0805 00:02:23.449474 75147 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe55255af50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:12:27.646471 test begin: paddle.Tensor.index_select(Tensor([454, 111902],"float32"), axis=0, index=Tensor([130],"int64"), )
W0805 00:12:28.591424 75674 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4b40a72e30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:22:32.136313 test begin: paddle.Tensor.index_select(Tensor([454, 111902],"float32"), axis=0, index=Tensor([91],"int64"), )
W0805 00:22:33.142599 76357 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.index_select 	 paddle.Tensor.index_select(Tensor([454, 111902],"float32"), axis=0, index=Tensor([91],"int64"), ) 	 50803599 	 1057032 	 141.37741231918335 	 128.187584400177 	 0.13670039176940918 	 0.12391829490661621 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 00:31:13.214678 test begin: paddle.Tensor.index_select(Tensor([454, 24],"float32"), axis=0, index=Tensor([25401601],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbc112ea980>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 00:41:24.961178 test begin: paddle.Tensor.inner(x=Tensor([2, 1058401, 3, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), )
W0805 00:41:25.644616 77531 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([2, 1058401, 3, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), ) 	 25401744 	 51721 	 100.92385077476501 	 100.83136940002441 	 0.28482604026794434 	 0.284618616104126 	 173.95379614830017 	 167.48373556137085 	 0.38204431533813477 	 0.3672060966491699 	 
2025-08-05 00:50:33.707010 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 2, 1058401, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 2, 1058401, 4],"float64"), ) 	 25401744 	 51721 	 75.04283666610718 	 75.19436717033386 	 0.21180248260498047 	 0.21236753463745117 	 210.5404942035675 	 210.2555229663849 	 0.461789608001709 	 0.46167850494384766 	 
2025-08-05 01:00:09.478597 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 423361, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 423361, 5, 4],"float64"), ) 	 25401780 	 51721 	 94.26737928390503 	 93.00726270675659 	 0.2660841941833496 	 0.26245856285095215 	 176.3510730266571 	 188.79987955093384 	 0.3865969181060791 	 0.41455626487731934 	 
2025-08-05 01:09:26.630122 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([635041, 2, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([635041, 2, 5, 4],"float64"), ) 	 25401760 	 51721 	 75.00156855583191 	 75.16981792449951 	 0.2116842269897461 	 0.2122189998626709 	 210.53516745567322 	 210.22875332832336 	 0.4618661403656006 	 0.461456298828125 	 
2025-08-05 01:19:02.332206 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 846721],"float64"), y=Tensor([3, 2, 5, 846721],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([2, 5, 3, 846721],"float64"), y=Tensor([3, 2, 5, 846721],"float64"), ) 	 50803260 	 51721 	 17.42278480529785 	 17.39259386062622 	 0.17214393615722656 	 0.1717972755432129 	 36.3396680355072 	 38.793890714645386 	 0.3589823246002197 	 0.3831937313079834 	 
2025-08-05 01:20:53.460885 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 635041, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([2, 5, 635041, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), ) 	 25401760 	 51721 	 100.76920533180237 	 101.18600940704346 	 0.2844686508178711 	 0.2855949401855469 	 173.83420491218567 	 167.4263210296631 	 0.38161659240722656 	 0.36705827713012695 	 
2025-08-05 01:30:02.055962 test begin: paddle.Tensor.inner(x=Tensor([2116801, 3, 4],"float64"), y=Tensor([2, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([2116801, 3, 4],"float64"), y=Tensor([2, 5, 4],"float64"), ) 	 25401652 	 51721 	 96.37521529197693 	 97.39263701438904 	 0.2720623016357422 	 0.2703220844268799 	 116.17121410369873 	 120.35250282287598 	 0.2644824981689453 	 0.2639491558074951 	 
2025-08-05 01:37:17.424969 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 2, 1058401, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 2, 1058401, 4],"float64"), ) 	 25401636 	 51721 	 70.30324363708496 	 70.6771171092987 	 0.19842791557312012 	 0.19949865341186523 	 137.1574866771698 	 136.2588086128235 	 0.3009181022644043 	 0.29903697967529297 	 
2025-08-05 01:44:12.855573 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 423361, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 423361, 5, 4],"float64"), ) 	 25401672 	 51721 	 71.22193813323975 	 71.7429358959198 	 0.20102834701538086 	 0.20250940322875977 	 122.43091082572937 	 126.79731059074402 	 0.2685260772705078 	 0.27829766273498535 	 
2025-08-05 01:50:48.109159 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([635041, 2, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([635041, 2, 5, 4],"float64"), ) 	 25401652 	 51721 	 70.69582653045654 	 71.10586881637573 	 0.1995551586151123 	 0.20071125030517578 	 137.16712641716003 	 136.26042795181274 	 0.30092597007751465 	 0.2990274429321289 	 
2025-08-05 01:57:44.345481 test begin: paddle.Tensor.inner(x=Tensor([3, 8467201],"float64"), y=Tensor([3, 2, 5, 8467201],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5e51556cb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 02:07:49.115024 test begin: paddle.Tensor.inner(x=Tensor([3, 846721],"float64"), y=Tensor([3, 2, 5, 846721],"float64"), )
W0805 02:07:49.842059 80847 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([3, 846721],"float64"), y=Tensor([3, 2, 5, 846721],"float64"), ) 	 27941793 	 51721 	 9.946867227554321 	 9.94377589225769 	 0.09829354286193848 	 0.09824752807617188 	 21.438705921173096 	 21.83468747138977 	 0.21177887916564941 	 0.21576261520385742 	 
2025-08-05 02:08:53.623159 test begin: paddle.Tensor.inner(x=Tensor([423361, 5, 3, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([423361, 5, 3, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), ) 	 25401780 	 51721 	 100.74868559837341 	 101.11418604850769 	 0.28455305099487305 	 0.2854037284851074 	 173.98243474960327 	 167.89943313598633 	 0.3826019763946533 	 0.3682701587677002 	 
2025-08-05 02:18:02.932806 test begin: paddle.Tensor.inner(x=Tensor([5, 1270081, 4],"float64"), y=Tensor([2, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([5, 1270081, 4],"float64"), y=Tensor([2, 5, 4],"float64"), ) 	 25401660 	 51721 	 96.33834767341614 	 95.63254499435425 	 0.2718968391418457 	 0.2699432373046875 	 115.92473244667053 	 117.65053987503052 	 0.254488468170166 	 0.2580094337463379 	 
2025-08-05 02:25:11.811767 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 1693441],"float64"), y=Tensor([2, 5, 1693441],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([5, 3, 1693441],"float64"), y=Tensor([2, 5, 1693441],"float64"), ) 	 42336025 	 51721 	 15.34138298034668 	 15.027780294418335 	 0.15153002738952637 	 0.14845871925354004 	 42.33479595184326 	 44.84997248649597 	 0.20904231071472168 	 0.2215282917022705 	 
2025-08-05 02:27:10.478614 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 2540161],"float64"), y=Tensor([2, 5, 2540161],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([5, 3, 2540161],"float64"), y=Tensor([2, 5, 2540161],"float64"), ) 	 63504025 	 51721 	 22.6136314868927 	 21.944541692733765 	 0.22333788871765137 	 0.21689367294311523 	 64.05015659332275 	 68.01274371147156 	 0.21085262298583984 	 0.22388052940368652 	 
2025-08-05 02:30:11.381600 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([1270081, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([1270081, 5, 4],"float64"), ) 	 25401680 	 51721 	 80.57183861732483 	 79.63138389587402 	 0.22740578651428223 	 0.22478318214416504 	 127.84456706047058 	 138.5343198776245 	 0.2803785800933838 	 0.30408692359924316 	 
2025-08-05 02:37:20.708005 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([2, 3175201, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([2, 3175201, 4],"float64"), ) 	 25401668 	 51721 	 80.34382104873657 	 79.78257632255554 	 0.22677183151245117 	 0.22422409057617188 	 144.3214888572693 	 144.635888338089 	 0.31634974479675293 	 0.31808924674987793 	 
2025-08-05 02:44:56.315090 test begin: paddle.Tensor.inner(x=Tensor([6350401, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([6350401, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), ) 	 25401724 	 51721 	 100.66977000236511 	 101.07443070411682 	 0.28412461280822754 	 0.28529834747314453 	 174.06899309158325 	 167.76221323013306 	 0.38267016410827637 	 0.36742067337036133 	 
2025-08-05 02:54:04.579909 test begin: paddle.Tensor.inverse(Tensor([4, 39690, 4, 4],"float64"), )
[Prof] paddle.Tensor.inverse 	 paddle.Tensor.inverse(Tensor([4, 39690, 4, 4],"float64"), ) 	 2540160 	 2707 	 22.506303071975708 	 0.9236831665039062 	 9.846687316894531e-05 	 0.00019359588623046875 	 14.596519470214844 	 5.2991251945495605 	 0.9195890426635742 	 0.28577160835266113 	 
2025-08-05 02:54:48.270984 test begin: paddle.Tensor.inverse(Tensor([70560, 6, 6],"float64"), )
[Prof] paddle.Tensor.inverse 	 paddle.Tensor.inverse(Tensor([70560, 6, 6],"float64"), ) 	 2540160 	 2707 	 10.660973310470581 	 1.0884733200073242 	 9.894371032714844e-05 	 0.00013208389282226562 	 5.9465415477752686 	 4.523461580276489 	 0.5620181560516357 	 0.340944766998291 	 
2025-08-05 02:55:10.617019 test begin: paddle.Tensor.inverse(Tensor([79380, 2, 4, 4],"float64"), )
[Prof] paddle.Tensor.inverse 	 paddle.Tensor.inverse(Tensor([79380, 2, 4, 4],"float64"), ) 	 2540160 	 2707 	 20.030457496643066 	 0.9295644760131836 	 0.00012755393981933594 	 7.319450378417969e-05 	 14.599363803863525 	 5.320964813232422 	 0.9196982383728027 	 0.286942720413208 	 
2025-08-05 02:55:53.261212 test begin: paddle.Tensor.is_complex(Tensor([201, 3, 100, 42337],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3, 100, 42337],"float64"), ) 	 2552921100 	 2814024 	 10.345655679702759 	 4.623692989349365 	 0.00012445449829101562 	 0.0002810955047607422 	 None 	 None 	 None 	 None 	 
2025-08-05 02:57:09.500868 test begin: paddle.Tensor.is_complex(Tensor([201, 3, 105841, 40],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3, 105841, 40],"float64"), ) 	 2552884920 	 2814024 	 11.823249816894531 	 6.224189043045044 	 0.0001442432403564453 	 0.0007381439208984375 	 None 	 None 	 None 	 None 	 
2025-08-05 02:58:44.632493 test begin: paddle.Tensor.is_complex(Tensor([201, 3, 40, 105841],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3, 40, 105841],"float64"), ) 	 2552884920 	 2814024 	 12.8712317943573 	 5.243770360946655 	 0.00017690658569335938 	 0.0001342296600341797 	 None 	 None 	 None 	 None 	 
2025-08-05 03:00:08.581364 test begin: paddle.Tensor.is_complex(Tensor([201, 3, 42337, 100],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3, 42337, 100],"float64"), ) 	 2552921100 	 2814024 	 10.423217058181763 	 4.599348068237305 	 0.00022721290588378906 	 0.00014519691467285156 	 None 	 None 	 None 	 None 	 
2025-08-05 03:01:43.338576 test begin: paddle.Tensor.is_complex(Tensor([201, 3176, 100, 40],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3176, 100, 40],"float64"), ) 	 2553504000 	 2814024 	 10.142358779907227 	 4.602199554443359 	 0.00025725364685058594 	 0.00011229515075683594 	 None 	 None 	 None 	 None 	 
2025-08-05 03:03:06.023644 test begin: paddle.Tensor.is_complex(Tensor([201, 3176, 40, 100],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3176, 40, 100],"float64"), ) 	 2553504000 	 2814024 	 10.390135765075684 	 4.701008558273315 	 7.534027099609375e-05 	 0.0001246929168701172 	 None 	 None 	 None 	 None 	 
2025-08-05 03:04:21.837427 test begin: paddle.Tensor.is_complex(Tensor([211701, 3, 100, 40],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([211701, 3, 100, 40],"float64"), ) 	 2540412000 	 2814024 	 13.179703712463379 	 4.583148241043091 	 9.202957153320312e-05 	 0.0003292560577392578 	 None 	 None 	 None 	 None 	 
2025-08-05 03:05:42.788757 test begin: paddle.Tensor.is_complex(Tensor([211701, 3, 40, 100],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([211701, 3, 40, 100],"float64"), ) 	 2540412000 	 2814024 	 10.013022422790527 	 4.526918172836304 	 6.937980651855469e-05 	 0.00011110305786132812 	 None 	 None 	 None 	 None 	 
2025-08-05 03:06:50.632877 test begin: paddle.Tensor.is_complex(Tensor([301, 100, 84673],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([301, 100, 84673],"float64"), ) 	 2548657300 	 2814024 	 10.775068759918213 	 5.0832788944244385 	 4.5299530029296875e-05 	 0.0003306865692138672 	 None 	 None 	 None 	 None 	 
2025-08-05 03:08:16.240984 test begin: paddle.Tensor.is_complex(Tensor([301, 211681, 40],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([301, 211681, 40],"float64"), ) 	 2548639240 	 2814024 	 11.152920722961426 	 4.767919540405273 	 0.00013065338134765625 	 0.00037097930908203125 	 None 	 None 	 None 	 None 	 
2025-08-05 03:09:40.354906 test begin: paddle.Tensor.is_complex(Tensor([635101, 100, 40],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([635101, 100, 40],"float64"), ) 	 2540404000 	 2814024 	 10.143090009689331 	 4.5538129806518555 	 7.343292236328125e-05 	 8.654594421386719e-05 	 None 	 None 	 None 	 None 	 
2025-08-05 03:10:48.218495 test begin: paddle.Tensor.isclose(x=Tensor([1270081, 4, 5],"float64"), y=Tensor([1270081, 4, 5],"float64"), )
[Prof] paddle.Tensor.isclose 	 paddle.Tensor.isclose(x=Tensor([1270081, 4, 5],"float64"), y=Tensor([1270081, 4, 5],"float64"), ) 	 50803240 	 27577 	 9.99801516532898 	 84.95104122161865 	 0.3704409599304199 	 0.24170279502868652 	 None 	 None 	 None 	 None 	 
2025-08-05 03:12:24.662383 test begin: paddle.Tensor.isclose(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), )
[Prof] paddle.Tensor.isclose 	 paddle.Tensor.isclose(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), ) 	 50803202 	 27577 	 9.99959135055542 	 84.94758105278015 	 0.3705277442932129 	 0.24171757698059082 	 None 	 None 	 None 	 None 	 
2025-08-05 03:14:02.123571 test begin: paddle.Tensor.isclose(x=Tensor([3, 1693441, 5],"float64"), y=Tensor([3, 1693441, 5],"float64"), )
[Prof] paddle.Tensor.isclose 	 paddle.Tensor.isclose(x=Tensor([3, 1693441, 5],"float64"), y=Tensor([3, 1693441, 5],"float64"), ) 	 50803230 	 27577 	 10.0020751953125 	 84.94767141342163 	 0.37055134773254395 	 0.24170994758605957 	 None 	 None 	 None 	 None 	 
2025-08-05 03:15:38.583021 test begin: paddle.Tensor.isclose(x=Tensor([3, 4, 2116801],"float64"), y=Tensor([3, 4, 2116801],"float64"), )
[Prof] paddle.Tensor.isclose 	 paddle.Tensor.isclose(x=Tensor([3, 4, 2116801],"float64"), y=Tensor([3, 4, 2116801],"float64"), ) 	 50803224 	 27577 	 9.999770402908325 	 84.94714307785034 	 0.37047886848449707 	 0.24169564247131348 	 None 	 None 	 None 	 None 	 
2025-08-05 03:17:17.461120 test begin: paddle.Tensor.isclose(x=Tensor([50803201],"float32"), y=Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.isclose 	 paddle.Tensor.isclose(x=Tensor([50803201],"float32"), y=Tensor([50803201],"float32"), ) 	 101606402 	 27577 	 11.714808940887451 	 91.29371953010559 	 0.4340834617614746 	 0.25986194610595703 	 None 	 None 	 None 	 None 	 
2025-08-05 03:19:02.212638 test begin: paddle.Tensor.isnan(Tensor([25401601],"float64"), )
[Prof] paddle.Tensor.isnan 	 paddle.Tensor.isnan(Tensor([25401601],"float64"), ) 	 25401601 	 55184 	 10.00853180885315 	 9.300090789794922 	 0.18538570404052734 	 0.17223405838012695 	 None 	 None 	 None 	 None 	 
2025-08-05 03:19:22.132776 test begin: paddle.Tensor.isnan(Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.isnan 	 paddle.Tensor.isnan(Tensor([50803201],"float32"), ) 	 50803201 	 55184 	 12.891873121261597 	 11.03246545791626 	 0.2387557029724121 	 0.18993806838989258 	 None 	 None 	 None 	 None 	 
2025-08-05 03:19:48.314567 test begin: paddle.Tensor.item(Tensor([201, 1, 12700801],"int64"), 0, )
[Prof] paddle.Tensor.item 	 paddle.Tensor.item(Tensor([201, 1, 12700801],"int64"), 0, ) 	 2552861001 	 533796 	 10.172586679458618 	 14.827528476715088 	 0.00011014938354492188 	 0.0002307891845703125 	 None 	 None 	 None 	 None 	 combined
2025-08-05 03:21:08.836772 test begin: paddle.Tensor.item(Tensor([201, 12700801, 1],"int64"), 0, )
[Prof] paddle.Tensor.item 	 paddle.Tensor.item(Tensor([201, 12700801, 1],"int64"), 0, ) 	 2552861001 	 533796 	 10.049490690231323 	 14.730329990386963 	 4.458427429199219e-05 	 6.961822509765625e-05 	 None 	 None 	 None 	 None 	 combined
2025-08-05 03:22:15.207256 test begin: paddle.Tensor.item(Tensor([2540160101, 1, 1],"int64"), 0, )
[Prof] paddle.Tensor.item 	 paddle.Tensor.item(Tensor([2540160101, 1, 1],"int64"), 0, ) 	 2540160101 	 533796 	 10.1673424243927 	 14.777222156524658 	 4.553794860839844e-05 	 0.00024509429931640625 	 None 	 None 	 None 	 None 	 combined
2025-08-05 03:23:25.624066 test begin: paddle.Tensor.kthvalue(Tensor([2, 200, 127009],"float32"), k=200, axis=1, )
[Prof] paddle.Tensor.kthvalue 	 paddle.Tensor.kthvalue(Tensor([2, 200, 127009],"float32"), k=200, axis=1, ) 	 50803600 	 1495 	 9.97473430633545 	 17.356276750564575 	 1.7014663219451904 	 11.863393783569336 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:24:00.656933 test begin: paddle.Tensor.kthvalue(Tensor([2, 2540161, 10],"float32"), k=200, axis=1, )
[Prof] paddle.Tensor.kthvalue 	 paddle.Tensor.kthvalue(Tensor([2, 2540161, 10],"float32"), k=200, axis=1, ) 	 50803220 	 1495 	 55.84718894958496 	 50.99065899848938 	 9.526464223861694 	 34.85796928405762 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:25:54.790329 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 317521],"float64"), y=Tensor([4, 5, 4, 317521],"float64"), weight=0.0, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 5, 4, 317521],"float64"), y=Tensor([4, 5, 4, 317521],"float64"), weight=0.0, ) 	 50803360 	 22191 	 9.989433765411377 	 9.87044644355774 	 0.23000574111938477 	 0.4545295238494873 	 10.656196117401123 	 13.228832006454468 	 0.4906959533691406 	 0.30465030670166016 	 
2025-08-05 03:26:40.614455 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 317521],"float64"), y=Tensor([4, 5, 4, 317521],"float64"), weight=0.5, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 5, 4, 317521],"float64"), y=Tensor([4, 5, 4, 317521],"float64"), weight=0.5, ) 	 50803360 	 22191 	 9.98975419998169 	 9.868277072906494 	 0.22999167442321777 	 0.4544956684112549 	 10.655811309814453 	 13.229446649551392 	 0.4907381534576416 	 0.3045516014099121 	 
2025-08-05 03:27:26.289904 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 317521],"float64"), y=Tensor([4, 5, 4, 317521],"float64"), weight=1.0, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 5, 4, 317521],"float64"), y=Tensor([4, 5, 4, 317521],"float64"), weight=1.0, ) 	 50803360 	 22191 	 9.989799499511719 	 9.871601819992065 	 0.2300255298614502 	 0.45447254180908203 	 10.65562915802002 	 13.229467868804932 	 0.49066877365112305 	 0.30458664894104004 	 
2025-08-05 03:28:11.724527 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 423361, 3],"float64"), y=Tensor([4, 5, 423361, 3],"float64"), weight=0.0, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 5, 423361, 3],"float64"), y=Tensor([4, 5, 423361, 3],"float64"), weight=0.0, ) 	 50803320 	 22191 	 9.993982076644897 	 9.872063875198364 	 0.23003482818603516 	 0.45450353622436523 	 10.673105716705322 	 13.22936201095581 	 0.49152040481567383 	 0.30461859703063965 	 
2025-08-05 03:28:57.203507 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 423361, 3],"float64"), y=Tensor([4, 5, 423361, 3],"float64"), weight=0.5, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 5, 423361, 3],"float64"), y=Tensor([4, 5, 423361, 3],"float64"), weight=0.5, ) 	 50803320 	 22191 	 9.991852283477783 	 9.870750904083252 	 0.23001790046691895 	 0.45448827743530273 	 10.672918796539307 	 13.228782415390015 	 0.4915168285369873 	 0.30463576316833496 	 
2025-08-05 03:29:42.655864 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 423361, 3],"float64"), y=Tensor([4, 5, 423361, 3],"float64"), weight=1.0, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 5, 423361, 3],"float64"), y=Tensor([4, 5, 423361, 3],"float64"), weight=1.0, ) 	 50803320 	 22191 	 9.992626667022705 	 9.867908716201782 	 0.23004865646362305 	 0.45441365242004395 	 10.673049926757812 	 13.22947907447815 	 0.4915761947631836 	 0.30458760261535645 	 
2025-08-05 03:30:28.103716 test begin: paddle.Tensor.lerp(x=Tensor([4, 529201, 4, 3],"float64"), y=Tensor([4, 529201, 4, 3],"float64"), weight=0.0, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 529201, 4, 3],"float64"), y=Tensor([4, 529201, 4, 3],"float64"), weight=0.0, ) 	 50803296 	 22191 	 9.991577625274658 	 9.867180824279785 	 0.23001885414123535 	 0.45452427864074707 	 10.673503398895264 	 13.229251623153687 	 0.49172329902648926 	 0.304563045501709 	 
2025-08-05 03:31:13.550971 test begin: paddle.Tensor.lerp(x=Tensor([4, 529201, 4, 3],"float64"), y=Tensor([4, 529201, 4, 3],"float64"), weight=0.5, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 529201, 4, 3],"float64"), y=Tensor([4, 529201, 4, 3],"float64"), weight=0.5, ) 	 50803296 	 22191 	 9.992064476013184 	 9.875598430633545 	 0.2301044464111328 	 0.4544346332550049 	 10.673237562179565 	 13.228795766830444 	 0.4915781021118164 	 0.30458688735961914 	 
2025-08-05 03:32:00.841218 test begin: paddle.Tensor.lerp(x=Tensor([4, 529201, 4, 3],"float64"), y=Tensor([4, 529201, 4, 3],"float64"), weight=1.0, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 529201, 4, 3],"float64"), y=Tensor([4, 529201, 4, 3],"float64"), weight=1.0, ) 	 50803296 	 22191 	 9.993762969970703 	 9.867652416229248 	 0.2301943302154541 	 0.4544527530670166 	 10.673349618911743 	 13.228882551193237 	 0.4915604591369629 	 0.3045821189880371 	 
2025-08-05 03:32:46.294153 test begin: paddle.Tensor.lerp(x=Tensor([423361, 5, 4, 3],"float64"), y=Tensor([423361, 5, 4, 3],"float64"), weight=0.0, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([423361, 5, 4, 3],"float64"), y=Tensor([423361, 5, 4, 3],"float64"), weight=0.0, ) 	 50803320 	 22191 	 9.991275548934937 	 9.86724853515625 	 0.23003721237182617 	 0.4544072151184082 	 10.67352557182312 	 13.229246854782104 	 0.49144697189331055 	 0.30452656745910645 	 
2025-08-05 03:33:31.765455 test begin: paddle.Tensor.lerp(x=Tensor([423361, 5, 4, 3],"float64"), y=Tensor([423361, 5, 4, 3],"float64"), weight=0.5, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([423361, 5, 4, 3],"float64"), y=Tensor([423361, 5, 4, 3],"float64"), weight=0.5, ) 	 50803320 	 22191 	 9.991882562637329 	 9.869746923446655 	 0.2300117015838623 	 0.4544403553009033 	 10.673120021820068 	 13.228276252746582 	 0.49163389205932617 	 0.3046720027923584 	 
2025-08-05 03:34:17.964625 test begin: paddle.Tensor.lerp(x=Tensor([423361, 5, 4, 3],"float64"), y=Tensor([423361, 5, 4, 3],"float64"), weight=1.0, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([423361, 5, 4, 3],"float64"), y=Tensor([423361, 5, 4, 3],"float64"), weight=1.0, ) 	 50803320 	 22191 	 9.991325616836548 	 9.873069524765015 	 0.23002910614013672 	 0.45443177223205566 	 10.673298835754395 	 13.229623556137085 	 0.4915645122528076 	 0.30460333824157715 	 
2025-08-05 03:35:04.216460 test begin: paddle.Tensor.less(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.less 	 paddle.Tensor.less(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), ) 	 101606420 	 30607 	 9.996372938156128 	 10.030492305755615 	 0.33385348320007324 	 0.3347811698913574 	 None 	 None 	 None 	 None 	 
2025-08-05 03:35:26.583232 test begin: paddle.Tensor.less(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float32"), )
[Prof] paddle.Tensor.less 	 paddle.Tensor.less(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float32"), ) 	 101607424 	 30607 	 10.005078792572021 	 11.301134586334229 	 0.3340928554534912 	 0.3347632884979248 	 None 	 None 	 None 	 None 	 
2025-08-05 03:35:50.964762 test begin: paddle.Tensor.lgamma(Tensor([100, 100, 2541],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([100, 100, 2541],"float64"), ) 	 25410000 	 14030 	 9.998262643814087 	 9.68476414680481 	 0.7283511161804199 	 0.7051270008087158 	 19.43425464630127 	 22.255115509033203 	 1.415675163269043 	 0.8105380535125732 	 
2025-08-05 03:36:53.768936 test begin: paddle.Tensor.lgamma(Tensor([100, 2541, 100],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([100, 2541, 100],"float64"), ) 	 25410000 	 14030 	 9.99767017364502 	 9.687074661254883 	 0.7282953262329102 	 0.705366849899292 	 19.432424306869507 	 22.257919788360596 	 1.4154696464538574 	 0.8106222152709961 	 
2025-08-05 03:37:56.557578 test begin: paddle.Tensor.lgamma(Tensor([2541, 100, 100],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([2541, 100, 100],"float64"), ) 	 25410000 	 14030 	 9.997212648391724 	 9.677315950393677 	 0.7281560897827148 	 0.7048654556274414 	 19.43666124343872 	 22.2481472492218 	 1.4158501625061035 	 0.8102893829345703 	 
2025-08-05 03:38:59.846418 test begin: paddle.Tensor.lgamma(Tensor([453601, 7, 8],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([453601, 7, 8],"float64"), ) 	 25401656 	 14030 	 9.993247032165527 	 9.677429437637329 	 0.7279782295227051 	 0.7049233913421631 	 19.373370885849 	 22.24703884124756 	 1.4111921787261963 	 0.8102662563323975 	 
2025-08-05 03:40:02.328059 test begin: paddle.Tensor.lgamma(Tensor([45361, 7, 8, 10],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([45361, 7, 8, 10],"float64"), ) 	 25402160 	 14030 	 9.993598222732544 	 9.676826000213623 	 0.7279443740844727 	 0.7049040794372559 	 19.372584104537964 	 22.245463132858276 	 1.4111323356628418 	 0.8101768493652344 	 
2025-08-05 03:41:04.726168 test begin: paddle.Tensor.lgamma(Tensor([5, 635041, 8],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([5, 635041, 8],"float64"), ) 	 25401640 	 14030 	 9.99256420135498 	 9.675941467285156 	 0.7278695106506348 	 0.7047393321990967 	 19.371243476867676 	 22.242888927459717 	 1.4110352993011475 	 0.8101263046264648 	 
2025-08-05 03:42:07.134195 test begin: paddle.Tensor.lgamma(Tensor([5, 63505, 8, 10],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([5, 63505, 8, 10],"float64"), ) 	 25402000 	 14030 	 9.991412162780762 	 9.685210466384888 	 0.7277984619140625 	 0.7050883769989014 	 19.385499954223633 	 22.25061011314392 	 1.4121267795562744 	 0.8104276657104492 	 
2025-08-05 03:43:10.171150 test begin: paddle.Tensor.lgamma(Tensor([5, 7, 725761],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([5, 7, 725761],"float64"), ) 	 25401635 	 14030 	 9.996122598648071 	 9.70139741897583 	 0.728201150894165 	 0.7052304744720459 	 19.378127336502075 	 22.25139021873474 	 1.411595344543457 	 0.8104443550109863 	 
2025-08-05 03:44:13.686605 test begin: paddle.Tensor.lgamma(Tensor([5, 7, 72577, 10],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([5, 7, 72577, 10],"float64"), ) 	 25401950 	 14030 	 9.996820449829102 	 9.678951978683472 	 0.7282557487487793 	 0.7050909996032715 	 19.40327477455139 	 22.249986171722412 	 1.413466215133667 	 0.810420036315918 	 
2025-08-05 03:45:18.105838 test begin: paddle.Tensor.lgamma(Tensor([5, 7, 8, 90721],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([5, 7, 8, 90721],"float64"), ) 	 25401880 	 14030 	 9.998079776763916 	 9.67851972579956 	 0.7282092571258545 	 0.7050001621246338 	 19.40082049369812 	 22.24893355369568 	 1.4132373332977295 	 0.8103189468383789 	 
2025-08-05 03:46:21.867535 test begin: paddle.Tensor.log(Tensor([100, 200, 1271],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([100, 200, 1271],"float64"), ) 	 25420000 	 33839 	 10.343555927276611 	 10.362136363983154 	 0.31234312057495117 	 0.31303858757019043 	 15.151302576065063 	 15.189540386199951 	 0.45755648612976074 	 0.45871496200561523 	 
2025-08-05 03:47:14.237521 test begin: paddle.Tensor.log(Tensor([100, 2541, 100],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([100, 2541, 100],"float64"), ) 	 25410000 	 33839 	 10.332818746566772 	 10.369955062866211 	 0.3120229244232178 	 0.312868595123291 	 15.146942615509033 	 15.183922290802002 	 0.45764660835266113 	 0.45855164527893066 	 
2025-08-05 03:48:07.238481 test begin: paddle.Tensor.log(Tensor([10000, 5, 509],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([10000, 5, 509],"float64"), ) 	 25450000 	 33839 	 10.35164475440979 	 10.376392126083374 	 0.31264305114746094 	 0.31329345703125 	 15.172449827194214 	 15.203464269638062 	 0.45830416679382324 	 0.4591796398162842 	 
2025-08-05 03:48:59.658834 test begin: paddle.Tensor.log(Tensor([10000, 847, 3],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([10000, 847, 3],"float64"), ) 	 25410000 	 33839 	 10.3336660861969 	 10.358434438705444 	 0.3121209144592285 	 0.31295132637023926 	 15.14766001701355 	 15.185515403747559 	 0.45755481719970703 	 0.4585750102996826 	 
2025-08-05 03:49:51.949982 test begin: paddle.Tensor.log(Tensor([1271, 200, 100],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([1271, 200, 100],"float64"), ) 	 25420000 	 33839 	 10.342597961425781 	 10.36196780204773 	 0.3123509883880615 	 0.3130013942718506 	 15.151948928833008 	 15.189714670181274 	 0.45778608322143555 	 0.4587850570678711 	 
2025-08-05 03:50:44.288064 test begin: paddle.Tensor.log(Tensor([1693441, 5, 3],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([1693441, 5, 3],"float64"), ) 	 25401615 	 33839 	 10.332612991333008 	 10.368280172348022 	 0.3120884895324707 	 0.31280994415283203 	 15.145135164260864 	 15.178229570388794 	 0.4572460651397705 	 0.4583778381347656 	 
2025-08-05 03:51:38.235371 test begin: paddle.Tensor.log(Tensor([4800, 10585],"float32"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([4800, 10585],"float32"), ) 	 50808000 	 33839 	 9.99936580657959 	 10.061878681182861 	 0.30205726623535156 	 0.3039562702178955 	 15.228831768035889 	 15.207560539245605 	 0.4599473476409912 	 0.45935750007629395 	 
2025-08-05 03:52:30.671284 test begin: paddle.Tensor.log(Tensor([503002, 101],"float32"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([503002, 101],"float32"), ) 	 50803202 	 33839 	 10.002246141433716 	 10.060969114303589 	 0.302079439163208 	 0.3038010597229004 	 15.225829362869263 	 15.206608533859253 	 0.45984935760498047 	 0.45927953720092773 	 
2025-08-05 03:53:23.052766 test begin: paddle.Tensor.log10(Tensor([101811, 499],"float32"), )
[Prof] paddle.Tensor.log10 	 paddle.Tensor.log10(Tensor([101811, 499],"float32"), ) 	 50803689 	 33813 	 9.992981195449829 	 10.056298732757568 	 0.3020448684692383 	 0.3039233684539795 	 15.208614349365234 	 25.205315828323364 	 0.459705114364624 	 0.38089942932128906 	 
2025-08-05 03:54:26.941392 test begin: paddle.Tensor.log10(Tensor([80, 635041],"float32"), )
[Prof] paddle.Tensor.log10 	 paddle.Tensor.log10(Tensor([80, 635041],"float32"), ) 	 50803280 	 33813 	 9.986658811569214 	 10.051584959030151 	 0.3018338680267334 	 0.30380725860595703 	 15.194012641906738 	 25.205132961273193 	 0.45923852920532227 	 0.38088130950927734 	 
2025-08-05 03:55:29.683588 test begin: paddle.Tensor.log1p(Tensor([16934401, 3],"float32"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([16934401, 3],"float32"), ) 	 50803203 	 33826 	 9.996551752090454 	 10.100637435913086 	 0.3021049499511719 	 0.30518007278442383 	 15.208455562591553 	 25.2148220539093 	 0.45937156677246094 	 0.3808939456939697 	 
2025-08-05 03:56:32.062649 test begin: paddle.Tensor.log1p(Tensor([2, 25401601],"float32"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([2, 25401601],"float32"), ) 	 50803202 	 33826 	 9.996214151382446 	 10.100979566574097 	 0.30205750465393066 	 0.3052496910095215 	 15.208454847335815 	 25.214601516723633 	 0.4594886302947998 	 0.38088274002075195 	 
2025-08-05 03:57:34.425971 test begin: paddle.Tensor.log1p(Tensor([2, 3, 4233601],"float64"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([2, 3, 4233601],"float64"), ) 	 25401606 	 33826 	 10.32085108757019 	 11.353788137435913 	 0.31179237365722656 	 0.34281301498413086 	 15.13761043548584 	 25.184406995773315 	 0.4574105739593506 	 0.38051342964172363 	 
2025-08-05 03:58:38.152106 test begin: paddle.Tensor.log1p(Tensor([2, 6350401, 2],"float64"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([2, 6350401, 2],"float64"), ) 	 25401604 	 33826 	 10.320655584335327 	 11.349133968353271 	 0.3117690086364746 	 0.3428995609283447 	 15.139384269714355 	 25.184704780578613 	 0.45703840255737305 	 0.38040637969970703 	 
2025-08-05 03:59:41.707387 test begin: paddle.Tensor.log1p(Tensor([25401601],"float64"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([25401601],"float64"), ) 	 25401601 	 33826 	 10.321939945220947 	 11.351532936096191 	 0.3118617534637451 	 0.342968225479126 	 15.138108015060425 	 25.184237003326416 	 0.45738959312438965 	 0.38041210174560547 	 
2025-08-05 04:00:45.286741 test begin: paddle.Tensor.log1p(Tensor([4233601, 3, 2],"float64"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([4233601, 3, 2],"float64"), ) 	 25401606 	 33826 	 10.32202935218811 	 12.197651386260986 	 0.3118550777435303 	 0.34423351287841797 	 15.142678022384644 	 25.190078735351562 	 0.45743393898010254 	 0.38045406341552734 	 
2025-08-05 04:01:49.290659 test begin: paddle.Tensor.logical_and(Tensor([50803201],"bool"), Tensor([50803201],"bool"), )
[Prof] paddle.Tensor.logical_and 	 paddle.Tensor.logical_and(Tensor([50803201],"bool"), Tensor([50803201],"bool"), ) 	 101606402 	 84274 	 9.985284328460693 	 9.764054298400879 	 0.1210470199584961 	 0.118408203125 	 None 	 None 	 None 	 None 	 
2025-08-05 04:02:10.571855 test begin: paddle.Tensor.logical_not(Tensor([508032010],"bool"), )
[Prof] paddle.Tensor.logical_not 	 paddle.Tensor.logical_not(Tensor([508032010],"bool"), ) 	 508032010 	 12687 	 10.00055718421936 	 9.475560665130615 	 0.8053877353668213 	 0.7631347179412842 	 None 	 None 	 None 	 None 	 
2025-08-05 04:02:40.959636 test begin: paddle.Tensor.logical_or(Tensor([50803201],"bool"), Tensor([50803201],"bool"), )
[Prof] paddle.Tensor.logical_or 	 paddle.Tensor.logical_or(Tensor([50803201],"bool"), Tensor([50803201],"bool"), ) 	 101606402 	 84635 	 9.985876083374023 	 9.78171992301941 	 0.1205606460571289 	 0.11806321144104004 	 None 	 None 	 None 	 None 	 
2025-08-05 04:03:02.139901 test begin: paddle.Tensor.logit(x=Tensor([4, 3, 423361, 5],"float64"), eps=0.2, )
[Prof] paddle.Tensor.logit 	 paddle.Tensor.logit(x=Tensor([4, 3, 423361, 5],"float64"), eps=0.2, ) 	 25401660 	 30800 	 10.002056360244751 	 9.326509714126587 	 0.3317878246307373 	 0.3093717098236084 	 13.648984909057617 	 13.823176622390747 	 0.4527590274810791 	 0.4585404396057129 	 
2025-08-05 04:03:50.950948 test begin: paddle.Tensor.logit(x=Tensor([4, 635041, 2, 5],"float64"), eps=0.2, )
[Prof] paddle.Tensor.logit 	 paddle.Tensor.logit(x=Tensor([4, 635041, 2, 5],"float64"), eps=0.2, ) 	 25401640 	 30800 	 10.002481937408447 	 9.321955680847168 	 0.33318614959716797 	 0.3091719150543213 	 13.648581743240356 	 13.819586277008057 	 0.45276331901550293 	 0.45845508575439453 	 
2025-08-05 04:04:40.045924 test begin: paddle.Tensor.logit(x=Tensor([846721, 3, 2, 5],"float64"), eps=0.2, )
[Prof] paddle.Tensor.logit 	 paddle.Tensor.logit(x=Tensor([846721, 3, 2, 5],"float64"), eps=0.2, ) 	 25401630 	 30800 	 10.018059730529785 	 9.322015047073364 	 0.333357572555542 	 0.30922913551330566 	 13.677947998046875 	 13.820680379867554 	 0.4536926746368408 	 0.45842504501342773 	 
2025-08-05 04:05:27.996990 test begin: paddle.Tensor.lu(Tensor([1693, 300],"float32"), )
/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:924: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
LU, pivots = torch.lu(A, compute_pivots)
should be replaced with
LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
and
LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
should be replaced with
LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2055.)
  LU, pivots, infos = torch._lu_with_info(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6b4ee62c20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 04:15:32.833838 test begin: paddle.Tensor.lu(Tensor([216, 3, 2, 2],"float64"), )
W0805 04:15:33.046418 97152 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6a40edae30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 04:25:37.834835 test begin: paddle.Tensor.lu(Tensor([3, 3, 422],"float64"), )
W0805 04:25:41.642131 106192 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:924: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
LU, pivots = torch.lu(A, compute_pivots)
should be replaced with
LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
and
LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
should be replaced with
LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2055.)
  LU, pivots, infos = torch._lu_with_info(
[Prof] paddle.Tensor.lu 	 paddle.Tensor.lu(Tensor([3, 3, 422],"float64"), ) 	 3798 	 64078 	 8.778301000595093 	 7.9075891971588135 	 0.00015115737915039062 	 0.00018262863159179688 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:26:22.518623 test begin: paddle.Tensor.lu(Tensor([301, 1193],"float32"), )
[Prof] paddle.Tensor.lu 	 paddle.Tensor.lu(Tensor([301, 1193],"float32"), ) 	 359093 	 64078 	 87.44525361061096 	 317.7717020511627 	 0.00014472007751464844 	 0.0004971027374267578 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:33:43.325085 test begin: paddle.Tensor.lu(Tensor([301, 422, 3],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f198dd1bd90>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 04:43:48.188108 test begin: paddle.Tensor.lu(Tensor([4, 187, 2, 2],"float64"), )
W0805 04:43:48.388157 122892 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8c44dfb010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 04:53:52.923714 test begin: paddle.Tensor.lu(Tensor([4, 3, 158, 2],"float64"), )
W0805 04:53:53.169236 139805 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:924: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
LU, pivots = torch.lu(A, compute_pivots)
should be replaced with
LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
and
LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
should be replaced with
LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2055.)
  LU, pivots, infos = torch._lu_with_info(
[Prof] paddle.Tensor.lu 	 paddle.Tensor.lu(Tensor([4, 3, 158, 2],"float64"), ) 	 3792 	 64078 	 18.82943058013916 	 6.966836214065552 	 0.00014352798461914062 	 0.0002205371856689453 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:54:48.777849 test begin: paddle.Tensor.lu(Tensor([4, 3, 2, 158],"float64"), )
[Prof] paddle.Tensor.lu 	 paddle.Tensor.lu(Tensor([4, 3, 2, 158],"float64"), ) 	 3792 	 64078 	 24.794960975646973 	 10.022104978561401 	 0.0001277923583984375 	 0.00011110305786132812 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 04:55:53.098976 test begin: paddle.Tensor.lu(Tensor([522, 3, 3],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff1381ebf70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 05:05:58.039761 test begin: paddle.Tensor.masked_fill(Tensor([1, 198451, 256],"float32"), Tensor([1, 198451, 1],"bool"), 0.0, )
W0805 05:05:59.238893 161017 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 198451, 256],"float32"), Tensor([1, 198451, 1],"bool"), 0.0, ) 	 51001907 	 70418 	 10.108075857162476 	 43.54916715621948 	 0.04887723922729492 	 0.21045756340026855 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:07:04.209733 test begin: paddle.Tensor.masked_fill(Tensor([1, 36828, 1380],"float32"), Tensor([1, 36828, 1380],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 36828, 1380],"float32"), Tensor([1, 36828, 1380],"bool"), 0.0, ) 	 101645280 	 70418 	 26.51232647895813 	 45.83106851577759 	 0.09622788429260254 	 0.2214951515197754 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:08:40.110747 test begin: paddle.Tensor.masked_fill(Tensor([1, 36828, 1380],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 36828, 1380],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, ) 	 50859468 	 70418 	 9.997419357299805 	 43.742157220840454 	 0.04834890365600586 	 0.21137404441833496 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:09:49.168574 test begin: paddle.Tensor.masked_fill(Tensor([1, 38367, 1325],"float32"), Tensor([1, 38367, 1325],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 38367, 1325],"float32"), Tensor([1, 38367, 1325],"bool"), 0.0, ) 	 101672550 	 70418 	 26.53698182106018 	 45.90820574760437 	 0.0962824821472168 	 0.22185540199279785 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:11:27.022817 test begin: paddle.Tensor.masked_fill(Tensor([1, 38367, 1325],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 38367, 1325],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, ) 	 50874642 	 70418 	 17.354993104934692 	 43.756420850753784 	 0.08398580551147461 	 0.211381196975708 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:12:48.455309 test begin: paddle.Tensor.masked_fill(Tensor([1, 8550, 5942],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 8550, 5942],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, ) 	 50812650 	 70418 	 9.990540027618408 	 43.42886424064636 	 0.04833555221557617 	 0.2098076343536377 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:13:55.027364 test begin: paddle.Tensor.masked_fill(Tensor([1, 8550, 5942],"float32"), Tensor([1, 8550, 5942],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 8550, 5942],"float32"), Tensor([1, 8550, 5942],"bool"), 0.0, ) 	 101608200 	 70418 	 26.509256839752197 	 45.80963897705078 	 0.09749150276184082 	 0.22138476371765137 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:15:29.635831 test begin: paddle.Tensor.masked_fill(Tensor([24, 8550, 256],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([24, 8550, 256],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, ) 	 52539750 	 70418 	 32.16504120826721 	 44.93895077705383 	 0.11684179306030273 	 0.32569193840026855 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:17:21.096570 test begin: paddle.Tensor.masked_fill(Tensor([24, 8550, 256],"float32"), Tensor([24, 8550, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([24, 8550, 256],"float32"), Tensor([24, 8550, 1],"bool"), 0.0, ) 	 52736400 	 70418 	 10.36575984954834 	 44.86349081993103 	 0.05014348030090332 	 0.32526350021362305 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:18:29.126875 test begin: paddle.Tensor.masked_fill(Tensor([6, 36828, 256],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([6, 36828, 256],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, ) 	 56604636 	 70418 	 34.56325936317444 	 48.49409461021423 	 0.12552952766418457 	 0.35153913497924805 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:20:28.412430 test begin: paddle.Tensor.masked_fill(Tensor([6, 36828, 256],"float32"), Tensor([6, 36828, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([6, 36828, 256],"float32"), Tensor([6, 36828, 1],"bool"), 0.0, ) 	 56788776 	 70418 	 11.116878271102905 	 48.22649335861206 	 0.05378317832946777 	 0.34981632232666016 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:21:42.082273 test begin: paddle.Tensor.masked_fill(Tensor([6, 38367, 256],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([6, 38367, 256],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, ) 	 58970079 	 70418 	 36.03579783439636 	 50.64252519607544 	 0.1323695182800293 	 0.24471759796142578 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:23:46.226545 test begin: paddle.Tensor.masked_fill(Tensor([6, 38367, 256],"float32"), Tensor([6, 38367, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([6, 38367, 256],"float32"), Tensor([6, 38367, 1],"bool"), 0.0, ) 	 59161914 	 70418 	 11.619345664978027 	 50.38594651222229 	 0.05619096755981445 	 0.24351859092712402 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:25:01.603703 test begin: paddle.Tensor.masked_select(Tensor([1016065, 50],"float32"), Tensor([1016065, 50],"bool"), )
[Prof] paddle.Tensor.masked_select 	 paddle.Tensor.masked_select(Tensor([1016065, 50],"float32"), Tensor([1016065, 50],"bool"), ) 	 101606500 	 7203 	 18.948495388031006 	 17.629940509796143 	 0.0016307830810546875 	 0.002329111099243164 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:25:59.042790 test begin: paddle.Tensor.masked_select(Tensor([15000, 3387],"float32"), Tensor([15000, 3387],"bool"), )
[Prof] paddle.Tensor.masked_select 	 paddle.Tensor.masked_select(Tensor([15000, 3387],"float32"), Tensor([15000, 3387],"bool"), ) 	 101610000 	 7203 	 9.907803297042847 	 17.667953729629517 	 0.0008544921875 	 0.002332925796508789 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:26:41.180982 test begin: paddle.Tensor.masked_select(Tensor([50803201],"float32"), Tensor([50803201],"bool"), )
[Prof] paddle.Tensor.masked_select 	 paddle.Tensor.masked_select(Tensor([50803201],"float32"), Tensor([50803201],"bool"), ) 	 101606402 	 7203 	 34.643150091171265 	 8.13724398612976 	 0.002925395965576172 	 0.0010361671447753906 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:28:01.876093 test begin: paddle.Tensor.masked_select(Tensor([60000, 847],"float32"), Tensor([60000, 847],"bool"), )
[Prof] paddle.Tensor.masked_select 	 paddle.Tensor.masked_select(Tensor([60000, 847],"float32"), Tensor([60000, 847],"bool"), ) 	 101640000 	 7203 	 9.926648616790771 	 17.547462940216064 	 0.0008459091186523438 	 0.002313375473022461 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 05:28:42.255408 test begin: paddle.Tensor.matmul(Tensor([110, 12, 197, 197],"float32"), Tensor([110, 12, 197, 64],"float32"), )
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([110, 12, 197, 197],"float32"), Tensor([110, 12, 197, 64],"float32"), ) 	 67870440 	 9746 	 10.019793033599854 	 10.018691301345825 	 1.0500757694244385 	 1.0499930381774902 	 17.095051050186157 	 17.09308958053589 	 0.895841121673584 	 0.8957066535949707 	 
2025-08-05 05:29:38.085051 test begin: paddle.Tensor.matmul(Tensor([124, 16, 100, 257],"float32"), Tensor([124, 16, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([124, 16, 100, 257],"float32"), Tensor([124, 16, 257, 64],"float32"), ) 	 83621632 	 9746 	 10.00327730178833 	 10.002162218093872 	 1.0494685173034668 	 1.0479459762573242 	 20.95984983444214 	 20.026684522628784 	 1.9684028625488281 	 1.0491881370544434 	 
2025-08-05 05:30:42.263725 test begin: paddle.Tensor.matmul(Tensor([124, 16, 257, 257],"float32"), Tensor([124, 16, 257, 100],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([124, 16, 257, 257],"float32"), Tensor([124, 16, 257, 100],"float32"), ) 	 182030016 	 9746 	 29.211010932922363 	 29.17882251739502 	 3.0624279975891113 	 3.0611398220062256 	 65.09742259979248 	 65.05072736740112 	 3.4135689735412598 	 3.4118082523345947 	 
2025-08-05 05:33:54.736063 test begin: paddle.Tensor.matmul(Tensor([124, 25, 257, 257],"float32"), Tensor([124, 25, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([124, 25, 257, 257],"float32"), Tensor([124, 25, 257, 64],"float32"), ) 	 255740700 	 9746 	 45.22421336174011 	 45.21600675582886 	 4.7457897663116455 	 4.740942716598511 	 82.1823205947876 	 82.11295247077942 	 4.308194875717163 	 4.302398204803467 	 
2025-08-05 05:38:15.755455 test begin: paddle.Tensor.matmul(Tensor([124, 7, 257, 257],"float32"), Tensor([124, 7, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([124, 7, 257, 257],"float32"), Tensor([124, 7, 257, 64],"float32"), ) 	 71607396 	 9746 	 13.046935558319092 	 13.056526184082031 	 1.3699305057525635 	 1.367335557937622 	 23.47185468673706 	 23.485597372055054 	 1.2299878597259521 	 1.2307617664337158 	 
2025-08-05 05:39:30.470557 test begin: paddle.Tensor.matmul(Tensor([128, 11, 197, 197],"float32"), Tensor([128, 11, 197, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 11, 197, 197],"float32"), Tensor([128, 11, 197, 64],"float32"), ) 	 72395136 	 9746 	 11.011780738830566 	 10.761643171310425 	 1.385707139968872 	 1.129410743713379 	 18.292500972747803 	 18.290639400482178 	 0.9598269462585449 	 0.9584333896636963 	 
2025-08-05 05:40:31.293676 test begin: paddle.Tensor.matmul(Tensor([128, 12, 168, 197],"float32"), Tensor([128, 12, 197, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 12, 168, 197],"float32"), Tensor([128, 12, 197, 64],"float32"), ) 	 70201344 	 9746 	 11.590794563293457 	 11.589843511581421 	 1.2164583206176758 	 1.21498703956604 	 18.036423683166504 	 18.048662185668945 	 0.9450347423553467 	 0.9456901550292969 	 
2025-08-05 05:41:31.996912 test begin: paddle.Tensor.matmul(Tensor([128, 12, 197, 197],"float32"), Tensor([128, 12, 197, 168],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 12, 197, 197],"float32"), Tensor([128, 12, 197, 168],"float32"), ) 	 110446080 	 9746 	 22.724082708358765 	 22.735523462295532 	 2.3849704265594482 	 2.382157325744629 	 41.98693370819092 	 41.98729991912842 	 2.200201988220215 	 2.2003705501556396 	 
2025-08-05 05:43:46.272779 test begin: paddle.Tensor.matmul(Tensor([128, 16, 257, 257],"float32"), Tensor([128, 16, 257, 97],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 16, 257, 257],"float32"), Tensor([128, 16, 257, 97],"float32"), ) 	 186322944 	 9746 	 29.832024335861206 	 29.815094709396362 	 3.129443407058716 	 3.127596378326416 	 66.86157393455505 	 66.84997868537903 	 3.506575345993042 	 3.505924940109253 	 
2025-08-05 05:47:03.644799 test begin: paddle.Tensor.matmul(Tensor([128, 16, 97, 257],"float32"), Tensor([128, 16, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 16, 97, 257],"float32"), Tensor([128, 16, 257, 64],"float32"), ) 	 84740096 	 9746 	 10.005088567733765 	 10.025147199630737 	 1.0485920906066895 	 1.0506303310394287 	 20.526795625686646 	 20.54150366783142 	 1.0784146785736084 	 1.0777926445007324 	 
2025-08-05 05:48:08.806653 test begin: paddle.Tensor.matmul(Tensor([128, 25, 257, 257],"float32"), Tensor([128, 25, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 25, 257, 257],"float32"), Tensor([128, 25, 257, 64],"float32"), ) 	 263990400 	 9746 	 46.43688249588013 	 46.43457818031311 	 4.871686697006226 	 4.866772651672363 	 84.57989168167114 	 84.63748359680176 	 4.434522867202759 	 4.504425287246704 	 
2025-08-05 05:52:41.808943 test begin: paddle.Tensor.matmul(Tensor([128, 32, 197, 197],"float32"), Tensor([128, 32, 197, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 32, 197, 197],"float32"), Tensor([128, 32, 197, 64],"float32"), ) 	 210604032 	 9746 	 30.250206232070923 	 30.251543521881104 	 3.1719892024993896 	 3.1707074642181396 	 51.93914437294006 	 51.957518339157104 	 2.7245028018951416 	 2.7258715629577637 	 
2025-08-05 05:55:32.590955 test begin: paddle.Tensor.matmul(Tensor([128, 7, 257, 257],"float32"), Tensor([128, 7, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 7, 257, 257],"float32"), Tensor([128, 7, 257, 64],"float32"), ) 	 73917312 	 9746 	 13.11961817741394 	 13.12299919128418 	 1.3751907348632812 	 1.378462314605713 	 23.895227670669556 	 23.88857412338257 	 1.2533786296844482 	 1.2533972263336182 	 
2025-08-05 05:56:48.125497 test begin: paddle.Tensor.matmul(Tensor([194, 16, 257, 257],"float32"), Tensor([194, 16, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([194, 16, 257, 257],"float32"), Tensor([194, 16, 257, 64],"float32"), ) 	 256070688 	 9746 	 45.37493920326233 	 45.3562753200531 	 4.7554075717926025 	 4.756348371505737 	 82.37668585777283 	 82.34994697570801 	 4.319886207580566 	 4.316661596298218 	 
2025-08-05 06:01:09.007559 test begin: paddle.Tensor.matmul(Tensor([336, 12, 197, 197],"float32"), Tensor([336, 12, 197, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([336, 12, 197, 197],"float32"), Tensor([336, 12, 197, 64],"float32"), ) 	 207313344 	 9746 	 29.848156452178955 	 29.84370231628418 	 3.1291520595550537 	 3.1291112899780273 	 51.1983437538147 	 51.19087028503418 	 2.6858503818511963 	 2.6853458881378174 	 
2025-08-04 11:30:59.521357 test begin: paddle.Tensor.matmul(Tensor([49, 16, 257, 257],"float32"), Tensor([49, 16, 257, 64],"float32"), )
W0804 11:31:00.699402 85352 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([49, 16, 257, 257],"float32"), Tensor([49, 16, 257, 64],"float32"), ) 	 64677648 	 9746 	 11.54973816871643 	 11.561660528182983 	 1.210991621017456 	 1.210822343826294 	 20.981102228164673 	 20.97221612930298 	 1.1000289916992188 	 1.0995442867279053 	 
2025-08-04 11:32:12.607515 test begin: paddle.Tensor.max(Tensor([1, 400, 127009],"float32"), -2, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([1, 400, 127009],"float32"), -2, ) 	 50803600 	 61050 	 15.971800565719604 	 11.81702184677124 	 0.2673954963684082 	 0.19758391380310059 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 11:33:54.800163 test begin: paddle.Tensor.max(Tensor([1, 400, 127009],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([1, 400, 127009],"float32"), axis=-1, keepdim=True, ) 	 50803600 	 61050 	 10.9264075756073 	 9.740696430206299 	 0.09143352508544922 	 0.16308212280273438 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 11:35:21.487828 test begin: paddle.Tensor.max(Tensor([1, 772, 65856],"float32"), -2, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([1, 772, 65856],"float32"), -2, ) 	 50840832 	 61050 	 18.86646342277527 	 9.777000904083252 	 0.15780067443847656 	 0.1636183261871338 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 11:37:05.876991 test begin: paddle.Tensor.max(Tensor([1, 772, 65856],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([1, 772, 65856],"float32"), axis=-1, keepdim=True, ) 	 50840832 	 61050 	 9.983542203903198 	 9.606881618499756 	 0.08356499671936035 	 0.16089892387390137 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 11:38:30.717945 test begin: paddle.Tensor.max(Tensor([2, 400, 65856],"float32"), -2, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([2, 400, 65856],"float32"), -2, ) 	 52684800 	 61050 	 15.74216628074646 	 10.26769733428955 	 0.2636101245880127 	 0.1718590259552002 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 11:40:10.312538 test begin: paddle.Tensor.max(Tensor([2, 400, 65856],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([2, 400, 65856],"float32"), axis=-1, keepdim=True, ) 	 52684800 	 61050 	 10.140451908111572 	 9.894536972045898 	 0.08485913276672363 	 0.16563749313354492 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 11:41:38.464612 test begin: paddle.Tensor.max(Tensor([324000, 157],"float32"), axis=1, keepdim=True, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([324000, 157],"float32"), axis=1, keepdim=True, ) 	 50868000 	 61050 	 35.313788414001465 	 25.115931034088135 	 0.591240644454956 	 0.42046117782592773 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 11:43:57.859327 test begin: paddle.Tensor.max(Tensor([635041, 80],"float32"), axis=1, keepdim=True, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([635041, 80],"float32"), axis=1, keepdim=True, ) 	 50803280 	 61050 	 32.911723613739014 	 32.95388174057007 	 0.5509669780731201 	 0.5516483783721924 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 11:46:21.531811 test begin: paddle.Tensor.mean(Tensor([124, 128, 34, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([124, 128, 34, 96],"float32"), 1, keepdim=True, ) 	 51806208 	 60303 	 12.269451379776001 	 9.132885456085205 	 0.20789813995361328 	 0.15459609031677246 	 8.880753993988037 	 11.887633562088013 	 0.15099215507507324 	 0.20154142379760742 	 
2025-08-04 11:47:04.710138 test begin: paddle.Tensor.mean(Tensor([124, 128, 96, 34],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([124, 128, 96, 34],"float32"), 1, keepdim=True, ) 	 51806208 	 60303 	 12.268593788146973 	 9.12355089187622 	 0.20789051055908203 	 0.15462708473205566 	 8.877951622009277 	 11.890493154525757 	 0.15014362335205078 	 0.20154523849487305 	 
2025-08-04 11:47:48.840462 test begin: paddle.Tensor.mean(Tensor([124, 45, 96, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([124, 45, 96, 96],"float32"), 1, keepdim=True, ) 	 51425280 	 60303 	 9.9941086769104 	 9.481824159622192 	 0.1693577766418457 	 0.16072535514831543 	 9.018158435821533 	 12.82099962234497 	 0.1549386978149414 	 0.2172560691833496 	 
2025-08-04 11:48:31.151339 test begin: paddle.Tensor.mean(Tensor([128, 128, 33, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([128, 128, 33, 96],"float32"), 1, keepdim=True, ) 	 51904512 	 60303 	 12.348258972167969 	 9.19252896308899 	 0.20923709869384766 	 0.15578389167785645 	 8.912089109420776 	 11.966469287872314 	 0.15122509002685547 	 0.20268893241882324 	 
2025-08-04 11:49:14.491951 test begin: paddle.Tensor.mean(Tensor([128, 128, 96, 33],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([128, 128, 96, 33],"float32"), 1, keepdim=True, ) 	 51904512 	 60303 	 12.830573081970215 	 10.30073881149292 	 0.20922398567199707 	 0.15578603744506836 	 8.876506567001343 	 11.975225925445557 	 0.15125155448913574 	 0.20296072959899902 	 
2025-08-04 11:50:00.535635 test begin: paddle.Tensor.mean(Tensor([128, 192, 22, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([128, 192, 22, 96],"float32"), 1, keepdim=True, ) 	 51904512 	 60303 	 13.154153823852539 	 9.066695928573608 	 0.22292137145996094 	 0.15367984771728516 	 8.872750997543335 	 11.893715381622314 	 0.15051651000976562 	 0.20158052444458008 	 
2025-08-04 11:50:45.019892 test begin: paddle.Tensor.mean(Tensor([128, 192, 96, 22],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([128, 192, 96, 22],"float32"), 1, keepdim=True, ) 	 51904512 	 60303 	 13.153267860412598 	 9.066360712051392 	 0.22289633750915527 	 0.15365076065063477 	 8.863521575927734 	 11.895099401473999 	 0.1504819393157959 	 0.20118999481201172 	 
2025-08-04 11:51:29.965199 test begin: paddle.Tensor.mean(Tensor([128, 44, 96, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([128, 44, 96, 96],"float32"), 1, keepdim=True, ) 	 51904512 	 60303 	 10.095678567886353 	 9.484108686447144 	 0.17107486724853516 	 0.160783052444458 	 8.988483905792236 	 13.002301931381226 	 0.15322637557983398 	 0.22054743766784668 	 
2025-08-04 11:52:13.260525 test begin: paddle.Tensor.mean(Tensor([29, 192, 96, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([29, 192, 96, 96],"float32"), 1, keepdim=True, ) 	 51314688 	 60303 	 9.99195909500122 	 8.92430067062378 	 0.16933083534240723 	 0.15120649337768555 	 8.581005811691284 	 11.583820819854736 	 0.14544224739074707 	 0.19623923301696777 	 
2025-08-04 11:52:55.032615 test begin: paddle.Tensor.mean(Tensor([44, 128, 96, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([44, 128, 96, 96],"float32"), 1, keepdim=True, ) 	 51904512 	 60303 	 10.10053825378418 	 9.087151288986206 	 0.1711273193359375 	 0.15398073196411133 	 8.718132257461548 	 12.159024238586426 	 0.14715099334716797 	 0.20605063438415527 	 
2025-08-04 11:53:39.021575 test begin: paddle.Tensor.min(Tensor([1, 193, 65856, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([1, 193, 65856, 4],"float32"), axis=-1, ) 	 50840832 	 45506 	 24.792067766189575 	 39.517765045166016 	 0.5496692657470703 	 0.8870842456817627 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 11:55:49.407562 test begin: paddle.Tensor.min(Tensor([1, 400, 31753, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([1, 400, 31753, 4],"float32"), axis=-1, ) 	 50804800 	 45506 	 24.490686893463135 	 39.468430280685425 	 0.5500681400299072 	 0.8864359855651855 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 11:57:58.004882 test begin: paddle.Tensor.min(Tensor([1, 400, 65856, 2],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([1, 400, 65856, 2],"float32"), axis=-1, ) 	 52684800 	 45506 	 23.526416540145874 	 37.384286403656006 	 0.5283980369567871 	 0.8396177291870117 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 12:00:11.045616 test begin: paddle.Tensor.min(Tensor([1, 400, 65856, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([1, 400, 65856, 4],"float32"), axis=-1, ) 	 105369600 	 45506 	 50.51519250869751 	 81.5738296508789 	 1.1343717575073242 	 1.831801414489746 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 12:04:38.003958 test begin: paddle.Tensor.min(Tensor([15661, 4, 811],"float32"), axis=1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([15661, 4, 811],"float32"), axis=1, ) 	 50804284 	 45506 	 9.997997283935547 	 13.633501052856445 	 0.22463631629943848 	 0.305056095123291 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 12:05:58.287853 test begin: paddle.Tensor.min(Tensor([24565, 3, 811],"float32"), axis=1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([24565, 3, 811],"float32"), axis=1, ) 	 59766645 	 45506 	 13.892096281051636 	 18.50273108482361 	 0.3119945526123047 	 0.415543794631958 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 12:07:43.874479 test begin: paddle.Tensor.min(Tensor([24565, 4, 518],"float32"), axis=1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([24565, 4, 518],"float32"), axis=1, ) 	 50898680 	 45506 	 10.30496597290039 	 12.092204809188843 	 0.23142504692077637 	 0.2714099884033203 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 12:09:03.104500 test begin: paddle.Tensor.min(Tensor([3, 525, 12096, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([3, 525, 12096, 4],"float32"), axis=-1, ) 	 76204800 	 45506 	 36.95119905471802 	 58.981324195861816 	 0.8228340148925781 	 1.3246722221374512 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 12:12:17.506339 test begin: paddle.Tensor.min(Tensor([4, 263, 12096, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([4, 263, 12096, 4],"float32"), axis=-1, ) 	 50899968 	 45506 	 24.53081178665161 	 39.56368708610535 	 0.5509054660797119 	 0.888167142868042 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 12:14:27.645097 test begin: paddle.Tensor.min(Tensor([4, 525, 12096, 3],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([4, 525, 12096, 3],"float32"), axis=-1, ) 	 76204800 	 45506 	 23.971680164337158 	 40.44487142562866 	 0.5381386280059814 	 0.9083783626556396 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 12:17:02.948924 test begin: paddle.Tensor.min(Tensor([4, 525, 6049, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([4, 525, 6049, 4],"float32"), axis=-1, ) 	 50811600 	 45506 	 24.460862159729004 	 39.44465160369873 	 0.5493738651275635 	 0.8865551948547363 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-04 12:19:11.465665 test begin: paddle.Tensor.mm(Tensor([10, 10],"float32"), Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.mm 	 paddle.Tensor.mm(Tensor([10, 10],"float32"), Tensor([10, 5080321],"float32"), ) 	 50803310 	 15341 	 10.030416250228882 	 10.028867721557617 	 0.13362383842468262 	 0.13336944580078125 	 22.17192554473877 	 22.14158535003662 	 0.21111416816711426 	 0.21082019805908203 	 
2025-08-04 12:20:17.670473 test begin: paddle.Tensor.mm(Tensor([5080321, 10],"float32"), Tensor([10, 10],"float32"), )
[Prof] paddle.Tensor.mm 	 paddle.Tensor.mm(Tensor([5080321, 10],"float32"), Tensor([10, 10],"float32"), ) 	 50803310 	 15341 	 9.98017168045044 	 10.009638547897339 	 0.13319802284240723 	 0.132781982421875 	 21.70741844177246 	 21.673484086990356 	 0.2066493034362793 	 0.20634770393371582 	 
2025-08-04 12:21:23.463553 test begin: paddle.Tensor.mode(Tensor([3, 2, 4233601],"float64"), )
[Prof] paddle.Tensor.mode 	 paddle.Tensor.mode(Tensor([3, 2, 4233601],"float64"), ) 	 25401606 	 1000 	 61.75782656669617 	 9.386207580566406 	 9.989738464355469e-05 	 0.0002486705780029297 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 12:22:39.264099 test begin: paddle.Tensor.mode(Tensor([3, 2, 4233601],"float64"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.mode 	 paddle.Tensor.mode(Tensor([3, 2, 4233601],"float64"), axis=2, keepdim=True, ) 	 25401606 	 1000 	 65.17957329750061 	 9.374964714050293 	 0.0001049041748046875 	 0.0002562999725341797 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 12:23:59.553159 test begin: paddle.Tensor.mode(Tensor([3, 2822401, 3],"float64"), axis=1, keepdim=False, )
[Prof] paddle.Tensor.mode 	 paddle.Tensor.mode(Tensor([3, 2822401, 3],"float64"), axis=1, keepdim=False, ) 	 25401609 	 1000 	 70.9770450592041 	 10.70407247543335 	 0.00010371208190917969 	 0.00024580955505371094 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 12:25:27.784452 test begin: paddle.Tensor.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=0, destination=2, ) 	 254018100 	 1457722 	 17.582465648651123 	 7.060297012329102 	 0.00012159347534179688 	 8.559226989746094e-05 	 60.39593172073364 	 79.8693437576294 	 0.00011324882507324219 	 0.00022363662719726562 	 
2025-08-04 12:28:23.980310 test begin: paddle.Tensor.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254018100 	 1457722 	 18.367707014083862 	 8.67965841293335 	 0.0001468658447265625 	 0.00012302398681640625 	 65.76281809806824 	 78.89527463912964 	 0.00011157989501953125 	 0.0002155303955078125 	 
2025-08-04 12:31:28.494740 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 1058401],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 1058401],"float64"), source=0, destination=2, ) 	 254016240 	 1457722 	 9.823031187057495 	 6.956895351409912 	 0.0001354217529296875 	 0.0002570152282714844 	 58.94923996925354 	 76.80594968795776 	 0.00010085105895996094 	 0.00022649765014648438 	 
2025-08-04 12:34:12.391840 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=0, destination=2, ) 	 254017680 	 1457722 	 9.811327457427979 	 7.032279968261719 	 0.0001289844512939453 	 0.00013327598571777344 	 58.566768646240234 	 79.81706476211548 	 0.00010919570922851562 	 0.00021886825561523438 	 
2025-08-04 12:37:00.088407 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254017680 	 1457722 	 10.705561876296997 	 8.786774158477783 	 0.00013256072998046875 	 0.00025177001953125 	 58.68442463874817 	 78.86196970939636 	 0.00010704994201660156 	 0.00021505355834960938 	 
2025-08-04 12:39:49.172470 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=0, destination=2, ) 	 254017200 	 1457722 	 9.825454950332642 	 7.117591857910156 	 0.00012969970703125 	 0.00026535987854003906 	 59.56188750267029 	 78.15108799934387 	 0.00010347366333007812 	 0.00022339820861816406 	 
2025-08-04 12:42:40.009690 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254017200 	 1457722 	 16.5033860206604 	 8.803587675094604 	 0.00017261505126953125 	 0.0007259845733642578 	 60.12505650520325 	 89.17458772659302 	 0.00030541419982910156 	 0.0002956390380859375 	 
2025-08-04 12:45:50.074513 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 635041, 5],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 635041, 5],"float64"), source=0, destination=2, ) 	 254016400 	 1457722 	 9.819989919662476 	 7.155023574829102 	 0.00025343894958496094 	 0.0005738735198974609 	 58.61160683631897 	 95.73811936378479 	 0.00014209747314453125 	 0.0002512931823730469 	 
2025-08-04 12:48:52.734852 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=0, destination=2, ) 	 254018800 	 1457722 	 9.8672194480896 	 7.110292434692383 	 0.00016999244689941406 	 0.00017786026000976562 	 59.5382285118103 	 86.56100487709045 	 0.0001049041748046875 	 0.00025773048400878906 	 
2025-08-04 12:51:50.972128 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254018800 	 1457722 	 10.704085350036621 	 8.611306190490723 	 0.00016689300537109375 	 9.1552734375e-05 	 58.76254367828369 	 96.59082674980164 	 0.00014209747314453125 	 0.00025343894958496094 	 
2025-08-04 12:54:57.773850 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 423361, 3, 5],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 423361, 3, 5],"float64"), source=0, destination=2, ) 	 254016600 	 1457722 	 9.706371068954468 	 6.91774582862854 	 0.00019359588623046875 	 0.00017404556274414062 	 58.347052335739136 	 89.23120045661926 	 0.00012922286987304688 	 0.0002396106719970703 	 
2025-08-04 12:57:53.180970 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=0, destination=2, ) 	 254020200 	 1457722 	 9.967983722686768 	 7.06274151802063 	 0.0001652240753173828 	 0.00036525726318359375 	 58.76450514793396 	 103.29651856422424 	 0.00012874603271484375 	 0.0002288818359375 	 
2025-08-04 13:01:03.631377 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254020200 	 1457722 	 10.845258474349976 	 8.893985033035278 	 0.000125885009765625 	 0.0001430511474609375 	 59.147441387176514 	 81.06577372550964 	 0.00011396408081054688 	 0.00022721290588378906 	 
2025-08-04 13:03:54.879081 test begin: paddle.Tensor.moveaxis(x=Tensor([8467210, 2, 3, 5],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([8467210, 2, 3, 5],"float64"), source=0, destination=2, ) 	 254016300 	 1457722 	 12.849775552749634 	 7.068552255630493 	 0.00012826919555664062 	 8.797645568847656e-05 	 59.31337118148804 	 78.74101853370667 	 0.00010371208190917969 	 0.00021696090698242188 	 
2025-08-04 13:06:45.291492 test begin: paddle.Tensor.multiply(Tensor([132301, 768],"float16"), Tensor([132301, 1],"float32"), )
W0804 13:06:47.182157 120443 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([132301, 768],"float16"), Tensor([132301, 1],"float32"), ) 	 101739469 	 33843 	 36.08922624588013 	 24.63140845298767 	 0.5448741912841797 	 0.7440850734710693 	 64.52048087120056 	 71.98968553543091 	 0.6489841938018799 	 0.5430819988250732 	 
2025-08-04 13:10:06.267315 test begin: paddle.Tensor.multiply(Tensor([160, 317521],"float32"), Tensor([160, 1],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([160, 317521],"float32"), Tensor([160, 1],"float32"), ) 	 50803520 	 33843 	 10.027902603149414 	 10.287216424942017 	 0.30287766456604004 	 0.3098466396331787 	 26.10063409805298 	 31.20955467224121 	 0.2624228000640869 	 0.23534727096557617 	 
2025-08-04 13:11:25.722569 test begin: paddle.Tensor.multiply(Tensor([160, 317521],"float32"), Tensor([160, 317521],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([160, 317521],"float32"), Tensor([160, 317521],"float32"), ) 	 101606720 	 33843 	 15.236482620239258 	 15.123569965362549 	 0.4601280689239502 	 0.45629072189331055 	 36.359418630599976 	 30.21598219871521 	 1.0981037616729736 	 0.45627832412719727 	 
2025-08-04 13:13:05.318955 test begin: paddle.Tensor.multiply(Tensor([160, 635041],"float16"), Tensor([160, 1],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([160, 635041],"float16"), Tensor([160, 1],"float32"), ) 	 101606720 	 33843 	 36.134615421295166 	 23.51341152191162 	 0.545720100402832 	 0.7100529670715332 	 65.00587701797485 	 72.28619623184204 	 0.4902312755584717 	 0.43603062629699707 	 
2025-08-04 13:16:26.001517 test begin: paddle.Tensor.multiply(Tensor([16538, 3072],"float32"), Tensor([16538, 1],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([16538, 3072],"float32"), Tensor([16538, 1],"float32"), ) 	 50821274 	 33843 	 9.995512962341309 	 10.421971082687378 	 0.30187463760375977 	 0.3143489360809326 	 24.911408185958862 	 30.461843729019165 	 0.3761270046234131 	 0.30649685859680176 	 
2025-08-04 13:17:43.646834 test begin: paddle.Tensor.multiply(Tensor([33076, 3072],"float16"), Tensor([33076, 1],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([33076, 3072],"float16"), Tensor([33076, 1],"float32"), ) 	 101642548 	 33843 	 36.13899803161621 	 24.488110303878784 	 0.5455732345581055 	 0.7394821643829346 	 64.3534414768219 	 72.09944558143616 	 0.6472694873809814 	 0.5439879894256592 	 
2025-08-04 13:21:04.505532 test begin: paddle.Tensor.multiply(Tensor([512, 198451],"float16"), Tensor([512, 1],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([512, 198451],"float16"), Tensor([512, 1],"float32"), ) 	 101607424 	 33843 	 36.22285509109497 	 23.56261920928955 	 0.5469212532043457 	 0.7107107639312744 	 65.81579327583313 	 72.93912029266357 	 0.49635839462280273 	 0.5502803325653076 	 
2025-08-04 13:24:29.330706 test begin: paddle.Tensor.nansum(Tensor([105841, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([105841, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401840 	 10691 	 11.377784252166748 	 2.0236403942108154 	 0.2720956802368164 	 0.19339847564697266 	 5.637876510620117 	 4.71785569190979 	 0.26947808265686035 	 0.15028166770935059 	 
2025-08-04 13:24:53.880285 test begin: paddle.Tensor.nansum(Tensor([2822401, 3, 3],"float64"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([2822401, 3, 3],"float64"), ) 	 25401609 	 10691 	 9.994466066360474 	 1.600637674331665 	 0.19087576866149902 	 0.07652044296264648 	 4.968382835388184 	 4.4156341552734375 	 0.23743081092834473 	 0.14065027236938477 	 
2025-08-04 13:25:16.456459 test begin: paddle.Tensor.nansum(Tensor([3, 2, 105841, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2, 105841, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401840 	 10691 	 11.377920389175415 	 2.0362777709960938 	 0.27213168144226074 	 0.1934518814086914 	 5.637936353683472 	 4.717613697052002 	 0.26948022842407227 	 0.15024971961975098 	 
2025-08-04 13:25:43.022547 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 141121, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2, 3, 141121, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401780 	 10691 	 60.446757078170776 	 1.8383121490478516 	 1.158341407775879 	 0.08783745765686035 	 4.975529432296753 	 4.462398290634155 	 0.23777341842651367 	 0.1421663761138916 	 
2025-08-04 13:26:55.341940 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 176401, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 176401, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401744 	 10691 	 10.409332513809204 	 2.032400608062744 	 0.24893665313720703 	 0.19408941268920898 	 5.464524269104004 	 4.7406089305877686 	 0.26111865043640137 	 0.15104389190673828 	 
2025-08-04 13:27:18.736195 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 1, 70561],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 1, 70561],"float64"), axis=3, keepdim=True, ) 	 25401960 	 10691 	 10.421967267990112 	 4.078345537185669 	 0.24915313720703125 	 0.19431424140930176 	 5.468088388442993 	 4.745636701583862 	 0.26138877868652344 	 0.15120267868041992 	 
2025-08-04 13:27:46.303008 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 35281, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 35281, 2],"float64"), axis=3, keepdim=True, ) 	 25402320 	 10691 	 10.410297393798828 	 2.0322530269622803 	 0.24898552894592285 	 0.1940915584564209 	 5.465662717819214 	 4.759467124938965 	 0.26128292083740234 	 0.1516284942626953 	 
2025-08-04 13:28:09.722510 test begin: paddle.Tensor.nansum(Tensor([3, 2822401, 3],"float64"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2822401, 3],"float64"), ) 	 25401609 	 10691 	 9.994392156600952 	 1.6004910469055176 	 0.19085097312927246 	 0.07649588584899902 	 4.969057559967041 	 4.4142138957977295 	 0.23888492584228516 	 0.14062285423278809 	 
2025-08-04 13:28:32.867832 test begin: paddle.Tensor.nansum(Tensor([3, 3, 2822401],"float64"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 3, 2822401],"float64"), ) 	 25401609 	 10691 	 9.99634075164795 	 1.6007487773895264 	 0.1909773349761963 	 0.07652902603149414 	 4.967597246170044 	 4.414059162139893 	 0.23743724822998047 	 0.14060425758361816 	 
2025-08-04 13:28:54.433714 test begin: paddle.Tensor.nansum(Tensor([3, 3, 5644801],"float32"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 3, 5644801],"float32"), ) 	 50803209 	 10691 	 10.786216020584106 	 1.6302096843719482 	 0.20598411560058594 	 0.07794594764709473 	 5.973745346069336 	 6.305953025817871 	 0.2855663299560547 	 0.20090937614440918 	 
2025-08-04 13:29:20.103209 test begin: paddle.Tensor.nansum(Tensor([3, 5644801, 3],"float32"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 5644801, 3],"float32"), ) 	 50803209 	 10691 	 10.7859525680542 	 1.6301991939544678 	 0.20597434043884277 	 0.07792019844055176 	 5.973867654800415 	 6.306080341339111 	 0.28549671173095703 	 0.20094752311706543 	 
2025-08-04 13:29:49.670182 test begin: paddle.Tensor.nansum(Tensor([3, 70561, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 70561, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401960 	 10691 	 11.375611782073975 	 2.023740291595459 	 0.2720458507537842 	 0.19349098205566406 	 5.664942502975464 	 4.721414804458618 	 0.27074289321899414 	 0.15044474601745605 	 
2025-08-04 13:30:14.245651 test begin: paddle.Tensor.nansum(Tensor([5644801, 3, 3],"float32"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([5644801, 3, 3],"float32"), ) 	 50803209 	 10691 	 10.786137342453003 	 1.630042314529419 	 0.20592975616455078 	 0.07791495323181152 	 5.972920894622803 	 6.304257154464722 	 0.28547191619873047 	 0.20088505744934082 	 
2025-08-04 13:30:43.700221 test begin: paddle.Tensor.neg(Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.neg 	 paddle.Tensor.neg(Tensor([50803201],"float32"), ) 	 50803201 	 33807 	 9.993345975875854 	 10.086137533187866 	 0.3021061420440674 	 0.30419468879699707 	 9.996036529541016 	 10.063225746154785 	 0.3022308349609375 	 0.30417394638061523 	 
2025-08-04 13:31:26.354649 test begin: paddle.Tensor.nonzero(Tensor([3628801, 14],"bool"), )
[Prof] paddle.Tensor.nonzero 	 paddle.Tensor.nonzero(Tensor([3628801, 14],"bool"), ) 	 50803214 	 1684 	 10.003602981567383 	 2.3962020874023438 	 0.004050731658935547 	 0.0013048648834228516 	 None 	 None 	 None 	 None 	 
2025-08-04 13:31:43.858453 test begin: paddle.Tensor.nonzero(Tensor([3907939, 13],"bool"), )
[Prof] paddle.Tensor.nonzero 	 paddle.Tensor.nonzero(Tensor([3907939, 13],"bool"), ) 	 50803207 	 1684 	 10.024717569351196 	 2.385167360305786 	 0.004045963287353516 	 0.0013048648834228516 	 None 	 None 	 None 	 None 	 
2025-08-04 13:31:57.051298 test begin: paddle.Tensor.nonzero(Tensor([4233601, 12],"bool"), )
[Prof] paddle.Tensor.nonzero 	 paddle.Tensor.nonzero(Tensor([4233601, 12],"bool"), ) 	 50803212 	 1684 	 10.01788854598999 	 2.4156017303466797 	 0.004090547561645508 	 0.001310110092163086 	 None 	 None 	 None 	 None 	 
2025-08-04 13:32:13.594799 test begin: paddle.Tensor.nonzero(Tensor([52640, 966],"bool"), )
[Prof] paddle.Tensor.nonzero 	 paddle.Tensor.nonzero(Tensor([52640, 966],"bool"), ) 	 50850240 	 1684 	 10.027148485183716 	 2.3870930671691895 	 0.004060506820678711 	 0.0013134479522705078 	 None 	 None 	 None 	 None 	 
2025-08-04 13:32:26.754625 test begin: paddle.Tensor.norm(Tensor([100352, 507],"float32"), )
[Prof] paddle.Tensor.norm 	 paddle.Tensor.norm(Tensor([100352, 507],"float32"), ) 	 50878464 	 65593 	 10.009693384170532 	 9.979512453079224 	 0.05189156532287598 	 0.07775306701660156 	 65.39595055580139 	 59.816752195358276 	 1.018726110458374 	 0.2331538200378418 	 
2025-08-04 13:34:53.423008 test begin: paddle.Tensor.norm(Tensor([507, 100352],"float32"), )
[Prof] paddle.Tensor.norm 	 paddle.Tensor.norm(Tensor([507, 100352],"float32"), ) 	 50878464 	 65593 	 10.009399890899658 	 9.979743719100952 	 0.05188250541687012 	 0.07774829864501953 	 65.44189476966858 	 59.81854820251465 	 1.0199275016784668 	 0.2331392765045166 	 
2025-08-04 13:37:19.594650 test begin: paddle.Tensor.norm(Tensor([6202, 8192],"float32"), )
[Prof] paddle.Tensor.norm 	 paddle.Tensor.norm(Tensor([6202, 8192],"float32"), ) 	 50806784 	 65593 	 9.98253345489502 	 9.967851161956787 	 0.05175971984863281 	 0.07767701148986816 	 65.56320285797119 	 59.68175148963928 	 1.2116525173187256 	 0.2326054573059082 	 
2025-08-04 13:39:47.583744 test begin: paddle.Tensor.norm(Tensor([8192, 6202],"float32"), )
[Prof] paddle.Tensor.norm 	 paddle.Tensor.norm(Tensor([8192, 6202],"float32"), ) 	 50806784 	 65593 	 9.982249975204468 	 9.967741012573242 	 0.05174565315246582 	 0.07766413688659668 	 65.33073306083679 	 59.684643030166626 	 1.0177922248840332 	 0.23263049125671387 	 
2025-08-04 13:42:14.327306 test begin: paddle.Tensor.norm(Tensor([886, 57344],"float32"), )
[Prof] paddle.Tensor.norm 	 paddle.Tensor.norm(Tensor([886, 57344],"float32"), ) 	 50806784 	 65593 	 9.98243236541748 	 9.968021392822266 	 0.05174851417541504 	 0.07765913009643555 	 65.3350293636322 	 59.6838812828064 	 1.0178675651550293 	 0.23260927200317383 	 
2025-08-04 13:44:45.660165 test begin: paddle.Tensor.not_equal(Tensor([128, 198451],"int64"), Tensor([128, 198451],"int64"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([128, 198451],"int64"), Tensor([128, 198451],"int64"), ) 	 50803456 	 72509 	 22.40566349029541 	 22.690501928329468 	 0.3157963752746582 	 0.3198115825653076 	 None 	 None 	 None 	 None 	 
2025-08-04 13:45:31.691405 test begin: paddle.Tensor.not_equal(Tensor([13, 1953970],"int64"), Tensor([1],"int64"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([13, 1953970],"int64"), Tensor([1],"int64"), ) 	 25401611 	 72509 	 12.769302368164062 	 13.054472923278809 	 0.17998743057250977 	 0.1839292049407959 	 None 	 None 	 None 	 None 	 
2025-08-04 13:45:59.314225 test begin: paddle.Tensor.not_equal(Tensor([13, 3907939],"bool"), Tensor([1],"bool"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([13, 3907939],"bool"), Tensor([1],"bool"), ) 	 50803208 	 72509 	 9.986063480377197 	 14.362006187438965 	 0.14074969291687012 	 0.2024543285369873 	 None 	 None 	 None 	 None 	 
2025-08-04 13:46:24.381240 test begin: paddle.Tensor.not_equal(Tensor([1814401, 14],"int64"), Tensor([1],"int64"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([1814401, 14],"int64"), Tensor([1],"int64"), ) 	 25401615 	 72509 	 12.77051067352295 	 13.056849956512451 	 0.1799910068511963 	 0.18396759033203125 	 None 	 None 	 None 	 None 	 
2025-08-04 13:46:51.146276 test begin: paddle.Tensor.not_equal(Tensor([198451, 128],"int64"), Tensor([198451, 128],"int64"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([198451, 128],"int64"), Tensor([198451, 128],"int64"), ) 	 50803456 	 72509 	 22.405579090118408 	 22.726954698562622 	 0.31578612327575684 	 0.31980252265930176 	 None 	 None 	 None 	 None 	 
2025-08-04 13:47:38.751381 test begin: paddle.Tensor.not_equal(Tensor([3628801, 14],"bool"), Tensor([1],"bool"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([3628801, 14],"bool"), Tensor([1],"bool"), ) 	 50803215 	 72509 	 9.99239706993103 	 14.361546993255615 	 0.14076614379882812 	 0.20241260528564453 	 None 	 None 	 None 	 None 	 
2025-08-04 13:48:03.816804 test begin: paddle.Tensor.outer(x=Tensor([12700801, 2],"float64"), y=Tensor([2, 3, 4],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([12700801, 2],"float64"), y=Tensor([2, 3, 4],"float64"), ) 	 25401626 	 6606 	 25.60189723968506 	 25.20252776145935 	 0.1584324836730957 	 0.9745743274688721 	 49.47881293296814 	 151.06372261047363 	 2.550226926803589 	 1.167072057723999 	 
2025-08-04 13:52:29.271097 test begin: paddle.Tensor.outer(x=Tensor([4, 2, 3175201],"float64"), y=Tensor([4, 2, 3],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2, 3175201],"float64"), y=Tensor([4, 2, 3],"float64"), ) 	 25401632 	 6606 	 25.60391330718994 	 25.276668310165405 	 0.15845227241516113 	 0.977550745010376 	 49.47008562088013 	 150.69423604011536 	 2.5494871139526367 	 1.1640677452087402 	 
2025-08-04 13:56:55.950744 test begin: paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4, 2, 3175201],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4, 2, 3175201],"float64"), ) 	 25401632 	 6606 	 26.654337644577026 	 46.96240472793579 	 0.16495418548583984 	 1.81636643409729 	 49.152705907821655 	 168.65711307525635 	 2.5335142612457275 	 1.304342269897461 	 
2025-08-04 14:02:01.463568 test begin: paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4, 2116801, 3],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4, 2116801, 3],"float64"), ) 	 25401636 	 6606 	 26.63878893852234 	 46.76006746292114 	 0.16480255126953125 	 1.8085031509399414 	 49.192569971084595 	 168.48595476150513 	 2.5357260704040527 	 1.3030459880828857 	 
2025-08-04 14:07:06.687795 test begin: paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4233601, 2, 3],"float64"), ) 	 25401630 	 6606 	 27.49375009536743 	 47.0570182800293 	 0.17014074325561523 	 1.8199396133422852 	 49.22021532058716 	 168.74511694908142 	 2.5372912883758545 	 1.3049864768981934 	 
2025-08-04 14:12:13.318221 test begin: paddle.Tensor.outer(x=Tensor([4, 2116801, 3],"float64"), y=Tensor([4, 2, 3],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2116801, 3],"float64"), y=Tensor([4, 2, 3],"float64"), ) 	 25401636 	 6606 	 25.606223821640015 	 25.32519507408142 	 0.15844392776489258 	 0.9782195091247559 	 49.52880358695984 	 150.7206392288208 	 2.608450174331665 	 1.1842560768127441 	 
2025-08-04 14:16:42.941842 test begin: paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2, 3, 4233601],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2, 3, 4233601],"float64"), ) 	 25401614 	 6606 	 10.072671175003052 	 15.648193597793579 	 0.062212228775024414 	 2.418606758117676 	 18.201975107192993 	 55.071916818618774 	 0.938204288482666 	 1.7029902935028076 	 
2025-08-04 14:18:29.737837 test begin: paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2, 3175201, 4],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2, 3175201, 4],"float64"), ) 	 25401616 	 6606 	 10.013458728790283 	 15.581684112548828 	 0.061920881271362305 	 2.4105818271636963 	 18.17153239250183 	 55.04649257659912 	 0.9365973472595215 	 1.702223777770996 	 
2025-08-04 14:20:13.658440 test begin: paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2116801, 3, 4],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2116801, 3, 4],"float64"), ) 	 25401620 	 6606 	 9.985524415969849 	 15.565873384475708 	 0.06170821189880371 	 2.4032411575317383 	 18.184782028198242 	 55.02575612068176 	 0.9371471405029297 	 1.7017614841461182 	 
2025-08-04 14:21:58.951234 test begin: paddle.Tensor.outer(x=Tensor([4, 6350401],"float64"), y=Tensor([2, 3, 4],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 6350401],"float64"), y=Tensor([2, 3, 4],"float64"), ) 	 25401628 	 6606 	 26.30366349220276 	 25.29965567588806 	 0.15842318534851074 	 0.9785895347595215 	 49.47570610046387 	 150.42748498916626 	 2.549802303314209 	 1.1621382236480713 	 
2025-08-04 14:26:25.323393 test begin: paddle.Tensor.outer(x=Tensor([4233601, 2, 3],"float64"), y=Tensor([4, 2, 3],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4233601, 2, 3],"float64"), y=Tensor([4, 2, 3],"float64"), ) 	 25401630 	 6606 	 25.60725450515747 	 25.248879432678223 	 0.1584029197692871 	 0.9759180545806885 	 49.47178387641907 	 150.56734371185303 	 2.550427198410034 	 1.1630504131317139 	 
2025-08-04 14:30:50.247489 test begin: paddle.Tensor.pow(Tensor([124, 128, 34, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([124, 128, 34, 96],"float32"), 2, ) 	 51806208 	 26857 	 10.322029829025269 	 10.539653778076172 	 0.3921539783477783 	 0.31003451347351074 	 12.319852352142334 	 28.807706594467163 	 0.4690380096435547 	 0.3654782772064209 	 
2025-08-04 14:31:54.164620 test begin: paddle.Tensor.pow(Tensor([124, 128, 96, 34],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([124, 128, 96, 34],"float32"), 2, ) 	 51806208 	 26857 	 10.15803575515747 	 8.146177530288696 	 0.3886139392852783 	 0.30997729301452637 	 12.356481075286865 	 28.807960271835327 	 0.47030043601989746 	 0.36548614501953125 	 
2025-08-04 14:32:55.488736 test begin: paddle.Tensor.pow(Tensor([124, 45, 96, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([124, 45, 96, 96],"float32"), 2, ) 	 51425280 	 26857 	 10.048058986663818 	 8.088279724121094 	 0.382892370223999 	 0.30781054496765137 	 12.281634092330933 	 28.598200798034668 	 0.4674859046936035 	 0.36280393600463867 	 
2025-08-04 14:33:58.331647 test begin: paddle.Tensor.pow(Tensor([128, 128, 33, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([128, 128, 33, 96],"float32"), 2, ) 	 51904512 	 26857 	 10.151538610458374 	 8.161731243133545 	 0.38996195793151855 	 0.3105790615081787 	 12.384809017181396 	 28.861879587173462 	 0.47112154960632324 	 0.36612558364868164 	 
2025-08-04 14:34:59.746363 test begin: paddle.Tensor.pow(Tensor([128, 128, 96, 33],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([128, 128, 96, 33],"float32"), 2, ) 	 51904512 	 26857 	 10.226139783859253 	 8.161813735961914 	 0.3906431198120117 	 0.31059908866882324 	 12.369400978088379 	 28.86195993423462 	 0.4711737632751465 	 0.3661916255950928 	 
2025-08-04 14:36:01.222359 test begin: paddle.Tensor.pow(Tensor([128, 192, 22, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([128, 192, 22, 96],"float32"), 2, ) 	 51904512 	 26857 	 10.108869552612305 	 8.1617271900177 	 0.38443422317504883 	 0.31060171127319336 	 12.398671388626099 	 28.861674070358276 	 0.4718341827392578 	 0.36612939834594727 	 
2025-08-04 14:37:04.207830 test begin: paddle.Tensor.pow(Tensor([128, 192, 96, 22],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([128, 192, 96, 22],"float32"), 2, ) 	 51904512 	 26857 	 10.102389335632324 	 8.161699295043945 	 0.38451695442199707 	 0.3105592727661133 	 12.39937949180603 	 28.861896276474 	 0.47175025939941406 	 0.36614012718200684 	 
2025-08-04 14:38:06.716292 test begin: paddle.Tensor.pow(Tensor([128, 44, 96, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([128, 44, 96, 96],"float32"), 2, ) 	 51904512 	 26857 	 10.117803573608398 	 8.163273572921753 	 0.38526463508605957 	 0.3105912208557129 	 12.39425539970398 	 28.862310886383057 	 0.47156429290771484 	 0.3661661148071289 	 
2025-08-04 14:39:09.411609 test begin: paddle.Tensor.pow(Tensor([29, 192, 96, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([29, 192, 96, 96],"float32"), 2, ) 	 51314688 	 26857 	 9.987299919128418 	 8.071739435195923 	 0.3800675868988037 	 0.30716848373413086 	 12.263212442398071 	 28.54571008682251 	 0.46663713455200195 	 0.36209893226623535 	 
2025-08-04 14:40:10.532841 test begin: paddle.Tensor.pow(Tensor([44, 128, 96, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([44, 128, 96, 96],"float32"), 2, ) 	 51904512 	 26857 	 10.165894985198975 	 8.161914110183716 	 0.388336181640625 	 0.31057000160217285 	 12.38266921043396 	 28.86331343650818 	 0.4710085391998291 	 0.3661940097808838 	 
2025-08-04 14:41:12.774153 test begin: paddle.Tensor.prod(Tensor([1, 386, 65856, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([1, 386, 65856, 2],"float32"), -1, ) 	 50840832 	 25773 	 10.004741191864014 	 12.060635328292847 	 0.39682555198669434 	 0.4785909652709961 	 43.21201229095459 	 53.00742864608765 	 1.7135069370269775 	 0.000701904296875 	 
2025-08-04 14:43:12.397917 test begin: paddle.Tensor.prod(Tensor([1, 400, 63505, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([1, 400, 63505, 2],"float32"), -1, ) 	 50804000 	 25773 	 10.04031491279602 	 12.053657293319702 	 0.39827895164489746 	 0.47789788246154785 	 43.23765325546265 	 52.96049880981445 	 1.7143912315368652 	 0.0007107257843017578 	 
2025-08-04 14:45:12.013796 test begin: paddle.Tensor.prod(Tensor([1, 400, 65856, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([1, 400, 65856, 2],"float32"), -1, ) 	 52684800 	 25773 	 10.373689889907837 	 12.515930891036987 	 0.41138696670532227 	 0.495802640914917 	 44.83686137199402 	 54.849568128585815 	 1.7778594493865967 	 0.0007309913635253906 	 
2025-08-04 14:47:18.109332 test begin: paddle.Tensor.prod(Tensor([2100, 12096, 3],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([2100, 12096, 3],"float32"), -1, ) 	 76204800 	 25773 	 10.674719333648682 	 13.874259233474731 	 0.4232933521270752 	 0.5239148139953613 	 48.73265027999878 	 70.11165356636047 	 1.933501958847046 	 0.0009999275207519531 	 
2025-08-04 14:49:45.249989 test begin: paddle.Tensor.prod(Tensor([2100, 12097, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([2100, 12097, 2],"float32"), -1, ) 	 50807400 	 25773 	 10.010036945343018 	 12.055443525314331 	 0.3969230651855469 	 0.47808194160461426 	 32.6736843585968 	 52.943676471710205 	 1.295729160308838 	 0.0007088184356689453 	 
2025-08-04 14:51:38.675518 test begin: paddle.Tensor.prod(Tensor([2101, 12096, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([2101, 12096, 2],"float32"), -1, ) 	 50827392 	 25773 	 10.01260232925415 	 12.05397367477417 	 0.3972012996673584 	 0.47826290130615234 	 32.64879894256592 	 52.96527290344238 	 1.2942633628845215 	 0.0006732940673828125 	 
2025-08-04 14:53:27.676171 test begin: paddle.Tensor.prod(Tensor([4, 525, 12096, 3],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([4, 525, 12096, 3],"float32"), -1, ) 	 76204800 	 25773 	 10.663242816925049 	 13.215148687362671 	 0.42291951179504395 	 0.5239934921264648 	 64.57535648345947 	 70.1129674911499 	 2.5606179237365723 	 0.001005411148071289 	 
2025-08-04 14:56:08.043209 test begin: paddle.Tensor.prod(Tensor([4, 525, 12097, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([4, 525, 12097, 2],"float32"), -1, ) 	 50807400 	 25773 	 10.009389400482178 	 12.057329416275024 	 0.39691758155822754 	 0.47812914848327637 	 43.25612258911133 	 52.955506324768066 	 1.7159662246704102 	 0.0007104873657226562 	 
2025-08-04 14:58:07.660086 test begin: paddle.Tensor.prod(Tensor([4, 526, 12096, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([4, 526, 12096, 2],"float32"), -1, ) 	 50899968 	 25773 	 10.026246547698975 	 12.083516359329224 	 0.3975858688354492 	 0.4791882038116455 	 43.327656269073486 	 53.06958293914795 	 1.7183971405029297 	 0.0007107257843017578 	 
2025-08-04 15:00:07.516230 test begin: paddle.Tensor.prod(Tensor([5, 525, 12096, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([5, 525, 12096, 2],"float32"), -1, ) 	 63504000 	 25773 	 12.474153757095337 	 15.045043230056763 	 0.49469637870788574 	 0.5957844257354736 	 54.01758003234863 	 65.6382577419281 	 2.14140248298645 	 0.0008900165557861328 	 
2025-08-04 15:02:38.608581 test begin: paddle.Tensor.quantile(Tensor([3, 405, 3, 4, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 405, 3, 4, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, ) 	 145800 	 17789 	 291.0883300304413 	 3.892550230026245 	 0.4255664348602295 	 8.0108642578125e-05 	 3.4788336753845215 	 4.530402421951294 	 4.673004150390625e-05 	 7.390975952148438e-05 	 
2025-08-04 15:07:42.300007 test begin: paddle.Tensor.quantile(Tensor([3, 405, 3, 4, 2, 5],"float64"), q=0.75, axis=5, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 405, 3, 4, 2, 5],"float64"), q=0.75, axis=5, ) 	 145800 	 17789 	 233.23376488685608 	 6.494369745254517 	 0.36841630935668945 	 0.0002474784851074219 	 3.2292182445526123 	 3.9371914863586426 	 4.00543212890625e-05 	 8.678436279296875e-05 	 
2025-08-04 15:11:50.363277 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 288],"float64"), q=0.75, axis=3, keepdim=True, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 288],"float64"), q=0.75, axis=3, keepdim=True, ) 	 124416 	 17789 	 248.698335647583 	 3.9223480224609375 	 0.36345839500427246 	 7.724761962890625e-05 	 3.5133321285247803 	 4.313072204589844 	 4.792213439941406e-05 	 7.843971252441406e-05 	 
2025-08-04 15:16:10.836688 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 288],"float64"), q=0.75, axis=5, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 288],"float64"), q=0.75, axis=5, ) 	 124416 	 17789 	 8.929620504379272 	 3.6500582695007324 	 5.316734313964844e-05 	 9.369850158691406e-05 	 3.25722336769104 	 4.002400875091553 	 5.364418029785156e-05 	 0.00015974044799804688 	 
2025-08-04 15:16:30.689828 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 235, 5],"float64"), q=0.75, axis=3, keepdim=True, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 235, 5],"float64"), q=0.75, axis=3, keepdim=True, ) 	 253800 	 17789 	 504.2188196182251 	 4.189942121505737 	 0.7367737293243408 	 7.581710815429688e-05 	 3.4786343574523926 	 4.324272632598877 	 5.221366882324219e-05 	 0.00018262863159179688 	 
2025-08-04 15:25:09.788987 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 235, 5],"float64"), q=0.75, axis=5, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 235, 5],"float64"), q=0.75, axis=5, ) 	 253800 	 17789 	 404.0142946243286 	 3.7351744174957275 	 0.638077974319458 	 8.058547973632812e-05 	 3.262073040008545 	 3.8798906803131104 	 5.173683166503906e-05 	 7.343292236328125e-05 	 
2025-08-04 15:32:04.717869 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 470, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 3, 470, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, ) 	 253800 	 17789 	 9.937864065170288 	 3.770549774169922 	 0.014160871505737305 	 8.845329284667969e-05 	 3.5506248474121094 	 4.318490982055664 	 5.14984130859375e-05 	 0.0001533031463623047 	 
2025-08-04 15:32:26.308637 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 470, 2, 5],"float64"), q=0.75, axis=5, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 3, 470, 2, 5],"float64"), q=0.75, axis=5, ) 	 253800 	 17789 	 404.09686756134033 	 3.690772533416748 	 0.6382086277008057 	 0.00018644332885742188 	 3.169999837875366 	 3.8586068153381348 	 4.887580871582031e-05 	 7.128715515136719e-05 	 
2025-08-04 15:39:21.161892 test begin: paddle.Tensor.quantile(Tensor([3, 6, 352, 4, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 352, 4, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, ) 	 253440 	 17789 	 503.921728849411 	 3.84706449508667 	 0.7364451885223389 	 7.867813110351562e-05 	 3.4122250080108643 	 4.281853199005127 	 4.9591064453125e-05 	 7.939338684082031e-05 	 
2025-08-04 15:47:56.667994 test begin: paddle.Tensor.quantile(Tensor([3, 6, 352, 4, 2, 5],"float64"), q=0.75, axis=5, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 352, 4, 2, 5],"float64"), q=0.75, axis=5, ) 	 253440 	 17789 	 403.6001260280609 	 3.6755597591400146 	 0.637561559677124 	 0.00016427040100097656 	 3.1687510013580322 	 3.8739709854125977 	 4.220008850097656e-05 	 6.556510925292969e-05 	 
2025-08-04 15:54:52.611450 test begin: paddle.Tensor.quantile(Tensor([352, 6, 3, 4, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([352, 6, 3, 4, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, ) 	 253440 	 17789 	 503.95433139801025 	 3.8156871795654297 	 0.736558198928833 	 7.653236389160156e-05 	 3.589956045150757 	 4.335885524749756 	 5.0067901611328125e-05 	 6.961822509765625e-05 	 
2025-08-04 16:03:28.349969 test begin: paddle.Tensor.quantile(Tensor([352, 6, 3, 4, 2, 5],"float64"), q=0.75, axis=5, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([352, 6, 3, 4, 2, 5],"float64"), q=0.75, axis=5, ) 	 253440 	 17789 	 403.589994430542 	 3.7296297550201416 	 0.6373615264892578 	 0.00010704994201660156 	 3.1564323902130127 	 4.036773204803467 	 4.0531158447265625e-05 	 6.866455078125e-05 	 
2025-08-04 16:10:22.911800 test begin: paddle.Tensor.rad2deg(x=Tensor([1587601, 4, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([1587601, 4, 4],"float64"), ) 	 25401616 	 33553 	 9.999221563339233 	 10.006484508514404 	 0.30461931228637695 	 0.3046703338623047 	 10.00126576423645 	 10.00264596939087 	 0.30464649200439453 	 0.30469799041748047 	 
2025-08-04 16:11:04.096622 test begin: paddle.Tensor.rad2deg(x=Tensor([396901, 4, 4, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 33553 	 9.995666980743408 	 10.002622604370117 	 0.3044450283050537 	 0.30466270446777344 	 10.000760316848755 	 10.002681970596313 	 0.30460357666015625 	 0.30459117889404297 	 
2025-08-04 16:11:47.874790 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 1587601, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 1587601, 4],"float64"), ) 	 25401616 	 33553 	 9.999452590942383 	 10.004562854766846 	 0.3045797348022461 	 0.30462121963500977 	 10.00122857093811 	 10.00243353843689 	 0.30460500717163086 	 0.3047513961791992 	 
2025-08-04 16:12:29.019555 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 33553 	 9.995738983154297 	 10.484726428985596 	 0.3044755458831787 	 0.30474042892456055 	 10.000726699829102 	 10.002747535705566 	 0.3045969009399414 	 0.304703950881958 	 
2025-08-04 16:13:12.410376 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 4, 1587601],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 4, 1587601],"float64"), ) 	 25401616 	 33553 	 9.99942922592163 	 10.00279450416565 	 0.30459046363830566 	 0.30472350120544434 	 10.001209259033203 	 10.002832174301147 	 0.3046088218688965 	 0.3046894073486328 	 
2025-08-04 16:13:53.580585 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 33553 	 9.995776891708374 	 10.002262353897095 	 0.3044712543487549 	 0.30463624000549316 	 10.00068187713623 	 10.002806425094604 	 0.30463719367980957 	 0.3046736717224121 	 
2025-08-04 16:14:34.719954 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 33553 	 9.995697021484375 	 10.00241231918335 	 0.304443359375 	 0.30474042892456055 	 10.000439643859863 	 10.002779245376587 	 0.30461692810058594 	 0.3046879768371582 	 
2025-08-04 16:15:15.888522 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 6350401],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 6350401],"float64"), ) 	 25401604 	 33553 	 9.999347448348999 	 10.7147696018219 	 0.30455899238586426 	 0.30465149879455566 	 10.000946044921875 	 10.002691268920898 	 0.30465054512023926 	 0.30463337898254395 	 
2025-08-04 16:15:58.757604 test begin: paddle.Tensor.rad2deg(x=Tensor([6350401, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([6350401, 4],"float64"), ) 	 25401604 	 33553 	 9.999319076538086 	 10.02118730545044 	 0.3045682907104492 	 0.3047139644622803 	 10.001269817352295 	 10.002985954284668 	 0.30463337898254395 	 0.30460500717163086 	 
2025-08-04 16:16:42.958825 test begin: paddle.Tensor.rank(Tensor([2560, 1536, 3, 44],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 1536, 3, 44],"float32"), ) 	 519045120 	 242345 	 9.549408435821533 	 7.004273414611816 	 4.076957702636719e-05 	 9.751319885253906e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:17:14.518868 test begin: paddle.Tensor.rank(Tensor([2560, 1536, 44, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 1536, 44, 3],"float32"), ) 	 519045120 	 242345 	 9.62273097038269 	 7.0191028118133545 	 3.814697265625e-05 	 6.437301635742188e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:17:47.513829 test begin: paddle.Tensor.rank(Tensor([2560, 2048, 3, 33],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 2048, 3, 33],"float32"), ) 	 519045120 	 242345 	 9.609171152114868 	 6.97451114654541 	 7.104873657226562e-05 	 0.00022125244140625 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:18:19.551340 test begin: paddle.Tensor.rank(Tensor([2560, 2048, 33, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 2048, 33, 3],"float32"), ) 	 519045120 	 242345 	 9.696005821228027 	 7.063015699386597 	 7.772445678710938e-05 	 0.00022149085998535156 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:18:53.101390 test begin: paddle.Tensor.rank(Tensor([2560, 22051, 3, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 22051, 3, 3],"float32"), ) 	 508055040 	 242345 	 9.917112350463867 	 7.147215127944946 	 6.985664367675781e-05 	 0.00021886825561523438 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:19:25.179242 test begin: paddle.Tensor.rank(Tensor([2560, 768, 3, 87],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 768, 3, 87],"float32"), ) 	 513146880 	 242345 	 9.999414443969727 	 7.699113845825195 	 6.437301635742188e-05 	 0.00023603439331054688 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:20:00.221755 test begin: paddle.Tensor.rank(Tensor([2560, 768, 87, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 768, 87, 3],"float32"), ) 	 513146880 	 242345 	 9.956267595291138 	 7.023967266082764 	 6.198883056640625e-05 	 0.00018739700317382812 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:20:32.330432 test begin: paddle.Tensor.rank(Tensor([27570, 2048, 3, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([27570, 2048, 3, 3],"float32"), ) 	 508170240 	 242345 	 9.899243831634521 	 7.00015115737915 	 7.033348083496094e-05 	 0.0002281665802001953 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:21:05.552127 test begin: paddle.Tensor.rank(Tensor([36760, 1536, 3, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([36760, 1536, 3, 3],"float32"), ) 	 508170240 	 242345 	 10.0045006275177 	 7.034984588623047 	 6.604194641113281e-05 	 0.00021839141845703125 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:21:39.334777 test begin: paddle.Tensor.rank(Tensor([73510, 768, 3, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([73510, 768, 3, 3],"float32"), ) 	 508101120 	 242345 	 10.153661727905273 	 7.117979526519775 	 6.747245788574219e-05 	 0.00022482872009277344 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 16:22:11.616200 test begin: paddle.Tensor.reciprocal(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33834 	 9.996924877166748 	 11.254311561584473 	 0.3020353317260742 	 0.3046989440917969 	 15.22987675666809 	 35.19871401786804 	 0.4600682258605957 	 0.35446596145629883 	 
2025-08-04 16:23:25.448233 test begin: paddle.Tensor.reciprocal(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33834 	 9.995122909545898 	 10.080195665359497 	 0.3018829822540283 	 0.304487943649292 	 15.220365285873413 	 35.195497274398804 	 0.45980262756347656 	 0.3542451858520508 	 
2025-08-04 16:24:39.815776 test begin: paddle.Tensor.reciprocal(Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([10, 5080321],"float32"), ) 	 50803210 	 33834 	 9.995179653167725 	 10.080323219299316 	 0.3019096851348877 	 0.30446362495422363 	 15.219944715499878 	 35.19742941856384 	 0.45978546142578125 	 0.3544182777404785 	 
2025-08-04 16:25:52.089179 test begin: paddle.Tensor.reciprocal(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33834 	 9.995417356491089 	 10.084118604660034 	 0.301973819732666 	 0.3045022487640381 	 15.220212936401367 	 35.1969211101532 	 0.459719181060791 	 0.3543741703033447 	 
2025-08-04 16:27:07.178385 test begin: paddle.Tensor.reciprocal(Tensor([2540161, 20],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([2540161, 20],"float32"), ) 	 50803220 	 33834 	 9.995268821716309 	 10.083428382873535 	 0.3019592761993408 	 0.30445432662963867 	 15.220218896865845 	 35.19749855995178 	 0.45972108840942383 	 0.3544747829437256 	 
2025-08-04 16:28:19.440070 test begin: paddle.Tensor.reciprocal(Tensor([4233601, 12],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([4233601, 12],"float32"), ) 	 50803212 	 33834 	 9.995128631591797 	 10.080052852630615 	 0.3019440174102783 	 0.3044884204864502 	 15.220835447311401 	 35.196802854537964 	 0.4597656726837158 	 0.35444021224975586 	 
2025-08-04 16:29:31.713408 test begin: paddle.Tensor.remainder(Tensor([2, 3, 8467201],"float32"), Tensor([2, 3, 8467201],"float32"), )
[Prof] paddle.Tensor.remainder 	 paddle.Tensor.remainder(Tensor([2, 3, 8467201],"float32"), Tensor([2, 3, 8467201],"float32"), ) 	 101606412 	 22180 	 9.994965076446533 	 9.983299732208252 	 0.4606661796569824 	 0.4589383602142334 	 None 	 None 	 None 	 None 	 
2025-08-04 16:29:54.199064 test begin: paddle.Tensor.remainder(Tensor([2, 6350401, 4],"float32"), Tensor([2, 6350401, 4],"float32"), )
[Prof] paddle.Tensor.remainder 	 paddle.Tensor.remainder(Tensor([2, 6350401, 4],"float32"), Tensor([2, 6350401, 4],"float32"), ) 	 101606416 	 22180 	 9.994603633880615 	 9.960469007492065 	 0.4605722427368164 	 0.45888495445251465 	 None 	 None 	 None 	 None 	 
2025-08-04 16:30:15.909386 test begin: paddle.Tensor.remainder(Tensor([4233601, 3, 4],"float32"), Tensor([4233601, 3, 4],"float32"), )
[Prof] paddle.Tensor.remainder 	 paddle.Tensor.remainder(Tensor([4233601, 3, 4],"float32"), Tensor([4233601, 3, 4],"float32"), ) 	 101606424 	 22180 	 9.99525761604309 	 9.959567785263062 	 0.4605247974395752 	 0.45891308784484863 	 None 	 None 	 None 	 None 	 
2025-08-04 16:30:37.682333 test begin: paddle.Tensor.repeat_interleave(Tensor([1, 1, 198451, 128],"float64"), 3, axis=1, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(Tensor([1, 1, 198451, 128],"float64"), 3, axis=1, ) 	 25401728 	 16163 	 14.495814561843872 	 14.36171841621399 	 0.4582836627960205 	 0.9080810546875 	 24.002474069595337 	 9.498745203018188 	 0.5062601566314697 	 0.60062575340271 	 
2025-08-04 16:31:45.522328 test begin: paddle.Tensor.repeat_interleave(Tensor([1, 1, 64, 396901],"float64"), 3, axis=1, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(Tensor([1, 1, 64, 396901],"float64"), 3, axis=1, ) 	 25401664 	 16163 	 14.516716480255127 	 14.262499570846558 	 0.45898008346557617 	 0.901848316192627 	 24.001489400863647 	 9.497763395309448 	 0.5062112808227539 	 0.6005294322967529 	 
2025-08-04 16:32:50.078082 test begin: paddle.Tensor.repeat_interleave(Tensor([1, 3101, 64, 128],"float64"), 3, axis=1, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(Tensor([1, 3101, 64, 128],"float64"), 3, axis=1, ) 	 25403392 	 16163 	 11.242289543151855 	 10.30311393737793 	 0.07376289367675781 	 0.6514356136322021 	 14.833388090133667 	 9.60598111152649 	 0.09437298774719238 	 0.607379674911499 	 
2025-08-04 16:33:39.610127 test begin: paddle.Tensor.repeat_interleave(Tensor([3101, 1, 64, 128],"float64"), 3, axis=1, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(Tensor([3101, 1, 64, 128],"float64"), 3, axis=1, ) 	 25403392 	 16163 	 10.000273942947388 	 10.30416750907898 	 0.31618571281433105 	 0.6514174938201904 	 14.81695818901062 	 9.60609245300293 	 0.3124556541442871 	 0.6073863506317139 	 
2025-08-04 16:34:26.635410 test begin: paddle.Tensor.repeat_interleave(x=Tensor([158761, 2, 4, 4, 5],"float64"), repeats=2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fad046995a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:44:31.790457 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 158761, 4, 5],"float64"), repeats=2, )
W0804 16:44:32.450233 28067 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3e5198ed10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 16:54:36.842225 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 158761, 5],"float64"), repeats=2, )
W0804 16:54:41.253985 31965 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2bf5373070>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754298277 (unix time) try "date -d @1754298277" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x7c3b) received by PID 31803 (TID 0x7f2bec3f9640) from PID 31803 ***]

2025-08-04 17:04:46.133165 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 198451],"float64"), repeats=2, )
W0804 17:04:46.823222 34549 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7d33303070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:14:50.869409 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 79381, 4, 4, 5],"float64"), repeats=2, )
W0804 17:14:51.535562 37119 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcc43e6eef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 17:24:55.353077 test begin: paddle.Tensor.reshape(Tensor([124040, 8192],"bfloat16"), list[-1,8192,], )
W0804 17:25:11.210623 39766 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.reshape 	 paddle.Tensor.reshape(Tensor([124040, 8192],"bfloat16"), list[-1,8192,], ) 	 1016135680 	 66716 	 0.35624098777770996 	 0.2835574150085449 	 3.647804260253906e-05 	 6.699562072753906e-05 	 3.058516502380371 	 299.9598867893219 	 4.315376281738281e-05 	 2.2973852157592773 	 
2025-08-04 17:30:32.792399 test begin: paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 19459 	 11.137124061584473 	 5.9002275466918945 	 0.5848753452301025 	 0.3098561763763428 	 17.16251516342163 	 5.908550024032593 	 0.45062685012817383 	 0.3103463649749756 	 
2025-08-04 17:31:14.880596 test begin: paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 19459 	 17.092694520950317 	 5.904973745346069 	 0.4488029479980469 	 0.3098454475402832 	 11.165932655334473 	 5.900871992111206 	 0.5863540172576904 	 0.30986595153808594 	 
2025-08-04 17:31:56.072749 test begin: paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 19459 	 17.010302305221558 	 5.900376796722412 	 0.44668102264404297 	 0.3099100589752197 	 11.16484785079956 	 5.89388370513916 	 0.5863804817199707 	 0.3095433712005615 	 
2025-08-04 17:32:39.054124 test begin: paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 19459 	 11.27122187614441 	 5.916173458099365 	 0.5919797420501709 	 0.31072020530700684 	 17.273828506469727 	 5.884894371032715 	 0.45357775688171387 	 0.30905628204345703 	 
2025-08-04 17:33:20.540100 test begin: paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 19459 	 19.752870798110962 	 5.918820858001709 	 0.5188474655151367 	 0.31070685386657715 	 11.332587480545044 	 5.917676687240601 	 0.5951693058013916 	 0.3107476234436035 	 
2025-08-04 17:34:04.592335 test begin: paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 19459 	 17.0161874294281 	 5.900458812713623 	 0.4468662738800049 	 0.3099057674407959 	 11.168224811553955 	 5.893808841705322 	 0.5865561962127686 	 0.3095130920410156 	 
2025-08-04 17:34:46.283169 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 19459 	 11.297952890396118 	 5.917500019073486 	 0.5933122634887695 	 0.3107895851135254 	 17.291007041931152 	 5.908323764801025 	 0.45403361320495605 	 0.31027817726135254 	 
2025-08-04 17:35:27.823723 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 19459 	 17.32326030731201 	 5.917644739151001 	 0.45498108863830566 	 0.310774564743042 	 11.167643547058105 	 5.900878667831421 	 0.5864157676696777 	 0.30989980697631836 	 
2025-08-04 17:36:09.378544 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 19459 	 17.165709257125854 	 8.059028625488281 	 0.45080065727233887 	 0.31089210510253906 	 11.307247400283813 	 5.946019411087036 	 0.5938291549682617 	 0.31226539611816406 	 
2025-08-04 17:36:54.998573 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 19459 	 11.299077272415161 	 5.917659521102905 	 0.5935320854187012 	 0.310802698135376 	 17.291844606399536 	 5.9085118770599365 	 0.4540090560913086 	 0.3103036880493164 	 
2025-08-04 17:37:38.354358 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 19459 	 17.351075649261475 	 5.9177327156066895 	 0.4557182788848877 	 0.3108787536621094 	 11.317561626434326 	 5.955170631408691 	 0.5939149856567383 	 0.3127148151397705 	 
2025-08-04 17:38:20.044674 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 19459 	 16.93868899345398 	 5.962173223495483 	 0.4446895122528076 	 0.3129231929779053 	 11.16103744506836 	 5.893861532211304 	 0.5864455699920654 	 0.30952954292297363 	 
2025-08-04 17:39:01.148829 test begin: paddle.Tensor.round(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.Tensor.round 	 paddle.Tensor.round(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33823 	 10.00245189666748 	 10.068814039230347 	 0.30219244956970215 	 0.3042335510253906 	 4.536244869232178 	 4.533745288848877 	 0.13686084747314453 	 0.13694095611572266 	 
2025-08-04 17:39:32.042470 test begin: paddle.Tensor.round(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.Tensor.round 	 paddle.Tensor.round(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33823 	 10.000582933425903 	 10.073760032653809 	 0.30220627784729004 	 0.3042943477630615 	 4.536523342132568 	 4.534281015396118 	 0.13698816299438477 	 0.13691926002502441 	 
2025-08-04 17:40:02.902968 test begin: paddle.Tensor.round(Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.round 	 paddle.Tensor.round(Tensor([10, 5080321],"float32"), ) 	 50803210 	 33823 	 10.000252962112427 	 10.0704026222229 	 0.30217695236206055 	 0.30422234535217285 	 4.537401914596558 	 4.534466743469238 	 0.13740205764770508 	 0.136918306350708 	 
2025-08-04 17:40:34.005535 test begin: paddle.Tensor.round(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.Tensor.round 	 paddle.Tensor.round(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33823 	 10.00158977508545 	 10.068528890609741 	 0.30212855339050293 	 0.30423879623413086 	 4.536347389221191 	 4.534644842147827 	 0.13633418083190918 	 0.1369459629058838 	 
2025-08-04 17:41:04.848234 test begin: paddle.Tensor.round(Tensor([2540161, 20],"float32"), )
[Prof] paddle.Tensor.round 	 paddle.Tensor.round(Tensor([2540161, 20],"float32"), ) 	 50803220 	 33823 	 10.000359296798706 	 10.068474769592285 	 0.30217528343200684 	 0.30426740646362305 	 4.535703897476196 	 4.539819240570068 	 0.1370401382446289 	 0.13696026802062988 	 
2025-08-04 17:41:38.558891 test begin: paddle.Tensor.rsqrt(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.Tensor.rsqrt 	 paddle.Tensor.rsqrt(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33809 	 10.000379085540771 	 10.065552234649658 	 0.3022880554199219 	 0.30428361892700195 	 15.219189405441284 	 35.174487829208374 	 0.46001505851745605 	 0.354489803314209 	 
2025-08-04 17:42:50.885381 test begin: paddle.Tensor.rsqrt(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.Tensor.rsqrt 	 paddle.Tensor.rsqrt(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33809 	 9.998515605926514 	 10.06507134437561 	 0.3022949695587158 	 0.30426478385925293 	 15.210541725158691 	 35.17492699623108 	 0.45984601974487305 	 0.3544926643371582 	 
2025-08-04 17:44:03.082313 test begin: paddle.Tensor.rsqrt(Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.rsqrt 	 paddle.Tensor.rsqrt(Tensor([10, 5080321],"float32"), ) 	 50803210 	 33809 	 9.998149871826172 	 10.065443754196167 	 0.3022305965423584 	 0.30427002906799316 	 15.210743427276611 	 35.17624545097351 	 0.4597940444946289 	 0.35443854331970215 	 
2025-08-04 17:45:17.673314 test begin: paddle.Tensor.rsqrt(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.Tensor.rsqrt 	 paddle.Tensor.rsqrt(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33809 	 9.998229503631592 	 10.067037582397461 	 0.3022582530975342 	 0.30428504943847656 	 15.21066403388977 	 35.174866676330566 	 0.45979881286621094 	 0.3544316291809082 	 
2025-08-04 17:46:30.007411 test begin: paddle.Tensor.rsqrt(Tensor([2540161, 20],"float32"), )
[Prof] paddle.Tensor.rsqrt 	 paddle.Tensor.rsqrt(Tensor([2540161, 20],"float32"), ) 	 50803220 	 33809 	 9.998188018798828 	 10.080636978149414 	 0.3021988868713379 	 0.304232120513916 	 15.211062908172607 	 35.17502737045288 	 0.4598195552825928 	 0.3544607162475586 	 
2025-08-04 17:47:43.372070 test begin: paddle.Tensor.scale(Tensor([100352, 1013],"bfloat16"), 0.006378560586546936, )
[Prof] paddle.Tensor.scale 	 paddle.Tensor.scale(Tensor([100352, 1013],"bfloat16"), 0.006378560586546936, ) 	 101656576 	 33533 	 10.002752780914307 	 19.86658477783203 	 0.30484700202941895 	 0.3027372360229492 	 19.719507932662964 	 25.14212965965271 	 0.600977897644043 	 0.3830890655517578 	 combined
2025-08-04 17:49:01.606473 test begin: paddle.Tensor.scale(Tensor([1013, 100352],"bfloat16"), 0.006378560586546936, )
[Prof] paddle.Tensor.scale 	 paddle.Tensor.scale(Tensor([1013, 100352],"bfloat16"), 0.006378560586546936, ) 	 101656576 	 33533 	 10.002846240997314 	 19.87366008758545 	 0.3048579692840576 	 0.3028073310852051 	 19.71979284286499 	 25.140265464782715 	 0.6010024547576904 	 0.3831501007080078 	 combined
2025-08-04 17:50:19.900273 test begin: paddle.Tensor.scale(Tensor([12404, 8192],"bfloat16"), 0.006378560586546936, )
[Prof] paddle.Tensor.scale 	 paddle.Tensor.scale(Tensor([12404, 8192],"bfloat16"), 0.006378560586546936, ) 	 101613568 	 33533 	 9.998313188552856 	 19.85806655883789 	 0.30472493171691895 	 0.30258655548095703 	 19.727271556854248 	 25.129978895187378 	 0.6011307239532471 	 0.38289570808410645 	 combined
2025-08-04 17:51:39.311579 test begin: paddle.Tensor.scale(Tensor([1772, 57344],"bfloat16"), 0.006378560586546936, )
[Prof] paddle.Tensor.scale 	 paddle.Tensor.scale(Tensor([1772, 57344],"bfloat16"), 0.006378560586546936, ) 	 101613568 	 33533 	 9.998550653457642 	 19.86158013343811 	 0.3047621250152588 	 0.30260372161865234 	 19.72589874267578 	 25.129859924316406 	 0.6012651920318604 	 0.38286662101745605 	 combined
2025-08-04 17:52:57.713690 test begin: paddle.Tensor.scale(Tensor([8192, 12404],"bfloat16"), 0.006378560586546936, )
[Prof] paddle.Tensor.scale 	 paddle.Tensor.scale(Tensor([8192, 12404],"bfloat16"), 0.006378560586546936, ) 	 101613568 	 33533 	 9.998188495635986 	 19.85860013961792 	 0.30471372604370117 	 0.30270934104919434 	 19.726147413253784 	 25.130019426345825 	 0.6012017726898193 	 0.3829317092895508 	 combined
2025-08-04 17:54:15.843404 test begin: paddle.Tensor.set_(Tensor([2001],"bool"), Tensor([15, 3386881],"bool"), list[20,], list[2,], 0, )
[Prof] paddle.Tensor.set_ 	 paddle.Tensor.set_(Tensor([2001],"bool"), Tensor([15, 3386881],"bool"), list[20,], list[2,], 0, ) 	 50805216 	 272766 	 9.820904970169067 	 0.5922951698303223 	 6.437301635742188e-05 	 6.890296936035156e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 17:54:36.646273 test begin: paddle.Tensor.set_(Tensor([2001],"bool"), Tensor([16934401, 3],"bool"), list[20,], list[2,], 0, )
[Prof] paddle.Tensor.set_ 	 paddle.Tensor.set_(Tensor([2001],"bool"), Tensor([16934401, 3],"bool"), list[20,], list[2,], 0, ) 	 50805204 	 272766 	 9.741041421890259 	 0.594092607498169 	 6.628036499023438e-05 	 4.76837158203125e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 17:54:56.253491 test begin: paddle.Tensor.set_(Tensor([50803201],"bool"), Tensor([1501, 3],"bool"), list[20,], list[2,], 0, )
[Prof] paddle.Tensor.set_ 	 paddle.Tensor.set_(Tensor([50803201],"bool"), Tensor([1501, 3],"bool"), list[20,], list[2,], 0, ) 	 50807704 	 272766 	 9.685233116149902 	 0.5986816883087158 	 6.198883056640625e-05 	 7.367134094238281e-05 	 None 	 None 	 None 	 None 	 
2025-08-04 17:55:15.765428 test begin: paddle.Tensor.sigmoid(Tensor([1, 1100, 46185],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1, 1100, 46185],"float32"), ) 	 50803500 	 33917 	 9.998785257339478 	 10.143588781356812 	 0.30124902725219727 	 0.3047957420349121 	 15.259631872177124 	 15.1453275680542 	 0.45984435081481934 	 0.4563753604888916 	 
2025-08-04 17:56:09.935100 test begin: paddle.Tensor.sigmoid(Tensor([1, 12700801, 4],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1, 12700801, 4],"float32"), ) 	 50803204 	 33917 	 9.999269247055054 	 10.122493505477905 	 0.30130553245544434 	 0.30477404594421387 	 15.259896039962769 	 15.14525055885315 	 0.4598567485809326 	 0.4563710689544678 	 
2025-08-04 17:57:03.849207 test begin: paddle.Tensor.sigmoid(Tensor([1, 6380, 7963],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1, 6380, 7963],"float32"), ) 	 50803940 	 33917 	 10.001407861709595 	 10.113769054412842 	 0.30139946937561035 	 0.3047504425048828 	 15.268967628479004 	 15.14536190032959 	 0.4600808620452881 	 0.45633411407470703 	 
2025-08-04 17:57:56.101982 test begin: paddle.Tensor.sigmoid(Tensor([1, 8550, 5942],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1, 8550, 5942],"float32"), ) 	 50804100 	 33917 	 9.999114751815796 	 10.113744258880615 	 0.30132436752319336 	 0.3047177791595459 	 15.260132551193237 	 15.145725727081299 	 0.45977091789245605 	 0.4563772678375244 	 
2025-08-04 17:58:48.334323 test begin: paddle.Tensor.sigmoid(Tensor([11547, 1100, 4],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([11547, 1100, 4],"float32"), ) 	 50806800 	 33917 	 10.00512981414795 	 10.11539363861084 	 0.3014695644378662 	 0.3047349452972412 	 15.258005857467651 	 15.146629095077515 	 0.4597187042236328 	 0.45638084411621094 	 
2025-08-04 17:59:43.129781 test begin: paddle.Tensor.sigmoid(Tensor([1486, 8550, 4],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1486, 8550, 4],"float32"), ) 	 50821200 	 33917 	 10.000372648239136 	 10.117172479629517 	 0.3013570308685303 	 0.30484437942504883 	 15.267698049545288 	 15.150062561035156 	 0.4599781036376953 	 0.456510066986084 	 
2025-08-04 18:00:39.212274 test begin: paddle.Tensor.sigmoid(Tensor([1991, 6380, 4],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1991, 6380, 4],"float32"), ) 	 50810320 	 33917 	 10.00391173362732 	 11.910269975662231 	 0.3014795780181885 	 0.30474233627319336 	 15.270042181015015 	 15.146121978759766 	 0.46019864082336426 	 0.45640134811401367 	 
2025-08-04 18:01:37.744988 test begin: paddle.Tensor.sign(Tensor([1016065, 5, 5],"float64"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([1016065, 5, 5],"float64"), ) 	 25401625 	 32403 	 10.000179052352905 	 9.66439938545227 	 0.3154122829437256 	 0.3047337532043457 	 9.655362606048584 	 4.3568644523620605 	 0.3045516014099121 	 0.13735651969909668 	 
2025-08-04 18:02:12.576161 test begin: paddle.Tensor.sign(Tensor([1124, 45199],"float32"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([1124, 45199],"float32"), ) 	 50803676 	 32403 	 11.167226314544678 	 9.644821882247925 	 0.35218024253845215 	 0.3041694164276123 	 9.584465265274048 	 4.344651937484741 	 0.3023204803466797 	 0.13689756393432617 	 
2025-08-04 18:02:49.635440 test begin: paddle.Tensor.sign(Tensor([12700801, 2],"float64"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([12700801, 2],"float64"), ) 	 25401602 	 32403 	 10.000766038894653 	 9.666028022766113 	 0.31539440155029297 	 0.30486249923706055 	 9.654679775238037 	 4.356946706771851 	 0.3045070171356201 	 0.13735270500183105 	 
2025-08-04 18:03:24.439102 test begin: paddle.Tensor.sign(Tensor([1587601, 32],"float32"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([1587601, 32],"float32"), ) 	 50803232 	 32403 	 11.168047666549683 	 9.643745183944702 	 0.35230398178100586 	 0.3041539192199707 	 9.584935903549194 	 4.347957134246826 	 0.30229997634887695 	 0.13692307472229004 	 
2025-08-04 18:04:03.586442 test begin: paddle.Tensor.sign(Tensor([50000, 102, 5],"float64"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([50000, 102, 5],"float64"), ) 	 25500000 	 32403 	 10.050736427307129 	 9.703183650970459 	 0.31699538230895996 	 0.30605483055114746 	 9.699642419815063 	 4.373700380325317 	 0.3059115409851074 	 0.13787293434143066 	 
2025-08-04 18:04:38.947892 test begin: paddle.Tensor.sign(Tensor([50000, 5, 102],"float64"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([50000, 5, 102],"float64"), ) 	 25500000 	 32403 	 10.050689458847046 	 9.702895879745483 	 0.31705665588378906 	 0.30608344078063965 	 9.699580192565918 	 4.3777992725372314 	 0.30588388442993164 	 0.13789701461791992 	 
2025-08-04 18:05:13.956585 test begin: paddle.Tensor.sign(Tensor([50000, 509],"float64"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([50000, 509],"float64"), ) 	 25450000 	 32403 	 10.018117904663086 	 9.700095176696777 	 0.31595349311828613 	 0.30541014671325684 	 9.67685341835022 	 4.365445137023926 	 0.3051638603210449 	 0.137620210647583 	 
2025-08-04 18:05:48.896795 test begin: paddle.Tensor.signbit(Tensor([12, 10584, 2],"float64"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([12, 10584, 2],"float64"), ) 	 254016 	 1000 	 10.136682271957397 	 0.010236978530883789 	 3.647804260253906e-05 	 3.552436828613281e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:05:59.114223 test begin: paddle.Tensor.signbit(Tensor([12, 20, 1058],"float64"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([12, 20, 1058],"float64"), ) 	 253920 	 1000 	 10.065425157546997 	 0.010185718536376953 	 4.0531158447265625e-05 	 3.075599670410156e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:06:09.236690 test begin: paddle.Tensor.signbit(Tensor([12, 20, 2116],"float32"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([12, 20, 2116],"float32"), ) 	 507840 	 1000 	 19.78151512145996 	 0.009979963302612305 	 3.838539123535156e-05 	 2.6226043701171875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:06:29.091441 test begin: paddle.Tensor.signbit(Tensor([12, 20, 4233],"int16"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([12, 20, 4233],"int16"), ) 	 1015920 	 1000 	 39.244507789611816 	 0.010330438613891602 	 4.4345855712890625e-05 	 4.315376281738281e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:07:08.436830 test begin: paddle.Tensor.signbit(Tensor([12, 21168, 2],"float32"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([12, 21168, 2],"float32"), ) 	 508032 	 1000 	 19.839240312576294 	 0.011676549911499023 	 4.458427429199219e-05 	 5.364418029785156e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:07:30.251465 test begin: paddle.Tensor.signbit(Tensor([12, 42336, 2],"int16"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([12, 42336, 2],"int16"), ) 	 1016064 	 1000 	 39.29947829246521 	 0.010338783264160156 	 4.076957702636719e-05 	 3.600120544433594e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:08:09.651925 test begin: paddle.Tensor.signbit(Tensor([12700, 20, 2],"float32"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([12700, 20, 2],"float32"), ) 	 508000 	 1000 	 19.773232460021973 	 0.009998559951782227 	 3.981590270996094e-05 	 2.6702880859375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:08:29.496074 test begin: paddle.Tensor.signbit(Tensor([25401, 20, 2],"int16"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([25401, 20, 2],"int16"), ) 	 1016040 	 1000 	 39.127944469451904 	 0.010449886322021484 	 4.5299530029296875e-05 	 3.933906555175781e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:09:08.721584 test begin: paddle.Tensor.signbit(Tensor([6350, 20, 2],"float64"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([6350, 20, 2],"float64"), ) 	 254000 	 1000 	 10.094490766525269 	 0.01011204719543457 	 2.1696090698242188e-05 	 2.7179718017578125e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-04 18:09:18.871673 test begin: paddle.Tensor.sin(Tensor([131072, 388],"float32"), )
[Prof] paddle.Tensor.sin 	 paddle.Tensor.sin(Tensor([131072, 388],"float32"), ) 	 50855936 	 33845 	 10.007251024246216 	 10.10178256034851 	 0.30217576026916504 	 0.30479955673217773 	 15.252654075622559 	 25.18976855278015 	 0.46056485176086426 	 0.3803069591522217 	 
2025-08-04 18:10:21.130037 test begin: paddle.Tensor.sin(Tensor([3175201, 16],"float32"), )
[Prof] paddle.Tensor.sin 	 paddle.Tensor.sin(Tensor([3175201, 16],"float32"), ) 	 50803216 	 33845 	 10.002164840698242 	 10.083745241165161 	 0.3019866943359375 	 0.30452919006347656 	 15.238448858261108 	 25.163913249969482 	 0.46015286445617676 	 0.37993574142456055 	 
2025-08-04 18:11:23.314190 test begin: paddle.Tensor.sin(Tensor([32768, 1551],"float32"), )
[Prof] paddle.Tensor.sin 	 paddle.Tensor.sin(Tensor([32768, 1551],"float32"), ) 	 50823168 	 33845 	 10.0013267993927 	 10.092641830444336 	 0.3019986152648926 	 0.3046302795410156 	 15.245346069335938 	 25.17386555671692 	 0.46037721633911133 	 0.38007450103759766 	 
2025-08-04 18:12:25.661037 test begin: paddle.Tensor.sin(Tensor([396901, 128],"float32"), )
[Prof] paddle.Tensor.sin 	 paddle.Tensor.sin(Tensor([396901, 128],"float32"), ) 	 50803328 	 33845 	 9.997268915176392 	 10.090946197509766 	 0.30190515518188477 	 0.3044881820678711 	 15.234899520874023 	 25.16535782814026 	 0.46006035804748535 	 0.3799421787261963 	 
2025-08-04 18:13:27.869498 test begin: paddle.Tensor.slice(Tensor([127008010, 4],"float32"), list[1,], list[0,], list[1,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8e5ca566e0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 18:23:46.314601 test begin: paddle.Tensor.slice(Tensor([40, 12700801],"float32"), list[1,], list[0,], list[1,], )
W0804 18:23:54.067391 54662 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.slice 	 paddle.Tensor.slice(Tensor([40, 12700801],"float32"), list[1,], list[0,], list[1,], ) 	 508032040 	 198496 	 2.4852755069732666 	 2.8046226501464844 	 0.00019502639770507812 	 8.320808410644531e-05 	 298.05084204673767 	 261.4839417934418 	 0.7677443027496338 	 0.6731100082397461 	 combined
2025-08-04 18:33:20.304936 test begin: paddle.Tensor.slice_scatter(Tensor([4233601, 6],"float64"), Tensor([4233601, 3],"float64"), list[1,], list[0,], list[6,], list[2,], )
[Prof] paddle.Tensor.slice_scatter 	 paddle.Tensor.slice_scatter(Tensor([4233601, 6],"float64"), Tensor([4233601, 3],"float64"), list[1,], list[0,], list[6,], list[2,], ) 	 38102409 	 32593 	 12.271195888519287 	 22.42221999168396 	 0.38475918769836426 	 0.2342684268951416 	 34.63818001747131 	 24.947338819503784 	 0.18100976943969727 	 0.19559073448181152 	 
2025-08-04 18:34:56.239722 test begin: paddle.Tensor.slice_scatter(Tensor([80, 3175201],"float64"), Tensor([80, 3],"float64"), list[1,], list[0,], list[6,], list[2,], )
[Prof] paddle.Tensor.slice_scatter 	 paddle.Tensor.slice_scatter(Tensor([80, 3175201],"float64"), Tensor([80, 3],"float64"), list[1,], list[0,], list[6,], list[2,], ) 	 254016320 	 32593 	 0.7572832107543945 	 99.99407601356506 	 0.00010037422180175781 	 1.0431067943572998 	 100.07837772369385 	 100.07626056671143 	 0.5219888687133789 	 0.7830085754394531 	 
2025-08-04 18:40:09.666864 test begin: paddle.Tensor.slice_scatter(Tensor([8467201, 6],"float64"), Tensor([8467201, 3],"float64"), list[1,], list[0,], list[6,], list[2,], )
[Prof] paddle.Tensor.slice_scatter 	 paddle.Tensor.slice_scatter(Tensor([8467201, 6],"float64"), Tensor([8467201, 3],"float64"), list[1,], list[0,], list[6,], list[2,], ) 	 76204809 	 32593 	 24.316396951675415 	 44.41675615310669 	 0.7625575065612793 	 0.464123010635376 	 68.4667136669159 	 49.05475425720215 	 0.35763072967529297 	 0.3844761848449707 	 
2025-08-04 18:43:19.427391 test begin: paddle.Tensor.sqrt(Tensor([276, 80, 48, 48],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([276, 80, 48, 48],"float32"), ) 	 50872320 	 33926 	 10.011895179748535 	 10.151662826538086 	 0.30160093307495117 	 0.3056929111480713 	 15.302334308624268 	 25.379716396331787 	 0.4609663486480713 	 0.3822798728942871 	 
2025-08-04 18:44:22.521821 test begin: paddle.Tensor.sqrt(Tensor([329, 80, 44, 44],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([329, 80, 44, 44],"float32"), ) 	 50955520 	 33926 	 10.028614282608032 	 10.162115335464478 	 0.3021092414855957 	 0.3061106204986572 	 15.3269624710083 	 25.421334266662598 	 0.46178674697875977 	 0.3828585147857666 	 
2025-08-04 18:45:25.791812 test begin: paddle.Tensor.sqrt(Tensor([397, 80, 40, 40],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([397, 80, 40, 40],"float32"), ) 	 50816000 	 33926 	 9.998838901519775 	 10.137242078781128 	 0.30121779441833496 	 0.3053719997406006 	 15.28615117073059 	 25.353809595108032 	 0.46047186851501465 	 0.38182806968688965 	 
2025-08-04 18:46:28.370642 test begin: paddle.Tensor.sqrt(Tensor([64, 345, 48, 48],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 345, 48, 48],"float32"), ) 	 50872320 	 33926 	 10.012035369873047 	 10.147840976715088 	 0.3016469478607178 	 0.3056972026824951 	 15.302581548690796 	 25.38091230392456 	 0.4609801769256592 	 0.382312536239624 	 
2025-08-04 18:47:31.062334 test begin: paddle.Tensor.sqrt(Tensor([64, 411, 44, 44],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 411, 44, 44],"float32"), ) 	 50924544 	 33926 	 10.026637077331543 	 10.156961679458618 	 0.3020620346069336 	 0.305983304977417 	 15.317378044128418 	 25.405636072158813 	 0.46146297454833984 	 0.38266706466674805 	 
2025-08-04 18:48:33.806561 test begin: paddle.Tensor.sqrt(Tensor([64, 497, 40, 40],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 497, 40, 40],"float32"), ) 	 50892800 	 33926 	 10.015969038009644 	 10.151119947433472 	 0.30170321464538574 	 0.30580663681030273 	 15.310242891311646 	 25.389829397201538 	 0.4612395763397217 	 0.3824474811553955 	 
2025-08-04 18:49:37.178107 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 207, 48],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 207, 48],"float32"), ) 	 50872320 	 33926 	 10.011796236038208 	 10.147696495056152 	 0.30162858963012695 	 0.3056812286376953 	 15.302984476089478 	 25.38098907470703 	 0.4609541893005371 	 0.3822927474975586 	 
2025-08-04 18:50:40.117163 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 226, 44],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 226, 44],"float32"), ) 	 50913280 	 33926 	 10.020127773284912 	 10.160122871398926 	 0.3018524646759033 	 0.30594873428344727 	 15.317620515823364 	 25.400490045547485 	 0.4613790512084961 	 0.38260746002197266 	 
2025-08-04 18:51:43.440633 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 249, 40],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 249, 40],"float32"), ) 	 50995200 	 33926 	 10.026666164398193 	 10.169268608093262 	 0.3020312786102295 	 0.30636000633239746 	 15.340984106063843 	 25.440662384033203 	 0.46211934089660645 	 0.3831815719604492 	 
2025-08-04 18:52:46.272083 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 40, 249],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 40, 249],"float32"), ) 	 50995200 	 33926 	 10.026648044586182 	 10.16903305053711 	 0.3020751476287842 	 0.3064098358154297 	 15.34139108657837 	 25.44088864326477 	 0.4621739387512207 	 0.3831956386566162 	 
2025-08-04 18:53:49.202717 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 44, 226],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 44, 226],"float32"), ) 	 50913280 	 33926 	 10.020217180252075 	 10.154756784439087 	 0.3018820285797119 	 0.3059220314025879 	 15.316265344619751 	 25.400221347808838 	 0.46146178245544434 	 0.3825969696044922 	 
2025-08-04 18:54:51.971292 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 48, 207],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 48, 207],"float32"), ) 	 50872320 	 33926 	 10.01179552078247 	 10.147668361663818 	 0.30161356925964355 	 0.3056979179382324 	 15.302502632141113 	 25.380770206451416 	 0.46097350120544434 	 0.382249116897583 	 
2025-08-04 18:55:54.698036 test begin: paddle.Tensor.square(Tensor([2, 25401601],"float32"), )
[Prof] paddle.Tensor.square 	 paddle.Tensor.square(Tensor([2, 25401601],"float32"), ) 	 50803202 	 33808 	 9.991297721862793 	 10.06288480758667 	 0.3020486831665039 	 0.3042325973510742 	 15.206359386444092 	 35.68555474281311 	 0.4596431255340576 	 0.2698965072631836 	 
2025-08-04 18:57:07.558070 test begin: paddle.Tensor.square(Tensor([396901, 128],"float32"), )
[Prof] paddle.Tensor.square 	 paddle.Tensor.square(Tensor([396901, 128],"float32"), ) 	 50803328 	 33808 	 9.99699091911316 	 10.062817811965942 	 0.3021383285522461 	 0.30419087409973145 	 15.208976030349731 	 35.685524225234985 	 0.4596409797668457 	 0.26984381675720215 	 
2025-08-04 18:58:20.297066 test begin: paddle.Tensor.square(Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.square 	 paddle.Tensor.square(Tensor([50803201],"float32"), ) 	 50803201 	 33808 	 9.991446495056152 	 10.062636613845825 	 0.30202794075012207 	 0.30419445037841797 	 15.206560134887695 	 35.68507742881775 	 0.45966076850891113 	 0.2698957920074463 	 
2025-08-04 18:59:32.955299 test begin: paddle.Tensor.square(Tensor([8, 6350401],"float32"), )
[Prof] paddle.Tensor.square 	 paddle.Tensor.square(Tensor([8, 6350401],"float32"), ) 	 50803208 	 33808 	 9.991570472717285 	 10.062856197357178 	 0.3021082878112793 	 0.3041412830352783 	 15.206453561782837 	 35.68560552597046 	 0.4597339630126953 	 0.2698404788970947 	 
2025-08-04 19:00:47.594553 test begin: paddle.Tensor.squeeze(Tensor([10, 2, 3840, 10240],"float32"), 0, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([10, 2, 3840, 10240],"float32"), 0, ) 	 786432000 	 2295230 	 10.334580659866333 	 17.8005690574646 	 0.00012993812561035156 	 0.00029587745666503906 	 99.02620267868042 	 114.21655821800232 	 0.0001442432403564453 	 0.0002338886260986328 	 
2025-08-04 19:05:14.309556 test begin: paddle.Tensor.squeeze(Tensor([10, 3, 1654, 10240],"float32"), 0, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([10, 3, 1654, 10240],"float32"), 0, ) 	 508108800 	 2295230 	 9.91435170173645 	 9.46611762046814 	 0.00010323524475097656 	 9.012222290039062e-05 	 113.52863931655884 	 115.2616696357727 	 0.00010824203491210938 	 0.00023245811462402344 	 
2025-08-04 19:09:40.033590 test begin: paddle.Tensor.squeeze(Tensor([10, 3, 3840, 10240],"float32"), 0, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([10, 3, 3840, 10240],"float32"), 0, ) 	 1179648000 	 2295230 	 10.131466627120972 	 9.554572582244873 	 0.0001468658447265625 	 0.00016188621520996094 	 98.30739784240723 	 114.71538162231445 	 0.0001068115234375 	 0.0002262592315673828 	 
2025-08-04 19:14:10.497906 test begin: paddle.Tensor.squeeze(Tensor([10, 3, 3840, 4411],"float32"), 0, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([10, 3, 3840, 4411],"float32"), 0, ) 	 508147200 	 2295230 	 20.476492881774902 	 17.701996088027954 	 0.00014019012451171875 	 0.00010180473327636719 	 114.46386671066284 	 114.0280077457428 	 0.0001308917999267578 	 0.00020051002502441406 	 
2025-08-04 19:18:54.340965 test begin: paddle.Tensor.squeeze(Tensor([160, 1, 125, 25500],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([160, 1, 125, 25500],"float32"), 1, ) 	 510000000 	 2295230 	 10.128350257873535 	 9.74231767654419 	 0.00012493133544921875 	 0.0002617835998535156 	 99.17314672470093 	 123.92865252494812 	 0.00011873245239257812 	 0.00022554397583007812 	 
2025-08-04 19:23:16.037548 test begin: paddle.Tensor.squeeze(Tensor([160, 1, 80, 39691],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([160, 1, 80, 39691],"float32"), 1, ) 	 508044800 	 2295230 	 10.102526903152466 	 9.767577886581421 	 9.226799011230469e-05 	 0.0002796649932861328 	 99.40166354179382 	 123.1557502746582 	 0.00010228157043457031 	 0.00022840499877929688 	 
2025-08-04 19:27:34.891932 test begin: paddle.Tensor.squeeze(Tensor([160, 2, 80, 25500],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([160, 2, 80, 25500],"float32"), 1, ) 	 652800000 	 2295230 	 20.365281581878662 	 9.571404933929443 	 0.0001418590545654297 	 9.441375732421875e-05 	 107.0333890914917 	 113.44166684150696 	 0.00012636184692382812 	 0.00022149085998535156 	 
2025-08-04 19:32:06.805105 test begin: paddle.Tensor.squeeze(Tensor([2000, 1, 127009, 2],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([2000, 1, 127009, 2],"float32"), 1, ) 	 508036000 	 2295230 	 10.09679388999939 	 18.114890813827515 	 7.319450378417969e-05 	 0.0003037452697753906 	 107.54389786720276 	 123.55215382575989 	 0.00011754035949707031 	 0.0002205371856689453 	 
2025-08-04 19:36:42.974907 test begin: paddle.Tensor.squeeze(Tensor([2000, 1, 37632, 7],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([2000, 1, 37632, 7],"float32"), 1, ) 	 526848000 	 2295230 	 20.892330169677734 	 17.664892435073853 	 0.0001468658447265625 	 0.0002913475036621094 	 104.89991545677185 	 123.78255796432495 	 0.000110626220703125 	 0.0002186298370361328 	 
2025-08-04 19:41:28.906147 test begin: paddle.Tensor.squeeze(Tensor([2000, 4, 37632, 2],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([2000, 4, 37632, 2],"float32"), 1, ) 	 602112000 	 2295230 	 10.016740560531616 	 9.552818536758423 	 4.649162292480469e-05 	 0.0002892017364501953 	 99.01007509231567 	 116.44412112236023 	 0.00012373924255371094 	 0.00023174285888671875 	 
2025-08-04 19:45:44.784780 test begin: paddle.Tensor.squeeze(Tensor([250, 1, 80, 25500],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([250, 1, 80, 25500],"float32"), 1, ) 	 510000000 	 2295230 	 10.133946895599365 	 9.874868154525757 	 0.0001010894775390625 	 9.202957153320312e-05 	 98.9968011379242 	 123.8295271396637 	 0.00010204315185546875 	 0.0002224445343017578 	 
2025-08-04 19:50:04.444933 test begin: paddle.Tensor.squeeze(Tensor([6760, 1, 37632, 2],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([6760, 1, 37632, 2],"float32"), 1, ) 	 508784640 	 2295230 	 10.19745922088623 	 9.797991275787354 	 6.818771362304688e-05 	 9.083747863769531e-05 	 99.72840452194214 	 124.54405045509338 	 0.00010848045349121094 	 0.00022649765014648438 	 
2025-08-04 19:54:27.080908 test begin: paddle.Tensor.std(Tensor([1024, 1024, 25],"float64"), )
W0804 19:54:27.564690 65898 dygraph_functions.cc:88394] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([1024, 1024, 25],"float64"), ) 	 26214400 	 9109 	 12.246450901031494 	 1.6823840141296387 	 4.482269287109375e-05 	 0.09435892105102539 	 13.943102359771729 	 7.216503143310547 	 0.19586730003356934 	 0.09027719497680664 	 
2025-08-04 19:55:03.261964 test begin: paddle.Tensor.std(Tensor([1024, 1024, 49],"float32"), )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([1024, 1024, 49],"float32"), ) 	 51380224 	 9109 	 10.058201313018799 	 1.5323121547698975 	 5.0067901611328125e-05 	 0.08596014976501465 	 12.332529783248901 	 7.139961242675781 	 0.1732480525970459 	 0.08933377265930176 	 
2025-08-04 19:55:35.190916 test begin: paddle.Tensor.std(Tensor([1024, 3101, 8],"float64"), )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([1024, 3101, 8],"float64"), ) 	 25403392 	 9109 	 11.938577651977539 	 1.6352615356445312 	 5.1021575927734375e-05 	 0.09171414375305176 	 13.511382341384888 	 7.00603461265564 	 0.18993759155273438 	 0.08758974075317383 	 
2025-08-04 19:56:09.861817 test begin: paddle.Tensor.std(Tensor([1024, 6202, 8],"float32"), )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([1024, 6202, 8],"float32"), ) 	 50806784 	 9109 	 10.01759386062622 	 1.5169446468353271 	 4.076957702636719e-05 	 0.08503031730651855 	 12.203507661819458 	 7.066110849380493 	 0.17142343521118164 	 0.08831930160522461 	 
2025-08-04 19:56:42.371431 test begin: paddle.Tensor.std(Tensor([1444, 35183],"float32"), axis=1, )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([1444, 35183],"float32"), axis=1, ) 	 50804252 	 9109 	 10.586549520492554 	 1.6055757999420166 	 4.5299530029296875e-05 	 0.18014955520629883 	 12.404828786849976 	 7.1231303215026855 	 0.17430377006530762 	 0.10005307197570801 	 
2025-08-04 19:57:15.008793 test begin: paddle.Tensor.std(Tensor([3101, 1024, 8],"float64"), )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([3101, 1024, 8],"float64"), ) 	 25403392 	 9109 	 11.932633876800537 	 1.636080265045166 	 3.5762786865234375e-05 	 0.09177589416503906 	 13.527355909347534 	 7.008831739425659 	 0.19006609916687012 	 0.08763289451599121 	 
2025-08-04 19:57:50.805279 test begin: paddle.Tensor.std(Tensor([49613, 1024],"float32"), axis=1, )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([49613, 1024],"float32"), axis=1, ) 	 50803712 	 9109 	 9.971388816833496 	 1.528364896774292 	 3.600120544433594e-05 	 0.1714630126953125 	 12.286156415939331 	 7.112896919250488 	 0.19690513610839844 	 0.09991025924682617 	 
2025-08-04 19:58:22.555347 test begin: paddle.Tensor.std(Tensor([6202, 1024, 8],"float32"), )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([6202, 1024, 8],"float32"), ) 	 50806784 	 9109 	 10.155000448226929 	 1.5164899826049805 	 5.4836273193359375e-05 	 0.0850832462310791 	 12.206601619720459 	 7.065846681594849 	 0.17149138450622559 	 0.08840417861938477 	 
2025-08-04 19:58:54.349359 test begin: paddle.Tensor.subtract(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.subtract 	 paddle.Tensor.subtract(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 22223 	 9.996616125106812 	 9.92136263847351 	 0.45978355407714844 	 0.45626187324523926 	 10.549338817596436 	 6.614760637283325 	 0.4851834774017334 	 0.3041806221008301 	 
2025-08-04 19:59:34.014919 test begin: paddle.Tensor.sum(Tensor([106496, 478],"float32"), axis=-1, )
[Prof] paddle.Tensor.sum 	 paddle.Tensor.sum(Tensor([106496, 478],"float32"), axis=-1, ) 	 50905088 	 68402 	 10.259205341339111 	 10.611071825027466 	 0.1532917022705078 	 0.15862131118774414 	 9.453168392181396 	 3.6085588932037354 	 0.14121651649475098 	 7.414817810058594e-05 	 
2025-08-04 20:00:08.804015 test begin: paddle.Tensor.sum(Tensor([108544, 469],"float32"), axis=-1, )
[Prof] paddle.Tensor.sum 	 paddle.Tensor.sum(Tensor([108544, 469],"float32"), axis=-1, ) 	 50907136 	 68402 	 10.272066593170166 	 10.605220317840576 	 0.15358614921569824 	 0.15849947929382324 	 9.434319734573364 	 3.8728792667388916 	 0.14087533950805664 	 0.00020837783813476562 	 
2025-08-04 20:00:43.843705 test begin: paddle.Tensor.sum(Tensor([111616, 456],"float32"), axis=-1, )
[Prof] paddle.Tensor.sum 	 paddle.Tensor.sum(Tensor([111616, 456],"float32"), axis=-1, ) 	 50896896 	 68402 	 10.367961168289185 	 10.449662923812866 	 0.15540742874145508 	 0.1561448574066162 	 9.458399057388306 	 3.611351490020752 	 0.14131760597229004 	 7.724761962890625e-05 	 
2025-08-04 20:01:18.587435 test begin: paddle.Tensor.sum(Tensor([14176, 3584],"float32"), axis=-1, )
[Prof] paddle.Tensor.sum 	 paddle.Tensor.sum(Tensor([14176, 3584],"float32"), axis=-1, ) 	 50806784 	 68402 	 10.002159833908081 	 10.694255590438843 	 0.1494426727294922 	 0.1598055362701416 	 9.374866724014282 	 3.6599419116973877 	 0.14043760299682617 	 0.00020074844360351562 	 
2025-08-04 20:01:53.180805 test begin: paddle.Tensor.take_along_axis(Tensor([128, 1000],"float32"), indices=Tensor([128, 396901],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([128, 1000],"float32"), indices=Tensor([128, 396901],"int32"), axis=-1, ) 	 50931328 	 33018 	 25.612677335739136 	 15.600010395050049 	 0.2642953395843506 	 0.48293471336364746 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 20:06:28.340136 test begin: paddle.Tensor.take_along_axis(Tensor([128, 396901],"float32"), indices=Tensor([128, 1],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([128, 396901],"float32"), indices=Tensor([128, 1],"int32"), axis=-1, ) 	 50803456 	 33018 	 10.002357006072998 	 0.5728378295898438 	 0.10330557823181152 	 7.915496826171875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 20:06:54.804939 test begin: paddle.Tensor.take_along_axis(Tensor([128, 396901],"float32"), indices=Tensor([128, 396901],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([128, 396901],"float32"), indices=Tensor([128, 396901],"int32"), axis=-1, ) 	 101606656 	 33018 	 46.33601140975952 	 24.35510563850403 	 0.47827720642089844 	 0.7536985874176025 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 20:09:03.146217 test begin: paddle.Tensor.take_along_axis(Tensor([50804, 1000],"float32"), indices=Tensor([50804, 1],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([50804, 1000],"float32"), indices=Tensor([50804, 1],"int32"), axis=-1, ) 	 50854804 	 33018 	 10.176584243774414 	 0.8583889007568359 	 0.1050865650177002 	 0.000209808349609375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 20:09:32.959764 test begin: paddle.Tensor.take_along_axis(Tensor([80, 1000],"float32"), indices=Tensor([80, 635041],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([80, 1000],"float32"), indices=Tensor([80, 635041],"int32"), axis=-1, ) 	 50883280 	 33018 	 25.202176809310913 	 15.535688877105713 	 0.2599928379058838 	 0.48088932037353516 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 20:14:18.279088 test begin: paddle.Tensor.take_along_axis(Tensor([80, 635041],"float32"), indices=Tensor([80, 1],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([80, 635041],"float32"), indices=Tensor([80, 1],"int32"), axis=-1, ) 	 50803360 	 33018 	 9.99729299545288 	 0.5687522888183594 	 0.10323953628540039 	 6.890296936035156e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 20:14:44.838975 test begin: paddle.Tensor.take_along_axis(Tensor([80, 635041],"float32"), indices=Tensor([80, 635041],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([80, 635041],"float32"), indices=Tensor([80, 635041],"int32"), axis=-1, ) 	 101606560 	 33018 	 46.40255093574524 	 24.60844588279724 	 0.4789159297943115 	 0.7617504596710205 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-04 20:16:53.949135 test begin: paddle.Tensor.tanh(Tensor([1, 16934401, 3],"float32"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([1, 16934401, 3],"float32"), ) 	 50803203 	 33854 	 9.997720003128052 	 10.086078405380249 	 0.30182385444641113 	 0.3044769763946533 	 15.219177007675171 	 15.114723205566406 	 0.4594755172729492 	 0.45633673667907715 	 
2025-08-04 20:17:49.682409 test begin: paddle.Tensor.tanh(Tensor([1, 2, 12700801],"float64"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([1, 2, 12700801],"float64"), ) 	 25401602 	 33854 	 10.132947444915771 	 10.165966749191284 	 0.30591392517089844 	 0.3068218231201172 	 15.176904439926147 	 15.044447183609009 	 0.4581470489501953 	 0.4541792869567871 	 
2025-08-04 20:18:43.385659 test begin: paddle.Tensor.tanh(Tensor([1, 2, 25401601],"float32"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([1, 2, 25401601],"float32"), ) 	 50803202 	 33854 	 9.997522830963135 	 10.091192960739136 	 0.30180907249450684 	 0.3044736385345459 	 15.220233678817749 	 15.115113258361816 	 0.45941758155822754 	 0.4563446044921875 	 
2025-08-04 20:19:39.554645 test begin: paddle.Tensor.tanh(Tensor([1, 8467201, 3],"float64"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([1, 8467201, 3],"float64"), ) 	 25401603 	 33854 	 10.132623434066772 	 10.615268230438232 	 0.3058903217315674 	 0.3068087100982666 	 15.177302837371826 	 15.046141624450684 	 0.4580574035644531 	 0.45421361923217773 	 
2025-08-04 20:20:33.100137 test begin: paddle.Tensor.tanh(Tensor([2, 12700801],"float64"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([2, 12700801],"float64"), ) 	 25401602 	 33854 	 10.132671117782593 	 10.163581371307373 	 0.305908203125 	 0.3068692684173584 	 15.177685976028442 	 15.045411348342896 	 0.4582223892211914 	 0.45418453216552734 	 
2025-08-04 20:21:24.708681 test begin: paddle.Tensor.tanh(Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([4233601, 2, 3],"float64"), ) 	 25401606 	 33854 	 10.132811784744263 	 10.163555145263672 	 0.30589938163757324 	 0.3067924976348877 	 15.177881479263306 	 15.045134544372559 	 0.4580192565917969 	 0.45417237281799316 	 
2025-08-04 20:22:16.305876 test begin: paddle.Tensor.tanh(Tensor([6350401, 4],"float64"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([6350401, 4],"float64"), ) 	 25401604 	 33854 	 10.132776021957397 	 10.173513174057007 	 0.3058938980102539 	 0.3068525791168213 	 15.177493333816528 	 15.045408487319946 	 0.4580967426300049 	 0.45421934127807617 	 
2025-08-04 20:23:07.990257 test begin: paddle.Tensor.tanh(Tensor([8467201, 2, 3],"float32"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([8467201, 2, 3],"float32"), ) 	 50803206 	 33854 	 9.997473239898682 	 10.10227632522583 	 0.30185413360595703 	 0.30449700355529785 	 15.220722436904907 	 15.115121364593506 	 0.4594848155975342 	 0.45629358291625977 	 
2025-08-04 20:24:00.882450 test begin: paddle.Tensor.tile(Tensor([198451, 1, 256],"float32"), tuple(1,1,1,), )
[Prof] paddle.Tensor.tile 	 paddle.Tensor.tile(Tensor([198451, 1, 256],"float32"), tuple(1,1,1,), ) 	 50803456 	 33795 	 10.00715184211731 	 10.577438831329346 	 0.30263376235961914 	 0.15994811058044434 	 10.636178731918335 	 1.667809247970581 	 0.16094422340393066 	 7.987022399902344e-05 	 
2025-08-04 20:24:39.214186 test begin: paddle.Tensor.tile(Tensor([36858, 1, 1379],"float32"), tuple(1,1,1,), )
[Prof] paddle.Tensor.tile 	 paddle.Tensor.tile(Tensor([36858, 1, 1379],"float32"), tuple(1,1,1,), ) 	 50827182 	 33795 	 10.012815713882446 	 10.58292841911316 	 0.30283236503601074 	 0.16001176834106445 	 10.703575372695923 	 1.7925169467926025 	 0.1618485450744629 	 0.0001819133758544922 	 
2025-08-04 20:25:14.013401 test begin: paddle.Tensor.tile(Tensor([36858, 6, 256],"float32"), tuple(1,1,1,), )
[Prof] paddle.Tensor.tile 	 paddle.Tensor.tile(Tensor([36858, 6, 256],"float32"), tuple(1,1,1,), ) 	 56613888 	 33795 	 11.130305051803589 	 11.682393550872803 	 0.33658695220947266 	 0.352466344833374 	 11.673936128616333 	 1.7110991477966309 	 0.35300254821777344 	 7.581710815429688e-05 	 
2025-08-04 20:25:53.214663 test begin: paddle.Tensor.tile(Tensor([38402, 1, 1323],"float32"), tuple(1,1,1,), )
[Prof] paddle.Tensor.tile 	 paddle.Tensor.tile(Tensor([38402, 1, 1323],"float32"), tuple(1,1,1,), ) 	 50805846 	 33795 	 9.998111963272095 	 10.572827577590942 	 0.3023192882537842 	 0.15982460975646973 	 10.397262811660767 	 1.6968553066253662 	 0.15720200538635254 	 0.0001068115234375 	 
2025-08-04 20:26:27.566705 test begin: paddle.Tensor.tile(Tensor([38402, 6, 256],"float32"), tuple(1,1,1,), )
[Prof] paddle.Tensor.tile 	 paddle.Tensor.tile(Tensor([38402, 6, 256],"float32"), tuple(1,1,1,), ) 	 58985472 	 33795 	 11.594756841659546 	 12.137266635894775 	 0.3506779670715332 	 0.36704516410827637 	 12.147343158721924 	 1.6337950229644775 	 0.3673534393310547 	 7.62939453125e-05 	 
2025-08-04 20:27:09.150276 test begin: paddle.Tensor.tolist(Tensor([11, 16, 32, 43],"int64"), )
[Prof] paddle.Tensor.tolist 	 paddle.Tensor.tolist(Tensor([11, 16, 32, 43],"int64"), ) 	 242176 	 1000 	 13.234552145004272 	 16.89897584915161 	 8.106231689453125e-05 	 0.00014448165893554688 	 None 	 None 	 None 	 None 	 
2025-08-04 20:27:39.633081 test begin: paddle.Tensor.tolist(Tensor([11, 25, 21, 43],"int64"), )
[Prof] paddle.Tensor.tolist 	 paddle.Tensor.tolist(Tensor([11, 25, 21, 43],"int64"), ) 	 248325 	 1000 	 13.746586322784424 	 17.230557918548584 	 8.845329284667969e-05 	 0.0001392364501953125 	 None 	 None 	 None 	 None 	 
2025-08-04 20:28:10.655480 test begin: paddle.Tensor.tolist(Tensor([11, 25, 32, 28],"int64"), )
[Prof] paddle.Tensor.tolist 	 paddle.Tensor.tolist(Tensor([11, 25, 32, 28],"int64"), ) 	 246400 	 1000 	 13.66402554512024 	 20.956663131713867 	 5.888938903808594e-05 	 0.00011587142944335938 	 None 	 None 	 None 	 None 	 
2025-08-04 20:28:45.546327 test begin: paddle.Tensor.tolist(Tensor([7, 25, 32, 43],"int64"), )
[Prof] paddle.Tensor.tolist 	 paddle.Tensor.tolist(Tensor([7, 25, 32, 43],"int64"), ) 	 240800 	 1000 	 12.485780715942383 	 16.909366846084595 	 7.271766662597656e-05 	 0.0001506805419921875 	 None 	 None 	 None 	 None 	 
2025-08-04 20:29:14.980818 test begin: paddle.Tensor.topk(Tensor([1, 50803201],"float32"), 5, 1, True, True, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc7b0621f60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 20:42:21.689323 test begin: paddle.Tensor.topk(Tensor([1024, 1034, 48],"float32"), 2, axis=-1, )
W0804 20:42:31.335916 67252 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.topk 	 paddle.Tensor.topk(Tensor([1024, 1034, 48],"float32"), 2, axis=-1, ) 	 50823168 	 26178 	 69.88303828239441 	 291.9702124595642 	 2.7282280921936035 	 5.697922706604004 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 20:48:47.434812 test begin: paddle.Tensor.topk(Tensor([1024, 8, 6202],"float32"), 2, axis=-1, )
[Prof] paddle.Tensor.topk 	 paddle.Tensor.topk(Tensor([1024, 8, 6202],"float32"), 2, axis=-1, ) 	 50806784 	 26178 	 9.98810076713562 	 40.274996757507324 	 0.38996171951293945 	 0.08746671676635742 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 20:49:48.566052 test begin: paddle.Tensor.topk(Tensor([128, 396901],"float32"), 5, 1, True, True, )
[Prof] paddle.Tensor.topk 	 paddle.Tensor.topk(Tensor([128, 396901],"float32"), 5, 1, True, True, ) 	 50803328 	 26178 	 38.76702117919922 	 38.817094564437866 	 1.5142056941986084 	 0.08430194854736328 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 20:51:18.273170 test begin: paddle.Tensor.topk(Tensor([132301, 8, 48],"float32"), 2, axis=-1, )
[Prof] paddle.Tensor.topk 	 paddle.Tensor.topk(Tensor([132301, 8, 48],"float32"), 2, axis=-1, ) 	 50803584 	 26178 	 69.64691686630249 	 291.81610918045044 	 2.719143867492676 	 5.698239326477051 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 20:57:39.545250 test begin: paddle.Tensor.topk(Tensor([50804, 1000],"float32"), 5, 1, True, True, )
[Prof] paddle.Tensor.topk 	 paddle.Tensor.topk(Tensor([50804, 1000],"float32"), 5, 1, True, True, ) 	 50804000 	 26178 	 17.045300245285034 	 63.17754530906677 	 0.6652994155883789 	 0.13728976249694824 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-04 20:59:09.589422 test begin: paddle.Tensor.transpose(Tensor([1064960, 955],"bfloat16"), list[1,0,], )
[Prof] paddle.Tensor.transpose 	 paddle.Tensor.transpose(Tensor([1064960, 955],"bfloat16"), list[1,0,], ) 	 1017036800 	 66690 	 0.20823359489440918 	 0.2999708652496338 	 1.4781951904296875e-05 	 5.5789947509765625e-05 	 2.796034336090088 	 300.10827374458313 	 5.269050598144531e-05 	 2.299586296081543 	 
2025-08-04 21:04:46.103165 test begin: paddle.Tensor.transpose(Tensor([1085440, 937],"bfloat16"), list[1,0,], )
[Prof] paddle.Tensor.transpose 	 paddle.Tensor.transpose(Tensor([1085440, 937],"bfloat16"), list[1,0,], ) 	 1017057280 	 66690 	 0.21163535118103027 	 0.3074803352355957 	 3.4332275390625e-05 	 9.298324584960938e-05 	 2.8315200805664062 	 300.1067054271698 	 6.151199340820312e-05 	 2.2997705936431885 	 
2025-08-04 21:10:22.394906 test begin: paddle.Tensor.transpose(Tensor([1116160, 911],"bfloat16"), list[1,0,], )
[Prof] paddle.Tensor.transpose 	 paddle.Tensor.transpose(Tensor([1116160, 911],"bfloat16"), list[1,0,], ) 	 1016821760 	 66690 	 0.21755623817443848 	 0.31130385398864746 	 3.790855407714844e-05 	 9.965896606445312e-05 	 2.865605592727661 	 300.03087544441223 	 4.982948303222656e-05 	 2.299182891845703 	 
2025-08-04 21:16:04.339904 test begin: paddle.Tensor.transpose(Tensor([141760, 7168],"bfloat16"), list[1,0,], )
[Prof] paddle.Tensor.transpose 	 paddle.Tensor.transpose(Tensor([141760, 7168],"bfloat16"), list[1,0,], ) 	 1016135680 	 66690 	 0.21347332000732422 	 0.30519604682922363 	 4.9591064453125e-05 	 6.461143493652344e-05 	 2.867187738418579 	 299.8563916683197 	 0.00013184547424316406 	 2.297537088394165 	 
2025-08-04 21:21:51.016226 test begin: paddle.Tensor.tril(Tensor([1, 2, 25401601],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([1, 2, 25401601],"float32"), -1, ) 	 50803202 	 33390 	 10.00014615058899 	 8.891730070114136 	 0.3061366081237793 	 0.2721381187438965 	 9.984000205993652 	 8.890763759613037 	 0.3056044578552246 	 0.2721097469329834 	 
2025-08-04 21:22:30.481120 test begin: paddle.Tensor.tril(Tensor([1, 25401601, 2],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([1, 25401601, 2],"float32"), -1, ) 	 50803202 	 33390 	 14.028531312942505 	 10.741570472717285 	 0.42935943603515625 	 0.32878541946411133 	 13.984984636306763 	 10.76950979232788 	 0.42790699005126953 	 0.32944560050964355 	 
2025-08-04 21:23:21.738672 test begin: paddle.Tensor.tril(Tensor([12700801, 2, 2],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([12700801, 2, 2],"float32"), -1, ) 	 50803204 	 33390 	 14.053227186203003 	 11.05260157585144 	 0.4301586151123047 	 0.3311913013458252 	 14.006288528442383 	 10.833163738250732 	 0.42870640754699707 	 0.33168840408325195 	 
2025-08-04 21:24:15.181072 test begin: paddle.Tensor.tril(Tensor([2, 12700801, 2],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([2, 12700801, 2],"float32"), -1, ) 	 50803204 	 33390 	 14.05426549911499 	 10.796036005020142 	 0.4301464557647705 	 0.330275297164917 	 14.005855083465576 	 10.792654037475586 	 0.42868781089782715 	 0.3298959732055664 	 
2025-08-04 21:25:06.521911 test begin: paddle.Tensor.tril(Tensor([2, 2, 12700801],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([2, 2, 12700801],"float32"), -1, ) 	 50803204 	 33390 	 10.010974407196045 	 8.89281964302063 	 0.306410551071167 	 0.27218103408813477 	 9.982932090759277 	 8.892435073852539 	 0.30553102493286133 	 0.27216577529907227 	 
2025-08-04 21:25:46.179794 test begin: paddle.Tensor.tril(Tensor([2, 25401601],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([2, 25401601],"float32"), -1, ) 	 50803202 	 33390 	 9.999207019805908 	 6.363438606262207 	 0.30608534812927246 	 0.18641901016235352 	 9.98361849784851 	 6.088666200637817 	 0.3055531978607178 	 0.18635320663452148 	 
2025-08-04 21:26:21.674121 test begin: paddle.Tensor.tril(Tensor([25401601, 2],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([25401601, 2],"float32"), -1, ) 	 50803202 	 33390 	 14.04087209701538 	 10.232653856277466 	 0.4301760196685791 	 0.3131287097930908 	 13.991699695587158 	 10.223039388656616 	 0.4287281036376953 	 0.31286096572875977 	 
2025-08-04 21:27:11.912456 test begin: paddle.Tensor.trunc(Tensor([18144010, 28],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([18144010, 28],"float32"), ) 	 508032280 	 34186 	 117.85857510566711 	 99.98265671730042 	 0.00011873245239257812 	 2.9889326095581055 	 84.70731163024902 	 44.85078477859497 	 8.273124694824219e-05 	 1.33976411819458 	 
2025-08-04 21:33:19.935955 test begin: paddle.Tensor.trunc(Tensor([20, 3175201, 8],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([20, 3175201, 8],"float32"), ) 	 508032160 	 34186 	 117.2365870475769 	 99.98634171485901 	 0.0001201629638671875 	 2.989163875579834 	 84.70525884628296 	 44.85149312019348 	 7.867813110351562e-05 	 1.339604377746582 	 
2025-08-04 21:39:26.038771 test begin: paddle.Tensor.trunc(Tensor([20, 8, 3175201],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([20, 8, 3175201],"float32"), ) 	 508032160 	 34186 	 117.2170786857605 	 99.982342004776 	 0.00013589859008789062 	 2.9890546798706055 	 84.64441299438477 	 44.83798003196716 	 8.7738037109375e-05 	 1.3396801948547363 	 
2025-08-04 21:45:35.217067 test begin: paddle.Tensor.trunc(Tensor([280, 1814401],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([280, 1814401],"float32"), ) 	 508032280 	 34186 	 118.61767220497131 	 99.98212337493896 	 0.00012540817260742188 	 2.9890971183776855 	 84.66859149932861 	 44.855085611343384 	 9.34600830078125e-05 	 1.3398022651672363 	 
2025-08-04 21:51:45.617634 test begin: paddle.Tensor.trunc(Tensor([63504010, 8],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([63504010, 8],"float32"), ) 	 508032080 	 34186 	 117.16059517860413 	 100.11767339706421 	 0.00023412704467773438 	 3.1221015453338623 	 84.61118459701538 	 44.846800565719604 	 0.00010180473327636719 	 1.3396868705749512 	 
2025-08-04 21:57:52.022897 test begin: paddle.Tensor.trunc(Tensor([7938010, 8, 8],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([7938010, 8, 8],"float32"), ) 	 508032640 	 34186 	 117.14172291755676 	 101.56736779212952 	 0.00015401840209960938 	 2.9888739585876465 	 84.65775942802429 	 44.84737586975098 	 8.20159912109375e-05 	 1.3395044803619385 	 
2025-08-04 22:04:01.546225 test begin: paddle.Tensor.trunc(Tensor([80, 6350401],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([80, 6350401],"float32"), ) 	 508032080 	 34186 	 117.1369903087616 	 99.9801893234253 	 0.00011873245239257812 	 2.9888410568237305 	 84.62842464447021 	 44.85167360305786 	 8.153915405273438e-05 	 1.33953857421875 	 
2025-08-04 22:10:07.906688 test begin: paddle.Tensor.unbind(Tensor([30, 115, 2304, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 115, 2304, 64],"float32"), 0, ) 	 508723200 	 85224 	 3.809964179992676 	 2.7353570461273193 	 0.00011563301086425781 	 0.00020956993103027344 	 300.1052987575531 	 253.19012570381165 	 3.598602056503296 	 3.0362253189086914 	 
2025-08-04 22:19:45.821229 test begin: paddle.Tensor.unbind(Tensor([30, 1351, 196, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 1351, 196, 64],"float32"), 0, ) 	 508408320 	 85224 	 3.4120028018951416 	 3.522919178009033 	 0.00011205673217773438 	 0.00022411346435546875 	 300.28283643722534 	 252.955637216568 	 3.600823402404785 	 3.033336877822876 	 
2025-08-04 22:29:23.332264 test begin: paddle.Tensor.unbind(Tensor([30, 60, 2304, 123],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 60, 2304, 123],"float32"), 0, ) 	 510105600 	 85224 	 3.8413662910461426 	 2.8136019706726074 	 0.00013589859008789062 	 9.942054748535156e-05 	 301.50261664390564 	 253.84258675575256 	 4.0743567943573 	 3.04410982131958 	 
2025-08-04 22:39:05.250086 test begin: paddle.Tensor.unbind(Tensor([30, 60, 4411, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 60, 4411, 64],"float32"), 0, ) 	 508147200 	 85224 	 3.3993186950683594 	 2.819882392883301 	 5.435943603515625e-05 	 0.00020384788513183594 	 299.86935472488403 	 252.88832068443298 	 3.5961384773254395 	 3.0329809188842773 	 
2025-08-04 22:48:44.410202 test begin: paddle.Tensor.unbind(Tensor([30, 864, 196, 101],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 864, 196, 101],"float32"), 0, ) 	 513112320 	 85224 	 3.3160457611083984 	 2.8220295906066895 	 0.000110626220703125 	 8.749961853027344e-05 	 302.5953414440155 	 255.38625073432922 	 3.6286721229553223 	 3.0627009868621826 	 
2025-08-04 22:58:25.975669 test begin: paddle.Tensor.unbind(Tensor([30, 864, 307, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 864, 307, 64],"float32"), 0, ) 	 509276160 	 85224 	 3.831207275390625 	 2.8393800258636475 	 0.00011563301086425781 	 0.00027871131896972656 	 300.87203216552734 	 253.47227573394775 	 3.608172655105591 	 3.03963565826416 	 
2025-08-04 23:08:06.723887 test begin: paddle.Tensor.unbind(Tensor([30, 960, 196, 91],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 960, 196, 91],"float32"), 0, ) 	 513676800 	 85224 	 3.4275431632995605 	 3.545011043548584 	 5.125999450683594e-05 	 0.0007996559143066406 	 303.50674843788147 	 255.65271162986755 	 3.639712333679199 	 3.0659615993499756 	 
2025-08-04 23:17:57.367559 test begin: paddle.Tensor.unbind(Tensor([30, 960, 276, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 960, 276, 64],"float32"), 0, ) 	 508723200 	 85224 	 3.3524720668792725 	 2.777085065841675 	 0.00018310546875 	 8.296966552734375e-05 	 299.87021684646606 	 253.16833806037903 	 3.597862958908081 	 3.0361156463623047 	 
2025-08-04 23:27:33.661260 test begin: paddle.Tensor.unbind(Tensor([50, 864, 196, 64],"float32"), 0, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fae7a84b820>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:37:45.916190 test begin: paddle.Tensor.unbind(Tensor([50, 960, 196, 64],"float32"), 0, )
W0804 23:37:56.001608 74163 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4e017e6e00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:47:53.759960 test begin: paddle.Tensor.unbind(Tensor([60, 60, 2304, 64],"float32"), 0, )
W0804 23:48:06.214933 74467 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fac86bb6e00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-04 23:58:00.896160 test begin: paddle.Tensor.unique(Tensor([25401601],"int64"), )
W0804 23:58:01.445814 74932 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.unique 	 paddle.Tensor.unique(Tensor([25401601],"int64"), ) 	 25401601 	 1487 	 10.004390478134155 	 4.863702058792114 	 0.00010991096496582031 	 0.00021886825561523438 	 None 	 None 	 None 	 None 	 
2025-08-04 23:58:16.567143 test begin: paddle.Tensor.unsqueeze(Tensor([1720, 544, 544],"float32"), 0, )
Warning: The core code of paddle.Tensor.unsqueeze is too complex.
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([1720, 544, 544],"float32"), 0, ) 	 509009920 	 2409550 	 9.797616958618164 	 17.39033818244934 	 0.00010657310485839844 	 0.0002872943878173828 	 107.20266079902649 	 130.4721302986145 	 0.00011157989501953125 	 0.000213623046875 	 
2025-08-05 00:02:59.321721 test begin: paddle.Tensor.unsqueeze(Tensor([1720, 544, 544],"float32"), 1, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([1720, 544, 544],"float32"), 1, ) 	 509009920 	 2409550 	 9.89576768875122 	 8.854390621185303 	 8.440017700195312e-05 	 0.00026988983154296875 	 106.84897375106812 	 131.05935335159302 	 0.00010633468627929688 	 0.00021529197692871094 	 
2025-08-05 00:07:33.720566 test begin: paddle.Tensor.unsqueeze(Tensor([20, 3840, 10240],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([20, 3840, 10240],"float32"), 0, ) 	 786432000 	 2409550 	 9.76613974571228 	 8.97342038154602 	 0.0001323223114013672 	 0.00014162063598632812 	 107.33355045318604 	 131.91366481781006 	 8.58306884765625e-05 	 0.00022482872009277344 	 
2025-08-05 00:12:17.927254 test begin: paddle.Tensor.unsqueeze(Tensor([2000, 467, 544],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([2000, 467, 544],"float32"), 0, ) 	 508096000 	 2409550 	 9.880980968475342 	 8.959304094314575 	 0.00013518333435058594 	 0.0002689361572265625 	 104.86025047302246 	 130.42169094085693 	 0.00011515617370605469 	 0.0002048015594482422 	 
2025-08-05 00:16:49.509177 test begin: paddle.Tensor.unsqueeze(Tensor([2000, 467, 544],"float32"), 1, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([2000, 467, 544],"float32"), 1, ) 	 508096000 	 2409550 	 9.906872272491455 	 8.948126316070557 	 0.00012350082397460938 	 0.00026345252990722656 	 106.0671808719635 	 131.09410643577576 	 9.489059448242188e-05 	 0.00021028518676757812 	 
2025-08-05 00:21:22.442404 test begin: paddle.Tensor.unsqueeze(Tensor([2000, 544, 467],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([2000, 544, 467],"float32"), 0, ) 	 508096000 	 2409550 	 9.672203063964844 	 9.025082111358643 	 0.00011420249938964844 	 9.72747802734375e-05 	 106.07727408409119 	 131.3770010471344 	 9.369850158691406e-05 	 0.0002117156982421875 	 
2025-08-05 00:25:55.478547 test begin: paddle.Tensor.unsqueeze(Tensor([2000, 544, 467],"float32"), 1, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([2000, 544, 467],"float32"), 1, ) 	 508096000 	 2409550 	 9.970846176147461 	 8.932202816009521 	 0.00011467933654785156 	 0.00012373924255371094 	 105.81668376922607 	 131.3006067276001 	 0.00010991096496582031 	 0.0002224445343017578 	 
2025-08-05 00:30:28.551414 test begin: paddle.Tensor.unsqueeze(Tensor([30, 1654, 10240],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([30, 1654, 10240],"float32"), 0, ) 	 508108800 	 2409550 	 9.88857913017273 	 8.880200624465942 	 0.00013899803161621094 	 9.441375732421875e-05 	 106.55305671691895 	 132.07944107055664 	 0.000118255615234375 	 0.00021314620971679688 	 
2025-08-05 00:35:03.781900 test begin: paddle.Tensor.unsqueeze(Tensor([30, 3840, 4411],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([30, 3840, 4411],"float32"), 0, ) 	 508147200 	 2409550 	 9.782223224639893 	 8.86671781539917 	 4.76837158203125e-05 	 8.0108642578125e-05 	 105.63705062866211 	 130.17931938171387 	 9.1552734375e-05 	 0.00021338462829589844 	 
2025-08-05 00:39:34.914001 test begin: paddle.Tensor.var(Tensor([1000, 50804],"float32"), axis=0, )
W0805 00:39:37.739974 77338 dygraph_functions.cc:88394] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.var 	 paddle.Tensor.var(Tensor([1000, 50804],"float32"), axis=0, ) 	 50804000 	 7832 	 9.957557201385498 	 1.3597276210784912 	 4.3392181396484375e-05 	 0.17740368843078613 	 11.220982551574707 	 6.0067338943481445 	 0.20914697647094727 	 0.19579339027404785 	 
2025-08-05 00:40:08.950779 test begin: paddle.Tensor.var(Tensor([100000, 255],"float64"), axis=0, )
[Prof] paddle.Tensor.var 	 paddle.Tensor.var(Tensor([100000, 255],"float64"), axis=0, ) 	 25500000 	 7832 	 13.648101568222046 	 1.52829909324646 	 7.081031799316406e-05 	 0.09975051879882812 	 13.380245685577393 	 5.97491717338562 	 0.24941635131835938 	 0.1559460163116455 	 
2025-08-05 00:40:47.628642 test begin: paddle.Tensor.var(Tensor([1000000, 26],"float64"), axis=0, )
[Prof] paddle.Tensor.var 	 paddle.Tensor.var(Tensor([1000000, 26],"float64"), axis=0, ) 	 26000000 	 7832 	 48.276957750320435 	 1.4830424785614014 	 4.100799560546875e-05 	 0.09675288200378418 	 30.781399726867676 	 6.1459033489227295 	 0.5738370418548584 	 0.16040611267089844 	 
2025-08-05 00:42:17.291861 test begin: paddle.Tensor.var(Tensor([6350401, 4],"float64"), axis=0, )
[Prof] paddle.Tensor.var 	 paddle.Tensor.var(Tensor([6350401, 4],"float64"), axis=0, ) 	 25401604 	 7832 	 90.45895051956177 	 1.9566333293914795 	 4.220008850097656e-05 	 0.12765860557556152 	 51.759002923965454 	 5.988892316818237 	 0.9648637771606445 	 0.15627050399780273 	 
2025-08-05 00:44:49.104605 test begin: paddle.Tensor.var(Tensor([64801, 784],"float32"), axis=0, )
[Prof] paddle.Tensor.var 	 paddle.Tensor.var(Tensor([64801, 784],"float32"), axis=0, ) 	 50803984 	 7832 	 11.235910415649414 	 2.0328431129455566 	 4.0531158447265625e-05 	 0.13248229026794434 	 11.731817722320557 	 6.274453163146973 	 0.21866893768310547 	 0.1637253761291504 	 
2025-08-05 00:45:21.339985 test begin: paddle.Tensor.zero_(Tensor([100352, 507],"float32"), )
[Prof] paddle.Tensor.zero_ 	 paddle.Tensor.zero_(Tensor([100352, 507],"float32"), ) 	 50878464 	 68861 	 9.94279432296753 	 9.25586748123169 	 0.1476595401763916 	 0.13717007637023926 	 None 	 None 	 None 	 None 	 
2025-08-05 00:45:47.507670 test begin: paddle.Tensor.zero_(Tensor([507, 100352],"float32"), )
[Prof] paddle.Tensor.zero_ 	 paddle.Tensor.zero_(Tensor([507, 100352],"float32"), ) 	 50878464 	 68861 	 9.948755264282227 	 9.249942779541016 	 0.14769220352172852 	 0.13718962669372559 	 None 	 None 	 None 	 None 	 
2025-08-05 00:46:10.737532 test begin: paddle.Tensor.zero_(Tensor([6202, 8192],"float32"), )
[Prof] paddle.Tensor.zero_ 	 paddle.Tensor.zero_(Tensor([6202, 8192],"float32"), ) 	 50806784 	 68861 	 10.0661039352417 	 9.234386205673218 	 0.14947128295898438 	 0.13698148727416992 	 None 	 None 	 None 	 None 	 
2025-08-05 00:46:34.036331 test begin: paddle.Tensor.zero_(Tensor([8192, 6202],"float32"), )
[Prof] paddle.Tensor.zero_ 	 paddle.Tensor.zero_(Tensor([8192, 6202],"float32"), ) 	 50806784 	 68861 	 10.067981243133545 	 9.239434242248535 	 0.14939236640930176 	 0.1370401382446289 	 None 	 None 	 None 	 None 	 
2025-08-05 00:46:57.341831 test begin: paddle.Tensor.zero_(Tensor([886, 57344],"float32"), )
[Prof] paddle.Tensor.zero_ 	 paddle.Tensor.zero_(Tensor([886, 57344],"float32"), ) 	 50806784 	 68861 	 10.066073656082153 	 9.235246419906616 	 0.1493537425994873 	 0.13700628280639648 	 None 	 None 	 None 	 None 	 
2025-08-05 00:47:20.611778 test begin: paddle.tensordot(Tensor([406426, 5, 5, 5],"float32"), Tensor([406426, 5, 5, 5],"float32"), list[0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([406426, 5, 5, 5],"float32"), Tensor([406426, 5, 5, 5],"float32"), list[0,], ) 	 101606500 	 20388 	 22.066086053848267 	 17.46617078781128 	 0.36881113052368164 	 0.4362192153930664 	 32.63446497917175 	 32.90986657142639 	 0.8224484920501709 	 0.8238623142242432 	 
2025-08-05 00:49:07.590522 test begin: paddle.tensordot(Tensor([406426, 5, 5, 5],"float32"), Tensor([406426, 5, 5, 5],"float32"), list[3,0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([406426, 5, 5, 5],"float32"), Tensor([406426, 5, 5, 5],"float32"), list[3,0,], ) 	 101606500 	 20388 	 23.656606674194336 	 72.37714195251465 	 0.2962207794189453 	 0.9056148529052734 	 16.435459852218628 	 16.406092405319214 	 0.20725297927856445 	 0.2056429386138916 	 
2025-08-05 00:51:18.202648 test begin: paddle.tensordot(Tensor([5, 406426, 5, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[0,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4cb5f90dc0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:01:23.173174 test begin: paddle.tensordot(Tensor([5, 406426, 5, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[3,0,], )
W0805 01:01:24.264932 78339 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 406426, 5, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[3,0,], ) 	 50803875 	 20388 	 29.0469069480896 	 14.807252883911133 	 0.363661527633667 	 0.1855928897857666 	 16.002283334732056 	 16.03681778907776 	 0.200439453125 	 0.20152926445007324 	 
2025-08-05 01:02:42.031920 test begin: paddle.tensordot(Tensor([5, 5, 406426, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[0,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f487985ea40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:12:49.044016 test begin: paddle.tensordot(Tensor([5, 5, 406426, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[3,0,], )
W0805 01:12:49.991698 78944 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 406426, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[3,0,], ) 	 50803875 	 20388 	 29.038657188415527 	 14.826272964477539 	 0.3635983467102051 	 0.18546772003173828 	 15.982529401779175 	 16.025850772857666 	 0.2008833885192871 	 0.20107436180114746 	 
2025-08-05 01:14:15.540095 test begin: paddle.tensordot(Tensor([5, 5, 5, 406426],"float32"), Tensor([5, 5, 5, 406426],"float32"), list[3,0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 5, 406426],"float32"), Tensor([5, 5, 5, 406426],"float32"), list[3,0,], ) 	 101606500 	 20388 	 42.65381598472595 	 51.40291786193848 	 0.533937931060791 	 0.6423778533935547 	 16.505548000335693 	 16.269301891326904 	 0.20638799667358398 	 0.20494484901428223 	 
2025-08-05 01:16:24.220912 test begin: paddle.tensordot(Tensor([5, 5, 5, 406426],"float32"), Tensor([5, 5, 5, 5],"float32"), list[0,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0e8d220970>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:26:29.068750 test begin: paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 406426, 5, 5],"float32"), list[0,], )
W0805 01:26:30.031450 79309 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f992c4d6cb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:36:34.038970 test begin: paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 406426, 5, 5],"float32"), list[3,0,], )
W0805 01:36:34.997545 79692 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 406426, 5, 5],"float32"), list[3,0,], ) 	 50803875 	 20388 	 17.75273370742798 	 25.951196908950806 	 0.22244811058044434 	 0.32496118545532227 	 16.39976930618286 	 16.336966276168823 	 0.20533466339111328 	 0.20479869842529297 	 
2025-08-05 01:37:53.460669 test begin: paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 5, 406426, 5],"float32"), list[0,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1707caeb60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:47:58.396130 test begin: paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 5, 406426, 5],"float32"), list[3,0,], )
W0805 01:47:59.466863 80099 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 5, 406426, 5],"float32"), list[3,0,], ) 	 50803875 	 20388 	 17.68911862373352 	 25.955419301986694 	 0.22152352333068848 	 0.3250105381011963 	 16.385146141052246 	 16.33847165107727 	 0.2054119110107422 	 0.2055954933166504 	 
2025-08-05 01:49:17.483137 test begin: paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 5, 5, 406426],"float32"), list[0,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0ce235ad40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 01:59:22.469094 test begin: paddle.tensordot(x=Tensor([4, 105841, 3, 5, 4],"float64"), y=Tensor([105841, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
W0805 01:59:23.970738 80574 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 105841, 3, 5, 4],"float64"), y=Tensor([105841, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 76205520 	 20388 	 34.42316699028015 	 27.842697620391846 	 0.43131065368652344 	 0.4650125503540039 	 68.17059564590454 	 69.56626534461975 	 0.24405598640441895 	 0.2490077018737793 	 
2025-08-05 02:02:50.249583 test begin: paddle.tensordot(x=Tensor([4, 2, 158761, 5, 4],"float64"), y=Tensor([2, 4, 158761, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 158761, 5, 4],"float64"), y=Tensor([2, 4, 158761, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 76205280 	 20388 	 34.04501461982727 	 27.59549832344055 	 0.4265134334564209 	 0.4610140323638916 	 67.59959864616394 	 69.63327431678772 	 0.24192142486572266 	 0.2492532730102539 	 
2025-08-05 02:06:10.819809 test begin: paddle.tensordot(x=Tensor([4, 2, 3, 132301, 4],"float64"), y=Tensor([2, 4, 3, 132301, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 3, 132301, 4],"float64"), y=Tensor([2, 4, 3, 132301, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 38102688 	 20388 	 17.14686346054077 	 14.097089052200317 	 0.21484589576721191 	 0.23545145988464355 	 33.87640976905823 	 35.162431716918945 	 0.21217799186706543 	 0.22928786277770996 	 
2025-08-05 02:07:51.996789 test begin: paddle.tensordot(x=Tensor([4, 2, 3, 264601, 4],"float64"), y=Tensor([2, 4, 3, 264601, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 3, 264601, 4],"float64"), y=Tensor([2, 4, 3, 264601, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 76205088 	 20388 	 34.053956031799316 	 28.29548740386963 	 0.4266669750213623 	 0.46245718002319336 	 67.64873123168945 	 69.61849164962769 	 0.2422487735748291 	 0.24921202659606934 	 
2025-08-05 02:11:18.168269 test begin: paddle.tensordot(x=Tensor([4, 2, 3, 5, 211681],"float64"), y=Tensor([2, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 3, 5, 211681],"float64"), y=Tensor([2, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 25402680 	 20388 	 10.777878046035767 	 4.061908721923828 	 0.1802053451538086 	 0.10144472122192383 	 7.151373863220215 	 7.732705354690552 	 0.11943340301513672 	 0.1291060447692871 	 
2025-08-05 02:11:50.196397 test begin: paddle.tensordot(x=Tensor([4, 2, 3, 5, 4],"float64"), y=Tensor([2, 4, 3, 5, 211681],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 3, 5, 4],"float64"), y=Tensor([2, 4, 3, 5, 211681],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 25402200 	 20388 	 10.125996112823486 	 9.940101861953735 	 0.1690208911895752 	 0.24871110916137695 	 7.715824842453003 	 7.526904106140137 	 0.12872886657714844 	 0.1256401538848877 	 
2025-08-05 02:12:29.110430 test begin: paddle.tensordot(x=Tensor([4, 2, 79381, 5, 4],"float64"), y=Tensor([2, 4, 79381, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 79381, 5, 4],"float64"), y=Tensor([2, 4, 79381, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 38102880 	 20388 	 17.232216596603394 	 14.094311475753784 	 0.2158949375152588 	 0.23536992073059082 	 33.881648778915405 	 34.92965865135193 	 0.2121884822845459 	 0.21865224838256836 	 
2025-08-05 02:14:14.023981 test begin: paddle.tensordot(x=Tensor([4, 52921, 3, 5, 4],"float64"), y=Tensor([52921, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 52921, 3, 5, 4],"float64"), y=Tensor([52921, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 38103120 	 20388 	 17.40690851211548 	 14.166238784790039 	 0.21809864044189453 	 0.23668432235717773 	 34.17490291595459 	 34.92734241485596 	 0.21403193473815918 	 0.21868038177490234 	 
2025-08-05 02:15:55.615433 test begin: paddle.tile(Tensor([102426, 248, 1, 1, 2],"float32"), list[1,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([102426, 248, 1, 1, 2],"float32"), list[1,1,1,1,1,], ) 	 50803296 	 33767 	 9.99524736404419 	 10.565141439437866 	 0.3025166988372803 	 0.15988826751708984 	 10.560905933380127 	 1.6888368129730225 	 0.15982913970947266 	 7.367134094238281e-05 	 
2025-08-05 02:16:30.154772 test begin: paddle.tile(Tensor([1511, 10, 1, 58, 58],"float32"), list[1,1,4,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([1511, 10, 1, 58, 58],"float32"), list[1,1,4,1,1,], ) 	 50830040 	 33767 	 66.71700310707092 	 31.63594627380371 	 1.1565120220184326 	 0.9573140144348145 	 63.73811745643616 	 24.685062885284424 	 1.9279382228851318 	 0.7471263408660889 	 
2025-08-05 02:19:41.416068 test begin: paddle.tile(Tensor([16, 1, 1, 3, 16538, 64],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 1, 1, 3, 16538, 64],"float32"), list[1,11,1,1,1,1,], ) 	 50804736 	 33767 	 204.97760248184204 	 92.75775384902954 	 3.101827383041382 	 1.4034662246704102 	 113.60456538200378 	 58.563507318496704 	 3.4378015995025635 	 0.886310338973999 	 
2025-08-05 02:27:42.063130 test begin: paddle.tile(Tensor([16, 1, 1, 3, 64, 16538],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 1, 1, 3, 64, 16538],"float32"), list[1,11,1,1,1,1,], ) 	 50804736 	 33767 	 204.980370759964 	 92.77096772193909 	 3.1019651889801025 	 1.403721570968628 	 113.63691759109497 	 58.563720703125 	 3.439771890640259 	 0.8862519264221191 	 
2025-08-05 02:35:43.278517 test begin: paddle.tile(Tensor([16, 1, 1, 776, 64, 64],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 1, 1, 776, 64, 64],"float32"), list[1,11,1,1,1,1,], ) 	 50855936 	 33767 	 205.34539556503296 	 92.97091484069824 	 3.107415199279785 	 1.407045841217041 	 113.64999628067017 	 58.60056233406067 	 3.439706802368164 	 0.8868453502655029 	 
2025-08-05 02:43:44.200855 test begin: paddle.tile(Tensor([16, 1, 259, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 1, 259, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], ) 	 50921472 	 33767 	 205.68955039978027 	 93.13544273376465 	 3.1123876571655273 	 1.4094674587249756 	 113.71902108192444 	 58.68099570274353 	 3.4414823055267334 	 0.8880019187927246 	 
2025-08-05 02:51:45.634634 test begin: paddle.tile(Tensor([16, 10, 1, 5475, 58],"float32"), list[1,1,4,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 10, 1, 5475, 58],"float32"), list[1,1,4,1,1,], ) 	 50808000 	 33767 	 65.60957884788513 	 29.278604745864868 	 0.9930570125579834 	 0.886162281036377 	 63.778090476989746 	 25.164659023284912 	 1.929225206375122 	 0.7616839408874512 	 
2025-08-05 02:54:53.722592 test begin: paddle.tile(Tensor([16, 10, 1, 58, 5475],"float32"), list[1,1,4,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 10, 1, 58, 5475],"float32"), list[1,1,4,1,1,], ) 	 50808000 	 33767 	 65.62063932418823 	 29.31408405303955 	 0.9928100109100342 	 0.8861696720123291 	 63.75202465057373 	 25.16500425338745 	 1.9298686981201172 	 0.7616684436798096 	 
2025-08-05 02:58:02.614037 test begin: paddle.tile(Tensor([16, 10, 95, 58, 58],"float32"), list[1,1,4,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 10, 95, 58, 58],"float32"), list[1,1,4,1,1,], ) 	 51132800 	 33767 	 66.0193943977356 	 29.434144258499146 	 0.9989917278289795 	 0.8906469345092773 	 64.14728736877441 	 24.929898500442505 	 1.9417784214019775 	 0.7545382976531982 	 
2025-08-05 03:01:13.880492 test begin: paddle.tile(Tensor([16, 259, 1, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 259, 1, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], ) 	 50921472 	 33767 	 205.4219832420349 	 93.25976729393005 	 3.108893871307373 	 1.4111676216125488 	 113.683842420578 	 58.68233919143677 	 3.4423675537109375 	 0.8880457878112793 	 
2025-08-05 03:09:15.139348 test begin: paddle.tile(Tensor([16, 944, 1, 58, 58],"float32"), list[1,1,4,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 944, 1, 58, 58],"float32"), list[1,1,4,1,1,], ) 	 50809856 	 33767 	 66.54340171813965 	 31.626180171966553 	 1.0068089962005615 	 0.9570913314819336 	 63.73015999794006 	 24.67627263069153 	 1.9286401271820068 	 0.7468621730804443 	 
2025-08-05 03:12:26.174543 test begin: paddle.tile(Tensor([216, 117601, 1, 1, 2],"float32"), list[1,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([216, 117601, 1, 1, 2],"float32"), list[1,1,1,1,1,], ) 	 50803632 	 33767 	 9.997865438461304 	 10.565410375595093 	 0.3026118278503418 	 0.15987515449523926 	 10.562427997589111 	 1.7483575344085693 	 0.15985345840454102 	 0.0001475811004638672 	 
2025-08-05 03:13:02.066616 test begin: paddle.tile(Tensor([216, 248, 1, 1, 949],"float32"), list[1,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([216, 248, 1, 1, 949],"float32"), list[1,1,1,1,1,], ) 	 50836032 	 33767 	 10.006497383117676 	 10.56730318069458 	 0.3028569221496582 	 0.15996861457824707 	 10.564382553100586 	 1.6863017082214355 	 0.1598508358001709 	 8.0108642578125e-05 	 
2025-08-05 03:13:38.624298 test begin: paddle.tile(Tensor([216, 248, 1, 475, 2],"float32"), list[1,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([216, 248, 1, 475, 2],"float32"), list[1,1,1,1,1,], ) 	 50889600 	 33767 	 11.034008741378784 	 10.579532623291016 	 0.30324554443359375 	 0.16010403633117676 	 10.583508491516113 	 1.758404016494751 	 0.1601557731628418 	 7.867813110351562e-05 	 
2025-08-05 03:14:16.120128 test begin: paddle.tile(Tensor([216, 248, 475, 1, 2],"float32"), list[1,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([216, 248, 475, 1, 2],"float32"), list[1,1,1,1,1,], ) 	 50889600 	 33767 	 10.017541408538818 	 10.58233380317688 	 0.3032104969024658 	 0.16008853912353516 	 10.583401679992676 	 1.7951886653900146 	 0.16016578674316406 	 7.843971252441406e-05 	 
2025-08-05 03:14:51.323853 test begin: paddle.tile(Tensor([4135, 1, 1, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([4135, 1, 1, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], ) 	 50810880 	 33767 	 169.0532579421997 	 83.53991603851318 	 2.558135747909546 	 1.804142713546753 	 113.3714370727539 	 59.00192356109619 	 3.4308621883392334 	 0.8929238319396973 	 
2025-08-05 03:22:06.380683 test begin: paddle.tolist(Tensor([10160, 5],"float32"), )
[Prof] paddle.tolist 	 paddle.tolist(Tensor([10160, 5],"float32"), ) 	 50800 	 11272 	 95.15448904037476 	 137.8313045501709 	 7.796287536621094e-05 	 0.00010347366333007812 	 None 	 None 	 None 	 None 	 
2025-08-05 03:25:59.383717 test begin: paddle.tolist(Tensor([2, 25400],"float32"), )
[Prof] paddle.tolist 	 paddle.tolist(Tensor([2, 25400],"float32"), ) 	 50800 	 11272 	 10.1686270236969 	 11.826883792877197 	 7.462501525878906e-05 	 0.0002567768096923828 	 None 	 None 	 None 	 None 	 
2025-08-05 03:26:21.387508 test begin: paddle.tolist(Tensor([8467, 3],"int64"), )
[Prof] paddle.tolist 	 paddle.tolist(Tensor([8467, 3],"int64"), ) 	 25401 	 11272 	 61.30621552467346 	 105.01101994514465 	 7.939338684082031e-05 	 0.00010180473327636719 	 None 	 None 	 None 	 None 	 
2025-08-05 03:29:08.145593 test begin: paddle.topk(Tensor([138, 369303],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([138, 369303],"float32"), k=1, axis=0, ) 	 50963814 	 4760 	 11.446920156478882 	 41.20519208908081 	 0.6130640506744385 	 8.842575788497925 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:30:31.461215 test begin: paddle.topk(Tensor([146, 349866],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([146, 349866],"float32"), k=1, axis=0, ) 	 51080436 	 4760 	 11.172597885131836 	 51.3884072303772 	 0.5978913307189941 	 11.033511877059937 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:32:06.586825 test begin: paddle.topk(Tensor([148, 343728],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([148, 343728],"float32"), k=1, axis=0, ) 	 50871744 	 4760 	 9.99713397026062 	 44.231971740722656 	 0.5359232425689697 	 9.495909929275513 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:33:31.291896 test begin: paddle.topk(Tensor([49, 1036801],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([49, 1036801],"float32"), k=1, axis=0, ) 	 50803249 	 4760 	 10.506234407424927 	 19.615094423294067 	 0.5619573593139648 	 4.2116429805755615 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:34:38.131845 test begin: paddle.topk(Tensor([53, 958551],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([53, 958551],"float32"), k=1, axis=0, ) 	 50803203 	 4760 	 10.639783382415771 	 19.749962329864502 	 0.5697636604309082 	 4.241058111190796 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:35:42.402970 test begin: paddle.topk(Tensor([55, 923695],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([55, 923695],"float32"), k=1, axis=0, ) 	 50803225 	 4760 	 10.488113403320312 	 20.704705953598022 	 0.5622317790985107 	 4.445286273956299 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 03:36:46.476738 test begin: paddle.trace(x=Tensor([2, 3, 4233601],"float64"), offset=0, axis1=-3, axis2=-2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([2, 3, 4233601],"float64"), offset=0, axis1=-3, axis2=-2, ) 	 25401606 	 153440 	 36.78129053115845 	 12.328093767166138 	 9.679794311523438e-05 	 0.08212399482727051 	 115.84811639785767 	 36.87974238395691 	 7.486343383789062e-05 	 0.1227104663848877 	 combined
2025-08-05 03:40:09.592702 test begin: paddle.trace(x=Tensor([2, 6350401, 2],"float64"), offset=1, axis1=0, axis2=2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([2, 6350401, 2],"float64"), offset=1, axis1=0, axis2=2, ) 	 25401604 	 153440 	 36.53937339782715 	 17.872090816497803 	 9.465217590332031e-05 	 0.11903810501098633 	 116.64749217033386 	 51.26111316680908 	 7.343292236328125e-05 	 0.1707019805908203 	 combined
2025-08-05 03:43:53.183505 test begin: paddle.trace(x=Tensor([20, 3, 42336],"float64"), offset=1, axis1=0, axis2=2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([20, 3, 42336],"float64"), offset=1, axis1=0, axis2=2, ) 	 2540160 	 153440 	 9.48037075996399 	 3.0568504333496094 	 9.560585021972656e-05 	 0.00011038780212402344 	 25.081880807876587 	 12.139498472213745 	 6.842613220214844e-05 	 0.0001914501190185547 	 combined
2025-08-05 03:44:43.001301 test begin: paddle.trace(x=Tensor([30, 84672],"float64"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([30, 84672],"float64"), offset=0, axis1=0, axis2=1, ) 	 2540160 	 153440 	 11.02014684677124 	 4.639429330825806 	 9.322166442871094e-05 	 0.00021195411682128906 	 21.645214080810547 	 11.806926012039185 	 7.104873657226562e-05 	 0.00011801719665527344 	 combined
2025-08-05 03:45:33.051641 test begin: paddle.trace(x=Tensor([4233601, 3, 2],"float64"), offset=0, axis1=-3, axis2=-2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([4233601, 3, 2],"float64"), offset=0, axis1=-3, axis2=-2, ) 	 25401606 	 153440 	 9.729626178741455 	 3.08927583694458 	 9.393692016601562e-05 	 7.462501525878906e-05 	 115.68322944641113 	 21.183833599090576 	 7.224082946777344e-05 	 0.07050371170043945 	 combined
2025-08-05 03:48:05.011259 test begin: paddle.trace(x=Tensor([4233601, 3, 2],"float64"), offset=1, axis1=0, axis2=2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([4233601, 3, 2],"float64"), offset=1, axis1=0, axis2=2, ) 	 25401606 	 153440 	 9.518775463104248 	 3.0487844944000244 	 0.00012254714965820312 	 7.200241088867188e-05 	 116.61034226417542 	 21.176136016845703 	 6.651878356933594e-05 	 0.07048749923706055 	 combined
2025-08-05 03:50:38.069675 test begin: paddle.trace(x=Tensor([6350401, 4],"float64"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([6350401, 4],"float64"), offset=0, axis1=0, axis2=1, ) 	 25401604 	 153440 	 9.691648960113525 	 3.2736785411834717 	 6.151199340820312e-05 	 0.00018548965454101562 	 91.32479810714722 	 21.18383526802063 	 7.414817810058594e-05 	 0.07047462463378906 	 combined
2025-08-05 03:52:44.136024 test begin: paddle.transpose(Tensor([20, 150, 512, 512],"float32"), list[0,2,3,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([20, 150, 512, 512],"float32"), list[0,2,3,1,], ) 	 786432000 	 2997001 	 10.849210023880005 	 13.177731275558472 	 0.0001327991485595703 	 9.34600830078125e-05 	 120.6515154838562 	 161.56621670722961 	 0.00011777877807617188 	 0.0002269744873046875 	 
2025-08-05 03:58:19.693914 test begin: paddle.transpose(Tensor([20, 7168, 7168],"bfloat16"), list[0,2,1,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa724330be0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 04:08:25.925653 test begin: paddle.transpose(Tensor([40, 150, 166, 512],"float32"), list[0,2,3,1,], )
W0805 04:08:33.843580 90675 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.transpose 	 paddle.transpose(Tensor([40, 150, 166, 512],"float32"), list[0,2,3,1,], ) 	 509952000 	 2997001 	 10.337329149246216 	 14.477659463882446 	 0.00011491775512695312 	 0.0002875328063964844 	 118.81118583679199 	 165.57820415496826 	 0.00012564659118652344 	 0.00022792816162109375 	 
2025-08-05 04:13:53.755538 test begin: paddle.transpose(Tensor([40, 150, 512, 166],"float32"), list[0,2,3,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([40, 150, 512, 166],"float32"), list[0,2,3,1,], ) 	 509952000 	 2997001 	 10.3648202419281 	 13.104314088821411 	 9.250640869140625e-05 	 8.249282836914062e-05 	 121.43355536460876 	 163.16720986366272 	 0.00010991096496582031 	 0.0002319812774658203 	 
2025-08-05 04:19:19.031087 test begin: paddle.transpose(Tensor([40, 3584, 7168],"bfloat16"), list[0,2,1,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2a1e91bb80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 04:29:25.902134 test begin: paddle.transpose(Tensor([40, 49, 512, 512],"float32"), list[0,2,3,1,], )
W0805 04:29:33.734772 109898 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.transpose 	 paddle.transpose(Tensor([40, 49, 512, 512],"float32"), list[0,2,3,1,], ) 	 513802240 	 2997001 	 10.318025827407837 	 14.436001777648926 	 0.00016927719116210938 	 0.00013494491577148438 	 123.47248864173889 	 167.09527802467346 	 0.0001049041748046875 	 0.0002677440643310547 	 
2025-08-05 04:35:00.897102 test begin: paddle.transpose(Tensor([60, 2363, 7168],"bfloat16"), list[0,2,1,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f28c5c6ee00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 04:45:08.794654 test begin: paddle.transpose(Tensor([60, 3584, 4726],"bfloat16"), list[0,2,1,], )
W0805 04:45:24.636745 124474 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f06803cb010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 04:55:15.235733 test begin: paddle.transpose(Tensor([60, 7168, 2363],"bfloat16"), list[0,2,1,], )
W0805 04:55:30.498823 142207 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa5a28f7010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 05:05:23.723475 test begin: paddle.tril(Tensor([1, 1, 2048, 24807],"bool"), )
W0805 05:05:24.700510 159957 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 1, 2048, 24807],"bool"), ) 	 50804736 	 32473 	 9.985740423202515 	 8.408012866973877 	 0.31557631492614746 	 0.26592206954956055 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:05:55.829214 test begin: paddle.tril(Tensor([1, 1, 2048, 24807],"float32"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 1, 2048, 24807],"float32"), ) 	 50804736 	 32473 	 10.083645105361938 	 10.812525749206543 	 0.3172783851623535 	 0.3397023677825928 	 10.087096691131592 	 10.785703420639038 	 0.3186769485473633 	 0.3392148017883301 	 
2025-08-05 05:06:41.393831 test begin: paddle.tril(Tensor([1, 1, 24807, 2048],"bool"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 1, 24807, 2048],"bool"), ) 	 50804736 	 32473 	 12.088407278060913 	 7.645270109176636 	 0.3802611827850342 	 0.2403099536895752 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:07:14.829198 test begin: paddle.tril(Tensor([1, 1, 24807, 2048],"float32"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 1, 24807, 2048],"float32"), ) 	 50804736 	 32473 	 13.457826137542725 	 12.220778703689575 	 0.42329931259155273 	 0.38437724113464355 	 13.445749044418335 	 12.220667362213135 	 0.423081636428833 	 0.3857855796813965 	 
2025-08-05 05:08:09.096439 test begin: paddle.tril(Tensor([1, 13, 2048, 2048],"bool"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 13, 2048, 2048],"bool"), ) 	 54525952 	 32473 	 12.407200574874878 	 10.050155639648438 	 0.391571044921875 	 0.25606465339660645 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:08:46.571454 test begin: paddle.tril(Tensor([1, 13, 2048, 2048],"float32"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 13, 2048, 2048],"float32"), ) 	 54525952 	 32473 	 13.471810579299927 	 12.357731103897095 	 0.4233670234680176 	 0.3886833190917969 	 13.462111949920654 	 12.361844062805176 	 0.42337870597839355 	 0.38869428634643555 	 
2025-08-05 05:09:41.752733 test begin: paddle.tril(Tensor([13, 1, 2048, 2048],"bool"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([13, 1, 2048, 2048],"bool"), ) 	 54525952 	 32473 	 12.409427881240845 	 8.145467281341553 	 0.3904154300689697 	 0.25731873512268066 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:10:16.226473 test begin: paddle.tril(Tensor([13, 1, 2048, 2048],"float32"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([13, 1, 2048, 2048],"float32"), ) 	 54525952 	 32473 	 13.475157499313354 	 12.374248027801514 	 0.42533230781555176 	 0.38936758041381836 	 13.466944456100464 	 12.384337902069092 	 0.42372894287109375 	 0.3912053108215332 	 
2025-08-05 05:11:09.761647 test begin: paddle.tril(Tensor([2048, 24807],"bool"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([2048, 24807],"bool"), ) 	 50804736 	 32473 	 9.997323274612427 	 8.391808986663818 	 0.3144876956939697 	 0.26384782791137695 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:11:40.658000 test begin: paddle.tril(Tensor([24807, 2048],"bool"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([24807, 2048],"bool"), ) 	 50804736 	 32473 	 12.07883071899414 	 7.898770570755005 	 0.3799901008605957 	 0.24117588996887207 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 05:12:15.924010 test begin: paddle.triu(Tensor([1, 1, 1024, 99226],"float16"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 1024, 99226],"float16"), diagonal=1, ) 	 101607424 	 30544 	 23.494537591934204 	 16.099753379821777 	 0.7856154441833496 	 0.5384695529937744 	 23.46079683303833 	 16.09563970565796 	 0.786527156829834 	 0.5383684635162354 	 
2025-08-05 05:13:41.076704 test begin: paddle.triu(Tensor([1, 1, 12404, 4096],"float32"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 12404, 4096],"float32"), diagonal=1, ) 	 50806784 	 30544 	 9.910729169845581 	 10.343192100524902 	 0.3312554359436035 	 0.34731554985046387 	 9.915586471557617 	 10.340469598770142 	 0.3315284252166748 	 0.3458244800567627 	 
2025-08-05 05:14:23.500181 test begin: paddle.triu(Tensor([1, 1, 2048, 49613],"float16"), )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 2048, 49613],"float16"), ) 	 101607424 	 30544 	 23.490508794784546 	 16.079718112945557 	 0.7855048179626465 	 0.5381419658660889 	 23.45982313156128 	 16.043768167495728 	 0.7859096527099609 	 0.538053035736084 	 
2025-08-05 05:15:46.480979 test begin: paddle.triu(Tensor([1, 1, 4096, 12404],"float32"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 4096, 12404],"float32"), diagonal=1, ) 	 50806784 	 30544 	 12.51414179801941 	 11.342087745666504 	 0.4199039936065674 	 0.37923574447631836 	 12.515218734741211 	 11.338973999023438 	 0.419938325881958 	 0.37926483154296875 	 
2025-08-05 05:16:36.104912 test begin: paddle.triu(Tensor([1, 1, 49613, 2048],"float16"), )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 49613, 2048],"float16"), ) 	 101607424 	 30544 	 17.986953496932983 	 11.150675296783447 	 0.6028046607971191 	 0.37331676483154297 	 17.962343215942383 	 11.16110110282898 	 0.6006369590759277 	 0.3732883930206299 	 
2025-08-05 05:17:40.984530 test begin: paddle.triu(Tensor([1, 1, 99226, 1024],"float16"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 99226, 1024],"float16"), diagonal=1, ) 	 101607424 	 30544 	 18.080285787582397 	 11.123496532440186 	 0.5968515872955322 	 0.37224793434143066 	 17.849390029907227 	 11.128949880599976 	 0.599555492401123 	 0.37221527099609375 	 
2025-08-05 05:18:44.326605 test begin: paddle.triu(Tensor([1, 25, 2048, 2048],"float16"), )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 25, 2048, 2048],"float16"), ) 	 104857600 	 30544 	 22.650830030441284 	 13.359668970108032 	 0.7577269077301025 	 0.4470369815826416 	 22.67890453338623 	 13.364520788192749 	 0.7582874298095703 	 0.4469926357269287 	 
2025-08-05 05:20:01.421339 test begin: paddle.triu(Tensor([1, 4, 4096, 4096],"float32"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 4, 4096, 4096],"float32"), diagonal=1, ) 	 67108864 	 30544 	 15.124165296554565 	 14.299227476119995 	 0.5057532787322998 	 0.47821044921875 	 15.130253791809082 	 14.297548532485962 	 0.5073161125183105 	 0.4778003692626953 	 
2025-08-05 05:21:05.092556 test begin: paddle.triu(Tensor([1, 97, 1024, 1024],"float16"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 97, 1024, 1024],"float16"), diagonal=1, ) 	 101711872 	 30544 	 22.81442356109619 	 13.02578854560852 	 0.7568140029907227 	 0.4355900287628174 	 22.569740772247314 	 13.023558139801025 	 0.755016565322876 	 0.43549466133117676 	 
2025-08-05 05:22:22.369818 test begin: paddle.triu(Tensor([25, 1, 2048, 2048],"float16"), )
[Prof] paddle.triu 	 paddle.triu(Tensor([25, 1, 2048, 2048],"float16"), ) 	 104857600 	 30544 	 22.662659168243408 	 13.348085165023804 	 0.7593529224395752 	 0.44615840911865234 	 22.647956132888794 	 13.334660768508911 	 0.7573568820953369 	 0.4464097023010254 	 
2025-08-05 05:23:41.549209 test begin: paddle.triu(Tensor([4, 1, 4096, 4096],"float32"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([4, 1, 4096, 4096],"float32"), diagonal=1, ) 	 67108864 	 30544 	 15.120839834213257 	 14.299288988113403 	 0.5055599212646484 	 0.4795503616333008 	 15.125463247299194 	 14.28320574760437 	 0.5072164535522461 	 0.47788047790527344 	 
2025-08-05 05:24:44.425919 test begin: paddle.triu(Tensor([97, 1, 1024, 1024],"float16"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([97, 1, 1024, 1024],"float16"), diagonal=1, ) 	 101711872 	 30544 	 22.57378315925598 	 13.052939176559448 	 0.7548820972442627 	 0.43587565422058105 	 22.56727147102356 	 13.03249478340149 	 0.7543504238128662 	 0.43578100204467773 	 
2025-08-05 05:25:59.561863 test begin: paddle.trunc(Tensor([200, 2540161],"float32"), )
[Prof] paddle.trunc 	 paddle.trunc(Tensor([200, 2540161],"float32"), ) 	 508032200 	 34308 	 117.18786978721619 	 100.39438462257385 	 0.00011873245239257812 	 2.9892046451568604 	 84.93509078025818 	 45.02736473083496 	 8.535385131835938e-05 	 1.3394269943237305 	 
2025-08-05 05:32:08.666099 test begin: paddle.trunc(Tensor([25401610, 20],"float32"), )
[Prof] paddle.trunc 	 paddle.trunc(Tensor([25401610, 20],"float32"), ) 	 508032200 	 34308 	 117.19955325126648 	 100.38374066352844 	 0.00015497207641601562 	 2.9903175830841064 	 84.99421668052673 	 45.02972340583801 	 9.679794311523438e-05 	 1.3405911922454834 	 
2025-08-05 05:38:15.854746 test begin: paddle.trunc(input=Tensor([1176010, 6, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([1176010, 6, 6, 6],"float64"), ) 	 254018160 	 34308 	 98.82561898231506 	 99.9657576084137 	 0.00013375282287597656 	 2.976392984390259 	 43.71156144142151 	 45.04057312011719 	 4.482269287109375e-05 	 1.344494342803955 	 
2025-08-05 05:43:16.033960 test begin: paddle.trunc(input=Tensor([196010, 6, 6, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([196010, 6, 6, 6, 6],"float64"), ) 	 254028960 	 34308 	 98.74096655845642 	 99.97356915473938 	 0.00011706352233886719 	 2.977572441101074 	 43.72168850898743 	 45.04194188117981 	 8.559226989746094e-05 	 1.3413152694702148 	 
2025-08-05 05:48:15.861015 test begin: paddle.trunc(input=Tensor([30, 39201, 6, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([30, 39201, 6, 6, 6],"float64"), ) 	 254022480 	 34308 	 98.8804292678833 	 100.721932888031 	 0.00011396408081054688 	 2.97908091545105 	 43.721965312957764 	 45.03918123245239 	 8.058547973632812e-05 	 1.3402900695800781 	 
2025-08-05 05:53:17.713786 test begin: paddle.trunc(input=Tensor([30, 6, 39201, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([30, 6, 39201, 6, 6],"float64"), ) 	 254022480 	 34308 	 98.88414573669434 	 99.97173833847046 	 0.00013327598571777344 	 2.979161024093628 	 43.720731258392334 	 45.04378032684326 	 0.0001544952392578125 	 1.3422191143035889 	 
2025-08-05 05:58:18.609001 test begin: paddle.trunc(input=Tensor([30, 6, 6, 39201, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([30, 6, 6, 39201, 6],"float64"), ) 	 254022480 	 34308 	 98.88357663154602 	 99.96612763404846 	 0.00012230873107910156 	 2.977810859680176 	 43.71604108810425 	 45.03502035140991 	 8.034706115722656e-05 	 1.342010736465454 	 
2025-08-05 06:03:18.565367 test begin: paddle.trunc(input=Tensor([30, 6, 6, 6, 39201],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([30, 6, 6, 6, 39201],"float64"), ) 	 254022480 	 34308 	 98.90293526649475 	 99.99725437164307 	 0.0003848075866699219 	 2.9777708053588867 	 43.7339768409729 	 45.055378437042236 	 4.2438507080078125e-05 	 1.3403358459472656 	 
2025-08-05 06:08:18.669901 test begin: paddle.trunc(input=Tensor([60, 117601, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([60, 117601, 6, 6],"float64"), ) 	 254018160 	 34308 	 99.01598453521729 	 99.99220204353333 	 0.0001087188720703125 	 2.9806785583496094 	 43.73170804977417 	 45.050015449523926 	 6.079673767089844e-05 	 1.3418233394622803 	 
2025-08-05 06:13:18.881903 test begin: paddle.trunc(input=Tensor([60, 6, 117601, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([60, 6, 117601, 6],"float64"), ) 	 254018160 	 34308 	 99.00482034683228 	 100.00637555122375 	 0.0001163482666015625 	 2.9777233600616455 	 43.72922396659851 	 45.053415060043335 	 8.296966552734375e-05 	 1.3419039249420166 	 
2025-08-05 06:18:20.816647 test begin: paddle.trunc(input=Tensor([60, 6, 6, 117601],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([60, 6, 6, 117601],"float64"), ) 	 254018160 	 34308 	 99.0158269405365 	 99.98889541625977 	 0.000102996826171875 	 2.976377487182617 	 43.72951412200928 	 45.05036520957947 	 5.078315734863281e-05 	 1.3418314456939697 	 
2025-08-05 06:23:22.298987 test begin: paddle.unbind(Tensor([20, 3, 1058401, 8],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([20, 3, 1058401, 8],"float32"), axis=0, ) 	 508032480 	 85063 	 2.434375047683716 	 2.000332832336426 	 7.772445678710938e-05 	 0.0001232624053955078 	 308.66749930381775 	 259.1439139842987 	 3.707059144973755 	 3.1125118732452393 	 
2025-08-05 06:33:13.624173 test begin: paddle.unbind(Tensor([20, 3, 8, 1058401],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([20, 3, 8, 1058401],"float32"), axis=0, ) 	 508032480 	 85063 	 2.452721118927002 	 2.631056785583496 	 0.00010848045349121094 	 0.0006706714630126953 	 309.10846996307373 	 259.14580035209656 	 4.149924039840698 	 3.1126372814178467 	 
2025-08-05 06:43:09.208847 test begin: paddle.unbind(Tensor([20, 396901, 8, 8],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([20, 396901, 8, 8],"float32"), axis=0, ) 	 508033280 	 85063 	 2.507281541824341 	 1.9768269062042236 	 0.00023484230041503906 	 0.0001971721649169922 	 299.3069667816162 	 254.31638741493225 	 3.597632884979248 	 3.0576648712158203 	 
2025-08-05 06:52:46.451795 test begin: paddle.unbind(Tensor([30, 3386881, 5],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([30, 3386881, 5],"float32"), axis=0, ) 	 508032150 	 85063 	 3.2976455688476562 	 2.83296275138855 	 0.0001327991485595703 	 0.0001232624053955078 	 312.60407757759094 	 261.1917407512665 	 3.757319688796997 	 3.1370444297790527 	 
2025-08-05 07:02:43.992739 test begin: paddle.unbind(Tensor([30, 9, 1881601],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([30, 9, 1881601],"float32"), axis=0, ) 	 508032270 	 85063 	 3.2619950771331787 	 2.8259429931640625 	 0.00011038780212402344 	 0.0006825923919677734 	 312.6364333629608 	 261.3549642562866 	 3.7576241493225098 	 3.1374635696411133 	 
2025-08-05 07:12:44.511707 test begin: paddle.unbind(Tensor([40, 2116801, 6],"float32"), )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([40, 2116801, 6],"float32"), ) 	 508032240 	 85063 	 4.13705039024353 	 3.6239311695098877 	 0.00021982192993164062 	 0.00010895729064941406 	 311.4626109600067 	 259.20185446739197 	 3.7402474880218506 	 3.1117448806762695 	 
2025-08-05 07:22:42.773297 test begin: paddle.unbind(Tensor([40, 5, 2540161],"float32"), )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([40, 5, 2540161],"float32"), ) 	 508032200 	 85063 	 4.206434726715088 	 3.7336864471435547 	 0.00027823448181152344 	 0.0006127357482910156 	 311.8847465515137 	 259.6527407169342 	 3.7487056255340576 	 3.1172268390655518 	 
2025-08-05 07:32:42.232447 test begin: paddle.unflatten(x=Tensor([4, 793801, 16],"float32"), axis=0, shape=Tensor([2],"int64"), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([4, 793801, 16],"float32"), axis=0, shape=Tensor([2],"int64"), ) 	 50803266 	 101360 	 12.611051321029663 	 0.5320143699645996 	 7.534027099609375e-05 	 7.963180541992188e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:33:02.226740 test begin: paddle.unflatten(x=Tensor([40, 6, 2116801],"float32"), axis=0, shape=Tensor([2],"int64"), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([40, 6, 2116801],"float32"), axis=0, shape=Tensor([2],"int64"), ) 	 508032242 	 101360 	 9.956346988677979 	 0.5457546710968018 	 0.0001468658447265625 	 0.00012731552124023438 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:33:34.211025 test begin: paddle.unflatten(x=Tensor([40, 793801, 16],"float32"), axis=0, shape=Tensor([2],"int64"), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([40, 793801, 16],"float32"), axis=0, shape=Tensor([2],"int64"), ) 	 508032642 	 101360 	 9.90986180305481 	 0.5289895534515381 	 6.270408630371094e-05 	 0.00010180473327636719 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:34:08.436824 test begin: paddle.unflatten(x=Tensor([5292010, 6, 16],"float32"), axis=0, shape=Tensor([2],"int64"), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([5292010, 6, 16],"float32"), axis=0, shape=Tensor([2],"int64"), ) 	 508032962 	 101360 	 9.801141262054443 	 0.5336453914642334 	 5.7220458984375e-05 	 8.940696716308594e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 07:34:43.258104 test begin: paddle.unfold(Tensor([50, 20321281],"float16"), 0, 5, 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcee1e56d40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 07:45:01.439516 test begin: paddle.unique(Tensor([25401601],"int64"), )
W0805 07:45:01.986562 26579 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.unique 	 paddle.unique(Tensor([25401601],"int64"), ) 	 25401601 	 1489 	 10.027994155883789 	 4.8628740310668945 	 8.368492126464844e-05 	 6.508827209472656e-05 	 None 	 None 	 None 	 None 	 
2025-08-05 07:45:17.203416 test begin: paddle.unique(Tensor([25401601],"int64"), return_index=True, return_inverse=True, return_counts=True, dtype="int32", )
[Prof] paddle.unique 	 paddle.unique(Tensor([25401601],"int64"), return_index=True, return_inverse=True, return_counts=True, dtype="int32", ) 	 25401601 	 1489 	 15.011305570602417 	 16.613317012786865 	 0.00018596649169921875 	 0.00020313262939453125 	 None 	 None 	 None 	 None 	 
2025-08-05 07:45:50.438264 test begin: paddle.unique_consecutive(Tensor([25401601],"float64"), )
[Prof] paddle.unique_consecutive 	 paddle.unique_consecutive(Tensor([25401601],"float64"), ) 	 25401601 	 6999 	 22.09610342979431 	 2.5456340312957764 	 0.00011229515075683594 	 0.0001010894775390625 	 None 	 None 	 None 	 None 	 
2025-08-05 07:46:15.740829 test begin: paddle.unique_consecutive(Tensor([25401601],"float64"), return_inverse=True, return_counts=True, )
[Prof] paddle.unique_consecutive 	 paddle.unique_consecutive(Tensor([25401601],"float64"), return_inverse=True, return_counts=True, ) 	 25401601 	 6999 	 60.261306285858154 	 7.095552444458008 	 7.891654968261719e-05 	 6.008148193359375e-05 	 None 	 None 	 None 	 None 	 
2025-08-05 07:47:23.671882 test begin: paddle.unique_consecutive(Tensor([2540],"float64"), return_inverse=True, return_counts=True, axis=-1, )
[Prof] paddle.unique_consecutive 	 paddle.unique_consecutive(Tensor([2540],"float64"), return_inverse=True, return_counts=True, axis=-1, ) 	 2540 	 6999 	 33.389013051986694 	 1.190922737121582 	 7.081031799316406e-05 	 5.888938903808594e-05 	 None 	 None 	 None 	 None 	 
2025-08-05 07:47:58.623203 test begin: paddle.unsqueeze(Tensor([250, 1024, 1024],"int64"), 1, )
Warning: The core code of paddle.unsqueeze is too complex.
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([250, 1024, 1024],"int64"), 1, ) 	 262144000 	 2405680 	 9.869277238845825 	 8.921261548995972 	 0.00012636184692382812 	 0.0002799034118652344 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 07:50:10.667590 test begin: paddle.unsqueeze(Tensor([39700, 50, 256],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([39700, 50, 256],"float32"), axis=2, ) 	 508160000 	 2405680 	 10.210969686508179 	 9.009098291397095 	 0.00010156631469726562 	 7.796287536621094e-05 	 111.29637217521667 	 131.78753566741943 	 0.000110626220703125 	 0.0002522468566894531 	 
2025-08-05 07:54:52.323056 test begin: paddle.unsqueeze(Tensor([40, 1024, 6202],"int64"), 1, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([40, 1024, 6202],"int64"), 1, ) 	 254033920 	 2405680 	 9.868271827697754 	 12.677477836608887 	 7.939338684082031e-05 	 0.00025081634521484375 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 07:57:13.509048 test begin: paddle.unsqueeze(Tensor([40, 6202, 1024],"int64"), 1, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([40, 6202, 1024],"int64"), 1, ) 	 254033920 	 2405680 	 17.53615951538086 	 8.975351333618164 	 0.00012612342834472656 	 0.0001266002655029297 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 07:59:43.187425 test begin: paddle.unsqueeze(Tensor([4160, 478, 256],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([4160, 478, 256],"float32"), axis=2, ) 	 509050880 	 2405680 	 10.222936391830444 	 9.007337808609009 	 0.0001125335693359375 	 0.00013113021850585938 	 106.13252305984497 	 131.57499384880066 	 0.0001811981201171875 	 0.0002315044403076172 	 
2025-08-05 08:04:17.191990 test begin: paddle.unsqueeze(Tensor([4160, 50, 2443],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([4160, 50, 2443],"float32"), axis=2, ) 	 508144000 	 2405680 	 10.260429382324219 	 9.133922815322876 	 7.677078247070312e-05 	 0.00025343894958496094 	 111.4716546535492 	 131.64667224884033 	 0.00010037422180175781 	 0.0002493858337402344 	 
2025-08-05 08:08:56.745701 test begin: paddle.unsqueeze(Tensor([5120, 388, 256],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([5120, 388, 256],"float32"), axis=2, ) 	 508559360 	 2405680 	 10.27266001701355 	 9.14509129524231 	 7.319450378417969e-05 	 8.726119995117188e-05 	 107.55763626098633 	 131.07863688468933 	 0.00011301040649414062 	 0.00022530555725097656 	 
2025-08-05 08:13:32.698134 test begin: paddle.unsqueeze(Tensor([5120, 50, 1985],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([5120, 50, 1985],"float32"), axis=2, ) 	 508160000 	 2405680 	 10.260877847671509 	 8.961026430130005 	 7.271766662597656e-05 	 8.20159912109375e-05 	 106.38790249824524 	 131.19663166999817 	 0.00011157989501953125 	 0.0002200603485107422 	 
2025-08-05 08:18:07.019827 test begin: paddle.unstack(Tensor([338689, 10, 15],"float32"), axis=-1, )
[Prof] paddle.unstack 	 paddle.unstack(Tensor([338689, 10, 15],"float32"), axis=-1, ) 	 50803350 	 30584 	 10.043545961380005 	 0.5796561241149902 	 0.33516573905944824 	 4.935264587402344e-05 	 14.517818927764893 	 136.19226908683777 	 0.48436856269836426 	 4.554546117782593 	 
2025-08-05 08:20:50.085611 test begin: paddle.unstack(Tensor([338689, 10, 15],"float32"), axis=-2, )
[Prof] paddle.unstack 	 paddle.unstack(Tensor([338689, 10, 15],"float32"), axis=-2, ) 	 50803350 	 30584 	 12.381293535232544 	 0.5792074203491211 	 0.41312503814697266 	 9.799003601074219e-05 	 10.73326563835144 	 38.72983169555664 	 0.35765671730041504 	 1.2941067218780518 	 
2025-08-05 08:21:54.514238 test begin: paddle.unstack(Tensor([5, 10, 1016065],"float32"), axis=-2, )
[Prof] paddle.unstack 	 paddle.unstack(Tensor([5, 10, 1016065],"float32"), axis=-2, ) 	 50803250 	 30584 	 11.914232730865479 	 0.41316914558410645 	 0.39731931686401367 	 5.53131103515625e-05 	 10.139498472213745 	 9.354832887649536 	 0.33823275566101074 	 0.31218957901000977 	 
2025-08-05 08:22:28.317530 test begin: paddle.unstack(Tensor([5, 677377, 15],"float32"), axis=-1, )
[Prof] paddle.unstack 	 paddle.unstack(Tensor([5, 677377, 15],"float32"), axis=-1, ) 	 50803275 	 30584 	 10.039819717407227 	 0.5769312381744385 	 0.33530211448669434 	 8.797645568847656e-05 	 14.581875801086426 	 136.2055265903473 	 0.4872093200683594 	 4.550244569778442 	 
2025-08-05 08:25:11.345507 test begin: paddle.unstack(x=Tensor([2, 32, 793801],"float32"), axis=0, )
[Prof] paddle.unstack 	 paddle.unstack(x=Tensor([2, 32, 793801],"float32"), axis=0, ) 	 50803264 	 30584 	 11.890166997909546 	 0.18975424766540527 	 0.39699888229370117 	 0.0001862049102783203 	 10.694875478744507 	 9.468955516815186 	 0.35684776306152344 	 0.3160066604614258 	 
2025-08-05 08:25:48.094565 test begin: paddle.unstack(x=Tensor([2, 49613, 512],"float32"), axis=0, )
[Prof] paddle.unstack 	 paddle.unstack(x=Tensor([2, 49613, 512],"float32"), axis=0, ) 	 50803712 	 30584 	 11.793957233428955 	 0.16855645179748535 	 0.39672207832336426 	 5.888938903808594e-05 	 10.73805022239685 	 9.351973533630371 	 0.3597545623779297 	 0.31200337409973145 	 
2025-08-05 08:26:22.014317 test begin: paddle.unstack(x=Tensor([3101, 32, 512],"float32"), axis=0, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f394dfb6110>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 08:36:27.834985 test begin: paddle.var(Tensor([264601, 192, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
W0805 08:36:28.797297 53408 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 08:36:28.832576 53408 dygraph_functions.cc:88394] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.var 	 paddle.var(Tensor([264601, 192, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50803392 	 10215 	 13.356839895248413 	 2.8089349269866943 	 0.11093950271606445 	 0.2807633876800537 	 16.126046657562256 	 7.9489710330963135 	 0.2684323787689209 	 0.19841313362121582 	 
2025-08-05 08:37:09.742739 test begin: paddle.var(Tensor([384, 132301, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([384, 132301, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50803584 	 10215 	 10.557336568832397 	 1.898282766342163 	 0.07524347305297852 	 0.09494924545288086 	 140.3799970149994 	 7.89865779876709 	 2.0103142261505127 	 0.15777158737182617 	 
2025-08-05 08:39:51.383129 test begin: paddle.var(Tensor([384, 192, 1, 690],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([384, 192, 1, 690],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50872320 	 10215 	 10.441104412078857 	 1.8328886032104492 	 0.07443118095397949 	 0.09152626991271973 	 14.072810888290405 	 7.884437084197998 	 0.20081090927124023 	 0.15754008293151855 	 
2025-08-05 08:40:26.647372 test begin: paddle.var(Tensor([384, 192, 690, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([384, 192, 690, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50872320 	 10215 	 10.439507722854614 	 1.8286864757537842 	 0.07442784309387207 	 0.0914924144744873 	 138.2278597354889 	 7.893355369567871 	 1.9782733917236328 	 0.15764689445495605 	 
2025-08-05 08:43:05.970596 test begin: paddle.var(Tensor([384, 96, 1, 1379],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([384, 96, 1, 1379],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50835456 	 10215 	 10.458279371261597 	 1.8468644618988037 	 0.07587599754333496 	 0.09238529205322266 	 14.070199966430664 	 7.904847145080566 	 0.20076489448547363 	 0.1592404842376709 	 
2025-08-05 08:43:41.152856 test begin: paddle.var(Tensor([384, 96, 1379, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([384, 96, 1379, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50835456 	 10215 	 10.457282543182373 	 1.8481242656707764 	 0.07453513145446777 	 0.09233498573303223 	 140.77680778503418 	 7.908227920532227 	 2.0121805667877197 	 0.15798664093017578 	 
2025-08-05 08:46:23.058358 test begin: paddle.var(Tensor([529201, 96, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([529201, 96, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50803296 	 10215 	 13.124552011489868 	 4.629577875137329 	 0.10900759696960449 	 0.4624767303466797 	 16.251183032989502 	 8.413700819015503 	 0.2707328796386719 	 0.20997381210327148 	 
2025-08-05 08:47:06.350324 test begin: paddle.var(Tensor([58801, 96, 3, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([58801, 96, 3, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50804064 	 10215 	 10.013078451156616 	 1.8059117794036865 	 0.08304095268249512 	 0.18061375617980957 	 13.907426118850708 	 7.865137100219727 	 0.23171210289001465 	 0.19646000862121582 	 
2025-08-05 08:47:41.884706 test begin: paddle.var(Tensor([96, 58801, 3, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([96, 58801, 3, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50804064 	 10215 	 10.148832321166992 	 2.224724769592285 	 0.07233095169067383 	 0.1112523078918457 	 13.767096042633057 	 8.041108131408691 	 0.19789385795593262 	 0.1606154441833496 	 
2025-08-05 08:48:16.965196 test begin: paddle.var(Tensor([96, 96, 1838, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([96, 96, 1838, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50817024 	 10215 	 10.11979341506958 	 2.18188214302063 	 0.07210922241210938 	 0.10891246795654297 	 13.754673719406128 	 8.032875776290894 	 0.19636893272399902 	 0.16041207313537598 	 
2025-08-05 08:48:52.672992 test begin: paddle.var(Tensor([96, 96, 3, 1838],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([96, 96, 3, 1838],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50817024 	 10215 	 10.115201711654663 	 2.180469274520874 	 0.07210445404052734 	 0.10892987251281738 	 13.764222860336304 	 8.03305721282959 	 0.19786715507507324 	 0.1604318618774414 	 
2025-08-05 08:49:27.638294 test begin: paddle.vecdot(Tensor([12700801, 4],"float32"), Tensor([12700801, 4],"float32"), axis=-1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([12700801, 4],"float32"), Tensor([12700801, 4],"float32"), axis=-1, ) 	 101606408 	 10704 	 12.33104157447815 	 9.99300241470337 	 0.3930180072784424 	 0.47608470916748047 	 17.141106605529785 	 7.323081970214844 	 0.5482597351074219 	 0.35048556327819824 	 combined
2025-08-05 08:50:16.441260 test begin: paddle.vecdot(Tensor([1270081, 4, 5],"float64"), Tensor([1270081, 4, 5],"float64"), axis=1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([1270081, 4, 5],"float64"), Tensor([1270081, 4, 5],"float64"), axis=1, ) 	 50803240 	 10704 	 11.037963390350342 	 6.786255598068237 	 0.352353572845459 	 0.32311415672302246 	 13.324824571609497 	 7.213327884674072 	 0.42397570610046387 	 0.3439481258392334 	 combined
2025-08-05 08:50:56.897084 test begin: paddle.vecdot(Tensor([2, 3, 4233601],"float64"), Tensor([2, 3, 4233601],"float64"), axis=-1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([2, 3, 4233601],"float64"), Tensor([2, 3, 4233601],"float64"), axis=-1, ) 	 50803212 	 10704 	 10.652698993682861 	 6.391839265823364 	 0.2537522315979004 	 0.20421981811523438 	 12.644212245941162 	 6.440860271453857 	 0.4021744728088379 	 0.30701494216918945 	 combined
2025-08-05 08:51:34.232906 test begin: paddle.vecdot(Tensor([2, 3175201, 4],"float64"), Tensor([2, 3175201, 4],"float64"), axis=-1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([2, 3175201, 4],"float64"), Tensor([2, 3175201, 4],"float64"), axis=-1, ) 	 50803216 	 10704 	 11.13581657409668 	 7.655876636505127 	 0.3396298885345459 	 0.36432504653930664 	 13.325552225112915 	 7.206469774246216 	 0.4254035949707031 	 0.3448143005371094 	 combined
2025-08-05 08:52:16.201244 test begin: paddle.vecdot(Tensor([2116801, 3, 4],"float64"), Tensor([2116801, 3, 4],"float64"), axis=-1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([2116801, 3, 4],"float64"), Tensor([2116801, 3, 4],"float64"), axis=-1, ) 	 50803224 	 10704 	 10.685853719711304 	 7.649440288543701 	 0.3396782875061035 	 0.36432337760925293 	 13.328727006912231 	 7.205635070800781 	 0.42537522315979004 	 0.3449394702911377 	 combined
2025-08-05 08:52:56.612185 test begin: paddle.vecdot(Tensor([3, 16934401],"float32"), Tensor([3, 16934401],"float32"), axis=-1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([3, 16934401],"float32"), Tensor([3, 16934401],"float32"), axis=-1, ) 	 101606406 	 10704 	 10.368175506591797 	 6.418842554092407 	 0.24712133407592773 	 0.20526719093322754 	 16.39727520942688 	 6.498427391052246 	 0.5213577747344971 	 0.30991578102111816 	 combined
2025-08-05 08:53:39.954814 test begin: paddle.vecdot(Tensor([3, 1693441, 5],"float64"), Tensor([3, 1693441, 5],"float64"), axis=1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([3, 1693441, 5],"float64"), Tensor([3, 1693441, 5],"float64"), axis=1, ) 	 50803230 	 10704 	 75.19414353370667 	 6.429863214492798 	 1.7974755764007568 	 0.20413851737976074 	 12.648444652557373 	 6.447094202041626 	 0.40221524238586426 	 0.30731725692749023 	 combined
2025-08-05 08:55:23.091119 test begin: paddle.vecdot(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), axis=1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), axis=1, ) 	 50803224 	 10704 	 9.98133373260498 	 6.757227420806885 	 0.3171999454498291 	 0.32230138778686523 	 14.341430425643921 	 9.635817289352417 	 0.45753026008605957 	 0.4594275951385498 	 combined
2025-08-05 08:56:05.208255 test begin: paddle.view(Tensor([100, 10, 10, 50804],"float32"), list[-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 10, 10, 50804],"float32"), list[-1,], ) 	 508040000 	 750201 	 10.435137033462524 	 2.8971264362335205 	 0.00012683868408203125 	 9.202957153320312e-05 	 33.949012994766235 	 39.385785818099976 	 0.00010132789611816406 	 0.0002682209014892578 	 
2025-08-05 08:57:49.018667 test begin: paddle.view(Tensor([100, 10, 10, 50804],"float32"), list[10,100,-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 10, 10, 50804],"float32"), list[10,100,-1,], ) 	 508040000 	 750201 	 10.401753664016724 	 2.949373960494995 	 0.00011658668518066406 	 0.00012421607971191406 	 33.83486080169678 	 40.429152965545654 	 0.00010561943054199219 	 0.0002067089080810547 	 
2025-08-05 08:59:34.156378 test begin: paddle.view(Tensor([100, 10, 25402, 20],"float32"), list[-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 10, 25402, 20],"float32"), list[-1,], ) 	 508040000 	 750201 	 10.34327483177185 	 2.8997790813446045 	 0.00011706352233886719 	 0.0001239776611328125 	 33.679922103881836 	 39.74049258232117 	 0.00011658668518066406 	 0.000202178955078125 	 
2025-08-05 09:01:18.232944 test begin: paddle.view(Tensor([100, 10, 25402, 20],"float32"), list[10,100,-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 10, 25402, 20],"float32"), list[10,100,-1,], ) 	 508040000 	 750201 	 11.68913197517395 	 2.9605844020843506 	 0.00011920928955078125 	 9.059906005859375e-05 	 33.9436514377594 	 40.649025201797485 	 0.00010704994201660156 	 0.000213623046875 	 
2025-08-05 09:03:05.684315 test begin: paddle.view(Tensor([100, 25402, 10, 20],"float32"), list[-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 25402, 10, 20],"float32"), list[-1,], ) 	 508040000 	 750201 	 14.332189321517944 	 2.9561989307403564 	 0.00012636184692382812 	 0.00012874603271484375 	 33.54434108734131 	 39.95357632637024 	 0.00010156631469726562 	 0.00020813941955566406 	 
2025-08-05 09:04:54.856510 test begin: paddle.view(Tensor([100, 25402, 10, 20],"float32"), list[10,100,-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 25402, 10, 20],"float32"), list[10,100,-1,], ) 	 508040000 	 750201 	 10.53786563873291 	 2.9688663482666016 	 0.00010538101196289062 	 0.00012111663818359375 	 33.76619505882263 	 40.83178114891052 	 0.00010776519775390625 	 0.0002009868621826172 	 
2025-08-05 09:06:40.452801 test begin: paddle.view(Tensor([254020, 10, 10, 20],"float32"), list[-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([254020, 10, 10, 20],"float32"), list[-1,], ) 	 508040000 	 750201 	 10.348023176193237 	 2.958205461502075 	 0.0001068115234375 	 0.00013399124145507812 	 33.497955083847046 	 39.735973596572876 	 0.00010585784912109375 	 0.00020313262939453125 	 
2025-08-05 09:08:24.691390 test begin: paddle.view(Tensor([254020, 10, 10, 20],"float32"), list[10,100,-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([254020, 10, 10, 20],"float32"), list[10,100,-1,], ) 	 508040000 	 750201 	 10.471154689788818 	 3.0698585510253906 	 0.00011777877807617188 	 0.00012755393981933594 	 34.17460513114929 	 40.648226499557495 	 0.00010776519775390625 	 0.00022339820861816406 	 
2025-08-05 09:10:10.815257 test begin: paddle.view_as(Tensor([10, 10, 10, 50804],"float32"), Tensor([10, 100, 50804],"float32"), )
[Prof] paddle.view_as 	 paddle.view_as(Tensor([10, 10, 10, 50804],"float32"), Tensor([10, 100, 50804],"float32"), ) 	 101608000 	 711815 	 10.338459491729736 	 2.2477636337280273 	 0.00011944770812988281 	 8.368492126464844e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-05 09:10:58.283648 test begin: paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[-1,1,3,], )
W0805 09:11:27.927829 41190 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[-1,1,3,], ) 	 254016120 	 588277 	 19.203246355056763 	 5.717218399047852 	 0.00010919570922851562 	 0.0003135204315185547 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 09:11:37.410550 test begin: paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[-1,], )
W0805 09:11:54.623749 44029 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[-1,], ) 	 254016120 	 588277 	 10.21965479850769 	 4.157388687133789 	 0.00011730194091796875 	 0.0002570152282714844 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 09:12:00.415901 test begin: paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[2,4,], )
W0805 09:12:21.720135 45584 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[2,4,], ) 	 254016120 	 588277 	 14.398324012756348 	 4.841711759567261 	 9.942054748535156e-05 	 0.00023174285888671875 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 09:12:28.259975 test begin: paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[-1,1,3,], )
W0805 09:12:57.434990 47823 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[-1,1,3,], ) 	 254016180 	 588277 	 18.69860339164734 	 5.575054407119751 	 0.00011587142944335938 	 8.535385131835938e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 09:13:05.357821 test begin: paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[-1,], )
W0805 09:13:29.187855 50387 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[-1,], ) 	 254016180 	 588277 	 16.7355694770813 	 6.4493608474731445 	 0.0001285076141357422 	 0.00025963783264160156 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 09:13:39.665047 test begin: paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[2,4,], )
W0805 09:14:01.881971 53210 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[2,4,], ) 	 254016180 	 588277 	 14.530170917510986 	 4.8027729988098145 	 0.0001227855682373047 	 0.00023174285888671875 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 09:14:08.248292 test begin: paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[-1,1,3,], )
W0805 09:14:39.633085 55177 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[-1,1,3,], ) 	 254016240 	 588277 	 18.82511878013611 	 5.559279441833496 	 0.00011420249938964844 	 0.00026607513427734375 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 09:14:47.251465 test begin: paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[-1,], )
W0805 09:15:06.349258 58005 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[-1,], ) 	 254016240 	 588277 	 10.355470180511475 	 4.134933710098267 	 0.0001227855682373047 	 0.00026726722717285156 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 09:15:12.106704 test begin: paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[2,4,], )
W0805 09:15:33.494859 59932 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[2,4,], ) 	 254016240 	 588277 	 14.461422681808472 	 4.824008941650391 	 0.00011110305786132812 	 0.0002148151397705078 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 09:15:40.428923 test begin: paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], ) 	 76204872 	 32416 	 30.554102659225464 	 29.925822019577026 	 0.16035103797912598 	 0.942094087600708 	 30.65187692642212 	 2.2819230556488037 	 0.16081523895263672 	 7.557868957519531e-05 	 
2025-08-05 09:17:17.573502 test begin: paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], name=None, ) 	 76204872 	 32416 	 30.555163383483887 	 29.928020000457764 	 0.16037559509277344 	 0.942025899887085 	 30.65529179573059 	 2.4000470638275146 	 0.1608564853668213 	 7.843971252441406e-05 	 
2025-08-05 09:18:55.440460 test begin: paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),], ) 	 25401624 	 32416 	 10.19961166381836 	 10.174025297164917 	 0.16051173210144043 	 0.1599586009979248 	 9.995177268981934 	 1.755760669708252 	 0.15734457969665527 	 0.00011968612670898438 	 
2025-08-05 09:19:28.758965 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401880 	 32416 	 10.44214677810669 	 10.49715256690979 	 0.08237171173095703 	 0.3303346633911133 	 10.43220853805542 	 2.37539005279541 	 0.08222293853759766 	 7.605552673339844e-05 	 
2025-08-05 09:20:03.897984 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], name=None, ) 	 25401880 	 32416 	 10.45069694519043 	 10.488994121551514 	 0.08238744735717773 	 0.3302888870239258 	 10.428272247314453 	 2.245190143585205 	 0.08222031593322754 	 0.0001723766326904297 	 
2025-08-05 09:20:39.405948 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401880 	 32416 	 10.46354627609253 	 10.395506381988525 	 0.08220362663269043 	 0.3288388252258301 	 10.493370532989502 	 2.3330349922180176 	 0.08241605758666992 	 8.440017700195312e-05 	 
2025-08-05 09:21:14.268307 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], name=None, ) 	 25401880 	 32416 	 10.464949607849121 	 10.403696775436401 	 0.08219575881958008 	 0.32709193229675293 	 10.49073576927185 	 2.289125919342041 	 0.08242559432983398 	 9.584426879882812e-05 	 
2025-08-05 09:21:49.244303 test begin: paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 76204980 	 32416 	 30.843648195266724 	 29.826438903808594 	 0.1618490219116211 	 0.941483736038208 	 30.815788984298706 	 2.3038008213043213 	 0.1617274284362793 	 7.581710815429688e-05 	 
2025-08-05 09:23:26.640965 test begin: paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], name=None, ) 	 76204980 	 32416 	 30.834794998168945 	 29.8141610622406 	 0.16184186935424805 	 0.9399569034576416 	 30.827738046646118 	 2.3436083793640137 	 0.1617274284362793 	 7.653236389160156e-05 	 
2025-08-05 09:25:03.882681 test begin: paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401660 	 32416 	 10.2442786693573 	 10.162885904312134 	 0.16126418113708496 	 0.15996742248535156 	 10.22380781173706 	 1.7524714469909668 	 0.16093111038208008 	 0.0001666545867919922 	 
2025-08-05 09:25:39.728295 test begin: paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 76204890 	 32416 	 30.600932836532593 	 30.15135407447815 	 0.16186928749084473 	 0.9520764350891113 	 30.590889930725098 	 2.3342833518981934 	 0.1605064868927002 	 7.581710815429688e-05 	 
2025-08-05 09:27:16.915161 test begin: paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], name=None, ) 	 76204890 	 32416 	 30.594759941101074 	 30.148592948913574 	 0.16056275367736816 	 0.9535963535308838 	 30.590805530548096 	 2.265063762664795 	 0.1604907512664795 	 7.343292236328125e-05 	 
2025-08-05 09:28:54.134149 test begin: paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401630 	 32416 	 10.199507713317871 	 10.156922578811646 	 0.16055536270141602 	 0.15995502471923828 	 9.996630668640137 	 1.9809198379516602 	 0.15734291076660156 	 7.700920104980469e-05 	 
2025-08-05 09:29:27.704457 test begin: paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401880 	 32416 	 10.412983655929565 	 10.091976881027222 	 0.08182835578918457 	 0.3192138671875 	 10.37227177619934 	 2.2815799713134766 	 0.0814824104309082 	 7.939338684082031e-05 	 
2025-08-05 09:30:02.025591 test begin: paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], name=None, ) 	 25401880 	 32416 	 10.413665294647217 	 10.563734769821167 	 0.08180379867553711 	 0.3177037239074707 	 10.371621370315552 	 2.3389711380004883 	 0.08146214485168457 	 9.942054748535156e-05 	 
2025-08-05 09:30:39.335768 test begin: paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 76204920 	 32416 	 30.74622917175293 	 29.935216665267944 	 0.16278433799743652 	 0.9437310695648193 	 30.723385334014893 	 2.317091941833496 	 0.16120433807373047 	 7.677078247070312e-05 	 
2025-08-05 09:32:16.591151 test begin: paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], name=None, ) 	 76204920 	 32416 	 30.741872787475586 	 29.925359964370728 	 0.16135358810424805 	 0.9423348903656006 	 30.73527693748474 	 2.2970292568206787 	 0.1612107753753662 	 8.082389831542969e-05 	 
2025-08-05 09:33:53.767223 test begin: paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401640 	 32416 	 10.245698928833008 	 10.16438603401184 	 0.16125059127807617 	 0.15996956825256348 	 10.222395181655884 	 1.7496552467346191 	 0.16090750694274902 	 7.700920104980469e-05 	 
2025-08-05 09:34:27.304671 test begin: paddle.where(Tensor([1, 400, 127009],"bool"), Tensor([1, 400, 127009],"float32"), Tensor([1, 400, 127009],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([1, 400, 127009],"bool"), Tensor([1, 400, 127009],"float32"), Tensor([1, 400, 127009],"float32"), ) 	 152410800 	 20615 	 10.006391286849976 	 9.998862266540527 	 0.49530696868896484 	 0.4948306083679199 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:35:01.567709 test begin: paddle.where(Tensor([1, 400, 65856],"bool"), Tensor([1, 400, 65856],"float32"), Tensor([2, 400, 65856],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([1, 400, 65856],"bool"), Tensor([1, 400, 65856],"float32"), Tensor([2, 400, 65856],"float32"), ) 	 105369600 	 20615 	 19.304837226867676 	 10.664488554000854 	 0.3187081813812256 	 0.5301268100738525 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:35:51.791639 test begin: paddle.where(Tensor([1, 400, 65856],"bool"), Tensor([2, 400, 65856],"float32"), Tensor([1, 400, 65856],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([1, 400, 65856],"bool"), Tensor([2, 400, 65856],"float32"), Tensor([1, 400, 65856],"float32"), ) 	 105369600 	 20615 	 19.300458669662476 	 10.655622720718384 	 0.31874918937683105 	 0.5290656089782715 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:36:42.153431 test begin: paddle.where(Tensor([1, 772, 65856],"bool"), Tensor([1, 772, 65856],"float32"), Tensor([1, 772, 65856],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([1, 772, 65856],"bool"), Tensor([1, 772, 65856],"float32"), Tensor([1, 772, 65856],"float32"), ) 	 152522496 	 20615 	 10.01733684539795 	 11.290839433670044 	 0.4959242343902588 	 0.4947357177734375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:37:18.302604 test begin: paddle.where(Tensor([2, 400, 65856],"bool"), Tensor([1, 400, 65856],"float32"), Tensor([1, 400, 65856],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([2, 400, 65856],"bool"), Tensor([1, 400, 65856],"float32"), Tensor([1, 400, 65856],"float32"), ) 	 105369600 	 20615 	 23.025810480117798 	 10.651997327804565 	 0.38300609588623047 	 0.5304343700408936 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:38:19.030872 test begin: paddle.where(Tensor([2, 400, 65856],"bool"), Tensor([2, 400, 65856],"float32"), Tensor([2, 400, 65856],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([2, 400, 65856],"bool"), Tensor([2, 400, 65856],"float32"), Tensor([2, 400, 65856],"float32"), ) 	 158054400 	 20615 	 10.381070613861084 	 10.325021266937256 	 0.5142922401428223 	 0.5111815929412842 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:38:54.493744 test begin: paddle.where(Tensor([4, 125, 320, 320],"bool"), Tensor([4, 125, 320, 320],"float32"), Tensor([4, 125, 320, 320],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 125, 320, 320],"bool"), Tensor([4, 125, 320, 320],"float32"), Tensor([4, 125, 320, 320],"float32"), ) 	 153600000 	 20615 	 10.086987018585205 	 10.044127464294434 	 0.4992353916168213 	 0.49806761741638184 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:39:30.147263 test begin: paddle.where(Tensor([4, 280, 376, 25, 5],"bool"), Tensor([4, 280, 376, 25, 5],"float32"), Tensor([4, 280, 376, 25, 5],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 280, 376, 25, 5],"bool"), Tensor([4, 280, 376, 25, 5],"float32"), Tensor([4, 280, 376, 25, 5],"float32"), ) 	 157920000 	 20615 	 10.364636659622192 	 10.315794467926025 	 0.5132224559783936 	 0.510735273361206 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:40:05.496698 test begin: paddle.where(Tensor([4, 280, 376, 41, 3],"bool"), Tensor([4, 280, 376, 41, 3],"float32"), Tensor([4, 280, 376, 41, 3],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 280, 376, 41, 3],"bool"), Tensor([4, 280, 376, 41, 3],"float32"), Tensor([4, 280, 376, 41, 3],"float32"), ) 	 155393280 	 20615 	 10.196443796157837 	 10.16966700553894 	 0.5047924518585205 	 0.5034215450286865 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:40:40.574290 test begin: paddle.where(Tensor([4, 280, 605, 25, 3],"bool"), Tensor([4, 280, 605, 25, 3],"float32"), Tensor([4, 280, 605, 25, 3],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 280, 605, 25, 3],"bool"), Tensor([4, 280, 605, 25, 3],"float32"), Tensor([4, 280, 605, 25, 3],"float32"), ) 	 152460000 	 20615 	 10.01020884513855 	 9.999810457229614 	 0.49547624588012695 	 0.4978907108306885 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:41:14.719542 test begin: paddle.where(Tensor([4, 451, 376, 25, 3],"bool"), Tensor([4, 451, 376, 25, 3],"float32"), Tensor([4, 451, 376, 25, 3],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 451, 376, 25, 3],"bool"), Tensor([4, 451, 376, 25, 3],"float32"), Tensor([4, 451, 376, 25, 3],"float32"), ) 	 152618400 	 20615 	 10.024480819702148 	 9.986513376235962 	 0.49711155891418457 	 0.4955563545227051 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:41:50.415367 test begin: paddle.where(Tensor([4, 64, 320, 621],"bool"), Tensor([4, 64, 320, 621],"float32"), Tensor([4, 64, 320, 621],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 64, 320, 621],"bool"), Tensor([4, 64, 320, 621],"float32"), Tensor([4, 64, 320, 621],"float32"), ) 	 152616960 	 20615 	 10.023371934890747 	 9.976656198501587 	 0.49599289894104004 	 0.4949522018432617 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:42:24.597662 test begin: paddle.where(Tensor([4, 64, 621, 320],"bool"), Tensor([4, 64, 621, 320],"float32"), Tensor([4, 64, 621, 320],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 64, 621, 320],"bool"), Tensor([4, 64, 621, 320],"float32"), Tensor([4, 64, 621, 320],"float32"), ) 	 152616960 	 20615 	 10.019641637802124 	 9.974239349365234 	 0.49681591987609863 	 0.4936809539794922 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:42:58.700483 test begin: paddle.where(Tensor([7, 280, 376, 25, 3],"bool"), Tensor([7, 280, 376, 25, 3],"float32"), Tensor([7, 280, 376, 25, 3],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([7, 280, 376, 25, 3],"bool"), Tensor([7, 280, 376, 25, 3],"float32"), Tensor([7, 280, 376, 25, 3],"float32"), ) 	 165816000 	 20615 	 10.878443241119385 	 10.834694385528564 	 0.5388000011444092 	 0.53633713722229 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:43:39.672844 test begin: paddle.where(Tensor([8, 64, 320, 320],"bool"), Tensor([8, 64, 320, 320],"float32"), Tensor([8, 64, 320, 320],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([8, 64, 320, 320],"bool"), Tensor([8, 64, 320, 320],"float32"), Tensor([8, 64, 320, 320],"float32"), ) 	 157286400 	 20615 	 10.326733827590942 	 10.273325204849243 	 0.5114455223083496 	 0.5085752010345459 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 09:44:14.815664 test begin: paddle.zeros_like(Tensor([16, 64, 320, 320],"float16"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([16, 64, 320, 320],"float16"), ) 	 104857600 	 74631 	 10.323512077331543 	 10.326339483261108 	 0.1410963535308838 	 0.14211559295654297 	 None 	 None 	 None 	 None 	 
2025-08-05 09:44:39.940571 test begin: paddle.zeros_like(Tensor([4, 1051, 12096],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 1051, 12096],"float32"), ) 	 50851584 	 74631 	 10.026754379272461 	 10.042564630508423 	 0.13704824447631836 	 0.1370551586151123 	 None 	 None 	 None 	 None 	 
2025-08-05 09:45:00.945798 test begin: paddle.zeros_like(Tensor([4, 125, 320, 320],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 125, 320, 320],"float32"), ) 	 51200000 	 74631 	 10.089300632476807 	 10.097483396530151 	 0.13787269592285156 	 0.137986421585083 	 None 	 None 	 None 	 None 	 
2025-08-05 09:45:22.013339 test begin: paddle.zeros_like(Tensor([4, 249, 320, 320],"float16"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 249, 320, 320],"float16"), ) 	 101990400 	 74631 	 10.046838760375977 	 10.058398008346558 	 0.13728046417236328 	 0.13732409477233887 	 None 	 None 	 None 	 None 	 
2025-08-05 09:45:44.085545 test begin: paddle.zeros_like(Tensor([4, 525, 24193],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 525, 24193],"float32"), ) 	 50805300 	 74631 	 10.020718336105347 	 10.029069900512695 	 0.1390073299407959 	 0.13833832740783691 	 None 	 None 	 None 	 None 	 
2025-08-05 09:46:05.044478 test begin: paddle.zeros_like(Tensor([4, 64, 1241, 320],"float16"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 64, 1241, 320],"float16"), ) 	 101662720 	 74631 	 10.014242172241211 	 10.015548944473267 	 0.1368117332458496 	 0.13687515258789062 	 None 	 None 	 None 	 None 	 
2025-08-05 09:46:27.136072 test begin: paddle.zeros_like(Tensor([4, 64, 320, 1241],"float16"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 64, 320, 1241],"float16"), ) 	 101662720 	 74631 	 10.011573076248169 	 10.032734870910645 	 0.1368422508239746 	 0.13700461387634277 	 None 	 None 	 None 	 None 	 
2025-08-05 09:46:50.297324 test begin: paddle.zeros_like(Tensor([4, 64, 320, 621],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 64, 320, 621],"float32"), ) 	 50872320 	 74631 	 10.026437044143677 	 10.04257583618164 	 0.1370677947998047 	 0.13842391967773438 	 None 	 None 	 None 	 None 	 
2025-08-05 09:47:11.267945 test begin: paddle.zeros_like(Tensor([4, 64, 621, 320],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 64, 621, 320],"float32"), ) 	 50872320 	 74631 	 10.026110887527466 	 10.040267705917358 	 0.13703298568725586 	 0.13718438148498535 	 None 	 None 	 None 	 None 	 
2025-08-05 09:47:32.205150 test begin: paddle.zeros_like(Tensor([8, 64, 320, 320],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([8, 64, 320, 320],"float32"), ) 	 52428800 	 74631 	 10.335600852966309 	 10.329726219177246 	 0.1411440372467041 	 0.141188383102417 	 None 	 None 	 None 	 None 	 
2025-08-05 09:47:53.760683 test begin: paddle.zeros_like(Tensor([9, 525, 12096],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([9, 525, 12096],"float32"), ) 	 57153600 	 74631 	 11.240318536758423 	 11.238755464553833 	 0.15492796897888184 	 0.15388703346252441 	 None 	 None 	 None 	 None 	 
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 9, in <module>
    from tester import (APIConfig, APITestAccuracy, APITestCINNVSDygraph,
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/__init__.py", line 74, in __getattr__
    from .api_config import APIConfig
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/__init__.py", line 22, in __getattr__
    from .config_analyzer import APIConfig
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py", line 9, in <module>
    import torch
  File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 409, in <module>
    from torch._C import *  # noqa: F403
  File "<frozen importlib._bootstrap>", line 216, in _lock_unlock_module
KeyboardInterrupt
