2025-08-05 21:37:43.957068 test begin: paddle.abs(Tensor([13, 64, 256, 256],"float32"), )
W0805 21:37:45.683583 108268 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.abs 	 paddle.abs(Tensor([13, 64, 256, 256],"float32"), ) 	 54525952 	 33746 	 10.738804340362549 	 10.767352104187012 	 0.32424211502075195 	 0.32596659660339355 	 16.297902584075928 	 26.886366605758667 	 0.4935135841369629 	 0.4070758819580078 	 
2025-08-05 21:38:51.800172 test begin: paddle.abs(Tensor([16, 128, 128, 194],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 128, 128, 194],"float32"), ) 	 50855936 	 33746 	 9.999807834625244 	 10.052241563796997 	 0.3028721809387207 	 0.304462194442749 	 15.207555770874023 	 25.089242935180664 	 0.46052026748657227 	 0.37986159324645996 	 
2025-08-05 21:39:54.148201 test begin: paddle.abs(Tensor([16, 128, 194, 128],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 128, 194, 128],"float32"), ) 	 50855936 	 33746 	 10.011366605758667 	 10.05698037147522 	 0.3028233051300049 	 0.3045320510864258 	 15.206740856170654 	 25.089460134506226 	 0.4605395793914795 	 0.37992119789123535 	 
2025-08-05 21:40:56.309173 test begin: paddle.abs(Tensor([16, 194, 128, 128],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 194, 128, 128],"float32"), ) 	 50855936 	 33746 	 10.002984762191772 	 10.065391778945923 	 0.30285024642944336 	 0.30452895164489746 	 15.206391096115112 	 25.088958978652954 	 0.4605844020843506 	 0.37987709045410156 	 
2025-08-05 21:41:59.132122 test begin: paddle.abs(Tensor([16, 256, 194, 64],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 256, 194, 64],"float32"), ) 	 50855936 	 33746 	 10.00010061264038 	 10.052357912063599 	 0.30290913581848145 	 0.30443310737609863 	 15.206439971923828 	 25.088325262069702 	 0.4605691432952881 	 0.3798675537109375 	 
2025-08-05 21:43:02.108922 test begin: paddle.abs(Tensor([16, 256, 64, 194],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 256, 64, 194],"float32"), ) 	 50855936 	 33746 	 9.99986720085144 	 10.072639226913452 	 0.30286288261413574 	 0.30441951751708984 	 15.205692052841187 	 25.087936878204346 	 0.4605906009674072 	 0.3799002170562744 	 
2025-08-05 21:44:07.940218 test begin: paddle.abs(Tensor([16, 49, 256, 256],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 49, 256, 256],"float32"), ) 	 51380224 	 33746 	 10.098885774612427 	 10.17147970199585 	 0.30581164360046387 	 0.3074920177459717 	 15.361275911331177 	 25.344128131866455 	 0.4651975631713867 	 0.3837714195251465 	 
2025-08-05 21:45:12.808859 test begin: paddle.abs(Tensor([16, 64, 194, 256],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 64, 194, 256],"float32"), ) 	 50855936 	 33746 	 9.999759435653687 	 11.5245840549469 	 0.3028755187988281 	 0.30440592765808105 	 15.20566725730896 	 25.087749004364014 	 0.4605569839477539 	 0.37988924980163574 	 
2025-08-05 21:46:18.224314 test begin: paddle.abs(Tensor([16, 64, 256, 194],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 64, 256, 194],"float32"), ) 	 50855936 	 33746 	 10.00110411643982 	 10.060298204421997 	 0.30286478996276855 	 0.30443787574768066 	 15.205891847610474 	 25.087265014648438 	 0.4603846073150635 	 0.3798532485961914 	 
2025-08-05 21:47:20.494657 test begin: paddle.abs(Tensor([16, 776, 64, 64],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 776, 64, 64],"float32"), ) 	 50855936 	 33746 	 9.999864101409912 	 10.052675724029541 	 0.30291223526000977 	 0.30446577072143555 	 15.205895900726318 	 25.089134693145752 	 0.46048927307128906 	 0.3798859119415283 	 
2025-08-05 21:48:22.539049 test begin: paddle.abs(Tensor([25, 128, 128, 128],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([25, 128, 128, 128],"float32"), ) 	 52428800 	 33746 	 10.30747389793396 	 10.360023498535156 	 0.3121657371520996 	 0.3137638568878174 	 15.66099214553833 	 25.861900329589844 	 0.474376916885376 	 0.39162206649780273 	 
2025-08-05 21:49:28.584088 test begin: paddle.abs(Tensor([49, 256, 64, 64],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([49, 256, 64, 64],"float32"), ) 	 51380224 	 33746 	 10.099268674850464 	 10.156098127365112 	 0.3058905601501465 	 0.30751919746398926 	 15.360835552215576 	 25.34538769721985 	 0.4650905132293701 	 0.38376355171203613 	 
2025-08-05 21:50:32.669546 test begin: paddle.acos(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.acos 	 paddle.acos(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33801 	 11.612529754638672 	 10.05629301071167 	 0.3020756244659424 	 0.304093599319458 	 15.21690583229065 	 70.32420372962952 	 0.46006011962890625 	 0.3543717861175537 	 
2025-08-05 21:52:22.406331 test begin: paddle.acos(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.acos 	 paddle.acos(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33801 	 9.993758201599121 	 10.056453943252563 	 0.30218505859375 	 0.3040132522583008 	 15.216370820999146 	 70.32591319084167 	 0.4602372646331787 	 0.3544590473175049 	 
2025-08-05 21:54:10.001175 test begin: paddle.acos(Tensor([10, 5080321],"float32"), )
[Prof] paddle.acos 	 paddle.acos(Tensor([10, 5080321],"float32"), ) 	 50803210 	 33801 	 9.993266105651855 	 10.474336862564087 	 0.3021693229675293 	 0.3040597438812256 	 15.217233896255493 	 70.32631421089172 	 0.4601931571960449 	 0.35441112518310547 	 
2025-08-05 21:55:59.733131 test begin: paddle.acos(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.acos 	 paddle.acos(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33801 	 9.993928909301758 	 10.073233604431152 	 0.30212998390197754 	 0.3040623664855957 	 15.216461658477783 	 70.32497215270996 	 0.4600965976715088 	 0.3544299602508545 	 
2025-08-05 21:57:49.824112 test begin: paddle.acos(Tensor([5080321, 10],"float32"), )
[Prof] paddle.acos 	 paddle.acos(Tensor([5080321, 10],"float32"), ) 	 50803210 	 33801 	 9.993902444839478 	 10.055768728256226 	 0.302203893661499 	 0.3040506839752197 	 15.217439651489258 	 70.32478213310242 	 0.45997166633605957 	 0.3544578552246094 	 
2025-08-05 21:59:40.412906 test begin: paddle.acos(x=Tensor([3, 3, 5644801],"float32"), )
[Prof] paddle.acos 	 paddle.acos(x=Tensor([3, 3, 5644801],"float32"), ) 	 50803209 	 33801 	 11.296716213226318 	 10.055805444717407 	 0.3021426200866699 	 0.30417752265930176 	 15.216999769210815 	 70.32418870925903 	 0.4601783752441406 	 0.3543996810913086 	 
2025-08-05 22:01:30.358258 test begin: paddle.acos(x=Tensor([3, 5644801, 3],"float32"), )
[Prof] paddle.acos 	 paddle.acos(x=Tensor([3, 5644801, 3],"float32"), ) 	 50803209 	 33801 	 9.992850065231323 	 10.057260513305664 	 0.30218005180358887 	 0.30524182319641113 	 15.216678380966187 	 70.32329487800598 	 0.45995044708251953 	 0.3543996810913086 	 
2025-08-05 22:03:21.464950 test begin: paddle.acos(x=Tensor([5644801, 3, 3],"float32"), )
[Prof] paddle.acos 	 paddle.acos(x=Tensor([5644801, 3, 3],"float32"), ) 	 50803209 	 33801 	 10.885043382644653 	 10.055979251861572 	 0.30220794677734375 	 0.30405187606811523 	 15.217490673065186 	 70.32470655441284 	 0.46013832092285156 	 0.35440754890441895 	 
2025-08-05 22:05:10.211793 test begin: paddle.acosh(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.acosh 	 paddle.acosh(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33859 	 9.995567798614502 	 10.124850749969482 	 0.30165672302246094 	 0.3052985668182373 	 15.297269344329834 	 45.30036735534668 	 0.4617431163787842 	 0.3418738842010498 	 
2025-08-05 22:06:33.881892 test begin: paddle.acosh(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.acosh 	 paddle.acosh(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33859 	 9.994988918304443 	 10.122912406921387 	 0.30171728134155273 	 0.3053586483001709 	 15.292150735855103 	 45.29962730407715 	 0.4617140293121338 	 0.3419461250305176 	 
2025-08-05 22:07:59.169373 test begin: paddle.acosh(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.acosh 	 paddle.acosh(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33859 	 10.73302173614502 	 10.115786790847778 	 0.30172228813171387 	 0.3053462505340576 	 15.292604207992554 	 45.29952621459961 	 0.46166539192199707 	 0.3419075012207031 	 
2025-08-05 22:09:24.004040 test begin: paddle.add(x=Tensor([2, 256, 320, 352],"float32"), y=Tensor([2, 256, 320, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([2, 256, 320, 352],"float32"), y=Tensor([2, 256, 320, 352],"float32"), ) 	 115343360 	 22147 	 11.308122873306274 	 11.209631204605103 	 0.521841287612915 	 0.517289400100708 	 12.126830101013184 	 1.5337464809417725 	 0.5596234798431396 	 7.843971252441406e-05 	 
2025-08-05 22:10:03.964471 test begin: paddle.add(x=Tensor([2, 256, 336, 336],"float32"), y=Tensor([2, 256, 336, 336],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([2, 256, 336, 336],"float32"), y=Tensor([2, 256, 336, 336],"float32"), ) 	 115605504 	 22147 	 11.334029197692871 	 11.237823724746704 	 0.5230188369750977 	 0.5186066627502441 	 12.155713081359863 	 1.6649978160858154 	 0.5609233379364014 	 7.653236389160156e-05 	 
2025-08-05 22:10:44.907786 test begin: paddle.add(x=Tensor([2, 256, 352, 352],"float32"), y=Tensor([2, 256, 352, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([2, 256, 352, 352],"float32"), y=Tensor([2, 256, 352, 352],"float32"), ) 	 126877696 	 22147 	 12.425109386444092 	 12.322807312011719 	 0.5732932090759277 	 0.5686962604522705 	 13.333773136138916 	 1.2997486591339111 	 0.6153409481048584 	 7.867813110351562e-05 	 
2025-08-05 22:11:27.562834 test begin: paddle.add(x=Tensor([8, 256, 320, 78],"float32"), y=Tensor([8, 256, 320, 78],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 320, 78],"float32"), y=Tensor([8, 256, 320, 78],"float32"), ) 	 102236160 	 22147 	 10.033709049224854 	 9.947030782699585 	 0.4630413055419922 	 0.4590020179748535 	 10.756814241409302 	 1.5431511402130127 	 0.49637317657470703 	 9.751319885253906e-05 	 
2025-08-05 22:12:04.006144 test begin: paddle.add(x=Tensor([8, 256, 336, 74],"float32"), y=Tensor([8, 256, 336, 74],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 336, 74],"float32"), y=Tensor([8, 256, 336, 74],"float32"), ) 	 101842944 	 22147 	 9.998831510543823 	 9.909496784210205 	 0.46123671531677246 	 0.4572310447692871 	 10.716658592224121 	 1.5568265914916992 	 0.49443984031677246 	 0.00010251998901367188 	 
2025-08-05 22:12:41.008248 test begin: paddle.add(x=Tensor([8, 256, 352, 71],"float32"), y=Tensor([8, 256, 352, 71],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 352, 71],"float32"), y=Tensor([8, 256, 352, 71],"float32"), ) 	 102367232 	 22147 	 10.050064325332642 	 9.959416389465332 	 0.4635353088378906 	 0.45964527130126953 	 10.76727843284607 	 1.5433571338653564 	 0.49684858322143555 	 7.82012939453125e-05 	 
2025-08-05 22:13:18.181300 test begin: paddle.add(x=Tensor([8, 256, 71, 352],"float32"), y=Tensor([8, 256, 71, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 71, 352],"float32"), y=Tensor([8, 256, 71, 352],"float32"), ) 	 102367232 	 22147 	 10.045832395553589 	 9.97239637374878 	 0.4635934829711914 	 0.45955991744995117 	 10.768859148025513 	 1.215378999710083 	 0.496936559677124 	 8.869171142578125e-05 	 
2025-08-05 22:13:55.314183 test begin: paddle.add(x=Tensor([8, 256, 74, 336],"float32"), y=Tensor([8, 256, 74, 336],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 74, 336],"float32"), y=Tensor([8, 256, 74, 336],"float32"), ) 	 101842944 	 22147 	 10.004983186721802 	 9.909415245056152 	 0.46118855476379395 	 0.45728063583374023 	 10.713807106018066 	 1.3365089893341064 	 0.49447011947631836 	 9.417533874511719e-05 	 
2025-08-05 22:14:30.077976 test begin: paddle.add(x=Tensor([8, 52, 352, 352],"float32"), y=Tensor([8, 52, 352, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 52, 352, 352],"float32"), y=Tensor([8, 52, 352, 352],"float32"), ) 	 103088128 	 22147 	 10.1175057888031 	 10.043228149414062 	 0.46685242652893066 	 0.4628293514251709 	 10.847570419311523 	 1.5427923202514648 	 0.5006163120269775 	 7.891654968261719e-05 	 
2025-08-05 22:15:06.079009 test begin: paddle.add(x=Tensor([8, 57, 320, 352],"float32"), y=Tensor([8, 57, 320, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 57, 320, 352],"float32"), y=Tensor([8, 57, 320, 352],"float32"), ) 	 102727680 	 22147 	 10.083566427230835 	 9.993710279464722 	 0.4654068946838379 	 0.46117663383483887 	 10.804526805877686 	 1.5689327716827393 	 0.4985945224761963 	 8.034706115722656e-05 	 
2025-08-05 22:15:44.829264 test begin: paddle.add(x=Tensor([8, 57, 336, 336],"float32"), y=Tensor([8, 57, 336, 336],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 57, 336, 336],"float32"), y=Tensor([8, 57, 336, 336],"float32"), ) 	 102961152 	 22147 	 10.106112957000732 	 10.024099588394165 	 0.46631765365600586 	 0.46230530738830566 	 10.83332109451294 	 1.2885000705718994 	 0.499950647354126 	 9.131431579589844e-05 	 
2025-08-05 22:16:22.195145 test begin: paddle.add_n(list[Tensor([194, 128, 64, 64],"float16"),Tensor([194, 128, 64, 64],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([194, 128, 64, 64],"float16"),Tensor([194, 128, 64, 64],"float16"),], ) 	 203423744 	 21148 	 12.102136611938477 	 33.3978807926178 	 0.5849740505218506 	 0.7757060527801514 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:17:15.731843 test begin: paddle.add_n(list[Tensor([388, 256, 32, 32],"float16"),Tensor([388, 256, 32, 32],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([388, 256, 32, 32],"float16"),Tensor([388, 256, 32, 32],"float16"),], ) 	 203423744 	 21148 	 12.121325969696045 	 32.141112089157104 	 0.5857579708099365 	 0.776700496673584 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:18:06.398923 test begin: paddle.add_n(list[Tensor([64, 128, 194, 64],"float16"),Tensor([64, 128, 194, 64],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 128, 194, 64],"float16"),Tensor([64, 128, 194, 64],"float16"),], ) 	 203423744 	 21148 	 12.147584438323975 	 32.14476418495178 	 0.58705735206604 	 0.7760305404663086 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:18:58.773769 test begin: paddle.add_n(list[Tensor([64, 128, 64, 194],"float16"),Tensor([64, 128, 64, 194],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 128, 64, 194],"float16"),Tensor([64, 128, 64, 194],"float16"),], ) 	 203423744 	 21148 	 12.11854076385498 	 32.11189842224121 	 0.5855739116668701 	 0.7760076522827148 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:19:46.883206 test begin: paddle.add_n(list[Tensor([64, 128, 64, 97],"float32"),Tensor([64, 128, 64, 97],"float32"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 128, 64, 97],"float32"),Tensor([64, 128, 64, 97],"float32"),], ) 	 101711872 	 21148 	 10.018256425857544 	 22.386544227600098 	 0.48349785804748535 	 0.5409455299377441 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:20:21.075663 test begin: paddle.add_n(list[Tensor([64, 128, 97, 64],"float32"),Tensor([64, 128, 97, 64],"float32"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 128, 97, 64],"float32"),Tensor([64, 128, 97, 64],"float32"),], ) 	 101711872 	 21148 	 10.007749080657959 	 22.3864426612854 	 0.483614444732666 	 0.5409319400787354 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:20:55.295472 test begin: paddle.add_n(list[Tensor([64, 1551, 32, 32],"float16"),Tensor([64, 1551, 32, 32],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 1551, 32, 32],"float16"),Tensor([64, 1551, 32, 32],"float16"),], ) 	 203292672 	 21148 	 12.093246936798096 	 32.093228816986084 	 0.5845062732696533 	 0.7754521369934082 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:21:45.198630 test begin: paddle.add_n(list[Tensor([64, 194, 64, 64],"float32"),Tensor([64, 194, 64, 64],"float32"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 194, 64, 64],"float32"),Tensor([64, 194, 64, 64],"float32"),], ) 	 101711872 	 21148 	 10.007381916046143 	 22.39316964149475 	 0.4836304187774658 	 0.5409965515136719 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:22:21.680291 test begin: paddle.add_n(list[Tensor([64, 256, 194, 32],"float16"),Tensor([64, 256, 194, 32],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 256, 194, 32],"float16"),Tensor([64, 256, 194, 32],"float16"),], ) 	 203423744 	 21148 	 12.146955251693726 	 32.13675045967102 	 0.5869617462158203 	 0.7764973640441895 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:23:14.109728 test begin: paddle.add_n(list[Tensor([64, 256, 32, 194],"float16"),Tensor([64, 256, 32, 194],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 256, 32, 194],"float16"),Tensor([64, 256, 32, 194],"float16"),], ) 	 203423744 	 21148 	 12.118496417999268 	 32.10891246795654 	 0.5856037139892578 	 0.7758753299713135 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:24:02.226359 test begin: paddle.add_n(list[Tensor([64, 388, 64, 64],"float16"),Tensor([64, 388, 64, 64],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 388, 64, 64],"float16"),Tensor([64, 388, 64, 64],"float16"),], ) 	 203423744 	 21148 	 12.14659070968628 	 33.73166227340698 	 0.5870242118835449 	 0.7759971618652344 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:24:54.018913 test begin: paddle.add_n(list[Tensor([97, 128, 64, 64],"float32"),Tensor([97, 128, 64, 64],"float32"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([97, 128, 64, 64],"float32"),Tensor([97, 128, 64, 64],"float32"),], ) 	 101711872 	 21148 	 10.007453441619873 	 22.38615894317627 	 0.4836709499359131 	 0.5408563613891602 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:25:30.708143 test begin: paddle.addmm(Tensor([1016065, 50],"float32"), Tensor([1016065, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([1016065, 50],"float32"), Tensor([1016065, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, ) 	 132092450 	 31761 	 32.42866563796997 	 31.536810159683228 	 0.5217244625091553 	 0.33757758140563965 	 85.02813148498535 	 60.243119955062866 	 0.39083313941955566 	 0.48349881172180176 	 
2025-08-05 22:29:03.326404 test begin: paddle.addmm(Tensor([30, 1693441],"float32"), Tensor([30, 80],"float32"), Tensor([80, 1693441],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([30, 1693441],"float32"), Tensor([30, 80],"float32"), Tensor([80, 1693441],"float32"), alpha=1.0, beta=2.0, ) 	 186280910 	 31761 	 37.813233375549316 	 36.58475375175476 	 0.40561985969543457 	 0.2945699691772461 	 102.16787576675415 	 67.19971251487732 	 0.4111318588256836 	 0.43048810958862305 	 
2025-08-05 22:33:11.042755 test begin: paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 1016065],"float32"), Tensor([1016065, 50],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 1016065],"float32"), Tensor([1016065, 50],"float32"), alpha=1.0, beta=2.0, ) 	 81286700 	 31761 	 10.096900224685669 	 9.965566635131836 	 0.10828852653503418 	 0.10718488693237305 	 34.82912993431091 	 20.202811002731323 	 0.18700098991394043 	 0.21585297584533691 	 
2025-08-05 22:34:27.502605 test begin: paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 1693441],"float32"), Tensor([1693441, 50],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 1693441],"float32"), Tensor([1693441, 50],"float32"), alpha=1.0, beta=2.0, ) 	 135476780 	 31761 	 16.09885811805725 	 16.09538173675537 	 0.1717967987060547 	 0.17179512977600098 	 58.00524592399597 	 33.812846422195435 	 0.23364996910095215 	 0.2172255516052246 	 
2025-08-05 22:36:38.223331 test begin: paddle.addmm(Tensor([30, 635041],"float32"), Tensor([30, 80],"float32"), Tensor([80, 635041],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([30, 635041],"float32"), Tensor([30, 80],"float32"), Tensor([80, 635041],"float32"), alpha=1.0, beta=2.0, ) 	 69856910 	 31761 	 13.575807571411133 	 13.256337404251099 	 0.21837282180786133 	 0.14220046997070312 	 38.75794076919556 	 25.175164222717285 	 0.178208589553833 	 0.2027733325958252 	 
2025-08-05 22:38:14.218739 test begin: paddle.addmm(Tensor([635041, 50],"float32"), Tensor([635041, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([635041, 50],"float32"), Tensor([635041, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, ) 	 82559330 	 31761 	 20.64570426940918 	 20.051528453826904 	 0.33246541023254395 	 0.21475720405578613 	 53.74712824821472 	 37.736762285232544 	 0.2470567226409912 	 0.30162596702575684 	 
2025-08-05 22:40:34.165270 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 5080321],"float64"), y=Tensor([5080321, 5],"float64"), )
[Prof] paddle.addmm 	 paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 5080321],"float64"), y=Tensor([5080321, 5],"float64"), ) 	 50803235 	 31761 	 19.686351776123047 	 19.729389190673828 	 0.21137189865112305 	 0.21161818504333496 	 95.50680899620056 	 80.41740489006042 	 0.21954703330993652 	 0.25875020027160645 	 
2025-08-05 22:44:12.487458 test begin: paddle.all(Tensor([423361, 6, 10],"float64"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([423361, 6, 10],"float64"), None, False, None, ) 	 25401660 	 49878 	 10.013907194137573 	 7.49597954750061 	 0.06828761100769043 	 0.07680892944335938 	 None 	 None 	 None 	 None 	 
2025-08-05 22:44:30.672107 test begin: paddle.all(Tensor([5, 508033, 10],"float64"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([5, 508033, 10],"float64"), None, False, None, ) 	 25401650 	 49878 	 10.01181936264038 	 7.494163274765015 	 0.06825900077819824 	 0.07676005363464355 	 None 	 None 	 None 	 None 	 
2025-08-05 22:44:50.944780 test begin: paddle.all(Tensor([5, 6, 846721],"float64"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([5, 6, 846721],"float64"), None, False, None, ) 	 25401630 	 49878 	 10.085239887237549 	 7.4952802658081055 	 0.06878209114074707 	 0.07684779167175293 	 None 	 None 	 None 	 None 	 
2025-08-05 22:45:10.862200 test begin: paddle.all(Tensor([50, 1016065, 10],"bool"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([50, 1016065, 10],"bool"), None, False, None, ) 	 508032500 	 49878 	 23.15453314781189 	 25.305479764938354 	 0.23721814155578613 	 0.2592430114746094 	 None 	 None 	 None 	 None 	 
2025-08-05 22:46:07.722585 test begin: paddle.all(Tensor([50, 6, 1693441],"bool"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([50, 6, 1693441],"bool"), None, False, None, ) 	 508032300 	 49878 	 23.154176473617554 	 25.305931329727173 	 0.23717594146728516 	 0.2592482566833496 	 None 	 None 	 None 	 None 	 
2025-08-05 22:47:04.035080 test begin: paddle.all(Tensor([508032010],"bool"), )
[Prof] paddle.all 	 paddle.all(Tensor([508032010],"bool"), ) 	 508032010 	 49878 	 23.295621156692505 	 25.303739547729492 	 0.23866939544677734 	 0.2592902183532715 	 None 	 None 	 None 	 None 	 
2025-08-05 22:48:00.170039 test begin: paddle.all(Tensor([8467210, 6, 10],"bool"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([8467210, 6, 10],"bool"), None, False, None, ) 	 508032600 	 49878 	 23.149381637573242 	 25.361034154891968 	 0.2371809482574463 	 0.2597789764404297 	 None 	 None 	 None 	 None 	 
2025-08-05 22:48:55.592226 test begin: paddle.allclose(Tensor([1124, 45199],"float32"), Tensor([1124, 45199],"float32"), )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([1124, 45199],"float32"), Tensor([1124, 45199],"float32"), ) 	 101607352 	 9683 	 10.021986722946167 	 33.02556920051575 	 1.0573208332061768 	 0.0002465248107910156 	 None 	 None 	 None 	 None 	 
2025-08-05 22:49:44.307936 test begin: paddle.allclose(Tensor([13, 32, 122124],"float32"), Tensor([13, 32, 122124],"float32"), rtol=0.0001, atol=0.0001, )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([13, 32, 122124],"float32"), Tensor([13, 32, 122124],"float32"), rtol=0.0001, atol=0.0001, ) 	 101607168 	 9683 	 10.167364358901978 	 33.01600527763367 	 1.072943925857544 	 0.0007054805755615234 	 None 	 None 	 None 	 None 	 
2025-08-05 22:50:29.318442 test begin: paddle.allclose(Tensor([13, 61062, 64],"float32"), Tensor([13, 61062, 64],"float32"), rtol=0.0001, atol=0.0001, )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([13, 61062, 64],"float32"), Tensor([13, 61062, 64],"float32"), rtol=0.0001, atol=0.0001, ) 	 101607168 	 9683 	 10.449727773666382 	 33.0526020526886 	 1.356351613998413 	 0.0006988048553466797 	 None 	 None 	 None 	 None 	 
2025-08-05 22:51:19.342366 test begin: paddle.allclose(Tensor([1587601, 32],"float32"), Tensor([1587601, 32],"float32"), )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([1587601, 32],"float32"), Tensor([1587601, 32],"float32"), ) 	 101606464 	 9683 	 10.09187626838684 	 33.04801845550537 	 1.0649232864379883 	 0.00023674964904785156 	 None 	 None 	 None 	 None 	 
2025-08-05 22:52:04.461710 test begin: paddle.allclose(Tensor([24807, 32, 64],"float32"), Tensor([24807, 32, 64],"float32"), rtol=0.0001, atol=0.0001, )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([24807, 32, 64],"float32"), Tensor([24807, 32, 64],"float32"), rtol=0.0001, atol=0.0001, ) 	 101609472 	 9683 	 10.216917514801025 	 32.996068239212036 	 1.0782084465026855 	 0.0002295970916748047 	 None 	 None 	 None 	 None 	 
2025-08-05 22:52:49.345393 test begin: paddle.allclose(Tensor([30522, 1665],"float32"), Tensor([30522, 1665],"float32"), )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([30522, 1665],"float32"), Tensor([30522, 1665],"float32"), ) 	 101638260 	 9683 	 10.032008647918701 	 32.97367477416992 	 1.0594143867492676 	 0.0003249645233154297 	 None 	 None 	 None 	 None 	 
2025-08-05 22:53:34.072438 test begin: paddle.allclose(Tensor([6350401, 8],"float32"), Tensor([6350401, 8],"float32"), )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([6350401, 8],"float32"), Tensor([6350401, 8],"float32"), ) 	 101606416 	 9683 	 10.09095549583435 	 32.99850654602051 	 1.0650734901428223 	 0.00023031234741210938 	 None 	 None 	 None 	 None 	 
2025-08-05 22:54:18.847984 test begin: paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[-1,-2,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 50766 	 11.38531494140625 	 9.045429229736328 	 0.11462759971618652 	 0.09107804298400879 	 56.38850927352905 	 66.66628456115723 	 0.2271432876586914 	 0.2235863208770752 	 
2025-08-05 22:56:50.673928 test begin: paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 50766 	 12.111061096191406 	 10.186162233352661 	 0.12193083763122559 	 0.10247230529785156 	 57.08176255226135 	 68.88847470283508 	 0.22999882698059082 	 0.23096609115600586 	 
2025-08-05 22:59:21.914469 test begin: paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[0,1,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[0,1,], keepdim=False, ) 	 50803300 	 50766 	 10.064095258712769 	 8.0935537815094 	 0.2025589942932129 	 0.16295886039733887 	 55.43032741546631 	 65.5138623714447 	 0.27904844284057617 	 0.26361632347106934 	 
2025-08-05 23:01:43.787972 test begin: paddle.amax(Tensor([10, 508033, 10],"float32"), axis=list[-1,-2,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 508033, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 50766 	 11.381749391555786 	 9.04492974281311 	 0.11454224586486816 	 0.09098052978515625 	 56.38650965690613 	 66.67869114875793 	 0.22711706161499023 	 0.22354698181152344 	 
2025-08-05 23:04:08.231881 test begin: paddle.amax(Tensor([10, 508033, 10],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 508033, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 50766 	 28.993149280548096 	 22.997621536254883 	 0.5836725234985352 	 0.4629485607147217 	 66.56929111480713 	 74.11253905296326 	 0.334949254989624 	 0.2981414794921875 	 
2025-08-05 23:07:23.004781 test begin: paddle.amax(Tensor([10, 508033, 10],"float32"), axis=list[0,1,], keepdim=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f022e9232b0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 23:17:28.614669 test begin: paddle.amax(Tensor([508033, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, )
W0805 23:17:29.552776 113204 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.amax 	 paddle.amax(Tensor([508033, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 50766 	 22.32671046257019 	 14.6202871799469 	 0.44960522651672363 	 0.29420018196105957 	 59.74297571182251 	 66.61775064468384 	 0.30066561698913574 	 0.26804661750793457 	 
2025-08-05 23:20:16.536944 test begin: paddle.amax(Tensor([508033, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([508033, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 50766 	 13.2173011302948 	 10.943576574325562 	 0.13314223289489746 	 0.11017203330993652 	 57.4616973400116 	 72.39190363883972 	 0.23153066635131836 	 0.24276232719421387 	 
2025-08-05 23:22:51.793102 test begin: paddle.amax(Tensor([508033, 10, 10],"float32"), axis=list[0,1,], keepdim=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdd34276740>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 23:33:02.373187 test begin: paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[-1,-2,], keepdim=False, )
W0805 23:33:03.396842 113624 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 50777 	 11.182404518127441 	 9.029913425445557 	 0.11252737045288086 	 0.09087395668029785 	 55.7609920501709 	 66.5962233543396 	 0.22455668449401855 	 0.22341346740722656 	 
2025-08-05 23:35:26.565183 test begin: paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 50777 	 12.477471113204956 	 10.158780574798584 	 0.12558698654174805 	 0.10222148895263672 	 56.978718280792236 	 68.84560632705688 	 0.22947955131530762 	 0.23075103759765625 	 
2025-08-05 23:37:59.148862 test begin: paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[0,1,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[0,1,], keepdim=False, ) 	 50803300 	 50777 	 10.064237356185913 	 8.104403495788574 	 0.2026817798614502 	 0.16312623023986816 	 55.45195293426514 	 65.46550607681274 	 0.2790231704711914 	 0.26326894760131836 	 
2025-08-05 23:40:19.253454 test begin: paddle.amin(Tensor([10, 508033, 10],"float32"), axis=list[-1,-2,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 508033, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 50777 	 11.760572910308838 	 9.039320230484009 	 0.11835670471191406 	 0.09098482131958008 	 56.320507287979126 	 66.62498879432678 	 0.2268373966217041 	 0.22330021858215332 	 
2025-08-05 23:42:44.790193 test begin: paddle.amin(Tensor([10, 508033, 10],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 508033, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 50777 	 28.98775553703308 	 22.987276554107666 	 0.583451509475708 	 0.4625716209411621 	 66.56575679779053 	 74.09173583984375 	 0.3347797393798828 	 0.2979772090911865 	 
2025-08-05 23:45:58.340702 test begin: paddle.amin(Tensor([10, 508033, 10],"float32"), axis=list[0,1,], keepdim=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2c58e9c7f0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 23:56:03.691089 test begin: paddle.amin(Tensor([508033, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, )
W0805 23:56:04.659407 114400 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.amin 	 paddle.amin(Tensor([508033, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 50777 	 22.326693534851074 	 14.614283561706543 	 0.44963860511779785 	 0.29416775703430176 	 59.75633502006531 	 66.63578772544861 	 0.300647497177124 	 0.26801204681396484 	 
2025-08-05 23:58:48.653024 test begin: paddle.amin(Tensor([508033, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([508033, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 50777 	 13.216587543487549 	 10.948663711547852 	 0.1330430507659912 	 0.11023116111755371 	 57.47127318382263 	 72.3913803100586 	 0.23152947425842285 	 0.2427377700805664 	 
2025-08-06 00:01:23.530303 test begin: paddle.amin(Tensor([508033, 10, 10],"float32"), axis=list[0,1,], keepdim=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc461faf2e0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 00:11:28.683342 test begin: paddle.any(Tensor([10, 12404, 4096],"bool"), )
W0806 00:11:42.757920 114811 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.any 	 paddle.any(Tensor([10, 12404, 4096],"bool"), ) 	 508067840 	 21580 	 10.037129402160645 	 11.410228967666626 	 0.2376871109008789 	 0.27016115188598633 	 None 	 None 	 None 	 None 	 
2025-08-06 00:12:06.477085 test begin: paddle.any(Tensor([10, 300, 169345],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([10, 300, 169345],"bool"), ) 	 508035000 	 21580 	 10.02994704246521 	 11.41952896118164 	 0.23751401901245117 	 0.2704479694366455 	 None 	 None 	 None 	 None 	 
2025-08-06 00:12:36.636040 test begin: paddle.any(Tensor([11240, 45199],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([11240, 45199],"bool"), ) 	 508036760 	 21580 	 9.992262840270996 	 11.408974647521973 	 0.23659944534301758 	 0.2701225280761719 	 None 	 None 	 None 	 None 	 
2025-08-06 00:13:05.898332 test begin: paddle.any(Tensor([15876010, 32],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([15876010, 32],"bool"), ) 	 508032320 	 21580 	 10.06776213645935 	 11.416501760482788 	 0.23837876319885254 	 0.27034735679626465 	 None 	 None 	 None 	 None 	 
2025-08-06 00:13:34.675343 test begin: paddle.any(Tensor([420, 300, 4096],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([420, 300, 4096],"bool"), ) 	 516096000 	 21580 	 10.187914609909058 	 11.637315034866333 	 0.24123597145080566 	 0.2755293846130371 	 None 	 None 	 None 	 None 	 
2025-08-06 00:14:04.660903 test begin: paddle.any(Tensor([5120, 99226],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([5120, 99226],"bool"), ) 	 508037120 	 21580 	 10.068087100982666 	 11.384974479675293 	 0.23840594291687012 	 0.26959729194641113 	 None 	 None 	 None 	 None 	 
2025-08-06 00:14:33.712378 test begin: paddle.argmax(Tensor([15877, 100, 32],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([15877, 100, 32],"float32"), axis=1, ) 	 50806400 	 15617 	 11.883453130722046 	 2.8561763763427734 	 0.7777454853057861 	 0.1869654655456543 	 None 	 None 	 None 	 None 	 
2025-08-06 00:14:49.322009 test begin: paddle.argmax(Tensor([29151, 100, 18],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([29151, 100, 18],"float32"), axis=1, ) 	 52471800 	 15617 	 10.006253719329834 	 2.7211759090423584 	 0.6548190116882324 	 0.17810416221618652 	 None 	 None 	 None 	 None 	 
2025-08-06 00:15:02.854393 test begin: paddle.argmax(Tensor([29151, 28, 64],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([29151, 28, 64],"float32"), axis=1, ) 	 52238592 	 15617 	 23.46571636199951 	 2.753675937652588 	 1.5356590747833252 	 0.18018341064453125 	 None 	 None 	 None 	 None 	 
2025-08-06 00:15:29.945810 test begin: paddle.argmax(Tensor([29151, 55, 32],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([29151, 55, 32],"float32"), axis=1, ) 	 51305760 	 15617 	 11.77877426147461 	 2.6372790336608887 	 0.7708699703216553 	 0.17237496376037598 	 None 	 None 	 None 	 None 	 
2025-08-06 00:15:45.472908 test begin: paddle.argmax(Tensor([39691, 20, 64],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([39691, 20, 64],"float32"), axis=1, ) 	 50804480 	 15617 	 31.92372989654541 	 2.750992774963379 	 2.0891332626342773 	 0.18001842498779297 	 None 	 None 	 None 	 None 	 
2025-08-06 00:16:21.200414 test begin: paddle.argmax(Tensor([7939, 100, 64],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([7939, 100, 64],"float32"), axis=1, ) 	 50809600 	 15617 	 11.858938932418823 	 2.817793607711792 	 0.7760119438171387 	 0.18430018424987793 	 None 	 None 	 None 	 None 	 
2025-08-06 00:16:38.771636 test begin: paddle.argmax(Tensor([80239, 10, 64],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([80239, 10, 64],"float32"), axis=1, ) 	 51352960 	 15617 	 64.47343754768372 	 3.078754425048828 	 4.219306707382202 	 0.2003769874572754 	 None 	 None 	 None 	 None 	 
2025-08-06 00:17:47.851089 test begin: paddle.argmax(Tensor([80239, 20, 32],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([80239, 20, 32],"float32"), axis=1, ) 	 51352960 	 15617 	 32.26679444313049 	 2.77197003364563 	 2.1115922927856445 	 0.18138504028320312 	 None 	 None 	 None 	 None 	 
2025-08-06 00:18:23.754417 test begin: paddle.argmin(Tensor([104534, 3, 3, 3, 3, 3],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([104534, 3, 3, 3, 3, 3],"float64"), axis=0, ) 	 25401762 	 22702 	 10.833512306213379 	 3.6474993228912354 	 0.4884822368621826 	 0.08208990097045898 	 None 	 None 	 None 	 None 	 
2025-08-06 00:18:41.712364 test begin: paddle.argmin(Tensor([203213, 5, 5, 5],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([203213, 5, 5, 5],"float64"), axis=0, ) 	 25401625 	 22702 	 11.040668487548828 	 3.691545248031616 	 0.49698352813720703 	 0.08308577537536621 	 None 	 None 	 None 	 None 	 
2025-08-06 00:18:57.009797 test begin: paddle.argmin(Tensor([3, 104534, 3, 3, 3, 3],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([3, 104534, 3, 3, 3, 3],"float64"), axis=0, ) 	 25401762 	 22702 	 154.48178887367249 	 4.622352361679077 	 6.954570293426514 	 0.20792865753173828 	 None 	 None 	 None 	 None 	 
2025-08-06 00:21:38.362518 test begin: paddle.argmin(Tensor([3, 3, 104534, 3, 3, 3],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([3, 3, 104534, 3, 3, 3],"float64"), axis=0, ) 	 25401762 	 22702 	 154.4862883090973 	 4.61816930770874 	 6.954962491989136 	 0.2079000473022461 	 None 	 None 	 None 	 None 	 
2025-08-06 00:24:18.084647 test begin: paddle.argmin(Tensor([3, 3, 3, 104534, 3, 3],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([3, 3, 3, 104534, 3, 3],"float64"), axis=0, ) 	 25401762 	 22702 	 154.49011659622192 	 4.632033586502075 	 6.9547953605651855 	 0.20787596702575684 	 None 	 None 	 None 	 None 	 
2025-08-06 00:26:58.383282 test begin: paddle.argmin(Tensor([3, 3, 3, 3, 104534, 3],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([3, 3, 3, 3, 104534, 3],"float64"), axis=0, ) 	 25401762 	 22702 	 154.48935055732727 	 4.618236541748047 	 6.954856872558594 	 0.20792245864868164 	 None 	 None 	 None 	 None 	 
2025-08-06 00:29:38.860756 test begin: paddle.argmin(Tensor([3, 3, 3, 3, 3, 104534],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([3, 3, 3, 3, 3, 104534],"float64"), axis=0, ) 	 25401762 	 22702 	 154.489497423172 	 4.618215322494507 	 6.954908847808838 	 0.20789003372192383 	 None 	 None 	 None 	 None 	 
2025-08-06 00:32:18.578558 test begin: paddle.argmin(Tensor([4, 4, 4, 4, 99226],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([4, 4, 4, 4, 99226],"float64"), axis=0, ) 	 25401856 	 22702 	 115.88925814628601 	 4.341144561767578 	 5.217151165008545 	 0.19521236419677734 	 None 	 None 	 None 	 None 	 
2025-08-06 00:34:19.393344 test begin: paddle.argmin(Tensor([4, 4, 4, 99226, 4],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([4, 4, 4, 99226, 4],"float64"), axis=0, ) 	 25401856 	 22702 	 115.88586568832397 	 4.335974216461182 	 5.217017650604248 	 0.19514727592468262 	 None 	 None 	 None 	 None 	 
2025-08-06 00:36:20.300861 test begin: paddle.argmin(Tensor([4, 4, 99226, 4, 4],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([4, 4, 99226, 4, 4],"float64"), axis=0, ) 	 25401856 	 22702 	 115.88624262809753 	 4.335835933685303 	 5.2170069217681885 	 0.19519925117492676 	 None 	 None 	 None 	 None 	 
2025-08-06 00:38:23.459244 test begin: paddle.argmin(Tensor([4, 99226, 4, 4, 4],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([4, 99226, 4, 4, 4],"float64"), axis=0, ) 	 25401856 	 22702 	 115.89035129547119 	 4.335900068283081 	 5.217085123062134 	 0.1951904296875 	 None 	 None 	 None 	 None 	 
2025-08-06 00:40:24.239599 test begin: paddle.argmin(Tensor([5, 203213, 5, 5],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([5, 203213, 5, 5],"float64"), axis=0, ) 	 25401625 	 22702 	 92.72755479812622 	 4.4675962924957275 	 4.174494028091431 	 0.20075345039367676 	 None 	 None 	 None 	 None 	 
2025-08-06 00:42:02.991113 test begin: paddle.argmin(Tensor([5, 5, 203213, 5],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([5, 5, 203213, 5],"float64"), axis=0, ) 	 25401625 	 22702 	 92.9675805568695 	 5.454662322998047 	 4.410843372344971 	 0.20060491561889648 	 None 	 None 	 None 	 None 	 
2025-08-06 00:43:43.310516 test begin: paddle.argmin(Tensor([5, 5, 5, 203213],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([5, 5, 5, 203213],"float64"), axis=0, ) 	 25401625 	 22702 	 92.72977185249329 	 4.478962421417236 	 4.174482345581055 	 0.20065665245056152 	 None 	 None 	 None 	 None 	 
2025-08-06 00:45:23.149622 test begin: paddle.argmin(Tensor([99226, 4, 4, 4, 4],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([99226, 4, 4, 4, 4],"float64"), axis=0, ) 	 25401856 	 22702 	 9.975020170211792 	 7.340582370758057 	 0.44890284538269043 	 0.1654198169708252 	 None 	 None 	 None 	 None 	 
2025-08-06 00:45:42.415861 test begin: paddle.argsort(Tensor([25401601],"float64"), stable=True, )
[Prof] paddle.argsort 	 paddle.argsort(Tensor([25401601],"float64"), stable=True, ) 	 25401601 	 1000 	 10.62746000289917 	 7.488684415817261 	 9.179115295410156e-05 	 0.3337087631225586 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 00:46:06.284878 test begin: paddle.argsort(Tensor([50803201],"float32"), stable=True, )
[Prof] paddle.argsort 	 paddle.argsort(Tensor([50803201],"float32"), stable=True, ) 	 50803201 	 1000 	 19.01622438430786 	 7.867427110671997 	 9.870529174804688e-05 	 0.535872220993042 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 00:46:46.791352 test begin: paddle.argsort(Tensor([50803201],"int32"), stable=True, )
[Prof] paddle.argsort 	 paddle.argsort(Tensor([50803201],"int32"), stable=True, ) 	 50803201 	 1000 	 16.251832246780396 	 7.206975221633911 	 0.00010514259338378906 	 0.49138665199279785 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 00:47:20.277991 test begin: paddle.as_complex(Tensor([320, 15, 207, 8, 32, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([320, 15, 207, 8, 32, 2],"float32"), ) 	 508723200 	 3339679 	 10.41608214378357 	 14.06494426727295 	 9.083747863769531e-05 	 0.00011754035949707031 	 130.1447148323059 	 184.79610538482666 	 9.942054748535156e-05 	 0.00022149085998535156 	 
2025-08-06 00:53:16.551829 test begin: paddle.as_complex(Tensor([320, 15, 8, 207, 32, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([320, 15, 8, 207, 32, 2],"float32"), ) 	 508723200 	 3339679 	 10.42069959640503 	 14.140512466430664 	 0.00011229515075683594 	 0.0002655982971191406 	 148.0621817111969 	 183.80044984817505 	 9.489059448242188e-05 	 0.0002651214599609375 	 
2025-08-06 00:59:26.926544 test begin: paddle.as_complex(Tensor([320, 15, 8, 8, 827, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([320, 15, 8, 8, 827, 2],"float32"), ) 	 508108800 	 3339679 	 10.426872491836548 	 14.159849166870117 	 0.00014543533325195312 	 7.939338684082031e-05 	 132.50731372833252 	 185.15712714195251 	 0.00011038780212402344 	 0.0002636909484863281 	 
2025-08-06 01:05:22.356313 test begin: paddle.as_complex(Tensor([320, 388, 8, 8, 32, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([320, 388, 8, 8, 32, 2],"float32"), ) 	 508559360 	 3339679 	 10.725720167160034 	 14.348377466201782 	 0.00013971328735351562 	 0.00011873245239257812 	 134.16387367248535 	 183.21946144104004 	 0.00010013580322265625 	 0.0002269744873046875 	 
2025-08-06 01:11:19.712111 test begin: paddle.as_complex(Tensor([8270, 15, 8, 8, 32, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([8270, 15, 8, 8, 32, 2],"float32"), ) 	 508108800 	 3339679 	 10.425587177276611 	 14.055785417556763 	 0.0001323223114013672 	 0.00026798248291015625 	 131.42378902435303 	 184.9856629371643 	 0.00010204315185546875 	 0.000217437744140625 	 
2025-08-06 01:17:15.674061 test begin: paddle.as_strided(Tensor([15876010, 32],"float32"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([15876010, 32],"float32"), shape=tuple(3,), stride=tuple(1,), ) 	 508032320 	 199910 	 3.55637526512146 	 0.8677024841308594 	 0.00014281272888183594 	 4.887580871582031e-05 	 300.35383582115173 	 262.82733392715454 	 0.7682526111602783 	 0.6712663173675537 	 
2025-08-06 01:26:51.937535 test begin: paddle.as_strided(Tensor([31752010, 32],"float16"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([31752010, 32],"float16"), shape=tuple(3,), stride=tuple(1,), ) 	 1016064320 	 199910 	 4.502844333648682 	 0.8708558082580566 	 0.00011324882507324219 	 7.987022399902344e-05 	 302.9291799068451 	 262.9552478790283 	 0.7741246223449707 	 0.6713805198669434 	 
2025-08-06 01:36:46.140081 test begin: paddle.as_strided(Tensor([31752010, 32],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([31752010, 32],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), ) 	 1016064320 	 199910 	 3.5107834339141846 	 0.89835524559021 	 0.00012254714965820312 	 4.792213439941406e-05 	 302.95887184143066 	 263.11976051330566 	 0.7742941379547119 	 0.6718738079071045 	 
2025-08-06 01:46:39.251709 test begin: paddle.as_strided(Tensor([320, 1587601],"float32"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([320, 1587601],"float32"), shape=tuple(3,), stride=tuple(1,), ) 	 508032320 	 199910 	 4.568253993988037 	 0.88193678855896 	 7.43865966796875e-05 	 7.462501525878906e-05 	 300.33461594581604 	 262.8320634365082 	 0.7682359218597412 	 0.6711583137512207 	 
2025-08-06 01:56:15.950581 test begin: paddle.as_strided(Tensor([320, 3175201],"float16"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([320, 3175201],"float16"), shape=tuple(3,), stride=tuple(1,), ) 	 1016064320 	 199910 	 3.5057663917541504 	 0.865837574005127 	 0.00011992454528808594 	 8.106231689453125e-05 	 302.8427634239197 	 262.944682598114 	 0.7748613357543945 	 0.6715202331542969 	 
2025-08-06 02:06:05.301290 test begin: paddle.as_strided(Tensor([320, 3175201],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([320, 3175201],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), ) 	 1016064320 	 199910 	 4.543458461761475 	 0.9078197479248047 	 0.00010895729064941406 	 0.00011610984802246094 	 302.9513554573059 	 263.1240530014038 	 0.7743470668792725 	 0.6718575954437256 	 
2025-08-06 02:15:55.715814 test begin: paddle.asin(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.asin 	 paddle.asin(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33858 	 10.001363754272461 	 10.068925619125366 	 0.3018956184387207 	 0.30389928817749023 	 15.239668607711792 	 60.38296985626221 	 0.4599640369415283 	 0.3645155429840088 	 
2025-08-06 02:17:33.139896 test begin: paddle.asin(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.asin 	 paddle.asin(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33858 	 10.004537582397461 	 10.068786859512329 	 0.3020195960998535 	 0.3039114475250244 	 15.247779607772827 	 60.38136601448059 	 0.460421085357666 	 0.3645179271697998 	 
2025-08-06 02:19:10.476108 test begin: paddle.asin(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.asin 	 paddle.asin(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33858 	 10.005783081054688 	 10.088410377502441 	 0.3020589351654053 	 0.3039383888244629 	 15.246460199356079 	 60.381837368011475 	 0.46036362648010254 	 0.36456298828125 	 
2025-08-06 02:20:49.934753 test begin: paddle.asinh(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.asinh 	 paddle.asinh(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33221 	 10.017107963562012 	 9.980497121810913 	 0.30831408500671387 	 0.3071250915527344 	 14.955134630203247 	 44.448084354400635 	 0.46019911766052246 	 0.3419206142425537 	 
2025-08-06 02:22:11.069053 test begin: paddle.asinh(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.asinh 	 paddle.asinh(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33221 	 10.00853443145752 	 10.77833104133606 	 0.307894229888916 	 0.3068985939025879 	 14.962742567062378 	 44.44916105270386 	 0.4602200984954834 	 0.3418896198272705 	 
2025-08-06 02:23:35.162975 test begin: paddle.asinh(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.asinh 	 paddle.asinh(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33221 	 10.01732611656189 	 9.981250047683716 	 0.30873942375183105 	 0.3070409297943115 	 14.961618185043335 	 44.448904275894165 	 0.4603242874145508 	 0.3419027328491211 	 
2025-08-06 02:24:56.475499 test begin: paddle.atan(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.atan 	 paddle.atan(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33597 	 10.000314235687256 	 10.699177742004395 	 0.3042135238647461 	 0.30394434928894043 	 15.118557691574097 	 35.046268701553345 	 0.45987701416015625 	 0.35538458824157715 	 
2025-08-06 02:26:13.174542 test begin: paddle.atan(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.atan 	 paddle.atan(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33597 	 10.00203251838684 	 10.689265012741089 	 0.3042471408843994 	 0.30393528938293457 	 15.130970239639282 	 35.045212268829346 	 0.46033644676208496 	 0.3554379940032959 	 
2025-08-06 02:27:26.393587 test begin: paddle.atan(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.atan 	 paddle.atan(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33597 	 10.005503177642822 	 10.70238208770752 	 0.3043341636657715 	 0.30393052101135254 	 15.130138635635376 	 35.045743465423584 	 0.4602675437927246 	 0.355426549911499 	 
2025-08-06 02:28:43.387307 test begin: paddle.atan2(Tensor([100],"float64"), Tensor([254017, 100],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(Tensor([100],"float64"), Tensor([254017, 100],"float64"), ) 	 25401800 	 22449 	 19.78935432434082 	 7.128600597381592 	 0.3004026412963867 	 0.32523488998413086 	 37.83444690704346 	 61.599247217178345 	 0.3440663814544678 	 0.254852294921875 	 
2025-08-06 02:30:51.295259 test begin: paddle.atan2(Tensor([111, 222, 1031],"float64"), Tensor([222, 1031],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(Tensor([111, 222, 1031],"float64"), Tensor([222, 1031],"float64"), ) 	 25634784 	 22449 	 19.807360649108887 	 8.190149545669556 	 0.30059242248535156 	 0.3343806266784668 	 27.699252605438232 	 67.06803154945374 	 0.31498098373413086 	 0.30530762672424316 	 
2025-08-06 02:32:57.811332 test begin: paddle.atan2(Tensor([111, 688, 333],"float64"), Tensor([688, 333],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(Tensor([111, 688, 333],"float64"), Tensor([688, 333],"float64"), ) 	 25659648 	 22449 	 19.83262348175049 	 7.282513380050659 	 0.3009984493255615 	 0.3335390090942383 	 27.580445766448975 	 67.24948835372925 	 0.31345605850219727 	 0.3061223030090332 	 
2025-08-06 02:35:02.472999 test begin: paddle.atan2(Tensor([344, 222, 333],"float64"), Tensor([222, 333],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(Tensor([344, 222, 333],"float64"), Tensor([222, 333],"float64"), ) 	 25504470 	 22449 	 19.775701999664307 	 7.243923187255859 	 0.300140380859375 	 0.3316011428833008 	 29.49715805053711 	 67.03774452209473 	 0.335315465927124 	 0.30514025688171387 	 
2025-08-06 02:37:11.823970 test begin: paddle.atan2(x=Tensor([19601, 6, 6, 6, 6],"float64"), y=Tensor([19601, 6, 6, 6, 6],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(x=Tensor([19601, 6, 6, 6, 6],"float64"), y=Tensor([19601, 6, 6, 6, 6],"float64"), ) 	 50805792 	 22449 	 10.01676321029663 	 11.541394233703613 	 0.456057071685791 	 0.46347856521606445 	 16.445451736450195 	 76.53597164154053 	 0.7487044334411621 	 0.387357234954834 	 
2025-08-06 02:39:10.708424 test begin: paddle.atan2(x=Tensor([3, 39201, 6, 6, 6],"float64"), y=Tensor([3, 39201, 6, 6, 6],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(x=Tensor([3, 39201, 6, 6, 6],"float64"), y=Tensor([3, 39201, 6, 6, 6],"float64"), ) 	 50804496 	 22449 	 10.014230489730835 	 10.238581418991089 	 0.4558851718902588 	 0.463456392288208 	 16.447096347808838 	 76.53723502159119 	 0.7487554550170898 	 0.38727355003356934 	 
2025-08-06 02:41:09.565143 test begin: paddle.atan2(x=Tensor([3, 6, 39201, 6, 6],"float64"), y=Tensor([3, 6, 39201, 6, 6],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(x=Tensor([3, 6, 39201, 6, 6],"float64"), y=Tensor([3, 6, 39201, 6, 6],"float64"), ) 	 50804496 	 22449 	 10.014249324798584 	 10.190627813339233 	 0.4559311866760254 	 0.463498592376709 	 16.44702386856079 	 76.53697681427002 	 0.7487483024597168 	 0.38734960556030273 	 
2025-08-06 02:43:06.520555 test begin: paddle.atan2(x=Tensor([3, 6, 6, 39201, 6],"float64"), y=Tensor([3, 6, 6, 39201, 6],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(x=Tensor([3, 6, 6, 39201, 6],"float64"), y=Tensor([3, 6, 6, 39201, 6],"float64"), ) 	 50804496 	 22449 	 10.01442813873291 	 10.186254024505615 	 0.455935001373291 	 0.4635009765625 	 16.44699001312256 	 76.53830862045288 	 0.7487683296203613 	 0.38734889030456543 	 
2025-08-06 02:45:01.374906 test begin: paddle.atan2(x=Tensor([3, 6, 6, 6, 39201],"float64"), y=Tensor([3, 6, 6, 6, 39201],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(x=Tensor([3, 6, 6, 6, 39201],"float64"), y=Tensor([3, 6, 6, 6, 39201],"float64"), ) 	 50804496 	 22449 	 10.014012336730957 	 10.182990550994873 	 0.45589447021484375 	 0.463533878326416 	 16.44693613052368 	 76.53405284881592 	 0.748776912689209 	 0.3873007297515869 	 
2025-08-06 02:46:59.280001 test begin: paddle.atanh(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.atanh 	 paddle.atanh(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33693 	 10.000959873199463 	 10.042693376541138 	 0.30342793464660645 	 0.3046433925628662 	 15.159987926483154 	 54.677408933639526 	 0.4598512649536133 	 0.331768274307251 	 
2025-08-06 02:48:33.541189 test begin: paddle.atanh(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.atanh 	 paddle.atanh(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33693 	 10.004778385162354 	 10.97534704208374 	 0.3034665584564209 	 0.30463266372680664 	 15.172967433929443 	 54.67764401435852 	 0.46032118797302246 	 0.33198118209838867 	 
2025-08-06 02:50:10.426019 test begin: paddle.atanh(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.atanh 	 paddle.atanh(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33693 	 10.005963802337646 	 10.048747777938843 	 0.3035402297973633 	 0.30460143089294434 	 15.173534393310547 	 54.67749834060669 	 0.46021461486816406 	 0.33183836936950684 	 
2025-08-06 02:51:44.216094 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), )
[Error] CUDA out of memory. Tried to allocate 18.93 GiB. GPU 0 has a total capacity of 39.39 GiB of which 18.08 GiB is free. Process 47719 has 21.30 GiB memory in use. Of the allocated memory 1024 bytes is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-06 02:52:54.165784 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
W0806 02:53:37.134029 120171 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 6288311 	 17.645310401916504 	 49.55807971954346 	 0.0001494884490966797 	 0.0002467632293701172 	 None 	 None 	 None 	 None 	 
2025-08-06 02:54:58.264086 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 1058401],"float64"), ) 	 2548629608 	 6288311 	 9.749297857284546 	 38.39776968955994 	 5.817413330078125e-05 	 8.678436279296875e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:56:41.070262 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), )
[Error] CUDA out of memory. Tried to allocate 18.99 GiB. GPU 0 has a total capacity of 39.39 GiB of which 1022.31 MiB is free. Process 103762 has 38.39 GiB memory in use. Of the allocated memory 189.00 KiB is allocated by PyTorch, and 1.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-06 02:57:51.893160 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), )
W0806 02:58:35.301129 120566 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), ) 	 2548654290 	 6288311 	 19.806434392929077 	 69.06245613098145 	 9.012222290039062e-05 	 0.00029397010803222656 	 None 	 None 	 None 	 None 	 
2025-08-06 03:00:17.538762 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), ) 	 25425720 	 6288311 	 18.72022843360901 	 46.33087468147278 	 0.00019693374633789062 	 0.0002460479736328125 	 None 	 None 	 None 	 None 	 
2025-08-06 03:01:25.014977 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 6288311 	 18.506762981414795 	 45.73090434074402 	 9.012222290039062e-05 	 0.0002524852752685547 	 None 	 None 	 None 	 None 	 
2025-08-06 03:03:25.685097 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 6288311 	 18.5455539226532 	 45.223732471466064 	 7.939338684082031e-05 	 0.00024509429931640625 	 None 	 None 	 None 	 None 	 
2025-08-06 03:05:23.479158 test begin: paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 6288311 	 18.68966245651245 	 45.01518988609314 	 0.00015687942504882812 	 0.0002532005310058594 	 None 	 None 	 None 	 None 	 
2025-08-06 03:06:27.852747 test begin: paddle.atleast_1d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 6288311 	 19.482787609100342 	 65.33932089805603 	 6.961822509765625e-05 	 0.00025200843811035156 	 None 	 None 	 None 	 None 	 
2025-08-06 03:08:47.875840 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 6288311 	 18.226911067962646 	 46.0399010181427 	 5.650520324707031e-05 	 0.000244140625 	 None 	 None 	 None 	 None 	 
2025-08-06 03:10:46.654172 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), ) 	 2548633220 	 6288311 	 18.530784845352173 	 46.11686158180237 	 0.00016307830810546875 	 0.00024056434631347656 	 None 	 None 	 None 	 None 	 
2025-08-06 03:12:57.365586 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), ) 	 2548632618 	 6288311 	 19.953476667404175 	 65.605642080307 	 6.580352783203125e-05 	 0.0002396106719970703 	 None 	 None 	 None 	 None 	 
2025-08-06 03:15:22.952723 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), ) 	 25406424 	 6288311 	 18.434863328933716 	 45.349785804748535 	 0.0001595020294189453 	 0.0002512931823730469 	 None 	 None 	 None 	 None 	 
2025-08-06 03:16:27.415365 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 6288311 	 18.34492588043213 	 65.55176162719727 	 8.893013000488281e-05 	 0.0002543926239013672 	 None 	 None 	 None 	 None 	 
2025-08-06 03:18:55.167518 test begin: paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 6288311 	 18.433598518371582 	 45.175490856170654 	 0.00016760826110839844 	 0.00023603439331054688 	 None 	 None 	 None 	 None 	 
2025-08-06 03:19:59.451235 test begin: paddle.atleast_1d(Tensor([301, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 423361, 5],"float64"), ) 	 2548633220 	 6288311 	 10.210771083831787 	 38.555708169937134 	 6.127357482910156e-05 	 7.2479248046875e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 03:21:43.590162 test begin: paddle.atleast_1d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 6288311 	 18.66499400138855 	 46.39936661720276 	 7.390975952148438e-05 	 0.00024890899658203125 	 None 	 None 	 None 	 None 	 
2025-08-06 03:23:46.792370 test begin: paddle.atleast_1d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 6288311 	 18.58836531639099 	 46.25646376609802 	 0.00011301040649414062 	 0.00021648406982421875 	 None 	 None 	 None 	 None 	 
2025-08-06 03:25:58.932547 test begin: paddle.atleast_1d(Tensor([301, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 846721, 2, 5],"float64"), ) 	 2548630210 	 6288311 	 10.186586141586304 	 38.608094215393066 	 7.62939453125e-05 	 7.510185241699219e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 03:27:41.382832 test begin: paddle.atleast_1d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 6288311 	 18.805879592895508 	 46.41707706451416 	 5.936622619628906e-05 	 0.0002415180206298828 	 None 	 None 	 None 	 None 	 
2025-08-06 03:29:40.910381 test begin: paddle.atleast_1d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 6288311 	 18.45952868461609 	 46.18193078041077 	 0.0001461505889892578 	 0.00023698806762695312 	 None 	 None 	 None 	 None 	 
2025-08-06 03:30:46.242501 test begin: paddle.atleast_1d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 6288311 	 18.223190307617188 	 50.27395796775818 	 7.843971252441406e-05 	 0.0003123283386230469 	 None 	 None 	 None 	 None 	 
2025-08-06 03:32:58.081563 test begin: paddle.atleast_1d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 6288311 	 18.555376052856445 	 46.32577967643738 	 0.0001468658447265625 	 0.00024008750915527344 	 None 	 None 	 None 	 None 	 
2025-08-06 03:34:03.607314 test begin: paddle.atleast_1d(Tensor([63504101, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([63504101, 4, 2, 5],"float64"), ) 	 2540164040 	 6288311 	 10.153515815734863 	 39.258615255355835 	 6.413459777832031e-05 	 7.843971252441406e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 03:35:45.929488 test begin: paddle.atleast_1d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 2540164280 	 6288311 	 18.49826145172119 	 46.54290986061096 	 5.888938903808594e-05 	 0.00021123886108398438 	 None 	 None 	 None 	 None 	 
2025-08-06 03:37:56.326056 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), ) 	 2540164280 	 5325424 	 21.456504106521606 	 56.11663770675659 	 8.130073547363281e-05 	 0.00024008750915527344 	 None 	 None 	 None 	 None 	 
2025-08-06 03:40:14.217689 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 2540164280 	 5325424 	 20.591834783554077 	 39.857264280319214 	 0.00013971328735351562 	 0.00024437904357910156 	 None 	 None 	 None 	 None 	 
2025-08-06 03:42:20.314220 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), ) 	 2540160856 	 5325424 	 20.39381718635559 	 39.53219389915466 	 0.00013589859008789062 	 0.0002181529998779297 	 None 	 None 	 None 	 None 	 
2025-08-06 03:44:28.016624 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 5325424 	 20.083074808120728 	 39.41209840774536 	 6.556510925292969e-05 	 0.00023889541625976562 	 None 	 None 	 None 	 None 	 
2025-08-06 03:46:21.160056 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 1058401],"float64"), ) 	 2548629608 	 5325424 	 10.197154760360718 	 33.69503688812256 	 5.745887756347656e-05 	 8.296966552734375e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 03:47:58.604240 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548653688 	 5325424 	 21.302027940750122 	 39.46540570259094 	 5.91278076171875e-05 	 0.0002827644348144531 	 None 	 None 	 None 	 None 	 
2025-08-06 03:49:53.864371 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548653688 	 5325424 	 21.237394332885742 	 39.3151490688324 	 6.604194641113281e-05 	 0.00023674964904785156 	 None 	 None 	 None 	 None 	 
2025-08-06 03:51:50.081847 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), ) 	 2548653688 	 5325424 	 21.339749813079834 	 39.45237445831299 	 7.915496826171875e-05 	 0.0002353191375732422 	 None 	 None 	 None 	 None 	 
2025-08-06 03:53:47.561652 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), ) 	 2548657300 	 5325424 	 21.338982582092285 	 39.84413194656372 	 0.00013208389282226562 	 0.00025153160095214844 	 None 	 None 	 None 	 None 	 
2025-08-06 03:56:02.490964 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), ) 	 2548654290 	 5325424 	 21.066962242126465 	 39.58622193336487 	 5.650520324707031e-05 	 0.0002491474151611328 	 None 	 None 	 None 	 None 	 
2025-08-06 03:58:00.445639 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), ) 	 25425720 	 5325424 	 20.81219244003296 	 39.93377375602722 	 0.0001380443572998047 	 0.00024247169494628906 	 None 	 None 	 None 	 None 	 
2025-08-06 03:59:02.015065 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 5325424 	 21.09000515937805 	 38.94516062736511 	 6.628036499023438e-05 	 0.00023698806762695312 	 None 	 None 	 None 	 None 	 
2025-08-06 04:00:55.996551 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 5325424 	 21.08075737953186 	 39.07288980484009 	 0.00014543533325195312 	 7.891654968261719e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 04:02:53.164663 test begin: paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 5325424 	 20.911063194274902 	 39.81576943397522 	 0.0001430511474609375 	 0.0002467632293701172 	 None 	 None 	 None 	 None 	 
2025-08-06 04:03:54.586997 test begin: paddle.atleast_2d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 5325424 	 20.75476598739624 	 39.952168464660645 	 6.604194641113281e-05 	 0.0002491474151611328 	 None 	 None 	 None 	 None 	 
2025-08-06 04:05:52.307618 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 5325424 	 20.712244510650635 	 40.19533395767212 	 0.00014853477478027344 	 0.00024962425231933594 	 None 	 None 	 None 	 None 	 
2025-08-06 04:07:50.693135 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), ) 	 2548633220 	 5325424 	 20.953608751296997 	 39.78882670402527 	 5.936622619628906e-05 	 0.00023984909057617188 	 None 	 None 	 None 	 None 	 
2025-08-06 04:09:46.674723 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), ) 	 2548632618 	 5325424 	 20.722947597503662 	 40.02087163925171 	 6.580352783203125e-05 	 0.000244140625 	 None 	 None 	 None 	 None 	 
2025-08-06 04:11:43.180493 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), ) 	 25406424 	 5325424 	 20.70946979522705 	 41.36971044540405 	 0.00013947486877441406 	 0.00027489662170410156 	 None 	 None 	 None 	 None 	 
2025-08-06 04:12:45.959276 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 5325424 	 20.964463472366333 	 39.854063272476196 	 6.127357482910156e-05 	 0.000240325927734375 	 None 	 None 	 None 	 None 	 
2025-08-06 04:14:41.228899 test begin: paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 5325424 	 20.75793719291687 	 43.36363935470581 	 0.00015211105346679688 	 0.00024580955505371094 	 None 	 None 	 None 	 None 	 
2025-08-06 04:15:46.018846 test begin: paddle.atleast_2d(Tensor([301, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 423361, 5],"float64"), ) 	 2548633220 	 5325424 	 10.231102466583252 	 33.4002366065979 	 5.984306335449219e-05 	 8.058547973632812e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 04:17:22.761848 test begin: paddle.atleast_2d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 5325424 	 22.28920006752014 	 54.33038401603699 	 0.00014519691467285156 	 0.0002465248107910156 	 None 	 None 	 None 	 None 	 
2025-08-06 04:19:47.067596 test begin: paddle.atleast_2d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 5325424 	 20.747844696044922 	 40.178799867630005 	 0.00014090538024902344 	 0.0002422332763671875 	 None 	 None 	 None 	 None 	 
2025-08-06 04:21:57.705563 test begin: paddle.atleast_2d(Tensor([301, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 846721, 2, 5],"float64"), ) 	 2548630210 	 5325424 	 10.264132261276245 	 32.98723602294922 	 6.151199340820312e-05 	 8.296966552734375e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 04:23:35.013799 test begin: paddle.atleast_2d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 5325424 	 21.17328429222107 	 39.85872769355774 	 6.699562072753906e-05 	 0.00024628639221191406 	 None 	 None 	 None 	 None 	 
2025-08-06 04:25:30.574632 test begin: paddle.atleast_2d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 5325424 	 20.494598150253296 	 40.62096548080444 	 0.0001499652862548828 	 0.0002455711364746094 	 None 	 None 	 None 	 None 	 
2025-08-06 04:26:32.357703 test begin: paddle.atleast_2d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 5325424 	 20.491841554641724 	 39.99469256401062 	 6.365776062011719e-05 	 0.0002415180206298828 	 None 	 None 	 None 	 None 	 
2025-08-06 04:28:28.651920 test begin: paddle.atleast_2d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 5325424 	 20.99164867401123 	 39.73905158042908 	 0.0001537799835205078 	 0.0002396106719970703 	 None 	 None 	 None 	 None 	 
2025-08-06 04:29:30.051816 test begin: paddle.atleast_2d(Tensor([63504101, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([63504101, 4, 2, 5],"float64"), ) 	 2540164040 	 5325424 	 10.348374843597412 	 32.92271447181702 	 5.745887756347656e-05 	 6.842613220214844e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 04:31:06.528779 test begin: paddle.atleast_2d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 2540164280 	 5325424 	 20.78570008277893 	 39.851561069488525 	 0.00013136863708496094 	 0.0002491474151611328 	 None 	 None 	 None 	 None 	 
2025-08-06 04:33:16.987543 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), ) 	 2540164280 	 4582436 	 22.172316551208496 	 35.5792818069458 	 6.413459777832031e-05 	 0.0002474784851074219 	 None 	 None 	 None 	 None 	 
2025-08-06 04:35:12.972303 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 2540164280 	 4582436 	 21.994059562683105 	 33.401013135910034 	 5.626678466796875e-05 	 0.0002391338348388672 	 None 	 None 	 None 	 None 	 
2025-08-06 04:37:02.475223 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), ) 	 2540160856 	 4582436 	 21.737342596054077 	 33.42206144332886 	 0.0001385211944580078 	 0.0002384185791015625 	 None 	 None 	 None 	 None 	 
2025-08-06 04:38:57.799833 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 4582436 	 21.56894016265869 	 33.52611422538757 	 5.650520324707031e-05 	 0.00023794174194335938 	 None 	 None 	 None 	 None 	 
2025-08-06 04:40:46.927867 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 1058401],"float64"), ) 	 2548629608 	 4582436 	 10.403544187545776 	 28.8616726398468 	 5.507469177246094e-05 	 0.00024890899658203125 	 None 	 None 	 None 	 None 	 
2025-08-06 04:42:36.560862 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548653688 	 4582436 	 22.938584089279175 	 33.45377516746521 	 0.0001308917999267578 	 7.367134094238281e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 04:44:44.996048 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548653688 	 4582436 	 22.69121527671814 	 33.4384024143219 	 0.0001399517059326172 	 7.343292236328125e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 04:46:46.923820 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 1058401],"float64"), ) 	 2548653688 	 4582436 	 22.436652421951294 	 33.54985523223877 	 6.246566772460938e-05 	 0.0002453327178955078 	 None 	 None 	 None 	 None 	 
2025-08-06 04:48:41.005783 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), ) 	 2548657300 	 4582436 	 22.564594507217407 	 33.60138249397278 	 5.459785461425781e-05 	 0.0002493858337402344 	 None 	 None 	 None 	 None 	 
2025-08-06 04:50:32.180123 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), ) 	 2548654290 	 4582436 	 22.677577018737793 	 33.5155668258667 	 5.91278076171875e-05 	 7.462501525878906e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 04:52:28.968641 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), ) 	 25425720 	 4582436 	 23.139415979385376 	 47.062798738479614 	 0.00016260147094726562 	 0.00025391578674316406 	 None 	 None 	 None 	 None 	 
2025-08-06 04:53:40.777932 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 4582436 	 22.624149560928345 	 33.14529776573181 	 5.793571472167969e-05 	 0.00023603439331054688 	 None 	 None 	 None 	 None 	 
2025-08-06 04:55:30.764496 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 4582436 	 22.795892238616943 	 33.383368492126465 	 0.00014853477478027344 	 0.00023794174194335938 	 None 	 None 	 None 	 None 	 
2025-08-06 04:57:28.067914 test begin: paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 4582436 	 23.1176176071167 	 46.8805251121521 	 0.00015497207641601562 	 0.0002560615539550781 	 None 	 None 	 None 	 None 	 
2025-08-06 04:58:39.758752 test begin: paddle.atleast_3d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 4582436 	 22.70204210281372 	 33.32846403121948 	 0.00014591217041015625 	 7.104873657226562e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 05:00:38.186319 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548633220 	 4582436 	 22.26827120780945 	 33.29331111907959 	 5.6743621826171875e-05 	 0.0002434253692626953 	 None 	 None 	 None 	 None 	 
2025-08-06 05:02:39.035702 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2116801],"float64"), ) 	 2548633220 	 4582436 	 22.2538480758667 	 33.41942501068115 	 0.00013303756713867188 	 0.0002422332763671875 	 None 	 None 	 None 	 None 	 
2025-08-06 05:04:40.603553 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), ) 	 2548632618 	 4582436 	 22.19011092185974 	 33.42151665687561 	 5.841255187988281e-05 	 0.0002377033233642578 	 None 	 None 	 None 	 None 	 
2025-08-06 05:06:30.663700 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), ) 	 25406424 	 4582436 	 21.743157386779785 	 33.07747530937195 	 0.00013303756713867188 	 0.0002415180206298828 	 None 	 None 	 None 	 None 	 
2025-08-06 05:07:26.611129 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 4582436 	 22.116202354431152 	 33.32456040382385 	 0.00014495849609375 	 0.0002574920654296875 	 None 	 None 	 None 	 None 	 
2025-08-06 05:09:27.871518 test begin: paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 4582436 	 21.83410358428955 	 33.33922481536865 	 0.0001316070556640625 	 0.00024509429931640625 	 None 	 None 	 None 	 None 	 
2025-08-06 05:10:23.708360 test begin: paddle.atleast_3d(Tensor([301, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 423361, 5],"float64"), ) 	 2548633220 	 4582436 	 10.392832279205322 	 28.598750114440918 	 5.555152893066406e-05 	 0.0002465248107910156 	 None 	 None 	 None 	 None 	 
2025-08-06 05:12:07.736529 test begin: paddle.atleast_3d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4, 423361, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548657300 	 4582436 	 22.69355845451355 	 33.28174448013306 	 0.00015044212341308594 	 7.534027099609375e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 05:14:07.217972 test begin: paddle.atleast_3d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 4233601, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 2548632618 	 4582436 	 22.161905765533447 	 33.402360677719116 	 6.747245788574219e-05 	 0.00025200843811035156 	 None 	 None 	 None 	 None 	 
2025-08-06 05:15:57.248935 test begin: paddle.atleast_3d(Tensor([301, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 846721, 2, 5],"float64"), ) 	 2548630210 	 4582436 	 10.308025360107422 	 29.022738695144653 	 6.198883056640625e-05 	 7.843971252441406e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 05:17:30.241677 test begin: paddle.atleast_3d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([301, 846721, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 2548654290 	 4582436 	 22.898377656936646 	 33.30156230926514 	 0.00013756752014160156 	 0.0002498626708984375 	 None 	 None 	 None 	 None 	 
2025-08-06 05:19:37.186805 test begin: paddle.atleast_3d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3175201, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), Tensor([301, 4, 2],"float64"), ) 	 25406424 	 4582436 	 21.83936643600464 	 33.38706350326538 	 0.0001423358917236328 	 0.0002453327178955078 	 None 	 None 	 None 	 None 	 
2025-08-06 05:20:33.378995 test begin: paddle.atleast_3d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([317520101, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 2540160856 	 4582436 	 21.912992477416992 	 33.56600379943848 	 0.00014281272888183594 	 0.00024509429931640625 	 None 	 None 	 None 	 None 	 
2025-08-06 05:22:31.169441 test begin: paddle.atleast_3d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([635041, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), Tensor([301, 4, 2, 5],"float64"), ) 	 25425720 	 4582436 	 22.463784217834473 	 33.27914214134216 	 0.000133514404296875 	 0.0002465248107910156 	 None 	 None 	 None 	 None 	 
2025-08-06 05:23:27.581200 test begin: paddle.atleast_3d(Tensor([63504101, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([63504101, 4, 2, 5],"float64"), ) 	 2540164040 	 4582436 	 11.242952108383179 	 35.019856214523315 	 7.05718994140625e-05 	 0.0002474784851074219 	 None 	 None 	 None 	 None 	 
2025-08-06 05:25:19.124099 test begin: paddle.atleast_3d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([63504101, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 2540164280 	 4582436 	 22.702117443084717 	 33.503883838653564 	 0.00013899803161621094 	 0.0002434253692626953 	 None 	 None 	 None 	 None 	 
2025-08-06 05:27:37.236710 test begin: paddle.bincount(Tensor([25401601],"int64"), minlength=Tensor([1],"int32"), )
[Prof] paddle.bincount 	 paddle.bincount(Tensor([25401601],"int64"), minlength=Tensor([1],"int32"), ) 	 25401602 	 10040 	 10.004472255706787 	 8.30224323272705 	 0.0005052089691162109 	 0.00045561790466308594 	 None 	 None 	 None 	 None 	 
2025-08-06 05:27:56.980502 test begin: paddle.bincount(Tensor([50803201],"int32"), weights=Tensor([50803201],"float32"), )
[Prof] paddle.bincount 	 paddle.bincount(Tensor([50803201],"int32"), weights=Tensor([50803201],"float32"), ) 	 101606402 	 10040 	 18.220645904541016 	 14.414777994155884 	 0.0010607242584228516 	 0.0010707378387451172 	 None 	 None 	 None 	 None 	 
2025-08-06 05:28:31.105713 test begin: paddle.bincount(x=Tensor([50803201],"int32"), weights=Tensor([50803201],"int32"), )
[Prof] paddle.bincount 	 paddle.bincount(x=Tensor([50803201],"int32"), weights=Tensor([50803201],"int32"), ) 	 101606402 	 10040 	 17.99378490447998 	 18.07792615890503 	 0.0010383129119873047 	 0.0009567737579345703 	 None 	 None 	 None 	 None 	 
2025-08-06 05:29:08.455474 test begin: paddle.bitwise_and(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84592 	 37.91449332237244 	 38.04531669616699 	 0.4579126834869385 	 0.45966386795043945 	 None 	 None 	 None 	 None 	 
2025-08-06 05:30:26.515882 test begin: paddle.bitwise_and(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84592 	 37.914677143096924 	 38.049500703811646 	 0.45835137367248535 	 0.4596364498138428 	 None 	 None 	 None 	 None 	 
2025-08-06 05:31:45.390792 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84592 	 37.918861389160156 	 38.04598140716553 	 0.4579005241394043 	 0.4595909118652344 	 None 	 None 	 None 	 None 	 
2025-08-06 05:33:03.387829 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), ) 	 203213880 	 84592 	 37.92025685310364 	 38.04570436477661 	 0.4580113887786865 	 0.4597136974334717 	 None 	 None 	 None 	 None 	 
2025-08-06 05:34:21.400070 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), ) 	 101607264 	 84592 	 10.025716066360474 	 9.921806812286377 	 0.12116718292236328 	 0.11990571022033691 	 None 	 None 	 None 	 None 	 
2025-08-06 05:34:42.846233 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), ) 	 101607264 	 84592 	 38.035563945770264 	 37.726868629455566 	 0.45958924293518066 	 0.4558370113372803 	 None 	 None 	 None 	 None 	 
2025-08-06 05:35:59.864874 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), ) 	 203213664 	 84592 	 37.91657066345215 	 38.050049781799316 	 0.4582362174987793 	 0.4596574306488037 	 None 	 None 	 None 	 None 	 
2025-08-06 05:37:18.811207 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 50807520 	 84592 	 15.055570602416992 	 19.225669384002686 	 0.18189287185668945 	 0.23226261138916016 	 None 	 None 	 None 	 None 	 
2025-08-06 05:37:55.335901 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 101610720 	 84592 	 26.69376492500305 	 40.57611322402954 	 0.3224031925201416 	 0.4901313781738281 	 None 	 None 	 None 	 None 	 
2025-08-06 05:39:03.783702 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 50807520 	 84592 	 25.046711206436157 	 26.065510749816895 	 0.3026008605957031 	 0.31490516662597656 	 None 	 None 	 None 	 None 	 
2025-08-06 05:39:55.526807 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), ) 	 101608560 	 84592 	 10.0203697681427 	 9.826333284378052 	 0.1210484504699707 	 0.11871767044067383 	 None 	 None 	 None 	 None 	 
2025-08-06 05:40:16.779385 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), ) 	 101608560 	 84592 	 38.04508638381958 	 37.726502418518066 	 0.4595317840576172 	 0.45584774017333984 	 None 	 None 	 None 	 None 	 
2025-08-06 05:41:33.803974 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), ) 	 203214960 	 84592 	 37.917940616607666 	 38.04742455482483 	 0.4580507278442383 	 0.4596712589263916 	 None 	 None 	 None 	 None 	 
2025-08-06 05:42:51.865887 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 50807520 	 84592 	 15.610473394393921 	 19.21781301498413 	 0.18858575820922852 	 0.23215842247009277 	 None 	 None 	 None 	 None 	 
2025-08-06 05:43:27.416926 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 101610720 	 84592 	 10.021087884902954 	 9.847246885299683 	 0.12107729911804199 	 0.11882448196411133 	 None 	 None 	 None 	 None 	 
2025-08-06 05:43:50.349852 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50807520 	 84592 	 25.011934280395508 	 26.05292272567749 	 0.302152156829834 	 0.3148190975189209 	 None 	 None 	 None 	 None 	 
2025-08-06 05:44:44.248408 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 101610720 	 84592 	 38.04269313812256 	 37.72922372817993 	 0.45957517623901367 	 0.4558217525482178 	 None 	 None 	 None 	 None 	 
2025-08-06 05:46:01.283963 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101610720 	 84592 	 29.168166637420654 	 40.45223307609558 	 0.3523976802825928 	 0.48877382278442383 	 None 	 None 	 None 	 None 	 
2025-08-06 05:47:11.923240 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 203217120 	 84592 	 37.92297697067261 	 38.04768133163452 	 0.4580109119415283 	 0.4596748352050781 	 None 	 None 	 None 	 None 	 
2025-08-06 05:48:29.969573 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), ) 	 101607480 	 84592 	 9.998722314834595 	 9.77040696144104 	 0.1208043098449707 	 0.11786198616027832 	 None 	 None 	 None 	 None 	 
2025-08-06 05:48:51.177787 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), ) 	 101607480 	 84592 	 38.047471046447754 	 37.72669816017151 	 0.45969057083129883 	 0.45578932762145996 	 None 	 None 	 None 	 None 	 
2025-08-06 05:50:08.229161 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84592 	 9.998717546463013 	 9.761323690414429 	 0.12080574035644531 	 0.11787104606628418 	 None 	 None 	 None 	 None 	 
2025-08-06 05:50:31.017685 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84592 	 38.04369902610779 	 37.763896465301514 	 0.4595677852630615 	 0.45581650733947754 	 None 	 None 	 None 	 None 	 
2025-08-06 05:51:50.315443 test begin: paddle.bitwise_and(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84592 	 9.99875521659851 	 9.757400512695312 	 0.12079930305480957 	 0.1178734302520752 	 None 	 None 	 None 	 None 	 
2025-08-06 05:52:11.473806 test begin: paddle.bitwise_and(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84592 	 38.042757987976074 	 37.726858377456665 	 0.4595146179199219 	 0.45586156845092773 	 None 	 None 	 None 	 None 	 
2025-08-06 05:53:28.537677 test begin: paddle.bitwise_and(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84592 	 9.998756408691406 	 9.767332315444946 	 0.12078595161437988 	 0.11787223815917969 	 None 	 None 	 None 	 None 	 
2025-08-06 05:53:52.108187 test begin: paddle.bitwise_and(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84592 	 38.04381036758423 	 37.727128982543945 	 0.45970964431762695 	 0.4557983875274658 	 None 	 None 	 None 	 None 	 
2025-08-06 05:55:09.401156 test begin: paddle.bitwise_and(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101608560 	 84592 	 10.020246505737305 	 9.824779033660889 	 0.12104082107543945 	 0.11873316764831543 	 None 	 None 	 None 	 None 	 
2025-08-06 05:55:30.675355 test begin: paddle.bitwise_and(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101608560 	 84592 	 38.04311752319336 	 37.72717046737671 	 0.459683895111084 	 0.45589399337768555 	 None 	 None 	 None 	 None 	 
2025-08-06 05:56:47.714173 test begin: paddle.bitwise_and(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214960 	 84592 	 37.925328493118286 	 38.04726004600525 	 0.45854854583740234 	 0.4595797061920166 	 None 	 None 	 None 	 None 	 
2025-08-06 05:58:05.728155 test begin: paddle.bitwise_invert(Tensor([12700801, 4, 1],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([12700801, 4, 1],"int32"), ) 	 50803204 	 33827 	 10.00083589553833 	 10.0697340965271 	 0.3021659851074219 	 0.3042323589324951 	 None 	 None 	 None 	 None 	 
2025-08-06 05:58:28.640390 test begin: paddle.bitwise_invert(Tensor([2, 1270081, 4, 5],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([2, 1270081, 4, 5],"int32"), ) 	 50803240 	 33827 	 10.000875234603882 	 11.79550838470459 	 0.30219388008117676 	 0.30420827865600586 	 None 	 None 	 None 	 None 	 
2025-08-06 05:58:52.188339 test begin: paddle.bitwise_invert(Tensor([2, 3, 1693441, 5],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([2, 3, 1693441, 5],"int32"), ) 	 50803230 	 33827 	 10.000634908676147 	 10.069419145584106 	 0.30213284492492676 	 0.30423951148986816 	 None 	 None 	 None 	 None 	 
2025-08-06 05:59:12.860093 test begin: paddle.bitwise_invert(Tensor([2, 3, 4, 2116801],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([2, 3, 4, 2116801],"int32"), ) 	 50803224 	 33827 	 10.000741004943848 	 10.069596767425537 	 0.3021209239959717 	 0.30422186851501465 	 None 	 None 	 None 	 None 	 
2025-08-06 05:59:33.527020 test begin: paddle.bitwise_invert(Tensor([3, 16934401, 1],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([3, 16934401, 1],"int32"), ) 	 50803203 	 33827 	 10.001155138015747 	 10.077195405960083 	 0.302182674407959 	 0.30422234535217285 	 None 	 None 	 None 	 None 	 
2025-08-06 05:59:54.216929 test begin: paddle.bitwise_invert(Tensor([3, 4, 4233601],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([3, 4, 4233601],"int32"), ) 	 50803212 	 33827 	 10.002089977264404 	 10.069425344467163 	 0.3021423816680908 	 0.30422449111938477 	 None 	 None 	 None 	 None 	 
2025-08-06 06:00:14.882374 test begin: paddle.bitwise_invert(Tensor([846721, 3, 4, 5],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([846721, 3, 4, 5],"int32"), ) 	 50803260 	 33827 	 10.000768899917603 	 10.069547891616821 	 0.30211877822875977 	 0.30425047874450684 	 None 	 None 	 None 	 None 	 
2025-08-06 06:00:38.330373 test begin: paddle.bitwise_left_shift(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), ) 	 101607000 	 22337 	 10.047326564788818 	 9.98080039024353 	 0.4597296714782715 	 0.4559037685394287 	 None 	 None 	 None 	 None 	 
2025-08-06 06:01:00.948063 test begin: paddle.bitwise_left_shift(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), ) 	 101606800 	 22337 	 10.046587944030762 	 9.964981079101562 	 0.4596850872039795 	 0.45589566230773926 	 None 	 None 	 None 	 None 	 
2025-08-06 06:01:23.297202 test begin: paddle.bitwise_left_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), ) 	 203213200 	 22337 	 10.001635789871216 	 11.552203893661499 	 0.45747995376586914 	 0.45950937271118164 	 None 	 None 	 None 	 None 	 
2025-08-06 06:01:48.005592 test begin: paddle.bitwise_left_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, ) 	 203213200 	 22337 	 10.001122236251831 	 10.048319339752197 	 0.45764851570129395 	 0.45955920219421387 	 None 	 None 	 None 	 None 	 
2025-08-06 06:02:10.119466 test begin: paddle.bitwise_left_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), ) 	 203213400 	 22337 	 10.000578880310059 	 10.04320478439331 	 0.45738983154296875 	 0.45958590507507324 	 None 	 None 	 None 	 None 	 
2025-08-06 06:02:32.266583 test begin: paddle.bitwise_left_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, ) 	 203213400 	 22337 	 9.99943470954895 	 10.051367998123169 	 0.45746326446533203 	 0.45960450172424316 	 None 	 None 	 None 	 None 	 
2025-08-06 06:02:54.441116 test begin: paddle.bitwise_not(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101607120 	 33833 	 10.087626934051514 	 10.105216264724731 	 0.30469298362731934 	 0.3024923801422119 	 None 	 None 	 None 	 None 	 
2025-08-06 06:03:17.655512 test begin: paddle.bitwise_not(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), ) 	 101607120 	 33833 	 10.087492227554321 	 10.01497197151184 	 0.30472636222839355 	 0.3026142120361328 	 None 	 None 	 None 	 None 	 
2025-08-06 06:03:39.556725 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), ) 	 101607120 	 33833 	 10.087571382522583 	 10.01479697227478 	 0.30472445487976074 	 0.3025505542755127 	 None 	 None 	 None 	 None 	 
2025-08-06 06:04:00.651615 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), ) 	 101606940 	 33833 	 10.064772844314575 	 10.018957376480103 	 0.304030179977417 	 0.3025095462799072 	 None 	 None 	 None 	 None 	 
2025-08-06 06:04:21.765045 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), ) 	 50803632 	 33833 	 9.997991800308228 	 10.071613073348999 	 0.30202746391296387 	 0.30423521995544434 	 None 	 None 	 None 	 None 	 
2025-08-06 06:04:44.206055 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), ) 	 101606832 	 33833 	 10.087171077728271 	 10.014979362487793 	 0.3047518730163574 	 0.3025350570678711 	 None 	 None 	 None 	 None 	 
2025-08-06 06:05:05.319996 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), ) 	 50804280 	 33833 	 10.00623631477356 	 10.071740627288818 	 0.30221986770629883 	 0.30429697036743164 	 None 	 None 	 None 	 None 	 
2025-08-06 06:05:25.995743 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), ) 	 101607480 	 33833 	 10.063152551651001 	 10.491284847259521 	 0.3039689064025879 	 0.30255126953125 	 None 	 None 	 None 	 None 	 
2025-08-06 06:05:48.904902 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 50805360 	 33833 	 10.000494718551636 	 10.07199501991272 	 0.30210232734680176 	 0.3041708469390869 	 None 	 None 	 None 	 None 	 
2025-08-06 06:06:09.573480 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 101608560 	 33833 	 10.063927173614502 	 10.014598846435547 	 0.30400562286376953 	 0.30248498916625977 	 None 	 None 	 None 	 None 	 
2025-08-06 06:06:30.659394 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), ) 	 50803740 	 33833 	 10.006608963012695 	 10.351728916168213 	 0.3022885322570801 	 0.3042271137237549 	 None 	 None 	 None 	 None 	 
2025-08-06 06:06:53.712619 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), ) 	 50803920 	 33833 	 10.004314661026001 	 10.0713951587677 	 0.3022327423095703 	 0.3041818141937256 	 None 	 None 	 None 	 None 	 
2025-08-06 06:07:14.385942 test begin: paddle.bitwise_not(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), ) 	 50803920 	 33833 	 10.004002094268799 	 10.07811188697815 	 0.3022034168243408 	 0.3042292594909668 	 None 	 None 	 None 	 None 	 
2025-08-06 06:07:37.763068 test begin: paddle.bitwise_not(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50803920 	 33833 	 10.00446081161499 	 10.080713033676147 	 0.30220842361450195 	 0.3042280673980713 	 None 	 None 	 None 	 None 	 
2025-08-06 06:07:58.489426 test begin: paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 1, 117601, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 1, 117601, 2],"bool"), ) 	 508036320 	 33833 	 28.64377498626709 	 25.273797035217285 	 0.8652708530426025 	 0.763481616973877 	 None 	 None 	 None 	 None 	 
2025-08-06 06:08:59.607250 test begin: paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 1, 5, 47041],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 1, 5, 47041],"bool"), ) 	 508042800 	 33833 	 28.996395587921143 	 26.94398832321167 	 0.8759205341339111 	 0.7640459537506104 	 None 	 None 	 None 	 None 	 
2025-08-06 06:10:03.609195 test begin: paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 508053600 	 33833 	 28.672298431396484 	 25.29886555671692 	 0.8664884567260742 	 0.7634713649749756 	 None 	 None 	 None 	 None 	 
2025-08-06 06:11:07.288305 test begin: paddle.bitwise_not(Tensor([20, 3, 3, 3, 94081, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 3, 3, 94081, 1, 5, 2],"bool"), ) 	 508037400 	 33833 	 29.003278732299805 	 25.28313183784485 	 0.8761470317840576 	 0.7635159492492676 	 None 	 None 	 None 	 None 	 
2025-08-06 06:12:08.605739 test begin: paddle.bitwise_not(Tensor([20, 3, 3, 70561, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 3, 70561, 4, 1, 5, 2],"bool"), ) 	 508039200 	 33833 	 28.68506145477295 	 25.272905826568604 	 0.8664717674255371 	 0.7634851932525635 	 None 	 None 	 None 	 None 	 
2025-08-06 06:13:09.795275 test begin: paddle.bitwise_not(Tensor([20, 3, 70561, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 3, 70561, 3, 4, 1, 5, 2],"bool"), ) 	 508039200 	 33833 	 28.683438539505005 	 25.272587299346924 	 0.866485595703125 	 0.7634422779083252 	 None 	 None 	 None 	 None 	 
2025-08-06 06:14:10.794934 test begin: paddle.bitwise_not(Tensor([20, 70561, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([20, 70561, 3, 3, 4, 1, 5, 2],"bool"), ) 	 508039200 	 33833 	 28.68333101272583 	 25.272687911987305 	 0.8664431571960449 	 0.7634241580963135 	 None 	 None 	 None 	 None 	 
2025-08-06 06:15:11.731057 test begin: paddle.bitwise_not(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50804280 	 33833 	 10.006044149398804 	 10.071447134017944 	 0.3022181987762451 	 0.3041999340057373 	 None 	 None 	 None 	 None 	 
2025-08-06 06:15:32.426274 test begin: paddle.bitwise_not(Tensor([470410, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([470410, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 508042800 	 33833 	 29.011618852615356 	 25.303861141204834 	 0.8763523101806641 	 0.7641017436981201 	 None 	 None 	 None 	 None 	 
2025-08-06 06:16:33.737919 test begin: paddle.bitwise_not(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101607480 	 33833 	 10.063422918319702 	 10.03173041343689 	 0.30398106575012207 	 0.3025381565093994 	 None 	 None 	 None 	 None 	 
2025-08-06 06:16:56.631698 test begin: paddle.bitwise_or(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84965 	 38.08409595489502 	 38.240500688552856 	 0.45812225341796875 	 0.4596748352050781 	 None 	 None 	 None 	 None 	 
2025-08-06 06:18:17.698977 test begin: paddle.bitwise_or(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84965 	 38.088056325912476 	 38.21693754196167 	 0.4580814838409424 	 0.45966315269470215 	 None 	 None 	 None 	 None 	 
2025-08-06 06:19:39.541201 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84965 	 38.09038782119751 	 38.21762180328369 	 0.4583139419555664 	 0.4597148895263672 	 None 	 None 	 None 	 None 	 
2025-08-06 06:20:57.885160 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), ) 	 203213880 	 84965 	 38.09276366233826 	 38.22391486167908 	 0.4580051898956299 	 0.45973730087280273 	 None 	 None 	 None 	 None 	 
2025-08-06 06:22:17.833040 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), ) 	 101607264 	 84965 	 10.027533292770386 	 9.94542121887207 	 0.12060236930847168 	 0.11958670616149902 	 None 	 None 	 None 	 None 	 
2025-08-06 06:22:39.207477 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), ) 	 101607264 	 84965 	 38.20881748199463 	 37.89648103713989 	 0.45967650413513184 	 0.45581865310668945 	 None 	 None 	 None 	 None 	 
2025-08-06 06:23:59.619000 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), ) 	 203213664 	 84965 	 38.0867223739624 	 40.43535876274109 	 0.45818328857421875 	 0.4595828056335449 	 None 	 None 	 None 	 None 	 
2025-08-06 06:25:21.831922 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 50807520 	 84965 	 13.213979721069336 	 19.34403944015503 	 0.1589336395263672 	 0.23238182067871094 	 None 	 None 	 None 	 None 	 
2025-08-06 06:25:58.205231 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 101610720 	 84965 	 26.81130886077881 	 40.77113723754883 	 0.322479248046875 	 0.4904325008392334 	 None 	 None 	 None 	 None 	 
2025-08-06 06:27:06.872735 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 50807520 	 84965 	 25.15800166130066 	 26.18155074119568 	 0.3026008605957031 	 0.31490516662597656 	 None 	 None 	 None 	 None 	 
2025-08-06 06:28:00.090918 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), ) 	 101608560 	 84965 	 10.024226665496826 	 9.844926834106445 	 0.12058877944946289 	 0.11841392517089844 	 None 	 None 	 None 	 None 	 
2025-08-06 06:28:21.425307 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), ) 	 101608560 	 84965 	 38.21612095832825 	 37.89647459983826 	 0.45962977409362793 	 0.45583033561706543 	 None 	 None 	 None 	 None 	 
2025-08-06 06:29:40.158747 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), ) 	 203214960 	 84965 	 38.09292387962341 	 38.21720767021179 	 0.45807886123657227 	 0.45963549613952637 	 None 	 None 	 None 	 None 	 
2025-08-06 06:30:58.688630 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 50807520 	 84965 	 14.73747992515564 	 19.267354011535645 	 0.1772751808166504 	 0.23161745071411133 	 None 	 None 	 None 	 None 	 
2025-08-06 06:31:33.446224 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 101610720 	 84965 	 10.024944305419922 	 9.865548610687256 	 0.12058830261230469 	 0.11857366561889648 	 None 	 None 	 None 	 None 	 
2025-08-06 06:31:54.810867 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50807520 	 84965 	 25.122642040252686 	 26.171464920043945 	 0.3021883964538574 	 0.314802885055542 	 None 	 None 	 None 	 None 	 
2025-08-06 06:32:46.748046 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 101610720 	 84965 	 38.214186668395996 	 37.89918494224548 	 0.4596714973449707 	 0.45583248138427734 	 None 	 None 	 None 	 None 	 
2025-08-06 06:34:04.686862 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101610720 	 84965 	 29.306852340698242 	 40.65369653701782 	 0.3525416851043701 	 0.4888627529144287 	 None 	 None 	 None 	 None 	 
2025-08-06 06:35:15.698980 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 203217120 	 84965 	 38.0959153175354 	 38.21826529502869 	 0.4580361843109131 	 0.45972323417663574 	 None 	 None 	 None 	 None 	 
2025-08-06 06:36:34.065380 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), ) 	 101607480 	 84965 	 9.998596906661987 	 9.77621579170227 	 0.12026429176330566 	 0.1174170970916748 	 None 	 None 	 None 	 None 	 
2025-08-06 06:36:58.451391 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), ) 	 101607480 	 84965 	 38.21797227859497 	 37.89783787727356 	 0.4597053527832031 	 0.45583534240722656 	 None 	 None 	 None 	 None 	 
2025-08-06 06:38:17.109485 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84965 	 9.998823165893555 	 9.76490569114685 	 0.12024188041687012 	 0.11744022369384766 	 None 	 None 	 None 	 None 	 
2025-08-06 06:38:39.206755 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84965 	 38.211556911468506 	 38.637173891067505 	 0.45960140228271484 	 0.45573878288269043 	 None 	 None 	 None 	 None 	 
2025-08-06 06:39:59.394502 test begin: paddle.bitwise_or(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84965 	 9.998685359954834 	 9.76493787765503 	 0.12025022506713867 	 0.11744213104248047 	 None 	 None 	 None 	 None 	 
2025-08-06 06:40:20.656824 test begin: paddle.bitwise_or(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84965 	 38.215548515319824 	 37.899505376815796 	 0.45967912673950195 	 0.45587944984436035 	 None 	 None 	 None 	 None 	 
2025-08-06 06:41:41.338799 test begin: paddle.bitwise_or(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84965 	 9.998569965362549 	 9.766165971755981 	 0.1202704906463623 	 0.11750984191894531 	 None 	 None 	 None 	 None 	 
2025-08-06 06:42:02.534631 test begin: paddle.bitwise_or(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84965 	 38.21657204627991 	 37.90433359146118 	 0.45972728729248047 	 0.4558522701263428 	 None 	 None 	 None 	 None 	 
2025-08-06 06:43:21.098051 test begin: paddle.bitwise_or(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101608560 	 84965 	 10.02439260482788 	 9.845746040344238 	 0.1205596923828125 	 0.11841893196105957 	 None 	 None 	 None 	 None 	 
2025-08-06 06:43:44.085479 test begin: paddle.bitwise_or(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101608560 	 84965 	 38.21170353889465 	 37.894126415252686 	 0.45957183837890625 	 0.455782413482666 	 None 	 None 	 None 	 None 	 
2025-08-06 06:45:01.458718 test begin: paddle.bitwise_or(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214960 	 84965 	 38.08836221694946 	 38.241862773895264 	 0.45801544189453125 	 0.45967626571655273 	 None 	 None 	 None 	 None 	 
2025-08-06 06:46:22.974359 test begin: paddle.bitwise_right_shift(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), ) 	 101607000 	 22351 	 10.053364992141724 	 9.999420166015625 	 0.4597351551055908 	 0.4558393955230713 	 None 	 None 	 None 	 None 	 combined
2025-08-06 06:46:44.314404 test begin: paddle.bitwise_right_shift(Tensor([200, 127009],"int64"), Tensor([200, 127009],"int64"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([200, 127009],"int64"), Tensor([200, 127009],"int64"), ) 	 50803600 	 22351 	 10.015839576721191 	 9.958174467086792 	 0.4580352306365967 	 0.4553196430206299 	 None 	 None 	 None 	 None 	 combined
2025-08-06 06:47:05.218510 test begin: paddle.bitwise_right_shift(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), ) 	 101606800 	 22351 	 10.052972316741943 	 9.969011306762695 	 0.4597921371459961 	 0.4558067321777344 	 None 	 None 	 None 	 None 	 combined
2025-08-06 06:47:27.969730 test begin: paddle.bitwise_right_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), ) 	 203213200 	 22351 	 9.998899698257446 	 10.355318069458008 	 0.4573380947113037 	 0.4596130847930908 	 None 	 None 	 None 	 None 	 combined
2025-08-06 06:47:52.574705 test begin: paddle.bitwise_right_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), ) 	 203213400 	 22351 	 9.996761322021484 	 10.050055503845215 	 0.4572873115539551 	 0.4595060348510742 	 None 	 None 	 None 	 None 	 combined
2025-08-06 06:48:14.796479 test begin: paddle.bitwise_right_shift(Tensor([84673, 300],"int64"), Tensor([84673, 300],"int64"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([84673, 300],"int64"), Tensor([84673, 300],"int64"), ) 	 50803800 	 22351 	 10.016871690750122 	 9.958204984664917 	 0.4581012725830078 	 0.45534205436706543 	 None 	 None 	 None 	 None 	 combined
2025-08-06 06:48:37.228994 test begin: paddle.bitwise_xor(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84955 	 38.07918047904968 	 38.210973501205444 	 0.45829176902770996 	 0.4596445560455322 	 None 	 None 	 None 	 None 	 
2025-08-06 06:49:57.606785 test begin: paddle.bitwise_xor(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84955 	 38.07925510406494 	 39.907081604003906 	 0.45812010765075684 	 0.45974159240722656 	 None 	 None 	 None 	 None 	 
2025-08-06 06:51:20.988667 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), ) 	 203214240 	 84955 	 38.08346343040466 	 38.21085810661316 	 0.4581751823425293 	 0.45969223976135254 	 None 	 None 	 None 	 None 	 
2025-08-06 06:52:39.946742 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), ) 	 203213880 	 84955 	 38.09015130996704 	 38.217796087265015 	 0.4585258960723877 	 0.45970940589904785 	 None 	 None 	 None 	 None 	 
2025-08-06 06:54:01.033293 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), ) 	 101607264 	 84955 	 10.025844812393188 	 9.964064121246338 	 0.12061738967895508 	 0.11986303329467773 	 None 	 None 	 None 	 None 	 
2025-08-06 06:54:22.528199 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), ) 	 101607264 	 84955 	 38.20145511627197 	 37.89447259902954 	 0.4595930576324463 	 0.45585060119628906 	 None 	 None 	 None 	 None 	 
2025-08-06 06:55:43.210051 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), ) 	 203213664 	 84955 	 38.07844543457031 	 38.20766019821167 	 0.4579465389251709 	 0.45966506004333496 	 None 	 None 	 None 	 None 	 
2025-08-06 06:57:03.784672 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 50807520 	 84955 	 14.306312084197998 	 19.310238122940063 	 0.1720890998840332 	 0.23229765892028809 	 None 	 None 	 None 	 None 	 
2025-08-06 06:57:38.894256 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 101610720 	 84955 	 26.806216716766357 	 40.75076389312744 	 0.32238245010375977 	 0.49065732955932617 	 None 	 None 	 None 	 None 	 
2025-08-06 06:58:49.377672 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 50807520 	 84955 	 25.15460753440857 	 26.201834201812744 	 0.30260396003723145 	 0.3149077892303467 	 None 	 None 	 None 	 None 	 
2025-08-06 06:59:44.728518 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), ) 	 101608560 	 84955 	 10.023061752319336 	 9.865849256515503 	 0.12058687210083008 	 0.11867070198059082 	 None 	 None 	 None 	 None 	 
2025-08-06 07:00:06.067854 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), ) 	 101608560 	 84955 	 38.20837616920471 	 37.894681215286255 	 0.45969676971435547 	 0.4557797908782959 	 None 	 None 	 None 	 None 	 
2025-08-06 07:01:23.423398 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), ) 	 203214960 	 84955 	 38.08722424507141 	 38.21055746078491 	 0.45854783058166504 	 0.459686279296875 	 None 	 None 	 None 	 None 	 
2025-08-06 07:02:44.761358 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 50807520 	 84955 	 15.014361143112183 	 19.29962992668152 	 0.18062996864318848 	 0.23218655586242676 	 None 	 None 	 None 	 None 	 
2025-08-06 07:03:19.828121 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 101610720 	 84955 	 10.023514986038208 	 9.878319501876831 	 0.12060904502868652 	 0.11882209777832031 	 None 	 None 	 None 	 None 	 
2025-08-06 07:03:43.078944 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50807520 	 84955 	 25.120288372039795 	 26.172382354736328 	 0.3022019863128662 	 0.31485795974731445 	 None 	 None 	 None 	 None 	 
2025-08-06 07:04:39.591569 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 101610720 	 84955 	 38.20740103721619 	 37.90966987609863 	 0.45964479446411133 	 0.45581483840942383 	 None 	 None 	 None 	 None 	 
2025-08-06 07:05:59.732622 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101610720 	 84955 	 29.316170930862427 	 40.62908411026001 	 0.35237932205200195 	 0.48870277404785156 	 None 	 None 	 None 	 None 	 
2025-08-06 07:07:12.207091 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 203217120 	 84955 	 38.08858370780945 	 38.211185932159424 	 0.4585435390472412 	 0.45958542823791504 	 None 	 None 	 None 	 None 	 
2025-08-06 07:08:30.737821 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), ) 	 101607480 	 84955 	 9.997379541397095 	 9.80208420753479 	 0.1202695369720459 	 0.1178290843963623 	 None 	 None 	 None 	 None 	 
2025-08-06 07:08:56.330060 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), ) 	 101607480 	 84955 	 38.21087861061096 	 37.936333656311035 	 0.4596576690673828 	 0.455913782119751 	 None 	 None 	 None 	 None 	 
2025-08-06 07:10:18.446791 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84955 	 9.997512578964233 	 9.798185110092163 	 0.12027573585510254 	 0.1177835464477539 	 None 	 None 	 None 	 None 	 
2025-08-06 07:10:40.285210 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84955 	 38.20923566818237 	 38.68718600273132 	 0.45973920822143555 	 0.4558126926422119 	 None 	 None 	 None 	 None 	 
2025-08-06 07:12:01.650071 test begin: paddle.bitwise_xor(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84955 	 9.997232913970947 	 9.801645278930664 	 0.12025976181030273 	 0.11785006523132324 	 None 	 None 	 None 	 None 	 
2025-08-06 07:12:22.887386 test begin: paddle.bitwise_xor(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84955 	 38.208749532699585 	 37.8906934261322 	 0.4596669673919678 	 0.45585155487060547 	 None 	 None 	 None 	 None 	 
2025-08-06 07:13:44.950841 test begin: paddle.bitwise_xor(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 84955 	 9.997291326522827 	 9.82372522354126 	 0.12024784088134766 	 0.11788177490234375 	 None 	 None 	 None 	 None 	 
2025-08-06 07:14:06.521043 test begin: paddle.bitwise_xor(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 84955 	 38.20857071876526 	 37.89922475814819 	 0.45956921577453613 	 0.4558095932006836 	 None 	 None 	 None 	 None 	 
2025-08-06 07:15:23.881884 test begin: paddle.bitwise_xor(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101608560 	 84955 	 10.02314305305481 	 9.882898569107056 	 0.12060165405273438 	 0.11864876747131348 	 None 	 None 	 None 	 None 	 
2025-08-06 07:15:49.246722 test begin: paddle.bitwise_xor(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101608560 	 84955 	 38.20929741859436 	 37.891183376312256 	 0.45970916748046875 	 0.45584750175476074 	 None 	 None 	 None 	 None 	 
2025-08-06 07:17:06.593179 test begin: paddle.bitwise_xor(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214960 	 84955 	 38.091776609420776 	 38.224554777145386 	 0.45816493034362793 	 0.45973992347717285 	 None 	 None 	 None 	 None 	 
2025-08-06 07:18:24.970150 test begin: paddle.bmm(Tensor([112, 1043, 435],"float32"), Tensor([112, 435, 64],"float32"), )
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.bmm 	 paddle.bmm(Tensor([112, 1043, 435],"float32"), Tensor([112, 435, 64],"float32"), ) 	 53933040 	 12079 	 10.799654006958008 	 10.800562620162964 	 0.913813591003418 	 0.9137210845947266 	 19.43024206161499 	 19.431280851364136 	 0.8218808174133301 	 0.8219847679138184 	 
2025-08-06 07:19:26.844120 test begin: paddle.bmm(Tensor([112, 435, 435],"float32"), Tensor([112, 435, 1043],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([112, 435, 435],"float32"), Tensor([112, 435, 1043],"float32"), ) 	 72008160 	 12079 	 40.693347454071045 	 40.698134899139404 	 3.4430370330810547 	 3.443573236465454 	 83.92351508140564 	 83.93549108505249 	 3.550563335418701 	 3.5508265495300293 	 
2025-08-06 07:23:39.765633 test begin: paddle.bmm(Tensor([14, 81, 7332],"float32"), Tensor([14, 7332, 512],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([14, 81, 7332],"float32"), Tensor([14, 7332, 512],"float32"), ) 	 60870264 	 12079 	 17.721032857894897 	 17.719224452972412 	 1.4993796348571777 	 1.4992303848266602 	 17.21295142173767 	 17.213005542755127 	 0.7281675338745117 	 0.7281157970428467 	 
2025-08-06 07:24:50.701099 test begin: paddle.bmm(Tensor([1825, 435, 435],"float32"), Tensor([1825, 435, 64],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([1825, 435, 435],"float32"), Tensor([1825, 435, 64],"float32"), ) 	 396143625 	 12079 	 72.68674802780151 	 72.6763687133789 	 6.150009870529175 	 6.148695468902588 	 120.19394373893738 	 120.20264077186584 	 5.0842273235321045 	 5.087347745895386 	 
2025-08-06 07:31:23.921312 test begin: paddle.bmm(Tensor([26, 1024, 1024],"float32"), Tensor([26, 1024, 1909],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([26, 1024, 1024],"float32"), Tensor([26, 1024, 1909],"float32"), ) 	 78088192 	 12079 	 71.76284050941467 	 71.7670545578003 	 6.071967363357544 	 6.07239031791687 	 145.82125210762024 	 145.80646347999573 	 6.168895483016968 	 6.168254613876343 	 
2025-08-06 07:38:41.918509 test begin: paddle.bmm(Tensor([26, 1909, 1024],"float32"), Tensor([26, 1024, 12],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([26, 1909, 1024],"float32"), Tensor([26, 1024, 12],"float32"), ) 	 51144704 	 12079 	 9.998726844787598 	 10.000047206878662 	 0.8460357189178467 	 0.8460938930511475 	 11.310456991195679 	 11.311744451522827 	 0.4784715175628662 	 0.4785192012786865 	 
2025-08-06 07:39:25.437412 test begin: paddle.bmm(Tensor([269, 435, 435],"float32"), Tensor([269, 435, 64],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([269, 435, 435],"float32"), Tensor([269, 435, 64],"float32"), ) 	 58390485 	 12079 	 10.818794012069702 	 10.820199489593506 	 0.9154057502746582 	 0.9152677059173584 	 17.944430828094482 	 17.946455001831055 	 0.7591209411621094 	 0.7591795921325684 	 
2025-08-06 07:40:27.214236 test begin: paddle.bmm(Tensor([4, 1733, 7332],"float32"), Tensor([4, 7332, 512],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([4, 1733, 7332],"float32"), Tensor([4, 7332, 512],"float32"), ) 	 65841360 	 12079 	 52.972251415252686 	 52.9790403842926 	 4.4820640087127686 	 4.4821038246154785 	 76.50323581695557 	 76.49924874305725 	 3.2363743782043457 	 3.2361178398132324 	 
2025-08-06 07:44:48.225756 test begin: paddle.bmm(Tensor([4, 81, 156801],"float32"), Tensor([4, 156801, 512],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f769693a920>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 07:55:27.447310 test begin: paddle.bmm(Tensor([4, 81, 24807],"float32"), Tensor([4, 24807, 512],"float32"), )
W0806 07:55:29.203410 130615 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.bmm 	 paddle.bmm(Tensor([4, 81, 24807],"float32"), Tensor([4, 24807, 512],"float32"), ) 	 58842204 	 12079 	 59.634785890579224 	 59.65250873565674 	 5.045742511749268 	 5.047307014465332 	 16.800830364227295 	 16.80269193649292 	 0.7109873294830322 	 0.7110714912414551 	 
2025-08-06 07:58:05.578189 test begin: paddle.bmm(Tensor([4, 81, 7332],"float32"), Tensor([4, 7332, 1733],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([4, 81, 7332],"float32"), Tensor([4, 7332, 1733],"float32"), ) 	 53200992 	 12079 	 17.720556497573853 	 17.72277307510376 	 1.4993584156036377 	 1.4995112419128418 	 19.847232818603516 	 19.85548710823059 	 0.8400511741638184 	 0.8398802280426025 	 
2025-08-06 07:59:21.777783 test begin: paddle.bmm(Tensor([49, 1024, 1024],"float32"), Tensor([49, 1024, 12],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([49, 1024, 1024],"float32"), Tensor([49, 1024, 12],"float32"), ) 	 51982336 	 12079 	 9.999417304992676 	 9.998769760131836 	 0.8459453582763672 	 0.8459939956665039 	 11.972332954406738 	 11.970398902893066 	 0.506479024887085 	 0.5063762664794922 	 
2025-08-06 08:00:06.740243 test begin: paddle.bmm(Tensor([86, 81, 7332],"float32"), Tensor([86, 7332, 512],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([86, 81, 7332],"float32"), Tensor([86, 7332, 512],"float32"), ) 	 373917336 	 12079 	 70.52449059486389 	 70.52220511436462 	 5.967008829116821 	 5.966933012008667 	 101.01630878448486 	 101.02014112472534 	 4.273372650146484 	 4.274878740310669 	 
2025-08-06 08:05:57.132867 test begin: paddle.broadcast_tensors(list[Tensor([127009, 200],"float64"),Tensor([127009, 200],"float64"),], )
[Prof] paddle.broadcast_tensors 	 paddle.broadcast_tensors(list[Tensor([127009, 200],"float64"),Tensor([127009, 200],"float64"),], ) 	 50803600 	 16272 	 9.993695497512817 	 0.11878061294555664 	 0.3137989044189453 	 5.9604644775390625e-05 	 10.181323766708374 	 1.085355520248413 	 0.15983796119689941 	 9.72747802734375e-05 	 
2025-08-06 08:06:21.027376 test begin: paddle.broadcast_tensors(list[Tensor([200, 127009],"float64"),Tensor([200, 127009],"float64"),], )
[Prof] paddle.broadcast_tensors 	 paddle.broadcast_tensors(list[Tensor([200, 127009],"float64"),Tensor([200, 127009],"float64"),], ) 	 50803600 	 16272 	 9.99269723892212 	 0.16897058486938477 	 0.31381845474243164 	 0.0002429485321044922 	 10.181256771087646 	 1.355250358581543 	 0.1598224639892578 	 0.0002238750457763672 	 
2025-08-06 08:06:45.254050 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([12700801],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7eff4aac30d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:16:50.851164 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([12700801],"float64"), out_int32=True, )
W0806 08:16:51.708117 131532 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7e21fceda0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:26:55.451487 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([12700801],"float64"), right=True, )
W0806 08:26:56.371109 131891 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2279556f80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:37:00.204514 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([4],"float64"), )
W0806 08:37:01.246140 132335 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4737d12ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:47:04.887610 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([4],"float64"), out_int32=True, )
W0806 08:47:05.806391 132728 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([4],"float64"), out_int32=True, ) 	 25401606 	 957494 	 270.99696373939514 	 234.68762230873108 	 0.2891976833343506 	 0.2505345344543457 	 None 	 None 	 None 	 None 	 
2025-08-06 08:55:39.688528 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([4],"float64"), right=True, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd9922f6a10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:05:47.602915 test begin: paddle.bucketize(Tensor([2, 25401601],"float64"), Tensor([25401601],"float64"), )
W0806 09:05:49.204285 133483 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f49eceb30d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:15:52.185498 test begin: paddle.bucketize(Tensor([2, 25401601],"float64"), Tensor([25401601],"float64"), out_int32=True, )
W0806 09:15:53.740461 133765 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe9a268af80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:25:57.719155 test begin: paddle.bucketize(Tensor([2, 25401601],"float64"), Tensor([25401601],"float64"), right=True, )
W0806 09:25:59.375608 134137 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f71d7ac2f80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:36:02.250492 test begin: paddle.bucketize(Tensor([2, 4],"float64"), Tensor([2540160101],"float64"), )
W0806 09:36:49.348150 134517 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 4],"float64"), Tensor([2540160101],"float64"), ) 	 2540160109 	 957494 	 10.010185718536377 	 10.560565710067749 	 0.0001380443572998047 	 7.700920104980469e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 09:37:24.566788 test begin: paddle.bucketize(Tensor([2, 4],"float64"), Tensor([2540160101],"float64"), right=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 4],"float64"), Tensor([2540160101],"float64"), right=True, ) 	 2540160109 	 957494 	 17.71621084213257 	 16.198241710662842 	 4.220008850097656e-05 	 0.00024437904357910156 	 None 	 None 	 None 	 None 	 
2025-08-06 09:39:08.036899 test begin: paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), ) 	 25402405 	 957494 	 13.414559364318848 	 9.964927673339844 	 0.00012493133544921875 	 0.0002353191375732422 	 None 	 None 	 None 	 None 	 
2025-08-06 09:39:32.167474 test begin: paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), out_int32=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), out_int32=True, ) 	 25402405 	 957494 	 10.49816608428955 	 10.20686411857605 	 0.00010418891906738281 	 0.00023627281188964844 	 None 	 None 	 None 	 None 	 
2025-08-06 09:39:54.344396 test begin: paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), right=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([201, 4],"float64"), Tensor([25401601],"float64"), right=True, ) 	 25402405 	 957494 	 10.335874319076538 	 10.136473178863525 	 0.00010943412780761719 	 0.0002357959747314453 	 None 	 None 	 None 	 None 	 
2025-08-06 09:40:15.377513 test begin: paddle.bucketize(Tensor([6350401, 4],"float64"), Tensor([4],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f155757ae00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:50:19.776363 test begin: paddle.bucketize(Tensor([6350401, 4],"float64"), Tensor([4],"float64"), out_int32=True, )
W0806 09:50:20.463284 135135 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([6350401, 4],"float64"), Tensor([4],"float64"), out_int32=True, ) 	 25401608 	 957494 	 270.8077414035797 	 234.69669651985168 	 0.28901124000549316 	 0.25051045417785645 	 None 	 None 	 None 	 None 	 
2025-08-06 09:58:49.085008 test begin: paddle.bucketize(Tensor([6350401, 4],"float64"), Tensor([4],"float64"), right=True, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7feb0c77eb30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:08:53.429568 test begin: paddle.cartesian_prod(list[Tensor([20],"complex128"),Tensor([50],"complex128"),Tensor([5080],"complex128"),], )
W0806 10:08:53.634099 135959 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f364dd5ef80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:18:57.951531 test begin: paddle.cartesian_prod(list[Tensor([30],"complex128"),Tensor([30],"complex128"),Tensor([5080],"complex128"),], )
W0806 10:18:58.211074 136485 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f40e9b72d40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:29:02.552257 test begin: paddle.cartesian_prod(list[Tensor([40],"int32"),Tensor([30],"int32"),Tensor([5080],"int32"),], )
W0806 10:29:02.751753 137090 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb5578d2f80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:39:07.164322 test begin: paddle.cartesian_prod(list[Tensor([40],"int32"),Tensor([400],"int32"),Tensor([508],"int32"),], )
W0806 10:39:07.375463 137707 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6b7c916f80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:49:11.782290 test begin: paddle.cast(Tensor([1, 1, 32768, 32768],"float16"), dtype=Dtype(float16), )
W0806 10:49:31.796175 138328 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cast 	 paddle.cast(Tensor([1, 1, 32768, 32768],"float16"), dtype=Dtype(float16), ) 	 1073741824 	 33565 	 108.59959387779236 	 0.07270669937133789 	 1.6534318923950195 	 3.0994415283203125e-05 	 108.52683210372925 	 1.6182825565338135 	 1.652245283126831 	 7.62939453125e-05 	 combined
2025-08-06 10:53:35.579711 test begin: paddle.cast(Tensor([128256, 793],"bfloat16"), Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([128256, 793],"bfloat16"), Dtype(float16), ) 	 101707008 	 33565 	 10.020561933517456 	 17.12592124938965 	 0.30509471893310547 	 0.5214090347290039 	 10.001586437225342 	 16.981935501098633 	 0.30451321601867676 	 0.5170624256134033 	 combined
2025-08-06 10:54:34.384255 test begin: paddle.cast(Tensor([2, 1, 1551, 32768],"float16"), dtype=Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([2, 1, 1551, 32768],"float16"), dtype=Dtype(float16), ) 	 101646336 	 33565 	 10.409517526626587 	 0.0661160945892334 	 0.31699037551879883 	 0.0001819133758544922 	 10.406535625457764 	 1.5714163780212402 	 0.3168618679046631 	 7.605552673339844e-05 	 combined
2025-08-06 10:55:00.621533 test begin: paddle.cast(Tensor([2, 1, 32768, 1551],"float16"), dtype=Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([2, 1, 32768, 1551],"float16"), dtype=Dtype(float16), ) 	 101646336 	 33565 	 10.409189462661743 	 0.06903266906738281 	 0.31692957878112793 	 7.939338684082031e-05 	 10.406277894973755 	 1.6200814247131348 	 0.3168799877166748 	 7.748603820800781e-05 	 combined
2025-08-06 10:55:27.000261 test begin: paddle.cast(Tensor([2, 1, 32768, 32768],"float16"), dtype=Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([2, 1, 32768, 32768],"float16"), dtype=Dtype(float16), ) 	 2147483648 	 33565 	 217.12351822853088 	 0.10053563117980957 	 3.3055813312530518 	 3.647804260253906e-05 	 217.25667333602905 	 1.6879265308380127 	 3.3076796531677246 	 7.319450378417969e-05 	 combined
2025-08-06 11:04:08.298537 test begin: paddle.cast(Tensor([2, 1024, 50304],"float16"), dtype="float32", )
[Prof] paddle.cast 	 paddle.cast(Tensor([2, 1024, 50304],"float16"), dtype="float32", ) 	 103022592 	 33565 	 16.392637252807617 	 18.663077116012573 	 0.4990377426147461 	 0.5681970119476318 	 15.342498302459717 	 15.429482460021973 	 0.4671659469604492 	 0.46979284286499023 	 combined
2025-08-06 11:05:18.214798 test begin: paddle.cast(Tensor([33076, 3072],"bfloat16"), Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([33076, 3072],"bfloat16"), Dtype(float16), ) 	 101609472 	 33565 	 9.999820470809937 	 17.11229395866394 	 0.3045158386230469 	 0.5206732749938965 	 9.99539589881897 	 16.966779708862305 	 0.30431032180786133 	 0.5165863037109375 	 combined
2025-08-06 11:06:18.087323 test begin: paddle.cast(Tensor([8, 1024, 12404],"float16"), dtype="float32", )
[Prof] paddle.cast 	 paddle.cast(Tensor([8, 1024, 12404],"float16"), dtype="float32", ) 	 101613568 	 33565 	 16.170721530914307 	 18.412967205047607 	 0.4924039840698242 	 0.5606153011322021 	 15.129942178726196 	 15.220309734344482 	 0.46080899238586426 	 0.4634125232696533 	 combined
2025-08-06 11:07:26.606837 test begin: paddle.cast(Tensor([8, 253, 50304],"float16"), dtype="float32", )
[Prof] paddle.cast 	 paddle.cast(Tensor([8, 253, 50304],"float16"), dtype="float32", ) 	 101815296 	 33565 	 16.192075967788696 	 18.442177295684814 	 0.4930698871612549 	 0.5615274906158447 	 15.152796268463135 	 15.25272250175476 	 0.4612758159637451 	 0.46429443359375 	 combined
2025-08-06 11:08:35.200813 test begin: paddle.cdist(Tensor([12700801, 4],"float32"), Tensor([1, 4],"float32"), p=1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff8db54a7a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 11:19:06.773155 test begin: paddle.cdist(Tensor([6380, 7963],"float32"), Tensor([1, 7963],"float32"), p=1, )
W0806 11:19:07.719132 139654 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cdist 	 paddle.cdist(Tensor([6380, 7963],"float32"), Tensor([1, 7963],"float32"), p=1, ) 	 50811903 	 22208 	 9.999826669692993 	 4.305097818374634 	 0.23006820678710938 	 0.19820022583007812 	 48.52445912361145 	 26.097289085388184 	 0.7430853843688965 	 0.23996210098266602 	 
2025-08-06 11:20:39.158652 test begin: paddle.cdist(Tensor([8550, 5942],"float32"), Tensor([1, 5942],"float32"), p=1, )
[Prof] paddle.cdist 	 paddle.cdist(Tensor([8550, 5942],"float32"), Tensor([1, 5942],"float32"), p=1, ) 	 50810042 	 22208 	 10.028880596160889 	 4.292240381240845 	 0.23062562942504883 	 0.19753766059875488 	 48.59128665924072 	 25.808866500854492 	 0.7441573143005371 	 0.23722529411315918 	 
2025-08-06 11:22:14.288205 test begin: paddle.cdist(Tensor([900, 56449],"float32"), Tensor([1, 56449],"float32"), p=1, )
[Prof] paddle.cdist 	 paddle.cdist(Tensor([900, 56449],"float32"), Tensor([1, 56449],"float32"), p=1, ) 	 50860549 	 22208 	 10.608880519866943 	 6.514864921569824 	 0.16262459754943848 	 0.299924373626709 	 47.77234387397766 	 26.96576166152954 	 0.7317090034484863 	 0.31020283699035645 	 
2025-08-06 11:23:47.114053 test begin: paddle.ceil(Tensor([12404, 32, 128],"float32"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([12404, 32, 128],"float32"), ) 	 50806784 	 33827 	 10.011791229248047 	 10.072479486465454 	 0.30251502990722656 	 0.3041818141937256 	 4.5352818965911865 	 4.5331032276153564 	 0.13690733909606934 	 0.13686776161193848 	 
2025-08-06 11:24:18.141126 test begin: paddle.ceil(Tensor([141121, 6, 3, 1, 2, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([141121, 6, 3, 1, 2, 5],"float64"), ) 	 25401780 	 33827 	 10.082406282424927 	 10.087162494659424 	 0.30446887016296387 	 0.30458617210388184 	 4.529568672180176 	 4.5461812019348145 	 0.13680672645568848 	 0.13726043701171875 	 
2025-08-06 11:24:48.513345 test begin: paddle.ceil(Tensor([3, 141121, 3, 4, 1, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 141121, 3, 4, 1, 5],"float64"), ) 	 25401780 	 33827 	 10.079334735870361 	 11.081122159957886 	 0.304485559463501 	 0.304685115814209 	 4.532505989074707 	 4.545494794845581 	 0.13678956031799316 	 0.13728976249694824 	 
2025-08-06 11:25:22.479937 test begin: paddle.ceil(Tensor([3, 282241, 3, 1, 2, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 282241, 3, 1, 2, 5],"float64"), ) 	 25401690 	 33827 	 10.07605242729187 	 10.118264198303223 	 0.3044447898864746 	 0.3046534061431885 	 4.533862590789795 	 4.546390771865845 	 0.1368238925933838 	 0.13730072975158691 	 
2025-08-06 11:25:54.750583 test begin: paddle.ceil(Tensor([3, 6, 141121, 1, 2, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 141121, 1, 2, 5],"float64"), ) 	 25401780 	 33827 	 10.076354265213013 	 10.084400415420532 	 0.30440783500671387 	 0.3046989440917969 	 4.529577016830444 	 4.545675992965698 	 0.13683652877807617 	 0.13735246658325195 	 
2025-08-06 11:26:25.104264 test begin: paddle.ceil(Tensor([3, 6, 3, 1, 2, 235201],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 1, 2, 235201],"float64"), ) 	 25401708 	 33827 	 10.059858798980713 	 10.095862627029419 	 0.3039684295654297 	 0.304624080657959 	 4.532296895980835 	 4.5460755825042725 	 0.13682126998901367 	 0.13731122016906738 	 
2025-08-06 11:26:59.039661 test begin: paddle.ceil(Tensor([3, 6, 3, 1, 94081, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 1, 94081, 5],"float64"), ) 	 25401870 	 33827 	 10.077346086502075 	 10.0875563621521 	 0.30446839332580566 	 0.30469202995300293 	 4.530915021896362 	 4.54999852180481 	 0.13679933547973633 	 0.13737249374389648 	 
2025-08-06 11:27:32.293468 test begin: paddle.ceil(Tensor([3, 6, 3, 4, 1, 117601],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 4, 1, 117601],"float64"), ) 	 25401816 	 33827 	 10.078996896743774 	 10.084593057632446 	 0.3045334815979004 	 0.30470943450927734 	 4.530329465866089 	 4.545541763305664 	 0.13681745529174805 	 0.13731741905212402 	 
2025-08-06 11:28:04.114356 test begin: paddle.ceil(Tensor([3, 6, 3, 4, 23521, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 4, 23521, 5],"float64"), ) 	 25402680 	 33827 	 10.07542896270752 	 10.084498643875122 	 0.304401159286499 	 0.30469250679016113 	 4.529629468917847 	 4.551390886306763 	 0.13678836822509766 	 0.13729262351989746 	 
2025-08-06 11:28:36.558021 test begin: paddle.ceil(Tensor([3, 6, 3, 47041, 2, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 47041, 2, 5],"float64"), ) 	 25402140 	 33827 	 10.092378616333008 	 10.084697723388672 	 0.304412841796875 	 0.30472254753112793 	 4.529670000076294 	 4.545712947845459 	 0.1367812156677246 	 0.13736391067504883 	 
2025-08-06 11:29:07.084975 test begin: paddle.ceil(Tensor([3, 6, 3, 94081, 1, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 94081, 1, 5],"float64"), ) 	 25401870 	 33827 	 10.077299118041992 	 10.084816455841064 	 0.3044471740722656 	 0.30472230911254883 	 4.530003547668457 	 4.550687551498413 	 0.13680696487426758 	 0.13728952407836914 	 
2025-08-06 11:29:39.441852 test begin: paddle.ceil(Tensor([3, 6, 70561, 4, 1, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 70561, 4, 1, 5],"float64"), ) 	 25401960 	 33827 	 11.312801599502563 	 10.084746360778809 	 0.30399107933044434 	 0.30467820167541504 	 4.530801296234131 	 4.545547246932983 	 0.1367948055267334 	 0.13727807998657227 	 
2025-08-06 11:30:13.609240 test begin: paddle.ceil(Tensor([32, 12404, 128],"float32"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([32, 12404, 128],"float32"), ) 	 50806784 	 33827 	 10.0132474899292 	 10.06785273551941 	 0.302523136138916 	 0.30422019958496094 	 4.532372236251831 	 4.536575794219971 	 0.1368706226348877 	 0.13728094100952148 	 
2025-08-06 11:30:44.497432 test begin: paddle.ceil(Tensor([32, 32, 49613],"float32"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([32, 32, 49613],"float32"), ) 	 50803712 	 33827 	 10.010297060012817 	 10.067333936691284 	 0.3023488521575928 	 0.30414509773254395 	 4.535498857498169 	 4.535592317581177 	 0.13696599006652832 	 0.13683390617370605 	 
2025-08-06 11:31:16.144658 test begin: paddle.ceil(Tensor([70561, 6, 3, 4, 1, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([70561, 6, 3, 4, 1, 5],"float64"), ) 	 25401960 	 33827 	 10.060102462768555 	 10.084226846694946 	 0.30396366119384766 	 0.3047029972076416 	 4.529419183731079 	 4.546687602996826 	 0.13677978515625 	 0.13722705841064453 	 
2025-08-06 11:31:46.675369 test begin: paddle.chunk(Tensor([115, 216, 64, 64],"float16"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([115, 216, 64, 64],"float16"), 3, axis=1, ) 	 101744640 	 29583 	 12.75984525680542 	 0.24310779571533203 	 0.4406602382659912 	 0.0001163482666015625 	 9.129368782043457 	 14.917983770370483 	 0.31546711921691895 	 0.5154471397399902 	 
2025-08-06 11:32:28.579457 test begin: paddle.chunk(Tensor([16, 128, 24807],"float32"), 2, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([16, 128, 24807],"float32"), 2, axis=1, ) 	 50804736 	 29583 	 9.976056098937988 	 0.19341731071472168 	 0.3446033000946045 	 6.914138793945312e-05 	 9.25396466255188 	 9.087858438491821 	 0.31981801986694336 	 0.31400561332702637 	 
2025-08-06 11:32:59.357658 test begin: paddle.chunk(Tensor([16, 128, 25500],"float32"), 2, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([16, 128, 25500],"float32"), 2, axis=1, ) 	 52224000 	 29583 	 10.439448356628418 	 0.19385695457458496 	 0.36063075065612793 	 6.532669067382812e-05 	 9.487761974334717 	 9.323694705963135 	 0.32778215408325195 	 0.321993350982666 	 
2025-08-06 11:33:31.553800 test begin: paddle.chunk(Tensor([4, 216, 1838, 64],"float16"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([4, 216, 1838, 64],"float16"), 3, axis=1, ) 	 101634048 	 29583 	 12.707977533340454 	 0.2286698818206787 	 0.4390532970428467 	 5.841255187988281e-05 	 9.128819465637207 	 14.844353914260864 	 0.31544065475463867 	 0.5126433372497559 	 
2025-08-06 11:34:12.279931 test begin: paddle.chunk(Tensor([4, 216, 64, 1838],"float16"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([4, 216, 64, 1838],"float16"), 3, axis=1, ) 	 101634048 	 29583 	 12.7133629322052 	 0.27655911445617676 	 0.43891000747680664 	 8.630752563476562e-05 	 9.128969669342041 	 14.84061050415039 	 0.3153865337371826 	 0.5126173496246338 	 
2025-08-06 11:34:56.787479 test begin: paddle.chunk(Tensor([4, 216, 64, 919],"float32"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([4, 216, 64, 919],"float32"), 3, axis=1, ) 	 50817024 	 29583 	 9.964338779449463 	 0.23140811920166016 	 0.34421706199645996 	 7.2479248046875e-05 	 9.130089044570923 	 9.018474340438843 	 0.31542253494262695 	 0.31158447265625 	 
2025-08-06 11:35:26.860993 test begin: paddle.chunk(Tensor([4, 216, 919, 64],"float32"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([4, 216, 919, 64],"float32"), 3, axis=1, ) 	 50817024 	 29583 	 9.96346116065979 	 0.22849559783935547 	 0.3442690372467041 	 8.225440979003906e-05 	 9.129812479019165 	 9.018710136413574 	 0.31536126136779785 	 0.31157827377319336 	 
2025-08-06 11:35:57.704477 test begin: paddle.chunk(Tensor([58, 216, 64, 64],"float32"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([58, 216, 64, 64],"float32"), 3, axis=1, ) 	 51314688 	 29583 	 10.184259414672852 	 0.2544097900390625 	 0.3517465591430664 	 9.131431579589844e-05 	 9.197604894638062 	 9.120718240737915 	 0.31769299507141113 	 0.31507229804992676 	 
2025-08-06 11:36:28.389606 test begin: paddle.clip(Tensor([1408, 36082],"float32"), min=-2, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([1408, 36082],"float32"), min=-2, max=2, ) 	 50803456 	 33866 	 10.004370212554932 	 10.079564809799194 	 0.30191826820373535 	 0.3042001724243164 	 15.242249727249146 	 24.859411239624023 	 0.45995545387268066 	 0.15006422996520996 	 
2025-08-06 11:37:30.833929 test begin: paddle.clip(Tensor([2, 3840, 10240],"float32"), 0, 255, )
[Prof] paddle.clip 	 paddle.clip(Tensor([2, 3840, 10240],"float32"), 0, 255, ) 	 78643200 	 33866 	 15.426822900772095 	 15.504403352737427 	 0.46549487113952637 	 0.46786952018737793 	 23.501126050949097 	 37.882941246032715 	 0.7091925144195557 	 0.22871685028076172 	 
2025-08-06 11:39:07.201792 test begin: paddle.clip(Tensor([23, 17, 256, 256],"float64"), min=0, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([23, 17, 256, 256],"float64"), min=0, max=2, ) 	 25624576 	 33866 	 10.181030750274658 	 10.203881978988647 	 0.30724334716796875 	 0.3073432445526123 	 15.327098369598389 	 24.558015823364258 	 0.4626929759979248 	 0.1482682228088379 	 
2025-08-06 11:40:11.291005 test begin: paddle.clip(Tensor([24, 17, 244, 256],"float64"), min=0, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([24, 17, 244, 256],"float64"), min=0, max=2, ) 	 25485312 	 33866 	 10.131207942962646 	 10.15865707397461 	 0.30571508407592773 	 0.3057558536529541 	 15.222457885742188 	 24.428358554840088 	 0.45920372009277344 	 0.14748907089233398 	 
2025-08-06 11:41:13.572427 test begin: paddle.clip(Tensor([24, 17, 256, 244],"float64"), min=0, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([24, 17, 256, 244],"float64"), min=0, max=2, ) 	 25485312 	 33866 	 10.14500093460083 	 10.699140787124634 	 0.3057069778442383 	 0.30576372146606445 	 15.221814393997192 	 24.428783655166626 	 0.4591867923736572 	 0.1475052833557129 	 
2025-08-06 11:42:16.405886 test begin: paddle.clip(Tensor([24, 17, 256, 256],"float64"), min=0, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([24, 17, 256, 256],"float64"), min=0, max=2, ) 	 26738688 	 33866 	 10.62552547454834 	 10.627056121826172 	 0.3206746578216553 	 0.32055091857910156 	 15.965750455856323 	 25.57252860069275 	 0.48159337043762207 	 0.1544194221496582 	 
2025-08-06 11:43:21.542186 test begin: paddle.clip(Tensor([3, 1654, 10240],"float32"), 0, 255, )
[Prof] paddle.clip 	 paddle.clip(Tensor([3, 1654, 10240],"float32"), 0, 255, ) 	 50810880 	 33866 	 10.01431131362915 	 10.081021785736084 	 0.30220556259155273 	 0.3042466640472412 	 15.247063875198364 	 24.816635370254517 	 0.4601278305053711 	 0.14980649948120117 	 
2025-08-06 11:44:23.771705 test begin: paddle.clip(Tensor([3, 3840, 4411],"float32"), 0, 255, )
[Prof] paddle.clip 	 paddle.clip(Tensor([3, 3840, 4411],"float32"), 0, 255, ) 	 50814720 	 33866 	 10.012258768081665 	 10.089477777481079 	 0.30213117599487305 	 0.304201602935791 	 15.249277114868164 	 24.86317801475525 	 0.4601917266845703 	 0.1500709056854248 	 
2025-08-06 11:45:28.014204 test begin: paddle.clip(Tensor([8269, 6144],"float32"), min=-2, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([8269, 6144],"float32"), min=-2, max=2, ) 	 50804736 	 33866 	 10.01366114616394 	 10.098344326019287 	 0.3021988868713379 	 0.3041958808898926 	 15.2456533908844 	 24.811440467834473 	 0.4600818157196045 	 0.14978313446044922 	 
2025-08-06 11:46:31.906663 test begin: paddle.clone(Tensor([145, 12, 112, 261],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([145, 12, 112, 261],"float32"), ) 	 50863680 	 32432 	 10.22387433052063 	 10.164064407348633 	 0.16108036041259766 	 0.1600964069366455 	 10.1607346534729 	 2.038416862487793 	 0.1600954532623291 	 8.225440979003906e-05 	 
2025-08-06 11:47:08.717118 test begin: paddle.clone(Tensor([145, 12, 261, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([145, 12, 261, 112],"float32"), ) 	 50863680 	 32432 	 10.223714828491211 	 10.160548210144043 	 0.16106271743774414 	 0.16005730628967285 	 10.160630702972412 	 1.8553287982940674 	 0.16008853912353516 	 9.512901306152344e-05 	 
2025-08-06 11:47:42.805538 test begin: paddle.clone(Tensor([145, 28, 112, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([145, 28, 112, 112],"float32"), ) 	 50928640 	 32432 	 10.076391220092773 	 10.076834917068481 	 0.317427396774292 	 0.31752586364746094 	 10.07900881767273 	 2.0913944244384766 	 0.31758546829223633 	 8.225440979003906e-05 	 
2025-08-06 11:48:16.943874 test begin: paddle.clone(Tensor([22, 185, 112, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([22, 185, 112, 112],"float32"), ) 	 51054080 	 32432 	 10.193788051605225 	 11.562299966812134 	 0.16059398651123047 	 0.16059327125549316 	 10.198420524597168 	 2.002222776412964 	 0.16066527366638184 	 7.557868957519531e-05 	 
2025-08-06 11:48:54.062311 test begin: paddle.clone(Tensor([22, 64, 112, 323],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([22, 64, 112, 323],"float32"), ) 	 50935808 	 32432 	 10.074483394622803 	 10.078061580657959 	 0.31745147705078125 	 0.3175845146179199 	 10.080783128738403 	 2.009366512298584 	 0.3176109790802002 	 7.748603820800781e-05 	 
2025-08-06 11:49:28.150928 test begin: paddle.clone(Tensor([22, 64, 323, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([22, 64, 323, 112],"float32"), ) 	 50935808 	 32432 	 10.074512720108032 	 10.082173824310303 	 0.3175070285797119 	 0.31755685806274414 	 10.080973148345947 	 2.0399715900421143 	 0.31766653060913086 	 8.082389831542969e-05 	 
2025-08-06 11:50:02.961795 test begin: paddle.clone(Tensor([338, 12, 112, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([338, 12, 112, 112],"float32"), ) 	 50878464 	 32432 	 10.06204891204834 	 11.430366277694702 	 0.3170664310455322 	 0.3172163963317871 	 10.069450855255127 	 2.0541141033172607 	 0.3173201084136963 	 8.296966552734375e-05 	 
2025-08-06 11:50:41.153410 test begin: paddle.clone(Tensor([43, 256, 56, 83],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([43, 256, 56, 83],"float32"), ) 	 51165184 	 32432 	 10.12363314628601 	 10.126932621002197 	 0.31891369819641113 	 0.31895017623901367 	 10.125874996185303 	 2.0397942066192627 	 0.31907057762145996 	 8.440017700195312e-05 	 
2025-08-06 11:51:15.274075 test begin: paddle.clone(Tensor([43, 256, 83, 56],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([43, 256, 83, 56],"float32"), ) 	 51165184 	 32432 	 10.119605302810669 	 10.129035949707031 	 0.31882429122924805 	 0.3189668655395508 	 10.125838041305542 	 2.0516419410705566 	 0.3190743923187256 	 7.915496826171875e-05 	 
2025-08-06 11:51:50.434207 test begin: paddle.clone(Tensor([43, 377, 56, 56],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([43, 377, 56, 56],"float32"), ) 	 50837696 	 32432 	 10.229511737823486 	 10.14096736907959 	 0.16112112998962402 	 0.15977215766906738 	 10.150815725326538 	 1.682649850845337 	 0.15990018844604492 	 0.00010967254638671875 	 
2025-08-06 11:52:24.442201 test begin: paddle.clone(Tensor([64, 256, 56, 56],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([64, 256, 56, 56],"float32"), ) 	 51380224 	 32432 	 10.160529851913452 	 11.165800094604492 	 0.32020020484924316 	 0.3203010559082031 	 10.167349338531494 	 2.0852444171905518 	 0.32032346725463867 	 8.249282836914062e-05 	 
2025-08-06 11:53:01.079168 test begin: paddle.clone(Tensor([64, 64, 112, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([64, 64, 112, 112],"float32"), ) 	 51380224 	 32432 	 10.160552740097046 	 10.167521715164185 	 0.32021641731262207 	 0.3202681541442871 	 10.1658296585083 	 1.6551859378814697 	 0.32033371925354004 	 0.00010585784912109375 	 
2025-08-06 11:53:38.076126 test begin: paddle.column_stack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], ) 	 76204872 	 32438 	 30.077821493148804 	 29.953670263290405 	 0.9473123550415039 	 0.9433615207672119 	 30.109183311462402 	 2.9967708587646484 	 0.9485645294189453 	 8.20159912109375e-05 	 
2025-08-06 11:55:16.156661 test begin: paddle.column_stack(list[Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2, 1058401],"float64"),], ) 	 25401624 	 32438 	 10.22205638885498 	 10.148693799972534 	 0.32193589210510254 	 0.1598215103149414 	 10.084510564804077 	 1.9549870491027832 	 0.3177509307861328 	 0.00010824203491210938 	 
2025-08-06 11:55:50.419079 test begin: paddle.column_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401870 	 32438 	 10.230358839035034 	 10.420025825500488 	 0.32231664657592773 	 0.32837986946105957 	 10.137048959732056 	 2.802144765853882 	 0.31930017471313477 	 9.5367431640625e-05 	 
2025-08-06 11:56:25.280240 test begin: paddle.column_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401870 	 32438 	 10.235224962234497 	 10.406155586242676 	 0.3224782943725586 	 0.3279707431793213 	 10.154008626937866 	 2.8642239570617676 	 0.3199446201324463 	 9.441375732421875e-05 	 
2025-08-06 11:57:00.525834 test begin: paddle.column_stack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], ) 	 76204836 	 32438 	 29.986945867538452 	 29.901848793029785 	 0.9448432922363281 	 0.9419360160827637 	 30.18825912475586 	 3.028905153274536 	 0.9511849880218506 	 8.535385131835938e-05 	 
2025-08-06 11:58:37.912036 test begin: paddle.column_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 25401654 	 32438 	 10.268084049224854 	 10.322980642318726 	 0.32312750816345215 	 0.32505083084106445 	 10.14771556854248 	 2.9397571086883545 	 0.31980204582214355 	 8.416175842285156e-05 	 
2025-08-06 11:59:15.031411 test begin: paddle.column_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401654 	 32438 	 10.23671841621399 	 10.337049722671509 	 0.32251405715942383 	 0.32564425468444824 	 10.145112752914429 	 2.2125391960144043 	 0.31955599784851074 	 8.320808410644531e-05 	 
2025-08-06 11:59:50.734528 test begin: paddle.column_stack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 76204980 	 32438 	 30.005409955978394 	 30.11103081703186 	 0.9456183910369873 	 0.9484307765960693 	 30.198785305023193 	 2.282756805419922 	 0.9513425827026367 	 0.00011396408081054688 	 
2025-08-06 12:01:26.863622 test begin: paddle.column_stack(list[Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401660 	 32438 	 10.195204734802246 	 10.156089782714844 	 0.32120752334594727 	 0.15981721878051758 	 10.097296953201294 	 2.2860982418060303 	 0.31815123558044434 	 0.0002143383026123047 	 
2025-08-06 12:02:01.510938 test begin: paddle.column_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401654 	 32438 	 10.25570297241211 	 10.111173152923584 	 0.3231024742126465 	 0.3186986446380615 	 10.146223068237305 	 3.2850780487060547 	 0.3195383548736572 	 0.00029850006103515625 	 
2025-08-06 12:02:37.817234 test begin: paddle.column_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 76204818 	 32438 	 30.166298866271973 	 30.07427954673767 	 0.9501254558563232 	 0.9473743438720703 	 30.656968116760254 	 2.266639232635498 	 0.9656920433044434 	 7.867813110351562e-05 	 
2025-08-06 12:04:15.710270 test begin: paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401870 	 32438 	 10.26238226890564 	 10.877490758895874 	 0.32335805892944336 	 0.3222198486328125 	 10.136619329452515 	 2.979526996612549 	 0.31935977935791016 	 0.00011038780212402344 	 
2025-08-06 12:04:54.342421 test begin: paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 76204890 	 32438 	 30.22493004798889 	 30.395533561706543 	 0.9522216320037842 	 0.9579002857208252 	 30.550915002822876 	 2.881654977798462 	 0.9626514911651611 	 9.775161743164062e-05 	 
2025-08-06 12:06:31.782054 test begin: paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401630 	 32438 	 10.265837907791138 	 10.143454313278198 	 0.32338571548461914 	 0.1598200798034668 	 10.12797474861145 	 1.8017289638519287 	 0.31906604766845703 	 7.128715515136719e-05 	 
2025-08-06 12:07:05.237485 test begin: paddle.column_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], ) 	 76204824 	 32438 	 31.54000163078308 	 45.24200701713562 	 0.9935660362243652 	 1.4256350994110107 	 31.122209310531616 	 2.8585712909698486 	 0.9803996086120605 	 0.00017714500427246094 	 
2025-08-06 12:09:00.666494 test begin: paddle.column_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 76204920 	 32438 	 30.125777006149292 	 33.42772436141968 	 0.9491815567016602 	 1.0531814098358154 	 30.421781301498413 	 2.2346138954162598 	 0.9584355354309082 	 0.00010347366333007812 	 
2025-08-06 12:10:41.356491 test begin: paddle.column_stack(list[Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401640 	 32438 	 10.001288175582886 	 10.14513349533081 	 0.31511998176574707 	 0.15981292724609375 	 10.615740299224854 	 1.7898666858673096 	 0.33448100090026855 	 8.0108642578125e-05 	 
2025-08-06 12:11:15.052938 test begin: paddle.combinations(Tensor([2540160101],"int64"), 0, True, )
[Prof] paddle.combinations 	 paddle.combinations(Tensor([2540160101],"int64"), 0, True, ) 	 2540160101 	 804184 	 9.865145683288574 	 3.3292980194091797 	 3.147125244140625e-05 	 7.843971252441406e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 12:12:31.978023 test begin: paddle.combinations(Tensor([50803201],"int32"), 1, True, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f09f7787ee0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 12:22:37.534035 test begin: paddle.complex(Tensor([1, 793801, 64],"float32"), Tensor([1, 793801, 64],"float32"), )
W0806 12:22:43.256177 141811 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.complex 	 paddle.complex(Tensor([1, 793801, 64],"float32"), Tensor([1, 793801, 64],"float32"), ) 	 101606528 	 20670 	 12.234891176223755 	 12.148025274276733 	 0.6044328212738037 	 0.6006381511688232 	 12.188812255859375 	 1.517408847808838 	 0.6026239395141602 	 6.890296936035156e-05 	 
2025-08-06 12:23:23.076473 test begin: paddle.complex(Tensor([1, 8192, 6202],"float32"), Tensor([1, 8192, 6202],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([1, 8192, 6202],"float32"), Tensor([1, 8192, 6202],"float32"), ) 	 101613568 	 20670 	 12.22573709487915 	 12.147762060165405 	 0.6044604778289795 	 0.6006286144256592 	 12.186774730682373 	 1.3926119804382324 	 0.6025190353393555 	 7.033348083496094e-05 	 
2025-08-06 12:24:05.144523 test begin: paddle.complex(Tensor([1, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([1, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), ) 	 51380224 	 20670 	 10.139418840408325 	 9.782959699630737 	 0.5014691352844238 	 0.4829874038696289 	 10.654667377471924 	 5.9357476234436035 	 0.5267758369445801 	 0.2934284210205078 	 
2025-08-06 12:24:44.316249 test begin: paddle.complex(Tensor([20, 2417, 1051],"float32"), Tensor([20, 2417, 1051],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([20, 2417, 1051],"float32"), Tensor([20, 2417, 1051],"float32"), ) 	 101610680 	 20670 	 12.240266561508179 	 12.147958517074585 	 0.605224609375 	 0.6006629467010498 	 12.189055442810059 	 1.4303088188171387 	 0.6026768684387207 	 6.890296936035156e-05 	 
2025-08-06 12:25:25.093257 test begin: paddle.complex(Tensor([20, 2538, 1001],"float32"), Tensor([20, 2538, 1001],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([20, 2538, 1001],"float32"), Tensor([20, 2538, 1001],"float32"), ) 	 101621520 	 20670 	 12.22661304473877 	 12.152766704559326 	 0.6045103073120117 	 0.6008594036102295 	 12.190845012664795 	 1.543182611465454 	 0.6027398109436035 	 9.799003601074219e-05 	 
2025-08-06 12:26:07.738169 test begin: paddle.complex(Tensor([20, 64, 39691],"float32"), Tensor([20, 64, 39691],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([20, 64, 39691],"float32"), Tensor([20, 64, 39691],"float32"), ) 	 101608960 	 20670 	 12.235399961471558 	 12.148382902145386 	 0.6049892902374268 	 0.6006741523742676 	 12.191240310668945 	 1.3839178085327148 	 0.6027967929840088 	 6.079673767089844e-05 	 
2025-08-06 12:26:49.604244 test begin: paddle.complex(Tensor([756, 64, 1051],"float32"), Tensor([756, 64, 1051],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([756, 64, 1051],"float32"), Tensor([756, 64, 1051],"float32"), ) 	 101703168 	 20670 	 12.234071016311646 	 12.158371448516846 	 0.6048848628997803 	 0.6011257171630859 	 12.198150396347046 	 1.491715908050537 	 0.6030988693237305 	 0.00016379356384277344 	 
2025-08-06 12:27:30.546500 test begin: paddle.complex(Tensor([794, 64, 1001],"float32"), Tensor([794, 64, 1001],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([794, 64, 1001],"float32"), Tensor([794, 64, 1001],"float32"), ) 	 101733632 	 20670 	 12.229245901107788 	 12.181698322296143 	 0.6046206951141357 	 0.6014087200164795 	 12.200388431549072 	 1.4076080322265625 	 0.603208065032959 	 0.00016021728515625 	 
2025-08-06 12:28:12.342495 test begin: paddle.complex(Tensor([97, 8192, 64],"float32"), Tensor([1, 8192, 64],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([97, 8192, 64],"float32"), Tensor([1, 8192, 64],"float32"), ) 	 51380224 	 20670 	 9.9470374584198 	 9.771073341369629 	 0.4918365478515625 	 0.4829373359680176 	 10.621519804000854 	 5.934818267822266 	 0.5251681804656982 	 0.2933783531188965 	 
2025-08-06 12:28:50.554384 test begin: paddle.complex(Tensor([97, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([97, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), ) 	 101711872 	 20670 	 12.249778985977173 	 12.159849643707275 	 0.6055128574371338 	 0.601224422454834 	 12.205303192138672 	 1.3722808361053467 	 0.6034517288208008 	 7.462501525878906e-05 	 
2025-08-06 12:29:33.305455 test begin: paddle.concat(list[Tensor([101606401],"bfloat16"),], )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([101606401],"bfloat16"),], ) 	 101606401 	 31651 	 9.981343030929565 	 9.916327238082886 	 0.16115069389343262 	 0.1600334644317627 	 19.56073808670044 	 14.352705240249634 	 0.31582069396972656 	 0.46332812309265137 	 
2025-08-06 12:30:30.530633 test begin: paddle.concat(list[Tensor([254, 32, 112, 112],"float16"),Tensor([254, 32, 112, 112],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([254, 32, 112, 112],"float16"),Tensor([254, 32, 112, 112],"float16"),], axis=1, ) 	 203915264 	 31651 	 19.304998636245728 	 28.646005868911743 	 0.6234161853790283 	 0.9249637126922607 	 29.586507558822632 	 2.02876615524292 	 0.9552738666534424 	 7.605552673339844e-05 	 
2025-08-06 12:31:57.680375 test begin: paddle.concat(list[Tensor([512, 16, 112, 112],"float16"),Tensor([512, 16, 112, 112],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([512, 16, 112, 112],"float16"),Tensor([512, 16, 112, 112],"float16"),], axis=1, ) 	 205520896 	 31651 	 19.444674968719482 	 28.876123428344727 	 0.6277709007263184 	 0.9321229457855225 	 29.599554777145386 	 1.9970998764038086 	 0.955669641494751 	 7.510185241699219e-05 	 
2025-08-06 12:33:26.929846 test begin: paddle.concat(list[Tensor([512, 16, 112, 112],"float16"),Tensor([512, 32, 112, 112],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([512, 16, 112, 112],"float16"),Tensor([512, 32, 112, 112],"float16"),], axis=1, ) 	 308281344 	 31651 	 30.692065954208374 	 50.89138197898865 	 0.9423880577087402 	 1.643934726715088 	 44.98591351509094 	 2.0608859062194824 	 1.4527649879455566 	 7.2479248046875e-05 	 
2025-08-06 12:35:48.144581 test begin: paddle.concat(list[Tensor([512, 32, 112, 112],"float16"),Tensor([512, 16, 112, 112],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([512, 32, 112, 112],"float16"),Tensor([512, 16, 112, 112],"float16"),], axis=1, ) 	 308281344 	 31651 	 29.22990584373474 	 48.73468017578125 	 0.9439120292663574 	 1.5733072757720947 	 45.03425455093384 	 2.118236780166626 	 1.4545199871063232 	 7.390975952148438e-05 	 
2025-08-06 12:38:04.654981 test begin: paddle.concat(list[Tensor([512, 32, 112, 56],"float16"),Tensor([512, 32, 112, 56],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([512, 32, 112, 56],"float16"),Tensor([512, 32, 112, 56],"float16"),], axis=1, ) 	 205520896 	 31651 	 19.42858910560608 	 28.91000008583069 	 0.6274073123931885 	 0.9335803985595703 	 29.579554557800293 	 1.9812345504760742 	 0.9552443027496338 	 7.295608520507812e-05 	 
2025-08-06 12:39:32.196720 test begin: paddle.concat(list[Tensor([512, 32, 56, 112],"float16"),Tensor([512, 32, 56, 112],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([512, 32, 56, 112],"float16"),Tensor([512, 32, 56, 112],"float16"),], axis=1, ) 	 205520896 	 31651 	 19.479963541030884 	 28.90543293952942 	 0.6284298896789551 	 0.9330713748931885 	 29.587575674057007 	 2.001126766204834 	 0.9553334712982178 	 7.104873657226562e-05 	 
2025-08-06 12:41:04.039491 test begin: paddle.conj(Tensor([2, 20, 2, 635041],"float32"), )
[Prof] paddle.conj 	 paddle.conj(Tensor([2, 20, 2, 635041],"float32"), ) 	 50803280 	 32583 	 10.0185387134552 	 0.059656381607055664 	 0.31421375274658203 	 1.9550323486328125e-05 	 9.993228912353516 	 1.4497456550598145 	 0.31342196464538574 	 0.0001499652862548828 	 
2025-08-06 12:41:27.280270 test begin: paddle.conj(Tensor([2, 20, 423361, 3],"float32"), )
[Prof] paddle.conj 	 paddle.conj(Tensor([2, 20, 423361, 3],"float32"), ) 	 50803320 	 32583 	 10.016133785247803 	 0.0595090389251709 	 0.31412315368652344 	 2.4080276489257812e-05 	 9.99377727508545 	 1.4528181552886963 	 0.31342554092407227 	 0.00016808509826660156 	 
2025-08-06 12:41:51.148714 test begin: paddle.conj(Tensor([2, 4233601, 2, 3],"float32"), )
[Prof] paddle.conj 	 paddle.conj(Tensor([2, 4233601, 2, 3],"float32"), ) 	 50803212 	 32583 	 10.0555260181427 	 0.060135841369628906 	 0.3154165744781494 	 1.9311904907226562e-05 	 9.99232292175293 	 1.4803931713104248 	 0.31341028213500977 	 0.0001633167266845703 	 
2025-08-06 12:42:14.435369 test begin: paddle.conj(Tensor([423361, 20, 2, 3],"float32"), )
[Prof] paddle.conj 	 paddle.conj(Tensor([423361, 20, 2, 3],"float32"), ) 	 50803320 	 32583 	 10.014883995056152 	 0.06001734733581543 	 0.3141367435455322 	 2.3603439331054688e-05 	 9.992536544799805 	 1.4842300415039062 	 0.31342267990112305 	 0.00014281272888183594 	 
2025-08-06 12:42:41.122155 test begin: paddle.copysign(Tensor([12, 1058401, 2],"float64"), Tensor([12, 1058401, 2],"float64"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([12, 1058401, 2],"float64"), Tensor([12, 1058401, 2],"float64"), ) 	 50803248 	 22302 	 10.008317708969116 	 9.875187158584595 	 0.4586634635925293 	 0.45250725746154785 	 16.575349807739258 	 33.79506802558899 	 0.7596380710601807 	 0.30997681617736816 	 
2025-08-06 12:43:55.411951 test begin: paddle.copysign(Tensor([12, 20, 105841],"float64"), Tensor([12, 20, 105841],"float64"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([12, 20, 105841],"float64"), Tensor([12, 20, 105841],"float64"), ) 	 50803680 	 22302 	 10.405538082122803 	 9.876857042312622 	 0.4587256908416748 	 0.45251917839050293 	 16.583229303359985 	 33.79611277580261 	 0.7598536014556885 	 0.3098585605621338 	 
2025-08-06 12:45:11.720087 test begin: paddle.copysign(Tensor([12, 20, 211681],"float32"), Tensor([12, 20, 211681],"float32"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([12, 20, 211681],"float32"), Tensor([12, 20, 211681],"float32"), ) 	 101606880 	 22302 	 10.045400142669678 	 9.961044073104858 	 0.46031880378723145 	 0.4563138484954834 	 26.812021732330322 	 34.62794804573059 	 1.2287721633911133 	 0.31772303581237793 	 
2025-08-06 12:46:38.673163 test begin: paddle.copysign(Tensor([12, 2116801, 2],"float32"), Tensor([12, 2116801, 2],"float32"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([12, 2116801, 2],"float32"), Tensor([12, 2116801, 2],"float32"), ) 	 101606448 	 22302 	 10.046184062957764 	 9.96024751663208 	 0.46027493476867676 	 0.4562525749206543 	 26.857824563980103 	 34.61999773979187 	 1.2307343482971191 	 0.31740236282348633 	 
2025-08-06 12:48:04.493422 test begin: paddle.copysign(Tensor([1270081, 20, 2],"float32"), Tensor([1270081, 20, 2],"float32"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([1270081, 20, 2],"float32"), Tensor([1270081, 20, 2],"float32"), ) 	 101606480 	 22302 	 10.044795989990234 	 9.956819295883179 	 0.460357666015625 	 0.4562041759490967 	 26.817973613739014 	 34.618229150772095 	 1.2290172576904297 	 0.31738805770874023 	 
2025-08-06 12:49:28.505232 test begin: paddle.copysign(Tensor([635041, 20, 2],"float64"), Tensor([635041, 20, 2],"float64"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([635041, 20, 2],"float64"), Tensor([635041, 20, 2],"float64"), ) 	 50803280 	 22302 	 10.009865283966064 	 9.874525308609009 	 0.4583556652069092 	 0.4525413513183594 	 16.56580400466919 	 33.7948784828186 	 0.759058952331543 	 0.3097987174987793 	 
2025-08-06 12:50:42.946451 test begin: paddle.cos(Tensor([1587601, 32],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([1587601, 32],"float32"), ) 	 50803232 	 33844 	 10.657379150390625 	 10.087526321411133 	 0.302046537399292 	 0.30457377433776855 	 15.241434812545776 	 35.22624468803406 	 0.46031856536865234 	 0.35458874702453613 	 
2025-08-06 12:51:56.910528 test begin: paddle.cos(Tensor([198451, 256],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([198451, 256],"float32"), ) 	 50803456 	 33844 	 9.99796724319458 	 10.085463523864746 	 0.30188679695129395 	 0.3046081066131592 	 15.239904880523682 	 35.226500034332275 	 0.46017932891845703 	 0.35460352897644043 	 
2025-08-06 12:53:09.191609 test begin: paddle.cos(Tensor([32768, 1551],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([32768, 1551],"float32"), ) 	 50823168 	 33844 	 10.002111911773682 	 11.58180570602417 	 0.3020305633544922 	 0.3046457767486572 	 15.243088960647583 	 35.23872780799866 	 0.4604353904724121 	 0.35477209091186523 	 
2025-08-06 12:54:24.005822 test begin: paddle.cos(Tensor([396901, 128],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([396901, 128],"float32"), ) 	 50803328 	 33844 	 10.006020307540894 	 10.089369297027588 	 0.30214691162109375 	 0.3045690059661865 	 15.238758087158203 	 35.226057052612305 	 0.46004438400268555 	 0.35459089279174805 	 
2025-08-06 12:55:38.139209 test begin: paddle.cos(Tensor([5000, 10161],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([5000, 10161],"float32"), ) 	 50805000 	 33844 	 10.78109359741211 	 10.085908651351929 	 0.30232906341552734 	 0.30457592010498047 	 15.242005109786987 	 35.22761058807373 	 0.4601559638977051 	 0.35465264320373535 	 
2025-08-06 12:56:52.484394 test begin: paddle.cos(Tensor([8192, 6202],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([8192, 6202],"float32"), ) 	 50806784 	 33844 	 10.005913257598877 	 10.085972309112549 	 0.3021280765533447 	 0.3045523166656494 	 15.24293303489685 	 35.22904634475708 	 0.4602832794189453 	 0.3546421527862549 	 
2025-08-06 12:58:04.780833 test begin: paddle.cosh(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33925 	 10.008651971817017 	 10.133373498916626 	 0.3015017509460449 	 0.3050844669342041 	 15.274457216262817 	 25.212594509124756 	 0.46010565757751465 	 0.3797454833984375 	 
2025-08-06 12:59:08.077406 test begin: paddle.cosh(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33925 	 10.008504152297974 	 10.12618899345398 	 0.3014965057373047 	 0.30503273010253906 	 15.272823810577393 	 25.212990283966064 	 0.4601469039916992 	 0.37981653213500977 	 
2025-08-06 13:00:10.732981 test begin: paddle.cosh(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33925 	 10.008441925048828 	 10.134407758712769 	 0.3015129566192627 	 0.30504322052001953 	 15.27296781539917 	 25.21277642250061 	 0.46003007888793945 	 0.37972354888916016 	 
2025-08-06 13:01:13.843116 test begin: paddle.cosh(Tensor([28, 32, 241, 241],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([28, 32, 241, 241],"float32"), ) 	 52040576 	 33925 	 10.243696689605713 	 10.387734651565552 	 0.3085603713989258 	 0.31252527236938477 	 15.632014751434326 	 25.822273015975952 	 0.4709959030151367 	 0.3889038562774658 	 
2025-08-06 13:02:18.667457 test begin: paddle.cosh(Tensor([8, 110, 241, 241],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([8, 110, 241, 241],"float32"), ) 	 51111280 	 33925 	 10.063257694244385 	 10.18650197982788 	 0.3031775951385498 	 0.3068373203277588 	 15.369121789932251 	 25.362931966781616 	 0.46303391456604004 	 0.3820071220397949 	 
2025-08-06 13:03:21.341655 test begin: paddle.cosh(Tensor([8, 32, 241, 824],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([8, 32, 241, 824],"float32"), ) 	 50837504 	 33925 	 10.005937337875366 	 10.131483793258667 	 0.3014373779296875 	 0.3052091598510742 	 15.281988859176636 	 25.23007845878601 	 0.46042776107788086 	 0.3799731731414795 	 
2025-08-06 13:04:24.768807 test begin: paddle.cosh(Tensor([8, 32, 824, 241],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([8, 32, 824, 241],"float32"), ) 	 50837504 	 33925 	 10.005823612213135 	 10.131084680557251 	 0.3014101982116699 	 0.3051564693450928 	 15.282105445861816 	 25.229853868484497 	 0.4603614807128906 	 0.3800065517425537 	 
2025-08-06 13:05:28.298087 test begin: paddle.cosh(x=Tensor([3, 3, 5644801],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(x=Tensor([3, 3, 5644801],"float32"), ) 	 50803209 	 33925 	 10.891280889511108 	 10.12436842918396 	 0.30135083198547363 	 0.3050217628479004 	 15.275030851364136 	 25.212603092193604 	 0.46007394790649414 	 0.37975120544433594 	 
2025-08-06 13:06:34.624022 test begin: paddle.cosh(x=Tensor([3, 5644801, 3],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(x=Tensor([3, 5644801, 3],"float32"), ) 	 50803209 	 33925 	 10.647741317749023 	 10.12432599067688 	 0.30141282081604004 	 0.3050200939178467 	 15.27596664428711 	 25.21345615386963 	 0.4602029323577881 	 0.37981081008911133 	 
2025-08-06 13:07:38.633579 test begin: paddle.cosh(x=Tensor([5644801, 3, 3],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(x=Tensor([5644801, 3, 3],"float32"), ) 	 50803209 	 33925 	 11.348619222640991 	 10.124664545059204 	 0.30136680603027344 	 0.30504298210144043 	 15.275922060012817 	 25.213024616241455 	 0.46019577980041504 	 0.3797175884246826 	 
2025-08-06 13:08:42.568861 test begin: paddle.count_nonzero(Tensor([1, 14, 129601, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 14, 129601, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, ) 	 25401796 	 16875 	 10.110215663909912 	 8.878520965576172 	 0.2041490077972412 	 0.17920184135437012 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:09:10.619465 test begin: paddle.count_nonzero(Tensor([1, 14, 129601, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 14, 129601, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, ) 	 25401796 	 16875 	 10.109536409378052 	 8.878706693649292 	 0.20418548583984375 	 0.1792612075805664 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 1, 129601, 1]) and output[0] has a shape of torch.Size([1, 129601]).
2025-08-06 13:09:41.139563 test begin: paddle.count_nonzero(Tensor([1, 14, 5, 362881],"float64"), axis=list[1,3,], keepdim=False, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 14, 5, 362881],"float64"), axis=list[1,3,], keepdim=False, name=None, ) 	 25401670 	 16875 	 10.360124588012695 	 9.17382001876831 	 0.15681052207946777 	 0.1388404369354248 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:10:09.688326 test begin: paddle.count_nonzero(Tensor([1, 14, 5, 362881],"float64"), axis=list[1,3,], keepdim=True, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 14, 5, 362881],"float64"), axis=list[1,3,], keepdim=True, name=None, ) 	 25401670 	 16875 	 10.359735012054443 	 9.172226190567017 	 0.1568772792816162 	 0.13874459266662598 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 1, 5, 1]) and output[0] has a shape of torch.Size([1, 5]).
2025-08-06 13:10:38.807658 test begin: paddle.count_nonzero(Tensor([1, 362881, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 362881, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, ) 	 25401670 	 16875 	 10.257712602615356 	 9.318748474121094 	 0.15525436401367188 	 0.14096522331237793 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:11:10.675812 test begin: paddle.count_nonzero(Tensor([1, 362881, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 362881, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, ) 	 25401670 	 16875 	 10.257843255996704 	 9.318886995315552 	 0.15522027015686035 	 0.14095616340637207 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 1, 5, 1]) and output[0] has a shape of torch.Size([1, 5]).
2025-08-06 13:11:41.956641 test begin: paddle.count_nonzero(Tensor([2, 1270081, 4, 5],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([2, 1270081, 4, 5],"float32"), axis=-1, keepdim=False, ) 	 50803240 	 16875 	 16.42967963218689 	 17.906878232955933 	 0.3318042755126953 	 0.3615748882293701 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:12:37.272257 test begin: paddle.count_nonzero(Tensor([2, 3, 1693441, 5],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([2, 3, 1693441, 5],"float32"), axis=-1, keepdim=False, ) 	 50803230 	 16875 	 16.4311420917511 	 17.90033483505249 	 0.3317222595214844 	 0.36156249046325684 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:13:29.784891 test begin: paddle.count_nonzero(Tensor([2, 3, 4, 2116801],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([2, 3, 4, 2116801],"float32"), axis=-1, keepdim=False, ) 	 50803224 	 16875 	 14.737857818603516 	 14.623970985412598 	 0.22311830520629883 	 0.22129487991333008 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:14:13.752361 test begin: paddle.count_nonzero(Tensor([25921, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([25921, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, ) 	 25402580 	 16875 	 10.00042176246643 	 8.904609441757202 	 0.2018890380859375 	 0.17976069450378418 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:14:42.393361 test begin: paddle.count_nonzero(Tensor([25921, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([25921, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, ) 	 25402580 	 16875 	 10.000275135040283 	 8.902919292449951 	 0.20189476013183594 	 0.179826021194458 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([25921, 1, 5, 1]) and output[0] has a shape of torch.Size([25921, 5]).
2025-08-06 13:15:10.449514 test begin: paddle.count_nonzero(Tensor([846721, 3, 4, 5],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([846721, 3, 4, 5],"float32"), axis=-1, keepdim=False, ) 	 50803260 	 16875 	 16.42322874069214 	 17.897040605545044 	 0.33173346519470215 	 0.36139345169067383 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:16:03.377009 test begin: paddle.crop(x=Tensor([201, 14112, 3, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([201, 14112, 3, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 	 25528608 	 547102 	 9.723740816116333 	 10.551707983016968 	 5.364418029785156e-05 	 0.0001068115234375 	 81.10408902168274 	 89.50766563415527 	 0.15157866477966309 	 0.01852726936340332 	 combined
2025-08-06 13:19:14.832997 test begin: paddle.crop(x=Tensor([201, 3, 14112, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([201, 3, 14112, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 	 25528608 	 547102 	 9.693004846572876 	 10.574296712875366 	 4.982948303222656e-05 	 0.00018906593322753906 	 82.1288571357727 	 93.27293395996094 	 0.1533048152923584 	 0.019034147262573242 	 combined
2025-08-06 13:22:31.066451 test begin: paddle.crop(x=Tensor([201, 3, 3, 14112],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([201, 3, 3, 14112],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 	 25528608 	 547102 	 9.72689175605774 	 10.54583477973938 	 3.528594970703125e-05 	 0.0001823902130126953 	 80.2823793888092 	 91.61591601371765 	 0.1499781608581543 	 0.01891350746154785 	 combined
2025-08-06 13:25:44.405090 test begin: paddle.crop(x=Tensor([301, 84672],"float64"), shape=list[2,2,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([301, 84672],"float64"), shape=list[2,2,], ) 	 25486272 	 547102 	 9.90477180480957 	 7.294469356536865 	 3.719329833984375e-05 	 6.985664367675781e-05 	 81.08287644386292 	 82.5347843170166 	 0.1513826847076416 	 0.03074049949645996 	 combined
2025-08-06 13:28:45.783471 test begin: paddle.crop(x=Tensor([8467201, 3],"float64"), shape=list[2,2,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([8467201, 3],"float64"), shape=list[2,2,], ) 	 25401603 	 547102 	 10.011393308639526 	 7.318779706954956 	 3.933906555175781e-05 	 0.00021266937255859375 	 81.10045862197876 	 78.39701437950134 	 0.15129446983337402 	 0.03661799430847168 	 combined
2025-08-06 13:31:43.165051 test begin: paddle.cross(x=Tensor([2822401, 3, 3],"float64"), y=Tensor([2822401, 3, 3],"float64"), axis=1, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([2822401, 3, 3],"float64"), y=Tensor([2822401, 3, 3],"float64"), axis=1, ) 	 50803218 	 22358 	 10.060892820358276 	 10.050354242324829 	 0.45983433723449707 	 0.4593513011932373 	 16.79776120185852 	 20.099846839904785 	 0.7678699493408203 	 0.4593386650085449 	 
2025-08-06 13:32:43.147463 test begin: paddle.cross(x=Tensor([2822401, 3, 3],"float64"), y=Tensor([2822401, 3, 3],"float64"), axis=2, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([2822401, 3, 3],"float64"), y=Tensor([2822401, 3, 3],"float64"), axis=2, ) 	 50803218 	 22358 	 10.075671195983887 	 10.077457427978516 	 0.46045780181884766 	 0.4606492519378662 	 16.938603162765503 	 20.152485609054565 	 0.774207592010498 	 0.4605426788330078 	 
2025-08-06 13:33:44.304498 test begin: paddle.cross(x=Tensor([3, 2822401, 3],"float64"), y=Tensor([3, 2822401, 3],"float64"), axis=0, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([3, 2822401, 3],"float64"), y=Tensor([3, 2822401, 3],"float64"), axis=0, ) 	 50803218 	 22358 	 10.607152700424194 	 10.027762174606323 	 0.4576122760772705 	 0.45841336250305176 	 16.558940887451172 	 20.05958914756775 	 0.7569198608398438 	 0.4584379196166992 	 
2025-08-06 13:34:45.009022 test begin: paddle.cross(x=Tensor([3, 2822401, 3],"float64"), y=Tensor([3, 2822401, 3],"float64"), axis=2, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([3, 2822401, 3],"float64"), y=Tensor([3, 2822401, 3],"float64"), axis=2, ) 	 50803218 	 22358 	 10.063475370407104 	 10.07831859588623 	 0.46053290367126465 	 0.4606335163116455 	 16.926872491836548 	 20.1540265083313 	 0.7736465930938721 	 0.4606473445892334 	 
2025-08-06 13:35:43.933984 test begin: paddle.cross(x=Tensor([3, 3, 2822401],"float64"), y=Tensor([3, 3, 2822401],"float64"), axis=0, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([3, 3, 2822401],"float64"), y=Tensor([3, 3, 2822401],"float64"), axis=0, ) 	 50803218 	 22358 	 10.014317750930786 	 10.028827667236328 	 0.4575626850128174 	 0.45848703384399414 	 16.559988975524902 	 20.061335802078247 	 0.7569320201873779 	 0.4583864212036133 	 
2025-08-06 13:36:43.648499 test begin: paddle.cross(x=Tensor([3, 3, 2822401],"float64"), y=Tensor([3, 3, 2822401],"float64"), axis=1, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([3, 3, 2822401],"float64"), y=Tensor([3, 3, 2822401],"float64"), axis=1, ) 	 50803218 	 22358 	 10.015797853469849 	 10.014421224594116 	 0.4577209949493408 	 0.4576249122619629 	 16.65922975540161 	 20.042135000228882 	 0.7614834308624268 	 0.4580268859863281 	 
2025-08-06 13:37:43.214389 test begin: paddle.cummax(Tensor([100, 2080],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1b15c9a800>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 13:47:58.777778 test begin: paddle.cummax(Tensor([100, 2080],"float32"), axis=-1, )
W0806 13:47:58.983584 143670 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cummax 	 paddle.cummax(Tensor([100, 2080],"float32"), axis=-1, ) 	 208000 	 55599 	 9.71858024597168 	 1.8544080257415771 	 0.17863130569458008 	 0.03406333923339844 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:48:14.126142 test begin: paddle.cummax(Tensor([10001, 2080],"float32"), axis=-2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f069da5ead0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 13:58:24.003855 test begin: paddle.cummax(Tensor([2080, 100],"float32"), )
W0806 13:58:24.249748 144066 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcc7e44f010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 14:08:28.586400 test begin: paddle.cummax(Tensor([2080, 100],"float32"), axis=-2, )
W0806 14:08:28.783859 144467 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cummax 	 paddle.cummax(Tensor([2080, 100],"float32"), axis=-2, ) 	 208000 	 55599 	 23.946603059768677 	 22.771240711212158 	 0.44019174575805664 	 0.41849827766418457 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:09:23.345487 test begin: paddle.cummax(Tensor([208001, 100],"float32"), axis=-1, )
[Prof] paddle.cummax 	 paddle.cummax(Tensor([208001, 100],"float32"), axis=-1, ) 	 20800100 	 55599 	 30.695736169815063 	 194.8333556652069 	 0.5642335414886475 	 3.582073926925659 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:37:44.072672 test begin: paddle.cummin(Tensor([100, 508033],"float32"), axis=-1, )
W0805 21:37:45.681919 108279 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fedfe303040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 21:47:48.672480 test begin: paddle.cummin(Tensor([100, 508033],"float32"), axis=-2, )
W0805 21:47:49.823393 110216 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cummin 	 paddle.cummin(Tensor([100, 508033],"float32"), axis=-2, ) 	 50803300 	 12631 	 10.025696992874146 	 9.983175277709961 	 0.8108246326446533 	 0.8074469566345215 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:49:28.621618 test begin: paddle.cummin(Tensor([508033, 100],"float32"), axis=-1, )
[Prof] paddle.cummin 	 paddle.cummin(Tensor([508033, 100],"float32"), axis=-1, ) 	 50803300 	 12631 	 16.801050901412964 	 107.82812714576721 	 1.3593828678131104 	 8.723265171051025 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:51:58.252678 test begin: paddle.cummin(Tensor([508033, 100],"float32"), axis=-2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2ce060afe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 22:06:17.915942 test begin: paddle.cumprod(Tensor([2, 127009, 10, 10],"float64"), 1, )
W0805 22:06:18.590709 110669 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fac61e72ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 22:16:22.624431 test begin: paddle.cumprod(Tensor([2, 3, 10, 423361],"float64"), 1, )
W0805 22:16:23.345398 111070 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 10, 423361],"float64"), 1, ) 	 25401660 	 33059 	 9.984951734542847 	 9.995206356048584 	 0.30869007110595703 	 0.3088505268096924 	 93.99918699264526 	 67.35751008987427 	 0.0003440380096435547 	 0.0012965202331542969 	 
2025-08-05 22:19:25.839904 test begin: paddle.cumprod(Tensor([2, 3, 3, 4, 705601],"float32"), dim=0, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 3, 4, 705601],"float32"), dim=0, ) 	 50803272 	 33059 	 10.832078218460083 	 10.294120073318481 	 0.3349025249481201 	 0.3182549476623535 	 112.49238657951355 	 69.84603524208069 	 0.0004718303680419922 	 0.0013175010681152344 	 
2025-08-05 22:22:54.061274 test begin: paddle.cumprod(Tensor([2, 3, 3, 4, 705601],"float32"), dim=1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 3, 4, 705601],"float32"), dim=1, ) 	 50803272 	 33059 	 10.617172241210938 	 10.501917839050293 	 0.32824087142944336 	 0.32465386390686035 	 107.71340298652649 	 70.26656007766724 	 0.00040984153747558594 	 0.0013391971588134766 	 
2025-08-05 22:26:18.287757 test begin: paddle.cumprod(Tensor([2, 3, 3, 564481, 5],"float32"), dim=0, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 3, 564481, 5],"float32"), dim=0, ) 	 50803290 	 33059 	 10.83652138710022 	 10.293517589569092 	 0.33502840995788574 	 0.31821131706237793 	 112.49739170074463 	 69.81985116004944 	 0.00044918060302734375 	 0.001322031021118164 	 
2025-08-05 22:29:43.598139 test begin: paddle.cumprod(Tensor([2, 3, 3, 564481, 5],"float32"), dim=1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 3, 564481, 5],"float32"), dim=1, ) 	 50803290 	 33059 	 10.63442063331604 	 10.521792888641357 	 0.3286476135253906 	 0.325092077255249 	 107.76614546775818 	 70.31636905670166 	 0.0004177093505859375 	 0.0013337135314941406 	 
2025-08-05 22:33:04.617954 test begin: paddle.cumprod(Tensor([2, 3, 423361, 10],"float64"), 1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 423361, 10],"float64"), 1, ) 	 25401660 	 33059 	 9.990790128707886 	 11.209931373596191 	 0.30888795852661133 	 0.30919456481933594 	 94.05737638473511 	 67.40900754928589 	 0.0003478527069091797 	 0.0012791156768798828 	 
2025-08-05 22:36:10.435664 test begin: paddle.cumprod(Tensor([2, 3, 423361, 4, 5],"float32"), dim=0, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 423361, 4, 5],"float32"), dim=0, ) 	 50803320 	 33059 	 10.844716787338257 	 10.767008781433105 	 0.33525967597961426 	 0.3183112144470215 	 112.50001692771912 	 69.78128910064697 	 0.0004744529724121094 	 0.0013265609741210938 	 
2025-08-05 22:39:39.080756 test begin: paddle.cumprod(Tensor([2, 3, 423361, 4, 5],"float32"), dim=1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 423361, 4, 5],"float32"), dim=1, ) 	 50803320 	 33059 	 11.746876955032349 	 12.205307722091675 	 0.32842540740966797 	 0.32486844062805176 	 107.7318069934845 	 70.22769975662231 	 0.0004169940948486328 	 0.0013363361358642578 	 
2025-08-05 22:43:04.412160 test begin: paddle.cumprod(Tensor([2, 423361, 3, 4, 5],"float32"), dim=0, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 423361, 3, 4, 5],"float32"), dim=0, ) 	 50803320 	 33059 	 10.847977638244629 	 12.060376405715942 	 0.3352537155151367 	 0.31830835342407227 	 112.50905680656433 	 69.82195925712585 	 0.0004775524139404297 	 0.0012874603271484375 	 
2025-08-05 22:46:32.192427 test begin: paddle.cumprod(Tensor([282241, 3, 3, 4, 5],"float32"), dim=1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([282241, 3, 3, 4, 5],"float32"), dim=1, ) 	 50803380 	 33059 	 10.833053827285767 	 10.687715530395508 	 0.3348422050476074 	 0.329573392868042 	 107.40492177009583 	 70.54334807395935 	 0.000423431396484375 	 0.001336812973022461 	 
2025-08-05 22:49:55.366090 test begin: paddle.cumprod(Tensor([84673, 3, 10, 10],"float64"), 1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([84673, 3, 10, 10],"float64"), 1, ) 	 25401900 	 33059 	 10.038275957107544 	 12.4377601146698 	 0.3102700710296631 	 0.31056666374206543 	 93.90086531639099 	 67.42437553405762 	 0.00033402442932128906 	 0.0012927055358886719 	 
2025-08-05 22:53:02.247555 test begin: paddle.cumsum(Tensor([50803201],"float32"), axis=0, )
[Prof] paddle.cumsum 	 paddle.cumsum(Tensor([50803201],"float32"), axis=0, ) 	 50803201 	 28767 	 9.967516660690308 	 9.455611944198608 	 4.458427429199219e-05 	 0.16794753074645996 	 11.390001773834229 	 27.18024468421936 	 9.560585021972656e-05 	 0.24153828620910645 	 
2025-08-05 22:54:02.003118 test begin: paddle.deg2rad(Tensor([25401601],"int64"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([25401601],"int64"), ) 	 25401601 	 33772 	 12.804901123046875 	 7.855100631713867 	 0.19373202323913574 	 0.23773765563964844 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 22:54:39.747250 test begin: paddle.deg2rad(Tensor([50803201],"float32"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([50803201],"float32"), ) 	 50803201 	 33772 	 11.194854497909546 	 10.057939291000366 	 0.30234408378601074 	 0.30414509773254395 	 9.996645212173462 	 10.048669576644897 	 0.3025181293487549 	 0.3040955066680908 	 
2025-08-05 22:55:23.405690 test begin: paddle.deg2rad(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33772 	 9.994767427444458 	 10.05479907989502 	 0.30248045921325684 	 0.3041646480560303 	 9.996960639953613 	 10.048535585403442 	 0.3025341033935547 	 0.30410170555114746 	 
2025-08-05 22:56:06.675659 test begin: paddle.deg2rad(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33772 	 9.986432075500488 	 10.050916194915771 	 0.3021814823150635 	 0.30416440963745117 	 9.996556282043457 	 10.04898738861084 	 0.30251431465148926 	 0.30406808853149414 	 
2025-08-05 22:56:50.694923 test begin: paddle.deg2rad(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33772 	 10.759544372558594 	 10.05063271522522 	 0.3022928237915039 	 0.3041853904724121 	 9.99685549736023 	 10.048953533172607 	 0.30256152153015137 	 0.3041198253631592 	 
2025-08-05 22:57:33.638190 test begin: paddle.diag(Tensor([20000, 25402],"float32"), )
[Prof] paddle.diag 	 paddle.diag(Tensor([20000, 25402],"float32"), ) 	 508040000 	 199725 	 1.6518661975860596 	 3.478393793106079 	 9.894371032714844e-05 	 7.510185241699219e-05 	 303.3120427131653 	 265.00600242614746 	 0.7761776447296143 	 0.6777942180633545 	 
2025-08-05 23:07:18.324303 test begin: paddle.diag(Tensor([20000, 25402],"float32"), offset=-1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([20000, 25402],"float32"), offset=-1, ) 	 508040000 	 199725 	 1.718231201171875 	 3.3870232105255127 	 7.271766662597656e-05 	 0.00018215179443359375 	 303.3213782310486 	 265.01230359077454 	 0.7760481834411621 	 0.6778552532196045 	 
2025-08-05 23:17:00.073905 test begin: paddle.diag(Tensor([20000, 25402],"float32"), offset=1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([20000, 25402],"float32"), offset=1, ) 	 508040000 	 199725 	 1.7042973041534424 	 3.37868070602417 	 0.00010251998901367188 	 6.604194641113281e-05 	 303.2908065319061 	 265.00285053253174 	 0.7763552665710449 	 0.6778266429901123 	 
2025-08-05 23:26:43.534034 test begin: paddle.diag(Tensor([254020, 2000],"float32"), )
[Prof] paddle.diag 	 paddle.diag(Tensor([254020, 2000],"float32"), ) 	 508040000 	 199725 	 1.6467723846435547 	 3.3996217250823975 	 7.271766662597656e-05 	 8.535385131835938e-05 	 300.7634325027466 	 263.3290889263153 	 0.770073413848877 	 0.6736788749694824 	 
2025-08-05 23:36:24.350624 test begin: paddle.diag(Tensor([254020, 2000],"float32"), offset=-1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([254020, 2000],"float32"), offset=-1, ) 	 508040000 	 199725 	 1.7072608470916748 	 3.3981587886810303 	 7.557868957519531e-05 	 0.00017762184143066406 	 300.7532386779785 	 263.32796454429626 	 0.7694375514984131 	 0.6735048294067383 	 
2025-08-05 23:46:02.950251 test begin: paddle.diag(Tensor([254020, 2000],"float32"), offset=1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([254020, 2000],"float32"), offset=1, ) 	 508040000 	 199725 	 1.7029247283935547 	 3.38710618019104 	 7.748603820800781e-05 	 7.224082946777344e-05 	 300.8043649196625 	 263.324688911438 	 0.7708065509796143 	 0.6736092567443848 	 
2025-08-05 23:55:43.613893 test begin: paddle.diag_embed(Tensor([1058401, 3, 8],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([1058401, 3, 8],"float64"), ) 	 25401624 	 3661 	 21.465383291244507 	 9.560075044631958 	 0.0001239776611328125 	 1.3343589305877686 	 None 	 None 	 None 	 None 	 
2025-08-05 23:56:15.233860 test begin: paddle.diag_embed(Tensor([1411201, 3, 6],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([1411201, 3, 6],"float64"), ) 	 25401618 	 3661 	 9.263251304626465 	 8.03332233428955 	 4.553794860839844e-05 	 1.1212077140808105 	 None 	 None 	 None 	 None 	 
2025-08-05 23:56:33.073005 test begin: paddle.diag_embed(Tensor([2, 1058401, 12],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([2, 1058401, 12],"float64"), ) 	 25401624 	 3661 	 23.945093393325806 	 14.023730278015137 	 0.00011754035949707031 	 0.9789326190948486 	 None 	 None 	 None 	 None 	 
2025-08-05 23:57:11.618881 test begin: paddle.diag_embed(Tensor([2, 1587601, 8],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([2, 1587601, 8],"float64"), ) 	 25401616 	 3661 	 10.832662343978882 	 9.560949802398682 	 4.696846008300781e-05 	 1.3340113162994385 	 None 	 None 	 None 	 None 	 
2025-08-05 23:57:32.571874 test begin: paddle.diag_embed(Tensor([2, 2116801, 6],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([2, 2116801, 6],"float64"), ) 	 25401612 	 3661 	 18.258956909179688 	 8.033636093139648 	 9.465217590332031e-05 	 1.1213319301605225 	 None 	 None 	 None 	 None 	 
2025-08-05 23:57:59.441691 test begin: paddle.diag_embed(Tensor([705601, 3, 12],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([705601, 3, 12],"float64"), ) 	 25401636 	 3661 	 21.861517667770386 	 14.021394968032837 	 0.00010561943054199219 	 0.9797482490539551 	 None 	 None 	 None 	 None 	 
2025-08-05 23:58:38.521490 test begin: paddle.diagonal(x=Tensor([117601, 6, 6, 6],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([117601, 6, 6, 6],"float64"), ) 	 25401816 	 1998070 	 7.209705114364624 	 15.362655878067017 	 5.602836608886719e-05 	 0.000335693359375 	 297.80523133277893 	 276.91175270080566 	 0.07615423202514648 	 0.07077383995056152 	 
2025-08-06 00:08:38.436775 test begin: paddle.diagonal(x=Tensor([176401, 6, 6, 2, 2],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([176401, 6, 6, 2, 2],"float64"), ) 	 25401744 	 1998070 	 7.171854496002197 	 8.802901029586792 	 5.817413330078125e-05 	 8.845329284667969e-05 	 299.0896897315979 	 276.86709547042847 	 0.0764472484588623 	 0.0707852840423584 	 
2025-08-06 00:18:31.009162 test begin: paddle.diagonal(x=Tensor([601, 1176, 6, 6],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 1176, 6, 6],"float64"), ) 	 25443936 	 1998070 	 7.230668783187866 	 8.762005090713501 	 5.14984130859375e-05 	 0.00015425682067871094 	 300.9654107093811 	 279.006635427475 	 0.07699823379516602 	 0.07127189636230469 	 
2025-08-06 00:28:27.958280 test begin: paddle.diagonal(x=Tensor([601, 1764, 6, 2, 2],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2e6583ecb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 00:38:32.530094 test begin: paddle.diagonal(x=Tensor([601, 6, 1176, 6],"float64"), )
W0806 00:38:33.316256 115620 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 6, 1176, 6],"float64"), ) 	 25443936 	 1998070 	 15.960121631622314 	 9.659800052642822 	 0.00012183189392089844 	 0.0002155303955078125 	 293.1288161277771 	 278.78531765937805 	 0.0748283863067627 	 0.0712435245513916 	 
2025-08-06 00:48:31.889592 test begin: paddle.diagonal(x=Tensor([601, 6, 1764, 2, 2],"float64"), axis1=-1, axis2=2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4777eb2d10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 00:58:36.813925 test begin: paddle.diagonal(x=Tensor([601, 6, 6, 1176],"float64"), )
W0806 00:58:42.208235 116399 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f182a6eb010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 01:08:47.483722 test begin: paddle.diagonal(x=Tensor([601, 6, 6, 2, 588],"float64"), )
W0806 01:08:48.231705 116784 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 6, 6, 2, 588],"float64"), ) 	 25443936 	 1998070 	 6.925455093383789 	 9.333030462265015 	 0.0001609325408935547 	 0.0003883838653564453 	 292.2637822628021 	 278.9298665523529 	 0.07475924491882324 	 0.0712575912475586 	 
2025-08-06 01:18:38.062696 test begin: paddle.diagonal(x=Tensor([601, 6, 6, 2, 588],"float64"), axis1=-1, axis2=2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7f05282a40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 01:28:45.706423 test begin: paddle.diagonal(x=Tensor([601, 6, 6, 588, 2],"float64"), )
W0806 01:28:46.404953 117354 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([601, 6, 6, 588, 2],"float64"), ) 	 25443936 	 1998070 	 7.73352575302124 	 9.875330448150635 	 0.0001373291015625 	 0.00012636184692382812 	 291.98408341407776 	 278.49828720092773 	 0.07463288307189941 	 0.07123756408691406 	 
2025-08-06 01:38:35.056115 test begin: paddle.diagonal_scatter(Tensor([10, 10160641],"int16"), Tensor([10],"int16"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([10, 10160641],"int16"), Tensor([10],"int16"), offset=0, axis1=0, axis2=1, ) 	 101606420 	 31122 	 10.391500234603882 	 9.829411506652832 	 0.07976531982421875 	 0.10738945007324219 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 01:39:09.322349 test begin: paddle.diagonal_scatter(Tensor([10, 5080321],"int32"), Tensor([10],"int32"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([10, 5080321],"int32"), Tensor([10],"int32"), offset=0, axis1=0, axis2=1, ) 	 50803220 	 31122 	 9.736998796463013 	 9.828829288482666 	 0.07978153228759766 	 0.1073758602142334 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 01:39:42.739753 test begin: paddle.diagonal_scatter(Tensor([100, 5080321],"bool"), Tensor([100],"bool"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([100, 5080321],"bool"), Tensor([100],"bool"), offset=0, axis1=0, axis2=1, ) 	 508032200 	 31122 	 24.167227029800415 	 24.076952695846558 	 0.1980147361755371 	 0.26300644874572754 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 01:41:09.138541 test begin: paddle.diagonal_scatter(Tensor([10160641, 10],"int16"), Tensor([10],"int16"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([10160641, 10],"int16"), Tensor([10],"int16"), offset=0, axis1=0, axis2=1, ) 	 101606420 	 31122 	 10.691531419754028 	 9.822212934494019 	 0.08166098594665527 	 0.10730838775634766 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 01:41:42.144764 test begin: paddle.diagonal_scatter(Tensor([5080321, 10],"int32"), Tensor([10],"int32"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([5080321, 10],"int32"), Tensor([10],"int32"), offset=0, axis1=0, axis2=1, ) 	 50803220 	 31122 	 9.97428846359253 	 9.824515104293823 	 0.08166074752807617 	 0.1073145866394043 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 01:42:15.647773 test begin: paddle.diagonal_scatter(Tensor([50803210, 10],"bool"), Tensor([10],"bool"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([50803210, 10],"bool"), Tensor([10],"bool"), offset=0, axis1=0, axis2=1, ) 	 508032110 	 31122 	 24.158461332321167 	 24.083583116531372 	 0.19791769981384277 	 0.2629702091217041 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 01:43:42.207889 test begin: paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 12388 	 11.696914196014404 	 3.235898971557617 	 0.3215606212615967 	 0.2669076919555664 	 None 	 None 	 None 	 None 	 
2025-08-06 01:43:57.745196 test begin: paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=-2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=-2, ) 	 25401664 	 12388 	 11.69409441947937 	 3.2357141971588135 	 0.3216397762298584 	 0.2669367790222168 	 None 	 None 	 None 	 None 	 
2025-08-06 01:44:13.232283 test begin: paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=2, ) 	 25401664 	 12388 	 11.698142051696777 	 3.2357239723205566 	 0.3216402530670166 	 0.266953706741333 	 None 	 None 	 None 	 None 	 
2025-08-06 01:44:31.080338 test begin: paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 12388 	 10.7881920337677 	 3.2656805515289307 	 0.2967038154602051 	 0.2669050693511963 	 None 	 None 	 None 	 None 	 
2025-08-06 01:44:45.991041 test begin: paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=-2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=-2, ) 	 25401664 	 12388 	 10.788989305496216 	 3.2357213497161865 	 0.2967343330383301 	 0.2669360637664795 	 None 	 None 	 None 	 None 	 
2025-08-06 01:45:00.569801 test begin: paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=2, ) 	 25401664 	 12388 	 10.791241884231567 	 3.2357990741729736 	 0.2967355251312256 	 0.2669353485107422 	 None 	 None 	 None 	 None 	 
2025-08-06 01:45:15.159551 test begin: paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 12388 	 10.788238048553467 	 3.2359681129455566 	 0.2967512607574463 	 0.2669541835784912 	 None 	 None 	 None 	 None 	 
2025-08-06 01:45:29.703271 test begin: paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=-2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=-2, ) 	 25401664 	 12388 	 13.25897765159607 	 4.743577241897583 	 0.3645906448364258 	 0.3058469295501709 	 None 	 None 	 None 	 None 	 
2025-08-06 01:45:51.051433 test begin: paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=2, ) 	 25401664 	 12388 	 13.261398792266846 	 3.7076215744018555 	 0.364513635635376 	 0.3058619499206543 	 None 	 None 	 None 	 None 	 
2025-08-06 01:46:08.651444 test begin: paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 12388 	 13.253325939178467 	 3.707573413848877 	 0.36452317237854004 	 0.3058133125305176 	 None 	 None 	 None 	 None 	 
2025-08-06 01:46:26.186565 test begin: paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=-2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=-2, ) 	 25401664 	 12388 	 9.991574764251709 	 3.2776541709899902 	 0.27483153343200684 	 0.2684810161590576 	 None 	 None 	 None 	 None 	 
2025-08-06 01:46:43.311426 test begin: paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=2, ) 	 25401664 	 12388 	 9.99315619468689 	 3.2573416233062744 	 0.2747344970703125 	 0.26845240592956543 	 None 	 None 	 None 	 None 	 
2025-08-06 01:46:59.781577 test begin: paddle.digamma(Tensor([16538, 3, 32, 32],"float32"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([16538, 3, 32, 32],"float32"), ) 	 50804736 	 10410 	 9.992619276046753 	 11.188638687133789 	 0.9807372093200684 	 1.1004464626312256 	 46.71168804168701 	 11.164884567260742 	 4.585726022720337 	 0.5480101108551025 	 
2025-08-06 01:48:20.681011 test begin: paddle.digamma(Tensor([8, 3, 32, 33076],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3, 32, 33076],"float64"), ) 	 25402368 	 10410 	 12.158311605453491 	 12.057481050491333 	 1.193629503250122 	 1.3444914817810059 	 89.01260352134705 	 11.28492283821106 	 8.73734736442566 	 0.5538995265960693 	 
2025-08-06 01:50:27.514633 test begin: paddle.digamma(Tensor([8, 3, 32, 66151],"float32"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3, 32, 66151],"float32"), ) 	 50803968 	 10410 	 9.993213176727295 	 11.661465406417847 	 0.9816744327545166 	 1.4063758850097656 	 46.71238613128662 	 11.158995628356934 	 4.585752487182617 	 0.5476772785186768 	 
2025-08-06 01:51:52.459567 test begin: paddle.digamma(Tensor([8, 3, 33076, 32],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3, 33076, 32],"float64"), ) 	 25402368 	 10410 	 12.161567449569702 	 11.881305694580078 	 1.1937789916992188 	 1.166599988937378 	 89.00306010246277 	 11.286016702651978 	 8.735196352005005 	 0.5539977550506592 	 
2025-08-06 01:53:58.462077 test begin: paddle.digamma(Tensor([8, 3, 66151, 32],"float32"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3, 66151, 32],"float32"), ) 	 50803968 	 10410 	 9.988833904266357 	 11.11042308807373 	 0.9805042743682861 	 1.0941498279571533 	 46.711893796920776 	 11.166492700576782 	 4.585967063903809 	 0.5481374263763428 	 
2025-08-06 01:55:19.581349 test begin: paddle.digamma(Tensor([8, 3101, 32, 32],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3101, 32, 32],"float64"), ) 	 25403392 	 10410 	 12.180152893066406 	 11.879900693893433 	 1.1942031383514404 	 1.1662676334381104 	 88.99808597564697 	 11.285189390182495 	 8.738803386688232 	 0.5539548397064209 	 
2025-08-06 01:57:25.148484 test begin: paddle.digamma(Tensor([8, 6202, 32, 32],"float32"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 6202, 32, 32],"float32"), ) 	 50806784 	 10410 	 9.997119903564453 	 11.163524866104126 	 0.9810075759887695 	 1.1313562393188477 	 46.7134325504303 	 11.16701340675354 	 4.58603835105896 	 0.5481410026550293 	 
2025-08-06 01:58:48.091158 test begin: paddle.digamma(Tensor([8269, 3, 32, 32],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8269, 3, 32, 32],"float64"), ) 	 25402368 	 10410 	 12.17405915260315 	 11.884434223175049 	 1.1938273906707764 	 1.166560411453247 	 89.02138948440552 	 11.286121845245361 	 8.738502025604248 	 0.5539577007293701 	 
2025-08-06 02:00:53.607445 test begin: paddle.digamma(x=Tensor([19601, 6, 6, 6, 6],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(x=Tensor([19601, 6, 6, 6, 6],"float64"), ) 	 25402896 	 10410 	 12.175759077072144 	 11.888410091400146 	 1.1939570903778076 	 1.1662397384643555 	 88.94532799720764 	 11.284821033477783 	 8.73207426071167 	 0.5539035797119141 	 
2025-08-06 02:03:00.737276 test begin: paddle.digamma(x=Tensor([3, 39201, 6, 6, 6],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(x=Tensor([3, 39201, 6, 6, 6],"float64"), ) 	 25402248 	 10410 	 12.160732507705688 	 11.879616022109985 	 1.193798542022705 	 1.1662943363189697 	 88.94656038284302 	 11.286293983459473 	 8.730090141296387 	 0.553964376449585 	 
2025-08-06 02:05:07.258464 test begin: paddle.digamma(x=Tensor([3, 6, 39201, 6, 6],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(x=Tensor([3, 6, 39201, 6, 6],"float64"), ) 	 25402248 	 10410 	 12.161206245422363 	 11.880415439605713 	 1.1938323974609375 	 1.166384220123291 	 88.93972659111023 	 11.286990404129028 	 8.732451677322388 	 0.5540950298309326 	 
2025-08-06 02:07:12.642243 test begin: paddle.digamma(x=Tensor([3, 6, 6, 39201, 6],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(x=Tensor([3, 6, 6, 39201, 6],"float64"), ) 	 25402248 	 10410 	 12.164802312850952 	 11.880138635635376 	 1.1942343711853027 	 1.166144847869873 	 88.9361834526062 	 11.289313077926636 	 8.730041265487671 	 0.5541243553161621 	 
2025-08-06 02:09:18.026163 test begin: paddle.digamma(x=Tensor([3, 6, 6, 6, 39201],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(x=Tensor([3, 6, 6, 6, 39201],"float64"), ) 	 25402248 	 10410 	 13.421157836914062 	 11.884305953979492 	 1.1938378810882568 	 1.166675090789795 	 88.9691870212555 	 11.290947914123535 	 8.733803749084473 	 0.5542285442352295 	 
2025-08-06 02:11:24.699139 test begin: paddle.dist(x=Tensor([10],"float64"), y=Tensor([2540161, 10],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([10],"float64"), y=Tensor([2540161, 10],"float64"), ) 	 25401620 	 21993 	 9.981237649917603 	 10.000314235687256 	 0.11572766304016113 	 0.1547400951385498 	 148.1471347808838 	 63.16489601135254 	 1.3789145946502686 	 0.22567319869995117 	 
2025-08-06 02:15:18.211757 test begin: paddle.dist(x=Tensor([113401, 1, 1, 4, 4],"float64"), y=Tensor([113401, 8, 7, 1, 4],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([113401, 1, 1, 4, 4],"float64"), y=Tensor([113401, 8, 7, 1, 4],"float64"), ) 	 27216240 	 21993 	 30.26270580291748 	 30.868019342422485 	 0.3508768081665039 	 0.47776293754577637 	 186.94931483268738 	 248.71906423568726 	 1.7364444732666016 	 0.8890707492828369 	 
2025-08-06 02:23:38.460728 test begin: paddle.dist(x=Tensor([1587601, 1, 4, 4],"float64"), y=Tensor([7, 1, 4],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f96d3ac3460>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 02:33:51.009783 test begin: paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 453601, 7, 1, 4],"float64"), )
W0806 02:33:51.632153 119339 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.dist 	 paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 453601, 7, 1, 4],"float64"), ) 	 25401688 	 21993 	 30.1487979888916 	 30.678595304489136 	 0.34957051277160645 	 0.47492003440856934 	 218.16035985946655 	 248.8477041721344 	 1.6886603832244873 	 0.826007604598999 	 
2025-08-06 02:42:43.902419 test begin: paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 8, 396901, 1, 4],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 8, 396901, 1, 4],"float64"), ) 	 25401696 	 21993 	 30.150625944137573 	 30.663153409957886 	 0.3495967388153076 	 0.4745335578918457 	 218.12896919250488 	 248.83796191215515 	 1.688405990600586 	 0.8259243965148926 	 
2025-08-06 02:51:34.622138 test begin: paddle.dist(x=Tensor([2, 1, 3175201, 4],"float64"), y=Tensor([7, 1, 4],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2d5396af80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:01:44.305296 test begin: paddle.dist(x=Tensor([2, 1, 4, 4],"float64"), y=Tensor([6350401, 1, 4],"float64"), )
W0806 03:01:44.910310 120779 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3ec5f92cb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:11:50.083940 test begin: paddle.dist(x=Tensor([2, 1, 793801, 4, 4],"float64"), y=Tensor([2, 8, 793801, 1, 4],"float64"), )
W0806 03:11:51.575771 121240 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff94123b0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:21:55.966003 test begin: paddle.dist(x=Tensor([2, 793801, 1, 4, 4],"float64"), y=Tensor([2, 793801, 7, 1, 4],"float64"), )
W0806 03:21:57.549049 121730 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f014bffaef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:32:01.835485 test begin: paddle.dist(x=Tensor([25401601],"float64"), y=Tensor([4, 25401601],"float64"), )
W0806 03:32:04.251366 122275 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.dist 	 paddle.dist(x=Tensor([25401601],"float64"), y=Tensor([4, 25401601],"float64"), ) 	 127008005 	 21993 	 51.56449294090271 	 51.792174339294434 	 0.5978636741638184 	 0.8012068271636963 	 186.30353140830994 	 277.05825781822205 	 2.1619253158569336 	 1.0726745128631592 	 
2025-08-06 03:41:32.666064 test begin: paddle.dist(x=Tensor([6350401],"float64"), y=Tensor([4, 6350401],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([6350401],"float64"), y=Tensor([4, 6350401],"float64"), ) 	 31752005 	 21993 	 13.22775650024414 	 13.173501968383789 	 0.15337705612182617 	 0.20381736755371094 	 47.11299109458923 	 70.14119005203247 	 0.5467202663421631 	 0.27149081230163574 	 
2025-08-06 03:43:57.072204 test begin: paddle.divide(Tensor([128, 396901],"float32"), Tensor([1, 396901],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([128, 396901],"float32"), Tensor([1, 396901],"float32"), ) 	 51200229 	 33832 	 10.143240690231323 	 10.541841506958008 	 0.30635929107666016 	 0.31783246994018555 	 27.092198133468628 	 62.143019676208496 	 0.40915751457214355 	 0.3127453327178955 	 
2025-08-06 03:45:51.103876 test begin: paddle.divide(Tensor([51059, 995],"float32"), Tensor([1, 995],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([51059, 995],"float32"), Tensor([1, 995],"float32"), ) 	 50804700 	 33832 	 10.053337335586548 	 10.468058347702026 	 0.30365848541259766 	 0.3162212371826172 	 30.5232892036438 	 62.96134877204895 	 0.307053804397583 	 0.2716848850250244 	 
2025-08-06 03:47:51.431900 test begin: paddle.divide(Tensor([51059, 995],"float32"), Tensor([51059, 995],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([51059, 995],"float32"), Tensor([51059, 995],"float32"), ) 	 101607410 	 33832 	 15.242721319198608 	 15.20426607131958 	 0.4602956771850586 	 0.4592928886413574 	 39.25950741767883 	 70.76138973236084 	 1.1872107982635498 	 0.4275689125061035 	 
2025-08-06 03:50:16.967716 test begin: paddle.divide(Tensor([512, 99226],"float32"), Tensor([1, 99226],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([512, 99226],"float32"), Tensor([1, 99226],"float32"), ) 	 50902938 	 33832 	 9.994982719421387 	 10.497667789459229 	 0.30197596549987793 	 0.3169553279876709 	 28.487447500228882 	 62.04525709152222 	 0.28658294677734375 	 0.31221866607666016 	 
2025-08-06 03:52:11.504265 test begin: paddle.divide(Tensor([544, 93431],"float32"), Tensor([1, 93431],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([544, 93431],"float32"), Tensor([1, 93431],"float32"), ) 	 50919895 	 33832 	 10.002021551132202 	 10.492008447647095 	 0.30210280418395996 	 0.3170173168182373 	 30.243523359298706 	 62.16090393066406 	 0.3041098117828369 	 0.31281352043151855 	 
2025-08-06 03:54:06.088936 test begin: paddle.divide(Tensor([544, 93431],"float32"), Tensor([544, 93431],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([544, 93431],"float32"), Tensor([544, 93431],"float32"), ) 	 101652928 	 33832 	 15.247599840164185 	 15.21260929107666 	 0.46058177947998047 	 0.459486722946167 	 39.27433657646179 	 70.79252290725708 	 1.1863765716552734 	 0.42775583267211914 	 
2025-08-06 03:56:29.138399 test begin: paddle.divide(x=Tensor([16934401, 3],"float32"), y=Tensor([3],"float32"), )
[Prof] paddle.divide 	 paddle.divide(x=Tensor([16934401, 3],"float32"), y=Tensor([3],"float32"), ) 	 50803206 	 33832 	 10.029887676239014 	 10.466028690338135 	 0.3029670715332031 	 0.3161187171936035 	 191.74245715141296 	 62.72211170196533 	 1.9320862293243408 	 0.2706327438354492 	 
2025-08-06 04:01:09.347044 test begin: paddle.divide(x=Tensor([187679, 271],"float32"), y=Tensor([271],"float32"), )
[Prof] paddle.divide 	 paddle.divide(x=Tensor([187679, 271],"float32"), y=Tensor([271],"float32"), ) 	 50861280 	 33832 	 10.720415353775024 	 10.499542474746704 	 0.3038499355316162 	 0.3163878917694092 	 33.96375489234924 	 62.13611650466919 	 0.34174346923828125 	 0.268115758895874 	 
2025-08-06 04:03:13.478197 test begin: paddle.dot(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), )
Warning: The core code of paddle.dot is too complex.
[Prof] paddle.dot 	 paddle.dot(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), ) 	 50803202 	 33667 	 206.45361804962158 	 9.869996786117554 	 6.266047716140747 	 0.14984345436096191 	 21.09101128578186 	 20.186044692993164 	 0.3201472759246826 	 0.30635905265808105 	 
2025-08-06 04:07:32.315243 test begin: paddle.dot(x=Tensor([50803201],"float32"), y=Tensor([50803201],"float32"), )
[Prof] paddle.dot 	 paddle.dot(x=Tensor([50803201],"float32"), y=Tensor([50803201],"float32"), ) 	 101606402 	 33667 	 9.975574254989624 	 9.850606441497803 	 0.3028392791748047 	 0.14954686164855957 	 23.909210920333862 	 20.319623470306396 	 0.36287498474121094 	 0.30840492248535156 	 
2025-08-06 04:08:40.565867 test begin: paddle.dot(x=Tensor([5080320],"int32"), y=Tensor([5080320],"int32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9673c0e740>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 04:22:11.260153 test begin: paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[-1,1,3,], )
W0806 04:22:15.067251 124316 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 04:22:42.364626 124316 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[-1,1,3,], ) 	 254016180 	 587939 	 18.166292190551758 	 6.236750602722168 	 0.000164031982421875 	 0.00012564659118652344 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:22:50.979043 test begin: paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[-1,], )
W0806 04:23:08.048650 124328 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[-1,], ) 	 254016180 	 587939 	 9.96528935432434 	 4.2020204067230225 	 0.00011539459228515625 	 9.441375732421875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:23:14.048129 test begin: paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[2,4,], )
W0806 04:23:35.176235 124333 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([14112010, 3, 6],"int64"), list[2,4,], ) 	 254016180 	 587939 	 13.981757402420044 	 4.947669744491577 	 0.00014162063598632812 	 0.00012302398681640625 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:23:42.759183 test begin: paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[-1,1,3,], )
W0806 04:24:10.114363 124341 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[-1,1,3,], ) 	 254016240 	 587939 	 18.083029985427856 	 5.718954801559448 	 0.000152587890625 	 8.893013000488281e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:24:17.903795 test begin: paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[-1,], )
W0806 04:24:35.154733 124350 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[-1,], ) 	 254016240 	 587939 	 10.08045482635498 	 4.191287517547607 	 0.00012040138244628906 	 0.00026726722717285156 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:24:43.286780 test begin: paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[2,4,], )
W0806 04:25:04.345790 124356 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 1058401, 6],"int64"), list[2,4,], ) 	 254016240 	 587939 	 13.937151670455933 	 5.003556251525879 	 0.00010752677917480469 	 0.00025916099548339844 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:25:14.465052 test begin: paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[-1,1,3,], )
W0806 04:25:43.134037 124364 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[-1,1,3,], ) 	 254016120 	 587939 	 18.05971598625183 	 5.701526165008545 	 0.00014853477478027344 	 8.749961853027344e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:25:51.166608 test begin: paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[-1,], )
W0806 04:26:08.065850 124376 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[-1,], ) 	 254016120 	 587939 	 9.917006015777588 	 4.195856809616089 	 0.00010132789611816406 	 0.00016379356384277344 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:26:13.849098 test begin: paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[2,4,], )
W0806 04:26:34.804674 124382 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.dsplit 	 paddle.dsplit(Tensor([40, 3, 2116801],"int64"), list[2,4,], ) 	 254016120 	 587939 	 13.954151153564453 	 4.924302816390991 	 0.00012874603271484375 	 0.0001747608184814453 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:26:42.016932 test begin: paddle.dstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], ) 	 76204872 	 31252 	 29.44847297668457 	 29.337204217910767 	 0.9629974365234375 	 0.9593362808227539 	 29.815272092819214 	 2.2169876098632812 	 0.9750373363494873 	 7.772445678710938e-05 	 
2025-08-06 04:28:16.269374 test begin: paddle.dstack(list[Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2, 1058401],"float64"),], ) 	 25401624 	 31252 	 10.005253791809082 	 9.781628370285034 	 0.3271796703338623 	 0.1597890853881836 	 9.934536695480347 	 1.8078033924102783 	 0.3248937129974365 	 9.298324584960938e-05 	 
2025-08-06 04:28:49.635963 test begin: paddle.dstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401900 	 31252 	 10.13063359260559 	 10.079230070114136 	 0.3312835693359375 	 0.32976555824279785 	 10.063699722290039 	 2.1685471534729004 	 0.3289961814880371 	 7.081031799316406e-05 	 
2025-08-06 04:29:23.232623 test begin: paddle.dstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401900 	 31252 	 10.129869222640991 	 10.282198667526245 	 0.3312561511993408 	 0.33614420890808105 	 10.03774118423462 	 2.2007157802581787 	 0.32823681831359863 	 7.2479248046875e-05 	 
2025-08-06 04:29:57.110907 test begin: paddle.dstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], ) 	 76204836 	 31252 	 30.279468774795532 	 29.34953737258911 	 0.9902684688568115 	 0.9597170352935791 	 30.18403434753418 	 2.172774076461792 	 0.9871339797973633 	 0.00010013580322265625 	 
2025-08-06 04:31:32.870102 test begin: paddle.dstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401660 	 31252 	 10.122973203659058 	 9.887749195098877 	 0.3311140537261963 	 0.32303619384765625 	 10.015454053878784 	 2.210029125213623 	 0.3275308609008789 	 7.05718994140625e-05 	 
2025-08-06 04:32:08.530010 test begin: paddle.dstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401660 	 31252 	 10.123734474182129 	 10.174428224563599 	 0.33106017112731934 	 0.33266472816467285 	 10.026165008544922 	 2.177217960357666 	 0.32789158821105957 	 0.00017905235290527344 	 
2025-08-06 04:32:42.548890 test begin: paddle.dstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2116801],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2116801],"float64"),], ) 	 25401660 	 31252 	 10.121618509292603 	 9.99912166595459 	 0.3309361934661865 	 0.32687807083129883 	 10.051658153533936 	 2.2054665088653564 	 0.3286869525909424 	 7.462501525878906e-05 	 
2025-08-06 04:33:17.033925 test begin: paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401900 	 31252 	 10.132803678512573 	 11.444994926452637 	 0.33141541481018066 	 0.32584595680236816 	 10.039767742156982 	 2.1660943031311035 	 0.32830810546875 	 0.00020813941955566406 	 
2025-08-06 04:33:54.255624 test begin: paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 76204980 	 31252 	 30.24171733856201 	 29.56640076637268 	 0.9890444278717041 	 0.9669489860534668 	 30.252320528030396 	 2.1700844764709473 	 0.9893307685852051 	 0.00012159347534179688 	 
2025-08-06 04:35:31.205820 test begin: paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401660 	 31252 	 10.116255044937134 	 9.775192022323608 	 0.33092665672302246 	 0.15980958938598633 	 10.026218891143799 	 1.795881986618042 	 0.32775139808654785 	 0.00021028518676757812 	 
2025-08-06 04:36:04.042791 test begin: paddle.dstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 76204818 	 31252 	 48.54169511795044 	 75.08395719528198 	 1.5874905586242676 	 2.455387592315674 	 81.0102767944336 	 2.1771786212921143 	 2.648974657058716 	 8.082389831542969e-05 	 
2025-08-06 04:39:34.316188 test begin: paddle.dstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 76204890 	 31252 	 30.078442811965942 	 78.81546688079834 	 0.9835896492004395 	 2.5776569843292236 	 33.580713510513306 	 2.3144969940185547 	 1.2638013362884521 	 6.794929504394531e-05 	 
2025-08-06 04:42:03.038734 test begin: paddle.dstack(list[Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401630 	 31252 	 12.17916750907898 	 9.774486541748047 	 0.3983769416809082 	 0.1598210334777832 	 12.372997522354126 	 1.7058508396148682 	 0.4046456813812256 	 7.462501525878906e-05 	 
2025-08-06 04:42:40.984927 test begin: paddle.dstack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], ) 	 76204824 	 31252 	 48.539294958114624 	 75.08460736274719 	 1.5872328281402588 	 2.45559024810791 	 81.04375123977661 	 2.1298019886016846 	 2.6500396728515625 	 7.390975952148438e-05 	 
2025-08-06 04:46:11.289364 test begin: paddle.dstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 76204920 	 31252 	 30.07830309867859 	 78.81483173370361 	 0.9836413860321045 	 2.577547550201416 	 33.27581572532654 	 2.194406509399414 	 1.0882985591888428 	 7.462501525878906e-05 	 
2025-08-06 04:48:41.270899 test begin: paddle.dstack(list[Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401640 	 31252 	 12.17790412902832 	 9.774538278579712 	 0.3983011245727539 	 0.1598069667816162 	 12.36624526977539 	 1.747849702835083 	 0.40436577796936035 	 7.581710815429688e-05 	 
2025-08-06 04:49:18.472968 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([200, 8, 498, 498],"float32"), Tensor([200, 8, 498, 64],"float32"), )
Warning: The core code of paddle.einsum is too complex.
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([200, 8, 498, 498],"float32"), Tensor([200, 8, 498, 64],"float32"), ) 	 447801600 	 12187 	 74.01436066627502 	 74.02655553817749 	 6.206508636474609 	 6.206632137298584 	 150.15093159675598 	 115.55925464630127 	 2.5195887088775635 	 4.845332145690918 	 
2025-08-06 04:56:21.769708 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([209, 8, 477, 477],"float32"), Tensor([209, 8, 477, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([209, 8, 477, 477],"float32"), Tensor([209, 8, 477, 64],"float32"), ) 	 431471304 	 12187 	 72.92552495002747 	 72.92289018630981 	 6.115490436553955 	 6.114956855773926 	 149.88692212104797 	 116.41132807731628 	 2.514352798461914 	 4.88093638420105 	 
2025-08-06 05:03:23.364555 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([218, 8, 457, 457],"float32"), Tensor([218, 8, 457, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([218, 8, 457, 457],"float32"), Tensor([218, 8, 457, 64],"float32"), ) 	 415241168 	 12187 	 74.42904043197632 	 73.87039589881897 	 6.195231914520264 	 6.19479513168335 	 151.10672569274902 	 119.43759346008301 	 2.5352630615234375 	 5.007903337478638 	 
2025-08-06 05:10:29.948322 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([26, 8, 498, 498],"float32"), Tensor([26, 8, 498, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([26, 8, 498, 498],"float32"), Tensor([26, 8, 498, 64],"float32"), ) 	 58214208 	 12187 	 9.997188806533813 	 9.997633457183838 	 0.8383703231811523 	 0.8383350372314453 	 20.138125896453857 	 15.543425798416138 	 0.3379647731781006 	 0.6516687870025635 	 
2025-08-06 05:11:26.758357 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([28, 8, 477, 477],"float32"), Tensor([28, 8, 477, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([28, 8, 477, 477],"float32"), Tensor([28, 8, 477, 64],"float32"), ) 	 57804768 	 12187 	 10.685802459716797 	 10.685836791992188 	 0.8961317539215088 	 0.8961136341094971 	 21.25370955467224 	 16.6562602519989 	 0.35667920112609863 	 0.6983249187469482 	 
2025-08-06 05:12:27.155684 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 54, 498, 498],"float32"), Tensor([30, 54, 498, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 54, 498, 498],"float32"), Tensor([30, 54, 498, 64],"float32"), ) 	 453399120 	 12187 	 74.0609495639801 	 74.05512475967407 	 6.210611343383789 	 6.210392475128174 	 151.17930793762207 	 116.18072032928467 	 2.5370445251464844 	 4.871273756027222 	 
2025-08-06 05:19:31.016151 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 56, 477, 477],"float32"), Tensor([30, 56, 477, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 56, 477, 477],"float32"), Tensor([30, 56, 477, 64],"float32"), ) 	 433535760 	 12187 	 74.607346534729 	 74.03720211982727 	 6.208905458450317 	 6.208779573440552 	 151.3464379310608 	 117.74826908111572 	 2.5400962829589844 	 4.937150955200195 	 
2025-08-06 05:26:39.908143 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 58, 457, 457],"float32"), Tensor([30, 58, 457, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 58, 457, 457],"float32"), Tensor([30, 58, 457, 64],"float32"), ) 	 414288780 	 12187 	 73.8696014881134 	 73.86216378211975 	 6.19447922706604 	 6.194181680679321 	 150.94913458824158 	 119.35658240318298 	 2.5332257747650146 	 5.005363941192627 	 
2025-08-06 05:33:45.983833 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 7, 498, 498],"float32"), Tensor([30, 7, 498, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 7, 498, 498],"float32"), Tensor([30, 7, 498, 64],"float32"), ) 	 58773960 	 12187 	 9.997286319732666 	 9.998022556304932 	 0.8383471965789795 	 0.838383674621582 	 20.26619839668274 	 15.586583137512207 	 0.3400559425354004 	 0.653465747833252 	 
2025-08-06 05:34:43.243657 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 426, 498],"float32"), Tensor([30, 8, 498, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 426, 498],"float32"), Tensor([30, 8, 498, 64],"float32"), ) 	 58564800 	 12187 	 11.22300100326538 	 11.221397638320923 	 0.9411556720733643 	 0.9409916400909424 	 20.6034996509552 	 16.062901258468628 	 0.3457214832305908 	 0.6734249591827393 	 
2025-08-06 05:35:45.780537 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 444, 477],"float32"), Tensor([30, 8, 477, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 444, 477],"float32"), Tensor([30, 8, 477, 64],"float32"), ) 	 58155840 	 12187 	 10.705543756484985 	 10.703900814056396 	 0.8977737426757812 	 0.8976011276245117 	 20.95679807662964 	 16.41147780418396 	 0.3516364097595215 	 0.6880319118499756 	 
2025-08-06 05:36:45.715354 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 457, 457],"float32"), Tensor([30, 8, 457, 464],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 457, 457],"float32"), Tensor([30, 8, 457, 464],"float32"), ) 	 101015280 	 12187 	 40.98560881614685 	 41.2158362865448 	 3.437062978744507 	 3.4370126724243164 	 92.80912375450134 	 82.14967775344849 	 1.5572011470794678 	 3.444445848464966 	 
2025-08-06 05:41:09.193304 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 464, 457],"float32"), Tensor([30, 8, 457, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 464, 457],"float32"), Tensor([30, 8, 457, 64],"float32"), ) 	 57911040 	 12187 	 10.357505798339844 	 10.358866453170776 	 0.8685872554779053 	 0.868488073348999 	 21.239298343658447 	 16.748640537261963 	 0.35642337799072266 	 0.7021462917327881 	 
2025-08-06 05:42:09.027065 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 477, 477],"float32"), Tensor([30, 8, 477, 444],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 477, 477],"float32"), Tensor([30, 8, 477, 444],"float32"), ) 	 105436080 	 12187 	 42.40573024749756 	 42.40598797798157 	 3.5560858249664307 	 3.556169271469116 	 93.26214504241943 	 82.15520691871643 	 1.5649011135101318 	 3.4445841312408447 	 
2025-08-06 05:46:32.033770 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 477, 477],"float32"), Tensor([30, 8, 477, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 477, 477],"float32"), Tensor([30, 8, 477, 64],"float32"), ) 	 61933680 	 12187 	 10.708914279937744 	 10.708972692489624 	 0.8980441093444824 	 0.8979992866516113 	 21.979453086853027 	 17.083542108535767 	 0.36887288093566895 	 0.7162363529205322 	 
2025-08-06 05:47:33.750399 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 498, 498],"float32"), Tensor([30, 8, 498, 426],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 498, 498],"float32"), Tensor([30, 8, 498, 426],"float32"), ) 	 110436480 	 12187 	 44.503639459609985 	 44.604363679885864 	 3.732103109359741 	 3.8320765495300293 	 94.29038429260254 	 82.87933135032654 	 1.5823299884796143 	 3.4749186038970947 	 
2025-08-06 05:52:04.203091 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 9, 457, 457],"float32"), Tensor([30, 9, 457, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 9, 457, 457],"float32"), Tensor([30, 9, 457, 64],"float32"), ) 	 64286190 	 12187 	 11.49143385887146 	 11.501538515090942 	 0.9637086391448975 	 0.9637908935546875 	 23.66879653930664 	 18.677815914154053 	 0.3971700668334961 	 0.7830586433410645 	 
2025-08-06 05:53:11.064157 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([31, 8, 457, 457],"float32"), Tensor([31, 8, 457, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([31, 8, 457, 457],"float32"), Tensor([31, 8, 457, 64],"float32"), ) 	 59048056 	 12187 	 11.464020013809204 	 11.475695610046387 	 0.9613604545593262 	 0.9619498252868652 	 22.680827379226685 	 18.093174695968628 	 0.3805992603302002 	 0.7585797309875488 	 
2025-08-06 05:54:17.760071 test begin: paddle.empty_like(Tensor([1016064010],"uint8"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([1016064010],"uint8"), ) 	 1016064010 	 694973 	 7.752743721008301 	 3.6436867713928223 	 7.62939453125e-05 	 8.106231689453125e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 05:54:38.684482 test begin: paddle.empty_like(Tensor([40960, 12404],"bool"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([40960, 12404],"bool"), ) 	 508067840 	 694973 	 7.906154632568359 	 4.3151421546936035 	 7.891654968261719e-05 	 0.00021338462829589844 	 None 	 None 	 None 	 None 	 
2025-08-06 05:55:00.343160 test begin: paddle.empty_like(Tensor([40960, 12404],"float32"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([40960, 12404],"float32"), ) 	 508067840 	 694973 	 7.785653352737427 	 3.698652982711792 	 6.532669067382812e-05 	 8.106231689453125e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 05:55:20.067174 test begin: paddle.empty_like(Tensor([7938010, 64],"bool"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([7938010, 64],"bool"), ) 	 508032640 	 694973 	 7.880437850952148 	 3.6529507637023926 	 9.751319885253906e-05 	 0.000152587890625 	 None 	 None 	 None 	 None 	 
2025-08-06 05:55:38.634468 test begin: paddle.empty_like(Tensor([7938010, 64],"float32"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([7938010, 64],"float32"), ) 	 508032640 	 694973 	 7.825258255004883 	 3.4518494606018066 	 3.528594970703125e-05 	 0.00020647048950195312 	 None 	 None 	 None 	 None 	 
2025-08-06 05:55:58.322978 test begin: paddle.equal(Tensor([4148, 6124],"int64"), Tensor([4148, 6124],"int64"), )
[Prof] paddle.equal 	 paddle.equal(Tensor([4148, 6124],"int64"), Tensor([4148, 6124],"int64"), ) 	 50804704 	 56247 	 17.399617671966553 	 17.605318546295166 	 0.31618309020996094 	 0.3198893070220947 	 None 	 None 	 None 	 None 	 
2025-08-06 05:56:34.279737 test begin: paddle.equal(Tensor([416, 61062],"int64"), 0, )
[Prof] paddle.equal 	 paddle.equal(Tensor([416, 61062],"int64"), 0, ) 	 25401792 	 56247 	 10.001267910003662 	 9.451437711715698 	 0.09084486961364746 	 0.17174601554870605 	 None 	 None 	 None 	 None 	 
2025-08-06 05:56:54.210264 test begin: paddle.equal(Tensor([512, 49613],"int64"), 0, )
[Prof] paddle.equal 	 paddle.equal(Tensor([512, 49613],"int64"), 0, ) 	 25401856 	 56247 	 9.965906858444214 	 9.451320886611938 	 0.09052777290344238 	 0.17173528671264648 	 None 	 None 	 None 	 None 	 
2025-08-06 05:57:14.088905 test begin: paddle.equal(Tensor([846721, 30],"int64"), 0, )
[Prof] paddle.equal 	 paddle.equal(Tensor([846721, 30],"int64"), 0, ) 	 25401630 	 56247 	 10.007745504379272 	 9.451425790786743 	 0.09090971946716309 	 0.17173004150390625 	 None 	 None 	 None 	 None 	 
2025-08-06 05:57:34.019770 test begin: paddle.equal(Tensor([846721, 30],"int64"), Tensor([846721, 30],"int64"), )
[Prof] paddle.equal 	 paddle.equal(Tensor([846721, 30],"int64"), Tensor([846721, 30],"int64"), ) 	 50803260 	 56247 	 17.46580481529236 	 17.60715389251709 	 0.31735920906066895 	 0.3199331760406494 	 None 	 None 	 None 	 None 	 
2025-08-06 05:58:10.011191 test begin: paddle.equal_all(Tensor([1, 2, 10, 2540161],"bool"), Tensor([1, 2, 10, 2540161],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 2, 10, 2540161],"bool"), Tensor([1, 2, 10, 2540161],"bool"), ) 	 101606440 	 555286 	 94.18337154388428 	 113.70184898376465 	 0.057741403579711914 	 0.00023746490478515625 	 None 	 None 	 None 	 None 	 
2025-08-06 06:01:40.902270 test begin: paddle.equal_all(Tensor([1, 2, 1587601, 16],"bool"), Tensor([1, 2, 1587601, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 2, 1587601, 16],"bool"), Tensor([1, 2, 1587601, 16],"bool"), ) 	 101606464 	 555286 	 94.18967628479004 	 114.51420378684998 	 0.057721853256225586 	 0.0002665519714355469 	 None 	 None 	 None 	 None 	 
2025-08-06 06:05:11.032978 test begin: paddle.equal_all(Tensor([1, 317521, 10, 16],"bool"), Tensor([1, 317521, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 317521, 10, 16],"bool"), Tensor([1, 317521, 10, 16],"bool"), ) 	 101606720 	 555286 	 94.18692803382874 	 114.0774199962616 	 0.057758331298828125 	 0.00023889541625976562 	 None 	 None 	 None 	 None 	 
2025-08-06 06:08:41.929460 test begin: paddle.equal_all(Tensor([101, 2, 10, 16],"bool"), Tensor([158761, 2, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([101, 2, 10, 16],"bool"), Tensor([158761, 2, 10, 16],"bool"), ) 	 50835840 	 555286 	 12.122812271118164 	 2.2589690685272217 	 0.00013637542724609375 	 0.00022482872009277344 	 None 	 None 	 None 	 None 	 
2025-08-06 06:08:57.806235 test begin: paddle.equal_all(Tensor([12801],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([12801],"float32"), Tensor([50803201],"float32"), ) 	 50816002 	 555286 	 9.307132720947266 	 1.366398572921753 	 9.560585021972656e-05 	 0.00010728836059570312 	 None 	 None 	 None 	 None 	 
2025-08-06 06:09:09.343716 test begin: paddle.equal_all(Tensor([158761, 2, 10, 16],"bool"), Tensor([101, 2, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([158761, 2, 10, 16],"bool"), Tensor([101, 2, 10, 16],"bool"), ) 	 50835840 	 555286 	 9.342417001724243 	 1.3779876232147217 	 9.369850158691406e-05 	 0.00010752677917480469 	 None 	 None 	 None 	 None 	 
2025-08-06 06:09:20.785743 test begin: paddle.equal_all(Tensor([158761, 2, 10, 16],"bool"), Tensor([158761, 2, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([158761, 2, 10, 16],"bool"), Tensor([158761, 2, 10, 16],"bool"), ) 	 101607040 	 555286 	 94.00760817527771 	 114.28801226615906 	 0.05761075019836426 	 0.00024271011352539062 	 None 	 None 	 None 	 None 	 
2025-08-06 06:12:50.509563 test begin: paddle.equal_all(Tensor([16, 3175201],"float32"), Tensor([16, 3175201],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([16, 3175201],"float32"), Tensor([16, 3175201],"float32"), ) 	 101606432 	 555286 	 210.81518292427063 	 231.40332698822021 	 0.1291675567626953 	 0.0002429485321044922 	 None 	 None 	 None 	 None 	 
2025-08-06 06:20:14.535602 test begin: paddle.equal_all(Tensor([16, 3175201],"float32"), Tensor([1601, 16],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([16, 3175201],"float32"), Tensor([1601, 16],"float32"), ) 	 50828832 	 555286 	 9.314929723739624 	 1.351691722869873 	 0.00010561943054199219 	 0.00018405914306640625 	 None 	 None 	 None 	 None 	 
2025-08-06 06:20:27.489924 test begin: paddle.equal_all(Tensor([1601, 16],"float32"), Tensor([16, 3175201],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1601, 16],"float32"), Tensor([16, 3175201],"float32"), ) 	 50828832 	 555286 	 13.177336692810059 	 2.3034658432006836 	 0.0001068115234375 	 0.00011587142944335938 	 None 	 None 	 None 	 None 	 
2025-08-06 06:20:45.881067 test begin: paddle.equal_all(Tensor([1601, 16],"float32"), Tensor([3175201, 16],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1601, 16],"float32"), Tensor([3175201, 16],"float32"), ) 	 50828832 	 555286 	 13.158058404922485 	 2.368896245956421 	 9.703636169433594e-05 	 0.00011515617370605469 	 None 	 None 	 None 	 None 	 
2025-08-06 06:21:02.327260 test begin: paddle.equal_all(Tensor([3175201, 16],"float32"), Tensor([1601, 16],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([3175201, 16],"float32"), Tensor([1601, 16],"float32"), ) 	 50828832 	 555286 	 13.168565511703491 	 2.2947754859924316 	 0.00010609626770019531 	 0.000125885009765625 	 None 	 None 	 None 	 None 	 
2025-08-06 06:21:18.675867 test begin: paddle.equal_all(Tensor([3175201, 16],"float32"), Tensor([3175201, 16],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([3175201, 16],"float32"), Tensor([3175201, 16],"float32"), ) 	 101606432 	 555286 	 210.8165566921234 	 231.47235870361328 	 0.1291208267211914 	 0.00023365020751953125 	 None 	 None 	 None 	 None 	 
2025-08-06 06:28:44.785942 test begin: paddle.equal_all(Tensor([50803201],"float32"), Tensor([12801],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([50803201],"float32"), Tensor([12801],"float32"), ) 	 50816002 	 555286 	 13.110646486282349 	 2.2984862327575684 	 0.0001049041748046875 	 0.00022220611572265625 	 None 	 None 	 None 	 None 	 
2025-08-06 06:29:01.103597 test begin: paddle.equal_all(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 555286 	 210.81338548660278 	 231.5738706588745 	 0.1291365623474121 	 0.0002810955047607422 	 None 	 None 	 None 	 None 	 
2025-08-06 06:36:25.254718 test begin: paddle.erf(Tensor([11, 2309237],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([11, 2309237],"float64"), ) 	 25401607 	 29636 	 10.262446880340576 	 8.999129056930542 	 0.3550231456756592 	 0.31038451194763184 	 13.258793354034424 	 48.524409532547 	 0.4572286605834961 	 0.33475255966186523 	 
2025-08-06 06:37:47.627289 test begin: paddle.erf(Tensor([1494212, 17],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([1494212, 17],"float64"), ) 	 25401604 	 29636 	 10.120410680770874 	 9.029658079147339 	 0.3499174118041992 	 0.310530424118042 	 13.259030818939209 	 48.526753664016724 	 0.45720696449279785 	 0.33476877212524414 	 
2025-08-06 06:39:11.382496 test begin: paddle.erf(Tensor([211681, 2, 3, 5, 4],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([211681, 2, 3, 5, 4],"float64"), ) 	 25401720 	 29636 	 10.0612952709198 	 9.762669801712036 	 0.34675002098083496 	 0.3106358051300049 	 13.275301694869995 	 48.527698278427124 	 0.4578268527984619 	 0.33472132682800293 	 
2025-08-06 06:40:38.745241 test begin: paddle.erf(Tensor([4, 105841, 3, 5, 4],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 105841, 3, 5, 4],"float64"), ) 	 25401840 	 29636 	 10.031068563461304 	 9.00459623336792 	 0.3463726043701172 	 0.3105924129486084 	 13.280754566192627 	 48.528783082962036 	 0.45803284645080566 	 0.3347194194793701 	 
2025-08-06 06:42:03.134296 test begin: paddle.erf(Tensor([4, 2, 158761, 5, 4],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 158761, 5, 4],"float64"), ) 	 25401760 	 29636 	 10.065646886825562 	 9.005454778671265 	 0.3481731414794922 	 0.3104393482208252 	 13.257669448852539 	 48.526748180389404 	 0.4571812152862549 	 0.3347434997558594 	 
2025-08-06 06:43:27.442127 test begin: paddle.erf(Tensor([4, 2, 3, 1058401],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 3, 1058401],"float64"), ) 	 25401624 	 29636 	 10.110672950744629 	 9.003785610198975 	 0.34954333305358887 	 0.3104696273803711 	 13.259035348892212 	 48.52717208862305 	 0.4572443962097168 	 0.33479738235473633 	 
2025-08-06 06:44:50.202134 test begin: paddle.erf(Tensor([4, 2, 3, 264601, 4],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 3, 264601, 4],"float64"), ) 	 25401696 	 29636 	 10.138778448104858 	 9.01349401473999 	 0.35001516342163086 	 0.3105740547180176 	 13.262545824050903 	 48.52720284461975 	 0.45734286308288574 	 0.33472132682800293 	 
2025-08-06 06:46:12.297408 test begin: paddle.erf(Tensor([4, 2, 3, 5, 211681],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 3, 5, 211681],"float64"), ) 	 25401720 	 29636 	 10.209466457366943 	 10.258673191070557 	 0.35370826721191406 	 0.31032252311706543 	 13.275333404541016 	 48.528666734695435 	 0.45776796340942383 	 0.33470630645751953 	 
2025-08-06 06:47:39.348406 test begin: paddle.erf(Tensor([4, 2, 635041, 5],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 635041, 5],"float64"), ) 	 25401640 	 29636 	 10.122608423233032 	 9.005592584609985 	 0.349717378616333 	 0.31058573722839355 	 13.250943183898926 	 48.526506185531616 	 0.45688843727111816 	 0.334766149520874 	 
2025-08-06 06:49:01.397603 test begin: paddle.erf(Tensor([4, 423361, 3, 5],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 423361, 3, 5],"float64"), ) 	 25401660 	 29636 	 10.184973239898682 	 8.999903917312622 	 0.35210609436035156 	 0.3105027675628662 	 13.25059723854065 	 48.528358459472656 	 0.45688390731811523 	 0.33475685119628906 	 
2025-08-06 06:50:23.587860 test begin: paddle.erf(Tensor([846721, 2, 3, 5],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([846721, 2, 3, 5],"float64"), ) 	 25401630 	 29636 	 10.129680395126343 	 9.005622863769531 	 0.3498964309692383 	 0.31055140495300293 	 13.25944447517395 	 48.52930688858032 	 0.4572887420654297 	 0.3347494602203369 	 
2025-08-06 06:51:45.801404 test begin: paddle.erfinv(x=Tensor([211681, 2, 3, 5, 4],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([211681, 2, 3, 5, 4],"float64"), ) 	 25401720 	 29882 	 10.42147707939148 	 10.771636247634888 	 0.35671401023864746 	 0.3307607173919678 	 13.381270170211792 	 49.04813766479492 	 0.4577312469482422 	 0.33554530143737793 	 
2025-08-06 06:53:12.449182 test begin: paddle.erfinv(x=Tensor([4, 105841, 3, 5, 4],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 105841, 3, 5, 4],"float64"), ) 	 25401840 	 29882 	 10.316900491714478 	 9.542097091674805 	 0.3541080951690674 	 0.3248109817504883 	 13.381457090377808 	 49.049548625946045 	 0.45764780044555664 	 0.33564233779907227 	 
2025-08-06 06:54:40.353607 test begin: paddle.erfinv(x=Tensor([4, 2, 158761, 5, 4],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 158761, 5, 4],"float64"), ) 	 25401760 	 29882 	 10.199154138565063 	 9.505274057388306 	 0.3508732318878174 	 0.3252747058868408 	 13.362167119979858 	 49.04936218261719 	 0.4569356441497803 	 0.3355534076690674 	 
2025-08-06 06:56:03.646608 test begin: paddle.erfinv(x=Tensor([4, 2, 3, 1058401],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 3, 1058401],"float64"), ) 	 25401624 	 29882 	 10.357975482940674 	 9.61890983581543 	 0.3562760353088379 	 0.32944536209106445 	 13.359755039215088 	 49.04879665374756 	 0.45687127113342285 	 0.3355724811553955 	 
2025-08-06 06:57:28.760049 test begin: paddle.erfinv(x=Tensor([4, 2, 3, 264601, 4],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 3, 264601, 4],"float64"), ) 	 25401696 	 29882 	 10.477333307266235 	 9.751954078674316 	 0.35906386375427246 	 0.3317382335662842 	 13.363238334655762 	 49.04739689826965 	 0.4571053981781006 	 0.33560681343078613 	 
2025-08-06 06:58:52.540194 test begin: paddle.erfinv(x=Tensor([4, 2, 3, 5, 211681],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 3, 5, 211681],"float64"), ) 	 25401720 	 29882 	 10.627954959869385 	 9.764807939529419 	 0.3638932704925537 	 0.3350367546081543 	 13.38133716583252 	 49.04692363739014 	 0.4577298164367676 	 0.33557796478271484 	 
2025-08-06 07:00:17.532879 test begin: paddle.erfinv(x=Tensor([4, 2, 3175201],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 3175201],"float64"), ) 	 25401608 	 29882 	 10.517693758010864 	 9.735232830047607 	 0.35748291015625 	 0.3325800895690918 	 13.360146045684814 	 49.05534887313843 	 0.45693087577819824 	 0.335543155670166 	 
2025-08-06 07:01:44.662082 test begin: paddle.erfinv(x=Tensor([4, 2, 635041, 5],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 635041, 5],"float64"), ) 	 25401640 	 29882 	 10.558389902114868 	 9.861054420471191 	 0.36253881454467773 	 0.3381919860839844 	 13.368035554885864 	 49.04895329475403 	 0.4572746753692627 	 0.3355214595794678 	 
2025-08-06 07:03:08.667267 test begin: paddle.erfinv(x=Tensor([4, 2116801, 3],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2116801, 3],"float64"), ) 	 25401612 	 29882 	 10.537502765655518 	 9.765012502670288 	 0.36058855056762695 	 0.33321404457092285 	 13.359720945358276 	 49.047746896743774 	 0.4569559097290039 	 0.3355369567871094 	 
2025-08-06 07:04:32.508687 test begin: paddle.erfinv(x=Tensor([4, 423361, 3, 5],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 423361, 3, 5],"float64"), ) 	 25401660 	 29882 	 10.616095066070557 	 9.779867887496948 	 0.3639192581176758 	 0.33486008644104004 	 13.36810302734375 	 49.04837894439697 	 0.45723533630371094 	 0.33559346199035645 	 
2025-08-06 07:05:56.466923 test begin: paddle.erfinv(x=Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4233601, 2, 3],"float64"), ) 	 25401606 	 29882 	 10.508268117904663 	 9.840912818908691 	 0.36063551902770996 	 0.33812594413757324 	 13.360430717468262 	 49.05381202697754 	 0.45688939094543457 	 0.33559751510620117 	 
2025-08-06 07:07:20.563355 test begin: paddle.erfinv(x=Tensor([846721, 2, 3, 5],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([846721, 2, 3, 5],"float64"), ) 	 25401630 	 29882 	 10.53181266784668 	 9.796068906784058 	 0.3606081008911133 	 0.33431220054626465 	 13.360124826431274 	 49.04745101928711 	 0.45692992210388184 	 0.3355374336242676 	 
2025-08-06 07:08:44.493780 test begin: paddle.exp(Tensor([125, 1, 640, 640],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([125, 1, 640, 640],"float32"), ) 	 51200000 	 33820 	 10.080507040023804 	 10.14088749885559 	 0.3045923709869385 	 0.30644750595092773 	 15.335878133773804 	 15.214287757873535 	 0.46347904205322266 	 0.45973849296569824 	 
2025-08-06 07:09:40.494861 test begin: paddle.exp(Tensor([13, 243, 1007, 16],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([13, 243, 1007, 16],"float32"), ) 	 50897808 	 33820 	 10.009024143218994 	 10.086878061294556 	 0.302471399307251 	 0.3046760559082031 	 15.244856119155884 	 15.125145673751831 	 0.4606926441192627 	 0.45703744888305664 	 
2025-08-06 07:10:33.657780 test begin: paddle.exp(Tensor([13, 64, 1007, 61],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([13, 64, 1007, 61],"float32"), ) 	 51107264 	 33820 	 10.052348852157593 	 10.12273383140564 	 0.303804874420166 	 0.3059275150299072 	 15.3038809299469 	 15.185645341873169 	 0.4624745845794678 	 0.4588778018951416 	 
2025-08-06 07:11:26.117540 test begin: paddle.exp(Tensor([13, 64, 3817, 16],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([13, 64, 3817, 16],"float32"), ) 	 50811904 	 33820 	 9.996909379959106 	 10.0683434009552 	 0.3021843433380127 	 0.30414867401123047 	 15.225414276123047 	 15.101816892623901 	 0.46011996269226074 	 0.4563870429992676 	 
2025-08-06 07:12:18.265063 test begin: paddle.exp(Tensor([16, 1, 4962, 640],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([16, 1, 4962, 640],"float32"), ) 	 50810880 	 33820 	 10.006614685058594 	 10.071606636047363 	 0.302382230758667 	 0.30415797233581543 	 15.224220037460327 	 15.1017165184021 	 0.4600815773010254 	 0.45635223388671875 	 
2025-08-06 07:13:11.025583 test begin: paddle.exp(Tensor([16, 1, 640, 4962],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([16, 1, 640, 4962],"float32"), ) 	 50810880 	 33820 	 10.006332397460938 	 10.082813739776611 	 0.30240511894226074 	 0.3040950298309326 	 15.223962545394897 	 15.100748538970947 	 0.45997190475463867 	 0.45636630058288574 	 
2025-08-06 07:14:04.877473 test begin: paddle.exp(Tensor([16, 8, 640, 640],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([16, 8, 640, 640],"float32"), ) 	 52428800 	 33820 	 10.313303232192993 	 10.378100395202637 	 0.31165385246276855 	 0.3135983943939209 	 15.704816341400146 	 15.574244499206543 	 0.47463417053222656 	 0.470639705657959 	 
2025-08-06 07:15:01.391952 test begin: paddle.exp(Tensor([50, 64, 1007, 16],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([50, 64, 1007, 16],"float32"), ) 	 51558400 	 33820 	 10.149755239486694 	 10.210458993911743 	 0.3066904544830322 	 0.3085601329803467 	 15.4461030960083 	 15.322184324264526 	 0.466815710067749 	 0.46300745010375977 	 
2025-08-06 07:15:54.272221 test begin: paddle.exp(Tensor([56, 1, 960, 960],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([56, 1, 960, 960],"float32"), ) 	 51609600 	 33820 	 10.159064292907715 	 10.219939231872559 	 0.30700254440307617 	 0.30884742736816406 	 15.461419820785522 	 15.336690187454224 	 0.4672119617462158 	 0.4634392261505127 	 
2025-08-06 07:16:47.182014 test begin: paddle.exp(Tensor([8, 1, 6616, 960],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([8, 1, 6616, 960],"float32"), ) 	 50810880 	 33820 	 10.006674766540527 	 10.068568468093872 	 0.3023500442504883 	 0.30413317680358887 	 15.224352359771729 	 15.101656913757324 	 0.46004796028137207 	 0.4563486576080322 	 
2025-08-06 07:17:40.537769 test begin: paddle.exp(Tensor([8, 1, 960, 6616],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([8, 1, 960, 6616],"float32"), ) 	 50810880 	 33820 	 10.007761001586914 	 10.064988374710083 	 0.3024106025695801 	 0.30411601066589355 	 15.224342346191406 	 15.101702213287354 	 0.4601006507873535 	 0.4563744068145752 	 
2025-08-06 07:18:32.676755 test begin: paddle.exp(Tensor([8, 7, 960, 960],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([8, 7, 960, 960],"float32"), ) 	 51609600 	 33820 	 10.159133195877075 	 10.221447467803955 	 0.30698513984680176 	 0.308835506439209 	 15.46105670928955 	 15.33672022819519 	 0.467207670211792 	 0.46346473693847656 	 
2025-08-06 07:19:25.589540 test begin: paddle.expand_as(Tensor([1621, 80, 1, 1],"float32"), Tensor([1621, 80, 28, 28],"float16"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([1621, 80, 1, 1],"float32"), Tensor([1621, 80, 28, 28],"float16"), ) 	 101798800 	 72722 	 19.635741233825684 	 0.26807355880737305 	 0.2758772373199463 	 6.365776062011719e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:20:10.578094 test begin: paddle.expand_as(Tensor([511, 127, 1, 1],"float32"), Tensor([511, 127, 28, 28],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([511, 127, 1, 1],"float32"), Tensor([511, 127, 28, 28],"float32"), ) 	 50944145 	 72722 	 9.970078468322754 	 0.2680356502532959 	 0.14011335372924805 	 0.000125885009765625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:20:33.522259 test begin: paddle.expand_as(Tensor([511, 80, 1, 1243],"float32"), Tensor([511, 80, 28, 1243],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f30f84cedd0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 07:30:45.336792 test begin: paddle.expand_as(Tensor([511, 80, 1, 1],"float32"), Tensor([511, 80, 28, 45],"float32"), )
W0806 07:30:46.290799 129631 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([511, 80, 1, 1],"float32"), Tensor([511, 80, 28, 45],"float32"), ) 	 51549680 	 72722 	 10.117659568786621 	 0.5494463443756104 	 0.14214015007019043 	 6.651878356933594e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:31:12.154757 test begin: paddle.expand_as(Tensor([511, 80, 1, 1],"float32"), Tensor([511, 80, 45, 28],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([511, 80, 1, 1],"float32"), Tensor([511, 80, 45, 28],"float32"), ) 	 51549680 	 72722 	 11.006548404693604 	 0.2683546543121338 	 0.1421506404876709 	 3.361701965332031e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:31:42.343515 test begin: paddle.expand_as(Tensor([511, 80, 1243, 1],"float32"), Tensor([511, 80, 1243, 28],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f320c5bae90>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 07:41:53.664631 test begin: paddle.expand_as(Tensor([512, 127, 1, 1],"float32"), Tensor([512, 127, 28, 28],"float32"), )
W0806 07:41:54.741937 130025 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 127, 1, 1],"float32"), Tensor([512, 127, 28, 28],"float32"), ) 	 51043840 	 72722 	 10.000746488571167 	 0.5493698120117188 	 0.14052438735961914 	 9.441375732421875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:42:17.736058 test begin: paddle.expand_as(Tensor([512, 254, 1, 1],"float32"), Tensor([512, 254, 28, 28],"float16"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 254, 1, 1],"float32"), Tensor([512, 254, 28, 28],"float16"), ) 	 102087680 	 72722 	 19.71565055847168 	 0.2655041217803955 	 0.27803564071655273 	 6.103515625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:43:03.012549 test begin: paddle.expand_as(Tensor([512, 80, 1, 1241],"float32"), Tensor([512, 80, 28, 1241],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7e23efb190>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 07:53:15.235116 test begin: paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 28, 45],"float32"), )
W0806 07:53:16.213598 130414 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 28, 45],"float32"), ) 	 51650560 	 72722 	 10.141124486923218 	 0.5459365844726562 	 0.1425464153289795 	 0.00030875205993652344 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:53:43.574743 test begin: paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 28, 89],"float16"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 28, 89],"float16"), ) 	 102113280 	 72722 	 19.711021661758423 	 0.2538471221923828 	 0.27701497077941895 	 3.1948089599609375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:54:28.278235 test begin: paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 45, 28],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 45, 28],"float32"), ) 	 51650560 	 72722 	 10.135940551757812 	 0.5153405666351318 	 0.14242172241210938 	 8.7738037109375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:54:51.796007 test begin: paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 89, 28],"float16"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 89, 28],"float16"), ) 	 102113280 	 72722 	 19.71147060394287 	 0.5202963352203369 	 0.27698230743408203 	 0.0002894401550292969 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:55:42.147822 test begin: paddle.expand_as(Tensor([512, 80, 1241, 1],"float32"), Tensor([512, 80, 1241, 28],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff0e9e1ef50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:05:58.337230 test begin: paddle.expand_as(Tensor([811, 80, 1, 1],"float32"), Tensor([811, 80, 28, 28],"float32"), )
W0806 08:05:59.332223 131071 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([811, 80, 1, 1],"float32"), Tensor([811, 80, 28, 28],"float32"), ) 	 50930800 	 72722 	 9.991625308990479 	 0.2904486656188965 	 0.14041996002197266 	 0.00019097328186035156 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 08:06:22.133497 test begin: paddle.expm1(Tensor([198451, 16, 32],"float16"), )
[Prof] paddle.expm1 	 paddle.expm1(Tensor([198451, 16, 32],"float16"), ) 	 101606912 	 29864 	 9.99755573272705 	 9.071418046951294 	 0.3421211242675781 	 0.3104822635650635 	 13.380993843078613 	 22.26071047782898 	 0.45803308486938477 	 0.38082385063171387 	 
2025-08-06 08:07:22.135658 test begin: paddle.expm1(Tensor([8, 16, 793801],"float16"), )
[Prof] paddle.expm1 	 paddle.expm1(Tensor([8, 16, 793801],"float16"), ) 	 101606528 	 29864 	 10.000538110733032 	 9.071239471435547 	 0.34224915504455566 	 0.3104822635650635 	 13.378677129745483 	 22.26240611076355 	 0.45786142349243164 	 0.3809475898742676 	 
2025-08-06 08:08:22.099887 test begin: paddle.expm1(Tensor([8, 396901, 32],"float16"), )
[Prof] paddle.expm1 	 paddle.expm1(Tensor([8, 396901, 32],"float16"), ) 	 101606656 	 29864 	 9.996408224105835 	 9.07318639755249 	 0.3420829772949219 	 0.3104078769683838 	 13.377933740615845 	 22.260685682296753 	 0.4576141834259033 	 0.3808746337890625 	 
2025-08-06 08:09:21.939187 test begin: paddle.fft.fftn(Tensor([226801, 7, 32],"float32"), )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([226801, 7, 32],"float32"), ) 	 50803424 	 1679 	 18.723689556121826 	 26.759642839431763 	 7.295608520507812e-05 	 1.8101584911346436 	 32.88814926147461 	 25.944939136505127 	 1.5397658348083496 	 1.9736261367797852 	 
2025-08-06 08:11:08.952212 test begin: paddle.fft.fftn(Tensor([39, 40708, 32],"float32"), )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([39, 40708, 32],"float32"), ) 	 50803584 	 1679 	 15.700471639633179 	 17.903310537338257 	 0.0001246929168701172 	 1.0909194946289062 	 20.19909906387329 	 16.888407945632935 	 1.023632526397705 	 1.1425385475158691 	 
2025-08-06 08:12:27.414257 test begin: paddle.fft.fftn(Tensor([39, 7, 186093],"float32"), )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([39, 7, 186093],"float32"), ) 	 50803389 	 1679 	 11.663245677947998 	 10.162108898162842 	 6.532669067382812e-05 	 0.774160623550415 	 12.6598482131958 	 9.400335550308228 	 0.7701659202575684 	 0.8177986145019531 	 
2025-08-06 08:13:13.738230 test begin: paddle.fft.fftn(Tensor([7, 32, 481, 481],"float32"), axes=list[2,3,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([7, 32, 481, 481],"float32"), axes=list[2,3,], ) 	 51824864 	 1679 	 10.009220600128174 	 7.389160871505737 	 6.0558319091796875e-05 	 0.9019002914428711 	 9.261704921722412 	 6.641101360321045 	 0.8054237365722656 	 1.0094568729400635 	 
2025-08-06 08:13:50.366062 test begin: paddle.fft.fftn(Tensor([8, 28, 481, 481],"float32"), axes=list[2,3,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([8, 28, 481, 481],"float32"), axes=list[2,3,], ) 	 51824864 	 1679 	 10.013121843338013 	 7.306113243103027 	 6.031990051269531e-05 	 0.8898825645446777 	 9.255961418151855 	 6.575446128845215 	 0.8049023151397705 	 1.0018808841705322 	 
2025-08-06 08:14:25.663479 test begin: paddle.fft.fftn(Tensor([8, 32, 413, 481],"float32"), axes=list[2,3,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([8, 32, 413, 481],"float32"), axes=list[2,3,], ) 	 50855168 	 1679 	 9.984675884246826 	 7.086519241333008 	 5.030632019042969e-05 	 0.8629052639007568 	 9.397133827209473 	 6.334292411804199 	 0.8172156810760498 	 0.9629712104797363 	 
2025-08-06 08:15:01.033997 test begin: paddle.fft.fftn(Tensor([8, 32, 481, 413],"float32"), axes=list[2,3,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([8, 32, 481, 413],"float32"), axes=list[2,3,], ) 	 50855168 	 1679 	 9.97127389907837 	 7.07761287689209 	 7.367134094238281e-05 	 0.86183762550354 	 9.42528772354126 	 6.305051803588867 	 0.819586992263794 	 0.9602761268615723 	 
2025-08-06 08:15:38.020914 test begin: paddle.fft.fftn(x=Tensor([50, 133, 39, 14, 14],"float32"), axes=list[-3,-2,-1,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(x=Tensor([50, 133, 39, 14, 14],"float32"), axes=list[-3,-2,-1,], ) 	 50832600 	 1679 	 10.122390747070312 	 5.010383367538452 	 8.58306884765625e-05 	 0.6081626415252686 	 7.349214553833008 	 4.184990406036377 	 0.6390111446380615 	 0.636671781539917 	 
2025-08-06 08:16:10.015942 test begin: paddle.fft.fftn(x=Tensor([50, 8, 39, 14, 233],"float32"), axes=list[-3,-2,-1,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(x=Tensor([50, 8, 39, 14, 233],"float32"), axes=list[-3,-2,-1,], ) 	 50887200 	 1679 	 11.439109802246094 	 6.191721677780151 	 5.91278076171875e-05 	 0.7541570663452148 	 10.004508018493652 	 5.390629529953003 	 0.8699934482574463 	 0.8211765289306641 	 
2025-08-06 08:16:45.200560 test begin: paddle.fft.fftn(x=Tensor([50, 8, 39, 233, 14],"float32"), axes=list[-3,-2,-1,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(x=Tensor([50, 8, 39, 233, 14],"float32"), axes=list[-3,-2,-1,], ) 	 50887200 	 1679 	 12.095603466033936 	 6.933342933654785 	 5.745887756347656e-05 	 0.8448593616485596 	 10.95045280456543 	 6.12359356880188 	 0.9522781372070312 	 0.9323554039001465 	 
2025-08-06 08:17:23.956961 test begin: paddle.fft.fftn(x=Tensor([50, 8, 649, 14, 14],"float32"), axes=list[-3,-2,-1,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(x=Tensor([50, 8, 649, 14, 14],"float32"), axes=list[-3,-2,-1,], ) 	 50881600 	 1679 	 10.673849821090698 	 5.964542627334595 	 6.198883056640625e-05 	 0.7259330749511719 	 8.322989463806152 	 5.156195640563965 	 0.7237756252288818 	 0.7843976020812988 	 
2025-08-06 08:17:57.592800 test begin: paddle.fft.fftn(x=Tensor([831, 8, 39, 14, 14],"float32"), axes=list[-3,-2,-1,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(x=Tensor([831, 8, 39, 14, 14],"float32"), axes=list[-3,-2,-1,], ) 	 50817312 	 1679 	 10.102866411209106 	 5.008576393127441 	 5.5789947509765625e-05 	 0.6079182624816895 	 7.344112396240234 	 4.1843483448028564 	 0.6386411190032959 	 0.6366167068481445 	 
2025-08-06 08:18:27.375950 test begin: paddle.fft.ifftn(x=Tensor([4, 4, 6, 264601],"float64"), s=list[2,4,], axes=tuple(0,1,), )
[Prof] paddle.fft.ifftn 	 paddle.fft.ifftn(x=Tensor([4, 4, 6, 264601],"float64"), s=list[2,4,], axes=tuple(0,1,), ) 	 25401696 	 41344 	 124.51558470726013 	 57.50420665740967 	 6.222724914550781e-05 	 0.3553426265716553 	 118.44926524162292 	 78.61459302902222 	 0.3660774230957031 	 0.3888261318206787 	 
2025-08-06 08:24:47.574801 test begin: paddle.fft.ifftn(x=Tensor([4, 4, 793801, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), )
[Prof] paddle.fft.ifftn 	 paddle.fft.ifftn(x=Tensor([4, 4, 793801, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), ) 	 25401632 	 41344 	 116.9411141872406 	 57.499760150909424 	 0.00014400482177734375 	 0.35526180267333984 	 107.45022583007812 	 78.60917019844055 	 0.33184003829956055 	 0.38895153999328613 	 
2025-08-06 08:30:50.609306 test begin: paddle.fft.ifftn(x=Tensor([4, 529201, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), )
[Prof] paddle.fft.ifftn 	 paddle.fft.ifftn(x=Tensor([4, 529201, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), ) 	 25401648 	 41344 	 11.161839008331299 	 20.390888452529907 	 4.5299530029296875e-05 	 0.12578582763671875 	 6.816061019897461 	 30.087636947631836 	 0.02102804183959961 	 0.09302186965942383 	 
2025-08-06 08:32:02.303251 test begin: paddle.fft.ifftn(x=Tensor([529201, 4, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), )
[Prof] paddle.fft.ifftn 	 paddle.fft.ifftn(x=Tensor([529201, 4, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), ) 	 25401648 	 41344 	 9.667110204696655 	 20.38960576057434 	 5.53131103515625e-05 	 0.12574553489685059 	 6.8075175285339355 	 11.586437463760376 	 0.021025419235229492 	 0.05736231803894043 	 
2025-08-06 08:32:51.466414 test begin: paddle.fft.ihfft(x=Tensor([2, 1411201, 3, 3],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb5fa764ee0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:42:56.470248 test begin: paddle.fft.ihfft(x=Tensor([2, 1411201, 3, 3],"float64"), n=2, )
W0806 08:42:57.238410 132593 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff945992d10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:53:01.484195 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 1058401, 3],"float64"), )
W0806 08:53:03.672231 132975 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd460e73010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:03:06.133418 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 1058401, 3],"float64"), n=2, )
W0806 09:03:06.793365 133364 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0423ca2d10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:13:14.568548 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 1058401, 3],"float64"), n=2, axis=1, )
W0806 09:13:15.294983 133651 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f70e0cdb070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:23:19.112568 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 3, 1058401],"float64"), )
W0806 09:23:19.800832 134018 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5bc4dbb070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:33:23.933014 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 3, 1058401],"float64"), n=2, axis=1, )
W0806 09:33:24.598157 134394 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f84d1ed3010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:43:28.630215 test begin: paddle.fft.ihfft(x=Tensor([201, 14112, 3, 3],"float64"), n=2, axis=1, )
W0806 09:43:29.362790 134875 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([201, 14112, 3, 3],"float64"), n=2, axis=1, ) 	 25528608 	 133166 	 9.364570617675781 	 7.499718189239502 	 8.96453857421875e-05 	 0.00016021728515625 	 23.348655223846436 	 21.6762056350708 	 0.022392988204956055 	 0.0015575885772705078 	 
2025-08-06 09:44:32.889709 test begin: paddle.fft.ihfft(x=Tensor([201, 4, 3, 10584],"float64"), n=2, )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([201, 4, 3, 10584],"float64"), n=2, ) 	 25528608 	 133166 	 9.389374256134033 	 6.137704610824585 	 5.888938903808594e-05 	 0.00012946128845214844 	 22.69269561767578 	 20.42663073539734 	 0.0217740535736084 	 0.03135991096496582 	 
2025-08-06 09:45:32.187749 test begin: paddle.fft.ihfft(x=Tensor([705601, 4, 3, 3],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f467944b070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:55:37.405601 test begin: paddle.fft.ihfft2(x=Tensor([1270081, 4, 5],"float64"), )
W0806 09:55:42.991111 135377 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7173623010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:05:45.074087 test begin: paddle.fft.ihfft2(x=Tensor([2822401, 3, 3],"float64"), s=tuple(1,2,), )
W0806 10:05:45.743822 135754 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuFFT, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/native/cuda/SpectralOps.cpp:269.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([2822401, 3, 3],"float64"), s=tuple(1,2,), ) 	 25401609 	 99947 	 77.18533754348755 	 63.07875204086304 	 0.13156867027282715 	 0.16122055053710938 	 114.90209102630615 	 116.10098886489868 	 0.14687347412109375 	 0.14835572242736816 	 
2025-08-06 10:11:58.757312 test begin: paddle.fft.ihfft2(x=Tensor([3, 1693441, 5],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f97e0622c20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:22:05.881128 test begin: paddle.fft.ihfft2(x=Tensor([3, 4, 2116801],"float64"), )
W0806 10:22:06.816324 136598 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2c6e303070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:32:12.608920 test begin: paddle.fft.ihfft2(x=Tensor([4, 2116801, 3],"float64"), s=tuple(1,2,), )
W0806 10:32:13.300335 137298 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuFFT, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/native/cuda/SpectralOps.cpp:269.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([4, 2116801, 3],"float64"), s=tuple(1,2,), ) 	 25401612 	 99947 	 11.7771155834198 	 6.512776613235474 	 0.00010442733764648438 	 0.00043201446533203125 	 16.25868248939514 	 18.799546241760254 	 0.020517349243164062 	 0.00021648406982421875 	 
2025-08-06 10:33:07.371037 test begin: paddle.fft.ihfft2(x=Tensor([4, 3, 3, 705601],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5e827b2d10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:43:12.063638 test begin: paddle.fft.ihfft2(x=Tensor([4, 3, 705601, 3],"float64"), )
W0806 10:43:12.742715 138005 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8d1a6fee30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:53:17.058555 test begin: paddle.fft.ihfft2(x=Tensor([4, 705601, 3, 3],"float64"), )
W0806 10:53:17.726312 138544 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6b36b0ee30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 11:03:21.758049 test begin: paddle.fft.ihfft2(x=Tensor([401, 21168, 3],"float64"), s=tuple(1,2,), )
W0806 11:03:22.434932 138920 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuFFT, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/native/cuda/SpectralOps.cpp:269.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([401, 21168, 3],"float64"), s=tuple(1,2,), ) 	 25465104 	 99947 	 11.347974061965942 	 6.27511739730835 	 7.414817810058594e-05 	 9.632110595703125e-05 	 17.246459007263184 	 18.320480585098267 	 0.022073984146118164 	 0.0002181529998779297 	 
2025-08-06 11:04:16.265307 test begin: paddle.fft.ihfft2(x=Tensor([940801, 3, 3, 3],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7383ef6e60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 11:14:20.992974 test begin: paddle.fft.ihfftn(Tensor([1270081, 4, 5],"float64"), None, tuple(-2,-1,), "backward", None, )
W0806 11:14:21.665308 139342 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([1270081, 4, 5],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401620 	 4772 	 10.01354455947876 	 10.165173768997192 	 0.35740065574645996 	 0.36284399032592773 	 19.06737470626831 	 16.775331258773804 	 0.5833542346954346 	 0.44925403594970703 	 
2025-08-06 11:15:21.898801 test begin: paddle.fft.ihfftn(Tensor([3, 1693441, 5],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([3, 1693441, 5],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401615 	 4772 	 42.19199728965759 	 35.16374731063843 	 0.695340633392334 	 0.6851036548614502 	 74.14189028739929 	 41.82630467414856 	 1.134192705154419 	 0.6883764266967773 	 
2025-08-06 11:18:39.260281 test begin: paddle.fft.ihfftn(Tensor([3, 4, 2116801],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([3, 4, 2116801],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401612 	 4772 	 43.48643946647644 	 34.891064167022705 	 0.6202027797698975 	 0.49788999557495117 	 65.0877194404602 	 62.676382064819336 	 0.9957640171051025 	 0.894965410232544 	 
2025-08-06 11:22:12.144784 test begin: paddle.fft.ihfftn(Tensor([4, 3, 3, 705601],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([4, 3, 3, 705601],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401636 	 4772 	 34.774452447891235 	 34.8283953666687 	 0.4960155487060547 	 0.49694228172302246 	 65.0133695602417 	 60.83626890182495 	 0.9946303367614746 	 0.8686997890472412 	 
2025-08-06 11:25:28.839285 test begin: paddle.fft.ihfftn(Tensor([4, 3, 705601, 3],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([4, 3, 705601, 3],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401636 	 4772 	 45.18766760826111 	 42.91321563720703 	 0.744877815246582 	 0.7078475952148438 	 72.10656094551086 	 50.06335234642029 	 1.1030452251434326 	 0.7145471572875977 	 
2025-08-06 11:29:05.482087 test begin: paddle.fft.ihfftn(Tensor([4, 705601, 3, 3],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([4, 705601, 3, 3],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401636 	 4772 	 10.827351093292236 	 11.360689878463745 	 0.3865342140197754 	 0.40540051460266113 	 19.269225120544434 	 18.396064281463623 	 0.5894584655761719 	 0.49271512031555176 	 
2025-08-06 11:30:09.680613 test begin: paddle.fft.ihfftn(Tensor([940801, 3, 3, 3],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([940801, 3, 3, 3],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401627 	 4772 	 10.826202392578125 	 11.364904403686523 	 0.38650012016296387 	 0.40533018112182617 	 19.26906180381775 	 18.39844584465027 	 0.5894601345062256 	 0.49285101890563965 	 
2025-08-06 11:31:16.190363 test begin: paddle.fft.ihfftn(x=Tensor([4, 3, 1058401, 2],"float64"), )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(x=Tensor([4, 3, 1058401, 2],"float64"), ) 	 25401624 	 4772 	 72.63479089736938 	 67.36570429801941 	 0.9142332077026367 	 1.0305180549621582 	 87.84210920333862 	 68.06945037841797 	 1.0459372997283936 	 1.0412812232971191 	 
2025-08-06 11:36:13.570311 test begin: paddle.fft.ihfftn(x=Tensor([4, 3, 5, 423361],"float64"), )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(x=Tensor([4, 3, 5, 423361],"float64"), ) 	 25401660 	 4772 	 34.88317012786865 	 30.492380142211914 	 0.43923473358154297 	 0.46768832206726074 	 73.34034371376038 	 52.00245976448059 	 0.982292652130127 	 0.7954905033111572 	 
2025-08-06 11:39:27.169294 test begin: paddle.fft.ihfftn(x=Tensor([4, 635041, 5, 2],"float64"), )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(x=Tensor([4, 635041, 5, 2],"float64"), ) 	 25401640 	 4772 	 80.3977279663086 	 74.77399063110352 	 1.01210618019104 	 1.2280421257019043 	 95.41249203681946 	 75.35752987861633 	 1.1354658603668213 	 1.1527550220489502 	 
2025-08-06 11:44:57.118704 test begin: paddle.fft.ihfftn(x=Tensor([846721, 3, 5, 2],"float64"), )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(x=Tensor([846721, 3, 5, 2],"float64"), ) 	 25401630 	 4772 	 90.31711483001709 	 74.27412557601929 	 1.136596441268921 	 1.324425220489502 	 81.80642414093018 	 74.956862449646 	 0.9747288227081299 	 1.3364806175231934 	 
2025-08-06 11:50:19.949056 test begin: paddle.fft.rfft(Tensor([20, 1210, 2101],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([20, 1210, 2101],"float32"), ) 	 50844200 	 7826 	 20.369455575942993 	 15.701547145843506 	 0.5315933227539062 	 0.6838788986206055 	 38.3419623374939 	 26.35926365852356 	 1.0021629333496094 	 1.148164987564087 	 
2025-08-06 11:52:03.583279 test begin: paddle.fft.rfft(Tensor([20, 1270, 2001],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([20, 1270, 2001],"float32"), ) 	 50825400 	 7826 	 15.48268985748291 	 10.400396585464478 	 0.4042019844055176 	 0.4523651599884033 	 28.80681276321411 	 15.801686525344849 	 0.7518939971923828 	 0.6882247924804688 	 
2025-08-06 11:53:15.733674 test begin: paddle.fft.rfft(Tensor([20, 64, 39691],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([20, 64, 39691],"float32"), ) 	 50804480 	 7826 	 37.025551557540894 	 32.22347331047058 	 0.48320508003234863 	 0.5263023376464844 	 71.14223766326904 	 58.82576823234558 	 0.9282131195068359 	 0.961766242980957 	 
2025-08-06 11:56:41.313448 test begin: paddle.fft.rfft(Tensor([378, 64, 2101],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([378, 64, 2101],"float32"), ) 	 50827392 	 7826 	 20.357996702194214 	 15.695441961288452 	 0.5312697887420654 	 0.6834690570831299 	 38.165815114974976 	 26.360723972320557 	 0.9959590435028076 	 1.148317813873291 	 
2025-08-06 11:58:23.839156 test begin: paddle.fft.rfft(Tensor([397, 64, 2001],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([397, 64, 2001],"float32"), ) 	 50841408 	 7826 	 15.501806497573853 	 10.39287281036377 	 0.404712438583374 	 0.4525120258331299 	 28.573288917541504 	 15.808052778244019 	 0.7460081577301025 	 0.687201738357544 	 
2025-08-06 11:59:37.905778 test begin: paddle.fft.rfft(Tensor([4, 32, 32, 12404],"float32"), axis=-1, norm="forward", )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([4, 32, 32, 12404],"float32"), axis=-1, norm="forward", ) 	 50806784 	 7826 	 38.24874186515808 	 33.9313440322876 	 0.4990255832672119 	 0.5543866157531738 	 79.14814138412476 	 68.37604308128357 	 0.9399516582489014 	 0.9930508136749268 	 
2025-08-06 12:03:23.815353 test begin: paddle.fft.rfft(Tensor([4, 32, 6202, 64],"float32"), axis=-1, norm="forward", )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([4, 32, 6202, 64],"float32"), axis=-1, norm="forward", ) 	 50806784 	 7826 	 9.999874830245972 	 4.9035327434539795 	 0.3264200687408447 	 0.3196263313293457 	 27.478487968444824 	 14.36420202255249 	 0.5978903770446777 	 0.46915745735168457 	 
2025-08-06 12:04:22.869680 test begin: paddle.fft.rfft(Tensor([4, 6202, 32, 64],"float32"), axis=-1, norm="forward", )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([4, 6202, 32, 64],"float32"), axis=-1, norm="forward", ) 	 50806784 	 7826 	 9.999842166900635 	 4.896000146865845 	 0.3264474868774414 	 0.31965136528015137 	 27.477479934692383 	 14.3649742603302 	 0.597898006439209 	 0.46919989585876465 	 
2025-08-06 12:05:21.402406 test begin: paddle.fft.rfft(Tensor([776, 32, 32, 64],"float32"), axis=-1, norm="forward", )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([776, 32, 32, 64],"float32"), axis=-1, norm="forward", ) 	 50855936 	 7826 	 10.014945030212402 	 4.900637626647949 	 0.3269643783569336 	 0.3199577331542969 	 27.503237009048462 	 14.376903295516968 	 0.5984694957733154 	 0.46956300735473633 	 
2025-08-06 12:06:19.630927 test begin: paddle.fft.rfft2(Tensor([26, 32, 250, 250],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([26, 32, 250, 250],"float32"), ) 	 52000000 	 6899 	 10.21047329902649 	 5.900978088378906 	 0.37779736518859863 	 0.4360690116882324 	 24.813923835754395 	 13.236759662628174 	 0.6126396656036377 	 0.4905385971069336 	 
2025-08-06 12:07:15.252365 test begin: paddle.fft.rfft2(Tensor([32, 26, 250, 250],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([32, 26, 250, 250],"float32"), ) 	 52000000 	 6899 	 10.199724197387695 	 5.816864013671875 	 0.3776271343231201 	 0.43224668502807617 	 24.811229705810547 	 13.2350332736969 	 0.612457275390625 	 0.4904932975769043 	 
2025-08-06 12:08:10.729054 test begin: paddle.fft.rfft2(Tensor([32, 32, 199, 250],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([32, 32, 199, 250],"float32"), ) 	 50944000 	 6899 	 18.556770086288452 	 10.401154518127441 	 0.6877799034118652 	 0.7731742858886719 	 41.053833961486816 	 22.446483612060547 	 1.0127043724060059 	 0.8322675228118896 	 
2025-08-06 12:09:45.643871 test begin: paddle.fft.rfft2(Tensor([32, 32, 250, 199],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([32, 32, 250, 199],"float32"), ) 	 50944000 	 6899 	 18.86318039894104 	 11.30649709701538 	 0.46549272537231445 	 0.4183635711669922 	 36.084206104278564 	 18.55496644973755 	 0.8904337882995605 	 0.6878025531768799 	 
2025-08-06 12:11:12.060626 test begin: paddle.fft.rfft2(Tensor([8, 102, 250, 250],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([8, 102, 250, 250],"float32"), ) 	 51000000 	 6899 	 10.010175943374634 	 5.731759309768677 	 0.37058019638061523 	 0.4244801998138428 	 24.336384057998657 	 12.989362001419067 	 0.6008493900299072 	 0.48133420944213867 	 
2025-08-06 12:12:07.794495 test begin: paddle.fft.rfft2(Tensor([8, 32, 250, 794],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([8, 32, 250, 794],"float32"), ) 	 50816000 	 6899 	 14.527412414550781 	 10.098790168762207 	 0.43016743659973145 	 0.4981648921966553 	 30.2268545627594 	 18.810894012451172 	 0.745915412902832 	 0.6973385810852051 	 
2025-08-06 12:13:24.091175 test begin: paddle.fft.rfft2(Tensor([8, 32, 794, 250],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([8, 32, 794, 250],"float32"), ) 	 50816000 	 6899 	 16.020753145217896 	 11.302823543548584 	 0.5937325954437256 	 0.8380031585693359 	 36.19593834877014 	 24.181421995162964 	 0.8929908275604248 	 0.8967065811157227 	 
2025-08-06 12:14:53.437370 test begin: paddle.fft.rfft2(x=Tensor([32, 15, 15, 7057],"float32"), axes=tuple(1,2,), norm="ortho", )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(x=Tensor([32, 15, 15, 7057],"float32"), axes=tuple(1,2,), norm="ortho", ) 	 50810400 	 6899 	 11.033175706863403 	 10.936606407165527 	 0.3268585205078125 	 0.4048910140991211 	 27.084933519363403 	 20.90129327774048 	 0.6684372425079346 	 0.6197390556335449 	 
2025-08-06 12:16:04.957311 test begin: paddle.fft.rfft2(x=Tensor([32, 15, 414, 256],"float32"), axes=tuple(1,2,), norm="ortho", )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(x=Tensor([32, 15, 414, 256],"float32"), axes=tuple(1,2,), norm="ortho", ) 	 50872320 	 6899 	 16.007508754730225 	 12.714442729949951 	 0.33876776695251465 	 0.3765602111816406 	 29.43830680847168 	 21.56309223175049 	 0.6229076385498047 	 0.5325522422790527 	 
2025-08-06 12:17:26.635848 test begin: paddle.fft.rfft2(x=Tensor([32, 414, 15, 256],"float32"), axes=tuple(1,2,), norm="ortho", )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(x=Tensor([32, 414, 15, 256],"float32"), axes=tuple(1,2,), norm="ortho", ) 	 50872320 	 6899 	 11.521280288696289 	 11.507601976394653 	 0.34127378463745117 	 0.42607951164245605 	 29.655203104019165 	 23.65118145942688 	 0.6276600360870361 	 0.5840003490447998 	 
2025-08-06 12:18:46.842687 test begin: paddle.fft.rfft2(x=Tensor([883, 15, 15, 256],"float32"), axes=tuple(1,2,), norm="ortho", )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(x=Tensor([883, 15, 15, 256],"float32"), axes=tuple(1,2,), norm="ortho", ) 	 50860800 	 6899 	 10.803483247756958 	 10.791135311126709 	 0.3200955390930176 	 0.39951062202453613 	 26.847505569458008 	 20.84873914718628 	 0.6626262664794922 	 0.6180956363677979 	 
2025-08-06 12:19:59.582384 test begin: paddle.fft.rfftn(Tensor([26, 32, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([26, 32, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 52000000 	 6899 	 10.203868627548218 	 5.800282955169678 	 0.37780022621154785 	 0.4287831783294678 	 24.81234312057495 	 13.23770260810852 	 0.6125705242156982 	 0.490556001663208 	 
2025-08-06 12:20:57.055277 test begin: paddle.fft.rfftn(Tensor([32, 15, 15, 7057],"float32"), None, tuple(1,2,), "ortho", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 15, 15, 7057],"float32"), None, tuple(1,2,), "ortho", None, ) 	 50810400 	 6899 	 11.042494773864746 	 10.941809177398682 	 0.32686901092529297 	 0.40488505363464355 	 27.08706569671631 	 20.90152597427368 	 0.6684420108795166 	 0.6197249889373779 	 
2025-08-06 12:22:13.975400 test begin: paddle.fft.rfftn(Tensor([32, 15, 414, 256],"float32"), None, tuple(1,2,), "ortho", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 15, 414, 256],"float32"), None, tuple(1,2,), "ortho", None, ) 	 50872320 	 6899 	 16.00762915611267 	 12.71431851387024 	 0.33875584602355957 	 0.37658023834228516 	 29.439078330993652 	 21.562602519989014 	 0.6229121685028076 	 0.532459020614624 	 
2025-08-06 12:23:39.104884 test begin: paddle.fft.rfftn(Tensor([32, 26, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 26, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 52000000 	 6899 	 10.209433555603027 	 5.837847471237183 	 0.3776264190673828 	 0.4345676898956299 	 24.811667442321777 	 13.237047672271729 	 0.6125195026397705 	 0.49082446098327637 	 
2025-08-06 12:24:42.690130 test begin: paddle.fft.rfftn(Tensor([32, 32, 199, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 32, 199, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 50944000 	 6899 	 18.56888198852539 	 10.412322044372559 	 0.6877665519714355 	 0.7734851837158203 	 41.0537006855011 	 22.444868803024292 	 1.0126886367797852 	 0.8322477340698242 	 
2025-08-06 12:26:16.788601 test begin: paddle.fft.rfftn(Tensor([32, 32, 250, 199],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 32, 250, 199],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 50944000 	 6899 	 18.86231231689453 	 11.29640817642212 	 0.46543121337890625 	 0.4180891513824463 	 36.08758783340454 	 18.54926824569702 	 0.8903391361236572 	 0.6879963874816895 	 
2025-08-06 12:27:45.126705 test begin: paddle.fft.rfftn(Tensor([32, 414, 15, 256],"float32"), None, tuple(1,2,), "ortho", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 414, 15, 256],"float32"), None, tuple(1,2,), "ortho", None, ) 	 50872320 	 6899 	 11.5230073928833 	 11.508475303649902 	 0.34122586250305176 	 0.4261150360107422 	 29.65426516532898 	 23.651265382766724 	 0.6274971961975098 	 0.5838661193847656 	 
2025-08-06 12:29:07.288664 test begin: paddle.fft.rfftn(Tensor([8, 102, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([8, 102, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 51000000 	 6899 	 10.010172367095947 	 5.744025230407715 	 0.370715856552124 	 0.4266011714935303 	 24.3359797000885 	 12.987203359603882 	 0.600691556930542 	 0.4812347888946533 	 
2025-08-06 12:30:02.497045 test begin: paddle.fft.rfftn(Tensor([8, 32, 250, 794],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([8, 32, 250, 794],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 50816000 	 6899 	 14.520512580871582 	 10.137513399124146 	 0.42997050285339355 	 0.5009689331054688 	 30.22526741027832 	 18.809076070785522 	 0.7459683418273926 	 0.6972606182098389 	 
2025-08-06 12:31:19.153391 test begin: paddle.fft.rfftn(Tensor([8, 32, 794, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([8, 32, 794, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 50816000 	 6899 	 16.029990911483765 	 11.257930994033813 	 0.5936412811279297 	 0.8352401256561279 	 36.17935752868652 	 24.18267560005188 	 0.8926541805267334 	 0.8965446949005127 	 
2025-08-06 12:32:55.000784 test begin: paddle.fft.rfftn(Tensor([883, 15, 15, 256],"float32"), None, tuple(1,2,), "ortho", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([883, 15, 15, 256],"float32"), None, tuple(1,2,), "ortho", None, ) 	 50860800 	 6899 	 10.803613901138306 	 10.812231302261353 	 0.32008981704711914 	 0.39945030212402344 	 26.84844994544983 	 20.838317394256592 	 0.6626136302947998 	 0.6178648471832275 	 
2025-08-06 12:34:07.490680 test begin: paddle.flatten(Tensor([40510, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([40510, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, ) 	 508157440 	 1742182 	 10.008267879486084 	 7.443858861923218 	 0.00026726722717285156 	 0.00010728836059570312 	 74.75652813911438 	 115.71694326400757 	 0.0001220703125 	 0.0005290508270263672 	 
2025-08-06 12:37:52.859743 test begin: paddle.flatten(Tensor([40960, 254, 7, 7],"float32"), start_axis=1, stop_axis=-1, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([40960, 254, 7, 7],"float32"), start_axis=1, stop_axis=-1, ) 	 509788160 	 1742182 	 9.917895078659058 	 8.284392356872559 	 0.00010538101196289062 	 0.00016498565673828125 	 75.73885154724121 	 102.65907049179077 	 0.00011110305786132812 	 0.00026607513427734375 	 
2025-08-06 12:41:27.355751 test begin: paddle.flatten(Tensor([40960, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([40960, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, ) 	 513802240 	 1742182 	 10.06608510017395 	 7.534775018692017 	 0.00015401840209960938 	 0.00034999847412109375 	 75.60048937797546 	 114.09588146209717 	 0.0001518726348876953 	 0.0002560615539550781 	 
2025-08-06 12:45:13.059781 test begin: paddle.flatten(Tensor([4160, 50, 10, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([4160, 50, 10, 256],"float32"), start_axis=2, ) 	 532480000 	 1742182 	 9.788069725036621 	 7.343434810638428 	 0.00025343894958496094 	 0.0002467632293701172 	 75.06132006645203 	 94.12623023986816 	 0.000148773193359375 	 0.00022649765014648438 	 
2025-08-06 12:48:37.911530 test begin: paddle.flatten(Tensor([4160, 50, 7, 349],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([4160, 50, 7, 349],"float32"), start_axis=2, ) 	 508144000 	 1742182 	 9.79971194267273 	 7.340479373931885 	 0.0003199577331542969 	 0.0005548000335693359 	 75.2909619808197 	 121.01587057113647 	 0.000133514404296875 	 0.00025963783264160156 	 
2025-08-06 12:52:31.159585 test begin: paddle.flatten(Tensor([4160, 69, 7, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([4160, 69, 7, 256],"float32"), start_axis=2, ) 	 514375680 	 1742182 	 18.615365505218506 	 7.381073236465454 	 0.0001480579376220703 	 0.00029206275939941406 	 79.49903321266174 	 116.00272965431213 	 0.00014352798461914062 	 0.00023865699768066406 	 
2025-08-06 12:56:29.550919 test begin: paddle.flatten(Tensor([5120, 50, 7, 284],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([5120, 50, 7, 284],"float32"), start_axis=2, ) 	 508928000 	 1742182 	 9.60575532913208 	 7.399420738220215 	 0.00022983551025390625 	 0.00011587142944335938 	 75.00014543533325 	 99.16802930831909 	 0.00015473365783691406 	 0.0006413459777832031 	 
2025-08-06 12:59:57.805543 test begin: paddle.flatten(Tensor([5120, 50, 8, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([5120, 50, 8, 256],"float32"), start_axis=2, ) 	 524288000 	 1742182 	 17.04113531112671 	 7.375636100769043 	 0.00013446807861328125 	 0.0006666183471679688 	 87.66880583763123 	 118.57053089141846 	 0.00021028518676757812 	 0.0002903938293457031 	 
2025-08-06 13:04:08.779532 test begin: paddle.flatten(Tensor([5120, 56, 7, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([5120, 56, 7, 256],"float32"), start_axis=2, ) 	 513802240 	 1742182 	 9.668036937713623 	 11.325615167617798 	 0.00024271011352539062 	 0.0001595020294189453 	 76.10208511352539 	 94.091561794281 	 0.00013256072998046875 	 0.00028705596923828125 	 
2025-08-06 13:07:41.358731 test begin: paddle.flatten(Tensor([5680, 50, 7, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([5680, 50, 7, 256],"float32"), start_axis=2, ) 	 508928000 	 1742182 	 10.754161596298218 	 7.430073499679565 	 0.00013017654418945312 	 0.00012183189392089844 	 75.27837777137756 	 110.82883024215698 	 0.00016427040100097656 	 0.00025725364685058594 	 
2025-08-06 13:11:26.724329 test begin: paddle.flip(Tensor([127, 8, 224, 224],"float32"), axis=list[3,], )
[Prof] paddle.flip 	 paddle.flip(Tensor([127, 8, 224, 224],"float32"), axis=list[3,], ) 	 50978816 	 10401 	 10.094700336456299 	 3.25642991065979 	 0.9916689395904541 	 0.3199453353881836 	 10.09335207939148 	 3.2550017833709717 	 0.9916656017303467 	 0.3198094367980957 	 
2025-08-06 13:11:55.157149 test begin: paddle.flip(Tensor([1351, 3, 112, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([1351, 3, 112, 112],"float32"), axis=-1, ) 	 50840832 	 10401 	 10.095115423202515 	 3.252286672592163 	 0.9919342994689941 	 0.3193786144256592 	 10.094228267669678 	 3.2504875659942627 	 0.991929292678833 	 0.3193976879119873 	 
2025-08-06 13:12:24.057494 test begin: paddle.flip(Tensor([3, 338, 224, 224],"float32"), axis=list[3,], )
[Prof] paddle.flip 	 paddle.flip(Tensor([3, 338, 224, 224],"float32"), axis=list[3,], ) 	 50878464 	 10401 	 10.06828236579895 	 3.250427484512329 	 0.9892444610595703 	 0.31940674781799316 	 10.066559791564941 	 3.249032735824585 	 0.9891307353973389 	 0.319202184677124 	 
2025-08-06 13:12:52.457707 test begin: paddle.flip(Tensor([3, 8, 224, 9451],"float32"), axis=list[3,], )
[Prof] paddle.flip 	 paddle.flip(Tensor([3, 8, 224, 9451],"float32"), axis=list[3,], ) 	 50808576 	 10401 	 10.070920705795288 	 3.2483701705932617 	 0.9895408153533936 	 0.31918978691101074 	 10.067662000656128 	 3.2476868629455566 	 0.9892458915710449 	 0.3190610408782959 	 
2025-08-06 13:13:22.968014 test begin: paddle.flip(Tensor([3, 8, 9451, 224],"float32"), axis=list[3,], )
[Prof] paddle.flip 	 paddle.flip(Tensor([3, 8, 9451, 224],"float32"), axis=list[3,], ) 	 50808576 	 10401 	 10.054593563079834 	 3.2452492713928223 	 0.9881236553192139 	 0.3188588619232178 	 10.052742719650269 	 3.244907855987549 	 0.9877324104309082 	 0.31882596015930176 	 
2025-08-06 13:13:51.522776 test begin: paddle.flip(Tensor([52, 3, 112, 2908],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([52, 3, 112, 2908],"float32"), axis=-1, ) 	 50808576 	 10401 	 10.071048021316528 	 3.2482831478118896 	 0.9896059036254883 	 0.3191545009613037 	 10.068146705627441 	 3.2478280067443848 	 0.9892816543579102 	 0.31914401054382324 	 
2025-08-06 13:14:20.012767 test begin: paddle.flip(Tensor([52, 3, 2908, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([52, 3, 2908, 112],"float32"), axis=-1, ) 	 50808576 	 10401 	 10.085400104522705 	 3.2501096725463867 	 0.9910178184509277 	 0.3191080093383789 	 10.083107233047485 	 3.2477426528930664 	 0.99082350730896 	 0.31908726692199707 	 
2025-08-06 13:14:48.538815 test begin: paddle.flip(Tensor([52, 78, 112, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([52, 78, 112, 112],"float32"), axis=-1, ) 	 50878464 	 10401 	 10.093476057052612 	 3.254328489303589 	 0.9918737411499023 	 0.319732666015625 	 10.094019651412964 	 3.250469207763672 	 0.9918243885040283 	 0.3193826675415039 	 
2025-08-06 13:15:17.083899 test begin: paddle.flip(Tensor([64, 3, 112, 2363],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([64, 3, 112, 2363],"float32"), axis=-1, ) 	 50813952 	 10401 	 10.082760334014893 	 4.336377382278442 	 0.9908075332641602 	 0.3192017078399658 	 10.079336166381836 	 3.2481706142425537 	 0.9905056953430176 	 0.31916379928588867 	 
2025-08-06 13:15:47.305477 test begin: paddle.flip(Tensor([64, 3, 2363, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([64, 3, 2363, 112],"float32"), axis=-1, ) 	 50813952 	 10401 	 10.087002992630005 	 3.247943162918091 	 0.9912118911743164 	 0.3191666603088379 	 10.08430790901184 	 3.2480905055999756 	 0.9909806251525879 	 0.31915950775146484 	 
2025-08-06 13:16:15.815506 test begin: paddle.flip(Tensor([64, 64, 112, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([64, 64, 112, 112],"float32"), axis=-1, ) 	 51380224 	 10401 	 10.196753978729248 	 4.212944984436035 	 1.0019967555999756 	 0.32283687591552734 	 10.196622610092163 	 3.282341241836548 	 1.0018644332885742 	 0.32251954078674316 	 
2025-08-06 13:16:48.274730 test begin: paddle.floor(Tensor([100000, 170, 3],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([100000, 170, 3],"float32"), ) 	 51000000 	 33815 	 10.037781000137329 	 10.10521411895752 	 0.3034019470214844 	 0.30536556243896484 	 4.54955530166626 	 4.552967071533203 	 0.13743805885314941 	 0.13753819465637207 	 
2025-08-06 13:17:19.384607 test begin: paddle.floor(Tensor([100000, 2, 255],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([100000, 2, 255],"float32"), ) 	 51000000 	 33815 	 10.03771710395813 	 11.003908634185791 	 0.3033578395843506 	 0.3054177761077881 	 4.551756381988525 	 4.554103136062622 	 0.13859295845031738 	 0.13753080368041992 	 
2025-08-06 13:17:53.812930 test begin: paddle.floor(Tensor([322, 157920],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([322, 157920],"float32"), ) 	 50850240 	 33815 	 10.011152744293213 	 10.07918906211853 	 0.30255794525146484 	 0.30452942848205566 	 4.536753416061401 	 4.537015676498413 	 0.13707518577575684 	 0.13707780838012695 	 
2025-08-06 13:18:24.801821 test begin: paddle.floor(Tensor([4, 12700801],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([4, 12700801],"float32"), ) 	 50803204 	 33815 	 9.999406099319458 	 10.073575019836426 	 0.3022284507751465 	 0.3041818141937256 	 4.536199331283569 	 4.535928249359131 	 0.13696074485778809 	 0.13692736625671387 	 
2025-08-06 13:18:55.932377 test begin: paddle.floor(Tensor([8467201, 2, 3],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([8467201, 2, 3],"float32"), ) 	 50803206 	 33815 	 9.999369144439697 	 10.066838502883911 	 0.3022000789642334 	 0.30426740646362305 	 4.5327842235565186 	 4.53505277633667 	 0.13704657554626465 	 0.13699030876159668 	 
2025-08-06 13:19:26.781578 test begin: paddle.floor(x=Tensor([100, 352, 38, 38],"float32"), )
[Prof] paddle.floor 	 paddle.floor(x=Tensor([100, 352, 38, 38],"float32"), ) 	 50828800 	 33815 	 10.007508516311646 	 10.072071075439453 	 0.3024613857269287 	 0.30439019203186035 	 4.5395495891571045 	 4.53684663772583 	 0.13704729080200195 	 0.13702130317687988 	 
2025-08-06 13:19:57.741143 test begin: paddle.floor(x=Tensor([100, 4, 3343, 38],"float32"), )
[Prof] paddle.floor 	 paddle.floor(x=Tensor([100, 4, 3343, 38],"float32"), ) 	 50813600 	 33815 	 10.002229928970337 	 10.086395978927612 	 0.30223703384399414 	 0.30427980422973633 	 4.537561416625977 	 4.534575462341309 	 0.13763856887817383 	 0.1369640827178955 	 
2025-08-06 13:20:28.863040 test begin: paddle.floor(x=Tensor([100, 4, 38, 3343],"float32"), )
[Prof] paddle.floor 	 paddle.floor(x=Tensor([100, 4, 38, 3343],"float32"), ) 	 50813600 	 33815 	 10.00212812423706 	 10.071743726730347 	 0.3022594451904297 	 0.3043060302734375 	 4.533742189407349 	 4.5348451137542725 	 0.13694143295288086 	 0.13697552680969238 	 
2025-08-06 13:20:59.747321 test begin: paddle.floor(x=Tensor([8796, 4, 38, 38],"float32"), )
[Prof] paddle.floor 	 paddle.floor(x=Tensor([8796, 4, 38, 38],"float32"), ) 	 50805696 	 33815 	 9.998233556747437 	 10.071320295333862 	 0.3022572994232178 	 0.30425286293029785 	 4.53623104095459 	 4.534218072891235 	 0.136915922164917 	 0.1369619369506836 	 
2025-08-06 13:21:31.576670 test begin: paddle.floor_divide(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 50803600 	 23479 	 10.045439720153809 	 11.59302020072937 	 0.43736791610717773 	 0.5042440891265869 	 None 	 None 	 None 	 None 	 
2025-08-06 13:21:54.121730 test begin: paddle.floor_divide(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), ) 	 50803600 	 23479 	 9.948006391525269 	 11.5100576877594 	 0.433058500289917 	 0.5010273456573486 	 None 	 None 	 None 	 None 	 
2025-08-06 13:22:16.439766 test begin: paddle.floor_divide(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 101606800 	 23479 	 10.623690843582153 	 10.685195446014404 	 0.46226954460144043 	 0.4651772975921631 	 None 	 None 	 None 	 None 	 
2025-08-06 13:22:42.274058 test begin: paddle.floor_divide(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), ) 	 50803220 	 23479 	 10.504822492599487 	 10.50885820388794 	 0.4572107791900635 	 0.45740365982055664 	 None 	 None 	 None 	 None 	 
2025-08-06 13:23:04.441493 test begin: paddle.floor_divide(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), ) 	 101606420 	 23479 	 10.61523175239563 	 10.685256481170654 	 0.46204400062561035 	 0.46507835388183594 	 None 	 None 	 None 	 None 	 
2025-08-06 13:23:27.441600 test begin: paddle.floor_divide(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), ) 	 50804736 	 23479 	 10.515043258666992 	 10.519973754882812 	 0.4578883647918701 	 0.4574258327484131 	 None 	 None 	 None 	 None 	 
2025-08-06 13:23:49.331872 test begin: paddle.floor_divide(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), ) 	 101606440 	 23479 	 10.616312980651855 	 10.685091972351074 	 0.46203184127807617 	 0.46516919136047363 	 None 	 None 	 None 	 None 	 
2025-08-06 13:24:12.258183 test begin: paddle.fmax(Tensor([10, 50803201],"float32"), Tensor([50803201],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6ccdd0ada0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 13:35:02.576843 test begin: paddle.fmax(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), )
W0806 13:35:04.306855 143383 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.fmax 	 paddle.fmax(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), ) 	 101606420 	 33694 	 15.171775102615356 	 15.051779747009277 	 0.4602060317993164 	 0.45624566078186035 	 24.700066328048706 	 89.04873561859131 	 0.7491734027862549 	 0.2078096866607666 	 
2025-08-06 13:37:31.386385 test begin: paddle.fmax(Tensor([10, 5080321],"float32"), Tensor([5080321],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([10, 5080321],"float32"), Tensor([5080321],"float32"), ) 	 55883531 	 33694 	 15.183003187179565 	 15.013250827789307 	 0.460524320602417 	 0.4553656578063965 	 152.4245617389679 	 92.52493381500244 	 4.623835563659668 	 0.20044612884521484 	 
2025-08-06 13:42:10.555797 test begin: paddle.fmax(Tensor([30, 200, 8468],"float32"), Tensor([30, 200, 8468],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([30, 200, 8468],"float32"), Tensor([30, 200, 8468],"float32"), ) 	 101616000 	 33694 	 15.178450107574463 	 15.044742107391357 	 0.4603769779205322 	 0.45636534690856934 	 24.717844247817993 	 88.95276165008545 	 0.7497262954711914 	 0.20758891105651855 	 
2025-08-06 13:44:38.537306 test begin: paddle.fmax(Tensor([30, 42337, 40],"float32"), Tensor([30, 42337, 40],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([30, 42337, 40],"float32"), Tensor([30, 42337, 40],"float32"), ) 	 101608800 	 33694 	 15.187775373458862 	 15.043545722961426 	 0.46038055419921875 	 0.45630836486816406 	 24.756772756576538 	 89.12091588973999 	 0.7509450912475586 	 0.2079448699951172 	 
2025-08-06 13:47:09.257837 test begin: paddle.fmax(Tensor([3386881, 15],"float32"), Tensor([15],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([3386881, 15],"float32"), Tensor([15],"float32"), ) 	 50803230 	 33694 	 9.98871922492981 	 10.214224576950073 	 0.3029146194458008 	 0.3097996711730957 	 281.0285098552704 	 83.29292273521423 	 8.52454137802124 	 0.16823458671569824 	 
2025-08-06 13:53:37.824639 test begin: paddle.fmax(Tensor([3386881, 15],"float32"), Tensor([3386881, 15],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([3386881, 15],"float32"), Tensor([3386881, 15],"float32"), ) 	 101606430 	 33694 	 15.176637887954712 	 15.042913675308228 	 0.4601857662200928 	 0.45629262924194336 	 24.724477767944336 	 89.05557179450989 	 0.7499115467071533 	 0.20780229568481445 	 
2025-08-06 13:56:08.105995 test begin: paddle.fmax(Tensor([6351, 200, 40],"float32"), Tensor([6351, 200, 40],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([6351, 200, 40],"float32"), Tensor([6351, 200, 40],"float32"), ) 	 101616000 	 33694 	 15.177603483200073 	 15.044716358184814 	 0.4602949619293213 	 0.45630979537963867 	 24.72594141960144 	 88.95564293861389 	 0.7499701976776123 	 0.2075974941253662 	 
2025-08-06 13:58:36.402823 test begin: paddle.fmin(Tensor([10, 50803201],"float32"), Tensor([50803201],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f66a9a12e90>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 14:09:26.646253 test begin: paddle.fmin(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), )
W0806 14:09:28.357529 144567 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.fmin 	 paddle.fmin(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), ) 	 101606420 	 33691 	 15.1690034866333 	 15.041219472885132 	 0.46021437644958496 	 0.456190824508667 	 24.656532526016235 	 89.03132486343384 	 0.7479088306427002 	 0.20778322219848633 	 
2025-08-06 14:11:54.397711 test begin: paddle.fmin(Tensor([10, 5080321],"float32"), Tensor([5080321],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([10, 5080321],"float32"), Tensor([5080321],"float32"), ) 	 55883531 	 33691 	 15.180076122283936 	 15.023833990097046 	 0.4606359004974365 	 0.45534706115722656 	 152.47174286842346 	 92.56282353401184 	 4.625403881072998 	 0.20052289962768555 	 
2025-08-06 14:16:31.519871 test begin: paddle.fmin(Tensor([30, 200, 8468],"float32"), Tensor([30, 200, 8468],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([30, 200, 8468],"float32"), Tensor([30, 200, 8468],"float32"), ) 	 101616000 	 33691 	 15.182608842849731 	 15.04654335975647 	 0.46028923988342285 	 0.45633816719055176 	 24.691458225250244 	 88.96128916740417 	 0.7488150596618652 	 0.20757555961608887 	 
2025-08-06 14:18:59.678447 test begin: paddle.fmin(Tensor([30, 42337, 40],"float32"), Tensor([30, 42337, 40],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([30, 42337, 40],"float32"), Tensor([30, 42337, 40],"float32"), ) 	 101608800 	 33691 	 15.180845022201538 	 15.053943395614624 	 0.4603550434112549 	 0.45628952980041504 	 24.7196307182312 	 89.12783312797546 	 0.7496931552886963 	 0.2091977596282959 	 
2025-08-06 14:21:26.412942 test begin: paddle.fmin(Tensor([3386881, 15],"float32"), Tensor([15],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([3386881, 15],"float32"), Tensor([15],"float32"), ) 	 50803230 	 33691 	 9.985177278518677 	 10.215762853622437 	 0.30283164978027344 	 0.3098266124725342 	 280.9674606323242 	 83.47158074378967 	 8.52522087097168 	 0.1699202060699463 	 
2025-08-06 14:27:52.999258 test begin: paddle.fmin(Tensor([3386881, 15],"float32"), Tensor([3386881, 15],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([3386881, 15],"float32"), Tensor([3386881, 15],"float32"), ) 	 101606430 	 33691 	 15.172469854354858 	 15.050597429275513 	 0.460176944732666 	 0.45626115798950195 	 24.724088430404663 	 89.06229400634766 	 0.7497949600219727 	 0.20783686637878418 	 
2025-08-06 14:30:24.008687 test begin: paddle.fmin(Tensor([6351, 200, 40],"float32"), Tensor([6351, 200, 40],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([6351, 200, 40],"float32"), Tensor([6351, 200, 40],"float32"), ) 	 101616000 	 33691 	 15.177998304367065 	 15.053669214248657 	 0.4603133201599121 	 0.45765256881713867 	 24.673606634140015 	 88.95747113227844 	 0.7482306957244873 	 0.20756101608276367 	 
2025-08-06 14:32:52.982580 test begin: paddle.frac(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 13371 	 10.797041654586792 	 3.9815690517425537 	 0.8237757682800293 	 0.3041830062866211 	 15.825073480606079 	 0.6383285522460938 	 0.6045496463775635 	 0.00015878677368164062 	 
2025-08-06 14:33:26.060256 test begin: paddle.frac(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 13371 	 10.775063514709473 	 3.9851057529449463 	 0.8229882717132568 	 0.30419111251831055 	 15.821171283721924 	 0.6951377391815186 	 0.60440993309021 	 7.653236389160156e-05 	 
2025-08-06 14:34:00.646569 test begin: paddle.frac(Tensor([16934401, 3],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([16934401, 3],"float32"), ) 	 50803203 	 13371 	 10.775370359420776 	 3.981454372406006 	 0.8229873180389404 	 0.3042426109313965 	 15.823157548904419 	 0.6894083023071289 	 0.6044888496398926 	 5.1021575927734375e-05 	 
2025-08-06 14:34:33.651510 test begin: paddle.frac(Tensor([2, 12700801],"float64"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([2, 12700801],"float64"), ) 	 25401602 	 13371 	 10.004688501358032 	 3.9868316650390625 	 0.7642476558685303 	 0.3056776523590088 	 14.249934673309326 	 0.7432956695556641 	 0.5443146228790283 	 9.560585021972656e-05 	 
2025-08-06 14:35:03.795117 test begin: paddle.frac(Tensor([2, 25401601],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([2, 25401601],"float32"), ) 	 50803202 	 13371 	 10.779037714004517 	 3.9878389835357666 	 0.8232970237731934 	 0.30423736572265625 	 15.8269362449646 	 0.6344144344329834 	 0.6046788692474365 	 6.0558319091796875e-05 	 
2025-08-06 14:35:37.914420 test begin: paddle.frac(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 13371 	 11.45149040222168 	 3.989022970199585 	 0.8246340751647949 	 0.30422401428222656 	 15.825131177902222 	 0.6558165550231934 	 0.6046373844146729 	 7.534027099609375e-05 	 
2025-08-06 14:36:13.094463 test begin: paddle.frac(Tensor([8467201, 3],"float64"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([8467201, 3],"float64"), ) 	 25401603 	 13371 	 10.007514476776123 	 4.443657636642456 	 0.7656176090240479 	 0.3044545650482178 	 14.249024629592896 	 0.6848981380462646 	 0.5446460247039795 	 7.152557373046875e-05 	 
2025-08-06 14:36:44.809965 test begin: paddle.full_like(Tensor([1, 300, 169345],"float32"), 1, )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([1, 300, 169345],"float32"), 1, ) 	 50803500 	 74569 	 9.991210460662842 	 9.997801780700684 	 0.1368241310119629 	 0.1368863582611084 	 None 	 None 	 None 	 None 	 
2025-08-06 14:37:05.789103 test begin: paddle.full_like(Tensor([10, 1, 2048, 24807],"bool"), -65504.0, dtype=Dtype(float16), )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([10, 1, 2048, 24807],"bool"), -65504.0, dtype=Dtype(float16), ) 	 508047360 	 74569 	 49.07457494735718 	 49.06082510948181 	 0.6718957424163818 	 0.67177414894104 	 None 	 None 	 None 	 None 	 
2025-08-06 14:38:51.637712 test begin: paddle.full_like(Tensor([10, 1, 24807, 2048],"bool"), -65504.0, dtype=Dtype(float16), )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([10, 1, 24807, 2048],"bool"), -65504.0, dtype=Dtype(float16), ) 	 508047360 	 74569 	 49.05984354019165 	 49.05876111984253 	 0.6718533039093018 	 0.6716766357421875 	 None 	 None 	 None 	 None 	 
2025-08-06 14:40:37.776071 test begin: paddle.full_like(Tensor([10, 13, 2048, 2048],"bool"), -65504.0, dtype=Dtype(float16), )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([10, 13, 2048, 2048],"bool"), -65504.0, dtype=Dtype(float16), ) 	 545259520 	 74569 	 52.63640522956848 	 52.87656879425049 	 0.7242505550384521 	 0.7207231521606445 	 None 	 None 	 None 	 None 	 
2025-08-06 14:42:33.252368 test begin: paddle.full_like(Tensor([199, 256000],"float32"), 0.0, )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([199, 256000],"float32"), 0.0, ) 	 50944000 	 74569 	 10.020637035369873 	 10.037304401397705 	 0.13710570335388184 	 0.13737010955810547 	 None 	 None 	 None 	 None 	 
2025-08-06 14:42:54.361776 test begin: paddle.full_like(Tensor([42, 300, 4096],"float32"), 1, )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([42, 300, 4096],"float32"), 1, ) 	 51609600 	 74569 	 10.146004438400269 	 10.159095048904419 	 0.1389176845550537 	 0.13910889625549316 	 None 	 None 	 None 	 None 	 
2025-08-06 14:43:15.539887 test begin: paddle.full_like(Tensor([6, 8467201],"float32"), 0.0, )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([6, 8467201],"float32"), 0.0, ) 	 50803206 	 74569 	 9.986912727355957 	 10.003129482269287 	 0.13678312301635742 	 0.13700175285339355 	 None 	 None 	 None 	 None 	 
2025-08-06 14:43:38.208624 test begin: paddle.gammainc(Tensor([1270081, 40],"float32"), y=Tensor([1270081, 40],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([1270081, 40],"float32"), y=Tensor([1270081, 40],"float32"), ) 	 101606480 	 2378 	 10.013315439224243 	 5.384827375411987 	 0.0030422210693359375 	 2.311931610107422 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-08-06 14:44:01.456260 test begin: paddle.gammainc(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), ) 	 101606480 	 2378 	 10.02465295791626 	 5.377540588378906 	 0.003038644790649414 	 2.310492992401123 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-08-06 14:44:22.283796 test begin: paddle.gammainc(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), ) 	 101606460 	 2378 	 10.002890825271606 	 5.389267444610596 	 0.003038644790649414 	 2.3118278980255127 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-08-06 14:44:43.846851 test begin: paddle.gammainc(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), ) 	 101606448 	 2378 	 10.009033441543579 	 6.397554636001587 	 0.003039836883544922 	 2.311771869659424 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-08-06 14:45:07.735891 test begin: paddle.gammainc(Tensor([3, 16934401],"float32"), y=Tensor([3, 16934401],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([3, 16934401],"float32"), y=Tensor([3, 16934401],"float32"), ) 	 101606406 	 2378 	 10.055651903152466 	 5.3779072761535645 	 0.003038644790649414 	 2.3104708194732666 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-08-06 14:45:28.600930 test begin: paddle.gammainc(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), ) 	 101606520 	 2378 	 10.001070261001587 	 5.377668380737305 	 0.003038167953491211 	 2.3105592727661133 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-08-06 14:45:49.463037 test begin: paddle.gammaincc(Tensor([1270081, 40],"float32"), Tensor([1270081, 40],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([1270081, 40],"float32"), Tensor([1270081, 40],"float32"), ) 	 101606480 	 2555 	 10.035589694976807 	 18.139967918395996 	 0.0027654170989990234 	 7.251704931259155 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-08-06 14:46:23.739827 test begin: paddle.gammaincc(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), ) 	 101606480 	 2555 	 10.000672340393066 	 18.144439458847046 	 0.002762317657470703 	 7.250177383422852 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-08-06 14:46:59.072287 test begin: paddle.gammaincc(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), ) 	 101606460 	 2555 	 9.99673843383789 	 18.919761419296265 	 0.002762317657470703 	 7.249087333679199 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-08-06 14:47:34.587346 test begin: paddle.gammaincc(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), ) 	 101606448 	 2555 	 10.827392578125 	 18.124009132385254 	 0.0027647018432617188 	 7.248790740966797 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-08-06 14:48:08.722797 test begin: paddle.gammaincc(Tensor([3, 16934401],"float32"), Tensor([3, 16934401],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([3, 16934401],"float32"), Tensor([3, 16934401],"float32"), ) 	 101606406 	 2555 	 10.034491062164307 	 18.12392234802246 	 0.002763032913208008 	 7.249299049377441 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-08-06 14:48:42.974889 test begin: paddle.gammaincc(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), ) 	 101606520 	 2555 	 10.070034742355347 	 18.12515640258789 	 0.0027616024017333984 	 7.250049829483032 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-08-06 14:49:16.068769 test begin: paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([482, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([482, 1],"int64"), ) 	 58720738 	 6316 	 0.5500612258911133 	 98.84883165359497 	 0.08899641036987305 	 9.059906005859375e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 14:50:59.141829 test begin: paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([496, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([496, 1],"int64"), ) 	 58720752 	 6316 	 0.5677809715270996 	 100.7347023487091 	 0.09187459945678711 	 0.0003027915954589844 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 14:52:44.108319 test begin: paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([512, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([512, 1],"int64"), ) 	 58720768 	 6316 	 0.5832705497741699 	 103.61261439323425 	 0.094390869140625 	 0.00012040138244628906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 14:54:32.637159 test begin: paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([482, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([482, 1],"int64"), ) 	 58720738 	 6316 	 0.549553394317627 	 97.84479451179504 	 0.08893322944641113 	 9.34600830078125e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 14:56:15.816200 test begin: paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([496, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([496, 1],"int64"), ) 	 58720752 	 6316 	 0.5649623870849609 	 100.39413380622864 	 0.09142255783081055 	 9.608268737792969e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 14:58:00.397075 test begin: paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([512, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([512, 1],"int64"), ) 	 58720768 	 6316 	 0.5806465148925781 	 104.03437542915344 	 0.09395647048950195 	 0.00015282630920410156 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 14:59:48.698057 test begin: paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([482, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([482, 1],"int64"), ) 	 50878946 	 6316 	 0.7437987327575684 	 98.88355374336243 	 0.12034153938293457 	 0.00021839141845703125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:01:31.508306 test begin: paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([496, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([496, 1],"int64"), ) 	 50878960 	 6316 	 0.7639756202697754 	 101.50257563591003 	 0.12342119216918945 	 9.274482727050781e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:03:18.762033 test begin: paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([512, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([512, 1],"int64"), ) 	 50878976 	 6316 	 0.787266731262207 	 107.86417818069458 	 0.12738275527954102 	 0.0002486705780029297 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:05:10.673233 test begin: paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([482, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([482, 1],"int64"), ) 	 50878946 	 6316 	 1.7810664176940918 	 98.84814095497131 	 0.2877039909362793 	 0.0002446174621582031 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:06:58.195879 test begin: paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([496, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([496, 1],"int64"), ) 	 50878960 	 6316 	 1.8304824829101562 	 101.29746747016907 	 0.2958719730377197 	 0.0002453327178955078 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:08:48.364707 test begin: paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([512, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([512, 1],"int64"), ) 	 50878976 	 6316 	 1.8928067684173584 	 104.21303462982178 	 0.3060448169708252 	 0.0002505779266357422 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:10:43.526668 test begin: paddle.gather_nd(Tensor([1001, 413, 128],"float32"), index=Tensor([20, 50, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([1001, 413, 128],"float32"), index=Tensor([20, 50, 2],"int64"), ) 	 52918864 	 4935 	 0.0547482967376709 	 389.2059180736542 	 1.1205673217773438e-05 	 0.0002231597900390625 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:17:15.474880 test begin: paddle.gather_nd(Tensor([1001, 413, 128],"float32"), index=Tensor([5, 50, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([1001, 413, 128],"float32"), index=Tensor([5, 50, 2],"int64"), ) 	 52917364 	 4935 	 0.053827762603759766 	 97.1843581199646 	 1.5020370483398438e-05 	 0.00021958351135253906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:18:54.413378 test begin: paddle.gather_nd(Tensor([101, 1417, 716],"bfloat16"), Tensor([778, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([101, 1417, 716],"bfloat16"), Tensor([778, 2],"int64"), ) 	 102473328 	 4935 	 0.051603078842163086 	 301.27079725265503 	 1.7404556274414062e-05 	 0.0001976490020751953 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:23:59.007832 test begin: paddle.gather_nd(Tensor([101, 1417, 716],"bfloat16"), Tensor([816, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([101, 1417, 716],"bfloat16"), Tensor([816, 2],"int64"), ) 	 102473404 	 4935 	 0.052846431732177734 	 318.0886175632477 	 1.9550323486328125e-05 	 0.00010085105895996094 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:29:20.375255 test begin: paddle.gather_nd(Tensor([101, 819, 1240],"bfloat16"), Tensor([778, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([101, 819, 1240],"bfloat16"), Tensor([778, 2],"int64"), ) 	 102573116 	 4935 	 0.05760359764099121 	 298.1315088272095 	 0.00530695915222168 	 0.00019288063049316406 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:34:21.910706 test begin: paddle.gather_nd(Tensor([101, 8192, 124],"bfloat16"), Tensor([816, 2],"int64"), )
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([101, 8192, 124],"bfloat16"), Tensor([816, 2],"int64"), ) 	 102598240 	 4935 	 0.05146074295043945 	 317.16138076782227 	 2.5987625122070312e-05 	 0.00017452239990234375 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:39:44.421668 test begin: paddle.gcd(Tensor([10, 50803],"int32"), Tensor([10, 50803],"int32"), )
W0806 15:39:54.450656 96558 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(Tensor([10, 50803],"int32"), Tensor([10, 50803],"int32"), ) 	 1016060 	 1588 	 9.974536895751953 	 0.041181325912475586 	 4.506111145019531e-05 	 0.02493882179260254 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:39:54.554718 test begin: paddle.gcd(Tensor([25401, 20],"int32"), Tensor([25401, 20],"int32"), )
W0806 15:40:04.476250 96928 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(Tensor([25401, 20],"int32"), Tensor([25401, 20],"int32"), ) 	 1016040 	 1588 	 9.864272594451904 	 0.04143118858337402 	 4.9591064453125e-05 	 0.024654865264892578 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:40:08.051207 test begin: paddle.gcd(x=Tensor([12700, 2, 4, 5],"int32"), y=Tensor([12700, 2, 4, 5],"int32"), )
W0806 15:40:21.192533 97236 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(x=Tensor([12700, 2, 4, 5],"int32"), y=Tensor([12700, 2, 4, 5],"int32"), ) 	 1016000 	 1588 	 13.086153268814087 	 0.04105210304260254 	 5.626678466796875e-05 	 0.02452826499938965 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:40:21.243659 test begin: paddle.gcd(x=Tensor([25401, 1, 4, 5],"int32"), y=Tensor([2, 1, 5],"int32"), )
W0806 15:40:32.273391 97699 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(x=Tensor([25401, 1, 4, 5],"int32"), y=Tensor([2, 1, 5],"int32"), ) 	 508030 	 1588 	 10.996937274932861 	 0.08848905563354492 	 4.982948303222656e-05 	 0.0569155216217041 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:40:32.391893 test begin: paddle.gcd(x=Tensor([6, 1, 16934, 5],"int32"), y=Tensor([2, 1, 5],"int32"), )
W0806 15:40:43.859790 98083 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(x=Tensor([6, 1, 16934, 5],"int32"), y=Tensor([2, 1, 5],"int32"), ) 	 508030 	 1588 	 11.437599420547485 	 0.08957648277282715 	 5.817413330078125e-05 	 0.05762743949890137 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:40:44.780441 test begin: paddle.gcd(x=Tensor([6, 1, 4, 21168],"int32"), y=Tensor([2, 1, 21168],"int32"), )
W0806 15:40:57.502530 98329 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(x=Tensor([6, 1, 4, 21168],"int32"), y=Tensor([2, 1, 21168],"int32"), ) 	 550368 	 1588 	 12.476693630218506 	 0.08853626251220703 	 5.173683166503906e-05 	 0.056967973709106445 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:40:57.648846 test begin: paddle.gcd(x=Tensor([6, 2, 4, 10584],"int32"), y=Tensor([6, 2, 4, 10584],"int32"), )
W0806 15:41:08.585618 98774 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(x=Tensor([6, 2, 4, 10584],"int32"), y=Tensor([6, 2, 4, 10584],"int32"), ) 	 1016064 	 1588 	 10.853168964385986 	 0.041162967681884766 	 4.935264587402344e-05 	 0.024766206741333008 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:41:09.266666 test begin: paddle.gcd(x=Tensor([6, 2, 8467, 5],"int32"), y=Tensor([6, 2, 8467, 5],"int32"), )
W0806 15:41:21.609383 99068 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(x=Tensor([6, 2, 8467, 5],"int32"), y=Tensor([6, 2, 8467, 5],"int32"), ) 	 1016040 	 1588 	 12.242683410644531 	 0.04142308235168457 	 6.389617919921875e-05 	 0.01461172103881836 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:41:21.659429 test begin: paddle.gcd(x=Tensor([6, 4233, 4, 5],"int32"), y=Tensor([6, 4233, 4, 5],"int32"), )
W0806 15:41:32.865605 99164 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.gcd 	 paddle.gcd(x=Tensor([6, 4233, 4, 5],"int32"), y=Tensor([6, 4233, 4, 5],"int32"), ) 	 1015920 	 1588 	 11.18131947517395 	 0.041716575622558594 	 6.103515625e-05 	 0.014739990234375 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:41:32.929830 test begin: paddle.geometric.segment_max(Tensor([40, 1270081],"float32"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_max 	 paddle.geometric.segment_max(Tensor([40, 1270081],"float32"), Tensor([40],"int64"), ) 	 50803280 	 9664 	 8.147685527801514 	 111.67549896240234 	 0.0008065700531005859 	 0.0003826618194580078 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:43:44.048087 test begin: paddle.geometric.segment_max(Tensor([40, 2540161],"float16"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_max 	 paddle.geometric.segment_max(Tensor([40, 2540161],"float16"), Tensor([40],"int64"), ) 	 101606480 	 9664 	 15.944952726364136 	 104.56064057350159 	 0.0016140937805175781 	 0.00025391578674316406 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:45:56.794358 test begin: paddle.geometric.segment_max(Tensor([40, 635041],"float64"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_max 	 paddle.geometric.segment_max(Tensor([40, 635041],"float64"), Tensor([40],"int64"), ) 	 25401680 	 9664 	 5.641725540161133 	 94.60229444503784 	 0.0005369186401367188 	 0.000308990478515625 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:47:48.625545 test begin: paddle.geometric.segment_mean(Tensor([1270081, 20],"float64"), Tensor([1270081],"int64"), )
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([1270081, 20],"float64"), Tensor([1270081],"int64"), ) 	 26671701 	 28038 	 13.887476205825806 	 26.425732135772705 	 0.00045371055603027344 	 0.0001842975616455078 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:49:42.249839 test begin: paddle.geometric.segment_mean(Tensor([25401601, 20],"float16"), Tensor([25401601],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8144589a80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 15:59:55.176451 test begin: paddle.geometric.segment_mean(Tensor([25401601, 20],"float32"), Tensor([25401601],"int64"), )
W0806 16:00:03.880611 130348 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb729446fb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 16:10:04.848050 test begin: paddle.geometric.segment_mean(Tensor([25401601, 20],"float64"), Tensor([25401601],"int64"), )
W0806 16:10:15.778839 147446 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd73fff2f50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 16:20:10.062519 test begin: paddle.geometric.segment_mean(Tensor([2540161, 20],"float32"), Tensor([2540161],"int64"), )
W0806 16:20:11.331403  7656 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([2540161, 20],"float32"), Tensor([2540161],"int64"), ) 	 53343381 	 28038 	 15.712195634841919 	 30.602235317230225 	 0.0005238056182861328 	 0.00023293495178222656 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 16:22:41.088038 test begin: paddle.geometric.segment_mean(Tensor([40, 1270081],"float32"), Tensor([40],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff4fb4e6f20>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754469161 (unix time) try "date -d @1754469161" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d3d) received by PID 7485 (TID 0x7ff4f6ab7640) from PID 7485 ***]

2025-08-06 16:32:49.493160 test begin: paddle.geometric.segment_mean(Tensor([40, 635041],"float64"), Tensor([40],"int64"), )
W0806 16:32:50.349196 41548 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7588d0ae60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 16:42:54.116000 test begin: paddle.geometric.segment_mean(Tensor([5080321, 20],"float16"), Tensor([5080321],"int64"), )
W0806 16:42:56.410485 67580 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([5080321, 20],"float16"), Tensor([5080321],"int64"), ) 	 106686741 	 28038 	 56.4779908657074 	 100.78918623924255 	 0.001958608627319336 	 0.0006363391876220703 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 16:50:57.445517 test begin: paddle.geometric.segment_min(Tensor([1270081, 20],"float64"), Tensor([1270081],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([1270081, 20],"float64"), Tensor([1270081],"int64"), ) 	 26671701 	 28537 	 19.428531646728516 	 34.89338493347168 	 0.0006489753723144531 	 0.0002720355987548828 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 16:52:16.602360 test begin: paddle.geometric.segment_min(Tensor([25401601, 20],"float16"), Tensor([25401601],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f180fd668c0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 17:02:21.678744 test begin: paddle.geometric.segment_min(Tensor([25401601, 20],"float32"), Tensor([25401601],"int64"), )
W0806 17:02:31.022222 117240 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc9ae1bad70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 17:12:27.016388 test begin: paddle.geometric.segment_min(Tensor([25401601, 20],"float64"), Tensor([25401601],"int64"), )
W0806 17:12:44.531571 142222 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7abba0af50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 17:22:34.928169 test begin: paddle.geometric.segment_min(Tensor([2540161, 20],"float32"), Tensor([2540161],"int64"), )
W0806 17:22:41.774187  4761 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([2540161, 20],"float32"), Tensor([2540161],"int64"), ) 	 53343381 	 28537 	 27.505362033843994 	 45.708660364151 	 0.0009286403656005859 	 0.0002644062042236328 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:24:21.620568 test begin: paddle.geometric.segment_min(Tensor([40, 1270081],"float32"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([40, 1270081],"float32"), Tensor([40],"int64"), ) 	 50803280 	 28537 	 24.02179503440857 	 30.96031618118286 	 0.0008082389831542969 	 0.00024199485778808594 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:25:42.665323 test begin: paddle.geometric.segment_min(Tensor([40, 2540161],"float16"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([40, 2540161],"float16"), Tensor([40],"int64"), ) 	 101606480 	 28537 	 48.76122450828552 	 65.57584977149963 	 0.0016634464263916016 	 0.0003859996795654297 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:28:10.208083 test begin: paddle.geometric.segment_min(Tensor([40, 635041],"float64"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([40, 635041],"float64"), Tensor([40],"int64"), ) 	 25401680 	 28537 	 18.657798051834106 	 28.95220971107483 	 0.0006103515625 	 0.00026416778564453125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:29:22.465262 test begin: paddle.geometric.segment_min(Tensor([5080321, 20],"float16"), Tensor([5080321],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([5080321, 20],"float16"), Tensor([5080321],"int64"), ) 	 106686741 	 28537 	 61.82595610618591 	 143.7362883090973 	 0.0021314620971679688 	 0.0003409385681152344 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:33:25.714079 test begin: paddle.geometric.segment_sum(Tensor([25401601, 15],"float16"), Tensor([25401601],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([25401601, 15],"float16"), Tensor([25401601],"int64"), ) 	 406425616 	 20010 	 124.15519714355469 	 98.54823565483093 	 0.006173133850097656 	 2.51600980758667 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:38:44.981341 test begin: paddle.geometric.segment_sum(Tensor([25401601, 15],"float32"), Tensor([25401601],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([25401601, 15],"float32"), Tensor([25401601],"int64"), ) 	 406425616 	 20010 	 104.32682681083679 	 83.88565230369568 	 0.005151987075805664 	 2.141805648803711 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:43:48.691206 test begin: paddle.geometric.segment_sum(Tensor([2540161, 20],"float32"), Tensor([2540161],"int32"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([2540161, 20],"float32"), Tensor([2540161],"int32"), ) 	 53343381 	 20010 	 14.085775375366211 	 11.316671371459961 	 0.0006735324859619141 	 0.28864002227783203 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:44:25.930080 test begin: paddle.geometric.segment_sum(Tensor([30, 1693441],"float32"), Tensor([30],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([30, 1693441],"float32"), Tensor([30],"int64"), ) 	 50803260 	 20010 	 11.370110273361206 	 7.7047038078308105 	 0.0005300045013427734 	 0.19659161567687988 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:44:59.895902 test begin: paddle.geometric.segment_sum(Tensor([30, 3386881],"float16"), Tensor([30],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([30, 3386881],"float16"), Tensor([30],"int64"), ) 	 101606460 	 20010 	 25.21507239341736 	 20.207008123397827 	 0.001215219497680664 	 0.515507698059082 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:46:08.240227 test begin: paddle.geometric.segment_sum(Tensor([3386881, 15],"float32"), Tensor([3386881],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([3386881, 15],"float32"), Tensor([3386881],"int64"), ) 	 54190096 	 20010 	 9.054823160171509 	 7.542009592056274 	 0.0004317760467529297 	 0.19251346588134766 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:46:38.524377 test begin: paddle.geometric.segment_sum(Tensor([40, 1270081],"float32"), Tensor([40],"int32"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([40, 1270081],"float32"), Tensor([40],"int32"), ) 	 50803280 	 20010 	 9.87395191192627 	 7.037111759185791 	 0.0004649162292480469 	 0.1809849739074707 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:47:10.929820 test begin: paddle.geometric.segment_sum(Tensor([50803201, 20],"float32"), Tensor([50803201],"int32"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([50803201, 20],"float32"), Tensor([50803201],"int32"), ) 	 1066867221 	 20010 	 165.60829257965088 	 130.71130108833313 	 0.00822901725769043 	 2.230071544647217 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:54:44.055355 test begin: paddle.geometric.segment_sum(Tensor([6773761, 15],"float16"), Tensor([6773761],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([6773761, 15],"float16"), Tensor([6773761],"int64"), ) 	 108380176 	 20010 	 33.46884846687317 	 26.417961835861206 	 0.0016362667083740234 	 0.6740868091583252 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:56:09.645228 test begin: paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "max", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "max", None, None, ) 	 25401640 	 27790 	 28.53207755088806 	 103.91973543167114 	 0.35114502906799316 	 0.0004627704620361328 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:58:59.557937 test begin: paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mean", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mean", None, None, ) 	 25401640 	 27790 	 30.122105836868286 	 121.21344208717346 	 0.2210845947265625 	 0.0002334117889404297 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 18:01:54.832872 test begin: paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "min", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "min", None, None, ) 	 25401640 	 27790 	 28.732613801956177 	 103.57258200645447 	 0.3522980213165283 	 0.0004570484161376953 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 18:04:45.175609 test begin: paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "max", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "max", None, None, ) 	 25401650 	 27790 	 12.503730058670044 	 63.052915811538696 	 0.15306782722473145 	 0.0004551410675048828 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 18:06:05.836003 test begin: paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mean", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mean", None, None, ) 	 25401650 	 27790 	 10.04107928276062 	 72.21276950836182 	 0.07389950752258301 	 0.00021219253540039062 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 18:07:33.212371 test begin: paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "min", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "min", None, None, ) 	 25401650 	 27790 	 12.502001285552979 	 62.00042200088501 	 0.1531848907470703 	 0.00044846534729003906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 18:08:52.803620 test begin: paddle.geometric.send_ue_recv(Tensor([10, 1693441],"float64"), Tensor([15, 1693441],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, )
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([10, 1693441],"float64"), Tensor([15, 1693441],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, ) 	 42336055 	 40736 	 34.87909913063049 	 109.80462861061096 	 0.17465949058532715 	 0.00021386146545410156 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 18:12:04.893323 test begin: paddle.geometric.send_ue_recv(Tensor([10, 2540161],"float64"), Tensor([15, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, )
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([10, 2540161],"float64"), Tensor([15, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, ) 	 63504055 	 40736 	 52.459673166275024 	 145.9281816482544 	 0.26271510124206543 	 0.0002219676971435547 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 18:16:34.416464 test begin: paddle.geometric.send_ue_recv(Tensor([10, 508033, 5],"float64"), Tensor([15, 508033, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe8271fa4d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 18:26:44.145689 test begin: paddle.geometric.send_ue_recv(Tensor([10, 508033, 5],"float64"), Tensor([15, 508033, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, )
W0806 18:26:45.211570 17774 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f23ba8f6a70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 18:36:48.916808 test begin: paddle.geometric.send_ue_recv(Tensor([10, 8, 317521],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, )
W0806 18:36:49.646454 63157 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbd48806ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 18:46:53.895572 test begin: paddle.geometric.send_ue_recv(Tensor([10, 8, 317521],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, )
W0806 18:46:54.920992 108856 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa12a95aef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 18:56:58.907637 test begin: paddle.geometric.send_ue_recv(Tensor([1270081, 20],"float64"), Tensor([15, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, )
W0806 18:56:59.687536 152987 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([1270081, 20],"float64"), Tensor([15, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, ) 	 25401950 	 40736 	 14.665356159210205 	 75.64511156082153 	 0.07354950904846191 	 0.0003173351287841797 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 18:58:40.205851 test begin: paddle.geometric.send_ue_recv(Tensor([635041, 8, 5],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, )
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([635041, 8, 5],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, ) 	 25401790 	 40736 	 22.103246212005615 	 72.1923975944519 	 9.250640869140625e-05 	 0.0004596710205078125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:00:28.367802 test begin: paddle.geometric.send_ue_recv(Tensor([635041, 8, 5],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, )
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([635041, 8, 5],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, ) 	 25401790 	 40736 	 9.957180261611938 	 57.156245470047 	 9.250640869140625e-05 	 0.00023055076599121094 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:01:52.682647 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 20],"float64"), Tensor([25401601],"int64"), Tensor([25401601],"int64"), "add", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4303ed6b60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 19:11:58.216840 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 20],"float64"), Tensor([25401601],"int64"), Tensor([25401601],"int64"), "mul", )
W0806 19:11:59.363091 55032 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2fa8c7ef20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 19:22:02.891111 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 254017],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
W0806 19:22:03.526530 99608 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f06c8ccae60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 19:32:07.592758 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 254017],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", )
W0806 19:32:08.253542 143081 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f758d95f040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 19:42:13.166750 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
W0806 19:42:14.352774 24704 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", ) 	 25401750 	 104455 	 9.252861261367798 	 1.9696433544158936 	 0.00012564659118652344 	 0.0002777576446533203 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:42:44.020848 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", ) 	 25401750 	 104455 	 9.486350297927856 	 1.2131621837615967 	 9.083747863769531e-05 	 8.368492126464844e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:43:27.322228 test begin: paddle.geometric.send_uv(Tensor([100, 20],"float64"), Tensor([100, 1],"float64"), Tensor([25401601],"int64"), Tensor([25401601],"int64"), "add", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc7e134f0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 19:53:32.245847 test begin: paddle.geometric.send_uv(Tensor([100, 20],"float64"), Tensor([25401601, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
W0806 19:53:33.081802 73928 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2149c43100>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 20:03:37.653052 test begin: paddle.geometric.send_uv(Tensor([100, 254017],"float64"), Tensor([100, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
W0806 20:03:43.508679 126400 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9f088aeec0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 20:13:45.887479 test begin: paddle.geometric.send_uv(Tensor([1270081, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
W0806 20:13:46.555261 15121 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([1270081, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", ) 	 26671731 	 104455 	 8.974989652633667 	 1.2310948371887207 	 0.00015091896057128906 	 7.891654968261719e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 20:14:53.765813 test begin: paddle.geometric.send_uv(Tensor([1270081, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([1270081, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", ) 	 26671731 	 104455 	 10.793851375579834 	 1.1744444370269775 	 9.751319885253906e-05 	 7.987022399902344e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 20:16:19.241569 test begin: paddle.geometric.send_uv(Tensor([1270081, 20],"float64"), Tensor([1270081, 1],"float64"), Tensor([1501],"int64"), Tensor([1501],"int64"), "add", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([1270081, 20],"float64"), Tensor([1270081, 1],"float64"), Tensor([1501],"int64"), Tensor([1501],"int64"), "add", ) 	 26674703 	 104455 	 9.683093786239624 	 1.2008469104766846 	 8.869171142578125e-05 	 0.00010347366333007812 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 20:17:27.441069 test begin: paddle.geometric.send_uv(Tensor([25401601, 1],"float64"), Tensor([25401601, 20],"float64"), Tensor([1501],"int64"), Tensor([1501],"int64"), "mul", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc00a052a10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 20:27:32.884813 test begin: paddle.geometric.send_uv(Tensor([25401601, 20],"float64"), Tensor([25401601, 1],"float64"), Tensor([1501],"int64"), Tensor([1501],"int64"), "add", )
W0806 20:27:43.112907 88026 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f16f5702ec0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 20:37:42.136758 test begin: paddle.geometric.send_uv(Tensor([25401601, 20],"float64"), Tensor([25401601, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
W0806 20:37:56.272150 141342 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f39d9b530a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 20:47:47.234934 test begin: paddle.greater_equal(Tensor([13, 15266, 16, 4, 1],"int64"), Tensor([13, 15266, 16, 1, 8],"int64"), )
W0806 20:47:47.922883 31395 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 15266, 16, 4, 1],"int64"), Tensor([13, 15266, 16, 1, 8],"int64"), ) 	 38103936 	 53247 	 29.438339471817017 	 26.699971437454224 	 0.565605878829956 	 0.5117392539978027 	 None 	 None 	 None 	 None 	 
2025-08-06 20:48:44.599433 test begin: paddle.greater_equal(Tensor([13, 2, 122124, 4, 1],"int64"), Tensor([13, 2, 122124, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 122124, 4, 1],"int64"), Tensor([13, 2, 122124, 1, 8],"int64"), ) 	 38102688 	 53247 	 29.67205834388733 	 26.711610555648804 	 0.5685563087463379 	 0.5132925510406494 	 None 	 None 	 None 	 None 	 
2025-08-06 20:49:41.824736 test begin: paddle.greater_equal(Tensor([13, 2, 16, 4, 15266],"int64"), Tensor([13, 2, 16, 1, 15266],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 16, 4, 15266],"int64"), Tensor([13, 2, 16, 1, 15266],"int64"), ) 	 31753280 	 53247 	 11.364498376846313 	 11.688280820846558 	 0.21782612800598145 	 0.22403645515441895 	 None 	 None 	 None 	 None 	 
2025-08-06 20:50:05.648857 test begin: paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 61062],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 61062],"int64"), ) 	 25403456 	 53247 	 30.062519788742065 	 23.671035766601562 	 0.5759928226470947 	 0.4562819004058838 	 None 	 None 	 None 	 None 	 
2025-08-06 20:51:00.966161 test begin: paddle.greater_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), ) 	 25405120 	 53247 	 55.5993218421936 	 48.41590332984924 	 1.0656664371490479 	 0.9325804710388184 	 None 	 None 	 None 	 None 	 
2025-08-06 20:52:45.471360 test begin: paddle.greater_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 61062, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 61062, 8],"int64"), ) 	 228616128 	 53247 	 95.25485897064209 	 83.64767217636108 	 1.8307209014892578 	 1.6031136512756348 	 None 	 None 	 None 	 None 	 
2025-08-06 20:55:48.314788 test begin: paddle.greater_equal(Tensor([13, 2, 244247, 4, 1],"int64"), Tensor([13, 2, 244247, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 244247, 4, 1],"int64"), Tensor([13, 2, 244247, 1, 8],"int64"), ) 	 76205064 	 53247 	 58.951170444488525 	 53.12027668952942 	 1.1350889205932617 	 1.0218429565429688 	 None 	 None 	 None 	 None 	 
2025-08-06 20:57:41.869318 test begin: paddle.greater_equal(Tensor([13, 30531, 16, 4, 1],"int64"), Tensor([13, 30531, 16, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 30531, 16, 4, 1],"int64"), Tensor([13, 30531, 16, 1, 8],"int64"), ) 	 76205376 	 53247 	 58.514228105545044 	 53.12301707267761 	 1.1241040229797363 	 1.0190210342407227 	 None 	 None 	 None 	 None 	 
2025-08-06 20:59:34.930570 test begin: paddle.greater_equal(Tensor([16935, 10, 15, 20],"float32"), Tensor([16935, 10, 15, 20],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([16935, 10, 15, 20],"float32"), Tensor([16935, 10, 15, 20],"float32"), ) 	 101610000 	 53247 	 17.42856502532959 	 17.48043179512024 	 0.3353602886199951 	 0.33486294746398926 	 None 	 None 	 None 	 None 	 
2025-08-06 21:00:12.425092 test begin: paddle.greater_equal(Tensor([198451, 2, 16, 4, 1],"int64"), Tensor([198451, 2, 16, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([198451, 2, 16, 4, 1],"int64"), Tensor([198451, 2, 16, 1, 8],"int64"), ) 	 76205184 	 53247 	 58.94350576400757 	 54.38742113113403 	 1.1310408115386963 	 1.0205633640289307 	 None 	 None 	 None 	 None 	 
2025-08-06 21:02:07.627848 test begin: paddle.greater_equal(Tensor([49613, 1024, 1, 1],"float32"), Tensor([1],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([49613, 1024, 1, 1],"float32"), Tensor([1],"float32"), ) 	 50803713 	 53247 	 10.021026134490967 	 12.318448543548584 	 0.19214534759521484 	 0.23586273193359375 	 None 	 None 	 None 	 None 	 
2025-08-06 21:02:30.890435 test begin: paddle.greater_equal(Tensor([5, 10, 15, 67738],"float32"), Tensor([5, 10, 15, 67738],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([5, 10, 15, 67738],"float32"), Tensor([5, 10, 15, 67738],"float32"), ) 	 101607000 	 53247 	 17.438748121261597 	 17.474361896514893 	 0.33434033393859863 	 0.3362998962402344 	 None 	 None 	 None 	 None 	 
2025-08-06 21:03:07.536041 test begin: paddle.greater_equal(Tensor([5, 10, 50804, 20],"float32"), Tensor([5, 10, 50804, 20],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([5, 10, 50804, 20],"float32"), Tensor([5, 10, 50804, 20],"float32"), ) 	 101608000 	 53247 	 17.434401035308838 	 17.460378408432007 	 0.3355679512023926 	 0.33487677574157715 	 None 	 None 	 None 	 None 	 
2025-08-06 21:03:44.225679 test begin: paddle.greater_equal(Tensor([5, 33869, 15, 20],"float32"), Tensor([5, 33869, 15, 20],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([5, 33869, 15, 20],"float32"), Tensor([5, 33869, 15, 20],"float32"), ) 	 101607000 	 53247 	 17.44528603553772 	 17.471649646759033 	 0.33423614501953125 	 0.3348402976989746 	 None 	 None 	 None 	 None 	 
2025-08-06 21:04:20.997686 test begin: paddle.greater_equal(Tensor([8, 1024, 1, 6202],"float32"), Tensor([1],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([8, 1024, 1, 6202],"float32"), Tensor([1],"float32"), ) 	 50806785 	 53247 	 9.985823631286621 	 12.302292585372925 	 0.191453218460083 	 0.23720216751098633 	 None 	 None 	 None 	 None 	 
2025-08-06 21:04:44.376654 test begin: paddle.greater_equal(Tensor([8, 1024, 6202, 1],"float32"), Tensor([1],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([8, 1024, 6202, 1],"float32"), Tensor([1],"float32"), ) 	 50806785 	 53247 	 9.992365598678589 	 12.304921865463257 	 0.19287705421447754 	 0.23575353622436523 	 None 	 None 	 None 	 None 	 
2025-08-06 21:05:07.793839 test begin: paddle.greater_equal(Tensor([8, 6350401, 1, 1],"float32"), Tensor([1],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([8, 6350401, 1, 1],"float32"), Tensor([1],"float32"), ) 	 50803209 	 53247 	 10.025261640548706 	 12.322360038757324 	 0.19210267066955566 	 0.23583579063415527 	 None 	 None 	 None 	 None 	 
2025-08-06 21:05:33.058724 test begin: paddle.greater_equal(Tensor([99226, 2, 16, 4, 1],"int64"), Tensor([99226, 2, 16, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([99226, 2, 16, 4, 1],"int64"), Tensor([99226, 2, 16, 1, 8],"int64"), ) 	 38102784 	 53247 	 29.40536856651306 	 26.715662956237793 	 0.5648257732391357 	 0.5133371353149414 	 None 	 None 	 None 	 None 	 
2025-08-06 21:06:30.083508 test begin: paddle.greater_than(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 50803600 	 53046 	 10.088746547698975 	 13.203861713409424 	 0.19552397727966309 	 0.2539248466491699 	 None 	 None 	 None 	 None 	 
2025-08-06 21:06:56.123694 test begin: paddle.greater_than(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), ) 	 50803600 	 53046 	 9.977768182754517 	 13.236138582229614 	 0.1918933391571045 	 0.25447893142700195 	 None 	 None 	 None 	 None 	 
2025-08-06 21:07:20.260385 test begin: paddle.greater_than(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 101606800 	 53046 	 17.370707750320435 	 17.403841257095337 	 0.33428454399108887 	 0.3348240852355957 	 None 	 None 	 None 	 None 	 
2025-08-06 21:07:58.387754 test begin: paddle.greater_than(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), ) 	 101606420 	 53046 	 17.378241777420044 	 17.408275604248047 	 0.3342013359069824 	 0.3362593650817871 	 None 	 None 	 None 	 None 	 
2025-08-06 21:08:35.178842 test begin: paddle.greater_than(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float32"), )
W0806 21:08:40.128762 137908 dygraph_functions.cc:90428] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float32"), ) 	 203212812 	 53046 	 60.05896496772766 	 38.1484854221344 	 0.577646017074585 	 0.7351257801055908 	 None 	 None 	 None 	 None 	 
2025-08-06 21:10:18.902225 test begin: paddle.greater_than(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float64"), ) 	 203212812 	 53046 	 107.74971032142639 	 51.96352767944336 	 1.0366616249084473 	 0.9993829727172852 	 None 	 None 	 None 	 None 	 
2025-08-06 21:13:02.836969 test begin: paddle.greater_than(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), ) 	 101606440 	 53046 	 17.380666732788086 	 17.396507740020752 	 0.3343174457550049 	 0.33490800857543945 	 None 	 None 	 None 	 None 	 
2025-08-06 21:13:42.083557 test begin: paddle.greater_than(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float32"), ) 	 203212816 	 53046 	 60.054816007614136 	 38.152727365493774 	 0.5776593685150146 	 0.7366516590118408 	 None 	 None 	 None 	 None 	 
2025-08-06 21:15:24.028773 test begin: paddle.greater_than(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float64"), ) 	 203212816 	 53046 	 107.74917364120483 	 51.95958971977234 	 1.0362117290496826 	 0.9995265007019043 	 None 	 None 	 None 	 None 	 
2025-08-06 21:18:07.982243 test begin: paddle.greater_than(Tensor([4, 3, 2116801],"float16"), Tensor([4, 3, 2116801],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3, 2116801],"float16"), Tensor([4, 3, 2116801],"float64"), ) 	 50803224 	 53046 	 27.452436447143555 	 13.330843687057495 	 0.26395368576049805 	 0.25733017921447754 	 None 	 None 	 None 	 None 	 
2025-08-06 21:18:50.895509 test begin: paddle.greater_than(Tensor([4, 3, 4233601],"float16"), Tensor([4, 3, 4233601],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3, 4233601],"float16"), Tensor([4, 3, 4233601],"float32"), ) 	 101606424 	 53046 	 30.271412134170532 	 19.274980306625366 	 0.2911796569824219 	 0.37076711654663086 	 None 	 None 	 None 	 None 	 
2025-08-06 21:19:42.320048 test begin: paddle.greater_than(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float32"), ) 	 203212824 	 53046 	 60.053168058395386 	 38.14825224876404 	 0.5777413845062256 	 0.7337088584899902 	 None 	 None 	 None 	 None 	 
2025-08-06 21:21:24.317701 test begin: paddle.greater_than(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float64"), ) 	 203212824 	 53046 	 107.75292944908142 	 51.95739126205444 	 1.0404820442199707 	 1.0009551048278809 	 None 	 None 	 None 	 None 	 
2025-08-06 21:24:08.568634 test begin: paddle.greater_than(Tensor([4, 3175201, 2],"float16"), Tensor([4, 3175201, 2],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3175201, 2],"float16"), Tensor([4, 3175201, 2],"float64"), ) 	 50803216 	 53046 	 27.446406841278076 	 14.60527515411377 	 0.26400113105773926 	 0.2560696601867676 	 None 	 None 	 None 	 None 	 
2025-08-06 21:24:53.242742 test begin: paddle.greater_than(Tensor([4, 6350401, 2],"float16"), Tensor([4, 6350401, 2],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 6350401, 2],"float16"), Tensor([4, 6350401, 2],"float32"), ) 	 101606416 	 53046 	 30.28223490715027 	 19.268780946731567 	 0.29119110107421875 	 0.3719789981842041 	 None 	 None 	 None 	 None 	 
2025-08-06 21:25:44.698617 test begin: paddle.greater_than(Tensor([4233601, 3, 2],"float16"), Tensor([4233601, 3, 2],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4233601, 3, 2],"float16"), Tensor([4233601, 3, 2],"float64"), ) 	 50803212 	 53046 	 27.454543828964233 	 13.31525444984436 	 0.2653956413269043 	 0.25892090797424316 	 None 	 None 	 None 	 None 	 
2025-08-06 21:26:26.643476 test begin: paddle.greater_than(Tensor([8467201, 3, 2],"float16"), Tensor([8467201, 3, 2],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([8467201, 3, 2],"float16"), Tensor([8467201, 3, 2],"float32"), ) 	 101606412 	 53046 	 30.26045846939087 	 19.280113220214844 	 0.29120659828186035 	 0.37224626541137695 	 None 	 None 	 None 	 None 	 
2025-08-06 21:27:18.072596 test begin: paddle.heaviside(Tensor([24807, 2048],"float32"), Tensor([1],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f334ea1b430>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 21:37:57.404982 test begin: paddle.heaviside(Tensor([24807, 2048],"float32"), Tensor([2048],"float32"), )
W0806 21:37:58.351981 126719 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([24807, 2048],"float32"), Tensor([2048],"float32"), ) 	 50806784 	 33800 	 10.067363023757935 	 10.34498643875122 	 0.3039734363555908 	 0.31232619285583496 	 None 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-08-06 21:39:05.129140 test begin: paddle.heaviside(Tensor([24807, 2048],"float32"), Tensor([24807, 2048],"float32"), )
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([24807, 2048],"float32"), Tensor([24807, 2048],"float32"), ) 	 101609472 	 33800 	 15.256829738616943 	 15.119786739349365 	 0.46042585372924805 	 0.4564352035522461 	 None 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-08-06 21:40:00.586390 test begin: paddle.heaviside(Tensor([300, 169345],"float32"), Tensor([169345],"float32"), )
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([300, 169345],"float32"), Tensor([169345],"float32"), ) 	 50972845 	 33800 	 10.020461320877075 	 10.348340034484863 	 0.30236220359802246 	 0.31374502182006836 	 None 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-08-06 21:40:45.545502 test begin: paddle.heaviside(Tensor([300, 169345],"float32"), Tensor([1],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6de34e6b00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 21:51:24.893964 test begin: paddle.heaviside(Tensor([300, 169345],"float32"), Tensor([300, 169345],"float32"), )
W0806 21:51:26.687948 33727 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([300, 169345],"float32"), Tensor([300, 169345],"float32"), ) 	 101607000 	 33800 	 15.235180377960205 	 15.113354921340942 	 0.46138453483581543 	 0.45908212661743164 	 None 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-08-06 21:52:21.610320 test begin: paddle.histogram(input=Tensor([4, 6350401],"int64"), )
[Prof] paddle.histogram 	 paddle.histogram(input=Tensor([4, 6350401],"int64"), ) 	 25401604 	 1544 	 9.892813444137573 	 1.1862621307373047 	 0.00036907196044921875 	 0.00043463706970214844 	 None 	 None 	 None 	 None 	 
2025-08-06 21:52:33.290107 test begin: paddle.histogram(input=Tensor([6350401, 4],"int64"), )
[Prof] paddle.histogram 	 paddle.histogram(input=Tensor([6350401, 4],"int64"), ) 	 25401604 	 1544 	 9.863255739212036 	 1.24580717086792 	 0.000370025634765625 	 0.00038695335388183594 	 None 	 None 	 None 	 None 	 
2025-08-06 21:52:44.866871 test begin: paddle.histogram_bin_edges(Tensor([2540161, 20],"float32"), bins=10, min=0, max=1, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([2540161, 20],"float32"), bins=10, min=0, max=1, ) 	 50803220 	 98929 	 10.561439514160156 	 1.6018140316009521 	 5.221366882324219e-05 	 0.0001049041748046875 	 None 	 None 	 None 	 None 	 combined
2025-08-06 21:52:57.987722 test begin: paddle.histogram_bin_edges(Tensor([5, 10160641],"float32"), bins=10, min=1, max=1, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([5, 10160641],"float32"), bins=10, min=1, max=1, ) 	 50803205 	 98929 	 10.903295040130615 	 1.6823046207427979 	 5.245208740234375e-05 	 0.0001266002655029297 	 None 	 None 	 None 	 None 	 combined
2025-08-06 21:53:11.454551 test begin: paddle.histogram_bin_edges(Tensor([50, 10160641],"float32"), bins=10, min=0, max=1, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([50, 10160641],"float32"), bins=10, min=0, max=1, ) 	 508032050 	 98929 	 9.999897718429565 	 1.6341381072998047 	 4.5299530029296875e-05 	 8.392333984375e-05 	 None 	 None 	 None 	 None 	 combined
2025-08-06 21:53:32.551782 test begin: paddle.histogram_bin_edges(Tensor([50, 10160641],"float32"), bins=10, min=1, max=1, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([50, 10160641],"float32"), bins=10, min=1, max=1, ) 	 508032050 	 98929 	 10.1204252243042 	 1.6839778423309326 	 3.0279159545898438e-05 	 0.00024962425231933594 	 None 	 None 	 None 	 None 	 combined
2025-08-06 21:53:52.832960 test begin: paddle.histogramdd(Tensor([1270, 2, 2],"float64"), bins=5, weights=Tensor([1270, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )
/usr/local/lib/python3.10/dist-packages/paddle/tensor/linalg.py:5741: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  edge = paddle.to_tensor(edge)
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([1270, 2, 2],"float64"), bins=5, weights=Tensor([1270, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 	 7620 	 5336 	 10.407394170761108 	 0.34988999366760254 	 2.7418136596679688e-05 	 0.00010418891906738281 	 None 	 None 	 None 	 None 	 
2025-08-06 21:54:03.701203 test begin: paddle.histogramdd(Tensor([4, 15876, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 15876],"float64"), ranges=None, density=False, )
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([4, 15876, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 15876],"float64"), ranges=None, density=False, ) 	 317520 	 5336 	 21.6078839302063 	 6.203443288803101 	 3.0040740966796875e-05 	 0.0002799034118652344 	 None 	 None 	 None 	 None 	 
2025-08-06 21:54:31.702897 test begin: paddle.histogramdd(Tensor([4, 15876, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 15876],"float64"), ranges=None, density=True, )
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([4, 15876, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 15876],"float64"), ranges=None, density=True, ) 	 317520 	 5336 	 21.803289890289307 	 7.159489154815674 	 2.8133392333984375e-05 	 0.0002796649932861328 	 None 	 None 	 None 	 None 	 
2025-08-06 21:55:00.687619 test begin: paddle.histogramdd(Tensor([4, 63504, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 63504],"float64"), ranges=None, density=False, )
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([4, 63504, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 63504],"float64"), ranges=None, density=False, ) 	 1270080 	 5336 	 112.39993119239807 	 23.12157940864563 	 5.030632019042969e-05 	 0.0003695487976074219 	 None 	 None 	 None 	 None 	 
2025-08-06 21:57:16.284360 test begin: paddle.histogramdd(Tensor([4, 63504, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 63504],"float64"), ranges=None, density=True, )
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([4, 63504, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 63504],"float64"), ranges=None, density=True, ) 	 1270080 	 5336 	 113.35498332977295 	 25.07561683654785 	 2.956390380859375e-05 	 0.0002818107604980469 	 None 	 None 	 None 	 None 	 
2025-08-06 21:59:34.835326 test begin: paddle.histogramdd(Tensor([63504, 2, 2],"float64"), bins=5, weights=Tensor([63504, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([63504, 2, 2],"float64"), bins=5, weights=Tensor([63504, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 	 381024 	 5336 	 17.494596242904663 	 0.6205084323883057 	 0.00017833709716796875 	 0.00012755393981933594 	 None 	 None 	 None 	 None 	 
2025-08-06 21:59:53.016600 test begin: paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[-1,1,3,], )
W0806 22:00:21.310462 23130 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[-1,1,3,], ) 	 254016180 	 578883 	 18.845454931259155 	 5.78736138343811 	 0.0001232624053955078 	 9.1552734375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 22:00:29.282471 test begin: paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[-1,], )
W0806 22:00:46.613984 26420 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[-1,], ) 	 254016180 	 578883 	 10.265267133712769 	 4.660099267959595 	 0.00011229515075683594 	 0.0002696514129638672 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 22:00:52.817952 test begin: paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[2,4,], )
W0806 22:01:14.514559 28276 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([14112010, 6, 3],"int64"), list[2,4,], ) 	 254016180 	 578883 	 14.503235816955566 	 5.10468053817749 	 0.00014495849609375 	 9.417533874511719e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 22:01:21.336912 test begin: paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[-1,1,3,], )
W0806 22:01:50.772471 30482 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[-1,1,3,], ) 	 254016120 	 578883 	 18.765963792800903 	 5.71301007270813 	 0.00012111663818359375 	 0.00029397010803222656 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 22:01:58.673271 test begin: paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[-1,], )
W0806 22:02:16.678546 34145 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[-1,], ) 	 254016120 	 578883 	 10.686026096343994 	 4.399550676345825 	 0.00012445449829101562 	 0.00012636184692382812 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 22:02:22.986782 test begin: paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[2,4,], )
W0806 22:02:44.242251 36099 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 2116801, 3],"int64"), list[2,4,], ) 	 254016120 	 578883 	 14.167770147323608 	 5.385576009750366 	 0.00012087821960449219 	 9.751319885253906e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 22:02:51.342275 test begin: paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[-1,1,3,], )
W0806 22:03:21.627034 38334 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[-1,1,3,], ) 	 254016240 	 578883 	 20.458311319351196 	 5.809467792510986 	 0.00011992454528808594 	 9.226799011230469e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 22:03:29.374267 test begin: paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[-1,], )
W0806 22:03:46.439596 41686 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[-1,], ) 	 254016240 	 578883 	 10.102298021316528 	 4.339696407318115 	 0.00011754035949707031 	 0.00023555755615234375 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 22:03:52.336523 test begin: paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[2,4,], )
W0806 22:04:14.047667 43609 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.hsplit 	 paddle.hsplit(Tensor([40, 6, 1058401],"int64"), list[2,4,], ) 	 254016240 	 578883 	 14.532605648040771 	 5.164964199066162 	 0.00011587142944335938 	 9.226799011230469e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 22:04:20.942582 test begin: paddle.hstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], ) 	 76204872 	 32425 	 30.146798133850098 	 29.971832275390625 	 0.9515652656555176 	 0.9437181949615479 	 30.123653173446655 	 2.3289096355438232 	 0.9491410255432129 	 7.82012939453125e-05 	 
2025-08-06 22:05:56.952552 test begin: paddle.hstack(list[Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2, 1058401],"float64"),], ) 	 25401624 	 32425 	 10.235538244247437 	 10.1605384349823 	 0.3221292495727539 	 0.16119718551635742 	 10.083806037902832 	 1.7972168922424316 	 0.3172769546508789 	 0.00010204315185546875 	 
2025-08-06 22:06:30.367436 test begin: paddle.hstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401870 	 32425 	 10.272037267684937 	 10.434842348098755 	 0.3247511386871338 	 0.3283119201660156 	 10.144243478775024 	 2.2531046867370605 	 0.3192932605743408 	 0.00010204315185546875 	 
2025-08-06 22:07:04.674271 test begin: paddle.hstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401870 	 32425 	 10.263504981994629 	 10.411541938781738 	 0.3229379653930664 	 0.32776856422424316 	 10.160407066345215 	 2.2370855808258057 	 0.31971096992492676 	 0.00010085105895996094 	 
2025-08-06 22:07:39.671349 test begin: paddle.hstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], ) 	 76204836 	 32425 	 30.081120252609253 	 29.92841386795044 	 0.9482855796813965 	 0.9434731006622314 	 30.18016767501831 	 2.2917566299438477 	 0.9540891647338867 	 7.82012939453125e-05 	 
2025-08-06 22:09:15.527089 test begin: paddle.hstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 25401654 	 32425 	 10.301045417785645 	 10.335236549377441 	 0.3242306709289551 	 0.32501673698425293 	 10.145594596862793 	 2.279914140701294 	 0.31931567192077637 	 0.00019860267639160156 	 
2025-08-06 22:09:50.247783 test begin: paddle.hstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401654 	 32425 	 10.269285678863525 	 10.33958888053894 	 0.32332324981689453 	 0.3253817558288574 	 10.142687320709229 	 2.2244222164154053 	 0.319108247756958 	 0.00010728836059570312 	 
2025-08-06 22:10:24.401548 test begin: paddle.hstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 76204980 	 32425 	 30.09239625930786 	 30.15972661972046 	 0.9511029720306396 	 0.9488232135772705 	 30.179231882095337 	 2.2595643997192383 	 0.9543092250823975 	 0.00010132789611816406 	 
2025-08-06 22:12:00.552467 test begin: paddle.hstack(list[Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401660 	 32425 	 10.198505640029907 	 10.163728475570679 	 0.3211359977722168 	 0.15984201431274414 	 10.099416732788086 	 1.7684714794158936 	 0.3177652359008789 	 8.106231689453125e-05 	 
2025-08-06 22:12:33.925066 test begin: paddle.hstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401654 	 32425 	 10.270700216293335 	 10.124287605285645 	 0.323408842086792 	 0.3198416233062744 	 10.144551992416382 	 2.293144464492798 	 0.3205685615539551 	 0.00010156631469726562 	 
2025-08-06 22:13:07.966698 test begin: paddle.hstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 76204818 	 32425 	 30.261249780654907 	 30.09101963043213 	 0.9524056911468506 	 0.9498319625854492 	 30.665176153182983 	 2.228268623352051 	 0.9645030498504639 	 8.702278137207031e-05 	 
2025-08-06 22:14:44.684111 test begin: paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401870 	 32425 	 10.264513731002808 	 10.234652042388916 	 0.3229639530181885 	 0.322113037109375 	 10.150518655776978 	 2.266453266143799 	 0.319286584854126 	 8.440017700195312e-05 	 
2025-08-06 22:15:18.737897 test begin: paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 76204890 	 32425 	 30.292559146881104 	 30.427003383636475 	 0.9562530517578125 	 0.9589152336120605 	 30.551668405532837 	 2.32835054397583 	 0.961585521697998 	 0.00016617774963378906 	 
2025-08-06 22:16:55.734357 test begin: paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401630 	 32425 	 10.263755798339844 	 10.159066915512085 	 0.3242928981781006 	 0.15983891487121582 	 10.130570650100708 	 1.8335063457489014 	 0.31876134872436523 	 7.987022399902344e-05 	 
2025-08-06 22:17:29.308873 test begin: paddle.hstack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], ) 	 76204824 	 32425 	 31.696424961090088 	 45.48967218399048 	 0.998960018157959 	 1.4284982681274414 	 31.133875370025635 	 2.236609935760498 	 0.9809181690216064 	 0.00010156631469726562 	 
2025-08-06 22:19:23.250043 test begin: paddle.hstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 76204920 	 32425 	 30.06537652015686 	 33.63719320297241 	 0.9461236000061035 	 1.2188720703125 	 30.476969480514526 	 2.2642710208892822 	 0.9629528522491455 	 8.225440979003906e-05 	 
2025-08-06 22:21:04.208226 test begin: paddle.hstack(list[Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401640 	 32425 	 9.984696388244629 	 10.158189535140991 	 0.3155996799468994 	 0.15984177589416504 	 10.594838619232178 	 1.7804863452911377 	 0.3333420753479004 	 7.772445678710938e-05 	 
2025-08-06 22:21:39.525291 test begin: paddle.hypot(Tensor([10, 5080321],"float32"), Tensor([10, 1],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([10, 5080321],"float32"), Tensor([10, 1],"float32"), ) 	 50803220 	 10387 	 10.00331163406372 	 3.3367063999176025 	 0.2470717430114746 	 0.3279886245727539 	 11.658915519714355 	 19.054596185684204 	 0.22910284996032715 	 0.31274843215942383 	 
2025-08-06 22:22:25.428472 test begin: paddle.hypot(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), ) 	 101606420 	 10387 	 15.381279945373535 	 4.663081407546997 	 0.38057422637939453 	 0.4591250419616699 	 17.341366052627563 	 18.640774250030518 	 0.34210848808288574 	 0.460695743560791 	 
2025-08-06 22:23:24.016077 test begin: paddle.hypot(Tensor([2540161, 20],"float32"), Tensor([2540161, 20],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([2540161, 20],"float32"), Tensor([2540161, 20],"float32"), ) 	 101606440 	 10387 	 15.377096176147461 	 4.665205478668213 	 0.3778691291809082 	 0.4579944610595703 	 17.34178614616394 	 18.63947582244873 	 0.3408851623535156 	 0.4577810764312744 	 
2025-08-06 22:24:23.329643 test begin: paddle.hypot(Tensor([50803201],"float32"), Tensor([1],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([50803201],"float32"), Tensor([1],"float32"), ) 	 50803202 	 10387 	 10.002323865890503 	 3.2800097465515137 	 0.24574685096740723 	 0.32097339630126953 	 11.00438666343689 	 18.76524829864502 	 0.21633052825927734 	 0.3067195415496826 	 
2025-08-06 22:25:08.065302 test begin: paddle.hypot(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 10387 	 15.383382558822632 	 4.661696910858154 	 0.377835750579834 	 0.45798158645629883 	 17.33628749847412 	 18.645647525787354 	 0.34230947494506836 	 0.459200382232666 	 
2025-08-06 22:26:06.946733 test begin: paddle.hypot(Tensor([5080321, 10],"float32"), Tensor([5080321, 1],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([5080321, 10],"float32"), Tensor([5080321, 1],"float32"), ) 	 55883531 	 10387 	 10.550657510757446 	 3.721264123916626 	 0.25921058654785156 	 0.3421146869659424 	 13.502990007400513 	 21.791263818740845 	 0.332808256149292 	 0.4295327663421631 	 
2025-08-06 22:27:01.470213 test begin: paddle.i0(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 21481 	 10.015814065933228 	 8.76618218421936 	 0.4771559238433838 	 0.416353702545166 	 9.611931562423706 	 18.38093066215515 	 0.45644116401672363 	 0.43685078620910645 	 
2025-08-06 22:27:50.234628 test begin: paddle.i0(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 21481 	 10.048548221588135 	 8.766823530197144 	 0.478712797164917 	 0.4177207946777344 	 9.564502000808716 	 18.38764238357544 	 0.4542813301086426 	 0.43678736686706543 	 
2025-08-06 22:28:39.993773 test begin: paddle.i0(Tensor([25401601],"float64"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([25401601],"float64"), ) 	 25401601 	 21481 	 10.639580011367798 	 9.985039472579956 	 0.5055513381958008 	 0.47693538665771484 	 11.063869953155518 	 19.698734521865845 	 0.5304808616638184 	 0.46787118911743164 	 
2025-08-06 22:29:32.680817 test begin: paddle.i0(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 21481 	 10.04297661781311 	 8.771733045578003 	 0.47723960876464844 	 0.41918277740478516 	 9.564273118972778 	 18.39435648918152 	 0.45428013801574707 	 0.4383366107940674 	 
2025-08-06 22:30:21.584306 test begin: paddle.i0(Tensor([50803201],"float32"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([50803201],"float32"), ) 	 50803201 	 21481 	 10.049726963043213 	 8.765892267227173 	 0.47866392135620117 	 0.4193606376647949 	 9.55752968788147 	 18.381114721298218 	 0.45569872856140137 	 0.43683457374572754 	 
2025-08-06 22:31:10.047392 test begin: paddle.i0e(Tensor([25401601],"float64"), )
[Prof] paddle.i0e 	 paddle.i0e(Tensor([25401601],"float64"), ) 	 25401601 	 25383 	 10.001955270767212 	 9.260412216186523 	 0.40198493003845215 	 0.3728780746459961 	 14.971444606781006 	 50.889206647872925 	 0.601813793182373 	 0.40936946868896484 	 
2025-08-06 22:32:37.921901 test begin: paddle.i0e(Tensor([50803201],"float32"), )
[Prof] paddle.i0e 	 paddle.i0e(Tensor([50803201],"float32"), ) 	 50803201 	 25383 	 10.925535202026367 	 9.332475423812866 	 0.4405491352081299 	 0.37497401237487793 	 14.911707162857056 	 49.07086753845215 	 0.5993690490722656 	 0.3946571350097656 	 
2025-08-06 22:34:04.386367 test begin: paddle.i1(Tensor([25401601],"float64"), )
[Prof] paddle.i1 	 paddle.i1(Tensor([25401601],"float64"), ) 	 25401601 	 19933 	 9.97512435913086 	 9.411365747451782 	 0.5133695602416992 	 0.48154163360595703 	 12.06640338897705 	 63.993244647979736 	 0.6193687915802002 	 0.2979857921600342 	 
2025-08-06 22:35:42.451622 test begin: paddle.i1(Tensor([50803201],"float32"), )
[Prof] paddle.i1 	 paddle.i1(Tensor([50803201],"float32"), ) 	 50803201 	 19933 	 6.7929136753082275 	 8.200774431228638 	 0.3492560386657715 	 0.4224386215209961 	 11.762452840805054 	 66.63099932670593 	 0.6020019054412842 	 0.31011033058166504 	 
2025-08-05 21:37:44.627233 test begin: paddle.i1e(Tensor([25401601],"float64"), )
W0805 21:37:46.191908 108289 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.i1e 	 paddle.i1e(Tensor([25401601],"float64"), ) 	 25401601 	 31796 	 12.570128917694092 	 11.917454481124878 	 0.40381860733032227 	 0.3834719657897949 	 18.731853246688843 	 122.33675169944763 	 0.601996660232544 	 0.30284857749938965 	 
2025-08-05 21:40:32.925306 test begin: paddle.i1e(Tensor([50803201],"float32"), )
[Prof] paddle.i1e 	 paddle.i1e(Tensor([50803201],"float32"), ) 	 50803201 	 31796 	 9.997546672821045 	 10.207173824310303 	 0.32126355171203613 	 0.303725004196167 	 18.667983293533325 	 128.57563185691833 	 0.6000370979309082 	 0.3182075023651123 	 
2025-08-05 21:43:26.767415 test begin: paddle.incubate.nn.functional.fused_dropout_add(Tensor([7576, 13412],"bfloat16"), Tensor([7576, 13412],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, )
/usr/local/lib/python3.10/dist-packages/paddle/incubate/nn/functional/fused_dropout_add.py:100: UserWarning: Currently, fused_dropout_add maybe has precision problem, so it falls back to dropout + add. 
  warnings.warn(
[Prof] paddle.incubate.nn.functional.fused_dropout_add 	 paddle.incubate.nn.functional.fused_dropout_add(Tensor([7576, 13412],"bfloat16"), Tensor([7576, 13412],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, ) 	 203218624 	 22272 	 9.992979526519775 	 10.028019666671753 	 0.4586632251739502 	 0.4600944519042969 	 21.43404221534729 	 10.1021888256073 	 0.9835805892944336 	 0.4635281562805176 	 combined
2025-08-05 21:44:27.374651 test begin: paddle.incubate.nn.functional.fused_dropout_add(Tensor([7712, 13176],"bfloat16"), Tensor([7712, 13176],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, )
[Prof] paddle.incubate.nn.functional.fused_dropout_add 	 paddle.incubate.nn.functional.fused_dropout_add(Tensor([7712, 13176],"bfloat16"), Tensor([7712, 13176],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, ) 	 203226624 	 22272 	 9.987924575805664 	 10.027205467224121 	 0.458266019821167 	 0.4600257873535156 	 21.43408179283142 	 10.102925777435303 	 0.9836156368255615 	 0.46361517906188965 	 combined
2025-08-05 21:45:27.326091 test begin: paddle.incubate.nn.functional.fused_dropout_add(Tensor([79381, 1280],"bfloat16"), Tensor([79381, 1280],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, )
[Prof] paddle.incubate.nn.functional.fused_dropout_add 	 paddle.incubate.nn.functional.fused_dropout_add(Tensor([79381, 1280],"bfloat16"), Tensor([79381, 1280],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, ) 	 203215360 	 22272 	 9.987850666046143 	 10.02739667892456 	 0.45841336250305176 	 0.46007204055786133 	 21.43373680114746 	 10.101928472518921 	 0.9835705757141113 	 0.4635748863220215 	 combined
2025-08-05 21:46:24.586463 test begin: paddle.incubate.nn.functional.fused_dropout_add(Tensor([8168, 12440],"bfloat16"), Tensor([8168, 12440],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, )
[Prof] paddle.incubate.nn.functional.fused_dropout_add 	 paddle.incubate.nn.functional.fused_dropout_add(Tensor([8168, 12440],"bfloat16"), Tensor([8168, 12440],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, ) 	 203219840 	 22272 	 9.982014656066895 	 11.769999980926514 	 0.45750999450683594 	 0.46017932891845703 	 21.433520317077637 	 10.102103471755981 	 0.9836554527282715 	 0.46346616744995117 	 combined
2025-08-05 21:47:29.493847 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 14176, 7168],"bfloat16"), Tensor([7168, 12800],"bfloat16"), Tensor([12800],"bfloat16"), )
W0805 21:47:55.691310 110127 backward.cc:462] While running Node (FusedGemmEpilogueGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([1, 14176, 7168],"bfloat16"), Tensor([7168, 12800],"bfloat16"), Tensor([12800],"bfloat16"), ) 	 193376768 	 1922 	 20.565500497817993 	 20.69655680656433 	 10.97091817855835 	 11.02922534942627 	 None 	 None 	 None 	 None 	 combined
2025-08-05 21:49:01.855144 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 12404],"bfloat16"), Tensor([12404, 8192],"bfloat16"), None, False, None, )
W0805 21:49:14.694392 110241 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 12404],"bfloat16"), Tensor([12404, 8192],"bfloat16"), None, False, None, ) 	 152420352 	 1922 	 9.95499563217163 	 10.156440734863281 	 2.651376962661743 	 2.6999902725219727 	 None 	 None 	 None 	 None 	 combined
2025-08-05 21:49:48.770772 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 16384],"bfloat16"), Tensor([16384, 6202],"bfloat16"), None, False, None, )
W0805 21:50:02.840838 110255 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 16384],"bfloat16"), Tensor([16384, 6202],"bfloat16"), None, False, None, ) 	 168722432 	 1922 	 11.143162965774536 	 11.164023637771606 	 2.9601378440856934 	 2.968168258666992 	 None 	 None 	 None 	 None 	 combined
2025-08-05 21:50:39.701024 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 24807],"bfloat16"), Tensor([24807, 8192],"bfloat16"), None, False, None, )
W0805 21:51:20.911357 110268 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 24807],"bfloat16"), Tensor([24807, 8192],"bfloat16"), None, False, None, ) 	 304828416 	 1922 	 36.2299964427948 	 40.83836841583252 	 9.631860971450806 	 10.856862545013428 	 None 	 None 	 None 	 None 	 combined
2025-08-05 21:53:33.374690 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 8192],"bfloat16"), Tensor([8192, 12404],"bfloat16"), None, transpose_weight=False, )
W0805 21:53:50.955338 110311 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 8192],"bfloat16"), Tensor([8192, 12404],"bfloat16"), None, transpose_weight=False, ) 	 135168000 	 1922 	 10.728006839752197 	 10.795973777770996 	 5.701009750366211 	 5.739659070968628 	 None 	 None 	 None 	 None 	 combined
2025-08-05 21:54:27.313280 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 6202, 16384],"bfloat16"), Tensor([16384, 8192],"bfloat16"), None, False, None, )
W0805 21:54:44.897102 110326 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([1, 6202, 16384],"bfloat16"), Tensor([16384, 8192],"bfloat16"), None, False, None, ) 	 235831296 	 1922 	 13.345032930374146 	 13.365341663360596 	 7.104452610015869 	 7.105499505996704 	 None 	 None 	 None 	 None 	 combined
2025-08-05 21:55:25.843768 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 8192, 12404],"bfloat16"), Tensor([12404, 12800],"bfloat16"), Tensor([12800],"bfloat16"), )
W0805 21:56:02.401726 110338 backward.cc:462] While running Node (FusedGemmEpilogueGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([1, 8192, 12404],"bfloat16"), Tensor([12404, 12800],"bfloat16"), Tensor([12800],"bfloat16"), ) 	 260397568 	 1922 	 31.152748823165894 	 31.73653531074524 	 16.581200122833252 	 16.892502307891846 	 None 	 None 	 None 	 None 	 combined
2025-08-05 21:57:40.247056 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 8192, 7168],"bfloat16"), Tensor([7168, 14176],"bfloat16"), Tensor([14176],"bfloat16"), )
W0805 21:57:59.744853 110373 backward.cc:462] While running Node (FusedGemmEpilogueGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([1, 8192, 7168],"bfloat16"), Tensor([7168, 14176],"bfloat16"), Tensor([14176],"bfloat16"), ) 	 160348000 	 1922 	 13.604594230651855 	 13.600664615631104 	 7.2409467697143555 	 7.240321397781372 	 None 	 None 	 None 	 None 	 combined
2025-08-05 21:58:43.523351 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([2, 4096, 16384],"bfloat16"), Tensor([16384, 8192],"bfloat16"), None, False, None, )
W0805 21:59:06.089985 110387 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([2, 4096, 16384],"bfloat16"), Tensor([16384, 8192],"bfloat16"), None, False, None, ) 	 268435456 	 1922 	 17.076817274093628 	 17.1087384223938 	 9.091473817825317 	 9.109066486358643 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:00:00.706931 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([2, 8192, 7168],"bfloat16"), Tensor([7168, 12800],"bfloat16"), Tensor([12800],"bfloat16"), )
W0805 22:00:30.921528 110407 backward.cc:462] While running Node (FusedGemmEpilogueGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.fused_linear 	 paddle.incubate.nn.functional.fused_linear(Tensor([2, 8192, 7168],"bfloat16"), Tensor([7168, 12800],"bfloat16"), Tensor([12800],"bfloat16"), ) 	 209203712 	 1922 	 24.035903692245483 	 23.94840908050537 	 12.78105616569519 	 12.738572120666504 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:01:45.061468 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 4096, 8192],"bfloat16"), Tensor([8192, 100352],"bfloat16"), None, transpose_weight=False, )
W0805 22:06:05.623209 110436 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4f910e9300>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 22:12:23.574980 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1016065, 50],"float32"), Tensor([40, 50],"float32"), None, False, True, )
W0805 22:12:24.519059 110922 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1016065, 50],"float32"), Tensor([40, 50],"float32"), None, False, True, ) 	 50805250 	 33751 	 15.79468035697937 	 15.753762006759644 	 0.4789607524871826 	 0.4766809940338135 	 30.449854850769043 	 30.52408480644226 	 0.307819128036499 	 0.3092515468597412 	 
2025-08-05 22:13:58.374755 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1016065, 50],"float32"), Tensor([50, 40],"float32"), None, False, False, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1016065, 50],"float32"), Tensor([50, 40],"float32"), None, False, False, ) 	 50805250 	 33751 	 15.67710566520691 	 15.725983619689941 	 0.4775574207305908 	 0.47612690925598145 	 30.75618886947632 	 30.866648197174072 	 0.3110504150390625 	 0.3113076686859131 	 
2025-08-05 22:15:33.496487 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1270081, 30],"float32"), Tensor([40, 1270081],"float32"), None, True, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1270081, 30],"float32"), Tensor([40, 1270081],"float32"), None, True, True, ) 	 88905670 	 33751 	 13.373578786849976 	 13.295507431030273 	 0.20248937606811523 	 0.20124101638793945 	 23.74415898323059 	 23.77091932296753 	 0.17982006072998047 	 0.1809830665588379 	 
2025-08-05 22:16:49.354065 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1693441, 30],"float32"), Tensor([40, 1693441],"float32"), None, True, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1693441, 30],"float32"), Tensor([40, 1693441],"float32"), None, True, True, ) 	 118540870 	 33751 	 17.53459095954895 	 17.452382564544678 	 0.2654609680175781 	 0.26424741744995117 	 31.563023805618286 	 31.40881896018982 	 0.23818111419677734 	 0.23732328414916992 	 
2025-08-05 22:18:31.546510 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1270081],"float32"), Tensor([1270081, 40],"float32"), None, False, False, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1270081],"float32"), Tensor([1270081, 40],"float32"), None, False, False, ) 	 88905670 	 33751 	 14.149860858917236 	 13.980445861816406 	 0.21423959732055664 	 0.2117016315460205 	 24.799998998641968 	 24.741740942001343 	 0.18805861473083496 	 0.1870403289794922 	 
2025-08-05 22:19:51.033389 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1270081],"float32"), Tensor([40, 1270081],"float32"), None, False, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1270081],"float32"), Tensor([40, 1270081],"float32"), None, False, True, ) 	 88905670 	 33751 	 13.641008853912354 	 13.56747579574585 	 0.20654082298278809 	 0.20544934272766113 	 23.318015575408936 	 23.356067419052124 	 0.17654967308044434 	 0.1771869659423828 	 
2025-08-05 22:21:09.054093 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1693441],"float32"), Tensor([1693441, 40],"float32"), None, False, False, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1693441],"float32"), Tensor([1693441, 40],"float32"), None, False, False, ) 	 118540870 	 33751 	 18.28758192062378 	 18.207051515579224 	 0.2768847942352295 	 0.27591490745544434 	 32.42721652984619 	 32.50564503669739 	 0.24548077583312988 	 0.24706029891967773 	 
2025-08-05 22:22:55.298406 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1693441],"float32"), Tensor([40, 1693441],"float32"), None, False, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1693441],"float32"), Tensor([40, 1693441],"float32"), None, False, True, ) 	 118540870 	 33751 	 17.907882928848267 	 17.819757223129272 	 0.2711970806121826 	 0.26979827880859375 	 31.339678287506104 	 30.893198013305664 	 0.23751044273376465 	 0.2350764274597168 	 
2025-08-05 22:24:38.499231 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([1016065, 50],"float32"), None, False, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([1016065, 50],"float32"), None, False, True, ) 	 50804750 	 33751 	 10.620476722717285 	 10.45889687538147 	 0.3153655529022217 	 0.31469106674194336 	 21.37078070640564 	 21.332131147384644 	 0.21558713912963867 	 0.2153618335723877 	 
2025-08-05 22:25:47.073817 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([50, 1016065],"float32"), None, False, False, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([50, 1016065],"float32"), None, False, False, ) 	 50804750 	 33751 	 10.084266662597656 	 10.134384632110596 	 0.30597782135009766 	 0.30821847915649414 	 22.002820014953613 	 22.13146471977234 	 0.22136497497558594 	 0.22489047050476074 	 
2025-08-05 22:26:52.800807 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 1016065],"float32"), Tensor([40, 50],"float32"), None, True, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 1016065],"float32"), Tensor([40, 50],"float32"), None, True, True, ) 	 50805250 	 33751 	 15.606396913528442 	 15.548125505447388 	 0.4736669063568115 	 0.4696688652038574 	 29.957599639892578 	 29.82684588432312 	 0.3023366928100586 	 0.3007655143737793 	 
2025-08-05 22:28:27.071323 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([1016065, 50],"float32"), None, True, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([1016065, 50],"float32"), None, True, True, ) 	 50804750 	 33751 	 10.453495502471924 	 10.461872339248657 	 0.3150293827056885 	 0.315387487411499 	 22.340431690216064 	 22.32257628440857 	 0.22520709037780762 	 0.22566485404968262 	 
2025-08-05 22:29:35.277769 test begin: paddle.incubate.nn.functional.swiglu(Tensor([14176, 7168],"bfloat16"), )
W0805 22:29:49.763476 111359 backward.cc:462] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.incubate.nn.functional.swiglu 	 paddle.incubate.nn.functional.swiglu(Tensor([14176, 7168],"bfloat16"), ) 	 101613568 	 43514 	 9.946848630905151 	 22.742538928985596 	 0.23340988159179688 	 0.26685547828674316 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:31:17.942646 test begin: paddle.incubate.segment_max(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_max" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_max" instead.
    Reason: paddle.incubate.segment_max will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_max" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_max" instead.
    Reason: paddle.incubate.segment_max will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.incubate.segment_max 	 paddle.incubate.segment_max(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), ) 	 50803206 	 9957 	 6.710397720336914 	 8.463038206100464 	 0.0006396770477294922 	 0.0002105236053466797 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 22:31:43.595531 test begin: paddle.incubate.segment_mean(Tensor([301, 16934],"float32"), Tensor([301],"int32"), )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_mean" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_mean" instead.
    Reason: paddle.incubate.segment_mean will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_mean" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_mean" instead.
    Reason: paddle.incubate.segment_mean will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.incubate.segment_mean 	 paddle.incubate.segment_mean(Tensor([301, 16934],"float32"), Tensor([301],"int32"), ) 	 5097435 	 151362 	 9.065853118896484 	 40.2451274394989 	 6.318092346191406e-05 	 0.00022721290588378906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 22:41:14.936603 test begin: paddle.incubate.segment_min(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_min" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_min" instead.
    Reason: paddle.incubate.segment_min will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_min" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_min" instead.
    Reason: paddle.incubate.segment_min will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.incubate.segment_min 	 paddle.incubate.segment_min(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), ) 	 50803206 	 8591 	 8.883878469467163 	 9.154268264770508 	 0.0010132789611816406 	 0.0002148151397705078 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 22:41:43.580721 test begin: paddle.incubate.segment_sum(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_sum" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_sum" instead.
    Reason: paddle.incubate.segment_sum will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_sum" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_sum" instead.
    Reason: paddle.incubate.segment_sum will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.incubate.segment_sum 	 paddle.incubate.segment_sum(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), ) 	 50803206 	 16032 	 11.138603210449219 	 8.883557081222534 	 0.0005843639373779297 	 0.28308796882629395 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 22:42:15.615397 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1013, 1, 224, 224],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1013, 1, 224, 224],"float32"), ) 	 50828288 	 38131 	 9.973834991455078 	 23.25309157371521 	 0.2673358917236328 	 0.15598082542419434 	 12.518449544906616 	 34.08428072929382 	 0.33542776107788086 	 0.45679402351379395 	 combined
2025-08-05 22:43:42.198374 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([145, 7, 224, 224],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([145, 7, 224, 224],"float32"), ) 	 50928640 	 38131 	 9.884579420089722 	 23.279163360595703 	 0.26482629776000977 	 0.15626931190490723 	 12.580926179885864 	 34.150765895843506 	 0.33715057373046875 	 0.45763659477233887 	 combined
2025-08-05 22:45:04.017821 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([3, 338, 224, 224],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([3, 338, 224, 224],"float32"), ) 	 50878464 	 38131 	 9.930508613586426 	 23.257532358169556 	 0.2661449909210205 	 0.15612173080444336 	 12.635732650756836 	 34.115761518478394 	 0.33859682083129883 	 0.4571709632873535 	 combined
2025-08-05 22:46:25.720293 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([4511, 11, 32, 32],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([4511, 11, 32, 32],"float32"), ) 	 50811904 	 38131 	 28.51061463356018 	 25.78178071975708 	 0.7641458511352539 	 0.17304253578186035 	 17.11655354499817 	 34.150078535079956 	 0.4587538242340088 	 0.45764684677124023 	 combined
2025-08-05 22:48:13.196374 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([5, 203, 224, 224],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([5, 203, 224, 224],"float32"), ) 	 50928640 	 38131 	 9.937339067459106 	 23.289847373962402 	 0.2663140296936035 	 0.15622591972351074 	 12.652695894241333 	 34.149468421936035 	 0.33907103538513184 	 0.45757579803466797 	 combined
2025-08-05 22:49:40.068538 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([7, 7088, 32, 32],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([7, 7088, 32, 32],"float32"), ) 	 50806784 	 38131 	 29.59355115890503 	 25.775267124176025 	 0.7632100582122803 	 0.17297768592834473 	 17.11405086517334 	 34.14459753036499 	 0.458693265914917 	 0.4575769901275635 	 combined
2025-08-05 22:51:29.596307 test begin: paddle.index_add(Tensor([100, 100, 25402],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 25402],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 100, 25402],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 25402],"float32"), ) 	 304824020 	 32129 	 63.956576347351074 	 110.29745697975159 	 0.6772933006286621 	 0.00020623207092285156 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 22:55:46.433596 test begin: paddle.index_add(Tensor([100, 100, 25],"float32"), Tensor([5081],"int32"), 2, Tensor([100, 100, 5081],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f29b8a5a440>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 23:05:51.442713 test begin: paddle.index_add(Tensor([100, 100, 5081],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 5081],"float32"), )
W0805 23:05:52.559314 112755 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 100, 5081],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 5081],"float32"), ) 	 60972020 	 32129 	 13.250818729400635 	 58.63814163208008 	 0.14033246040344238 	 0.00022292137145996094 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:07:22.839929 test begin: paddle.index_add(Tensor([100, 100, 5081],"float32"), Tensor([20],"int32"), 2, Tensor([100, 100, 20],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 100, 5081],"float32"), Tensor([20],"int32"), 2, Tensor([100, 100, 20],"float32"), ) 	 51010020 	 32129 	 10.964191913604736 	 57.21756625175476 	 0.11606240272521973 	 0.0002052783966064453 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:08:44.223530 test begin: paddle.index_add(Tensor([100, 101607, 5],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 5],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 101607, 5],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 5],"float32"), ) 	 50813520 	 32129 	 9.984515190124512 	 56.92026352882385 	 0.10566997528076172 	 0.00020265579223632812 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:10:03.076926 test begin: paddle.index_add(Tensor([100, 2540161],"float32"), Tensor([20],"int32"), 0, Tensor([20, 2540161],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 2540161],"float32"), Tensor([20],"int32"), 0, Tensor([20, 2540161],"float32"), ) 	 304819340 	 32129 	 64.13783550262451 	 105.73827195167542 	 0.6791398525238037 	 0.00020194053649902344 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:14:13.510544 test begin: paddle.index_add(Tensor([100, 508033],"float32"), Tensor([20],"int32"), 0, Tensor([20, 508033],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 508033],"float32"), Tensor([20],"int32"), 0, Tensor([20, 508033],"float32"), ) 	 60963980 	 32129 	 12.928834915161133 	 74.0525598526001 	 0.13689756393432617 	 0.00020837783813476562 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:15:57.640550 test begin: paddle.index_add(Tensor([10160641, 5],"float32"), Tensor([20],"int32"), 0, Tensor([20, 5],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([10160641, 5],"float32"), Tensor([20],"int32"), 0, Tensor([20, 5],"float32"), ) 	 50803325 	 32129 	 10.216392517089844 	 55.64762496948242 	 0.10814547538757324 	 0.0001933574676513672 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:17:15.501846 test begin: paddle.index_fill(Tensor([10, 1016065, 10],"float16"), Tensor([5],"int64"), 1, 0.5, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 1016065, 10],"float16"), Tensor([5],"int64"), 1, 0.5, ) 	 101606505 	 29793 	 24.427868366241455 	 9.432700157165527 	 0.0006692409515380859 	 0.10762619972229004 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:18:18.008660 test begin: paddle.index_fill(Tensor([10, 15, 169345],"int64"), Tensor([5],"int32"), 1, -1, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 15, 169345],"int64"), Tensor([5],"int32"), 1, -1, ) 	 25401755 	 29793 	 29.79440140724182 	 10.972687721252441 	 0.06366682052612305 	 0.12528252601623535 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 23:19:29.647902 test begin: paddle.index_fill(Tensor([10, 15, 338689],"bool"), Tensor([5],"int32"), 1, True, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 15, 338689],"bool"), Tensor([5],"int32"), 1, True, ) 	 50803355 	 29793 	 27.41140341758728 	 5.071850776672363 	 0.0007569789886474609 	 0.05797266960144043 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 23:20:29.581105 test begin: paddle.index_fill(Tensor([10, 15, 677377],"float16"), Tensor([5],"int64"), 1, 0.5, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 15, 677377],"float16"), Tensor([5],"int64"), 1, 0.5, ) 	 101606555 	 29793 	 60.82512378692627 	 14.421337366104126 	 0.0018973350524902344 	 0.16475176811218262 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:22:48.106226 test begin: paddle.index_fill(Tensor([10, 254017, 10],"int64"), Tensor([5],"int32"), 1, -1, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 254017, 10],"int64"), Tensor([5],"int32"), 1, -1, ) 	 25401705 	 29793 	 19.649908304214478 	 9.433455228805542 	 0.04480862617492676 	 0.10767364501953125 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 23:23:38.022737 test begin: paddle.index_fill(Tensor([10, 508033, 10],"bool"), Tensor([5],"int32"), 1, True, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 508033, 10],"bool"), Tensor([5],"int32"), 1, True, ) 	 50803305 	 29793 	 10.177404165267944 	 2.6097605228424072 	 0.00014328956604003906 	 0.029801130294799805 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 23:24:01.063875 test begin: paddle.index_fill(Tensor([169345, 15, 10],"int64"), Tensor([5],"int32"), 1, -1, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([169345, 15, 10],"int64"), Tensor([5],"int32"), 1, -1, ) 	 25401755 	 29793 	 32.6546790599823 	 14.922312498092651 	 0.0697331428527832 	 0.17047858238220215 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 23:25:22.350467 test begin: paddle.index_fill(Tensor([338689, 15, 10],"bool"), Tensor([5],"int32"), 1, True, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([338689, 15, 10],"bool"), Tensor([5],"int32"), 1, True, ) 	 50803355 	 29793 	 32.72819209098816 	 4.915172815322876 	 0.0008962154388427734 	 0.056186676025390625 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 23:26:32.669236 test begin: paddle.index_fill(Tensor([677377, 15, 10],"float16"), Tensor([5],"int64"), 1, 0.5, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([677377, 15, 10],"float16"), Tensor([5],"int64"), 1, 0.5, ) 	 101606555 	 29793 	 73.18821859359741 	 15.230468988418579 	 0.0023055076599121094 	 0.17403388023376465 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:29:16.652042 test begin: paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, ) 	 25628144 	 28767 	 10.547969579696655 	 14.27679705619812 	 0.022002220153808594 	 0.012157678604125977 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:29:53.842760 test begin: paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), False, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), False, ) 	 25628144 	 28767 	 10.270387411117554 	 9.454763412475586 	 0.026055097579956055 	 0.08376932144165039 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:30:24.927821 test begin: paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, ) 	 25628144 	 28767 	 10.271225214004517 	 12.643086194992065 	 0.026048898696899414 	 0.013084650039672852 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:30:59.010514 test begin: paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, ) 	 25541904 	 28767 	 10.314580202102661 	 12.897761344909668 	 0.021506309509277344 	 0.012093067169189453 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:31:33.718162 test begin: paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), False, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), False, ) 	 25541904 	 28767 	 10.03098201751709 	 9.420633792877197 	 0.02544379234313965 	 0.0834956169128418 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:32:04.639635 test begin: paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, ) 	 25541904 	 28767 	 10.032142877578735 	 12.61585283279419 	 0.025452852249145508 	 0.013062000274658203 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:32:41.652404 test begin: paddle.index_put(Tensor([193, 42, 56, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([193, 42, 56, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, ) 	 25435280 	 28767 	 10.260708332061768 	 12.858949422836304 	 0.02139568328857422 	 0.012092828750610352 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:33:16.239924 test begin: paddle.index_put(Tensor([193, 42, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([193, 42, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, ) 	 25435280 	 28767 	 9.987382173538208 	 12.810365915298462 	 0.025332212448120117 	 0.012901067733764648 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:33:51.628303 test begin: paddle.index_sample(Tensor([1865664, 100],"float32"), Tensor([1865664, 14],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([1865664, 100],"float32"), Tensor([1865664, 14],"int64"), ) 	 212685696 	 83783 	 62.71072554588318 	 95.31808757781982 	 0.764984130859375 	 0.3879077434539795 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:38:44.168787 test begin: paddle.index_sample(Tensor([1865664, 28],"float32"), Tensor([1865664, 14],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([1865664, 28],"float32"), Tensor([1865664, 14],"int64"), ) 	 78357888 	 83783 	 43.882336139678955 	 67.55117440223694 	 0.535254716873169 	 0.27491164207458496 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:41:30.326950 test begin: paddle.index_sample(Tensor([1865664, 28],"float32"), Tensor([1865664, 1],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([1865664, 28],"float32"), Tensor([1865664, 1],"int64"), ) 	 54104256 	 83783 	 34.70905423164368 	 12.8922278881073 	 0.42340755462646484 	 0.0784916877746582 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:42:59.116039 test begin: paddle.index_sample(Tensor([25401601, 100],"float32"), Tensor([25401601, 1],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6010253e50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 23:53:09.166838 test begin: paddle.index_sample(Tensor([25401601, 20],"float32"), Tensor([25401601, 1],"int64"), )
W0805 23:53:17.212198 114183 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0536bb2d70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 00:03:13.900115 test begin: paddle.index_sample(Tensor([2540161, 20],"float32"), Tensor([2540161, 1],"int64"), )
W0806 00:03:14.928779 114601 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([2540161, 20],"float32"), Tensor([2540161, 1],"int64"), ) 	 53343381 	 83783 	 45.015685081481934 	 16.615113019943237 	 0.5491504669189453 	 0.1013326644897461 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:05:04.967822 test begin: paddle.index_sample(Tensor([508033, 100],"float32"), Tensor([508033, 1],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([508033, 100],"float32"), Tensor([508033, 1],"int64"), ) 	 51311333 	 83783 	 9.980736255645752 	 4.38686728477478 	 0.12172579765319824 	 0.02668619155883789 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:05:42.992210 test begin: paddle.index_sample(Tensor([5135296, 10],"float32"), Tensor([5135296, 1],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([5135296, 10],"float32"), Tensor([5135296, 1],"int64"), ) 	 56488256 	 83783 	 80.18976283073425 	 26.55929446220398 	 0.978163480758667 	 0.16202211380004883 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:08:43.580238 test begin: paddle.index_sample(Tensor([5135296, 10],"float32"), Tensor([5135296, 5],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([5135296, 10],"float32"), Tensor([5135296, 5],"int64"), ) 	 77029440 	 83783 	 89.64640831947327 	 72.13552522659302 	 1.0935020446777344 	 0.29352831840515137 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:12:44.086395 test begin: paddle.index_sample(Tensor([5135296, 20],"float32"), Tensor([5135296, 5],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([5135296, 20],"float32"), Tensor([5135296, 5],"int64"), ) 	 128382400 	 83783 	 93.70426559448242 	 82.89129781723022 	 1.1430308818817139 	 0.33725953102111816 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:17:18.299375 test begin: paddle.index_sample(Tensor([932832, 100],"float32"), Tensor([932832, 28],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([932832, 100],"float32"), Tensor([932832, 28],"int64"), ) 	 119402496 	 83783 	 42.41814637184143 	 73.63910627365112 	 0.5174398422241211 	 0.2997157573699951 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:20:38.559636 test begin: paddle.index_sample(Tensor([932832, 55],"float32"), Tensor([932832, 1],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([932832, 55],"float32"), Tensor([932832, 1],"int64"), ) 	 52238592 	 83783 	 18.231707096099854 	 7.321184158325195 	 0.21694231033325195 	 0.044667720794677734 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:21:33.721553 test begin: paddle.index_sample(Tensor([932832, 55],"float32"), Tensor([932832, 28],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([932832, 55],"float32"), Tensor([932832, 28],"int64"), ) 	 77425056 	 83783 	 32.597923278808594 	 65.11638927459717 	 0.3975660800933838 	 0.2649827003479004 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:24:06.100866 test begin: paddle.index_select(Tensor([16, 11109, 286],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([16, 11109, 286],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50834864 	 49358 	 9.932467937469482 	 10.911649227142334 	 0.205657958984375 	 0.2256779670715332 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:24:54.537828 test begin: paddle.index_select(Tensor([16, 12096, 263],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([16, 12096, 263],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50900048 	 49358 	 10.482932090759277 	 11.357154130935669 	 0.21706104278564453 	 0.2351372241973877 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:25:45.706306 test begin: paddle.index_select(Tensor([16, 39201, 81],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([16, 39201, 81],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50804576 	 49358 	 31.147172451019287 	 27.519788026809692 	 0.6447789669036865 	 0.5695760250091553 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:27:25.000758 test begin: paddle.index_select(Tensor([205, 3060, 81],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([205, 3060, 81],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50811380 	 49358 	 31.143852949142456 	 27.6505343914032 	 0.6448123455047607 	 0.5723276138305664 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:29:02.919258 test begin: paddle.index_select(Tensor([52, 12096, 81],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([52, 12096, 81],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50948432 	 49358 	 31.235759258270264 	 27.653334617614746 	 0.6467859745025635 	 0.572556734085083 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:30:42.429056 test begin: paddle.index_select(Tensor([57, 11109, 81],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([57, 11109, 81],"float32"), Tensor([80],"int64"), axis=-1, ) 	 51290333 	 49358 	 31.479871034622192 	 27.789833068847656 	 0.6518344879150391 	 0.5754668712615967 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:32:20.962725 test begin: paddle.index_select(Tensor([64, 3060, 260],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([64, 3060, 260],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50918480 	 49358 	 10.596363306045532 	 11.408288478851318 	 0.21939301490783691 	 0.23624396324157715 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:33:10.672371 test begin: paddle.index_select(Tensor([64, 9801, 81],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([64, 9801, 81],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50808464 	 49358 	 31.140816926956177 	 27.467531442642212 	 0.6447923183441162 	 0.5686995983123779 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:34:47.862302 test begin: paddle.inner(Tensor([20, 1270081],"float64"), Tensor([1270081],"float64"), )
[Prof] paddle.inner 	 paddle.inner(Tensor([20, 1270081],"float64"), Tensor([1270081],"float64"), ) 	 26671701 	 62291 	 10.276700496673584 	 10.161295175552368 	 0.0843191146850586 	 0.08338689804077148 	 22.20157742500305 	 22.67883563041687 	 0.12136578559875488 	 0.12395954132080078 	 
2025-08-06 00:35:54.229616 test begin: paddle.inner(Tensor([20, 25401601],"float64"), Tensor([25401601],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6e859f7d60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 00:45:59.382199 test begin: paddle.inner(Tensor([508033, 50],"float64"), Tensor([50],"float64"), )
W0806 00:46:00.149739 115894 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.inner 	 paddle.inner(Tensor([508033, 50],"float64"), Tensor([50],"float64"), ) 	 25401700 	 62291 	 9.813791513442993 	 9.899740934371948 	 0.1609969139099121 	 0.16251015663146973 	 23.603331327438354 	 23.586701154708862 	 0.12893295288085938 	 0.12890028953552246 	 
2025-08-06 00:47:07.704613 test begin: paddle.is_complex(Tensor([1003520, 507],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([1003520, 507],"float32"), ) 	 508784640 	 2834372 	 10.650692701339722 	 4.811338663101196 	 9.846687316894531e-05 	 0.00026488304138183594 	 None 	 None 	 None 	 None 	 
2025-08-06 00:47:31.725776 test begin: paddle.is_complex(Tensor([5070, 100352],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([5070, 100352],"float32"), ) 	 508784640 	 2834372 	 13.7353515625 	 6.272216081619263 	 0.0001323223114013672 	 0.000316619873046875 	 None 	 None 	 None 	 None 	 
2025-08-06 00:48:00.546903 test begin: paddle.is_complex(Tensor([62020, 8192],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([62020, 8192],"float32"), ) 	 508067840 	 2834372 	 13.506042957305908 	 6.318816661834717 	 0.00015807151794433594 	 0.0002970695495605469 	 None 	 None 	 None 	 None 	 
2025-08-06 00:48:30.110344 test begin: paddle.is_complex(Tensor([81920, 6202],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([81920, 6202],"float32"), ) 	 508067840 	 2834372 	 13.939198732376099 	 6.235580921173096 	 0.0003695487976074219 	 0.0003190040588378906 	 None 	 None 	 None 	 None 	 
2025-08-06 00:48:58.665636 test begin: paddle.is_complex(Tensor([8860, 57344],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([8860, 57344],"float32"), ) 	 508067840 	 2834372 	 10.589080572128296 	 4.864415168762207 	 0.00017952919006347656 	 0.0005548000335693359 	 None 	 None 	 None 	 None 	 
2025-08-06 00:49:22.620043 test begin: paddle.is_empty(Tensor([101606410, 5],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(Tensor([101606410, 5],"float32"), ) 	 508032050 	 2953527 	 10.170132875442505 	 4.504967212677002 	 0.00016260147094726562 	 0.0003077983856201172 	 None 	 None 	 None 	 None 	 combined
2025-08-06 00:49:46.201077 test begin: paddle.is_empty(Tensor([169344010, 3],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(Tensor([169344010, 3],"float32"), ) 	 508032030 	 2953527 	 10.190595149993896 	 4.490251302719116 	 0.00010609626770019531 	 0.0003123283386230469 	 None 	 None 	 None 	 None 	 combined
2025-08-06 00:50:09.271251 test begin: paddle.is_empty(Tensor([20, 25401601],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(Tensor([20, 25401601],"float32"), ) 	 508032020 	 2953527 	 10.108433961868286 	 5.598282814025879 	 0.00011229515075683594 	 0.0003609657287597656 	 None 	 None 	 None 	 None 	 combined
2025-08-06 00:50:33.241981 test begin: paddle.is_empty(Tensor([30, 16934401],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(Tensor([30, 16934401],"float32"), ) 	 508032030 	 2953527 	 10.171542167663574 	 4.567356109619141 	 0.00010704994201660156 	 0.0002918243408203125 	 None 	 None 	 None 	 None 	 combined
2025-08-06 00:50:57.114082 test begin: paddle.is_empty(x=Tensor([40, 32, 396901],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(x=Tensor([40, 32, 396901],"float32"), ) 	 508033280 	 2953527 	 16.218363523483276 	 4.600428104400635 	 0.00014662742614746094 	 0.0001323223114013672 	 None 	 None 	 None 	 None 	 combined
2025-08-06 00:51:26.605336 test begin: paddle.is_empty(x=Tensor([40, 396901, 32],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(x=Tensor([40, 396901, 32],"float32"), ) 	 508033280 	 2953527 	 10.580952882766724 	 4.459186315536499 	 0.00011157989501953125 	 0.00033783912658691406 	 None 	 None 	 None 	 None 	 combined
2025-08-06 00:51:49.964457 test begin: paddle.is_empty(x=Tensor([496130, 32, 32],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(x=Tensor([496130, 32, 32],"float32"), ) 	 508037120 	 2953527 	 12.016420841217041 	 4.96012806892395 	 0.0001354217529296875 	 0.0002646446228027344 	 None 	 None 	 None 	 None 	 combined
2025-08-06 00:52:16.113129 test begin: paddle.isclose(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), rtol=1e-05, atol=1e-08, )
[Prof] paddle.isclose 	 paddle.isclose(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), rtol=1e-05, atol=1e-08, ) 	 50803220 	 27502 	 9.913697481155396 	 84.75050330162048 	 0.3683583736419678 	 0.24175238609313965 	 None 	 None 	 None 	 None 	 
2025-08-06 00:53:51.998410 test begin: paddle.isclose(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), rtol=1e-05, atol=1e-08, )
[Prof] paddle.isclose 	 paddle.isclose(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), rtol=1e-05, atol=1e-08, ) 	 50803220 	 27502 	 9.913695812225342 	 84.74898958206177 	 0.36832714080810547 	 0.24174880981445312 	 None 	 None 	 None 	 None 	 
2025-08-06 00:55:29.448269 test begin: paddle.isclose(x=Tensor([1270081, 4, 5],"float64"), y=Tensor([1270081, 4, 5],"float64"), )
[Prof] paddle.isclose 	 paddle.isclose(x=Tensor([1270081, 4, 5],"float64"), y=Tensor([1270081, 4, 5],"float64"), ) 	 50803240 	 27502 	 9.908819675445557 	 84.75086855888367 	 0.3680896759033203 	 0.24181842803955078 	 None 	 None 	 None 	 None 	 
2025-08-06 00:57:06.212876 test begin: paddle.isclose(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), )
[Prof] paddle.isclose 	 paddle.isclose(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), ) 	 50803202 	 27502 	 9.913711786270142 	 84.75052785873413 	 0.36834263801574707 	 0.2418079376220703 	 None 	 None 	 None 	 None 	 
2025-08-06 00:58:41.960650 test begin: paddle.isclose(x=Tensor([3, 1693441, 5],"float64"), y=Tensor([3, 1693441, 5],"float64"), )
[Prof] paddle.isclose 	 paddle.isclose(x=Tensor([3, 1693441, 5],"float64"), y=Tensor([3, 1693441, 5],"float64"), ) 	 50803230 	 27502 	 9.915502071380615 	 84.75166511535645 	 0.36840176582336426 	 0.24178433418273926 	 None 	 None 	 None 	 None 	 
2025-08-06 01:00:17.785784 test begin: paddle.isclose(x=Tensor([3, 4, 2116801],"float64"), y=Tensor([3, 4, 2116801],"float64"), )
[Prof] paddle.isclose 	 paddle.isclose(x=Tensor([3, 4, 2116801],"float64"), y=Tensor([3, 4, 2116801],"float64"), ) 	 50803224 	 27502 	 9.908653020858765 	 84.75000786781311 	 0.3681166172027588 	 0.24175596237182617 	 None 	 None 	 None 	 None 	 
2025-08-06 01:01:53.535599 test begin: paddle.isfinite(Tensor([1738, 94, 311],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([1738, 94, 311],"float32"), ) 	 50808692 	 42769 	 9.990402460098267 	 33.71170616149902 	 0.23876643180847168 	 0.2012927532196045 	 None 	 None 	 None 	 None 	 
2025-08-06 01:02:38.719943 test begin: paddle.isfinite(Tensor([28462, 17, 5, 6, 7],"float16"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([28462, 17, 5, 6, 7],"float16"), ) 	 101609340 	 42769 	 16.82210659980774 	 41.560895681381226 	 0.40198373794555664 	 0.2482609748840332 	 None 	 None 	 None 	 None 	 
2025-08-06 01:03:40.192147 test begin: paddle.isfinite(Tensor([4, 280, 376, 25, 5],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 280, 376, 25, 5],"float32"), ) 	 52640000 	 42769 	 10.340776205062866 	 34.809306383132935 	 0.24709653854370117 	 0.2078838348388672 	 None 	 None 	 None 	 None 	 
2025-08-06 01:04:26.299390 test begin: paddle.isfinite(Tensor([4, 280, 376, 41, 3],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 280, 376, 41, 3],"float32"), ) 	 51797760 	 42769 	 10.173098802566528 	 34.33355116844177 	 0.24309921264648438 	 0.20499157905578613 	 None 	 None 	 None 	 None 	 
2025-08-06 01:05:12.598366 test begin: paddle.isfinite(Tensor([4, 280, 605, 25, 3],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 280, 605, 25, 3],"float32"), ) 	 50820000 	 42769 	 10.007304906845093 	 33.71675968170166 	 0.2391366958618164 	 0.20128917694091797 	 None 	 None 	 None 	 None 	 
2025-08-06 01:05:59.202491 test begin: paddle.isfinite(Tensor([4, 40839, 311],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 40839, 311],"float32"), ) 	 50803716 	 42769 	 9.99315595626831 	 33.63736438751221 	 0.23882842063903809 	 0.20085835456848145 	 None 	 None 	 None 	 None 	 
2025-08-06 01:06:43.821548 test begin: paddle.isfinite(Tensor([4, 451, 376, 25, 3],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 451, 376, 25, 3],"float32"), ) 	 50872800 	 42769 	 10.023720264434814 	 33.69335722923279 	 0.23951172828674316 	 0.2011568546295166 	 None 	 None 	 None 	 None 	 
2025-08-06 01:07:28.386325 test begin: paddle.isfinite(Tensor([4, 94, 135115],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 94, 135115],"float32"), ) 	 50803240 	 42769 	 10.007592678070068 	 33.66923260688782 	 0.2391362190246582 	 0.20104742050170898 	 None 	 None 	 None 	 None 	 
2025-08-06 01:08:16.019080 test begin: paddle.isfinite(Tensor([7, 280, 376, 25, 3],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([7, 280, 376, 25, 3],"float32"), ) 	 55272000 	 42769 	 10.864548683166504 	 36.517850160598755 	 0.25960469245910645 	 0.21802687644958496 	 None 	 None 	 None 	 None 	 
2025-08-06 01:09:04.401424 test begin: paddle.isfinite(Tensor([8, 17, 17789, 6, 7],"float16"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([8, 17, 17789, 6, 7],"float16"), ) 	 101610768 	 42769 	 16.827895164489746 	 41.48826575279236 	 0.40218043327331543 	 0.24782085418701172 	 None 	 None 	 None 	 None 	 
2025-08-06 01:10:05.222058 test begin: paddle.isfinite(Tensor([8, 17, 5, 21346, 7],"float16"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([8, 17, 5, 21346, 7],"float16"), ) 	 101606960 	 42769 	 16.825918912887573 	 41.52728986740112 	 0.40205907821655273 	 0.2479691505432129 	 None 	 None 	 None 	 None 	 
2025-08-06 01:11:08.693421 test begin: paddle.isfinite(Tensor([8, 17, 5, 6, 24904],"float16"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([8, 17, 5, 6, 24904],"float16"), ) 	 101608320 	 42769 	 16.820719480514526 	 41.56056451797485 	 0.40198588371276855 	 0.2482891082763672 	 None 	 None 	 None 	 None 	 
2025-08-06 01:12:09.145879 test begin: paddle.isfinite(Tensor([8, 60481, 5, 6, 7],"float16"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([8, 60481, 5, 6, 7],"float16"), ) 	 101608080 	 42769 	 16.84136462211609 	 41.53522968292236 	 0.4024186134338379 	 0.24807429313659668 	 None 	 None 	 None 	 None 	 
2025-08-06 01:13:09.547500 test begin: paddle.isin(Tensor([396901, 64],"float64"), Tensor([4, 256],"float64"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([396901, 64],"float64"), Tensor([4, 256],"float64"), False, False, ) 	 25402688 	 3671 	 9.985973119735718 	 78.10107898712158 	 0.0024156570434570312 	 0.0008459091186523438 	 None 	 None 	 None 	 None 	 
2025-08-06 01:14:40.661565 test begin: paddle.isin(Tensor([793801, 64],"float32"), Tensor([4, 256],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([793801, 64],"float32"), Tensor([4, 256],"float32"), False, False, ) 	 50804288 	 3671 	 16.344390153884888 	 92.66449689865112 	 0.004174470901489258 	 0.0020313262939453125 	 None 	 None 	 None 	 None 	 
2025-08-06 01:16:30.803381 test begin: paddle.isin(Tensor([793801, 64],"float32"), Tensor([4, 256],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([793801, 64],"float32"), Tensor([4, 256],"float32"), False, True, ) 	 50804288 	 3671 	 16.64034867286682 	 92.65167880058289 	 0.0042684078216552734 	 0.0020241737365722656 	 None 	 None 	 None 	 None 	 
2025-08-06 01:18:21.804481 test begin: paddle.isin(Tensor([793801, 64],"float32"), Tensor([793801, 256],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([793801, 64],"float32"), Tensor([793801, 256],"float32"), False, False, ) 	 254016320 	 3671 	 352.09736013412476 	 165.98038482666016 	 0.05260133743286133 	 0.002593994140625 	 None 	 None 	 None 	 None 	 
2025-08-06 01:27:04.905511 test begin: paddle.isin(Tensor([793801, 64],"float32"), Tensor([793801, 256],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([793801, 64],"float32"), Tensor([793801, 256],"float32"), False, True, ) 	 254016320 	 3671 	 351.8795747756958 	 165.86307787895203 	 0.05275464057922363 	 0.0025925636291503906 	 None 	 None 	 None 	 None 	 
2025-08-06 01:35:47.019864 test begin: paddle.isin(Tensor([8, 3175201],"float64"), Tensor([4, 256],"float64"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 3175201],"float64"), Tensor([4, 256],"float64"), False, False, ) 	 25402632 	 3671 	 9.976111888885498 	 78.0331461429596 	 0.0024709701538085938 	 0.0008714199066162109 	 None 	 None 	 None 	 None 	 
2025-08-06 01:37:15.580186 test begin: paddle.isin(Tensor([8, 3175201],"float64"), Tensor([4, 3175201],"float64"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 3175201],"float64"), Tensor([4, 3175201],"float64"), False, False, ) 	 38102412 	 3671 	 106.13342046737671 	 99.68044376373291 	 0.01774883270263672 	 0.0009849071502685547 	 None 	 None 	 None 	 None 	 
2025-08-06 01:40:42.273450 test begin: paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 256],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 256],"float32"), False, False, ) 	 50804232 	 3671 	 16.286187648773193 	 92.66311168670654 	 0.004206180572509766 	 0.0020401477813720703 	 None 	 None 	 None 	 None 	 
2025-08-06 01:42:32.068050 test begin: paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 256],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 256],"float32"), False, True, ) 	 50804232 	 3671 	 16.598170518875122 	 92.63358688354492 	 0.004271030426025391 	 0.0020363330841064453 	 None 	 None 	 None 	 None 	 
2025-08-06 01:44:22.148719 test begin: paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 6350401],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 6350401],"float32"), False, False, ) 	 76204812 	 3671 	 199.84649848937988 	 109.92809629440308 	 0.03629326820373535 	 0.002218008041381836 	 None 	 None 	 None 	 None 	 
2025-08-06 01:49:33.286482 test begin: paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 6350401],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 6350401],"float32"), False, True, ) 	 76204812 	 3671 	 197.2453169822693 	 109.92449259757996 	 0.03643512725830078 	 0.002215862274169922 	 None 	 None 	 None 	 None 	 
2025-08-06 01:54:42.221621 test begin: paddle.isin(Tensor([8, 64],"float32"), Tensor([198451, 256],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float32"), Tensor([198451, 256],"float32"), False, False, ) 	 50803968 	 3671 	 71.37117624282837 	 30.34777593612671 	 4.8160552978515625e-05 	 0.0002796649932861328 	 None 	 None 	 None 	 None 	 
2025-08-06 01:56:24.849244 test begin: paddle.isin(Tensor([8, 64],"float32"), Tensor([198451, 256],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float32"), Tensor([198451, 256],"float32"), False, True, ) 	 50803968 	 3671 	 76.2815363407135 	 30.358415842056274 	 5.7697296142578125e-05 	 0.0007026195526123047 	 None 	 None 	 None 	 None 	 
2025-08-06 01:58:13.018423 test begin: paddle.isin(Tensor([8, 64],"float32"), Tensor([4, 12700801],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float32"), Tensor([4, 12700801],"float32"), False, False, ) 	 50803716 	 3671 	 71.54942512512207 	 30.425127029418945 	 5.650520324707031e-05 	 0.0002765655517578125 	 None 	 None 	 None 	 None 	 
2025-08-06 01:59:56.920182 test begin: paddle.isin(Tensor([8, 64],"float32"), Tensor([4, 12700801],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float32"), Tensor([4, 12700801],"float32"), False, True, ) 	 50803716 	 3671 	 73.44134497642517 	 30.403066635131836 	 0.00012350082397460938 	 0.0002796649932861328 	 None 	 None 	 None 	 None 	 
2025-08-06 02:01:41.673386 test begin: paddle.isin(Tensor([8, 64],"float64"), Tensor([4, 6350401],"float64"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float64"), Tensor([4, 6350401],"float64"), False, False, ) 	 25402116 	 3671 	 41.21757674217224 	 44.02626967430115 	 5.626678466796875e-05 	 0.00025463104248046875 	 None 	 None 	 None 	 None 	 
2025-08-06 02:03:07.607868 test begin: paddle.isin(Tensor([8, 64],"float64"), Tensor([99226, 256],"float64"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float64"), Tensor([99226, 256],"float64"), False, False, ) 	 25402368 	 3671 	 48.220956563949585 	 43.96946477890015 	 5.507469177246094e-05 	 0.0002455711364746094 	 None 	 None 	 None 	 None 	 
2025-08-06 02:04:40.529338 test begin: paddle.isinf(Tensor([14, 226801, 16],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([14, 226801, 16],"float32"), ) 	 50803424 	 42950 	 10.024581670761108 	 21.085000038146973 	 0.2385549545288086 	 0.2480454444885254 	 None 	 None 	 None 	 None 	 
2025-08-06 02:05:14.263534 test begin: paddle.isinf(Tensor([14, 36655, 99],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([14, 36655, 99],"float32"), ) 	 50803830 	 42950 	 10.032973289489746 	 20.853954553604126 	 0.23866891860961914 	 0.24808835983276367 	 None 	 None 	 None 	 None 	 
2025-08-06 02:05:46.034127 test begin: paddle.isinf(Tensor([14, 64, 56701],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([14, 64, 56701],"float32"), ) 	 50804096 	 42950 	 10.040308713912964 	 20.851346254348755 	 0.23891186714172363 	 0.24811077117919922 	 None 	 None 	 None 	 None 	 
2025-08-06 02:06:17.773591 test begin: paddle.isinf(Tensor([14, 7, 518401],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([14, 7, 518401],"float32"), ) 	 50803298 	 42950 	 10.041666746139526 	 20.850895881652832 	 0.2389516830444336 	 0.2480790615081787 	 None 	 None 	 None 	 None 	 
2025-08-06 02:06:49.493703 test begin: paddle.isinf(Tensor([28462, 17, 5, 6, 7],"float16"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([28462, 17, 5, 6, 7],"float16"), ) 	 101609340 	 42950 	 16.67772078514099 	 22.378076553344727 	 0.396801233291626 	 0.2659752368927002 	 None 	 None 	 None 	 None 	 
2025-08-06 02:07:33.167938 test begin: paddle.isinf(Tensor([49613, 64, 16],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([49613, 64, 16],"float32"), ) 	 50803712 	 42950 	 9.983756065368652 	 20.8508358001709 	 0.23755717277526855 	 0.24810075759887695 	 None 	 None 	 None 	 None 	 
2025-08-06 02:08:04.814993 test begin: paddle.isinf(Tensor([73310, 7, 99],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([73310, 7, 99],"float32"), ) 	 50803830 	 42950 	 10.032182931900024 	 20.867405652999878 	 0.23873567581176758 	 0.24808049201965332 	 None 	 None 	 None 	 None 	 
2025-08-06 02:08:38.779310 test begin: paddle.isinf(Tensor([8, 17, 17789, 6, 7],"float16"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([8, 17, 17789, 6, 7],"float16"), ) 	 101610768 	 42950 	 16.690266609191895 	 22.353907823562622 	 0.39711999893188477 	 0.26596617698669434 	 None 	 None 	 None 	 None 	 
2025-08-06 02:09:21.707184 test begin: paddle.isinf(Tensor([8, 17, 5, 21346, 7],"float16"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([8, 17, 5, 21346, 7],"float16"), ) 	 101606960 	 42950 	 16.672973155975342 	 22.362199068069458 	 0.39672279357910156 	 0.2659287452697754 	 None 	 None 	 None 	 None 	 
2025-08-06 02:10:03.681497 test begin: paddle.isinf(Tensor([8, 17, 5, 6, 24904],"float16"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([8, 17, 5, 6, 24904],"float16"), ) 	 101608320 	 42950 	 16.685250759124756 	 22.35392713546753 	 0.3970615863800049 	 0.26596879959106445 	 None 	 None 	 None 	 None 	 
2025-08-06 02:10:44.792714 test begin: paddle.isinf(Tensor([8, 60481, 5, 6, 7],"float16"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([8, 60481, 5, 6, 7],"float16"), ) 	 101608080 	 42950 	 16.69041872024536 	 22.354166984558105 	 0.39716005325317383 	 0.2659749984741211 	 None 	 None 	 None 	 None 	 
2025-08-06 02:11:25.706168 test begin: paddle.isnan(Tensor([10445, 4864],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([10445, 4864],"float32"), ) 	 50804480 	 42698 	 9.963807821273804 	 7.948103427886963 	 0.23851227760314941 	 0.1899564266204834 	 None 	 None 	 None 	 None 	 
2025-08-06 02:11:47.732330 test begin: paddle.isnan(Tensor([16, 64, 320, 320],"float16"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([16, 64, 320, 320],"float16"), ) 	 104857600 	 42698 	 16.985363960266113 	 9.894629001617432 	 0.4065546989440918 	 0.23683691024780273 	 None 	 None 	 None 	 None 	 
2025-08-06 02:12:16.539021 test begin: paddle.isnan(Tensor([4, 125, 320, 320],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 125, 320, 320],"float32"), ) 	 51200000 	 42698 	 10.015149593353271 	 7.994683027267456 	 0.2397174835205078 	 0.19131755828857422 	 None 	 None 	 None 	 None 	 
2025-08-06 02:12:37.376579 test begin: paddle.isnan(Tensor([4, 249, 320, 320],"float16"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 249, 320, 320],"float16"), ) 	 101990400 	 42698 	 16.499781847000122 	 9.636809587478638 	 0.39498448371887207 	 0.23052144050598145 	 None 	 None 	 None 	 None 	 
2025-08-06 02:13:05.403501 test begin: paddle.isnan(Tensor([4, 64, 1241, 320],"float16"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 64, 1241, 320],"float16"), ) 	 101662720 	 42698 	 16.442283153533936 	 9.602003812789917 	 0.39354515075683594 	 0.22979259490966797 	 None 	 None 	 None 	 None 	 
2025-08-06 02:13:33.288924 test begin: paddle.isnan(Tensor([4, 64, 320, 1241],"float16"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 64, 320, 1241],"float16"), ) 	 101662720 	 42698 	 16.442175149917603 	 9.612046241760254 	 0.3935420513153076 	 0.22984695434570312 	 None 	 None 	 None 	 None 	 
2025-08-06 02:14:02.682486 test begin: paddle.isnan(Tensor([4, 64, 320, 621],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 64, 320, 621],"float32"), ) 	 50872320 	 42698 	 9.971185445785522 	 7.947520971298218 	 0.2386472225189209 	 0.19019556045532227 	 None 	 None 	 None 	 None 	 
2025-08-06 02:14:21.413969 test begin: paddle.isnan(Tensor([4, 64, 621, 320],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 64, 621, 320],"float32"), ) 	 50872320 	 42698 	 9.971094369888306 	 7.947669744491577 	 0.23865890502929688 	 0.19024014472961426 	 None 	 None 	 None 	 None 	 
2025-08-06 02:14:42.363489 test begin: paddle.isnan(Tensor([4864, 10445],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4864, 10445],"float32"), ) 	 50804480 	 42698 	 9.963956594467163 	 7.937639236450195 	 0.23849010467529297 	 0.18996763229370117 	 None 	 None 	 None 	 None 	 
2025-08-06 02:15:01.080800 test begin: paddle.isnan(Tensor([8, 64, 320, 320],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([8, 64, 320, 320],"float32"), ) 	 52428800 	 42698 	 10.265505075454712 	 8.185048818588257 	 0.24568629264831543 	 0.1958153247833252 	 None 	 None 	 None 	 None 	 
2025-08-06 02:15:20.364471 test begin: paddle.isneginf(Tensor([11, 17, 2716],"int32"), )
[Prof] paddle.isneginf 	 paddle.isneginf(Tensor([11, 17, 2716],"int32"), ) 	 507892 	 1000 	 19.643845319747925 	 0.011365413665771484 	 4.863739013671875e-05 	 3.600120544433594e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:15:41.349635 test begin: paddle.isneginf(Tensor([11, 17, 5433],"int16"), )
[Prof] paddle.isneginf 	 paddle.isneginf(Tensor([11, 17, 5433],"int16"), ) 	 1015971 	 1000 	 42.70627522468567 	 0.011716842651367188 	 4.4345855712890625e-05 	 4.076957702636719e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:16:24.130341 test begin: paddle.isneginf(Tensor([11, 4618, 10],"int32"), )
[Prof] paddle.isneginf 	 paddle.isneginf(Tensor([11, 4618, 10],"int32"), ) 	 507980 	 1000 	 19.723422288894653 	 0.01136469841003418 	 4.792213439941406e-05 	 3.457069396972656e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:16:43.898477 test begin: paddle.isneginf(Tensor([11, 46184],"float32"), )
[Prof] paddle.isneginf 	 paddle.isneginf(Tensor([11, 46184],"float32"), ) 	 508024 	 1000 	 19.637702703475952 	 0.010429859161376953 	 4.4345855712890625e-05 	 3.5762786865234375e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:17:03.599573 test begin: paddle.isneginf(Tensor([11, 9236, 10],"int16"), )
[Prof] paddle.isneginf 	 paddle.isneginf(Tensor([11, 9236, 10],"int16"), ) 	 1015960 	 1000 	 45.20609378814697 	 0.016965150833129883 	 4.887580871582031e-05 	 5.650520324707031e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:17:48.893398 test begin: paddle.isneginf(Tensor([2988, 17, 10],"int32"), )
[Prof] paddle.isneginf 	 paddle.isneginf(Tensor([2988, 17, 10],"int32"), ) 	 507960 	 1000 	 23.213297843933105 	 0.011255025863647461 	 4.458427429199219e-05 	 7.081031799316406e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:18:12.154550 test begin: paddle.isneginf(Tensor([29884, 17],"float32"), )
[Prof] paddle.isneginf 	 paddle.isneginf(Tensor([29884, 17],"float32"), ) 	 508028 	 1000 	 20.047059297561646 	 0.010172605514526367 	 3.814697265625e-05 	 3.981590270996094e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:18:34.607461 test begin: paddle.isneginf(Tensor([5976, 17, 10],"int16"), )
[Prof] paddle.isneginf 	 paddle.isneginf(Tensor([5976, 17, 10],"int16"), ) 	 1015920 	 1000 	 45.44960141181946 	 0.01844191551208496 	 5.7697296142578125e-05 	 6.270408630371094e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:19:20.137830 test begin: paddle.isposinf(Tensor([11, 17, 2716],"int32"), )
[Prof] paddle.isposinf 	 paddle.isposinf(Tensor([11, 17, 2716],"int32"), ) 	 507892 	 1000 	 19.628400802612305 	 0.011214971542358398 	 4.0531158447265625e-05 	 5.4836273193359375e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:19:40.609276 test begin: paddle.isposinf(Tensor([11, 17, 5433],"int16"), )
[Prof] paddle.isposinf 	 paddle.isposinf(Tensor([11, 17, 5433],"int16"), ) 	 1015971 	 1000 	 45.064576625823975 	 0.01161336898803711 	 4.7206878662109375e-05 	 3.9577484130859375e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:20:25.742672 test begin: paddle.isposinf(Tensor([11, 4618, 10],"int32"), )
[Prof] paddle.isposinf 	 paddle.isposinf(Tensor([11, 4618, 10],"int32"), ) 	 507980 	 1000 	 19.807889938354492 	 0.017974376678466797 	 5.626678466796875e-05 	 4.458427429199219e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:20:45.601965 test begin: paddle.isposinf(Tensor([11, 46184],"float32"), )
[Prof] paddle.isposinf 	 paddle.isposinf(Tensor([11, 46184],"float32"), ) 	 508024 	 1000 	 19.974942445755005 	 0.017071247100830078 	 4.9591064453125e-05 	 4.076957702636719e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:21:07.480493 test begin: paddle.isposinf(Tensor([11, 9236, 10],"int16"), )
[Prof] paddle.isposinf 	 paddle.isposinf(Tensor([11, 9236, 10],"int16"), ) 	 1015960 	 1000 	 44.95809197425842 	 0.011728286743164062 	 4.744529724121094e-05 	 4.172325134277344e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:21:52.506436 test begin: paddle.isposinf(Tensor([2988, 17, 10],"int32"), )
[Prof] paddle.isposinf 	 paddle.isposinf(Tensor([2988, 17, 10],"int32"), ) 	 507960 	 1000 	 19.7293541431427 	 0.011204719543457031 	 4.4345855712890625e-05 	 2.86102294921875e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:22:12.277240 test begin: paddle.isposinf(Tensor([29884, 17],"float32"), )
[Prof] paddle.isposinf 	 paddle.isposinf(Tensor([29884, 17],"float32"), ) 	 508028 	 1000 	 19.615645170211792 	 0.010450124740600586 	 4.291534423828125e-05 	 3.266334533691406e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:22:31.932801 test begin: paddle.isposinf(Tensor([5976, 17, 10],"int16"), )
[Prof] paddle.isposinf 	 paddle.isposinf(Tensor([5976, 17, 10],"int16"), ) 	 1015920 	 1000 	 45.207480669021606 	 0.013535737991333008 	 4.6253204345703125e-05 	 4.1484832763671875e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 02:23:17.208189 test begin: paddle.isreal(Tensor([15876010, 32],"bool"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([15876010, 32],"bool"), ) 	 508032320 	 25942 	 9.96613335609436 	 8.583317756652832 	 0.39258551597595215 	 0.3375091552734375 	 None 	 None 	 None 	 None 	 
2025-08-06 02:23:42.524731 test begin: paddle.isreal(Tensor([31752010, 32],"bfloat16"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([31752010, 32],"bfloat16"), ) 	 1016064320 	 25942 	 19.86304807662964 	 18.211361169815063 	 0.7824273109436035 	 0.6718745231628418 	 None 	 None 	 None 	 None 	 
2025-08-06 02:24:39.721509 test begin: paddle.isreal(Tensor([31752010, 32],"float16"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([31752010, 32],"float16"), ) 	 1016064320 	 25942 	 19.842623472213745 	 17.071939706802368 	 0.781785249710083 	 0.6722209453582764 	 None 	 None 	 None 	 None 	 
2025-08-06 02:25:38.814735 test begin: paddle.isreal(Tensor([640, 1587601],"bfloat16"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([640, 1587601],"bfloat16"), ) 	 1016064640 	 25942 	 19.877623319625854 	 17.062524795532227 	 0.7824192047119141 	 0.6722960472106934 	 None 	 None 	 None 	 None 	 
2025-08-06 02:26:34.846407 test begin: paddle.isreal(Tensor([640, 1587601],"float16"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([640, 1587601],"float16"), ) 	 1016064640 	 25942 	 19.865856885910034 	 17.064431190490723 	 0.7826898097991943 	 0.6719088554382324 	 None 	 None 	 None 	 None 	 
2025-08-06 02:27:32.325152 test begin: paddle.isreal(Tensor([640, 793801],"bool"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([640, 793801],"bool"), ) 	 508032640 	 25942 	 9.967455863952637 	 8.82654881477356 	 0.3925340175628662 	 0.33754944801330566 	 None 	 None 	 None 	 None 	 
2025-08-06 02:28:00.242428 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([22336, 5, 4, 3, 2],"float32"), )
[Prof] paddle.kron 	 paddle.kron(Tensor([10, 10],"float32"), Tensor([22336, 5, 4, 3, 2],"float32"), ) 	 2680420 	 1311 	 12.779458999633789 	 2.5086302757263184 	 0.00010800361633300781 	 1.3955531120300293 	 19.6641583442688 	 7.860563278198242 	 7.748603820800781e-05 	 1.2255363464355469 	 
2025-08-06 02:28:50.588032 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 22336, 4, 3, 2],"float32"), )
[Prof] paddle.kron 	 paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 22336, 4, 3, 2],"float32"), ) 	 2680420 	 1311 	 12.781631231307983 	 1.790475845336914 	 0.00011229515075683594 	 1.3954112529754639 	 19.638065814971924 	 8.469741582870483 	 8.797645568847656e-05 	 1.8938543796539307 	 
2025-08-06 02:29:38.915879 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 13868, 3, 2],"float32"), )
[Prof] paddle.kron 	 paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 13868, 3, 2],"float32"), ) 	 2080300 	 1311 	 9.959186315536499 	 1.4040148258209229 	 9.298324584960938e-05 	 1.0838634967803955 	 15.2610023021698 	 6.303758144378662 	 8.916854858398438e-05 	 0.98244309425354 	 
2025-08-06 02:30:17.873582 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 4, 15401, 2],"float32"), )
[Prof] paddle.kron 	 paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 4, 15401, 2],"float32"), ) 	 3080300 	 1311 	 14.560110330581665 	 2.060337543487549 	 0.0001266002655029297 	 1.6058950424194336 	 22.22593379020691 	 8.492213726043701 	 9.036064147949219e-05 	 1.3239667415618896 	 
2025-08-06 02:31:14.322014 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 4, 3, 8934],"float32"), )
[Prof] paddle.kron 	 paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 4, 3, 8934],"float32"), ) 	 2680300 	 1311 	 12.64919400215149 	 1.7924530506134033 	 0.0001499652862548828 	 1.3969762325286865 	 22.610800743103027 	 6.550288915634155 	 0.00017833709716796875 	 1.020796537399292 	 
2025-08-06 02:32:02.244926 test begin: paddle.kthvalue(Tensor([30, 200, 8468],"float32"), k=1, axis=1, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([30, 200, 8468],"float32"), k=1, axis=1, ) 	 50808000 	 3230 	 12.966491937637329 	 13.365792989730835 	 1.023777961730957 	 4.231858968734741 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:32:32.851575 test begin: paddle.kthvalue(Tensor([30, 200, 8468],"float32"), k=1, axis=1, keepdim=True, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([30, 200, 8468],"float32"), k=1, axis=1, keepdim=True, ) 	 50808000 	 3230 	 12.96970510482788 	 13.35022234916687 	 1.0238995552062988 	 4.2233922481536865 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:33:00.711300 test begin: paddle.kthvalue(Tensor([30, 200, 8468],"float32"), k=2, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([30, 200, 8468],"float32"), k=2, ) 	 50808000 	 3230 	 10.015716791152954 	 8.07013487815857 	 3.169081926345825 	 2.553542137145996 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:33:20.270478 test begin: paddle.kthvalue(Tensor([30, 42337, 40],"float32"), k=1, axis=1, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([30, 42337, 40],"float32"), k=1, axis=1, ) 	 50804400 	 3230 	 14.596535921096802 	 35.63625121116638 	 1.26680326461792 	 11.271862268447876 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:34:14.497301 test begin: paddle.kthvalue(Tensor([30, 42337, 40],"float32"), k=1, axis=1, keepdim=True, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([30, 42337, 40],"float32"), k=1, axis=1, keepdim=True, ) 	 50804400 	 3230 	 14.446120500564575 	 36.384934425354004 	 1.140550136566162 	 11.504528522491455 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:35:08.986366 test begin: paddle.kthvalue(Tensor([30, 42337, 40],"float32"), k=2, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([30, 42337, 40],"float32"), k=2, ) 	 50804400 	 3230 	 16.90956425666809 	 16.630990266799927 	 5.350167989730835 	 5.262386798858643 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:35:44.940629 test begin: paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=1, axis=1, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=1, axis=1, ) 	 50808000 	 3230 	 13.09152865409851 	 13.40996265411377 	 1.2289831638336182 	 4.2405617237091064 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:36:15.237832 test begin: paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=1, axis=1, keepdim=True, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=1, axis=1, keepdim=True, ) 	 50808000 	 3230 	 12.874658584594727 	 13.404454469680786 	 1.0164704322814941 	 4.23943567276001 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:36:44.545157 test begin: paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=2, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=2, ) 	 50808000 	 3230 	 16.890540599822998 	 16.638031005859375 	 5.341399192810059 	 5.264947175979614 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:37:20.495144 test begin: paddle.lcm(Tensor([1],"int64"), Tensor([25401601],"int64"), )
[Prof] paddle.lcm 	 paddle.lcm(Tensor([1],"int64"), Tensor([25401601],"int64"), ) 	 25401602 	 1000 	 87.82628083229065 	 5.6760218143463135 	 0.002262115478515625 	 0.0009067058563232422 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:38:55.025717 test begin: paddle.lcm(Tensor([25401601],"int64"), Tensor([1],"int64"), )
[Prof] paddle.lcm 	 paddle.lcm(Tensor([25401601],"int64"), Tensor([1],"int64"), ) 	 25401602 	 1000 	 96.98627090454102 	 5.70408821105957 	 0.00226593017578125 	 0.0009133815765380859 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:40:40.499691 test begin: paddle.lcm(Tensor([25401601],"int64"), Tensor([25401601],"int64"), )
[Prof] paddle.lcm 	 paddle.lcm(Tensor([25401601],"int64"), Tensor([25401601],"int64"), ) 	 50803202 	 1000 	 102.00291013717651 	 5.874366521835327 	 0.002421855926513672 	 0.0009219646453857422 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:42:33.976418 test begin: paddle.lcm(Tensor([50803201],"int32"), Tensor([1],"int32"), )
[Prof] paddle.lcm 	 paddle.lcm(Tensor([50803201],"int32"), Tensor([1],"int32"), ) 	 50803202 	 1000 	 101.02597522735596 	 7.978774785995483 	 0.0023224353790283203 	 0.0013027191162109375 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:44:25.367859 test begin: paddle.ldexp(Tensor([25401601],"float64"), Tensor([25401601],"int32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcab9cf3010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 02:54:58.376974 test begin: paddle.ldexp(Tensor([25401601],"int64"), Tensor([25401601],"int32"), )
W0806 02:54:59.229502 120450 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.ldexp 	 paddle.ldexp(Tensor([25401601],"int64"), Tensor([25401601],"int32"), ) 	 50803202 	 12113 	 9.991719722747803 	 7.795016050338745 	 0.1686091423034668 	 0.2190723419189453 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 03:01:49.446505 test begin: paddle.ldexp(Tensor([50803201],"float64"), Tensor([50803201],"int32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa5e65fee60>,)) (kwargs={}) timed out after 600.000000 seconds.

W0806 03:11:49.573801 120781 backward.cc:462] While running Node (ElementwisePowGradNode) raises an EnforceNotMet exception
terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754421109 (unix time) try "date -d @1754421109" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d630) received by PID 120368 (TID 0x7fa5dd7fb640) from PID 120368 ***]

2025-08-06 03:11:59.529380 test begin: paddle.ldexp(Tensor([50803201],"int32"), Tensor([50803201],"int32"), )
W0806 03:12:00.762639 121339 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa554d36f50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:22:04.363128 test begin: paddle.ldexp(Tensor([50803201],"int64"), Tensor([50803201],"int32"), )
W0806 03:22:05.781138 121814 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7297abafb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:32:09.029312 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 28, 604801],"float32"), 0.36, )
W0806 03:32:09.810459 122359 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 28, 604801],"float32"), 0.36, ) 	 50803285 	 33528 	 10.027841806411743 	 10.137718439102173 	 0.1528301239013672 	 0.3089437484741211 	 21.22920298576355 	 25.110048055648804 	 0.21541523933410645 	 0.19112443923950195 	 
2025-08-06 03:33:19.224582 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 604801, 28],"float32"), 0.36, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 604801, 28],"float32"), 0.36, ) 	 50803285 	 33528 	 10.031625747680664 	 10.145179986953735 	 0.15282416343688965 	 0.30906057357788086 	 21.24973964691162 	 25.106935262680054 	 0.21567010879516602 	 0.19109773635864258 	 
2025-08-06 03:34:28.305285 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([64801, 28, 28],"float32"), 0.36, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([64801, 28, 28],"float32"), 0.36, ) 	 50803985 	 33528 	 10.02574872970581 	 10.138826131820679 	 0.15276026725769043 	 0.30907106399536133 	 21.249959707260132 	 25.106664419174194 	 0.2156388759613037 	 0.19111943244934082 	 
2025-08-06 03:35:38.937198 test begin: paddle.lerp(Tensor([1, 1814401, 28],"float32"), Tensor([3, 1814401, 28],"float32"), 1.0, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 1814401, 28],"float32"), Tensor([3, 1814401, 28],"float32"), 1.0, ) 	 203212912 	 33528 	 44.955551862716675 	 44.43955755233765 	 0.6850681304931641 	 1.3545868396759033 	 74.40086007118225 	 78.9582302570343 	 1.1339709758758545 	 0.8021512031555176 	 
2025-08-06 03:39:47.919932 test begin: paddle.lerp(Tensor([1, 28, 1814401],"float32"), Tensor([3, 28, 1814401],"float32"), 1.0, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 28, 1814401],"float32"), Tensor([3, 28, 1814401],"float32"), 1.0, ) 	 203212912 	 33528 	 44.951547384262085 	 44.43799829483032 	 0.6852054595947266 	 1.3545799255371094 	 74.40185594558716 	 78.9578206539154 	 1.134035348892212 	 0.8022048473358154 	 
2025-08-06 03:44:01.143049 test begin: paddle.lerp(Tensor([1, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.0, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.0, ) 	 50804768 	 33528 	 10.010781049728394 	 10.21912693977356 	 0.15257477760314941 	 0.3110785484313965 	 26.6332688331604 	 26.211706399917603 	 0.2703979015350342 	 0.1995103359222412 	 
2025-08-06 03:45:17.866618 test begin: paddle.lerp(Tensor([3, 28, 604801],"float32"), Tensor([3, 28, 604801],"float32"), 1.2, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([3, 28, 604801],"float32"), Tensor([3, 28, 604801],"float32"), 1.2, ) 	 101606568 	 33528 	 15.182426452636719 	 14.980950117111206 	 0.231398344039917 	 0.4566030502319336 	 15.867231130599976 	 19.959227323532104 	 0.4835972785949707 	 0.3041417598724365 	 
2025-08-06 03:46:28.216346 test begin: paddle.lerp(Tensor([3, 604801, 28],"float32"), Tensor([3, 604801, 28],"float32"), 1.2, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([3, 604801, 28],"float32"), Tensor([3, 604801, 28],"float32"), 1.2, ) 	 101606568 	 33528 	 15.18624472618103 	 14.978652238845825 	 0.2314286231994629 	 0.4565587043762207 	 15.866584062576294 	 19.95919132232666 	 0.48375606536865234 	 0.30417537689208984 	 
2025-08-06 03:47:38.789595 test begin: paddle.lerp(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.0, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.0, ) 	 101607968 	 33528 	 15.180056810379028 	 14.978893995285034 	 0.23136019706726074 	 0.45661401748657227 	 15.86781358718872 	 19.959742307662964 	 0.4837524890899658 	 0.3041810989379883 	 
2025-08-06 03:48:49.205746 test begin: paddle.lerp(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.2, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.2, ) 	 101607968 	 33528 	 15.18604326248169 	 14.978837728500366 	 0.2314155101776123 	 0.4565892219543457 	 15.868316173553467 	 19.959888219833374 	 0.48366856575012207 	 0.30422186851501465 	 
2025-08-06 03:49:59.098400 test begin: paddle.less(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 50803600 	 53034 	 10.135352849960327 	 12.918044805526733 	 0.19531536102294922 	 0.24867033958435059 	 None 	 None 	 None 	 None 	 
2025-08-06 03:50:25.679760 test begin: paddle.less(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), ) 	 50803600 	 53034 	 10.003934144973755 	 12.980746030807495 	 0.19270944595336914 	 0.24927973747253418 	 None 	 None 	 None 	 None 	 
2025-08-06 03:50:51.803846 test begin: paddle.less(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 101606800 	 53034 	 17.340621948242188 	 17.399853229522705 	 0.33419322967529297 	 0.3348870277404785 	 None 	 None 	 None 	 None 	 
2025-08-06 03:51:30.057449 test begin: paddle.less(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), ) 	 101606420 	 53034 	 17.346028804779053 	 17.37800693511963 	 0.3343038558959961 	 0.33487749099731445 	 None 	 None 	 None 	 None 	 
2025-08-06 03:52:06.490701 test begin: paddle.less(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), ) 	 101606420 	 53034 	 17.35241150856018 	 17.38423180580139 	 0.3343489170074463 	 0.3349928855895996 	 None 	 None 	 None 	 None 	 
2025-08-06 03:52:44.901160 test begin: paddle.less(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), ) 	 101606440 	 53034 	 17.353559494018555 	 17.377805709838867 	 0.33443140983581543 	 0.33492445945739746 	 None 	 None 	 None 	 None 	 
2025-08-06 03:53:21.314974 test begin: paddle.less(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float32"), ) 	 101607424 	 53034 	 17.34632706642151 	 18.252755165100098 	 0.33431077003479004 	 0.33492469787597656 	 None 	 None 	 None 	 None 	 
2025-08-06 03:54:01.454609 test begin: paddle.less(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 53034 	 17.339726209640503 	 17.379419088363647 	 0.3341190814971924 	 0.33495259284973145 	 None 	 None 	 None 	 None 	 
2025-08-06 03:54:40.144101 test begin: paddle.less_equal(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 50803600 	 53036 	 10.102205276489258 	 12.913201570510864 	 0.19466185569763184 	 0.24873065948486328 	 None 	 None 	 None 	 None 	 
2025-08-06 03:55:06.719621 test begin: paddle.less_equal(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), ) 	 50803600 	 53036 	 9.962973356246948 	 12.943332195281982 	 0.1919546127319336 	 0.24943780899047852 	 None 	 None 	 None 	 None 	 
2025-08-06 03:55:30.464879 test begin: paddle.less_equal(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 101606800 	 53036 	 17.35434055328369 	 17.378600120544434 	 0.3344838619232178 	 0.33492517471313477 	 None 	 None 	 None 	 None 	 
2025-08-06 03:56:09.986307 test begin: paddle.less_equal(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), ) 	 101606420 	 53036 	 17.34568166732788 	 17.384655475616455 	 0.3342742919921875 	 0.3349184989929199 	 None 	 None 	 None 	 None 	 
2025-08-06 03:56:46.497902 test begin: paddle.less_equal(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float32"), )
W0806 03:56:49.824393 123482 dygraph_functions.cc:90806] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float32"), ) 	 203212812 	 53036 	 59.96146869659424 	 38.14785146713257 	 0.577911376953125 	 0.7349188327789307 	 None 	 None 	 None 	 None 	 
2025-08-06 03:58:28.239363 test begin: paddle.less_equal(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), ) 	 101606440 	 53036 	 17.346505403518677 	 17.3806209564209 	 0.33432674407958984 	 0.33490967750549316 	 None 	 None 	 None 	 None 	 
2025-08-06 03:59:04.761699 test begin: paddle.less_equal(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float32"), ) 	 203212816 	 53036 	 59.97546148300171 	 38.14136552810669 	 0.5778422355651855 	 0.7350833415985107 	 None 	 None 	 None 	 None 	 
2025-08-06 04:00:47.274861 test begin: paddle.less_equal(Tensor([4, 3, 4233601],"float16"), Tensor([4, 3, 4233601],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([4, 3, 4233601],"float16"), Tensor([4, 3, 4233601],"float32"), ) 	 101606424 	 53036 	 30.23962950706482 	 19.266791582107544 	 0.2912158966064453 	 0.37125635147094727 	 None 	 None 	 None 	 None 	 
2025-08-06 04:01:42.344151 test begin: paddle.less_equal(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float32"), ) 	 203212824 	 53036 	 59.97133469581604 	 38.15405988693237 	 0.5778059959411621 	 0.7349810600280762 	 None 	 None 	 None 	 None 	 
2025-08-06 04:03:24.021975 test begin: paddle.less_equal(Tensor([4, 6350401, 2],"float16"), Tensor([4, 6350401, 2],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([4, 6350401, 2],"float16"), Tensor([4, 6350401, 2],"float32"), ) 	 101606416 	 53036 	 30.22336721420288 	 19.269598722457886 	 0.2912125587463379 	 0.3713226318359375 	 None 	 None 	 None 	 None 	 
2025-08-06 04:04:18.521937 test begin: paddle.less_equal(Tensor([8467201, 3, 2],"float16"), Tensor([8467201, 3, 2],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([8467201, 3, 2],"float16"), Tensor([8467201, 3, 2],"float32"), ) 	 101606412 	 53036 	 30.22229790687561 	 19.28819513320923 	 0.29112744331359863 	 0.3713359832763672 	 None 	 None 	 None 	 None 	 
2025-08-06 04:05:09.811091 test begin: paddle.less_than(Tensor([1, 128, 198451],"int64"), Tensor([1, 128, 198451],"int64"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 128, 198451],"int64"), Tensor([1, 128, 198451],"int64"), ) 	 50803456 	 56808 	 17.57403063774109 	 17.79035449028015 	 0.3162262439727783 	 0.3200693130493164 	 None 	 None 	 None 	 None 	 
2025-08-06 04:05:46.091079 test begin: paddle.less_than(Tensor([1, 128, 256],"float32"), Tensor([1551, 128, 256],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 128, 256],"float32"), Tensor([1551, 128, 256],"float32"), ) 	 50855936 	 56808 	 10.812619686126709 	 13.859198093414307 	 0.1945781707763672 	 0.24931669235229492 	 None 	 None 	 None 	 None 	 
2025-08-06 04:06:11.656923 test begin: paddle.less_than(Tensor([1, 128, 256],"int64"), Tensor([776, 128, 256],"int64"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 128, 256],"int64"), Tensor([776, 128, 256],"int64"), ) 	 25460736 	 56808 	 11.700910568237305 	 10.319671630859375 	 0.21050405502319336 	 0.18564128875732422 	 None 	 None 	 None 	 None 	 
2025-08-06 04:06:35.457199 test begin: paddle.less_than(Tensor([1, 128, 396901],"float32"), Tensor([1, 128, 396901],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 128, 396901],"float32"), Tensor([1, 128, 396901],"float32"), ) 	 101606656 	 56808 	 18.585359573364258 	 18.61652684211731 	 0.3340141773223877 	 0.33495283126831055 	 None 	 None 	 None 	 None 	 
2025-08-06 04:07:17.565242 test begin: paddle.less_than(Tensor([1, 198451, 256],"float32"), Tensor([1, 198451, 256],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 198451, 256],"float32"), Tensor([1, 198451, 256],"float32"), ) 	 101606912 	 56808 	 18.58244562149048 	 21.66909646987915 	 0.3342583179473877 	 0.3349735736846924 	 None 	 None 	 None 	 None 	 
2025-08-06 04:08:02.265134 test begin: paddle.less_than(Tensor([1, 99226, 256],"int64"), Tensor([1, 99226, 256],"int64"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 99226, 256],"int64"), Tensor([1, 99226, 256],"int64"), ) 	 50803712 	 56808 	 17.57967972755432 	 17.779654502868652 	 0.3161284923553467 	 0.3198843002319336 	 None 	 None 	 None 	 None 	 
2025-08-06 04:08:40.563825 test begin: paddle.less_than(Tensor([1551, 128, 256],"float32"), Tensor([1, 128, 256],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1551, 128, 256],"float32"), Tensor([1, 128, 256],"float32"), ) 	 50855936 	 56808 	 10.705218076705933 	 13.881625890731812 	 0.19250154495239258 	 0.24968433380126953 	 None 	 None 	 None 	 None 	 
2025-08-06 04:09:08.490603 test begin: paddle.less_than(Tensor([1551, 128, 256],"float32"), Tensor([1551, 128, 256],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1551, 128, 256],"float32"), Tensor([1551, 128, 256],"float32"), ) 	 101646336 	 56808 	 18.595471143722534 	 18.621644735336304 	 0.3345050811767578 	 0.3349792957305908 	 None 	 None 	 None 	 None 	 
2025-08-06 04:09:47.447984 test begin: paddle.less_than(Tensor([3101, 1, 128, 128],"float32"), Tensor([3101, 1, 128, 128],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([3101, 1, 128, 128],"float32"), Tensor([3101, 1, 128, 128],"float32"), ) 	 101613568 	 56808 	 18.58705759048462 	 18.618983268737793 	 0.33440184593200684 	 0.3349299430847168 	 None 	 None 	 None 	 None 	 
2025-08-06 04:10:27.772288 test begin: paddle.less_than(Tensor([776, 128, 256],"int64"), Tensor([1, 128, 256],"int64"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([776, 128, 256],"int64"), Tensor([1, 128, 256],"int64"), ) 	 25460736 	 56808 	 9.961954832077026 	 10.325011253356934 	 0.17918992042541504 	 0.18553471565246582 	 None 	 None 	 None 	 None 	 
2025-08-06 04:10:52.616083 test begin: paddle.less_than(Tensor([776, 128, 256],"int64"), Tensor([776, 128, 256],"int64"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([776, 128, 256],"int64"), Tensor([776, 128, 256],"int64"), ) 	 50855936 	 56808 	 17.612914323806763 	 17.822826623916626 	 0.31687307357788086 	 0.32030558586120605 	 None 	 None 	 None 	 None 	 
2025-08-06 04:11:31.802515 test begin: paddle.less_than(Tensor([8, 1, 128, 128],"float32"), Tensor([8, 388, 128, 128],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([8, 1, 128, 128],"float32"), Tensor([8, 388, 128, 128],"float32"), ) 	 50987008 	 56808 	 10.951479196548462 	 14.714022397994995 	 0.19702458381652832 	 0.2647535800933838 	 None 	 None 	 None 	 None 	 
2025-08-06 04:11:58.325879 test begin: paddle.less_than(Tensor([8, 1, 128, 49613],"float32"), Tensor([8, 1, 128, 49613],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([8, 1, 128, 49613],"float32"), Tensor([8, 1, 128, 49613],"float32"), ) 	 101607424 	 56808 	 18.578309774398804 	 18.614588499069214 	 0.3342580795288086 	 0.33495163917541504 	 None 	 None 	 None 	 None 	 
2025-08-06 04:12:39.701944 test begin: paddle.less_than(Tensor([8, 1, 49613, 128],"float32"), Tensor([8, 1, 49613, 128],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([8, 1, 49613, 128],"float32"), Tensor([8, 1, 49613, 128],"float32"), ) 	 101607424 	 56808 	 18.60234832763672 	 18.615086317062378 	 0.3344559669494629 	 0.3349313735961914 	 None 	 None 	 None 	 None 	 
2025-08-06 04:13:21.407848 test begin: paddle.less_than(Tensor([8, 388, 128, 128],"float32"), Tensor([8, 1, 128, 128],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([8, 388, 128, 128],"float32"), Tensor([8, 1, 128, 128],"float32"), ) 	 50987008 	 56808 	 10.830244302749634 	 14.732832431793213 	 0.19486212730407715 	 0.265056848526001 	 None 	 None 	 None 	 None 	 
2025-08-06 04:13:49.099869 test begin: paddle.less_than(Tensor([8, 388, 128, 128],"float32"), Tensor([8, 388, 128, 128],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([8, 388, 128, 128],"float32"), Tensor([8, 388, 128, 128],"float32"), ) 	 101711872 	 56808 	 18.602278232574463 	 18.63304591178894 	 0.33461713790893555 	 0.33519864082336426 	 None 	 None 	 None 	 None 	 
2025-08-06 04:14:28.257019 test begin: paddle.lgamma(Tensor([10, 10, 10, 25402],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([10, 10, 10, 25402],"float64"), ) 	 25402000 	 25074 	 17.934881448745728 	 17.36550498008728 	 0.730963945388794 	 0.70786452293396 	 34.685210943222046 	 39.872722148895264 	 1.4134125709533691 	 0.8123490810394287 	 
2025-08-06 04:16:20.135386 test begin: paddle.lgamma(Tensor([10, 10, 127009, 2],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([10, 10, 127009, 2],"float64"), ) 	 25401800 	 25074 	 17.90572190284729 	 17.336705207824707 	 0.7296035289764404 	 0.7065596580505371 	 34.72388505935669 	 39.824599742889404 	 1.4151854515075684 	 0.811424732208252 	 
2025-08-06 04:18:14.280293 test begin: paddle.lgamma(Tensor([10, 127009, 10, 2],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([10, 127009, 10, 2],"float64"), ) 	 25401800 	 25074 	 17.88357138633728 	 17.31435990333557 	 0.7287404537200928 	 0.705683708190918 	 34.69614934921265 	 39.80197048187256 	 1.414231777191162 	 0.8110885620117188 	 
2025-08-06 04:20:05.886599 test begin: paddle.lgamma(Tensor([100, 254017],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([100, 254017],"float64"), ) 	 25401700 	 25074 	 17.874649047851562 	 17.308021783828735 	 0.7285640239715576 	 0.7054517269134521 	 34.656986474990845 	 39.794249057769775 	 1.4126343727111816 	 0.8109700679779053 	 
2025-08-06 04:21:56.600821 test begin: paddle.lgamma(Tensor([127009, 10, 10, 2],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([127009, 10, 10, 2],"float64"), ) 	 25401800 	 25074 	 17.87263298034668 	 17.326668739318848 	 0.7284412384033203 	 0.7055051326751709 	 34.687872648239136 	 39.791584730148315 	 1.4138705730438232 	 0.8108651638031006 	 
2025-08-06 04:23:47.382677 test begin: paddle.lgamma(Tensor([1948, 26080],"float32"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([1948, 26080],"float32"), ) 	 50803840 	 25074 	 10.02391767501831 	 9.67899489402771 	 0.4099733829498291 	 0.3925786018371582 	 24.120707035064697 	 37.848204612731934 	 0.9831240177154541 	 0.7713239192962646 	 
2025-08-06 04:25:13.840651 test begin: paddle.lgamma(Tensor([254017, 100],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([254017, 100],"float64"), ) 	 25401700 	 25074 	 17.880517959594727 	 17.318416357040405 	 0.7284917831420898 	 0.7059426307678223 	 34.66572403907776 	 39.81182241439819 	 1.4131371974945068 	 0.8113934993743896 	 
2025-08-06 04:27:05.199396 test begin: paddle.lgamma(Tensor([50803201, 1],"float32"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([50803201, 1],"float32"), ) 	 50803201 	 25074 	 9.999526977539062 	 9.6227388381958 	 0.40756893157958984 	 0.39170217514038086 	 24.1385600566864 	 37.873626708984375 	 0.9838323593139648 	 0.7718315124511719 	 
2025-08-06 04:28:30.603162 test begin: paddle.linalg.cond(x=Tensor([4, 396901, 4, 4],"float64"), p=-1, )
[Prof] paddle.linalg.cond 	 paddle.linalg.cond(x=Tensor([4, 396901, 4, 4],"float64"), p=-1, ) 	 25401664 	 1000 	 78.41220331192017 	 3.48970890045166 	 0.0012364387512207031 	 0.05355501174926758 	 None 	 None 	 None 	 None 	 
[Error] one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.DoubleTensor [4, 396901, 4, 4]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
2025-08-06 04:30:16.180499 test begin: paddle.linalg.cond(x=Tensor([4, 396901, 4, 4],"float64"), p=-math.inf, )
[Prof] paddle.linalg.cond 	 paddle.linalg.cond(x=Tensor([4, 396901, 4, 4],"float64"), p=-math.inf, ) 	 25401664 	 1000 	 79.37908029556274 	 3.4776065349578857 	 0.0011620521545410156 	 0.05331993103027344 	 None 	 None 	 None 	 None 	 
[Error] one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.DoubleTensor [4, 396901, 4, 4]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
2025-08-06 04:32:01.540130 test begin: paddle.linalg.cond(x=Tensor([793801, 2, 4, 4],"float64"), p=-1, )
[Prof] paddle.linalg.cond 	 paddle.linalg.cond(x=Tensor([793801, 2, 4, 4],"float64"), p=-1, ) 	 25401632 	 1000 	 84.84513854980469 	 3.490415334701538 	 0.0011773109436035156 	 0.05358457565307617 	 None 	 None 	 None 	 None 	 
[Error] one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.DoubleTensor [793801, 2, 4, 4]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
2025-08-06 04:33:51.100503 test begin: paddle.linalg.cond(x=Tensor([793801, 2, 4, 4],"float64"), p=-math.inf, )
[Prof] paddle.linalg.cond 	 paddle.linalg.cond(x=Tensor([793801, 2, 4, 4],"float64"), p=-math.inf, ) 	 25401632 	 1000 	 77.05574893951416 	 3.485158681869507 	 0.0011625289916992188 	 0.053293704986572266 	 None 	 None 	 None 	 None 	 
[Error] one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.DoubleTensor [793801, 2, 4, 4]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
2025-08-06 04:35:34.703163 test begin: paddle.linalg.corrcoef(Tensor([4, 12700801],"float32"), )
[Prof] paddle.linalg.corrcoef 	 paddle.linalg.corrcoef(Tensor([4, 12700801],"float32"), ) 	 50803204 	 5085 	 14.681790590286255 	 11.86683464050293 	 0.16432881355285645 	 0.0020132064819335938 	 23.065367221832275 	 17.503321170806885 	 0.10404467582702637 	 0.06362390518188477 	 
2025-08-06 04:36:45.598639 test begin: paddle.linalg.corrcoef(Tensor([4, 6350401],"float64"), )
[Prof] paddle.linalg.corrcoef 	 paddle.linalg.corrcoef(Tensor([4, 6350401],"float64"), ) 	 25401604 	 5085 	 10.029513835906982 	 6.427149772644043 	 0.11209249496459961 	 0.0009479522705078125 	 25.1659893989563 	 17.01757836341858 	 0.15333795547485352 	 0.07855486869812012 	 
2025-08-06 04:37:47.366983 test begin: paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=None, aweights=Tensor([50803201],"int32"), )
W0806 04:37:57.238765 124630 backward.cc:462] While running Node (CastGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=None, aweights=Tensor([50803201],"int32"), ) 	 50803401 	 18486 	 9.86392092704773 	 7.200762033462524 	 5.1975250244140625e-05 	 0.0001125335693359375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:38:04.446247 test begin: paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int64"), aweights=Tensor([25401601],"float64"), )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int64"), aweights=Tensor([25401601],"float64"), ) 	 25401811 	 18486 	 14.577536344528198 	 7.729799270629883 	 4.1484832763671875e-05 	 0.00010609626770019531 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:38:35.892286 test begin: paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([25401601],"int64"), aweights=Tensor([10],"float64"), )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([25401601],"int64"), aweights=Tensor([10],"float64"), ) 	 25401811 	 18486 	 18.314093351364136 	 10.162480115890503 	 4.076957702636719e-05 	 0.00020456314086914062 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:39:15.473696 test begin: paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([50803201],"int32"), aweights=None, )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([50803201],"int32"), aweights=None, ) 	 50803401 	 18486 	 14.500688791275024 	 8.892614603042603 	 4.4345855712890625e-05 	 0.00013589859008789062 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:39:48.298660 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=None, aweights=Tensor([10],"int32"), )
W0806 04:40:29.252887 124657 backward.cc:462] While running Node (CastGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=None, aweights=Tensor([10],"int32"), ) 	 25401630 	 18486 	 39.99348306655884 	 31.391592979431152 	 0.0015671253204345703 	 0.0009164810180664062 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:41:00.745852 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=None, aweights=Tensor([1270081],"int32"), )
W0806 04:41:41.201521 124673 backward.cc:462] While running Node (CastGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=None, aweights=Tensor([1270081],"int32"), ) 	 26671701 	 18486 	 39.510091066360474 	 31.747292041778564 	 0.0015149116516113281 	 0.0008990764617919922 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:42:13.226476 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int32"), aweights=None, )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int32"), aweights=None, ) 	 25401630 	 18486 	 41.75486969947815 	 31.519397735595703 	 0.0016667842864990234 	 0.0009217262268066406 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:44:40.651228 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int64"), aweights=Tensor([10],"float64"), )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int64"), aweights=Tensor([10],"float64"), ) 	 25401640 	 18486 	 44.749733209609985 	 33.85726618766785 	 0.0016613006591796875 	 0.0008962154388427734 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:47:14.798495 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([1270081],"int32"), aweights=None, )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([1270081],"int32"), aweights=None, ) 	 26671701 	 18486 	 41.81659746170044 	 31.09972882270813 	 0.0016818046569824219 	 0.0009071826934814453 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:49:37.818672 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([1270081],"int64"), aweights=Tensor([1270081],"float64"), )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([1270081],"int64"), aweights=Tensor([1270081],"float64"), ) 	 27941782 	 18486 	 44.57592248916626 	 32.784987688064575 	 0.0016644001007080078 	 0.0009181499481201172 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:52:11.180585 test begin: paddle.linalg.det(Tensor([12737, 3, 5, 5],"float32"), )
[Prof] paddle.linalg.det 	 paddle.linalg.det(Tensor([12737, 3, 5, 5],"float32"), ) 	 955275 	 1000 	 11.253451108932495 	 0.1280512809753418 	 6.723403930664062e-05 	 8.988380432128906e-05 	 2.0103847980499268 	 0.22848749160766602 	 9.131431579589844e-05 	 4.887580871582031e-05 	 
2025-08-06 04:52:26.403468 test begin: paddle.linalg.det(Tensor([3, 12737, 5, 5],"float32"), )
[Prof] paddle.linalg.det 	 paddle.linalg.det(Tensor([3, 12737, 5, 5],"float32"), ) 	 955275 	 1000 	 11.648492574691772 	 0.1251530647277832 	 0.00010895729064941406 	 6.365776062011719e-05 	 3.0831456184387207 	 0.22843289375305176 	 6.818771362304688e-05 	 3.3855438232421875e-05 	 
2025-08-06 04:52:43.066537 test begin: paddle.linalg.inv(x=Tensor([5, 31752, 4, 4],"float64"), )
[Prof] paddle.linalg.inv 	 paddle.linalg.inv(x=Tensor([5, 31752, 4, 4],"float64"), ) 	 2540160 	 1322 	 10.656800746917725 	 0.4596853256225586 	 5.984306335449219e-05 	 7.748603820800781e-05 	 7.133759021759033 	 2.5973780155181885 	 0.9201641082763672 	 0.2867882251739502 	 
2025-08-06 04:53:04.049654 test begin: paddle.linalg.inv(x=Tensor([52920, 3, 4, 4],"float64"), )
[Prof] paddle.linalg.inv 	 paddle.linalg.inv(x=Tensor([52920, 3, 4, 4],"float64"), ) 	 2540160 	 1322 	 9.584462881088257 	 0.4511876106262207 	 7.152557373046875e-05 	 6.461143493652344e-05 	 7.138068675994873 	 2.598236560821533 	 0.9207625389099121 	 0.28688955307006836 	 
2025-08-06 04:53:23.938143 test begin: paddle.linalg.lu(Tensor([103, 5, 5, 5],"float64"), )
/usr/local/lib/python3.10/dist-packages/torch/functional.py:2162: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
LU, pivots = torch.lu(A, compute_pivots)
should be replaced with
LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
and
LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
should be replaced with
LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2055.)
  return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([103, 5, 5, 5],"float64"), ) 	 12875 	 22178 	 296.90801453590393 	 0.8781013488769531 	 0.00010204315185546875 	 6.508827209472656e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:00:11.008712 test begin: paddle.linalg.lu(Tensor([106, 5, 5, 5],"float32"), )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([106, 5, 5, 5],"float32"), ) 	 13250 	 22178 	 299.43746519088745 	 0.8628747463226318 	 9.775161743164062e-05 	 6.628036499023438e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:06:56.487175 test begin: paddle.linalg.lu(Tensor([3, 138, 5, 5],"float64"), )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 138, 5, 5],"float64"), ) 	 10350 	 22178 	 236.26122951507568 	 1.14935302734375 	 0.00010943412780761719 	 8.988380432128906e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:12:39.354593 test begin: paddle.linalg.lu(Tensor([3, 177, 5, 5],"float32"), )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 177, 5, 5],"float32"), ) 	 13275 	 22178 	 296.7731349468231 	 0.8645725250244141 	 0.00010991096496582031 	 5.91278076171875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:19:21.229433 test begin: paddle.linalg.lu(Tensor([3, 177, 5, 5],"float32"), pivot=True, get_infos=True, )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 177, 5, 5],"float32"), pivot=True, get_infos=True, ) 	 13275 	 22178 	 295.94943356513977 	 0.8502554893493652 	 0.00010013580322265625 	 6.651878356933594e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:26:01.883695 test begin: paddle.linalg.lu(Tensor([3, 5, 138, 5],"float64"), )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 5, 138, 5],"float64"), ) 	 10350 	 22178 	 10.945056676864624 	 2.6682024002075195 	 8.177757263183594e-05 	 0.0001735687255859375 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:26:26.131536 test begin: paddle.linalg.lu(Tensor([3, 5, 177, 5],"float32"), )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 5, 177, 5],"float32"), ) 	 13275 	 22178 	 10.336033344268799 	 2.6767423152923584 	 5.7220458984375e-05 	 7.43865966796875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:26:49.993697 test begin: paddle.linalg.lu(Tensor([3, 5, 177, 5],"float32"), pivot=True, get_infos=True, )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 5, 177, 5],"float32"), pivot=True, get_infos=True, ) 	 13275 	 22178 	 10.345906019210815 	 2.6390538215637207 	 9.870529174804688e-05 	 5.6743621826171875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:27:13.733289 test begin: paddle.linalg.lu(Tensor([3, 5, 5, 138],"float64"), )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 5, 5, 138],"float64"), ) 	 10350 	 22178 	 12.5406494140625 	 3.729323387145996 	 0.0001227855682373047 	 9.441375732421875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:27:41.196563 test begin: paddle.linalg.lu(Tensor([3, 5, 5, 177],"float32"), )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 5, 5, 177],"float32"), ) 	 13275 	 22178 	 12.760567426681519 	 3.8942711353302 	 0.00010466575622558594 	 0.00010180473327636719 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:28:10.505353 test begin: paddle.linalg.lu(Tensor([3, 5, 5, 177],"float32"), pivot=True, get_infos=True, )
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 5, 5, 177],"float32"), pivot=True, get_infos=True, ) 	 13275 	 22178 	 12.445419073104858 	 3.888485908508301 	 9.965896606445312e-05 	 0.0001819133758544922 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:28:37.897601 test begin: paddle.linalg.lu_unpack(Tensor([20321, 5, 5, 5],"float32"), Tensor([203, 5, 5],"int32"), )
[Prof] paddle.linalg.lu_unpack 	 paddle.linalg.lu_unpack(Tensor([20321, 5, 5, 5],"float32"), Tensor([203, 5, 5],"int32"), ) 	 2545200 	 10765 	 145.90637850761414 	 1.208188533782959 	 0.00010800361633300781 	 0.014093399047851562 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([203, 5, 5, 5]) and output[0] has a shape of torch.Size([20321, 5, 5, 5]).
2025-08-06 05:31:10.329775 test begin: paddle.linalg.lu_unpack(Tensor([20321, 5, 5, 5],"float64"), Tensor([203, 5, 5],"int32"), )
[Prof] paddle.linalg.lu_unpack 	 paddle.linalg.lu_unpack(Tensor([20321, 5, 5, 5],"float64"), Tensor([203, 5, 5],"int32"), ) 	 2545200 	 10765 	 92.94579219818115 	 1.584355354309082 	 0.00010037422180175781 	 0.018771648406982422 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([203, 5, 5, 5]) and output[0] has a shape of torch.Size([20321, 5, 5, 5]).
2025-08-06 05:32:48.511836 test begin: paddle.linalg.lu_unpack(Tensor([3, 5, 5, 338689],"float64"), Tensor([3, 5, 5],"int32"), )
[Prof] paddle.linalg.lu_unpack 	 paddle.linalg.lu_unpack(Tensor([3, 5, 5, 338689],"float64"), Tensor([3, 5, 5],"int32"), ) 	 25401750 	 10765 	 10.16434645652771 	 3.8866257667541504 	 9.465217590332031e-05 	 0.04618644714355469 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:33:19.824402 test begin: paddle.linalg.lu_unpack(Tensor([3, 5, 5, 677377],"float32"), Tensor([3, 5, 5],"int32"), )
[Prof] paddle.linalg.lu_unpack 	 paddle.linalg.lu_unpack(Tensor([3, 5, 5, 677377],"float32"), Tensor([3, 5, 5],"int32"), ) 	 50803350 	 10765 	 12.910833597183228 	 4.3579490184783936 	 0.0001010894775390625 	 0.051766395568847656 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:33:59.665728 test begin: paddle.linalg.lu_unpack(Tensor([4064, 5, 5, 5],"float32"), Tensor([406, 5, 5],"int32"), )
[Prof] paddle.linalg.lu_unpack 	 paddle.linalg.lu_unpack(Tensor([4064, 5, 5, 5],"float32"), Tensor([406, 5, 5],"int32"), ) 	 518150 	 10765 	 181.76025557518005 	 0.7189164161682129 	 0.000102996826171875 	 6.628036499023438e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([406, 5, 5, 5]) and output[0] has a shape of torch.Size([4064, 5, 5, 5]).
2025-08-06 05:37:03.237546 test begin: paddle.linalg.lu_unpack(Tensor([6773, 5, 5, 3],"float32"), Tensor([277, 5, 3],"int32"), )
[Prof] paddle.linalg.lu_unpack 	 paddle.linalg.lu_unpack(Tensor([6773, 5, 5, 3],"float32"), Tensor([277, 5, 3],"int32"), ) 	 512130 	 10765 	 139.58160638809204 	 1.0705466270446777 	 0.00011873245239257812 	 8.630752563476562e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([277, 5, 5, 5]) and output[0] has a shape of torch.Size([6773, 5, 5, 5]).
2025-08-06 05:39:25.221830 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 4233601],"float64"), p="fro", axis=list[0,1,], keepdim=False, )
[Prof] paddle.linalg.matrix_norm 	 paddle.linalg.matrix_norm(x=Tensor([2, 3, 4233601],"float64"), p="fro", axis=list[0,1,], keepdim=False, ) 	 25401606 	 42429 	 9.926162004470825 	 7.456346273422241 	 0.11957478523254395 	 0.1794571876525879 	 54.95686483383179 	 53.25678515434265 	 0.44120240211486816 	 0.3207559585571289 	 
2025-08-06 05:41:32.240641 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 846721, 5],"float64"), p=-2, axis=list[1,2,], keepdim=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4078f57850>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 05:51:42.898246 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 846721, 5],"float64"), p=-2, axis=list[1,2,], keepdim=True, )
W0806 05:51:43.590497 126585 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbd96402cb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 06:01:47.584283 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3175201, 4],"float64"), p="fro", axis=list[0,1,], keepdim=False, )
W0806 06:01:48.281534 126900 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.linalg.matrix_norm 	 paddle.linalg.matrix_norm(x=Tensor([2, 3175201, 4],"float64"), p="fro", axis=list[0,1,], keepdim=False, ) 	 25401608 	 42429 	 223.3493094444275 	 6.734345436096191 	 1.7948377132415771 	 0.0810542106628418 	 45.92315125465393 	 38.49673891067505 	 0.3686380386352539 	 0.23194527626037598 	 
2025-08-06 06:07:03.786953 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 635041, 4, 5],"float64"), p=-2, axis=list[1,2,], keepdim=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fae7b006da0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 06:17:08.527383 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 635041, 4, 5],"float64"), p=-2, axis=list[1,2,], keepdim=True, )
W0806 06:17:09.269690 127474 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff4f4a52cb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 06:27:13.006217 test begin: paddle.linalg.matrix_norm(x=Tensor([2116801, 3, 4],"float64"), p="fro", axis=list[0,1,], keepdim=False, )
W0806 06:27:13.659165 127780 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.linalg.matrix_norm 	 paddle.linalg.matrix_norm(x=Tensor([2116801, 3, 4],"float64"), p="fro", axis=list[0,1,], keepdim=False, ) 	 25401612 	 42429 	 223.36861896514893 	 6.74051570892334 	 1.795048475265503 	 0.08117079734802246 	 45.95001816749573 	 38.508728981018066 	 0.36893463134765625 	 0.2319936752319336 	 
2025-08-06 06:32:30.353150 test begin: paddle.linalg.matrix_power(x=Tensor([2068, 2, 3, 2, 1, 32, 32],"float64"), n=-10, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([2068, 2, 3, 2, 1, 32, 32],"float64"), n=-10, ) 	 25411584 	 2109 	 15.163034915924072 	 13.179177522659302 	 0.003223896026611328 	 0.0015308856964111328 	 43.30718207359314 	 14.251324653625488 	 0.016379117965698242 	 0.460188627243042 	 
2025-08-06 06:33:57.938280 test begin: paddle.linalg.matrix_power(x=Tensor([2068, 2, 3, 2, 1, 32, 32],"float64"), n=-2, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([2068, 2, 3, 2, 1, 32, 32],"float64"), n=-2, ) 	 25411584 	 2109 	 9.913074016571045 	 10.581189155578613 	 0.00031948089599609375 	 0.000362396240234375 	 13.014891624450684 	 5.5177741050720215 	 0.002210855484008789 	 0.44544410705566406 	 
2025-08-06 06:34:39.714921 test begin: paddle.linalg.matrix_power(x=Tensor([3, 1379, 3, 2, 1, 32, 32],"float64"), n=-10, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 1379, 3, 2, 1, 32, 32],"float64"), n=-10, ) 	 25417728 	 2109 	 15.163353681564331 	 13.223460674285889 	 0.003223419189453125 	 0.0015327930450439453 	 43.32485842704773 	 14.25544023513794 	 0.016401290893554688 	 0.46036195755004883 	 
2025-08-06 06:36:10.337273 test begin: paddle.linalg.matrix_power(x=Tensor([3, 1379, 3, 2, 1, 32, 32],"float64"), n=-2, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 1379, 3, 2, 1, 32, 32],"float64"), n=-2, ) 	 25417728 	 2109 	 9.936289310455322 	 10.593331813812256 	 0.00031757354736328125 	 0.0003650188446044922 	 13.01996397972107 	 5.519076824188232 	 0.0022149085998535156 	 0.4456210136413574 	 
2025-08-06 06:36:50.656552 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 2005, 6, 1, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 2005, 6, 1, 11, 4, 4],"float64"), n=3, ) 	 25407360 	 2109 	 36.72772717475891 	 36.04733872413635 	 0.3431246280670166 	 0.35038161277770996 	 97.95131325721741 	 89.62857937812805 	 0.36282873153686523 	 0.4258453845977783 	 
2025-08-06 06:41:14.676073 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 1719, 1, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 1719, 1, 11, 4, 4],"float64"), n=3, ) 	 25413696 	 2109 	 36.716283321380615 	 36.03435778617859 	 0.342609167098999 	 0.3504631519317627 	 97.70033383369446 	 89.71417093276978 	 0.3629434108734131 	 0.4263877868652344 	 
2025-08-06 06:45:39.775220 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 6, 1, 3151, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 6, 1, 3151, 4, 4],"float64"), n=3, ) 	 25409664 	 2109 	 37.577208042144775 	 36.048175573349 	 0.3436155319213867 	 0.3503396511077881 	 97.71524262428284 	 89.83890461921692 	 0.3644287586212158 	 0.4280083179473877 	 
2025-08-06 06:50:07.108222 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 6, 287, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 6, 287, 11, 4, 4],"float64"), n=3, ) 	 25458048 	 2109 	 36.87913656234741 	 36.08881998062134 	 0.3445737361907959 	 0.3508913516998291 	 97.91612648963928 	 89.78952097892761 	 0.36364054679870605 	 0.42661380767822266 	 
2025-08-06 06:54:31.949224 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2068, 2, 1, 32, 32],"float64"), n=-10, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2068, 2, 1, 32, 32],"float64"), n=-10, ) 	 25411584 	 2109 	 15.187642812728882 	 13.20458722114563 	 0.003255128860473633 	 0.0015306472778320312 	 43.34196090698242 	 14.252291440963745 	 0.016398191452026367 	 0.460277795791626 	 
2025-08-06 06:55:59.316339 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2068, 2, 1, 32, 32],"float64"), n=-2, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2068, 2, 1, 32, 32],"float64"), n=-2, ) 	 25411584 	 2109 	 9.946157455444336 	 10.599584102630615 	 0.0003173351287841797 	 0.00036644935607910156 	 13.037007093429565 	 5.517789840698242 	 0.0022149085998535156 	 0.44556403160095215 	 
2025-08-06 06:56:39.781161 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 1379, 1, 32, 32],"float64"), n=-10, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 1379, 1, 32, 32],"float64"), n=-10, ) 	 25417728 	 2109 	 15.18016505241394 	 13.194866418838501 	 0.0032203197479248047 	 0.0015208721160888672 	 43.34529709815979 	 14.255578517913818 	 0.016412019729614258 	 0.4603872299194336 	 
2025-08-06 06:58:14.851135 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 1379, 1, 32, 32],"float64"), n=-2, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 1379, 1, 32, 32],"float64"), n=-2, ) 	 25417728 	 2109 	 9.930858373641968 	 10.613353729248047 	 0.00031876564025878906 	 0.00036406517028808594 	 13.033791542053223 	 5.519089221954346 	 0.002216339111328125 	 0.4455888271331787 	 
2025-08-06 06:58:55.360037 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 690, 32, 32],"float64"), n=-10, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 690, 32, 32],"float64"), n=-10, ) 	 25436160 	 2109 	 15.181662559509277 	 13.205095052719116 	 0.00322723388671875 	 0.0015347003936767578 	 43.37153887748718 	 14.26424503326416 	 0.016406774520874023 	 0.46057653427124023 	 
2025-08-06 07:00:22.584268 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 690, 32, 32],"float64"), n=-2, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 690, 32, 32],"float64"), n=-2, ) 	 25436160 	 2109 	 9.97461462020874 	 10.60353422164917 	 0.00031685829162597656 	 0.0003693103790283203 	 13.041911125183105 	 5.520541429519653 	 0.0022182464599609375 	 0.4457125663757324 	 
2025-08-06 07:01:03.136703 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 573, 7, 6, 1, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 573, 7, 6, 1, 11, 4, 4],"float64"), n=3, ) 	 25413696 	 2109 	 36.75235176086426 	 36.04365801811218 	 0.343341588973999 	 0.35025668144226074 	 97.90259408950806 	 89.90214490890503 	 0.363955020904541 	 0.42812037467956543 	 
2025-08-06 07:05:28.766300 test begin: paddle.linalg.matrix_power(x=Tensor([3, 573, 2, 7, 6, 1, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 573, 2, 7, 6, 1, 11, 4, 4],"float64"), n=3, ) 	 25413696 	 2109 	 36.76754927635193 	 36.060402631759644 	 0.3434755802154541 	 0.35040807723999023 	 97.87504124641418 	 89.82176947593689 	 0.36441731452941895 	 0.42611217498779297 	 
2025-08-06 07:09:51.438590 test begin: paddle.linalg.matrix_power(x=Tensor([860, 2, 2, 7, 6, 1, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([860, 2, 2, 7, 6, 1, 11, 4, 4],"float64"), n=3, ) 	 25428480 	 2109 	 36.83587884902954 	 36.03778886795044 	 0.3497593402862549 	 0.3502371311187744 	 98.0873396396637 	 90.01224708557129 	 0.36888861656188965 	 0.42763233184814453 	 
2025-08-06 07:14:15.677753 test begin: paddle.linalg.matrix_transpose(Tensor([20, 3, 8467201],"float32"), )
[Prof] paddle.linalg.matrix_transpose 	 paddle.linalg.matrix_transpose(Tensor([20, 3, 8467201],"float32"), ) 	 508032060 	 2337440 	 10.625482082366943 	 8.806481838226318 	 7.271766662597656e-05 	 0.00024271011352539062 	 98.24629616737366 	 134.71243619918823 	 0.00010180473327636719 	 0.0002205371856689453 	 combined
2025-08-06 07:18:45.947023 test begin: paddle.linalg.matrix_transpose(Tensor([20, 6350401, 4],"float32"), )
[Prof] paddle.linalg.matrix_transpose 	 paddle.linalg.matrix_transpose(Tensor([20, 6350401, 4],"float32"), ) 	 508032080 	 2337440 	 10.540432453155518 	 8.837463855743408 	 5.53131103515625e-05 	 0.00024247169494628906 	 97.41926145553589 	 136.3397831916809 	 9.751319885253906e-05 	 0.0002181529998779297 	 combined
2025-08-06 07:23:17.311426 test begin: paddle.linalg.matrix_transpose(Tensor([42336010, 3, 4],"float32"), )
[Prof] paddle.linalg.matrix_transpose 	 paddle.linalg.matrix_transpose(Tensor([42336010, 3, 4],"float32"), ) 	 508032120 	 2337440 	 10.555250883102417 	 17.101526975631714 	 5.7697296142578125e-05 	 0.0002903938293457031 	 110.6337583065033 	 136.44634366035461 	 0.00010347366333007812 	 0.000217437744140625 	 combined
2025-08-06 07:28:09.451871 test begin: paddle.linalg.multi_dot(list[Tensor([25401601],"float64"),Tensor([25401601, 31],"float64"),], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f59f0cd7b20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 07:38:18.864932 test begin: paddle.linalg.multi_dot(list[Tensor([4, 4],"float64"),Tensor([4, 6350401],"float64"),], )
W0806 07:38:19.480755 129896 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([4, 4],"float64"),Tensor([4, 6350401],"float64"),], ) 	 25401620 	 60322 	 46.63876461982727 	 46.699256896972656 	 0.11286449432373047 	 0.11301302909851074 	 122.47815442085266 	 123.46112728118896 	 0.23059630393981934 	 0.23241662979125977 	 
2025-08-06 07:44:03.386875 test begin: paddle.linalg.multi_dot(list[Tensor([4233601, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 5],"float64"),], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff35ea66da0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 07:54:07.906056 test begin: paddle.linalg.multi_dot(list[Tensor([4],"float64"),Tensor([4, 6350401],"float64"),], )
W0806 07:54:08.607316 130510 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([4],"float64"),Tensor([4, 6350401],"float64"),], ) 	 25401608 	 60322 	 11.684216022491455 	 11.680476665496826 	 0.1979684829711914 	 0.1979212760925293 	 34.394550800323486 	 34.08337926864624 	 0.06477665901184082 	 0.0641474723815918 	 
2025-08-06 07:55:42.141506 test begin: paddle.linalg.multi_dot(list[Tensor([6350401, 4],"float64"),Tensor([4, 31],"float64"),], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f719d412200>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:05:46.806662 test begin: paddle.linalg.multi_dot(list[Tensor([8, 3175201],"float64"),Tensor([3175201, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 5],"float64"),], )
W0806 08:05:48.495651 130983 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 3175201],"float64"),Tensor([3175201, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 5],"float64"),], ) 	 34927243 	 60322 	 24.212167978286743 	 24.590523719787598 	 0.10238909721374512 	 0.10397648811340332 	 118.10002279281616 	 97.3395643234253 	 0.1111903190612793 	 0.1371610164642334 	 
2025-08-06 08:10:20.418124 test begin: paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 6350401],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 6350401],"float64"),], ) 	 25401682 	 60322 	 82.0967788696289 	 84.97705054283142 	 0.15467238426208496 	 0.16010785102844238 	 209.13922119140625 	 128.4084882736206 	 0.1475210189819336 	 0.1669936180114746 	 
2025-08-06 08:18:46.807262 test begin: paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 5080321],"float64"),Tensor([5080321, 5],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 5080321],"float64"),Tensor([5080321, 5],"float64"),], ) 	 40642634 	 60322 	 39.197348833084106 	 38.28758525848389 	 0.16571712493896484 	 0.16175317764282227 	 184.2287561893463 	 150.83837246894836 	 0.15610790252685547 	 0.18248438835144043 	 
2025-08-06 08:25:41.709068 test begin: paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 8467201],"float64"),Tensor([8467201, 5],"float64"),], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7efd07e0a8c0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:35:47.168347 test begin: paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 4233601],"float64"),Tensor([4233601, 4],"float64"),Tensor([4, 5],"float64"),], )
W0806 08:35:48.113634 132167 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 4233601],"float64"),Tensor([4233601, 4],"float64"),Tensor([4, 5],"float64"),], ) 	 42336078 	 60322 	 31.810819387435913 	 31.94549322128296 	 0.13444948196411133 	 0.13504838943481445 	 155.64533972740173 	 128.82830715179443 	 0.1318497657775879 	 0.155914306640625 	 
2025-08-06 08:41:38.388627 test begin: paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 6350401],"float64"),Tensor([6350401, 4],"float64"),Tensor([4, 5],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 6350401],"float64"),Tensor([6350401, 4],"float64"),Tensor([4, 5],"float64"),], ) 	 63504078 	 60322 	 46.578863859176636 	 46.866899490356445 	 0.1968379020690918 	 0.19814586639404297 	 231.45478439331055 	 192.66032338142395 	 0.1641695499420166 	 0.18204426765441895 	 
2025-08-06 08:50:20.978516 test begin: paddle.linalg.multi_dot(list[Tensor([8, 8467201],"float64"),Tensor([8467201, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 5],"float64"),], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbbc31ae8f0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:00:25.821834 test begin: paddle.linalg.multi_dot(list[Tensor([819407],"float64"),Tensor([819407, 31],"float64"),], )
W0806 09:00:26.453379 133153 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([819407],"float64"),Tensor([819407, 31],"float64"),], ) 	 26221024 	 60322 	 10.12595510482788 	 10.007568836212158 	 0.08576226234436035 	 0.08471131324768066 	 26.938230514526367 	 20.334534645080566 	 0.22823119163513184 	 0.17216181755065918 	 
2025-08-06 09:01:44.476942 test begin: paddle.linalg.norm(Tensor([12700801, 1, 4],"float32"), p=1.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([12700801, 1, 4],"float32"), p=1.0, axis=-1, ) 	 50803204 	 66479 	 26.914331197738647 	 32.4776291847229 	 0.41369199752807617 	 0.49933743476867676 	 132.08974266052246 	 42.502952337265015 	 2.0289318561553955 	 0.3266329765319824 	 
2025-08-06 09:05:43.096862 test begin: paddle.linalg.norm(Tensor([25402, 50, 20],"float64"), p=2.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([25402, 50, 20],"float64"), p=2.0, axis=-1, ) 	 25402000 	 66479 	 21.25865149497986 	 17.7669677734375 	 0.1633741855621338 	 0.27306294441223145 	 97.53357481956482 	 62.17256021499634 	 1.4987831115722656 	 0.23907780647277832 	 
2025-08-06 09:09:03.993842 test begin: paddle.linalg.norm(Tensor([50, 25402, 20],"float64"), p=2.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([50, 25402, 20],"float64"), p=2.0, axis=-1, ) 	 25402000 	 66479 	 21.242286920547485 	 17.751227378845215 	 0.16326069831848145 	 0.27287888526916504 	 97.3995463848114 	 62.17466473579407 	 1.4966368675231934 	 0.23910784721374512 	 
2025-08-06 09:12:23.210070 test begin: paddle.linalg.norm(Tensor([50, 50, 10161],"float64"), p=2.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([50, 50, 10161],"float64"), p=2.0, axis=-1, ) 	 25402500 	 66479 	 10.459251642227173 	 10.081836700439453 	 0.08037281036376953 	 0.15502285957336426 	 97.83749651908875 	 60.518033504486084 	 1.5040318965911865 	 0.23266863822937012 	 
2025-08-06 09:15:25.059812 test begin: paddle.linalg.norm(Tensor([50803201],"float32"), p=2, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([50803201],"float32"), p=2, ) 	 50803201 	 66479 	 10.153137922286987 	 10.128690958023071 	 0.051924943923950195 	 0.07786369323730469 	 66.4429988861084 	 60.51243996620178 	 1.0222320556640625 	 0.23268938064575195 	 
2025-08-06 09:17:53.752265 test begin: paddle.linalg.norm(Tensor([8550, 1, 5942],"float32"), p=1.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([8550, 1, 5942],"float32"), p=1.0, axis=-1, ) 	 50804100 	 66479 	 9.996127605438232 	 10.457019805908203 	 0.15364551544189453 	 0.16075539588928223 	 127.75749111175537 	 40.279484033584595 	 1.9656822681427002 	 0.30962038040161133 	 
2025-08-06 09:21:03.101620 test begin: paddle.linalg.norm(Tensor([8550, 1486, 4],"float32"), p=1.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([8550, 1486, 4],"float32"), p=1.0, axis=-1, ) 	 50821200 	 66479 	 26.901371955871582 	 32.481205701828 	 0.41347241401672363 	 0.49925875663757324 	 131.99729418754578 	 42.51199960708618 	 2.0284547805786133 	 0.3267629146575928 	 
2025-08-06 09:24:58.067391 test begin: paddle.linalg.pinv(Tensor([21, 6, 5, 4],"float64"), rcond=1e-15, hermitian=False, )
[Prof] paddle.linalg.pinv 	 paddle.linalg.pinv(Tensor([21, 6, 5, 4],"float64"), rcond=1e-15, hermitian=False, ) 	 2520 	 1074 	 58.9301872253418 	 0.4438011646270752 	 3.790855407714844e-05 	 0.00011372566223144531 	 0.49086642265319824 	 0.3115544319152832 	 3.123283386230469e-05 	 7.271766662597656e-05 	 
2025-08-06 09:25:58.641134 test begin: paddle.linalg.pinv(Tensor([22, 20, 3],"float64"), rcond=1e-15, hermitian=False, )
[Prof] paddle.linalg.pinv 	 paddle.linalg.pinv(Tensor([22, 20, 3],"float64"), rcond=1e-15, hermitian=False, ) 	 1320 	 1074 	 10.419110774993896 	 0.40433692932128906 	 1.7881393432617188e-05 	 0.00011134147644042969 	 0.5098991394042969 	 0.3178985118865967 	 5.698204040527344e-05 	 7.009506225585938e-05 	 
2025-08-06 09:26:10.313324 test begin: paddle.linalg.pinv(Tensor([3, 22, 5, 4],"float64"), rcond=1e-15, hermitian=False, )
[Prof] paddle.linalg.pinv 	 paddle.linalg.pinv(Tensor([3, 22, 5, 4],"float64"), rcond=1e-15, hermitian=False, ) 	 1320 	 1074 	 31.99795961380005 	 0.4065113067626953 	 4.0531158447265625e-05 	 7.367134094238281e-05 	 0.4887416362762451 	 0.32635498046875 	 3.838539123535156e-05 	 6.580352783203125e-05 	 
2025-08-06 09:26:43.728418 test begin: paddle.linalg.pinv(x=Tensor([58, 4, 4],"float64"), )
[Prof] paddle.linalg.pinv 	 paddle.linalg.pinv(x=Tensor([58, 4, 4],"float64"), ) 	 928 	 1074 	 27.135144233703613 	 0.4006636142730713 	 3.3855438232421875e-05 	 8.177757263183594e-05 	 0.43753838539123535 	 0.30469202995300293 	 3.123283386230469e-05 	 7.05718994140625e-05 	 
2025-08-06 09:27:12.043351 test begin: paddle.linalg.qr(Tensor([105, 3, 50, 8],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([105, 3, 50, 8],"float64"), ) 	 126000 	 5519 	 168.27180433273315 	 58.93464994430542 	 0.00010085105895996094 	 0.003798246383666992 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:30:59.324498 test begin: paddle.linalg.qr(Tensor([112, 3, 20, 6],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([112, 3, 20, 6],"float64"), ) 	 40320 	 5519 	 158.33138418197632 	 57.450936794281006 	 9.560585021972656e-05 	 0.0034766197204589844 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:34:36.447369 test begin: paddle.linalg.qr(Tensor([2, 105, 100, 12],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 105, 100, 12],"float64"), ) 	 252000 	 5519 	 166.258638381958 	 43.165879249572754 	 9.846687316894531e-05 	 0.00411224365234375 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:38:07.079166 test begin: paddle.linalg.qr(Tensor([2, 158, 100, 8],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 158, 100, 8],"float64"), ) 	 252800 	 5519 	 169.20390820503235 	 60.9427604675293 	 0.00010538101196289062 	 0.003919124603271484 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:41:57.926895 test begin: paddle.linalg.qr(Tensor([2, 211, 100, 6],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 211, 100, 6],"float64"), ) 	 253200 	 5519 	 219.04437732696533 	 78.5905249118805 	 0.00011181831359863281 	 0.0037775039672851562 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:46:56.807452 test begin: paddle.linalg.qr(Tensor([2, 3, 100, 423],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 3, 100, 423],"float64"), ) 	 253800 	 5519 	 24.408849954605103 	 261.2871241569519 	 9.1552734375e-05 	 0.5680339336395264 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:51:42.955341 test begin: paddle.linalg.qr(Tensor([2, 3, 3528, 12],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 3, 3528, 12],"float64"), ) 	 254016 	 5519 	 9.777721166610718 	 8.361314058303833 	 6.175041198730469e-05 	 0.02001786231994629 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:52:01.106952 test begin: paddle.linalg.qr(Tensor([2, 3, 529201, 8],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 3, 529201, 8],"float64"), ) 	 25401648 	 5519 	 70.53803586959839 	 62.8257155418396 	 0.0008950233459472656 	 0.14064478874206543 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:54:15.069937 test begin: paddle.linalg.qr(Tensor([2, 3, 705601, 6],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 3, 705601, 6],"float64"), ) 	 25401636 	 5519 	 75.94020390510559 	 67.49980330467224 	 0.0009000301361083984 	 0.15218234062194824 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:56:39.132107 test begin: paddle.linalg.qr(Tensor([70, 3, 50, 12],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([70, 3, 50, 12],"float64"), ) 	 126000 	 5519 	 127.68063592910767 	 42.25116848945618 	 0.00010228157043457031 	 0.004063606262207031 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:59:29.178797 test begin: paddle.linalg.slogdet(Tensor([3, 6773, 5, 5],"float32"), )
[Prof] paddle.linalg.slogdet 	 paddle.linalg.slogdet(Tensor([3, 6773, 5, 5],"float32"), ) 	 507975 	 1538 	 9.757948875427246 	 1.0571002960205078 	 5.1021575927734375e-05 	 9.107589721679688e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 3, 6773]) and output[0] has a shape of torch.Size([3, 6773]).
2025-08-06 09:59:42.991177 test begin: paddle.linalg.slogdet(Tensor([6773, 3, 5, 5],"float32"), )
[Prof] paddle.linalg.slogdet 	 paddle.linalg.slogdet(Tensor([6773, 3, 5, 5],"float32"), ) 	 507975 	 1538 	 9.647139072418213 	 0.24425315856933594 	 0.00010085105895996094 	 6.580352783203125e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 6773, 3]) and output[0] has a shape of torch.Size([6773, 3]).
2025-08-06 09:59:54.711701 test begin: paddle.linalg.solve(x=Tensor([129601, 14, 14],"float64"), y=Tensor([129601, 14, 2],"float64"), )
[Prof] paddle.linalg.solve 	 paddle.linalg.solve(x=Tensor([129601, 14, 14],"float64"), y=Tensor([129601, 14, 2],"float64"), ) 	 29030624 	 2715 	 10.002996921539307 	 6.409769535064697 	 0.0012292861938476562 	 0.00010132789611816406 	 16.20016384124756 	 7.529339075088501 	 0.002691030502319336 	 0.25781965255737305 	 
2025-08-06 10:00:37.885477 test begin: paddle.linalg.solve(x=Tensor([14, 14],"float64"), y=Tensor([14, 1814401],"float64"), )
[Prof] paddle.linalg.solve 	 paddle.linalg.solve(x=Tensor([14, 14],"float64"), y=Tensor([14, 1814401],"float64"), ) 	 25401810 	 2715 	 13.669645547866821 	 10.561669826507568 	 0.0039174556732177734 	 0.0002880096435546875 	 16.336804151535034 	 11.217688083648682 	 0.0045413970947265625 	 0.46953392028808594 	 
2025-08-06 10:01:34.763121 test begin: paddle.linalg.solve(x=Tensor([4, 14, 14],"float64"), y=Tensor([4, 14, 453601],"float64"), )
[Prof] paddle.linalg.solve 	 paddle.linalg.solve(x=Tensor([4, 14, 14],"float64"), y=Tensor([4, 14, 453601],"float64"), ) 	 25402440 	 2715 	 11.324564218521118 	 7.8368799686431885 	 0.0030469894409179688 	 0.00011396408081054688 	 29.819572925567627 	 27.88582706451416 	 0.009495258331298828 	 0.5850963592529297 	 
2025-08-06 10:03:00.249410 test begin: paddle.linalg.solve(x=Tensor([907201, 14, 14],"float64"), y=Tensor([907201, 14, 2],"float64"), )
[Prof] paddle.linalg.solve 	 paddle.linalg.solve(x=Tensor([907201, 14, 14],"float64"), y=Tensor([907201, 14, 2],"float64"), ) 	 203213024 	 2715 	 68.75553917884827 	 42.041832447052 	 0.008588314056396484 	 0.0007119178771972656 	 111.30458807945251 	 51.52797055244446 	 0.018912792205810547 	 0.4145927429199219 	 
2025-08-06 10:07:41.604765 test begin: paddle.linalg.svdvals(Tensor([10, 3, 8467],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7265c966e0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:17:46.674602 test begin: paddle.linalg.svdvals(Tensor([10, 4233, 6],"float64"), )
W0806 10:17:46.868268 136388 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f51d10ef070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:27:51.402088 test begin: paddle.linalg.svdvals(Tensor([10, 5080],"float32"), )
W0806 10:27:51.605191 136993 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.linalg.svdvals 	 paddle.linalg.svdvals(Tensor([10, 5080],"float32"), ) 	 50800 	 21631 	 51.10692238807678 	 16.65549898147583 	 0.00011539459228515625 	 0.00030422210693359375 	 137.74805855751038 	 1.9261512756347656 	 0.00014853477478027344 	 0.0001785755157470703 	 
2025-08-06 10:31:22.502518 test begin: paddle.linalg.svdvals(Tensor([40, 6350],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0f7ea22e30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:41:27.183796 test begin: paddle.linalg.svdvals(Tensor([611, 3, 6],"float64"), )
W0806 10:41:27.390867 137899 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.linalg.svdvals 	 paddle.linalg.svdvals(Tensor([611, 3, 6],"float64"), ) 	 10998 	 21631 	 98.01406455039978 	 10.81635069847107 	 0.00010061264038085938 	 0.0001125335693359375 	 119.02849650382996 	 2.190366744995117 	 0.00012373924255371094 	 7.82012939453125e-05 	 
2025-08-06 10:45:17.989904 test begin: paddle.linalg.svdvals(Tensor([623, 12],"float32"), )
[Prof] paddle.linalg.svdvals 	 paddle.linalg.svdvals(Tensor([623, 12],"float32"), ) 	 7476 	 21631 	 9.270419597625732 	 17.78494906425476 	 8.893013000488281e-05 	 0.0002446174621582031 	 30.204400300979614 	 1.8041260242462158 	 7.104873657226562e-05 	 8.869171142578125e-05 	 
2025-08-06 10:46:17.129200 test begin: paddle.linalg.svdvals(Tensor([635, 40],"float64"), )
[Prof] paddle.linalg.svdvals 	 paddle.linalg.svdvals(Tensor([635, 40],"float64"), ) 	 25400 	 21631 	 79.67688584327698 	 62.61243772506714 	 9.131431579589844e-05 	 0.00023889541625976562 	 248.8940749168396 	 1.8296575546264648 	 8.034706115722656e-05 	 6.818771362304688e-05 	 
2025-08-06 10:52:50.171304 test begin: paddle.log(Tensor([192, 40, 6625],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([192, 40, 6625],"float32"), ) 	 50880000 	 33842 	 10.03151559829712 	 10.078676223754883 	 0.30294346809387207 	 0.3043785095214844 	 15.261754989624023 	 15.237282276153564 	 0.4609541893005371 	 0.4601874351501465 	 
2025-08-06 10:53:44.802425 test begin: paddle.log(Tensor([307, 25, 6626],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([307, 25, 6626],"float32"), ) 	 50854550 	 33842 	 10.023436546325684 	 10.075472354888916 	 0.30254292488098145 	 0.30423879623413086 	 15.257076501846313 	 15.228924751281738 	 0.46080732345581055 	 0.45992255210876465 	 
2025-08-06 10:54:38.937694 test begin: paddle.log(Tensor([64, 120, 6625],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([64, 120, 6625],"float32"), ) 	 50880000 	 33842 	 10.547712802886963 	 10.078707218170166 	 0.30298328399658203 	 0.3043849468231201 	 15.259737968444824 	 15.237516164779663 	 0.46083974838256836 	 0.46011972427368164 	 
2025-08-06 10:55:33.206146 test begin: paddle.log(Tensor([64, 120, 6626],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([64, 120, 6626],"float32"), ) 	 50887680 	 33842 	 10.02920126914978 	 10.803152561187744 	 0.30285096168518066 	 0.3043808937072754 	 15.260729551315308 	 15.23945164680481 	 0.4608945846557617 	 0.4602327346801758 	 
2025-08-06 10:56:27.207135 test begin: paddle.log(Tensor([64, 25, 31753],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([64, 25, 31753],"float32"), ) 	 50804800 	 33842 	 10.006825685501099 	 10.073917865753174 	 0.30214571952819824 	 0.3040487766265869 	 15.238762140274048 	 15.21433711051941 	 0.4603390693664551 	 0.4594876766204834 	 
2025-08-06 10:57:19.963256 test begin: paddle.log(Tensor([64, 40, 19846],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([64, 40, 19846],"float32"), ) 	 50805760 	 33842 	 10.01539397239685 	 10.066298007965088 	 0.3025076389312744 	 0.30398130416870117 	 15.242887735366821 	 15.21460771560669 	 0.46025609970092773 	 0.4594273567199707 	 
2025-08-06 10:58:15.322110 test begin: paddle.log(Tensor([64, 80, 9923],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([64, 80, 9923],"float32"), ) 	 50805760 	 33842 	 10.018192768096924 	 10.075120449066162 	 0.30241847038269043 	 0.3040752410888672 	 15.243016481399536 	 15.214437484741211 	 0.46030235290527344 	 0.45947694778442383 	 
2025-08-06 10:59:08.768820 test begin: paddle.log(Tensor([96, 80, 6625],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([96, 80, 6625],"float32"), ) 	 50880000 	 33842 	 10.026369094848633 	 11.040588617324829 	 0.30283498764038086 	 0.304337739944458 	 15.265437126159668 	 15.237842798233032 	 0.46100306510925293 	 0.460221529006958 	 
2025-08-06 11:00:03.615967 test begin: paddle.log10(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.log10 	 paddle.log10(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33848 	 10.016236066818237 	 10.066731691360474 	 0.3024289608001709 	 0.3039844036102295 	 15.240079402923584 	 25.244263172149658 	 0.46010661125183105 	 0.38105273246765137 	 
2025-08-06 11:01:09.038552 test begin: paddle.log10(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.log10 	 paddle.log10(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33848 	 10.852020263671875 	 10.436982154846191 	 0.302243709564209 	 0.30393147468566895 	 15.239696025848389 	 25.24435782432556 	 0.46021318435668945 	 0.38110899925231934 	 
2025-08-06 11:02:14.697383 test begin: paddle.log10(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.log10 	 paddle.log10(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33848 	 10.012068033218384 	 10.635179996490479 	 0.30227231979370117 	 0.30388522148132324 	 15.240220308303833 	 25.24438977241516 	 0.4601104259490967 	 0.38109254837036133 	 
2025-08-06 11:03:18.788224 test begin: paddle.log10(x=Tensor([12700801, 2],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([12700801, 2],"float64"), ) 	 25401602 	 33848 	 10.36985993385315 	 10.388388872146606 	 0.3125934600830078 	 0.31337690353393555 	 15.166274785995483 	 25.216192722320557 	 0.45804452896118164 	 0.3806784152984619 	 
2025-08-06 11:04:21.103815 test begin: paddle.log10(x=Tensor([2, 12700801],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([2, 12700801],"float64"), ) 	 25401602 	 33848 	 10.353932857513428 	 10.377418041229248 	 0.31264758110046387 	 0.3133246898651123 	 15.166618585586548 	 25.21704626083374 	 0.4575531482696533 	 0.38074278831481934 	 
2025-08-06 11:05:23.365261 test begin: paddle.log10(x=Tensor([2, 3, 2, 2116801],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([2, 3, 2, 2116801],"float64"), ) 	 25401612 	 33848 	 10.353836297988892 	 10.37881326675415 	 0.31258201599121094 	 0.3133695125579834 	 15.167335510253906 	 25.216557502746582 	 0.45812153816223145 	 0.38072752952575684 	 
2025-08-06 11:06:27.632618 test begin: paddle.log10(x=Tensor([2, 3, 2116801, 2],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([2, 3, 2116801, 2],"float64"), ) 	 25401612 	 33848 	 10.354609966278076 	 10.378840446472168 	 0.31259942054748535 	 0.31334567070007324 	 15.168469667434692 	 25.21647810935974 	 0.45798420906066895 	 0.3806321620941162 	 
2025-08-06 11:07:31.321266 test begin: paddle.log10(x=Tensor([2, 3175201, 2, 2],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([2, 3175201, 2, 2],"float64"), ) 	 25401608 	 33848 	 10.353996515274048 	 10.377117395401001 	 0.3126652240753174 	 0.3133082389831543 	 15.165393114089966 	 25.217121839523315 	 0.45797228813171387 	 0.3807506561279297 	 
2025-08-06 11:08:34.469306 test begin: paddle.log10(x=Tensor([2116801, 3, 2, 2],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([2116801, 3, 2, 2],"float64"), ) 	 25401612 	 33848 	 10.356893301010132 	 10.377142906188965 	 0.3126354217529297 	 0.3133070468902588 	 15.166223764419556 	 25.216790914535522 	 0.45792579650878906 	 0.3806781768798828 	 
2025-08-06 11:09:39.657040 test begin: paddle.log1p(Tensor([10, 16935, 300],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([10, 16935, 300],"float32"), ) 	 50805000 	 33832 	 10.815019130706787 	 10.108144044876099 	 0.30223536491394043 	 0.30534911155700684 	 15.234151601791382 	 25.233014583587646 	 0.4602346420288086 	 0.3811004161834717 	 
2025-08-06 11:10:44.630085 test begin: paddle.log1p(Tensor([10, 200, 25402],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([10, 200, 25402],"float32"), ) 	 50804000 	 33832 	 10.008074045181274 	 10.535255908966064 	 0.30228233337402344 	 0.30535197257995605 	 15.23319697380066 	 25.232385396957397 	 0.46012449264526367 	 0.3811018466949463 	 
2025-08-06 11:11:49.084323 test begin: paddle.log1p(Tensor([1016065, 5, 5],"float64"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([1016065, 5, 5],"float64"), ) 	 25401625 	 33832 	 10.321979999542236 	 12.790291786193848 	 0.31177520751953125 	 0.3437356948852539 	 15.160116910934448 	 25.204564094543457 	 0.45792531967163086 	 0.38068318367004395 	 
2025-08-06 11:12:55.076573 test begin: paddle.log1p(Tensor([108, 157920, 3],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([108, 157920, 3],"float32"), ) 	 51166080 	 33832 	 10.083393812179565 	 10.176751375198364 	 0.30457258224487305 	 0.30744385719299316 	 15.345953226089478 	 25.415587663650513 	 0.46349263191223145 	 0.3838615417480469 	 
2025-08-06 11:13:58.353900 test begin: paddle.log1p(Tensor([4, 157920, 81],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([4, 157920, 81],"float32"), ) 	 51166080 	 33832 	 10.08503532409668 	 10.177128791809082 	 0.3045957088470459 	 0.3074517250061035 	 15.340946912765503 	 25.417618989944458 	 0.4634079933166504 	 0.3838682174682617 	 
2025-08-06 11:15:01.967564 test begin: paddle.log1p(Tensor([4, 4233601, 3],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([4, 4233601, 3],"float32"), ) 	 50803212 	 33832 	 10.004910230636597 	 10.106926679611206 	 0.3022315502166748 	 0.3052792549133301 	 15.235401153564453 	 25.230428218841553 	 0.46016383171081543 	 0.38109827041625977 	 
2025-08-06 11:16:04.665449 test begin: paddle.log1p(Tensor([50000, 102, 5],"float64"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([50000, 102, 5],"float64"), ) 	 25500000 	 33832 	 10.362393379211426 	 11.422807693481445 	 0.31301307678222656 	 0.3450171947479248 	 15.22178030014038 	 25.30176591873169 	 0.45976710319519043 	 0.3821277618408203 	 
2025-08-06 11:17:08.197032 test begin: paddle.log1p(Tensor([50000, 5, 102],"float64"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([50000, 5, 102],"float64"), ) 	 25500000 	 33832 	 10.36242127418518 	 11.419975996017456 	 0.3129916191101074 	 0.3449673652648926 	 15.220067262649536 	 25.301400184631348 	 0.4599113464355469 	 0.3822183609008789 	 
2025-08-06 11:18:12.535583 test begin: paddle.log1p(Tensor([847, 200, 300],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([847, 200, 300],"float32"), ) 	 50820000 	 33832 	 10.010455131530762 	 10.605315923690796 	 0.3023712635040283 	 0.30544114112854004 	 15.242540121078491 	 25.24033761024475 	 0.4603610038757324 	 0.3811643123626709 	 
2025-08-06 11:19:17.558862 test begin: paddle.log2(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33844 	 10.640859365463257 	 10.067444324493408 	 0.30208444595336914 	 0.3040127754211426 	 15.236782312393188 	 25.239892721176147 	 0.4601621627807617 	 0.3810398578643799 	 
2025-08-06 11:20:21.777612 test begin: paddle.log2(Tensor([10, 2540161],"float64"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([10, 2540161],"float64"), ) 	 25401610 	 33844 	 10.645741701126099 	 10.37300705909729 	 0.31264448165893555 	 0.3132295608520508 	 15.162418603897095 	 25.207526206970215 	 0.4576389789581299 	 0.380603551864624 	 
2025-08-06 11:21:25.363059 test begin: paddle.log2(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33844 	 10.009071826934814 	 10.070288181304932 	 0.30225658416748047 	 0.30399537086486816 	 15.239899635314941 	 25.239271879196167 	 0.4600539207458496 	 0.38111114501953125 	 
2025-08-06 11:22:29.504662 test begin: paddle.log2(Tensor([10, 5080321],"float32"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([10, 5080321],"float32"), ) 	 50803210 	 33844 	 10.00883674621582 	 10.07471776008606 	 0.3022642135620117 	 0.30399322509765625 	 15.239678382873535 	 25.23903727531433 	 0.46031761169433594 	 0.381084680557251 	 
2025-08-06 11:23:32.595830 test begin: paddle.log2(Tensor([2116801, 12],"float64"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([2116801, 12],"float64"), ) 	 25401612 	 33844 	 10.356158256530762 	 12.711521863937378 	 0.3126680850982666 	 0.3132445812225342 	 15.168153762817383 	 25.207106113433838 	 0.45825719833374023 	 0.38057422637939453 	 
2025-08-06 11:24:40.060867 test begin: paddle.log2(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33844 	 10.968763589859009 	 10.882423162460327 	 0.3023185729980469 	 0.30399346351623535 	 15.240483045578003 	 25.239784955978394 	 0.46024036407470703 	 0.3810243606567383 	 
2025-08-06 11:25:47.217085 test begin: paddle.log2(Tensor([4233601, 12],"float32"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([4233601, 12],"float32"), ) 	 50803212 	 33844 	 10.008969068527222 	 11.489574432373047 	 0.3022594451904297 	 0.30405282974243164 	 15.240304946899414 	 25.240512132644653 	 0.460176944732666 	 0.3810896873474121 	 
2025-08-06 11:26:52.371592 test begin: paddle.logaddexp(Tensor([10, 16935, 300],"float32"), Tensor([10, 16935, 300],"float32"), )
[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([10, 16935, 300],"float32"), Tensor([10, 16935, 300],"float32"), ) 	 101610000 	 4307 	 10.918514013290405 	 1.9501755237579346 	 0.3700835704803467 	 0.4603769779205322 	 19.91746425628662 	 12.831315279006958 	 0.5246446132659912 	 0.3805727958679199 	 
2025-08-06 11:27:45.160998 test begin: paddle.logaddexp(Tensor([10, 16935, 300],"int32"), Tensor([10, 16935, 300],"int32"), )
W0806 11:27:59.205534 139854 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int32) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():7.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([10, 16935, 300],"int32"), Tensor([10, 16935, 300],"int32"), ) 	 101610000 	 4307 	 12.189308166503906 	 1.9400153160095215 	 0.36154913902282715 	 0.46031880378723145 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 11:28:01.391413 test begin: paddle.logaddexp(Tensor([10, 200, 12701],"int64"), Tensor([10, 200, 12701],"int64"), )
W0806 11:28:12.445171 139855 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int64) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():9.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([10, 200, 12701],"int64"), Tensor([10, 200, 12701],"int64"), ) 	 50804000 	 4307 	 9.990188837051392 	 0.989140510559082 	 0.29614782333374023 	 0.23469877243041992 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 11:28:13.656099 test begin: paddle.logaddexp(Tensor([10, 200, 25402],"float32"), Tensor([10, 200, 25402],"float32"), )
[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([10, 200, 25402],"float32"), Tensor([10, 200, 25402],"float32"), ) 	 101608000 	 4307 	 10.919150352478027 	 1.9490206241607666 	 0.3701002597808838 	 0.4603908061981201 	 19.915530920028687 	 12.831226587295532 	 0.5245363712310791 	 0.38057565689086914 	 
2025-08-06 11:29:01.857203 test begin: paddle.logaddexp(Tensor([10, 200, 25402],"int32"), Tensor([10, 200, 25402],"int32"), )
W0806 11:29:15.838864 139956 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int32) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():7.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([10, 200, 25402],"int32"), Tensor([10, 200, 25402],"int32"), ) 	 101608000 	 4307 	 12.189786434173584 	 1.9398887157440186 	 0.36153173446655273 	 0.4602932929992676 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 11:29:18.073723 test begin: paddle.logaddexp(Tensor([10, 8468, 300],"int64"), Tensor([10, 8468, 300],"int64"), )
W0806 11:29:29.177888 139959 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int64) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():9.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([10, 8468, 300],"int64"), Tensor([10, 8468, 300],"int64"), ) 	 50808000 	 4307 	 9.992195844650269 	 0.9889378547668457 	 0.29624009132385254 	 0.23459720611572266 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 11:29:30.337733 test begin: paddle.logaddexp(Tensor([424, 200, 300],"int64"), Tensor([424, 200, 300],"int64"), )
W0806 11:29:42.378921 139964 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int64) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():9.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([424, 200, 300],"int64"), Tensor([424, 200, 300],"int64"), ) 	 50880000 	 4307 	 10.01862382888794 	 0.9955801963806152 	 0.2968015670776367 	 0.23504352569580078 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 11:29:44.448285 test begin: paddle.logaddexp(Tensor([847, 200, 300],"float32"), Tensor([847, 200, 300],"float32"), )
[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([847, 200, 300],"float32"), Tensor([847, 200, 300],"float32"), ) 	 101640000 	 4307 	 10.922142744064331 	 1.9406909942626953 	 0.370241641998291 	 0.4604771137237549 	 19.91671633720398 	 12.835180521011353 	 0.5246672630310059 	 0.38066816329956055 	 
2025-08-06 11:30:32.576182 test begin: paddle.logaddexp(Tensor([847, 200, 300],"int32"), Tensor([847, 200, 300],"int32"), )
W0806 11:30:46.529701 139978 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int32) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():7.] (at ../paddle/phi/core/dense_tensor.cc:153)

[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([847, 200, 300],"int32"), Tensor([847, 200, 300],"int32"), ) 	 101640000 	 4307 	 12.192206859588623 	 1.9409453868865967 	 0.36157774925231934 	 0.4605252742767334 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 11:30:48.711254 test begin: paddle.logcumsumexp(Tensor([10, 10, 508033],"float32"), axis=-1, )
[Prof] paddle.logcumsumexp 	 paddle.logcumsumexp(Tensor([10, 10, 508033],"float32"), axis=-1, ) 	 50803300 	 2955 	 10.218145847320557 	 7.8389458656311035 	 3.659180164337158 	 3.036893606185913 	 32.24510478973389 	 31.99038600921631 	 1.01454496383667 	 0.5529401302337646 	 
2025-08-06 11:32:14.935108 test begin: paddle.logcumsumexp(Tensor([10, 10, 508033],"float32"), axis=0, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f87625fb820>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 11:42:34.685310 test begin: paddle.logcumsumexp(Tensor([10, 508033, 10],"float32"), axis=-1, )
W0806 11:42:42.730262 140385 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8493962ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 11:52:51.509322 test begin: paddle.logcumsumexp(Tensor([10, 508033, 10],"float32"), axis=0, )
W0806 11:52:52.456190 140833 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe5d628ae60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 12:02:59.566837 test begin: paddle.logcumsumexp(Tensor([508033, 10, 10],"float32"), axis=-1, )
W0806 12:03:00.513863 141135 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc1eb03eef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 12:13:08.015812 test begin: paddle.logical_and(Tensor([138, 369303],"bool"), Tensor([138, 369303],"bool"), )
W0806 12:13:09.548566 141431 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.logical_and 	 paddle.logical_and(Tensor([138, 369303],"bool"), Tensor([138, 369303],"bool"), ) 	 101927628 	 84691 	 10.009332418441772 	 9.802541017532349 	 0.12078666687011719 	 0.11838293075561523 	 None 	 None 	 None 	 None 	 
2025-08-06 12:13:29.585511 test begin: paddle.logical_and(Tensor([146, 349866],"bool"), Tensor([146, 349866],"bool"), )
[Prof] paddle.logical_and 	 paddle.logical_and(Tensor([146, 349866],"bool"), Tensor([146, 349866],"bool"), ) 	 102160872 	 84691 	 9.99191427230835 	 9.815654039382935 	 0.12059712409973145 	 0.11836409568786621 	 None 	 None 	 None 	 None 	 
2025-08-06 12:13:50.958170 test begin: paddle.logical_and(Tensor([49, 1036801],"bool"), Tensor([49, 1036801],"bool"), )
[Prof] paddle.logical_and 	 paddle.logical_and(Tensor([49, 1036801],"bool"), Tensor([49, 1036801],"bool"), ) 	 101606498 	 84691 	 9.941671133041382 	 9.816016435623169 	 0.1199648380279541 	 0.11844158172607422 	 None 	 None 	 None 	 None 	 
2025-08-06 12:14:12.353841 test begin: paddle.logical_and(Tensor([53, 958551],"bool"), Tensor([53, 958551],"bool"), )
[Prof] paddle.logical_and 	 paddle.logical_and(Tensor([53, 958551],"bool"), Tensor([53, 958551],"bool"), ) 	 101606406 	 84691 	 9.937207460403442 	 9.817750692367554 	 0.11990141868591309 	 0.11847281455993652 	 None 	 None 	 None 	 None 	 
2025-08-06 12:14:33.708918 test begin: paddle.logical_and(Tensor([55, 923695],"bool"), Tensor([55, 923695],"bool"), )
[Prof] paddle.logical_and 	 paddle.logical_and(Tensor([55, 923695],"bool"), Tensor([55, 923695],"bool"), ) 	 101606450 	 84691 	 10.757506370544434 	 9.81831407546997 	 0.11992549896240234 	 0.11844825744628906 	 None 	 None 	 None 	 None 	 
2025-08-06 12:14:57.633518 test begin: paddle.logical_not(Tensor([2150400, 237],"bool"), )
[Prof] paddle.logical_not 	 paddle.logical_not(Tensor([2150400, 237],"bool"), ) 	 509644800 	 12728 	 9.945752382278442 	 10.86259937286377 	 0.7986845970153809 	 0.765618085861206 	 None 	 None 	 None 	 None 	 
2025-08-06 12:15:28.192666 test begin: paddle.logical_not(Tensor([2204160, 231],"bool"), )
[Prof] paddle.logical_not 	 paddle.logical_not(Tensor([2204160, 231],"bool"), ) 	 509160960 	 12728 	 10.006633281707764 	 9.533576011657715 	 0.8034508228302002 	 0.7651963233947754 	 None 	 None 	 None 	 None 	 
2025-08-06 12:15:57.743224 test begin: paddle.logical_not(Tensor([2257920, 226],"bool"), )
[Prof] paddle.logical_not 	 paddle.logical_not(Tensor([2257920, 226],"bool"), ) 	 510289920 	 12728 	 9.958806037902832 	 9.546873092651367 	 0.7995884418487549 	 0.76664137840271 	 None 	 None 	 None 	 None 	 
2025-08-06 12:16:26.375341 test begin: paddle.logical_not(Tensor([6350410, 80],"bool"), )
[Prof] paddle.logical_not 	 paddle.logical_not(Tensor([6350410, 80],"bool"), ) 	 508032800 	 12728 	 9.919360637664795 	 9.522330045700073 	 0.7958498001098633 	 0.7646229267120361 	 None 	 None 	 None 	 None 	 
2025-08-06 12:16:52.644698 test begin: paddle.logical_or(Tensor([50803201],"bool"), Tensor([50803201],"bool"), )
[Prof] paddle.logical_or 	 paddle.logical_or(Tensor([50803201],"bool"), Tensor([50803201],"bool"), ) 	 101606402 	 85027 	 10.03755497932434 	 9.831478357315063 	 0.12067842483520508 	 0.11812901496887207 	 None 	 None 	 None 	 None 	 
2025-08-06 12:17:13.900084 test begin: paddle.logical_or(Tensor([640, 79381],"bool"), Tensor([640, 79381],"bool"), )
[Prof] paddle.logical_or 	 paddle.logical_or(Tensor([640, 79381],"bool"), Tensor([640, 79381],"bool"), ) 	 101607680 	 85027 	 10.036893129348755 	 9.751472473144531 	 0.12063860893249512 	 0.11720561981201172 	 None 	 None 	 None 	 None 	 
2025-08-06 12:17:35.054718 test begin: paddle.logical_or(Tensor([79381, 640],"bool"), Tensor([79381, 640],"bool"), )
[Prof] paddle.logical_or 	 paddle.logical_or(Tensor([79381, 640],"bool"), Tensor([79381, 640],"bool"), ) 	 101607680 	 85027 	 10.03692078590393 	 9.751259803771973 	 0.12061834335327148 	 0.1171870231628418 	 None 	 None 	 None 	 None 	 
2025-08-06 12:17:56.240445 test begin: paddle.logical_xor(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.logical_xor 	 paddle.logical_xor(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 50803600 	 52894 	 10.104755401611328 	 12.581215381622314 	 0.19522929191589355 	 0.2430713176727295 	 None 	 None 	 None 	 None 	 
2025-08-06 12:18:19.728763 test begin: paddle.logical_xor(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.logical_xor 	 paddle.logical_xor(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), ) 	 50803600 	 52894 	 9.958321571350098 	 12.59808897972107 	 0.19243216514587402 	 0.24341559410095215 	 None 	 None 	 None 	 None 	 
2025-08-06 12:18:44.713647 test begin: paddle.logical_xor(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.logical_xor 	 paddle.logical_xor(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 101606800 	 52894 	 17.266794204711914 	 17.326032400131226 	 0.3335695266723633 	 0.3347468376159668 	 None 	 None 	 None 	 None 	 
2025-08-06 12:19:21.077426 test begin: paddle.logical_xor(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.logical_xor 	 paddle.logical_xor(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), ) 	 101606420 	 52894 	 17.279813528060913 	 17.335062503814697 	 0.33389854431152344 	 0.33487725257873535 	 None 	 None 	 None 	 None 	 
2025-08-06 12:19:59.094247 test begin: paddle.logical_xor(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.logical_xor 	 paddle.logical_xor(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), ) 	 101606440 	 52894 	 17.279927968978882 	 17.329598665237427 	 0.3338508605957031 	 0.3347647190093994 	 None 	 None 	 None 	 None 	 
2025-08-06 12:20:37.845708 test begin: paddle.logit(Tensor([10, 20, 254017],"float32"), 0.001, )
[Prof] paddle.logit 	 paddle.logit(Tensor([10, 20, 254017],"float32"), 0.001, ) 	 50803400 	 33608 	 10.28534746170044 	 10.071170568466187 	 0.3040337562561035 	 0.3074812889099121 	 15.137145519256592 	 15.118763446807861 	 0.4602324962615967 	 0.4597318172454834 	 
2025-08-06 12:21:30.440550 test begin: paddle.logit(Tensor([10, 5080321, 1],"float32"), 0.001, )
[Prof] paddle.logit 	 paddle.logit(Tensor([10, 5080321, 1],"float32"), 0.001, ) 	 50803210 	 33608 	 10.015384197235107 	 10.069734334945679 	 0.3045377731323242 	 0.306225061416626 	 15.136090993881226 	 15.11882996559143 	 0.4603118896484375 	 0.45977330207824707 	 
2025-08-06 12:22:22.487528 test begin: paddle.logit(Tensor([2540161, 20, 1],"float32"), 0.001, )
[Prof] paddle.logit 	 paddle.logit(Tensor([2540161, 20, 1],"float32"), 0.001, ) 	 50803220 	 33608 	 10.015058517456055 	 10.07001519203186 	 0.3045389652252197 	 0.3062708377838135 	 15.137617349624634 	 15.118908166885376 	 0.4602510929107666 	 0.4597809314727783 	 
2025-08-06 12:23:14.551298 test begin: paddle.logit(Tensor([50803201],"float32"), 1e-08, )
[Prof] paddle.logit 	 paddle.logit(Tensor([50803201],"float32"), 1e-08, ) 	 50803201 	 33608 	 11.42046308517456 	 10.075458526611328 	 0.3045387268066406 	 0.3061830997467041 	 15.135947465896606 	 15.118858098983765 	 0.46031665802001953 	 0.4597206115722656 	 
2025-08-06 12:24:08.314923 test begin: paddle.logit(x=Tensor([4, 3, 2, 1058401],"float64"), eps=0.2, )
[Prof] paddle.logit 	 paddle.logit(x=Tensor([4, 3, 2, 1058401],"float64"), eps=0.2, ) 	 25401624 	 33608 	 10.996281862258911 	 11.185973167419434 	 0.3358452320098877 	 0.30934786796569824 	 14.906712055206299 	 15.07854175567627 	 0.4532890319824219 	 0.4584507942199707 	 
2025-08-06 12:25:03.728273 test begin: paddle.logit(x=Tensor([4, 3, 423361, 5],"float64"), eps=0.2, )
[Prof] paddle.logit 	 paddle.logit(x=Tensor([4, 3, 423361, 5],"float64"), eps=0.2, ) 	 25401660 	 33608 	 10.981123924255371 	 10.175189733505249 	 0.3346712589263916 	 0.30949974060058594 	 14.898010730743408 	 15.078587055206299 	 0.4530634880065918 	 0.45856666564941406 	 
2025-08-06 12:25:56.131574 test begin: paddle.logit(x=Tensor([4, 635041, 2, 5],"float64"), eps=0.2, )
[Prof] paddle.logit 	 paddle.logit(x=Tensor([4, 635041, 2, 5],"float64"), eps=0.2, ) 	 25401640 	 33608 	 10.956482172012329 	 10.174788236618042 	 0.33492422103881836 	 0.3093605041503906 	 14.897868156433105 	 15.07924485206604 	 0.45302724838256836 	 0.4585001468658447 	 
2025-08-06 12:26:49.576056 test begin: paddle.logit(x=Tensor([846721, 3, 2, 5],"float64"), eps=0.2, )
[Prof] paddle.logit 	 paddle.logit(x=Tensor([846721, 3, 2, 5],"float64"), eps=0.2, ) 	 25401630 	 33608 	 10.97853136062622 	 10.174704551696777 	 0.33392810821533203 	 0.30938124656677246 	 14.90603494644165 	 15.07865023612976 	 0.4533061981201172 	 0.45850634574890137 	 
2025-08-06 12:27:43.941268 test begin: paddle.logsumexp(Tensor([1024, 49613],"float32"), axis=1, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([1024, 49613],"float32"), axis=1, ) 	 50803712 	 15717 	 11.175665140151978 	 14.662353515625 	 0.10382366180419922 	 0.10622572898864746 	 12.727762699127197 	 14.226823329925537 	 0.8276424407958984 	 0.30834460258483887 	 
2025-08-06 12:28:41.775382 test begin: paddle.logsumexp(Tensor([30, 200, 8468],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([30, 200, 8468],"float32"), axis=-1, keepdim=False, ) 	 50808000 	 15717 	 9.964746952056885 	 14.739689588546753 	 0.12932872772216797 	 0.10672187805175781 	 19.029734134674072 	 14.293045997619629 	 1.237565279006958 	 0.3097808361053467 	 
2025-08-06 12:29:43.567391 test begin: paddle.logsumexp(Tensor([30, 200, 8468],"float32"), axis=list[0,2,], keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([30, 200, 8468],"float32"), axis=list[0,2,], keepdim=False, ) 	 50808000 	 15717 	 11.509998321533203 	 15.454919338226318 	 0.10689282417297363 	 0.09144854545593262 	 18.959816932678223 	 14.338247537612915 	 1.2322883605957031 	 0.31071901321411133 	 
2025-08-06 12:30:45.309729 test begin: paddle.logsumexp(Tensor([30, 42337, 40],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([30, 42337, 40],"float32"), axis=-1, keepdim=False, ) 	 50804400 	 15717 	 10.420161724090576 	 25.599063873291016 	 0.33879590034484863 	 0.18519163131713867 	 19.38413977622986 	 14.43672513961792 	 1.2604825496673584 	 0.31290197372436523 	 
2025-08-06 12:31:58.213599 test begin: paddle.logsumexp(Tensor([30, 42337, 40],"float32"), axis=list[0,2,], keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([30, 42337, 40],"float32"), axis=list[0,2,], keepdim=False, ) 	 50804400 	 15717 	 11.2872154712677 	 14.9294273853302 	 0.14653730392456055 	 0.10808730125427246 	 19.57758402824402 	 14.34843134880066 	 1.7302274703979492 	 0.311046838760376 	 
2025-08-06 12:33:00.087772 test begin: paddle.logsumexp(Tensor([6351, 200, 40],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([6351, 200, 40],"float32"), axis=-1, keepdim=False, ) 	 50808000 	 15717 	 10.43204116821289 	 25.61267852783203 	 0.33913493156433105 	 0.18524837493896484 	 19.39525866508484 	 14.43791913986206 	 1.2613725662231445 	 0.31289005279541016 	 
2025-08-06 12:34:10.835941 test begin: paddle.logsumexp(Tensor([6351, 200, 40],"float32"), axis=list[0,2,], keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([6351, 200, 40],"float32"), axis=list[0,2,], keepdim=False, ) 	 50808000 	 15717 	 11.592621803283691 	 15.54113245010376 	 0.10764813423156738 	 0.09191393852233887 	 18.97844624519348 	 14.33718490600586 	 1.2337465286254883 	 0.3107726573944092 	 
2025-08-06 12:35:12.140996 test begin: paddle.masked_fill(Tensor([20, 127009, 20],"int32"), Tensor([20, 127009, 20],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([20, 127009, 20],"int32"), Tensor([20, 127009, 20],"bool"), 0, ) 	 101607200 	 26391 	 9.975297212600708 	 17.189181804656982 	 0.09666800498962402 	 0.22175359725952148 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 12:35:48.512337 test begin: paddle.masked_fill(Tensor([20, 60, 42337],"int32"), Tensor([20, 60, 42337],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([20, 60, 42337],"int32"), Tensor([20, 60, 42337],"bool"), 0, ) 	 101608800 	 26391 	 9.948780536651611 	 17.1839702129364 	 0.09622621536254883 	 0.22165417671203613 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 12:36:26.283201 test begin: paddle.masked_fill(Tensor([28225, 60, 30],"int32"), Tensor([28225, 60, 30],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([28225, 60, 30],"int32"), Tensor([28225, 60, 30],"bool"), 0, ) 	 101610000 	 26391 	 9.966845750808716 	 17.157708883285522 	 0.0965874195098877 	 0.22135257720947266 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 12:37:05.976696 test begin: paddle.masked_fill(Tensor([30, 56449, 30],"int32"), Tensor([30, 56449, 30],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([30, 56449, 30],"int32"), Tensor([30, 56449, 30],"bool"), 0, ) 	 101608200 	 26391 	 10.718031167984009 	 17.162260055541992 	 0.0966348648071289 	 0.22139239311218262 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 12:37:43.113239 test begin: paddle.masked_fill(Tensor([30, 60, 28225],"int32"), Tensor([30, 60, 28225],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([30, 60, 28225],"int32"), Tensor([30, 60, 28225],"bool"), 0, ) 	 101610000 	 26391 	 9.96978235244751 	 17.161449909210205 	 0.0966043472290039 	 0.22137022018432617 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 12:38:19.495704 test begin: paddle.masked_fill(Tensor([42337, 60, 20],"int32"), Tensor([42337, 60, 20],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([42337, 60, 20],"int32"), Tensor([42337, 60, 20],"bool"), 0, ) 	 101608800 	 26391 	 9.929431200027466 	 19.04971408843994 	 0.09623074531555176 	 0.22159051895141602 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 12:38:59.070420 test begin: paddle.masked_scatter(Tensor([120],"float32"), Tensor([300, 120],"bool"), Tensor([169345, 300],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([120],"float32"), Tensor([300, 120],"bool"), Tensor([169345, 300],"float32"), ) 	 50839620 	 22631 	 10.270514011383057 	 0.7878661155700684 	 0.00011968612670898438 	 0.00010085105895996094 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 12:39:16.053206 test begin: paddle.masked_scatter(Tensor([120],"float32"), Tensor([300, 120],"bool"), Tensor([300, 169345],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([120],"float32"), Tensor([300, 120],"bool"), Tensor([300, 169345],"float32"), ) 	 50839620 	 22631 	 10.299967288970947 	 0.7885465621948242 	 4.4345855712890625e-05 	 0.00019621849060058594 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 12:39:32.975533 test begin: paddle.masked_scatter(Tensor([300, 40],"float32"), Tensor([40],"bool"), Tensor([169345, 300],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([300, 40],"float32"), Tensor([40],"bool"), Tensor([169345, 300],"float32"), ) 	 50815540 	 22631 	 10.455376148223877 	 0.999974250793457 	 5.245208740234375e-05 	 8.869171142578125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 12:39:49.038552 test begin: paddle.masked_scatter(Tensor([300, 40],"float32"), Tensor([40],"bool"), Tensor([300, 169345],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([300, 40],"float32"), Tensor([40],"bool"), Tensor([300, 169345],"float32"), ) 	 50815540 	 22631 	 10.74319338798523 	 1.1683151721954346 	 4.38690185546875e-05 	 0.0001919269561767578 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 12:40:05.634040 test begin: paddle.masked_scatter(Tensor([6, 8, 9, 18],"float32"), Tensor([6, 8, 9, 18],"bool"), Tensor([169345, 300],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([6, 8, 9, 18],"float32"), Tensor([6, 8, 9, 18],"bool"), Tensor([169345, 300],"float32"), ) 	 50819052 	 22631 	 9.79795503616333 	 0.7779927253723145 	 2.956390380859375e-05 	 7.200241088867188e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 12:40:20.680528 test begin: paddle.masked_scatter(Tensor([6, 8, 9, 18],"float32"), Tensor([6, 8, 9, 18],"bool"), Tensor([300, 169345],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([6, 8, 9, 18],"float32"), Tensor([6, 8, 9, 18],"bool"), Tensor([300, 169345],"float32"), ) 	 50819052 	 22631 	 9.722997426986694 	 0.7729783058166504 	 1.7881393432617188e-05 	 7.343292236328125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 12:40:36.479082 test begin: paddle.masked_select(Tensor([16, 10164, 313],"float32"), Tensor([16, 10164, 313],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([16, 10164, 313],"float32"), Tensor([16, 10164, 313],"bool"), ) 	 101802624 	 7185 	 10.316713094711304 	 22.432103157043457 	 0.0008509159088134766 	 0.0030052661895751953 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 12:41:21.937188 test begin: paddle.masked_select(Tensor([16, 11109, 286],"float32"), Tensor([16, 11109, 286],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([16, 11109, 286],"float32"), Tensor([16, 11109, 286],"bool"), ) 	 101669568 	 7185 	 9.899172306060791 	 22.41064143180847 	 0.0008451938629150391 	 0.003004789352416992 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 12:42:07.090104 test begin: paddle.masked_select(Tensor([16, 12096, 263],"float32"), Tensor([16, 12096, 263],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([16, 12096, 263],"float32"), Tensor([16, 12096, 263],"bool"), ) 	 101799936 	 7185 	 9.927623987197876 	 22.427639961242676 	 0.0008530616760253906 	 0.0030050277709960938 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 12:42:51.922599 test begin: paddle.masked_select(Tensor([16, 46695, 68],"float32"), Tensor([16, 46695, 68],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([16, 46695, 68],"float32"), Tensor([16, 46695, 68],"bool"), ) 	 101608320 	 7185 	 9.98035454750061 	 22.43298625946045 	 0.0008563995361328125 	 0.0029783248901367188 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 12:43:38.913196 test begin: paddle.masked_select(Tensor([62, 12096, 68],"float32"), Tensor([62, 12096, 68],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([62, 12096, 68],"float32"), Tensor([62, 12096, 68],"bool"), ) 	 101993472 	 7185 	 10.627360105514526 	 22.514257192611694 	 0.0008509159088134766 	 0.003017425537109375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 12:44:24.695003 test begin: paddle.masked_select(Tensor([68, 11109, 68],"float32"), Tensor([68, 11109, 68],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([68, 11109, 68],"float32"), Tensor([68, 11109, 68],"bool"), ) 	 102736032 	 7185 	 10.081712245941162 	 22.79205346107483 	 0.0008604526519775391 	 0.003044605255126953 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 12:45:11.208722 test begin: paddle.masked_select(Tensor([74, 10164, 68],"float32"), Tensor([74, 10164, 68],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([74, 10164, 68],"float32"), Tensor([74, 10164, 68],"bool"), ) 	 102290496 	 7185 	 9.982770919799805 	 22.541720151901245 	 0.0008566379547119141 	 0.0030248165130615234 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 12:45:56.225357 test begin: paddle.matmul(Tensor([1, 32, 388, 4096],"float32"), Tensor([1, 32, 4096, 128],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([1, 32, 388, 4096],"float32"), Tensor([1, 32, 4096, 128],"float32"), ) 	 67633152 	 12781 	 21.072611331939697 	 21.056100845336914 	 1.6848015785217285 	 1.684234857559204 	 23.45101308822632 	 23.444421529769897 	 0.9374904632568359 	 0.9372358322143555 	 
2025-08-06 12:47:26.581523 test begin: paddle.matmul(Tensor([1, 32, 4096, 4096],"float32"), Tensor([1, 32, 4096, 128],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8e67757fd0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 12:57:45.078202 test begin: paddle.matmul(Tensor([1, 32, 4096, 4096],"float32"), Tensor([1, 32, 4096, 388],"float32"), )
W0806 12:57:59.021847 142444 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0c7e37ef50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 13:07:53.677281 test begin: paddle.matmul(Tensor([1, 32, 4096, 4096],"float32"), Tensor([4, 32, 4096, 128],"float32"), )
W0806 13:08:02.762693 142738 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6503ca6d70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 13:18:01.991668 test begin: paddle.matmul(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 4, 4096, 128],"float32"), )
W0806 13:18:03.256716 142967 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.matmul 	 paddle.matmul(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 4, 4096, 128],"float32"), ) 	 69206016 	 12781 	 21.049537897109985 	 21.031919240951538 	 1.6835715770721436 	 1.681612253189087 	 34.219319581985474 	 34.201499700546265 	 1.3675868511199951 	 1.3673744201660156 	 
2025-08-06 13:19:54.409044 test begin: paddle.matmul(Tensor([1, 97, 4096, 4096],"float32"), Tensor([1, 97, 4096, 128],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f44d5477100>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 13:30:26.841697 test begin: paddle.matmul(Tensor([10, 23, 499, 3600],"float32"), Tensor([10, 23, 3600, 64],"float32"), )
W0806 13:30:33.834178 143221 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 23, 499, 3600],"float32"), Tensor([10, 23, 3600, 64],"float32"), ) 	 466164000 	 12781 	 82.77288770675659 	 82.76897931098938 	 6.618622779846191 	 6.6177451610565186 	 126.77074456214905 	 126.37052249908447 	 5.0672502517700195 	 5.0511345863342285 	 
2025-08-06 13:37:34.103468 test begin: paddle.matmul(Tensor([10, 3, 499, 3600],"float32"), Tensor([10, 3, 3600, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 3, 499, 3600],"float32"), Tensor([10, 3, 3600, 64],"float32"), ) 	 60804000 	 12781 	 18.467063188552856 	 18.478468418121338 	 1.47658109664917 	 1.477555751800537 	 17.919097900390625 	 17.92434000968933 	 0.7164113521575928 	 0.7166085243225098 	 
2025-08-06 13:38:47.964784 test begin: paddle.matmul(Tensor([10, 8, 177, 3600],"float32"), Tensor([10, 8, 3600, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 8, 177, 3600],"float32"), Tensor([10, 8, 3600, 64],"float32"), ) 	 69408000 	 12781 	 18.481130838394165 	 18.483572721481323 	 1.4780423641204834 	 1.478431224822998 	 18.850285053253174 	 18.85842728614807 	 0.753547191619873 	 0.7536325454711914 	 
2025-08-06 13:40:03.826381 test begin: paddle.matmul(Tensor([10, 8, 499, 1273],"float32"), Tensor([10, 8, 1273, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 8, 499, 1273],"float32"), Tensor([10, 8, 1273, 64],"float32"), ) 	 57335920 	 12781 	 9.967485904693604 	 9.965394735336304 	 0.7970883846282959 	 0.796367883682251 	 16.124290227890015 	 16.119816541671753 	 0.6446943283081055 	 0.6444323062896729 	 
2025-08-06 13:40:57.201707 test begin: paddle.matmul(Tensor([10, 8, 499, 3600],"float32"), Tensor([10, 8, 3600, 177],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 8, 499, 3600],"float32"), Tensor([10, 8, 3600, 177],"float32"), ) 	 194688000 	 12781 	 55.223509311676025 	 55.16126084327698 	 4.414976596832275 	 4.410062551498413 	 97.62172961235046 	 97.5689001083374 	 3.9016740322113037 	 3.9004709720611572 	 
2025-08-06 13:46:06.097096 test begin: paddle.matmul(Tensor([10, 8, 499, 9923],"float32"), Tensor([10, 8, 9923, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 8, 499, 9923],"float32"), Tensor([10, 8, 9923, 64],"float32"), ) 	 446931920 	 12781 	 75.97315073013306 	 75.98498892784119 	 6.077872276306152 	 6.074250936508179 	 118.4423508644104 	 118.3701822757721 	 4.733705759048462 	 4.733289480209351 	 
2025-08-06 13:52:44.457975 test begin: paddle.matmul(Tensor([1379, 4, 256, 256],"float32"), Tensor([1379, 4, 256, 36],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([1379, 4, 256, 256],"float32"), Tensor([1379, 4, 256, 36],"float32"), ) 	 412332032 	 12781 	 68.65290021896362 	 69.24750232696533 	 5.465318202972412 	 5.465555191040039 	 93.177574634552 	 93.16465878486633 	 3.725713014602661 	 3.7251510620117188 	 
2025-08-06 13:58:20.816723 test begin: paddle.matmul(Tensor([194, 4, 256, 256],"float32"), Tensor([194, 4, 256, 36],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([194, 4, 256, 256],"float32"), Tensor([194, 4, 256, 36],"float32"), ) 	 58007552 	 12781 	 10.085999250411987 	 10.085147380828857 	 0.8065695762634277 	 0.8064374923706055 	 13.70954942703247 	 13.679768085479736 	 0.5477626323699951 	 0.5468690395355225 	 
2025-08-06 13:59:09.488037 test begin: paddle.matmul(Tensor([28, 8, 499, 3600],"float32"), Tensor([28, 8, 3600, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([28, 8, 499, 3600],"float32"), Tensor([28, 8, 3600, 64],"float32"), ) 	 454003200 	 12781 	 82.61251711845398 	 82.61983704566956 	 6.605323076248169 	 6.6065919399261475 	 123.70449423789978 	 123.71046543121338 	 4.9461705684661865 	 4.947040796279907 	 
2025-08-06 14:06:10.000391 test begin: paddle.matmul(Tensor([4, 32, 4096, 4096],"float32"), Tensor([4, 32, 4096, 128],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa0104cec20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 14:16:46.861359 test begin: paddle.matmul(Tensor([4, 8, 499, 3600],"float32"), Tensor([4, 8, 3600, 64],"float32"), )
W0806 14:16:48.139444 147612 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.matmul 	 paddle.matmul(Tensor([4, 8, 499, 3600],"float32"), Tensor([4, 8, 3600, 64],"float32"), ) 	 64857600 	 12781 	 18.46658730506897 	 18.478917360305786 	 1.4769115447998047 	 1.4772346019744873 	 18.33599352836609 	 18.33919668197632 	 0.7329659461975098 	 0.7329602241516113 	 
2025-08-06 14:18:02.447894 test begin: paddle.matmul(Tensor([512, 11, 256, 256],"float32"), Tensor([512, 11, 256, 36],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([512, 11, 256, 256],"float32"), Tensor([512, 11, 256, 36],"float32"), ) 	 421003264 	 12781 	 69.75622606277466 	 69.73518061637878 	 5.578217267990112 	 5.576025485992432 	 95.09627056121826 	 95.05371165275574 	 3.799994468688965 	 3.7997071743011475 	 
2025-08-06 14:23:42.894730 test begin: paddle.matmul(Tensor([512, 2, 256, 256],"float32"), Tensor([512, 2, 256, 36],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([512, 2, 256, 256],"float32"), Tensor([512, 2, 256, 36],"float32"), ) 	 76546048 	 12781 	 12.778889894485474 	 12.780267000198364 	 1.02294921875 	 1.0228779315948486 	 17.460787534713745 	 17.451544046401978 	 0.698925256729126 	 0.6990511417388916 	 
2025-08-06 14:24:44.930507 test begin: paddle.matmul(Tensor([512, 4, 256, 256],"float32"), Tensor([512, 4, 256, 97],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([512, 4, 256, 256],"float32"), Tensor([512, 4, 256, 97],"float32"), ) 	 185073664 	 12781 	 25.571555137634277 	 25.565529584884644 	 2.0452308654785156 	 2.044943332672119 	 47.25243258476257 	 47.24354529380798 	 1.8902809619903564 	 1.8900034427642822 	 
2025-08-06 14:27:14.979846 test begin: paddle.matmul(Tensor([512, 4, 97, 256],"float32"), Tensor([512, 4, 256, 36],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([512, 4, 97, 256],"float32"), Tensor([512, 4, 256, 36],"float32"), ) 	 69730304 	 12781 	 12.739914417266846 	 12.740772247314453 	 1.0184924602508545 	 1.018251657485962 	 15.613566875457764 	 15.617079496383667 	 0.6253042221069336 	 0.6242117881774902 	 
2025-08-06 14:28:13.037977 test begin: paddle.matrix_transpose(Tensor([20, 12700801, 4],"float16"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 12700801, 4],"float16"), ) 	 1016064080 	 2360594 	 10.481892585754395 	 8.652617692947388 	 0.00011563301086425781 	 0.0002410411834716797 	 94.57998514175415 	 124.05979466438293 	 0.00010561943054199219 	 0.00020742416381835938 	 combined
2025-08-06 14:32:50.348477 test begin: paddle.matrix_transpose(Tensor([20, 3, 16934401],"float16"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 3, 16934401],"float16"), ) 	 1016064060 	 2360594 	 10.485435485839844 	 8.654222249984741 	 0.00010371208190917969 	 9.441375732421875e-05 	 103.41585373878479 	 125.05412006378174 	 0.00012946128845214844 	 0.00021529197692871094 	 combined
2025-08-06 14:37:37.626811 test begin: paddle.matrix_transpose(Tensor([20, 3, 4233601],"float64"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 3, 4233601],"float64"), ) 	 254016060 	 2360594 	 10.56239628791809 	 8.612212181091309 	 0.0001239776611328125 	 9.059906005859375e-05 	 95.97415018081665 	 124.84270000457764 	 0.00011301040649414062 	 0.000209808349609375 	 combined
2025-08-06 14:41:48.775987 test begin: paddle.matrix_transpose(Tensor([20, 3, 8467201],"float32"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 3, 8467201],"float32"), ) 	 508032060 	 2360594 	 10.500130414962769 	 8.634008169174194 	 0.00012063980102539062 	 0.00016427040100097656 	 94.20863962173462 	 124.2274854183197 	 0.00012540817260742188 	 0.0002124309539794922 	 combined
2025-08-06 14:46:05.459433 test begin: paddle.matrix_transpose(Tensor([20, 3175201, 4],"float64"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 3175201, 4],"float64"), ) 	 254016080 	 2360594 	 10.39435863494873 	 8.642969369888306 	 0.00013017654418945312 	 8.463859558105469e-05 	 94.11996722221375 	 123.68123412132263 	 0.0001087188720703125 	 0.0002105236053466797 	 combined
2025-08-06 14:50:14.418546 test begin: paddle.matrix_transpose(Tensor([20, 6350401, 4],"float32"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([20, 6350401, 4],"float32"), ) 	 508032080 	 2360594 	 16.559450149536133 	 8.724764585494995 	 0.0001304149627685547 	 0.0001862049102783203 	 93.90115427970886 	 125.57698488235474 	 0.00010895729064941406 	 0.0002238750457763672 	 combined
2025-08-06 14:54:37.984222 test begin: paddle.matrix_transpose(Tensor([21168010, 3, 4],"float64"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([21168010, 3, 4],"float64"), ) 	 254016120 	 2360594 	 16.21767568588257 	 8.769361734390259 	 0.00011706352233886719 	 9.179115295410156e-05 	 97.61447238922119 	 126.15334057807922 	 0.0001361370086669922 	 0.00021457672119140625 	 combined
2025-08-06 14:58:59.200872 test begin: paddle.matrix_transpose(Tensor([42336010, 3, 4],"float32"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([42336010, 3, 4],"float32"), ) 	 508032120 	 2360594 	 10.402988195419312 	 8.686634063720703 	 0.00010991096496582031 	 0.00012183189392089844 	 94.36348533630371 	 125.43636798858643 	 0.00011491775512695312 	 0.000217437744140625 	 combined
2025-08-06 15:03:15.705825 test begin: paddle.matrix_transpose(Tensor([84672010, 3, 4],"float16"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([84672010, 3, 4],"float16"), ) 	 1016064120 	 2360594 	 10.375101804733276 	 8.681264400482178 	 7.295608520507812e-05 	 0.0001571178436279297 	 93.88364911079407 	 125.48307394981384 	 0.00010204315185546875 	 0.00021576881408691406 	 combined
2025-08-06 15:07:54.449199 test begin: paddle.max(Tensor([416, 50, 10, 256],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([416, 50, 10, 256],"float32"), axis=1, ) 	 53248000 	 65890 	 12.90729308128357 	 10.58768916130066 	 0.20005583763122559 	 0.1642005443572998 	 74.96962690353394 	 91.77030801773071 	 0.29059410095214844 	 0.28433966636657715 	 
2025-08-06 15:11:08.015385 test begin: paddle.max(Tensor([416, 50, 7, 349],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([416, 50, 7, 349],"float32"), axis=1, ) 	 50814400 	 65890 	 12.905298233032227 	 10.696559190750122 	 0.20001912117004395 	 0.16585516929626465 	 72.97524666786194 	 87.95006346702576 	 0.28279829025268555 	 0.27247190475463867 	 
2025-08-06 15:14:13.443506 test begin: paddle.max(Tensor([416, 69, 7, 256],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([416, 69, 7, 256],"float32"), axis=1, ) 	 51437568 	 65890 	 12.674495697021484 	 10.221987247467041 	 0.1964702606201172 	 0.15850567817687988 	 72.09328365325928 	 88.1531240940094 	 0.27947330474853516 	 0.2731361389160156 	 
2025-08-06 15:17:18.108485 test begin: paddle.max(Tensor([49, 1024, 1024],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.max 	 paddle.max(Tensor([49, 1024, 1024],"float32"), axis=-1, keepdim=True, ) 	 51380224 	 65890 	 10.363636255264282 	 9.835999965667725 	 0.16062164306640625 	 0.1524968147277832 	 69.66882276535034 	 84.88436245918274 	 0.27007055282592773 	 0.26299571990966797 	 
2025-08-06 15:20:14.660791 test begin: paddle.max(Tensor([512, 50, 7, 284],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([512, 50, 7, 284],"float32"), axis=1, ) 	 50892800 	 65890 	 13.35219955444336 	 10.226740598678589 	 0.20697498321533203 	 0.15848636627197266 	 73.6425142288208 	 87.96107888221741 	 0.2866995334625244 	 0.2725222110748291 	 
2025-08-06 15:23:20.747004 test begin: paddle.max(Tensor([512, 50, 8, 256],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([512, 50, 8, 256],"float32"), axis=1, ) 	 52428800 	 65890 	 12.752989053726196 	 10.404907941818237 	 0.19784855842590332 	 0.1613168716430664 	 74.05417919158936 	 90.40938234329224 	 0.28859376907348633 	 0.28154754638671875 	 
2025-08-06 15:26:29.272395 test begin: paddle.max(Tensor([512, 56, 7, 256],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([512, 56, 7, 256],"float32"), axis=1, ) 	 51380224 	 65890 	 12.920548439025879 	 10.12938666343689 	 0.20030665397644043 	 0.15707683563232422 	 72.1549346446991 	 88.26488995552063 	 0.27973222732543945 	 0.2735104560852051 	 
2025-08-06 15:29:33.665764 test begin: paddle.max(Tensor([568, 50, 7, 256],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([568, 50, 7, 256],"float32"), axis=1, ) 	 50892800 	 65890 	 12.5049467086792 	 10.095681428909302 	 0.1938333511352539 	 0.15665388107299805 	 72.05712270736694 	 87.79859042167664 	 0.27937817573547363 	 0.2720756530761719 	 
2025-08-06 15:32:38.037331 test begin: paddle.max(Tensor([8, 1024, 6202],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.max 	 paddle.max(Tensor([8, 1024, 6202],"float32"), axis=-1, keepdim=True, ) 	 50806784 	 65890 	 9.998931646347046 	 10.75574541091919 	 0.15502333641052246 	 0.16670513153076172 	 69.10604572296143 	 84.98931765556335 	 0.2679009437561035 	 0.2647390365600586 	 
2025-08-06 15:35:33.786892 test begin: paddle.max(Tensor([8, 6202, 1024],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.max 	 paddle.max(Tensor([8, 6202, 1024],"float32"), axis=-1, keepdim=True, ) 	 50806784 	 65890 	 10.255711555480957 	 9.733809232711792 	 0.15900921821594238 	 0.15088725090026855 	 68.92600297927856 	 84.03583216667175 	 0.2684357166290283 	 0.2603752613067627 	 
2025-08-06 15:38:27.626360 test begin: paddle.maximum(Tensor([11585, 4386],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([11585, 4386],"float32"), Tensor([1],"float32"), ) 	 50811811 	 33726 	 10.00450849533081 	 10.26983904838562 	 0.3030233383178711 	 0.3109548091888428 	 24.979480266571045 	 111.48574590682983 	 0.2518763542175293 	 0.2810249328613281 	 
2025-08-06 15:41:09.230534 test begin: paddle.maximum(Tensor([120961, 420],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([120961, 420],"float32"), Tensor([1],"float32"), ) 	 50803621 	 33726 	 10.002478837966919 	 10.267259120941162 	 0.3029906749725342 	 0.3123586177825928 	 24.96608257293701 	 111.48728895187378 	 0.25171709060668945 	 0.2809481620788574 	 
2025-08-06 15:43:47.827863 test begin: paddle.maximum(Tensor([121539, 418],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([121539, 418],"float32"), Tensor([1],"float32"), ) 	 50803303 	 33726 	 10.00000262260437 	 10.26732873916626 	 0.3041408061981201 	 0.31238794326782227 	 24.965553522109985 	 111.33870124816895 	 0.2528719902038574 	 0.28062009811401367 	 
2025-08-06 15:46:26.266994 test begin: paddle.maximum(Tensor([14877, 3415],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([14877, 3415],"float32"), Tensor([1],"float32"), ) 	 50804956 	 33726 	 10.002288341522217 	 10.283609867095947 	 0.3029944896697998 	 0.3108994960784912 	 24.976393699645996 	 111.25274181365967 	 0.25185060501098633 	 0.2803943157196045 	 
2025-08-06 15:49:06.277267 test begin: paddle.maximum(Tensor([16121, 3152],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([16121, 3152],"float32"), Tensor([1],"float32"), ) 	 50813393 	 33726 	 10.006142139434814 	 10.296674251556396 	 0.3030281066894531 	 0.31236886978149414 	 24.980148315429688 	 111.30325651168823 	 0.2519185543060303 	 0.28052377700805664 	 
2025-08-06 15:51:46.657139 test begin: paddle.maximum(Tensor([62643, 811],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([62643, 811],"float32"), Tensor([1],"float32"), ) 	 50803474 	 33726 	 10.004395723342896 	 10.265191078186035 	 0.3029508590698242 	 0.31090855598449707 	 24.97552180290222 	 111.44889497756958 	 0.25174617767333984 	 0.2808847427368164 	 
2025-08-06 15:54:25.065022 test begin: paddle.mean(Tensor([7573, 11, 1280],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([7573, 11, 1280],"bfloat16"), axis=1, ) 	 106627840 	 54851 	 11.51900863647461 	 10.753742218017578 	 0.21455097198486328 	 0.20025873184204102 	 19.061424732208252 	 24.74048686027527 	 0.3549234867095947 	 0.23029637336730957 	 
2025-08-06 15:55:33.856806 test begin: paddle.mean(Tensor([7573, 8, 1678],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([7573, 8, 1678],"bfloat16"), axis=1, ) 	 101659952 	 54851 	 10.148951292037964 	 10.002396821975708 	 0.1890857219696045 	 0.18627691268920898 	 19.15190315246582 	 24.338467121124268 	 0.3566567897796631 	 0.2266530990600586 	 
2025-08-06 15:56:41.703521 test begin: paddle.mean(Tensor([7710, 11, 1280],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([7710, 11, 1280],"bfloat16"), axis=1, ) 	 108556800 	 54851 	 11.746116876602173 	 10.911135911941528 	 0.22011137008666992 	 0.20460891723632812 	 19.31294274330139 	 25.165435075759888 	 0.3596208095550537 	 0.23579025268554688 	 
2025-08-06 15:57:50.812835 test begin: paddle.mean(Tensor([7710, 8, 1648],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([7710, 8, 1648],"bfloat16"), axis=1, ) 	 101648640 	 54851 	 10.175578355789185 	 9.786616802215576 	 0.1895580291748047 	 0.18233704566955566 	 19.186349868774414 	 24.46723699569702 	 0.3587057590484619 	 0.2278120517730713 	 
2025-08-06 15:58:57.073672 test begin: paddle.mean(Tensor([8162, 10, 1280],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([8162, 10, 1280],"bfloat16"), axis=1, ) 	 104473600 	 54851 	 11.255523204803467 	 10.507644176483154 	 0.2095794677734375 	 0.1955869197845459 	 18.944899082183838 	 24.611812591552734 	 0.352771520614624 	 0.22911834716796875 	 
2025-08-06 16:00:05.602238 test begin: paddle.mean(Tensor([8162, 8, 1557],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([8162, 8, 1557],"bfloat16"), axis=1, ) 	 101665872 	 54851 	 10.15658688545227 	 11.837035655975342 	 0.18911981582641602 	 0.22024822235107422 	 19.196187019348145 	 25.28161883354187 	 0.35757017135620117 	 0.2353806495666504 	 
2025-08-06 16:01:15.893632 test begin: paddle.mean(Tensor([9923, 8, 1280],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([9923, 8, 1280],"bfloat16"), axis=1, ) 	 101611520 	 54851 	 9.998535633087158 	 9.672158479690552 	 0.18620514869689941 	 0.17999839782714844 	 19.235983848571777 	 24.85836696624756 	 0.3581554889678955 	 0.23143267631530762 	 
2025-08-06 16:02:23.400645 test begin: paddle.median(Tensor([2, 254016],"float32"), axis=1, mode="min", )
[Prof] paddle.median 	 paddle.median(Tensor([2, 254016],"float32"), axis=1, mode="min", ) 	 508032 	 3680 	 35.26381182670593 	 3.9790382385253906 	 0.19364714622497559 	 1.1044247150421143 	 None 	 None 	 None 	 None 	 combined
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 16:03:06.163219 test begin: paddle.median(Tensor([254016],"int64"), )
W0806 16:03:27.190888 135652 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.median 	 paddle.median(Tensor([254016],"int64"), ) 	 254016 	 3680 	 21.01542353630066 	 0.6340241432189941 	 0.24077177047729492 	 0.0070035457611083984 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 16:03:27.876655 test begin: paddle.median(Tensor([5080, 100],"float32"), axis=1, mode="min", )
[Prof] paddle.median 	 paddle.median(Tensor([5080, 100],"float32"), axis=1, mode="min", ) 	 508000 	 3680 	 9.926172018051147 	 0.16190791130065918 	 0.07001638412475586 	 0.044881582260131836 	 None 	 None 	 None 	 None 	 combined
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 16:03:41.596054 test begin: paddle.median(Tensor([508032],"float32"), )
[Prof] paddle.median 	 paddle.median(Tensor([508032],"float32"), ) 	 508032 	 3680 	 25.105243921279907 	 0.5801990032196045 	 0.38502931594848633 	 0.009403705596923828 	 1.864506721496582 	 0.5784893035888672 	 0.04340314865112305 	 9.918212890625e-05 	 combined
2025-08-06 16:04:09.760091 test begin: paddle.min(Tensor([10, 10, 28225, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1f401af130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 16:14:15.125712 test begin: paddle.min(Tensor([10, 10, 9, 28225],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
W0806 16:14:15.807569 155754 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.min 	 paddle.min(Tensor([10, 10, 9, 28225],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402502 	 65799 	 22.081414699554443 	 10.177044868469238 	 0.00022745132446289062 	 0.08016180992126465 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 16:16:05.633106 test begin: paddle.min(Tensor([10, 31361, 9, 9],"float64"), Tensor([2],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbcc0656bc0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 16:26:10.862261 test begin: paddle.min(Tensor([10, 31361, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
W0806 16:26:11.568572 24108 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa83c35af80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 16:36:15.622829 test begin: paddle.min(Tensor([10, 5, 56449, 9],"float64"), Tensor([2],"int64"), )
W0806 16:36:16.314077 50629 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.min 	 paddle.min(Tensor([10, 5, 56449, 9],"float64"), Tensor([2],"int64"), ) 	 25402052 	 65799 	 13.346245765686035 	 10.156922340393066 	 0.0001614093780517578 	 0.15761423110961914 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 16:37:55.420112 test begin: paddle.min(Tensor([10, 5, 9, 56449],"float64"), Tensor([2],"int64"), )
[Prof] paddle.min 	 paddle.min(Tensor([10, 5, 9, 56449],"float64"), Tensor([2],"int64"), ) 	 25402052 	 65799 	 13.705088376998901 	 10.154261589050293 	 0.00016355514526367188 	 0.15760231018066406 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 16:39:40.585160 test begin: paddle.min(Tensor([31361, 10, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.min 	 paddle.min(Tensor([31361, 10, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402412 	 65799 	 22.53571653366089 	 12.123488903045654 	 0.00022912025451660156 	 0.09411001205444336 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 16:41:32.391811 test begin: paddle.min(Tensor([62721, 5, 9, 9],"float64"), Tensor([2],"int64"), )
[Prof] paddle.min 	 paddle.min(Tensor([62721, 5, 9, 9],"float64"), Tensor([2],"int64"), ) 	 25402007 	 65799 	 57.13953971862793 	 10.9739990234375 	 0.0008234977722167969 	 0.17028450965881348 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 16:44:18.716994 test begin: paddle.min(Tensor([64, 1, 28, 28351],"float32"), )
[Prof] paddle.min 	 paddle.min(Tensor([64, 1, 28, 28351],"float32"), ) 	 50804992 	 65799 	 10.00864052772522 	 10.082890033721924 	 0.07767224311828613 	 0.07828259468078613 	 68.75807356834412 	 82.27118444442749 	 0.2135939598083496 	 0.21413421630859375 	 
2025-08-06 16:47:12.152379 test begin: paddle.min(Tensor([64, 1, 28351, 28],"float32"), )
[Prof] paddle.min 	 paddle.min(Tensor([64, 1, 28351, 28],"float32"), ) 	 50804992 	 65799 	 10.007762670516968 	 10.078910112380981 	 0.07767438888549805 	 0.07826352119445801 	 68.76010918617249 	 82.27367806434631 	 0.213578462600708 	 0.21274018287658691 	 
2025-08-06 16:50:04.579289 test begin: paddle.min(Tensor([64, 1013, 28, 28],"float32"), )
[Prof] paddle.min 	 paddle.min(Tensor([64, 1013, 28, 28],"float32"), ) 	 50828288 	 65799 	 9.999019384384155 	 10.088093757629395 	 0.07759284973144531 	 0.07825517654418945 	 68.80357503890991 	 82.30389642715454 	 0.21375274658203125 	 0.21282076835632324 	 
2025-08-06 16:52:58.391776 test begin: paddle.min(Tensor([64801, 1, 28, 28],"float32"), )
[Prof] paddle.min 	 paddle.min(Tensor([64801, 1, 28, 28],"float32"), ) 	 50803984 	 65799 	 9.99646544456482 	 10.084137439727783 	 0.07756805419921875 	 0.07826709747314453 	 68.77344942092896 	 82.28029179573059 	 0.2136671543121338 	 0.21276259422302246 	 
2025-08-06 16:55:50.375408 test begin: paddle.minimum(Tensor([13, 1, 113],"float32"), Tensor([451143, 113],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([13, 1, 113],"float32"), Tensor([451143, 113],"float32"), ) 	 50980628 	 3827 	 14.509328842163086 	 15.6871976852417 	 3.8738555908203125 	 2.096120595932007 	 71.2372682094574 	 180.42498588562012 	 4.74775505065918 	 2.1851577758789062 	 
2025-08-06 17:00:44.713193 test begin: paddle.minimum(Tensor([13, 1, 2],"float32"), Tensor([25401601, 2],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([13, 1, 2],"float32"), Tensor([25401601, 2],"float32"), ) 	 50803228 	 3827 	 14.443377017974854 	 15.635091066360474 	 3.856656551361084 	 2.088045597076416 	 122.87488651275635 	 178.85083317756653 	 8.189539909362793 	 2.1647024154663086 	 
2025-08-06 17:06:30.232830 test begin: paddle.minimum(Tensor([16, 1, 113],"float32"), Tensor([451143, 113],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([16, 1, 113],"float32"), Tensor([451143, 113],"float32"), ) 	 50980967 	 3827 	 17.85294818878174 	 19.28381323814392 	 4.7664172649383545 	 2.5756704807281494 	 86.76918435096741 	 219.58293890953064 	 5.784562826156616 	 2.657573938369751 	 
2025-08-06 17:12:31.763389 test begin: paddle.minimum(Tensor([16, 1, 2],"float32"), Tensor([25401601, 2],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([16, 1, 2],"float32"), Tensor([25401601, 2],"float32"), ) 	 50803234 	 3827 	 17.774712800979614 	 19.200783491134644 	 4.747787237167358 	 2.562685489654541 	 146.727192401886 	 224.6186397075653 	 9.78860592842102 	 2.7220966815948486 	 
2025-08-06 17:19:34.385325 test begin: paddle.minimum(Tensor([9, 1, 113],"float32"), Tensor([451143, 113],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([9, 1, 113],"float32"), Tensor([451143, 113],"float32"), ) 	 50980176 	 3827 	 10.050373077392578 	 10.846059799194336 	 2.686704397201538 	 2.8962953090667725 	 50.26993012428284 	 122.73616814613342 	 3.3534841537475586 	 2.5213561058044434 	 
2025-08-06 17:22:57.047108 test begin: paddle.minimum(Tensor([9, 1, 2],"float32"), Tensor([25401601, 2],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([9, 1, 2],"float32"), Tensor([25401601, 2],"float32"), ) 	 50803220 	 3827 	 10.010627031326294 	 10.795346021652222 	 2.673915147781372 	 2.885316848754883 	 89.54195380210876 	 122.53349566459656 	 5.971841812133789 	 2.5143439769744873 	 
2025-08-06 17:26:58.425994 test begin: paddle.mm(Tensor([1838, 6, 144, 144],"float32"), Tensor([1838, 6, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([1838, 6, 144, 144],"float32"), Tensor([1838, 6, 144, 32],"float32"), ) 	 279493632 	 7310 	 44.427164793014526 	 44.412198305130005 	 6.2102062702178955 	 6.1995790004730225 	 69.59418749809265 	 69.6173357963562 	 4.866466045379639 	 4.867237567901611 	 
2025-08-06 17:30:52.708656 test begin: paddle.mm(Tensor([2048, 2, 144, 144],"float32"), Tensor([2048, 2, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([2048, 2, 144, 144],"float32"), Tensor([2048, 2, 144, 32],"float32"), ) 	 103809024 	 7310 	 16.702567100524902 	 16.592271327972412 	 2.4332244396209717 	 2.3204092979431152 	 25.993229150772095 	 25.98037338256836 	 1.8168020248413086 	 1.8137085437774658 	 
2025-08-06 17:32:22.698340 test begin: paddle.mm(Tensor([2048, 6, 144, 144],"float32"), Tensor([2048, 6, 144, 29],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([2048, 6, 144, 144],"float32"), Tensor([2048, 6, 144, 29],"float32"), ) 	 306118656 	 7310 	 49.42307639122009 	 49.39992690086365 	 6.922120571136475 	 6.898351430892944 	 75.56585144996643 	 75.3486487865448 	 5.2849647998809814 	 5.265130519866943 	 
2025-08-06 17:36:42.697542 test begin: paddle.mm(Tensor([2048, 6, 144, 144],"float32"), Tensor([2048, 6, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([2048, 6, 144, 144],"float32"), Tensor([2048, 6, 144, 32],"float32"), ) 	 311427072 	 7310 	 49.389243602752686 	 49.284510135650635 	 6.8938257694244385 	 6.889134407043457 	 77.26760792732239 	 77.2205753326416 	 5.397071838378906 	 5.400791883468628 	 
2025-08-06 17:41:01.962541 test begin: paddle.mm(Tensor([2048, 6, 29, 144],"float32"), Tensor([2048, 6, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([2048, 6, 29, 144],"float32"), Tensor([2048, 6, 144, 32],"float32"), ) 	 107937792 	 7310 	 24.65478539466858 	 24.680721521377563 	 3.449340343475342 	 3.446593761444092 	 27.34239387512207 	 27.298952102661133 	 1.9115502834320068 	 1.9085352420806885 	 
2025-08-06 17:42:49.309680 test begin: paddle.mm(Tensor([2757, 4, 144, 144],"float32"), Tensor([2757, 4, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([2757, 4, 144, 144],"float32"), Tensor([2757, 4, 144, 32],"float32"), ) 	 279493632 	 7310 	 44.25527858734131 	 44.32333731651306 	 6.184951305389404 	 6.191300868988037 	 69.450763463974 	 69.55509543418884 	 4.852512359619141 	 4.864766359329224 	 
2025-08-06 17:46:42.912628 test begin: paddle.mm(Tensor([3840, 1, 144, 144],"float32"), Tensor([3840, 1, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([3840, 1, 144, 144],"float32"), Tensor([3840, 1, 144, 32],"float32"), ) 	 97320960 	 7310 	 15.607411623001099 	 15.599094152450562 	 2.1742284297943115 	 2.181049108505249 	 24.4276385307312 	 24.40811824798584 	 1.711416244506836 	 1.7085442543029785 	 
2025-08-06 17:48:06.955443 test begin: paddle.mm(Tensor([3840, 1, 144, 144],"float32"), Tensor([3840, 4, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([3840, 1, 144, 144],"float32"), Tensor([3840, 4, 144, 32],"float32"), ) 	 150405120 	 7310 	 69.5907609462738 	 72.55613350868225 	 0.0001327991485595703 	 5.0727972984313965 	 106.8073365688324 	 105.19700336456299 	 0.0011601448059082031 	 4.9020466804504395 	 
2025-08-06 17:54:05.474130 test begin: paddle.mm(Tensor([3840, 3, 144, 144],"float32"), Tensor([3840, 3, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([3840, 3, 144, 144],"float32"), Tensor([3840, 3, 144, 32],"float32"), ) 	 291962880 	 7310 	 46.24554443359375 	 46.29598665237427 	 6.4671547412872314 	 6.472622394561768 	 72.60322666168213 	 72.50916790962219 	 5.0684309005737305 	 5.070190668106079 	 
2025-08-06 17:58:09.849318 test begin: paddle.mm(Tensor([3840, 4, 144, 144],"float32"), Tensor([3840, 4, 144, 23],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([3840, 4, 144, 144],"float32"), Tensor([3840, 4, 144, 23],"float32"), ) 	 369377280 	 7310 	 61.71850919723511 	 61.64176607131958 	 8.613658666610718 	 8.620382070541382 	 87.41164708137512 	 87.34357213973999 	 6.120718955993652 	 6.097134351730347 	 
2025-08-06 18:03:14.961873 test begin: paddle.mm(Tensor([3840, 4, 23, 144],"float32"), Tensor([3840, 4, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([3840, 4, 23, 144],"float32"), Tensor([3840, 4, 144, 32],"float32"), ) 	 121651200 	 7310 	 30.89211130142212 	 30.876424312591553 	 4.319285869598389 	 4.317026376724243 	 30.82423186302185 	 30.858358144760132 	 2.153453826904297 	 2.1591005325317383 	 
2025-08-06 18:05:20.703899 test begin: paddle.mm(Tensor([409, 6, 144, 144],"float32"), Tensor([409, 6, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([409, 6, 144, 144],"float32"), Tensor([409, 6, 144, 32],"float32"), ) 	 62194176 	 7310 	 10.00176739692688 	 10.006913423538208 	 1.399390459060669 	 1.3989331722259521 	 15.909731388092041 	 15.646455526351929 	 1.358466386795044 	 1.0938804149627686 	 
2025-08-06 18:06:14.003391 test begin: paddle.mm(Tensor([4096, 1, 144, 144],"float32"), Tensor([4096, 1, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([4096, 1, 144, 144],"float32"), Tensor([4096, 1, 144, 32],"float32"), ) 	 103809024 	 7310 	 16.50381326675415 	 16.501603364944458 	 2.3063621520996094 	 2.3048095703125 	 25.833505392074585 	 25.84353280067444 	 1.8049330711364746 	 1.8047404289245605 	 
2025-08-06 18:07:41.751309 test begin: paddle.mm(Tensor([4096, 1, 144, 144],"float32"), Tensor([4096, 4, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([4096, 1, 144, 144],"float32"), Tensor([4096, 4, 144, 32],"float32"), ) 	 160432128 	 7310 	 74.45123291015625 	 77.31358170509338 	 0.00010561943054199219 	 5.4116151332855225 	 113.80952286720276 	 112.28586745262146 	 0.0012707710266113281 	 5.229679822921753 	 
2025-08-06 18:14:03.892391 test begin: paddle.mm(Tensor([4096, 3, 144, 144],"float32"), Tensor([4096, 3, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([4096, 3, 144, 144],"float32"), Tensor([4096, 3, 144, 32],"float32"), ) 	 311427072 	 7310 	 49.32081127166748 	 49.31312131881714 	 6.905693531036377 	 6.90456223487854 	 77.33732771873474 	 77.21331715583801 	 5.398975372314453 	 5.3969504833221436 	 
2025-08-06 18:18:25.137297 test begin: paddle.mm(Tensor([4096, 4, 144, 144],"float32"), Tensor([4096, 4, 144, 22],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([4096, 4, 144, 144],"float32"), Tensor([4096, 4, 144, 22],"float32"), ) 	 391643136 	 7310 	 65.68220567703247 	 65.70827960968018 	 9.182481050491333 	 9.197734117507935 	 93.0577323436737 	 93.04176163673401 	 6.501314163208008 	 6.507626533508301 	 
2025-08-05 21:37:48.575805 test begin: paddle.mm(Tensor([4096, 4, 22, 144],"float32"), Tensor([4096, 4, 144, 32],"float32"), )
W0805 21:37:53.344311 108497 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.mm 	 paddle.mm(Tensor([4096, 4, 22, 144],"float32"), Tensor([4096, 4, 144, 32],"float32"), ) 	 127401984 	 7310 	 32.8719322681427 	 32.874840259552 	 4.593250751495361 	 4.596045255661011 	 32.87210011482239 	 32.85791349411011 	 2.297959089279175 	 2.294632911682129 	 
2025-08-05 21:40:05.892178 test begin: paddle.mm(Tensor([613, 4, 144, 144],"float32"), Tensor([613, 4, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([613, 4, 144, 144],"float32"), Tensor([613, 4, 144, 32],"float32"), ) 	 62143488 	 7310 	 9.998369932174683 	 9.997225522994995 	 1.397840976715088 	 1.397688627243042 	 15.627504587173462 	 15.626089334487915 	 1.0923664569854736 	 1.0922322273254395 	 
2025-08-05 21:40:58.743115 test begin: paddle.mod(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), ) 	 50803220 	 22295 	 9.988821744918823 	 9.966318607330322 	 0.457904577255249 	 0.4568617343902588 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:41:40.042234 test begin: paddle.mod(Tensor([10, 5080321],"int32"), Tensor([10, 5080321],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([10, 5080321],"int32"), Tensor([10, 5080321],"int32"), ) 	 101606420 	 22295 	 10.06736159324646 	 10.021074533462524 	 0.4606814384460449 	 0.45931053161621094 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:42:31.601871 test begin: paddle.mod(Tensor([1270081, 2, 4, 5],"int32"), Tensor([1270081, 2, 4, 5],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([1270081, 2, 4, 5],"int32"), Tensor([1270081, 2, 4, 5],"int32"), ) 	 101606480 	 22295 	 10.053571462631226 	 10.022833347320557 	 0.4609050750732422 	 0.45923876762390137 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:43:20.489202 test begin: paddle.mod(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), ) 	 50804736 	 22295 	 10.004127740859985 	 9.96435546875 	 0.4586455821990967 	 0.45675110816955566 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:43:58.395071 test begin: paddle.mod(Tensor([2540161, 20],"int32"), Tensor([2540161, 20],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([2540161, 20],"int32"), Tensor([2540161, 20],"int32"), ) 	 101606440 	 22295 	 10.053616046905518 	 10.029915571212769 	 0.4608340263366699 	 0.45940351486206055 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:44:49.544896 test begin: paddle.mod(Tensor([6, 2, 4, 1058401],"int32"), Tensor([6, 2, 4, 1058401],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([6, 2, 4, 1058401],"int32"), Tensor([6, 2, 4, 1058401],"int32"), ) 	 101606496 	 22295 	 10.058368444442749 	 10.021267652511597 	 0.46082377433776855 	 0.45935821533203125 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:45:39.304054 test begin: paddle.mod(Tensor([6, 2, 846721, 5],"int32"), Tensor([6, 2, 846721, 5],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([6, 2, 846721, 5],"int32"), Tensor([6, 2, 846721, 5],"int32"), ) 	 101606520 	 22295 	 10.068337440490723 	 10.02098298072815 	 0.46088123321533203 	 0.45932626724243164 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:46:30.805173 test begin: paddle.mod(Tensor([6, 423361, 4, 5],"int32"), Tensor([6, 423361, 4, 5],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([6, 423361, 4, 5],"int32"), Tensor([6, 423361, 4, 5],"int32"), ) 	 101606640 	 22295 	 10.051516056060791 	 10.020658016204834 	 0.46074771881103516 	 0.4593679904937744 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:47:20.388916 test begin: paddle.mode(Tensor([2, 10, 12],"float64"), -1, )
[Prof] paddle.mode 	 paddle.mode(Tensor([2, 10, 12],"float64"), -1, ) 	 240 	 1169 	 10.79464602470398 	 0.03188371658325195 	 0.00013065338134765625 	 5.412101745605469e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:47:31.342047 test begin: paddle.mode(Tensor([2, 10, 12],"float64"), -1, keepdim=True, )
[Prof] paddle.mode 	 paddle.mode(Tensor([2, 10, 12],"float64"), -1, keepdim=True, ) 	 240 	 1169 	 10.835562229156494 	 0.028598308563232422 	 9.679794311523438e-05 	 3.838539123535156e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:47:42.847773 test begin: paddle.mode(Tensor([2, 10, 12],"float64"), 1, )
[Prof] paddle.mode 	 paddle.mode(Tensor([2, 10, 12],"float64"), 1, ) 	 240 	 1169 	 13.011668682098389 	 0.05209493637084961 	 0.00011873245239257812 	 9.942054748535156e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:47:56.060199 test begin: paddle.mode(Tensor([2, 12, 10],"float64"), -1, )
[Prof] paddle.mode 	 paddle.mode(Tensor([2, 12, 10],"float64"), -1, ) 	 240 	 1169 	 12.900577306747437 	 0.03148794174194336 	 0.00010180473327636719 	 4.00543212890625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:48:09.086678 test begin: paddle.mode(Tensor([2, 12, 10],"float64"), -1, keepdim=True, )
[Prof] paddle.mode 	 paddle.mode(Tensor([2, 12, 10],"float64"), -1, keepdim=True, ) 	 240 	 1169 	 12.931280136108398 	 0.02838873863220215 	 0.00011038780212402344 	 3.647804260253906e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:48:22.137549 test begin: paddle.mode(Tensor([2, 12, 10],"float64"), 1, )
[Prof] paddle.mode 	 paddle.mode(Tensor([2, 12, 10],"float64"), 1, ) 	 240 	 1169 	 10.873263597488403 	 0.0511622428894043 	 0.00010037422180175781 	 4.5299530029296875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:48:33.151798 test begin: paddle.moveaxis(Tensor([20, 3, 120961, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([20, 3, 120961, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 254018100 	 1394939 	 11.102113962173462 	 8.616445779800415 	 0.0001270771026611328 	 0.000125885009765625 	 58.632487773895264 	 76.70174241065979 	 9.655952453613281e-05 	 0.00021123886108398438 	 
2025-08-05 21:51:21.887930 test begin: paddle.moveaxis(Tensor([20, 3, 4, 151201, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([20, 3, 4, 151201, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 254017680 	 1394939 	 10.969419002532959 	 8.540517330169678 	 0.000133514404296875 	 7.987022399902344e-05 	 58.335638999938965 	 76.66400957107544 	 0.0001266002655029297 	 0.0002193450927734375 	 
2025-08-05 21:54:11.504103 test begin: paddle.moveaxis(Tensor([20, 3, 4, 5, 211681],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([20, 3, 4, 5, 211681],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 254017200 	 1394939 	 11.02475118637085 	 8.708496332168579 	 0.0001385211944580078 	 0.00012826919555664062 	 58.848437547683716 	 76.2587366104126 	 0.00010776519775390625 	 0.0002663135528564453 	 
2025-08-05 21:56:58.020713 test begin: paddle.moveaxis(Tensor([20, 90721, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([20, 90721, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 254018800 	 1394939 	 11.076687574386597 	 8.520033836364746 	 0.000133514404296875 	 8.20159912109375e-05 	 58.68041157722473 	 76.22626519203186 	 0.00011110305786132812 	 0.00022029876708984375 	 
2025-08-05 21:59:43.493443 test begin: paddle.moveaxis(Tensor([604810, 3, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([604810, 3, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 254020200 	 1394939 	 11.019093990325928 	 8.57249927520752 	 0.0001201629638671875 	 0.0001232624053955078 	 59.71902060508728 	 76.49674868583679 	 0.00010967254638671875 	 0.00022077560424804688 	 
2025-08-05 22:02:32.302445 test begin: paddle.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=0, destination=2, ) 	 254018100 	 1394939 	 9.749098539352417 	 6.825950860977173 	 5.269050598144531e-05 	 0.0002560615539550781 	 59.55583667755127 	 76.40471315383911 	 0.00012993812561035156 	 0.00023436546325683594 	 
2025-08-05 22:05:15.888891 test begin: paddle.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254018100 	 1394939 	 10.32469367980957 	 8.6190664768219 	 0.0001537799835205078 	 0.0002818107604980469 	 58.9709529876709 	 76.72934651374817 	 0.00010609626770019531 	 0.00022530555725097656 	 
2025-08-05 22:08:04.196363 test begin: paddle.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=0, destination=2, ) 	 254017680 	 1394939 	 17.0041024684906 	 11.829816102981567 	 0.00016951560974121094 	 0.000759124755859375 	 67.99566578865051 	 80.58427596092224 	 0.00019931793212890625 	 0.0002689361572265625 	 
2025-08-05 22:11:14.081044 test begin: paddle.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254017680 	 1394939 	 17.549296140670776 	 14.438746690750122 	 0.00032401084899902344 	 0.0001380443572998047 	 68.63997507095337 	 96.51160836219788 	 0.00015282630920410156 	 0.0005781650543212891 	 
2025-08-05 22:14:43.439907 test begin: paddle.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=0, destination=2, ) 	 254017200 	 1394939 	 13.886841058731079 	 6.834529876708984 	 0.00011897087097167969 	 0.00010132789611816406 	 58.59456396102905 	 75.90201544761658 	 0.00010967254638671875 	 0.0002238750457763672 	 
2025-08-05 22:17:31.695967 test begin: paddle.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254017200 	 1394939 	 10.357449531555176 	 8.462162494659424 	 0.00012755393981933594 	 0.0002574920654296875 	 58.85421299934387 	 75.87933206558228 	 0.00013828277587890625 	 0.00022172927856445312 	 
2025-08-05 22:20:16.317471 test begin: paddle.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=0, destination=2, ) 	 254018800 	 1394939 	 17.031481504440308 	 6.860141754150391 	 0.00013947486877441406 	 0.0001220703125 	 60.08243274688721 	 76.99620866775513 	 0.00011229515075683594 	 0.0002276897430419922 	 
2025-08-05 22:23:10.018926 test begin: paddle.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254018800 	 1394939 	 10.30026125907898 	 9.709656238555908 	 0.00012874603271484375 	 0.00026226043701171875 	 59.06400394439697 	 76.42214274406433 	 0.00010180473327636719 	 0.0002224445343017578 	 
2025-08-05 22:25:57.264126 test begin: paddle.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=0, destination=2, ) 	 254020200 	 1394939 	 9.691172361373901 	 11.31723165512085 	 0.00012755393981933594 	 0.0003063678741455078 	 58.70857357978821 	 77.00954294204712 	 0.00011181831359863281 	 0.0002238750457763672 	 
2025-08-05 22:28:45.053612 test begin: paddle.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254020200 	 1394939 	 10.326377391815186 	 8.445562601089478 	 0.00012350082397460938 	 9.72747802734375e-05 	 58.93507432937622 	 76.48817229270935 	 0.00010347366333007812 	 0.00022935867309570312 	 
2025-08-05 22:31:30.247332 test begin: paddle.multigammaln(Tensor([10, 2540161],"float64"), 2, )
[Prof] paddle.multigammaln 	 paddle.multigammaln(Tensor([10, 2540161],"float64"), 2, ) 	 25401610 	 4159 	 12.293707847595215 	 11.652284622192383 	 0.5029609203338623 	 0.5723714828491211 	 16.777944326400757 	 15.263652563095093 	 1.0318801403045654 	 0.7496857643127441 	 
2025-08-05 22:32:27.518987 test begin: paddle.multigammaln(Tensor([10, 5080321],"float32"), 2, )
[Prof] paddle.multigammaln 	 paddle.multigammaln(Tensor([10, 5080321],"float32"), 2, ) 	 50803210 	 4159 	 10.021757125854492 	 10.7742018699646 	 0.410707950592041 	 0.5297701358795166 	 14.394545316696167 	 16.57605767250061 	 0.8854496479034424 	 0.8144383430480957 	 
2025-08-05 22:33:21.030220 test begin: paddle.multigammaln(Tensor([1270081, 20],"float64"), 2, )
[Prof] paddle.multigammaln 	 paddle.multigammaln(Tensor([1270081, 20],"float64"), 2, ) 	 25401620 	 4159 	 12.286273002624512 	 11.651538610458374 	 0.502732515335083 	 0.5723211765289307 	 16.781985998153687 	 15.262935400009155 	 1.0317409038543701 	 0.74959397315979 	 
2025-08-05 22:34:18.142157 test begin: paddle.multigammaln(Tensor([2540161, 20],"float32"), 2, )
[Prof] paddle.multigammaln 	 paddle.multigammaln(Tensor([2540161, 20],"float32"), 2, ) 	 50803220 	 4159 	 10.021495819091797 	 10.79761004447937 	 0.40997815132141113 	 0.530113935470581 	 14.394033908843994 	 16.577733993530273 	 0.884469747543335 	 0.8145906925201416 	 
2025-08-05 22:35:15.451323 test begin: paddle.multiplex(inputs=list[Tensor([12700801, 4],"float32"),Tensor([12700801, 4],"float32"),], index=Tensor([127, 1],"int32"), )
[Prof] paddle.multiplex 	 paddle.multiplex(inputs=list[Tensor([12700801, 4],"float32"),Tensor([12700801, 4],"float32"),], index=Tensor([127, 1],"int32"), ) 	 101606535 	 29797 	 13.918164014816284 	 107.82195258140564 	 0.00011301040649414062 	 0.000225067138671875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 22:37:35.029772 test begin: paddle.multiplex(inputs=list[Tensor([12700801, 4],"float32"),Tensor([12700801, 4],"float32"),], index=Tensor([601, 1],"int32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f58cbe66f50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 22:47:46.337417 test begin: paddle.multiplex(inputs=list[Tensor([7, 7257601],"float32"),Tensor([7, 7257601],"float32"),], index=Tensor([6, 1],"int32"), )
W0805 22:47:48.021332 112023 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.multiplex 	 paddle.multiplex(inputs=list[Tensor([7, 7257601],"float32"),Tensor([7, 7257601],"float32"),], index=Tensor([6, 1],"int32"), ) 	 101606420 	 29797 	 9.975383520126343 	 13.756659030914307 	 0.0002796649932861328 	 0.0002760887145996094 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-05 22:48:33.967744 test begin: paddle.multiply(Tensor([298, 872, 14, 14],"float32"), Tensor([298, 872, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([298, 872, 14, 14],"float32"), Tensor([298, 872, 1, 1],"float32"), ) 	 51191632 	 33806 	 10.048315048217773 	 10.390801429748535 	 0.30376482009887695 	 0.3141050338745117 	 29.721250772476196 	 30.827784538269043 	 0.4491755962371826 	 0.310577392578125 	 
2025-08-05 22:50:00.354116 test begin: paddle.multiply(Tensor([512, 507, 14, 14],"float32"), Tensor([512, 507, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([512, 507, 14, 14],"float32"), Tensor([512, 507, 1, 1],"float32"), ) 	 51138048 	 33806 	 10.035175323486328 	 10.380601644515991 	 0.3033730983734131 	 0.31336307525634766 	 29.697566032409668 	 30.792814016342163 	 0.44881534576416016 	 0.3101692199707031 	 
2025-08-05 22:51:24.307213 test begin: paddle.multiply(Tensor([512, 872, 14, 9],"float32"), Tensor([512, 872, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([512, 872, 14, 9],"float32"), Tensor([512, 872, 1, 1],"float32"), ) 	 56700928 	 33806 	 11.098920583724976 	 13.361018180847168 	 0.3354802131652832 	 0.34686827659606934 	 30.28663921356201 	 34.791367530822754 	 0.45782470703125 	 0.3504199981689453 	 
2025-08-05 22:53:00.030510 test begin: paddle.multiply(Tensor([512, 872, 14, 9],"float32"), Tensor([512, 872, 1, 9],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([512, 872, 14, 9],"float32"), Tensor([512, 872, 1, 9],"float32"), ) 	 60272640 	 33806 	 11.434427738189697 	 11.998175144195557 	 0.34568262100219727 	 0.36249685287475586 	 29.430551767349243 	 35.15356159210205 	 0.44487476348876953 	 0.3540914058685303 	 
2025-08-05 22:54:32.637695 test begin: paddle.multiply(Tensor([512, 872, 9, 14],"float32"), Tensor([512, 872, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([512, 872, 9, 14],"float32"), Tensor([512, 872, 1, 1],"float32"), ) 	 56700928 	 33806 	 11.097611427307129 	 11.47438383102417 	 0.3355083465576172 	 0.3468739986419678 	 30.282593488693237 	 34.79576110839844 	 0.45777344703674316 	 0.3504931926727295 	 
2025-08-05 22:56:02.202082 test begin: paddle.multiply(Tensor([512, 872, 9, 14],"float32"), Tensor([512, 872, 9, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([512, 872, 9, 14],"float32"), Tensor([512, 872, 9, 1],"float32"), ) 	 60272640 	 33806 	 11.43763518333435 	 11.880326509475708 	 0.34582042694091797 	 0.35852932929992676 	 31.505658864974976 	 40.11996245384216 	 0.4761946201324463 	 0.4041409492492676 	 
2025-08-05 22:57:43.019793 test begin: paddle.multiply(x=Tensor([128, 127, 56, 56],"float32"), y=Tensor([128, 127, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 127, 56, 56],"float32"), y=Tensor([128, 127, 1, 1],"float32"), ) 	 50995072 	 33806 	 10.033443927764893 	 10.438880920410156 	 0.30310988426208496 	 0.3155655860900879 	 25.064127683639526 	 30.587165594100952 	 0.3788435459136963 	 0.30806756019592285 	 
2025-08-05 22:59:00.856731 test begin: paddle.multiply(x=Tensor([128, 224, 32, 56],"float32"), y=Tensor([128, 224, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 224, 32, 56],"float32"), y=Tensor([128, 224, 1, 1],"float32"), ) 	 51408896 	 33806 	 10.10888934135437 	 10.582684516906738 	 0.3055593967437744 	 0.3197448253631592 	 25.20316243171692 	 30.857372760772705 	 0.3809542655944824 	 0.3108055591583252 	 
2025-08-05 23:00:22.774510 test begin: paddle.multiply(x=Tensor([128, 224, 32, 56],"float32"), y=Tensor([128, 224, 32, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 224, 32, 56],"float32"), y=Tensor([128, 224, 32, 1],"float32"), ) 	 52297728 	 33806 	 10.199323654174805 	 10.556688785552979 	 0.3082923889160156 	 0.31912779808044434 	 29.09146547317505 	 36.82628321647644 	 0.43972086906433105 	 0.37096381187438965 	 
2025-08-05 23:01:51.296499 test begin: paddle.multiply(x=Tensor([128, 224, 56, 32],"float32"), y=Tensor([128, 224, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 224, 56, 32],"float32"), y=Tensor([128, 224, 1, 1],"float32"), ) 	 51408896 	 33806 	 10.110466957092285 	 10.576636791229248 	 0.30560851097106934 	 0.31973886489868164 	 25.20327591896057 	 30.857259035110474 	 0.38092994689941406 	 0.3107757568359375 	 
2025-08-05 23:03:10.419108 test begin: paddle.multiply(x=Tensor([128, 224, 56, 32],"float32"), y=Tensor([128, 224, 1, 32],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 224, 56, 32],"float32"), y=Tensor([128, 224, 1, 32],"float32"), ) 	 52297728 	 33806 	 10.19019365310669 	 10.6780264377594 	 0.308074951171875 	 0.32253432273864746 	 26.540000915527344 	 31.052231550216675 	 0.40112876892089844 	 0.31276559829711914 	 
2025-08-05 23:04:31.323491 test begin: paddle.multiply(x=Tensor([128, 256, 28, 56],"float32"), y=Tensor([128, 256, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 256, 28, 56],"float32"), y=Tensor([128, 256, 1, 1],"float32"), ) 	 51412992 	 33806 	 10.109086275100708 	 10.544152736663818 	 0.30561089515686035 	 0.3186979293823242 	 25.30069375038147 	 30.881067037582397 	 0.3824129104614258 	 0.31102848052978516 	 
2025-08-05 23:05:51.461572 test begin: paddle.multiply(x=Tensor([128, 256, 28, 56],"float32"), y=Tensor([128, 256, 28, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 256, 28, 56],"float32"), y=Tensor([128, 256, 28, 1],"float32"), ) 	 52297728 	 33806 	 10.19685983657837 	 10.556646347045898 	 0.3083155155181885 	 0.3191540241241455 	 29.091015815734863 	 36.821282148361206 	 0.43972301483154297 	 0.3709864616394043 	 
2025-08-05 23:07:22.822616 test begin: paddle.multiply(x=Tensor([128, 256, 56, 28],"float32"), y=Tensor([128, 256, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 256, 56, 28],"float32"), y=Tensor([128, 256, 1, 1],"float32"), ) 	 51412992 	 33806 	 10.108757019042969 	 10.54243278503418 	 0.30562543869018555 	 0.31867289543151855 	 25.30108904838562 	 30.878188371658325 	 0.3824007511138916 	 0.3110389709472656 	 
2025-08-05 23:08:42.278037 test begin: paddle.multiply(x=Tensor([128, 256, 56, 28],"float32"), y=Tensor([128, 256, 1, 28],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 256, 56, 28],"float32"), y=Tensor([128, 256, 1, 28],"float32"), ) 	 52297728 	 33806 	 10.193930387496948 	 10.69183611869812 	 0.3081176280975342 	 0.3232126235961914 	 34.05635118484497 	 31.196980476379395 	 0.5147626399993896 	 0.3142423629760742 	 
2025-08-05 23:10:10.878437 test begin: paddle.multiply(x=Tensor([64, 256, 56, 56],"float32"), y=Tensor([64, 256, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([64, 256, 56, 56],"float32"), y=Tensor([64, 256, 1, 1],"float32"), ) 	 51396608 	 33806 	 10.105350494384766 	 10.533658742904663 	 0.30550098419189453 	 0.3181803226470947 	 25.2365665435791 	 30.819379568099976 	 0.3814413547515869 	 0.3104383945465088 	 
2025-08-05 23:11:29.418315 test begin: paddle.multiply(x=Tensor([73, 224, 56, 56],"float32"), y=Tensor([73, 224, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([73, 224, 56, 56],"float32"), y=Tensor([73, 224, 1, 1],"float32"), ) 	 51296224 	 33806 	 10.089625358581543 	 10.503052473068237 	 0.30506062507629395 	 0.31751203536987305 	 25.195732593536377 	 30.762927055358887 	 0.38085246086120605 	 0.3098294734954834 	 
2025-08-05 23:12:48.966380 test begin: paddle.mv(Tensor([1411201, 36],"float32"), Tensor([36],"float32"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([1411201, 36],"float32"), Tensor([36],"float32"), ) 	 50803272 	 64031 	 12.65167498588562 	 12.410727977752686 	 0.20184731483459473 	 0.19806575775146484 	 31.436277389526367 	 26.773863554000854 	 0.16716599464416504 	 0.14236879348754883 	 
2025-08-05 23:14:13.990328 test begin: paddle.mv(Tensor([254017, 100],"float64"), Tensor([100],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([254017, 100],"float64"), Tensor([100],"float64"), ) 	 25401800 	 64031 	 9.97303295135498 	 9.994958639144897 	 0.1593012809753418 	 0.15955185890197754 	 21.594905376434326 	 21.16862154006958 	 0.11483621597290039 	 0.11258077621459961 	 
2025-08-05 23:15:17.486392 test begin: paddle.mv(Tensor([3, 16934401],"float32"), Tensor([16934401],"float32"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([3, 16934401],"float32"), Tensor([16934401],"float32"), ) 	 67737604 	 64031 	 15.072395086288452 	 15.024394512176514 	 0.12024140357971191 	 0.11983561515808105 	 40.266072511672974 	 38.78911566734314 	 0.32131481170654297 	 0.3094947338104248 	 
2025-08-05 23:17:09.342584 test begin: paddle.mv(Tensor([3, 50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([3, 50803201],"float32"), Tensor([50803201],"float32"), ) 	 203212804 	 64031 	 43.42546200752258 	 43.37789726257324 	 0.3464357852935791 	 0.3462405204772949 	 118.80039262771606 	 115.43161344528198 	 0.9480154514312744 	 0.9211583137512207 	 
2025-08-05 23:22:34.362615 test begin: paddle.mv(Tensor([5, 25401601],"float64"), Tensor([25401601],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([5, 25401601],"float64"), Tensor([25401601],"float64"), ) 	 152409606 	 64031 	 60.59387278556824 	 60.26920008659363 	 0.4836699962615967 	 0.4810605049133301 	 153.7474868297577 	 154.22028827667236 	 1.2269947528839111 	 1.2307579517364502 	 
2025-08-05 23:29:50.503706 test begin: paddle.mv(Tensor([5, 5080321],"float64"), Tensor([5080321],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([5, 5080321],"float64"), Tensor([5080321],"float64"), ) 	 30481926 	 64031 	 14.888956546783447 	 14.366055488586426 	 0.11889386177062988 	 0.11467528343200684 	 31.412748098373413 	 31.53781533241272 	 0.25069189071655273 	 0.25162482261657715 	 
2025-08-05 23:31:25.605268 test begin: paddle.mv(Tensor([64, 396901],"float64"), Tensor([396901],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([64, 396901],"float64"), Tensor([396901],"float64"), ) 	 25798565 	 64031 	 9.982184886932373 	 9.908591985702515 	 0.07967138290405273 	 0.07907605171203613 	 20.871415615081787 	 21.21718716621399 	 0.1664879322052002 	 0.1692953109741211 	 
2025-08-05 23:32:29.501343 test begin: paddle.mv(Tensor([793801, 32],"float64"), Tensor([32],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([793801, 32],"float64"), Tensor([32],"float64"), ) 	 25401664 	 64031 	 10.992058038711548 	 10.402197122573853 	 0.17544913291931152 	 0.16601109504699707 	 21.404027700424194 	 21.519005060195923 	 0.11381697654724121 	 0.1143951416015625 	 
2025-08-05 23:33:35.098188 test begin: paddle.nan_to_num(Tensor([148, 114422, 3],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([148, 114422, 3],"float32"), neginf=-1.1920928955078125e-07, ) 	 50803368 	 3470 	 10.436154127120972 	 1.054586410522461 	 0.2795250415802002 	 0.30430150032043457 	 4.358709335327148 	 4.02458119392395 	 0.427931547164917 	 0.2370617389678955 	 
2025-08-05 23:34:02.979621 test begin: paddle.nan_to_num(Tensor([148, 5, 68653],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([148, 5, 68653],"float32"), neginf=-1.1920928955078125e-07, ) 	 50803220 	 3470 	 10.440212965011597 	 1.0335330963134766 	 0.27959752082824707 	 0.30435657501220703 	 4.359072685241699 	 4.023157596588135 	 0.42794275283813477 	 0.23699235916137695 	 
2025-08-05 23:34:28.481293 test begin: paddle.nan_to_num(Tensor([1948, 26080],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([1948, 26080],"float32"), neginf=-1.1920928955078125e-07, ) 	 50803840 	 3470 	 10.436148881912231 	 1.0411794185638428 	 0.2795524597167969 	 0.3043234348297119 	 4.372147798538208 	 4.020007371902466 	 0.4292614459991455 	 0.23677468299865723 	 
2025-08-05 23:34:50.136707 test begin: paddle.nan_to_num(Tensor([25401601, 1],"float64"), neginf=-2.220446049250313e-16, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([25401601, 1],"float64"), neginf=-2.220446049250313e-16, ) 	 25401601 	 3470 	 10.00570297241211 	 1.0372693538665771 	 0.2679481506347656 	 0.3054499626159668 	 3.416827917098999 	 3.5733587741851807 	 0.33541107177734375 	 0.21046876907348633 	 
2025-08-05 23:35:10.360274 test begin: paddle.nan_to_num(Tensor([25401601, 2],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([25401601, 2],"float32"), neginf=-1.1920928955078125e-07, ) 	 50803202 	 3470 	 10.44442343711853 	 1.033458948135376 	 0.27961134910583496 	 0.30431485176086426 	 4.3591368198394775 	 4.023078441619873 	 0.42794275283813477 	 0.23696208000183105 	 
2025-08-05 23:35:32.285506 test begin: paddle.nan_to_num(Tensor([3386881, 5, 3],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([3386881, 5, 3],"float32"), neginf=-1.1920928955078125e-07, ) 	 50803215 	 3470 	 10.443952083587646 	 1.033482551574707 	 0.2796180248260498 	 0.30436158180236816 	 4.360401630401611 	 4.023420095443726 	 0.42807602882385254 	 0.23696446418762207 	 
2025-08-05 23:35:53.906246 test begin: paddle.nan_to_num(Tensor([400, 63505],"float64"), neginf=-2.220446049250313e-16, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([400, 63505],"float64"), neginf=-2.220446049250313e-16, ) 	 25402000 	 3470 	 10.009299516677856 	 1.0373899936676025 	 0.26801204681396484 	 0.3054206371307373 	 3.4151132106781006 	 3.5775105953216553 	 0.33525538444519043 	 0.210693359375 	 
2025-08-05 23:36:13.125662 test begin: paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), -1, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), -1, False, ) 	 50803240 	 5042 	 12.536864280700684 	 8.53117561340332 	 0.25380921363830566 	 0.28827524185180664 	 3.588716983795166 	 3.732808828353882 	 0.24263858795166016 	 0.18919777870178223 	 
2025-08-05 23:36:42.687386 test begin: paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), 2, True, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), 2, True, ) 	 50803240 	 5042 	 14.24397325515747 	 6.944501161575317 	 0.288330078125 	 0.23447465896606445 	 3.730750799179077 	 3.9693427085876465 	 0.25222253799438477 	 0.2011725902557373 	 
2025-08-05 23:37:12.708549 test begin: paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), None, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), None, False, ) 	 50803240 	 5042 	 9.9903404712677 	 5.5498740673065186 	 0.16841602325439453 	 0.1406087875366211 	 2.8364124298095703 	 2.999004364013672 	 0.19211149215698242 	 0.1520371437072754 	 
2025-08-05 23:37:39.300649 test begin: paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), -1, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), -1, False, ) 	 50803230 	 5042 	 12.544580221176147 	 8.52960991859436 	 0.2537989616394043 	 0.28810667991638184 	 3.5623342990875244 	 3.7321784496307373 	 0.24086380004882812 	 0.18915486335754395 	 
2025-08-05 23:38:11.702084 test begin: paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), 2, True, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), 2, True, ) 	 50803230 	 5042 	 71.24569845199585 	 5.697713851928711 	 1.201164960861206 	 0.14441490173339844 	 2.8307576179504395 	 3.1806857585906982 	 0.1914365291595459 	 0.16126060485839844 	 
2025-08-05 23:39:39.147949 test begin: paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), None, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), None, False, ) 	 50803230 	 5042 	 9.990862846374512 	 5.550478219985962 	 0.16841912269592285 	 0.14069509506225586 	 2.80830717086792 	 2.9976046085357666 	 0.18989181518554688 	 0.15196728706359863 	 
2025-08-05 23:40:05.951725 test begin: paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), -1, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), -1, False, ) 	 50803224 	 5042 	 10.28626823425293 	 5.555465936660767 	 0.17340397834777832 	 0.14075326919555664 	 2.8134562969207764 	 3.085437297821045 	 0.19024038314819336 	 0.1564502716064453 	 
2025-08-05 23:40:28.542794 test begin: paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), 2, True, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), 2, True, ) 	 50803224 	 5042 	 11.746531009674072 	 6.936720371246338 	 0.2378838062286377 	 0.2342360019683838 	 3.7987935543060303 	 3.9921162128448486 	 0.25676512718200684 	 0.2023327350616455 	 
2025-08-05 23:40:56.086895 test begin: paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), None, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), None, False, ) 	 50803224 	 5042 	 9.991344213485718 	 5.550840139389038 	 0.16843128204345703 	 0.14061212539672852 	 2.8082895278930664 	 2.9974308013916016 	 0.1899125576019287 	 0.15198397636413574 	 
2025-08-05 23:41:18.958148 test begin: paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), -1, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), -1, False, ) 	 50803260 	 5042 	 12.533577919006348 	 8.539373874664307 	 0.25376248359680176 	 0.2881619930267334 	 3.5882794857025146 	 3.7314884662628174 	 0.24257183074951172 	 0.1891155242919922 	 
2025-08-05 23:41:50.467079 test begin: paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), 2, True, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), 2, True, ) 	 50803260 	 5042 	 14.236121416091919 	 6.9498443603515625 	 0.2882418632507324 	 0.23447775840759277 	 3.7304067611694336 	 3.9681694507598877 	 0.25219273567199707 	 0.2011256217956543 	 
2025-08-05 23:42:21.485258 test begin: paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), None, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), None, False, ) 	 50803260 	 5042 	 9.99014925956726 	 5.54956841468811 	 0.16841959953308105 	 0.14066815376281738 	 2.835740566253662 	 2.9980170726776123 	 0.19175481796264648 	 0.1520085334777832 	 
2025-08-05 23:42:43.741009 test begin: paddle.nanmedian(Tensor([2, 254016],"float32"), axis=1, mode="min", )
[Prof] paddle.nanmedian 	 paddle.nanmedian(Tensor([2, 254016],"float32"), axis=1, mode="min", ) 	 508032 	 2836 	 10.009087562561035 	 3.316296339035034 	 0.003412485122680664 	 1.1950750350952148 	 None 	 None 	 None 	 None 	 combined
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 23:42:57.325745 test begin: paddle.nanmedian(Tensor([508032],"float32"), keepdim=False, )
[Prof] paddle.nanmedian 	 paddle.nanmedian(Tensor([508032],"float32"), keepdim=False, ) 	 508032 	 2836 	 19.524363040924072 	 1.1640055179595947 	 0.0067675113677978516 	 7.343292236328125e-05 	 0.16566729545593262 	 1.537766933441162 	 4.100799560546875e-05 	 0.000331878662109375 	 combined
2025-08-05 23:43:22.588346 test begin: paddle.nanmedian(Tensor([508032],"float32"), keepdim=False, mode="min", )
[Prof] paddle.nanmedian 	 paddle.nanmedian(Tensor([508032],"float32"), keepdim=False, mode="min", ) 	 508032 	 2836 	 19.531137943267822 	 0.9212963581085205 	 0.006771087646484375 	 0.00013637542724609375 	 0.19321084022521973 	 0.40312790870666504 	 6.222724914550781e-05 	 7.43865966796875e-05 	 combined
2025-08-05 23:43:43.868311 test begin: paddle.nanquantile(Tensor([4, 10584, 6],"float64"), q=0, axis=1, )
W0805 23:43:43.882586 113889 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([4, 10584, 6],"float64"), q=0, axis=1, ) 	 254016 	 24384 	 13.691699504852295 	 9.957155227661133 	 5.555152893066406e-05 	 0.0002467632293701172 	 6.522324562072754 	 5.7541184425354 	 9.608268737792969e-05 	 7.748603820800781e-05 	 
2025-08-05 23:44:19.922767 test begin: paddle.nanquantile(Tensor([4, 10584, 6],"float64"), q=0.35, axis=2, keepdim=True, )
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([4, 10584, 6],"float64"), q=0.35, axis=2, keepdim=True, ) 	 254016 	 24384 	 462.88691568374634 	 5.312344789505005 	 0.588310956954956 	 7.200241088867188e-05 	 4.2235517501831055 	 5.36128044128418 	 4.2438507080078125e-05 	 7.033348083496094e-05 	 
2025-08-05 23:52:17.770176 test begin: paddle.nanquantile(Tensor([4, 7, 9072],"float64"), q=0, axis=1, )
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([4, 7, 9072],"float64"), q=0, axis=1, ) 	 254016 	 24384 	 397.9310586452484 	 5.506020784378052 	 0.45684361457824707 	 7.700920104980469e-05 	 5.046683311462402 	 5.751681327819824 	 3.5762786865234375e-05 	 7.557868957519531e-05 	 
2025-08-05 23:59:12.038501 test begin: paddle.nanquantile(Tensor([4, 7, 9072],"float64"), q=0.35, axis=2, keepdim=True, )
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([4, 7, 9072],"float64"), q=0.35, axis=2, keepdim=True, ) 	 254016 	 24384 	 10.031818151473999 	 7.526038885116577 	 0.0001354217529296875 	 0.006762266159057617 	 4.194398641586304 	 5.0784125328063965 	 9.012222290039062e-05 	 0.0001761913299560547 	 
2025-08-05 23:59:39.263362 test begin: paddle.nanquantile(Tensor([6048, 7, 6],"float64"), q=0, axis=1, )
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([6048, 7, 6],"float64"), q=0, axis=1, ) 	 254016 	 24384 	 398.25992155075073 	 5.513486862182617 	 0.45729565620422363 	 7.605552673339844e-05 	 4.954073429107666 	 5.791062355041504 	 5.841255187988281e-05 	 7.510185241699219e-05 	 
2025-08-06 00:06:33.865123 test begin: paddle.nanquantile(Tensor([6048, 7, 6],"float64"), q=0.35, axis=2, keepdim=True, )
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([6048, 7, 6],"float64"), q=0.35, axis=2, keepdim=True, ) 	 254016 	 24384 	 462.66397976875305 	 5.33028769493103 	 0.58785080909729 	 9.870529174804688e-05 	 4.212349891662598 	 5.109382152557373 	 4.76837158203125e-05 	 7.915496826171875e-05 	 
2025-08-06 00:14:31.217806 test begin: paddle.nansum(Tensor([2, 1270081, 4, 5],"float32"), axis=None, keepdim=False, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 1270081, 4, 5],"float32"), axis=None, keepdim=False, name=None, ) 	 50803240 	 10275 	 10.36597728729248 	 1.5702250003814697 	 0.2059009075164795 	 0.07812881469726562 	 5.683214426040649 	 6.065269708633423 	 0.28266072273254395 	 0.20103764533996582 	 
2025-08-06 00:14:55.793738 test begin: paddle.nansum(Tensor([2, 1270081, 4, 5],"float32"), axis=None, keepdim=True, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 1270081, 4, 5],"float32"), axis=None, keepdim=True, name=None, ) 	 50803240 	 10275 	 10.364419221878052 	 1.5703485012054443 	 0.20597243309020996 	 0.07808256149291992 	 5.682818651199341 	 6.065459489822388 	 0.28261613845825195 	 0.20110058784484863 	 
2025-08-06 00:15:21.334389 test begin: paddle.nansum(Tensor([2, 3, 1693441, 5],"float32"), axis=None, keepdim=False, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 3, 1693441, 5],"float32"), axis=None, keepdim=False, name=None, ) 	 50803230 	 10275 	 10.369487524032593 	 1.5706379413604736 	 0.2059929370880127 	 0.07813024520874023 	 5.684461832046509 	 6.065293312072754 	 0.2826976776123047 	 0.20109152793884277 	 
2025-08-06 00:15:50.399732 test begin: paddle.nansum(Tensor([2, 3, 1693441, 5],"float32"), axis=None, keepdim=True, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 3, 1693441, 5],"float32"), axis=None, keepdim=True, name=None, ) 	 50803230 	 10275 	 10.368019104003906 	 1.5703909397125244 	 0.20592546463012695 	 0.07815003395080566 	 5.682722568511963 	 6.06500768661499 	 0.2825937271118164 	 0.20110082626342773 	 
2025-08-06 00:16:14.951726 test begin: paddle.nansum(Tensor([2, 3, 4, 2116801],"float32"), axis=None, keepdim=False, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 3, 4, 2116801],"float32"), axis=None, keepdim=False, name=None, ) 	 50803224 	 10275 	 10.364590406417847 	 1.5702741146087646 	 0.20590901374816895 	 0.07811403274536133 	 5.684031009674072 	 6.064526796340942 	 0.2825932502746582 	 0.2010495662689209 	 
2025-08-06 00:16:42.654767 test begin: paddle.nansum(Tensor([2, 3, 4, 2116801],"float32"), axis=None, keepdim=True, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 3, 4, 2116801],"float32"), axis=None, keepdim=True, name=None, ) 	 50803224 	 10275 	 10.367154836654663 	 1.5704188346862793 	 0.20591259002685547 	 0.07814455032348633 	 5.682835340499878 	 6.064300537109375 	 0.28255510330200195 	 0.2010636329650879 	 
2025-08-06 00:17:07.215000 test begin: paddle.nansum(Tensor([846721, 3, 4, 5],"float32"), axis=None, keepdim=False, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([846721, 3, 4, 5],"float32"), axis=None, keepdim=False, name=None, ) 	 50803260 	 10275 	 10.370606184005737 	 1.570237159729004 	 0.20592641830444336 	 0.07812309265136719 	 5.682550430297852 	 6.063855409622192 	 0.2826204299926758 	 0.20102906227111816 	 
2025-08-06 00:17:31.756524 test begin: paddle.nansum(Tensor([846721, 3, 4, 5],"float32"), axis=None, keepdim=True, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([846721, 3, 4, 5],"float32"), axis=None, keepdim=True, name=None, ) 	 50803260 	 10275 	 10.36765456199646 	 1.5703403949737549 	 0.20621705055236816 	 0.07811474800109863 	 5.682646036148071 	 6.063685655593872 	 0.28258347511291504 	 0.20099973678588867 	 
2025-08-06 00:17:56.298892 test begin: paddle.nansum(x=Tensor([105841, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([105841, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401840 	 10275 	 10.938430786132812 	 1.9453632831573486 	 0.27225494384765625 	 0.19352006912231445 	 5.443074464797974 	 4.537022829055786 	 0.27069926261901855 	 0.15035438537597656 	 
2025-08-06 00:18:19.872465 test begin: paddle.nansum(x=Tensor([3, 2, 105841, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 2, 105841, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401840 	 10275 	 10.938144207000732 	 1.9517574310302734 	 0.27220773696899414 	 0.19347047805786133 	 5.444973945617676 	 4.537147283554077 	 0.2707188129425049 	 0.15036296844482422 	 
2025-08-06 00:18:48.530987 test begin: paddle.nansum(x=Tensor([3, 2, 3, 141121, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 2, 3, 141121, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401780 	 10275 	 57.79103899002075 	 1.782684564590454 	 1.152130126953125 	 0.08868646621704102 	 4.7821760177612305 	 4.289582252502441 	 0.23778295516967773 	 0.1421515941619873 	 
2025-08-06 00:20:00.824624 test begin: paddle.nansum(x=Tensor([3, 2, 3, 4, 176401, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 2, 3, 4, 176401, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401744 	 10275 	 10.012081623077393 	 1.9571332931518555 	 0.24908947944641113 	 0.19403791427612305 	 5.254672527313232 	 4.55931544303894 	 0.2612881660461426 	 0.15109038352966309 	 
2025-08-06 00:20:23.404962 test begin: paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 1, 70561],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 1, 70561],"float64"), axis=3, keepdim=True, ) 	 25401960 	 10275 	 10.01461672782898 	 2.7529256343841553 	 0.24920892715454102 	 0.19431781768798828 	 5.254627704620361 	 4.563563346862793 	 0.26129770278930664 	 0.1512303352355957 	 
2025-08-06 00:20:47.881492 test begin: paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 35281, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 35281, 2],"float64"), axis=3, keepdim=True, ) 	 25402320 	 10275 	 10.017044305801392 	 1.9545786380767822 	 0.24919986724853516 	 0.1941535472869873 	 5.2563018798828125 	 4.576432466506958 	 0.261364221572876 	 0.15171599388122559 	 
2025-08-06 00:21:12.507603 test begin: paddle.nansum(x=Tensor([3, 70561, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 70561, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401960 	 10275 	 10.950183391571045 	 1.945389747619629 	 0.2723672389984131 	 0.19347143173217773 	 5.435916423797607 	 4.540326833724976 	 0.27031421661376953 	 0.15049242973327637 	 
2025-08-06 00:21:38.937512 test begin: paddle.neg(Tensor([3175201, 8],"float64"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([3175201, 8],"float64"), ) 	 25401608 	 33849 	 11.179392337799072 	 10.088545083999634 	 0.3045351505279541 	 0.30461788177490234 	 10.073278903961182 	 10.086658954620361 	 0.30412936210632324 	 0.3045351505279541 	 
2025-08-06 00:22:22.535043 test begin: paddle.neg(Tensor([32, 1587601],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([32, 1587601],"float32"), ) 	 50803232 	 33849 	 10.003305196762085 	 10.086455583572388 	 0.3020620346069336 	 0.3041973114013672 	 10.02278447151184 	 10.075162649154663 	 0.30257153511047363 	 0.304210901260376 	 
2025-08-06 00:23:04.568141 test begin: paddle.neg(Tensor([32, 793801],"float64"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([32, 793801],"float64"), ) 	 25401632 	 33849 	 10.089715957641602 	 10.089048385620117 	 0.3045642375946045 	 0.30467915534973145 	 10.073366641998291 	 10.087332963943481 	 0.3041360378265381 	 0.3045766353607178 	 
2025-08-06 00:23:46.066565 test begin: paddle.neg(Tensor([6350401, 8],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([6350401, 8],"float32"), ) 	 50803208 	 33849 	 10.003333568572998 	 10.07590103149414 	 0.30196475982666016 	 0.3041837215423584 	 10.022738695144653 	 10.075334548950195 	 0.302626371383667 	 0.3041853904724121 	 
2025-08-06 00:24:29.258416 test begin: paddle.neg(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33849 	 11.148151636123657 	 10.075830221176147 	 0.3022043704986572 	 0.3042006492614746 	 10.022942543029785 	 10.07508134841919 	 0.3025503158569336 	 0.30414748191833496 	 
2025-08-06 00:25:13.567741 test begin: paddle.neg(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33849 	 10.009023427963257 	 10.745524644851685 	 0.30214929580688477 	 0.3042795658111572 	 10.022890090942383 	 10.075302362442017 	 0.3025996685028076 	 0.30419111251831055 	 
2025-08-06 00:25:57.301746 test begin: paddle.neg(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33849 	 10.007115840911865 	 10.075727224349976 	 0.30216193199157715 	 0.30419468879699707 	 10.022908210754395 	 10.075127124786377 	 0.3026149272918701 	 0.30417299270629883 	 
2025-08-06 00:26:39.489540 test begin: paddle.negative(Tensor([1693441, 3, 4, 5],"float16"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([1693441, 3, 4, 5],"float16"), ) 	 101606460 	 33834 	 10.896620273590088 	 10.01278018951416 	 0.30499911308288574 	 0.3024561405181885 	 10.085752487182617 	 10.013013362884521 	 0.30464863777160645 	 0.3024141788482666 	 
2025-08-06 00:27:24.707748 test begin: paddle.negative(Tensor([2, 1270081, 4, 5],"float32"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 1270081, 4, 5],"float32"), ) 	 50803240 	 33834 	 9.998926877975464 	 10.071470975875854 	 0.30204176902770996 	 0.30423474311828613 	 10.018317699432373 	 10.070760488510132 	 0.30260348320007324 	 0.304180383682251 	 
2025-08-06 00:28:06.999953 test begin: paddle.negative(Tensor([2, 2540161, 4, 5],"float16"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 2540161, 4, 5],"float16"), ) 	 101606440 	 33834 	 10.097312211990356 	 10.012752056121826 	 0.3050081729888916 	 0.30249500274658203 	 10.085405826568604 	 10.014583349227905 	 0.30462050437927246 	 0.3024754524230957 	 
2025-08-06 00:28:51.037170 test begin: paddle.negative(Tensor([2, 3, 1693441, 5],"float32"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 1693441, 5],"float32"), ) 	 50803230 	 33834 	 9.998623371124268 	 10.736460208892822 	 0.30202436447143555 	 0.304248571395874 	 10.018716096878052 	 10.070481300354004 	 0.3026583194732666 	 0.30417418479919434 	 
2025-08-06 00:29:34.849788 test begin: paddle.negative(Tensor([2, 3, 3386881, 5],"float16"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 3386881, 5],"float16"), ) 	 101606430 	 33834 	 10.876712322235107 	 10.012949705123901 	 0.3050222396850586 	 0.3025217056274414 	 10.0858473777771 	 10.013059139251709 	 0.30464935302734375 	 0.3024623394012451 	 
2025-08-06 00:30:19.929787 test begin: paddle.negative(Tensor([2, 3, 4, 1058401],"float64"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 4, 1058401],"float64"), ) 	 25401624 	 33834 	 10.668156623840332 	 10.093399286270142 	 0.30454564094543457 	 0.3046236038208008 	 10.068871021270752 	 10.083274841308594 	 0.3041219711303711 	 0.3046140670776367 	 
2025-08-06 00:31:02.795608 test begin: paddle.negative(Tensor([2, 3, 4, 2116801],"float32"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 4, 2116801],"float32"), ) 	 50803224 	 33834 	 9.999085664749146 	 11.711142301559448 	 0.30201220512390137 	 0.30425453186035156 	 10.018152475357056 	 10.070683717727661 	 0.30259132385253906 	 0.3042023181915283 	 
2025-08-06 00:31:47.378340 test begin: paddle.negative(Tensor([2, 3, 4, 4233601],"float16"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 4, 4233601],"float16"), ) 	 101606424 	 33834 	 10.100425004959106 	 10.012624979019165 	 0.3050069808959961 	 0.30242204666137695 	 10.085508108139038 	 10.012852907180786 	 0.304628849029541 	 0.30245089530944824 	 
2025-08-06 00:32:31.423736 test begin: paddle.negative(Tensor([2, 3, 846721, 5],"float64"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 846721, 5],"float64"), ) 	 25401630 	 33834 	 10.78869342803955 	 10.089296817779541 	 0.3045618534088135 	 0.3045523166656494 	 10.068847417831421 	 10.082741498947144 	 0.3041255474090576 	 0.30460262298583984 	 
2025-08-06 00:33:14.885326 test begin: paddle.negative(Tensor([2, 635041, 4, 5],"float64"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 635041, 4, 5],"float64"), ) 	 25401640 	 33834 	 10.08297061920166 	 10.63883376121521 	 0.30455756187438965 	 0.3046743869781494 	 10.068801641464233 	 10.082652568817139 	 0.3041384220123291 	 0.3045072555541992 	 
2025-08-06 00:33:58.750931 test begin: paddle.negative(Tensor([423361, 3, 4, 5],"float64"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([423361, 3, 4, 5],"float64"), ) 	 25401660 	 33834 	 10.082774639129639 	 10.08641505241394 	 0.3046097755432129 	 0.3045930862426758 	 10.068754434585571 	 10.083095788955688 	 0.3041553497314453 	 0.30455851554870605 	 
2025-08-06 00:34:41.695441 test begin: paddle.negative(Tensor([846721, 3, 4, 5],"float32"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([846721, 3, 4, 5],"float32"), ) 	 50803260 	 33834 	 10.003079175949097 	 10.071401357650757 	 0.3020634651184082 	 0.3041837215423584 	 10.018310070037842 	 10.070693969726562 	 0.30263328552246094 	 0.3042173385620117 	 
2025-08-06 00:35:23.776963 test begin: paddle.nextafter(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), ) 	 101606480 	 22191 	 10.000949144363403 	 9.96378231048584 	 0.4606308937072754 	 0.45863938331604004 	 None 	 None 	 None 	 None 	 
2025-08-06 00:35:48.378631 test begin: paddle.nextafter(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), ) 	 101606460 	 22191 	 10.003358840942383 	 9.959333658218384 	 0.46064281463623047 	 0.45867919921875 	 None 	 None 	 None 	 None 	 
2025-08-06 00:36:11.808870 test begin: paddle.nextafter(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), ) 	 101606448 	 22191 	 10.001914024353027 	 9.95921802520752 	 0.46065330505371094 	 0.458651065826416 	 None 	 None 	 None 	 None 	 
2025-08-06 00:36:33.521747 test begin: paddle.nextafter(Tensor([4, 3, 2116801],"float32"), Tensor([4, 3, 2116801],"float64"), )
W0806 00:36:34.373232 115503 dygraph_functions.cc:57914] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3, 2116801],"float32"), Tensor([4, 3, 2116801],"float64"), ) 	 50803224 	 22191 	 15.196092367172241 	 8.603007078170776 	 0.34987664222717285 	 0.39620113372802734 	 None 	 None 	 None 	 None 	 
2025-08-06 00:36:58.345434 test begin: paddle.nextafter(Tensor([4, 3, 2116801],"float64"), Tensor([4, 3, 2116801],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3, 2116801],"float64"), Tensor([4, 3, 2116801],"float32"), ) 	 50803224 	 22191 	 15.196542978286743 	 8.519893646240234 	 0.3499717712402344 	 0.3921315670013428 	 None 	 None 	 None 	 None 	 
2025-08-06 00:37:24.162005 test begin: paddle.nextafter(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float64"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float64"), ) 	 101606424 	 22191 	 30.256253719329834 	 17.007264614105225 	 0.6966660022735596 	 0.7832279205322266 	 None 	 None 	 None 	 None 	 
2025-08-06 00:38:13.489480 test begin: paddle.nextafter(Tensor([4, 3, 4233601],"float64"), Tensor([4, 3, 4233601],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3, 4233601],"float64"), Tensor([4, 3, 4233601],"float32"), ) 	 101606424 	 22191 	 30.25899863243103 	 16.835952758789062 	 0.6967427730560303 	 0.7752130031585693 	 None 	 None 	 None 	 None 	 
2025-08-06 00:39:02.574983 test begin: paddle.nextafter(Tensor([4, 3175201, 2],"float32"), Tensor([4, 3175201, 2],"float64"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3175201, 2],"float32"), Tensor([4, 3175201, 2],"float64"), ) 	 50803216 	 22191 	 15.189754247665405 	 8.60292911529541 	 0.34970521926879883 	 0.3961975574493408 	 None 	 None 	 None 	 None 	 
2025-08-06 00:39:27.385269 test begin: paddle.nextafter(Tensor([4, 3175201, 2],"float64"), Tensor([4, 3175201, 2],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3175201, 2],"float64"), Tensor([4, 3175201, 2],"float32"), ) 	 50803216 	 22191 	 15.210425615310669 	 8.51718544960022 	 0.35021233558654785 	 0.3920712471008301 	 None 	 None 	 None 	 None 	 
2025-08-06 00:39:52.527007 test begin: paddle.nextafter(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float64"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float64"), ) 	 101606416 	 22191 	 30.253037214279175 	 17.007273197174072 	 0.6965963840484619 	 0.7832441329956055 	 None 	 None 	 None 	 None 	 
2025-08-06 00:40:42.278437 test begin: paddle.nextafter(Tensor([4, 6350401, 2],"float64"), Tensor([4, 6350401, 2],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 6350401, 2],"float64"), Tensor([4, 6350401, 2],"float32"), ) 	 101606416 	 22191 	 30.265154361724854 	 16.837340593338013 	 0.6968727111816406 	 0.7754113674163818 	 None 	 None 	 None 	 None 	 
2025-08-06 00:41:33.234274 test begin: paddle.nextafter(Tensor([4233601, 3, 2],"float32"), Tensor([4233601, 3, 2],"float64"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4233601, 3, 2],"float32"), Tensor([4233601, 3, 2],"float64"), ) 	 50803212 	 22191 	 15.204854249954224 	 8.60328722000122 	 0.35005950927734375 	 0.3961467742919922 	 None 	 None 	 None 	 None 	 
2025-08-06 00:41:58.494265 test begin: paddle.nextafter(Tensor([4233601, 3, 2],"float64"), Tensor([4233601, 3, 2],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4233601, 3, 2],"float64"), Tensor([4233601, 3, 2],"float32"), ) 	 50803212 	 22191 	 15.190780639648438 	 8.513936042785645 	 0.349884033203125 	 0.3921060562133789 	 None 	 None 	 None 	 None 	 
2025-08-06 00:42:23.233944 test begin: paddle.nextafter(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float64"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float64"), ) 	 101606412 	 22191 	 30.25419807434082 	 17.007636547088623 	 0.6967558860778809 	 0.7832705974578857 	 None 	 None 	 None 	 None 	 
2025-08-06 00:43:12.471660 test begin: paddle.nextafter(Tensor([8467201, 3, 2],"float64"), Tensor([8467201, 3, 2],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([8467201, 3, 2],"float64"), Tensor([8467201, 3, 2],"float32"), ) 	 101606412 	 22191 	 30.257047176361084 	 16.840887784957886 	 0.6965668201446533 	 0.7752676010131836 	 None 	 None 	 None 	 None 	 
2025-08-06 00:44:01.557296 test begin: paddle.nextafter(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), ) 	 101606520 	 22191 	 10.001643657684326 	 9.973678350448608 	 0.46062397956848145 	 0.4586491584777832 	 None 	 None 	 None 	 None 	 
2025-08-06 00:44:24.965167 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([124, 1536, 267],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([124, 1536, 267],"float32"), 1, None, ) 	 50853888 	 52811 	 15.949977397918701 	 9.25160002708435 	 0.30867981910705566 	 0.17901349067687988 	 44.347939252853394 	 8.9650137424469 	 0.428877592086792 	 0.17334580421447754 	 
2025-08-06 00:45:45.519492 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([124, 8362, 49],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([124, 8362, 49],"float32"), 1, None, ) 	 50807512 	 52811 	 18.775920867919922 	 19.964287996292114 	 0.362656831741333 	 0.386641263961792 	 45.74812126159668 	 9.222101211547852 	 0.44281888008117676 	 0.17847561836242676 	 
2025-08-06 00:47:20.125366 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([128, 1536, 259],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([128, 1536, 259],"float32"), 1, None, ) 	 50921472 	 52811 	 15.074973583221436 	 8.116360187530518 	 0.2917468547821045 	 0.1570286750793457 	 44.422783851623535 	 9.018917560577393 	 0.429811954498291 	 0.1745450496673584 	 
2025-08-06 00:48:40.642212 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([128, 8101, 49],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([128, 8101, 49],"float32"), 1, None, ) 	 50809472 	 52811 	 19.234424114227295 	 19.98220181465149 	 0.3636355400085449 	 0.3867971897125244 	 45.761717319488525 	 9.226094722747803 	 0.4426589012145996 	 0.17860794067382812 	 
2025-08-06 00:50:16.297664 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([345, 1024, 144],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([345, 1024, 144],"float32"), 1, None, ) 	 50872320 	 52811 	 24.2622127532959 	 11.59499716758728 	 0.469010591506958 	 0.22436213493347168 	 44.838958978652954 	 9.020198822021484 	 0.43382692337036133 	 0.17435622215270996 	 
2025-08-06 00:51:47.052153 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([64, 1024, 776],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([64, 1024, 776],"float32"), 1, None, ) 	 50855936 	 52811 	 10.000447034835815 	 8.134793519973755 	 0.19353008270263672 	 0.1573953628540039 	 43.465500593185425 	 9.05974268913269 	 0.420696496963501 	 0.17493414878845215 	 
2025-08-06 00:52:59.571867 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([64, 5513, 144],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([64, 5513, 144],"float32"), 1, None, ) 	 50807808 	 52811 	 24.19682812690735 	 11.556727647781372 	 0.46822500228881836 	 0.22368335723876953 	 44.774720430374146 	 8.99561858177185 	 0.43328166007995605 	 0.17404627799987793 	 
2025-08-06 00:54:30.076488 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([676, 1536, 49],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([676, 1536, 49],"float32"), 1, None, ) 	 50878464 	 52811 	 18.80659008026123 	 19.99527645111084 	 0.3635523319244385 	 0.38709378242492676 	 45.82080006599426 	 9.243313074111938 	 0.44336748123168945 	 0.1789391040802002 	 
2025-08-06 00:56:05.912739 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 2048, 2, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 2048, 2, 7],"float32"), output_size=1, ) 	 58404864 	 33312 	 10.012756586074829 	 12.16032099723816 	 0.30721211433410645 	 0.37306642532348633 	 34.2754909992218 	 7.488412857055664 	 0.5257689952850342 	 0.22973966598510742 	 
2025-08-06 00:57:10.916188 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 2048, 7, 2],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 2048, 7, 2],"float32"), output_size=1, ) 	 58404864 	 33312 	 10.011941909790039 	 12.164143800735474 	 0.30716848373413086 	 0.3729860782623291 	 34.275322675704956 	 7.486851692199707 	 0.5257863998413086 	 0.2296886444091797 	 
2025-08-06 00:58:16.880396 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 509, 7, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 509, 7, 7],"float32"), output_size=1, ) 	 50804817 	 33312 	 11.834530591964722 	 12.586095094680786 	 0.36307334899902344 	 0.3861393928527832 	 28.82541537284851 	 5.813473224639893 	 0.44207239151000977 	 0.17831063270568848 	 
2025-08-06 00:59:17.910409 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 2048, 2, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 2048, 2, 7],"float32"), output_size=1, ) 	 58634240 	 33312 	 10.044752359390259 	 12.200050354003906 	 0.3081400394439697 	 0.3743109703063965 	 34.385382890701294 	 7.502875089645386 	 0.5273752212524414 	 0.23014473915100098 	 
2025-08-06 01:00:23.116183 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 2048, 7, 2],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 2048, 7, 2],"float32"), output_size=1, ) 	 58634240 	 33312 	 10.041894435882568 	 12.203139066696167 	 0.30811285972595215 	 0.3742649555206299 	 34.38244819641113 	 7.5027830600738525 	 0.5274257659912109 	 0.23015284538269043 	 
2025-08-06 01:01:29.270685 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 507, 7, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 507, 7, 7],"float32"), output_size=1, ) 	 50803935 	 33312 	 11.836981296539307 	 12.579248905181885 	 0.36319446563720703 	 0.38590192794799805 	 28.824219703674316 	 5.813313961029053 	 0.44216251373291016 	 0.1783463954925537 	 
2025-08-06 01:02:30.301316 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 2048, 2, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 2048, 2, 7],"float32"), output_size=1, ) 	 58720256 	 33312 	 10.05764651298523 	 12.217257976531982 	 0.30852723121643066 	 0.37482786178588867 	 34.44086408615112 	 7.511141777038574 	 0.5283501148223877 	 0.23045015335083008 	 
2025-08-06 01:03:37.488790 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 2048, 7, 2],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 2048, 7, 2],"float32"), output_size=1, ) 	 58720256 	 33312 	 10.868062019348145 	 12.21988582611084 	 0.3087747097015381 	 0.374863862991333 	 34.459166288375854 	 7.512437343597412 	 0.5283200740814209 	 0.23044133186340332 	 
2025-08-06 01:04:46.441734 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 507, 7, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 507, 7, 7],"float32"), output_size=1, ) 	 50878464 	 33312 	 11.858948707580566 	 12.602418184280396 	 0.3639397621154785 	 0.386568546295166 	 28.928852796554565 	 5.8296284675598145 	 0.4437415599822998 	 0.1788804531097412 	 
2025-08-06 01:05:47.568491 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([507, 2048, 7, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([507, 2048, 7, 7],"float32"), output_size=1, ) 	 50878464 	 33312 	 11.854116678237915 	 12.59851360321045 	 0.36365413665771484 	 0.3865482807159424 	 28.909641981124878 	 5.829091310501099 	 0.4433777332305908 	 0.1788172721862793 	 
2025-08-06 01:06:48.072880 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 45361, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 45361, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50804320 	 19743 	 17.680408477783203 	 2.9410953521728516 	 0.9152021408081055 	 0.152268648147583 	 44.10811996459961 	 3.412520170211792 	 1.1419038772583008 	 0.17664551734924316 	 
2025-08-06 01:07:57.632094 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 50401, 16, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 50401, 16, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50804208 	 19743 	 9.981610298156738 	 2.9950778484344482 	 0.5167455673217773 	 0.15503430366516113 	 44.79009461402893 	 3.407111883163452 	 1.158031940460205 	 0.17639398574829102 	 
2025-08-06 01:09:01.060432 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 50401, 16, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 50401, 16, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50804208 	 19743 	 10.763979196548462 	 2.995059013366699 	 0.5571000576019287 	 0.15507936477661133 	 44.76175808906555 	 3.40751576423645 	 1.156935214996338 	 0.1763172149658203 	 
2025-08-06 01:10:03.865340 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 1051, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 1051, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50851584 	 19743 	 315.64044857025146 	 2.9798402786254883 	 16.339921951293945 	 0.1542832851409912 	 44.678581953048706 	 3.267594337463379 	 1.1556057929992676 	 0.1691446304321289 	 
2025-08-06 01:16:11.526279 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 1051, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 1051, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50851584 	 19743 	 381.6752219200134 	 2.979844570159912 	 19.761040210723877 	 0.1542215347290039 	 44.70459342002869 	 3.2573776245117188 	 1.1561520099639893 	 0.16862702369689941 	 
2025-08-06 01:23:25.040161 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 414, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f17b1b4a8c0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 01:34:11.628921 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 460, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
W0806 01:34:16.521289 117588 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3d8c456ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 01:44:16.392927 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 591, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
W0806 01:44:17.380642 117970 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 591, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50835456 	 19743 	 482.7215647697449 	 2.9475128650665283 	 24.985922813415527 	 0.15256118774414062 	 43.86647605895996 	 3.2572786808013916 	 1.1354260444641113 	 0.16853070259094238 	 
2025-08-06 01:53:10.711240 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 591],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 591],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50835456 	 19743 	 458.34008860588074 	 2.956639051437378 	 23.69854426383972 	 0.15305304527282715 	 42.71590065956116 	 3.254387140274048 	 1.1055986881256104 	 0.16852378845214844 	 
2025-08-06 02:01:40.789221 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 9, 460],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0376533d30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 02:12:24.677064 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 946, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
W0806 02:12:25.636224 118668 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 946, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50856960 	 19743 	 343.6588625907898 	 2.957821846008301 	 17.790992259979248 	 0.15314149856567383 	 43.97835731506348 	 3.2562403678894043 	 1.1381700038909912 	 0.16853570938110352 	 
2025-08-06 02:19:00.052647 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([60, 768, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([60, 768, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 51609600 	 19743 	 17.781398057937622 	 2.9796221256256104 	 0.9203863143920898 	 0.15424084663391113 	 44.77459359169006 	 3.474391222000122 	 1.1587743759155273 	 0.17987775802612305 	 
2025-08-06 02:20:09.948369 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([66, 768, 16, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([66, 768, 16, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 51093504 	 19743 	 10.036976337432861 	 3.003917694091797 	 0.5198225975036621 	 0.15546488761901855 	 45.00926613807678 	 3.4207072257995605 	 1.1649448871612549 	 0.1770799160003662 	 
2025-08-06 02:21:14.854218 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([66, 768, 16, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([66, 768, 16, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 51093504 	 19743 	 10.82796573638916 	 3.0037002563476562 	 0.5606589317321777 	 0.1555347442626953 	 44.99905490875244 	 3.4208672046661377 	 1.1646997928619385 	 0.1770319938659668 	 
2025-08-06 02:22:17.991839 test begin: paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([128, 16],"float32"), Tensor([128],"int64"), Tensor([16, 3175201],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, )
W0806 02:22:18.824617 118843 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.adaptive_log_softmax_with_loss 	 paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([128, 16],"float32"), Tensor([128],"int64"), Tensor([16, 3175201],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, ) 	 50805392 	 1469 	 17.90526509284973 	 11.975975751876831 	 0.009468555450439453 	 0.006634235382080078 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:23:20.267260 test begin: paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([25401601, 16],"float32"), Tensor([25401601],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, )
[Prof] paddle.nn.functional.adaptive_log_softmax_with_loss 	 paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([25401601, 16],"float32"), Tensor([25401601],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, ) 	 431827345 	 1469 	 73.31726026535034 	 30.962846755981445 	 0.007304191589355469 	 0.006116390228271484 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:26:13.600785 test begin: paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([3175201, 16],"float32"), Tensor([3175201],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, )
[Prof] paddle.nn.functional.adaptive_log_softmax_with_loss 	 paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([3175201, 16],"float32"), Tensor([3175201],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, ) 	 53978545 	 1469 	 9.9563307762146 	 4.745043039321899 	 0.0008022785186767578 	 0.0006177425384521484 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:26:43.243873 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 4233601],"float64"), 8, False, None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5944947df0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 02:37:51.417669 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 8467201],"float32"), 16, False, None, )
W0806 02:37:52.447126 119477 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa831e1f010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 02:47:58.451661 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 8467201],"float32"), output_size=16, )
W0806 02:47:59.604105 119943 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0708722d10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 02:58:06.509594 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 396901, 32],"float64"), 8, False, None, )
W0806 02:58:07.209729 120649 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 396901, 32],"float64"), 8, False, None, ) 	 25401664 	 48051 	 10.015082597732544 	 41.51756715774536 	 0.21297407150268555 	 0.8827214241027832 	 25.075741052627563 	 37.389657735824585 	 0.26677608489990234 	 0.3977038860321045 	 
2025-08-06 03:00:01.964199 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 793801, 32],"float32"), 16, False, None, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 793801, 32],"float32"), 16, False, None, ) 	 50803264 	 48051 	 19.6442391872406 	 82.36556649208069 	 0.41758155822753906 	 1.7470085620880127 	 42.67763137817383 	 67.78301668167114 	 0.4537773132324219 	 0.7207529544830322 	 
2025-08-06 03:03:40.493608 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 793801, 32],"float32"), output_size=16, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 793801, 32],"float32"), output_size=16, ) 	 50803264 	 48051 	 20.38853645324707 	 82.13901233673096 	 0.4190068244934082 	 1.7470333576202393 	 42.679805755615234 	 67.78671646118164 	 0.4537949562072754 	 0.7207815647125244 	 
2025-08-06 03:07:15.804208 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([264601, 3, 32],"float64"), 8, False, None, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([264601, 3, 32],"float64"), 8, False, None, ) 	 25401696 	 48051 	 10.01436471939087 	 164.10998964309692 	 0.21297430992126465 	 3.4891865253448486 	 25.08259916305542 	 159.89263153076172 	 0.2667045593261719 	 1.7002785205841064 	 
2025-08-06 03:13:16.899403 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([529201, 3, 32],"float32"), 16, False, None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f56a4333df0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:23:24.185037 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([529201, 3, 32],"float32"), output_size=16, )
W0806 03:23:25.204480 121996 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f72fcd4ac20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:33:31.764404 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 1209601, 7],"float32"), output_size=5, return_mask=False, name=None, )
W0806 03:33:32.852070 122557 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd6837a70d0>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754423011 (unix time) try "date -d @1754423011" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1de6b) received by PID 122475 (TID 0x7fd67ed6d640) from PID 122475 ***]

2025-08-06 03:43:46.417514 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 1209601],"float32"), output_size=5, return_mask=False, name=None, )
W0806 03:43:47.386824 122956 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa5564730d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:53:54.224081 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 1209601],"float32"), output_size=list[2,5,], return_mask=False, name=None, )
W0806 03:53:55.249825 123352 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f155a84f0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 04:03:59.370606 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 1209601],"float32"), output_size=list[3,3,], return_mask=False, name=None, )
W0806 04:04:00.366070 123755 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb6e5277130>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754424839 (unix time) try "date -d @1754424839" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e318) received by PID 123672 (TID 0x7fb6dc3f9640) from PID 123672 ***]

2025-08-06 04:14:06.477113 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )
W0806 04:14:07.430399 124109 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.adaptive_max_pool2d 	 paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803298 	 25828 	 15.038644552230835 	 120.93404722213745 	 0.5950655937194824 	 4.783292531967163 	 28.14298915863037 	 30.5160391330719 	 0.5572910308837891 	 0.6036636829376221 	 
2025-08-06 04:17:25.286948 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=list[2,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool2d 	 paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=list[2,5,], return_mask=False, name=None, ) 	 50803298 	 25828 	 11.298515319824219 	 68.08073568344116 	 0.4469869136810303 	 2.69657826423645 	 26.142849683761597 	 25.003713846206665 	 0.5171771049499512 	 0.4946606159210205 	 
2025-08-06 04:19:38.431788 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=list[3,3,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool2d 	 paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=list[3,3,], return_mask=False, name=None, ) 	 50803298 	 25828 	 10.961472272872925 	 84.81056332588196 	 0.3964698314666748 	 3.3556408882141113 	 26.70301842689514 	 25.866877555847168 	 0.52846360206604 	 0.5117025375366211 	 
2025-08-06 04:22:09.955927 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803690 	 6398 	 12.566343069076538 	 65.87679839134216 	 2.0071029663085938 	 0.6577963829040527 	 8.063742637634277 	 9.552396535873413 	 0.644031286239624 	 0.08972024917602539 	 
2025-08-06 04:23:47.316124 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, ) 	 50803690 	 6398 	 10.009832620620728 	 35.47602581977844 	 1.5988850593566895 	 0.8092434406280518 	 3.0473403930664062 	 3.814521312713623 	 0.24408364295959473 	 0.07607865333557129 	 
2025-08-06 04:24:43.832489 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, ) 	 50803690 	 6398 	 12.257334232330322 	 43.96267890930176 	 1.9577646255493164 	 0.7021350860595703 	 3.698970079421997 	 5.202345132827759 	 0.29547595977783203 	 0.07546043395996094 	 
2025-08-06 04:25:49.931505 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 172801, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 172801, 7, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803494 	 6398 	 195.0181872844696 	 255.870947599411 	 31.18653106689453 	 40.801676988601685 	 0.9703786373138428 	 0.8789212703704834 	 0.07745146751403809 	 0.0701303482055664 	 
2025-08-06 04:33:23.586422 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 172801, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f67c1c0a890>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 04:44:26.649092 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 172801, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
W0806 04:44:29.322278 124798 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa73f7a3130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 04:56:03.803519 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 241921, 7],"float32"), output_size=5, return_mask=False, name=None, )
W0806 04:56:04.754495 125027 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 241921, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803410 	 6398 	 153.52812838554382 	 192.74792528152466 	 24.5324649810791 	 30.80325436592102 	 0.9629764556884766 	 0.8806061744689941 	 0.07688283920288086 	 0.07029366493225098 	 
2025-08-06 05:01:56.247945 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 241921, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd5f8870220>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 05:13:19.833523 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 241921, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
W0806 05:13:20.786022 125494 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5e5a812ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 05:25:05.414068 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 241921],"float32"), output_size=5, return_mask=False, name=None, )
W0806 05:25:06.393076 125803 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 241921],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803410 	 6398 	 64.87159204483032 	 75.83649754524231 	 10.364179134368896 	 12.113543033599854 	 0.9645318984985352 	 0.8812932968139648 	 0.07709908485412598 	 0.0703122615814209 	 
2025-08-06 05:27:29.425162 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 241921],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 241921],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, ) 	 50803410 	 6398 	 199.08727025985718 	 229.5464653968811 	 31.806945085525513 	 36.660179138183594 	 0.9586782455444336 	 0.8805842399597168 	 0.07645630836486816 	 0.0702657699584961 	 
2025-08-06 05:34:41.238082 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 241921],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd42cf66ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 05:45:45.638810 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )
W0806 05:45:46.659139 126332 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803935 	 6398 	 12.575060844421387 	 65.88221764564514 	 2.0086560249328613 	 0.6577255725860596 	 8.05029845237732 	 9.548051118850708 	 0.6429543495178223 	 0.08964729309082031 	 
2025-08-06 05:47:23.649438 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, ) 	 50803935 	 6398 	 9.998377084732056 	 35.47863054275513 	 1.597060203552246 	 0.8094120025634766 	 3.051008939743042 	 3.8167033195495605 	 0.24364590644836426 	 0.07614278793334961 	 
2025-08-06 05:48:17.740328 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, ) 	 50803935 	 6398 	 12.258765697479248 	 43.95191669464111 	 1.9581267833709717 	 0.7017908096313477 	 3.7133162021636963 	 5.2053821086883545 	 0.29657435417175293 	 0.0755164623260498 	 
2025-08-06 05:49:23.832480 test begin: paddle.nn.functional.avg_pool1d(Tensor([13, 1, 3907939],"float32"), 25, 1, 0, True, False, None, )
W0806 05:49:24.673329 126387 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.avg_pool1d 	 paddle.nn.functional.avg_pool1d(Tensor([13, 1, 3907939],"float32"), 25, 1, 0, True, False, None, ) 	 50803207 	 1335 	 49.064494371414185 	 2.1130528450012207 	 37.56028199195862 	 1.6174640655517578 	 61.15927982330322 	 4.983475208282471 	 46.819724559783936 	 3.8149795532226562 	 
2025-08-06 05:51:23.092621 test begin: paddle.nn.functional.avg_pool1d(Tensor([13, 32567, 120],"float32"), 25, 1, 0, True, False, None, )
[Prof] paddle.nn.functional.avg_pool1d 	 paddle.nn.functional.avg_pool1d(Tensor([13, 32567, 120],"float32"), 25, 1, 0, True, False, None, ) 	 50804520 	 1335 	 10.009539604187012 	 1.8454606533050537 	 7.662758111953735 	 1.412571907043457 	 15.236255884170532 	 5.142770290374756 	 11.664026498794556 	 3.9368834495544434 	 
2025-08-06 05:51:56.897246 test begin: paddle.nn.functional.avg_pool1d(Tensor([16, 1, 3175201],"float32"), 25, 1, 0, True, False, None, )
[Prof] paddle.nn.functional.avg_pool1d 	 paddle.nn.functional.avg_pool1d(Tensor([16, 1, 3175201],"float32"), 25, 1, 0, True, False, None, ) 	 50803216 	 1335 	 49.05560255050659 	 4.164903879165649 	 37.554142475128174 	 1.617391586303711 	 61.15144419670105 	 4.983857154846191 	 46.81403112411499 	 3.8152658939361572 	 
2025-08-06 05:53:59.072466 test begin: paddle.nn.functional.avg_pool1d(Tensor([16, 2, 1587601],"float32"), 25, 1, 0, True, False, None, )
[Prof] paddle.nn.functional.avg_pool1d 	 paddle.nn.functional.avg_pool1d(Tensor([16, 2, 1587601],"float32"), 25, 1, 0, True, False, None, ) 	 50803232 	 1335 	 24.65001940727234 	 2.1225805282592773 	 18.86531949043274 	 1.6157658100128174 	 30.60892415046692 	 4.984166622161865 	 23.43214178085327 	 3.815509557723999 	 
2025-08-06 05:55:03.411906 test begin: paddle.nn.functional.avg_pool1d(Tensor([16, 26461, 120],"float32"), 25, 1, 0, True, False, None, )
[Prof] paddle.nn.functional.avg_pool1d 	 paddle.nn.functional.avg_pool1d(Tensor([16, 26461, 120],"float32"), 25, 1, 0, True, False, None, ) 	 50805120 	 1335 	 10.008319854736328 	 1.848938226699829 	 7.661640167236328 	 1.412553071975708 	 15.235292911529541 	 5.420307397842407 	 11.663145065307617 	 4.2145302295684814 	 
2025-08-06 05:55:38.568819 test begin: paddle.nn.functional.avg_pool2d(Tensor([128, 127, 56, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([128, 127, 56, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 50978816 	 52538 	 10.91502070426941 	 21.874465227127075 	 0.19854521751403809 	 0.4254496097564697 	 18.544913291931152 	 82.43591594696045 	 0.360745906829834 	 1.6035003662109375 	 
2025-08-06 05:57:54.965727 test begin: paddle.nn.functional.avg_pool2d(Tensor([128, 256, 28, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([128, 256, 28, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51380224 	 52538 	 10.450887203216553 	 22.10233974456787 	 0.20328497886657715 	 0.4296915531158447 	 17.498842000961304 	 82.84837365150452 	 0.3403911590576172 	 1.6115424633026123 	 
2025-08-06 06:00:10.674022 test begin: paddle.nn.functional.avg_pool2d(Tensor([128, 256, 56, 28],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([128, 256, 56, 28],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51380224 	 52538 	 10.501269102096558 	 23.02503776550293 	 0.2042980194091797 	 0.42902040481567383 	 17.567782163619995 	 82.88037991523743 	 0.34169483184814453 	 1.6120426654815674 	 
2025-08-06 06:02:26.669167 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 128, 256, 97],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([16, 128, 256, 97],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 50855936 	 52538 	 10.099058866500854 	 21.542704582214355 	 0.19646143913269043 	 0.41905760765075684 	 17.350730657577515 	 82.15556454658508 	 0.3374214172363281 	 1.5982632637023926 	 
2025-08-06 06:04:44.210454 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 128, 97, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([16, 128, 97, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 50855936 	 52538 	 10.001787424087524 	 21.348615407943726 	 0.19450044631958008 	 0.41535329818725586 	 17.299479722976685 	 81.50600934028625 	 0.3363931179046631 	 1.585482120513916 	 
2025-08-06 06:06:55.448872 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 49, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([16, 49, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51380224 	 52538 	 10.321631908416748 	 21.77470111846924 	 0.200758695602417 	 0.42353010177612305 	 18.548873901367188 	 82.37983512878418 	 0.3607759475708008 	 1.6027061939239502 	 
2025-08-06 06:09:09.996743 test begin: paddle.nn.functional.avg_pool2d(Tensor([4, 256, 240, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([4, 256, 240, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 58982400 	 52538 	 11.716840982437134 	 25.254642248153687 	 0.22793030738830566 	 0.49076151847839355 	 20.119447708129883 	 95.10411214828491 	 0.3912031650543213 	 1.84995698928833 	 
2025-08-06 06:11:43.455281 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 256, 56, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([64, 256, 56, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51380224 	 52538 	 10.280102968215942 	 22.093327045440674 	 0.19998526573181152 	 0.4297456741333008 	 18.581522226333618 	 83.06132936477661 	 0.3614237308502197 	 1.6155946254730225 	 
2025-08-06 06:13:59.075735 test begin: paddle.nn.functional.avg_pool2d(Tensor([7, 128, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([7, 128, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 58720256 	 52538 	 11.622715473175049 	 25.5739688873291 	 0.22605466842651367 	 0.48293161392211914 	 19.983416318893433 	 94.01155591011047 	 0.388704776763916 	 1.8287098407745361 	 
2025-08-06 06:16:33.144388 test begin: paddle.nn.functional.avg_pool2d(Tensor([8, 111, 240, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([8, 111, 240, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51148800 	 52538 	 10.223015069961548 	 21.922425508499146 	 0.198838472366333 	 0.4262197017669678 	 17.582706451416016 	 82.5890622138977 	 0.34200382232666016 	 1.607327938079834 	 
2025-08-06 06:18:46.571374 test begin: paddle.nn.functional.avg_pool2d(Tensor([8, 256, 104, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([8, 256, 104, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51118080 	 52538 	 10.209075450897217 	 21.929728984832764 	 0.1986086368560791 	 0.425976037979126 	 17.56526827812195 	 82.49906253814697 	 0.341778039932251 	 1.605825424194336 	 
2025-08-06 06:21:01.743350 test begin: paddle.nn.functional.avg_pool2d(Tensor([8, 256, 240, 104],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([8, 256, 240, 104],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51118080 	 52538 	 10.159784078598022 	 22.59002685546875 	 0.19762659072875977 	 0.4253346920013428 	 17.558063983917236 	 82.78557562828064 	 0.3406698703765869 	 1.610539436340332 	 
2025-08-06 06:23:18.769748 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 776, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(Tensor([2, 776, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", ) 	 50855936 	 56552 	 9.97968602180481 	 23.941197872161865 	 0.18032550811767578 	 0.4326484203338623 	 24.07182812690735 	 28.213703632354736 	 0.43520188331604004 	 0.2549300193786621 	 
2025-08-06 06:24:45.941668 test begin: paddle.nn.functional.avg_pool3d(Tensor([517, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(Tensor([517, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", ) 	 50823168 	 56552 	 10.064972162246704 	 23.918241024017334 	 0.18187260627746582 	 0.4321932792663574 	 31.572441339492798 	 28.1872239112854 	 0.5717604160308838 	 0.25464868545532227 	 
2025-08-06 06:26:24.597557 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([127, 2048, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([127, 2048, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 	 50978816 	 56552 	 13.576303482055664 	 128.2822241783142 	 0.2453470230102539 	 0.5753076076507568 	 177.67061638832092 	 163.68351030349731 	 3.340571880340576 	 0.1742246150970459 	 
2025-08-06 06:34:31.120707 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([127, 256, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([127, 256, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 	 50978816 	 56552 	 14.910910844802856 	 112.83677840232849 	 0.26937413215637207 	 2.037654161453247 	 163.22294926643372 	 162.78694796562195 	 2.9497909545898438 	 0.17302179336547852 	 
2025-08-06 06:42:05.733124 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 111, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1d96650280>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 06:52:22.816249 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 7, 111],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
W0806 06:52:23.837467 128493 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 06:52:23.839797 128493 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 7, 111],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 	 50921472 	 56552 	 21.766201496124268 	 43.60944151878357 	 0.3932633399963379 	 0.7879855632781982 	 174.16740489006042 	 55.51329183578491 	 3.1476454734802246 	 0.3345835208892822 	 
2025-08-06 06:57:24.482400 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 64, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc81854ba90>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 07:07:29.620242 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 111, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
W0806 07:07:30.705448 128976 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 07:07:30.707289 128976 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f64d9002f50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 07:17:50.527611 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 7, 111],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
W0806 07:17:51.558280 129198 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 07:17:51.561324 129198 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 7, 111],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 	 50921472 	 56552 	 23.457087993621826 	 56.352603912353516 	 0.42397260665893555 	 1.6068699359893799 	 173.36326122283936 	 54.35043931007385 	 3.1450936794281006 	 0.3276181221008301 	 
2025-08-06 07:23:02.242265 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 507, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1181f1bac0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 07:33:21.820755 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 32401, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
W0806 07:33:22.768188 129757 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 07:33:22.770109 129757 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 32401, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 	 50804768 	 56552 	 13.538180589675903 	 126.82077407836914 	 0.2446727752685547 	 0.5722372531890869 	 176.94343948364258 	 163.26930451393127 	 3.1978471279144287 	 0.1735219955444336 	 
2025-08-06 07:41:35.224411 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 4051, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 4051, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 	 50815744 	 56552 	 15.027578353881836 	 112.39039707183838 	 0.2715604305267334 	 2.0303633213043213 	 163.0070605278015 	 162.22675585746765 	 2.945836305618286 	 0.1724412441253662 	 
2025-08-06 07:49:12.077148 test begin: paddle.nn.functional.batch_norm(Tensor([30, 40, 50, 847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), use_global_stats=True, data_format="NHWC", )
[Prof] paddle.nn.functional.batch_norm 	 paddle.nn.functional.batch_norm(Tensor([30, 40, 50, 847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), use_global_stats=True, data_format="NHWC", ) 	 50823388 	 29508 	 11.023340702056885 	 11.056442975997925 	 0.3818347454071045 	 0.3828623294830322 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([30, 40, 50, 847]) and output[0] has a shape of torch.Size([30, 847, 40, 50]).
2025-08-06 07:51:42.318977 test begin: paddle.nn.functional.batch_norm(Tensor([30, 40, 706, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", )
[Prof] paddle.nn.functional.batch_norm 	 paddle.nn.functional.batch_norm(Tensor([30, 40, 706, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", ) 	 50832240 	 29508 	 10.947500228881836 	 10.979706048965454 	 0.37926435470581055 	 0.3798646926879883 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([30, 40, 706, 60]) and output[0] has a shape of torch.Size([30, 60, 40, 706]).
2025-08-06 07:53:49.892253 test begin: paddle.nn.functional.batch_norm(Tensor([30, 565, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", )
[Prof] paddle.nn.functional.batch_norm 	 paddle.nn.functional.batch_norm(Tensor([30, 565, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", ) 	 50850240 	 29508 	 11.128020524978638 	 11.12043023109436 	 0.38541316986083984 	 0.38517093658447266 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([30, 565, 50, 60]) and output[0] has a shape of torch.Size([30, 60, 565, 50]).
2025-08-06 07:56:02.460823 test begin: paddle.nn.functional.batch_norm(Tensor([424, 40, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", )
[Prof] paddle.nn.functional.batch_norm 	 paddle.nn.functional.batch_norm(Tensor([424, 40, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", ) 	 50880240 	 29508 	 10.012574434280396 	 10.24494457244873 	 0.3467423915863037 	 0.35480237007141113 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([424, 40, 50, 60]) and output[0] has a shape of torch.Size([424, 60, 40, 50]).
2025-08-06 07:58:01.628593 test begin: paddle.nn.functional.bilinear(Tensor([25401601, 1],"float32"), Tensor([25401601, 2],"float32"), Tensor([4, 1, 2],"float32"), Tensor([1, 4],"float32"), None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f22eb6d7340>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:08:11.651296 test begin: paddle.nn.functional.bilinear(Tensor([3, 1],"float32"), Tensor([3, 12700801],"float32"), Tensor([4, 1, 12700801],"float32"), Tensor([1, 4],"float32"), None, )
W0806 08:08:13.112593 131268 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.bilinear 	 paddle.nn.functional.bilinear(Tensor([3, 1],"float32"), Tensor([3, 12700801],"float32"), Tensor([4, 1, 12700801],"float32"), Tensor([1, 4],"float32"), None, ) 	 88905614 	 2209 	 7.496368646621704 	 101.81782078742981 	 0.053414106369018555 	 0.760406494140625 	 14.669278144836426 	 110.99545669555664 	 0.09034585952758789 	 0.5565457344055176 	 
2025-08-06 08:12:15.516708 test begin: paddle.nn.functional.bilinear(Tensor([3, 1],"float32"), Tensor([3, 16934401],"float32"), Tensor([4, 1, 16934401],"float32"), Tensor([1, 4],"float32"), None, )
[Prof] paddle.nn.functional.bilinear 	 paddle.nn.functional.bilinear(Tensor([3, 1],"float32"), Tensor([3, 16934401],"float32"), Tensor([4, 1, 16934401],"float32"), Tensor([1, 4],"float32"), None, ) 	 118540814 	 2209 	 10.065882682800293 	 132.21739506721497 	 0.057473182678222656 	 0.7931396961212158 	 19.395875930786133 	 148.22145080566406 	 0.09789919853210449 	 0.6096999645233154 	 
2025-08-06 08:17:27.601152 test begin: paddle.nn.functional.bilinear(Tensor([5, 5],"float32"), Tensor([5, 10161],"float32"), Tensor([1000, 5, 10161],"float32"), Tensor([1, 1000],"float32"), None, )
[Prof] paddle.nn.functional.bilinear 	 paddle.nn.functional.bilinear(Tensor([5, 5],"float32"), Tensor([5, 10161],"float32"), Tensor([1000, 5, 10161],"float32"), Tensor([1, 1000],"float32"), None, ) 	 50856830 	 2209 	 69.03255105018616 	 94.01525068283081 	 0.007961034774780273 	 0.0002460479736328125 	 86.34904432296753 	 227.79847526550293 	 0.006605863571166992 	 0.00021386146545410156 	 
2025-08-06 08:25:27.353626 test begin: paddle.nn.functional.bilinear(Tensor([50803201, 1],"float32"), Tensor([50803201, 2],"float32"), Tensor([4, 1, 2],"float32"), Tensor([1, 4],"float32"), None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1c7a2fea10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:35:32.030894 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([16, 10164, 313],"float32"), Tensor([16, 10164, 313],"float32"), weight=Tensor([16, 10164, 313],"float32"), reduction="sum", )
W0806 08:35:34.443434 132083 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([16, 10164, 313],"float32"), Tensor([16, 10164, 313],"float32"), weight=Tensor([16, 10164, 313],"float32"), reduction="sum", ) 	 152703936 	 9554 	 10.029348850250244 	 10.079756259918213 	 0.2673606872558594 	 0.21535396575927734 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 08:36:12.970172 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([16, 11109, 286],"float32"), Tensor([16, 11109, 286],"float32"), weight=Tensor([16, 11109, 286],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([16, 11109, 286],"float32"), Tensor([16, 11109, 286],"float32"), weight=Tensor([16, 11109, 286],"float32"), reduction="sum", ) 	 152504352 	 9554 	 10.00066876411438 	 10.073472499847412 	 0.2670750617980957 	 0.21529531478881836 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 08:36:54.473568 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([16, 12096, 263],"float32"), Tensor([16, 12096, 263],"float32"), weight=Tensor([16, 12096, 263],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([16, 12096, 263],"float32"), Tensor([16, 12096, 263],"float32"), weight=Tensor([16, 12096, 263],"float32"), reduction="sum", ) 	 152699904 	 9554 	 10.020906686782837 	 10.08203673362732 	 0.26753950119018555 	 0.21540307998657227 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 08:37:37.246546 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([16, 39691, 80],"float32"), Tensor([16, 39691, 80],"float32"), weight=Tensor([16, 39691, 80],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([16, 39691, 80],"float32"), Tensor([16, 39691, 80],"float32"), weight=Tensor([16, 39691, 80],"float32"), reduction="sum", ) 	 152413440 	 9554 	 10.877587795257568 	 10.06284236907959 	 0.26709556579589844 	 0.2149803638458252 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 08:38:21.493954 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([53, 12096, 80],"float32"), Tensor([53, 12096, 80],"float32"), weight=Tensor([53, 12096, 80],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([53, 12096, 80],"float32"), Tensor([53, 12096, 80],"float32"), weight=Tensor([53, 12096, 80],"float32"), reduction="sum", ) 	 153861120 	 9554 	 10.095168352127075 	 10.158366441726685 	 0.26964378356933594 	 0.21696877479553223 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 08:39:02.619369 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([58, 11109, 80],"float32"), Tensor([58, 11109, 80],"float32"), weight=Tensor([58, 11109, 80],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([58, 11109, 80],"float32"), Tensor([58, 11109, 80],"float32"), weight=Tensor([58, 11109, 80],"float32"), reduction="sum", ) 	 154637280 	 9554 	 10.139691829681396 	 10.206093549728394 	 0.27075767517089844 	 0.2180800437927246 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 08:39:44.762060 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([63, 10164, 80],"float32"), Tensor([63, 10164, 80],"float32"), weight=Tensor([63, 10164, 80],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([63, 10164, 80],"float32"), Tensor([63, 10164, 80],"float32"), weight=Tensor([63, 10164, 80],"float32"), reduction="sum", ) 	 153679680 	 9554 	 10.089223861694336 	 10.15330171585083 	 0.26944565773010254 	 0.21681737899780273 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 08:40:25.881774 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([16, 300, 10585],"float32"), Tensor([16, 300, 10585],"float32"), weight=Tensor([16, 300, 10585],"float32"), reduction="none", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([16, 300, 10585],"float32"), Tensor([16, 300, 10585],"float32"), weight=Tensor([16, 300, 10585],"float32"), reduction="none", ) 	 152424000 	 9646 	 10.018109321594238 	 21.3638334274292 	 0.3541851043701172 	 0.37715649604797363 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 08:41:19.160106 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([16, 39691, 80],"float32"), Tensor([16, 39691, 80],"float32"), weight=Tensor([16, 39691, 80],"float32"), reduction="none", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([16, 39691, 80],"float32"), Tensor([16, 39691, 80],"float32"), weight=Tensor([16, 39691, 80],"float32"), reduction="none", ) 	 152413440 	 9646 	 10.014048337936401 	 21.34772300720215 	 0.35401034355163574 	 0.3771533966064453 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 08:42:12.370826 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([2117, 300, 80],"float32"), Tensor([2117, 300, 80],"float32"), weight=Tensor([2117, 300, 80],"float32"), reduction="none", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([2117, 300, 80],"float32"), Tensor([2117, 300, 80],"float32"), weight=Tensor([2117, 300, 80],"float32"), reduction="none", ) 	 152424000 	 9646 	 10.020165920257568 	 21.360559940338135 	 0.35423731803894043 	 0.3771538734436035 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 08:43:05.663808 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([300, 169345],"float32"), Tensor([300, 169345],"float32"), weight=Tensor([300, 169345],"float32"), reduction="none", pos_weight=None, )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([300, 169345],"float32"), Tensor([300, 169345],"float32"), weight=Tensor([300, 169345],"float32"), reduction="none", pos_weight=None, ) 	 152410500 	 9646 	 10.01656699180603 	 21.352757930755615 	 0.35408759117126465 	 0.37719273567199707 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 08:44:01.112327 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([50804, 1000],"float32"), Tensor([50804, 1000],"float32"), weight=Tensor([50804, 1000],"float32"), reduction="none", pos_weight=None, )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([50804, 1000],"float32"), Tensor([50804, 1000],"float32"), weight=Tensor([50804, 1000],"float32"), reduction="none", pos_weight=None, ) 	 152412000 	 9646 	 10.014394283294678 	 21.35000729560852 	 0.35397768020629883 	 0.3771054744720459 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 08:44:54.904586 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([512, 28, 3544],"float32"), Tensor([512, 28, 3544],"float32"), weight=Tensor([512, 1, 3544],"float32"), reduction="mean", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([512, 28, 3544],"float32"), Tensor([512, 28, 3544],"float32"), weight=Tensor([512, 1, 3544],"float32"), reduction="mean", ) 	 103428096 	 9646 	 10.056708335876465 	 21.616753578186035 	 0.21276307106018066 	 0.28641390800476074 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 08:45:44.297109 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([512, 3544, 28],"float32"), Tensor([512, 3544, 28],"float32"), weight=Tensor([512, 3544, 1],"float32"), reduction="mean", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([512, 3544, 28],"float32"), Tensor([512, 3544, 28],"float32"), weight=Tensor([512, 3544, 1],"float32"), reduction="mean", ) 	 103428096 	 9646 	 10.051687955856323 	 21.570837259292603 	 0.21274423599243164 	 0.28591132164001465 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 08:46:34.240014 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), weight=Tensor([64801, 1, 1],"float32"), reduction="mean", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), weight=Tensor([64801, 1, 1],"float32"), reduction="mean", ) 	 101672769 	 9646 	 10.010714054107666 	 21.509520769119263 	 0.21162652969360352 	 0.2850313186645508 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 08:47:26.173101 test begin: paddle.nn.functional.celu(Tensor([1587601, 4, 4],"float64"), 0.2, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([1587601, 4, 4],"float64"), 0.2, None, ) 	 25401616 	 32447 	 9.959505081176758 	 9.854552984237671 	 0.31366729736328125 	 0.3104219436645508 	 14.50694465637207 	 14.576033115386963 	 0.4569087028503418 	 0.459104061126709 	 
2025-08-06 08:48:18.918871 test begin: paddle.nn.functional.celu(Tensor([1587601, 4, 4],"float64"), 1.0, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([1587601, 4, 4],"float64"), 1.0, None, ) 	 25401616 	 32447 	 9.962857723236084 	 9.856331586837769 	 0.31372809410095215 	 0.31032276153564453 	 14.510319709777832 	 14.576515674591064 	 0.45703625679016113 	 0.4591538906097412 	 
2025-08-06 08:49:08.950215 test begin: paddle.nn.functional.celu(Tensor([2, 3175201, 4],"float64"), 0.2, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([2, 3175201, 4],"float64"), 0.2, None, ) 	 25401608 	 32447 	 9.96055293083191 	 9.86489200592041 	 0.31371426582336426 	 0.31032681465148926 	 14.5104660987854 	 14.575985670089722 	 0.4570047855377197 	 0.4590799808502197 	 
2025-08-06 08:49:59.014468 test begin: paddle.nn.functional.celu(Tensor([2, 3175201, 4],"float64"), 1.0, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([2, 3175201, 4],"float64"), 1.0, None, ) 	 25401608 	 32447 	 9.959690809249878 	 9.853081226348877 	 0.31370043754577637 	 0.3103489875793457 	 14.510326147079468 	 14.576253175735474 	 0.4570319652557373 	 0.4591031074523926 	 
2025-08-06 08:50:49.284437 test begin: paddle.nn.functional.celu(Tensor([2, 4, 3175201],"float64"), 0.2, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([2, 4, 3175201],"float64"), 0.2, None, ) 	 25401608 	 32447 	 9.962486028671265 	 9.855283737182617 	 0.31369614601135254 	 0.3102571964263916 	 14.510443210601807 	 14.576627492904663 	 0.45703959465026855 	 0.45912933349609375 	 
2025-08-06 08:51:42.938354 test begin: paddle.nn.functional.celu(Tensor([2, 4, 3175201],"float64"), 1.0, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([2, 4, 3175201],"float64"), 1.0, None, ) 	 25401608 	 32447 	 9.966445207595825 	 9.85447883605957 	 0.31371617317199707 	 0.31167078018188477 	 14.510193109512329 	 14.576712131500244 	 0.45704054832458496 	 0.4591331481933594 	 
2025-08-06 08:52:32.966751 test begin: paddle.nn.functional.celu(x=Tensor([1587601, 4, 4],"float64"), )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(x=Tensor([1587601, 4, 4],"float64"), ) 	 25401616 	 32447 	 9.959932804107666 	 9.858741998672485 	 0.3137190341949463 	 0.3103458881378174 	 14.51033878326416 	 14.576725244522095 	 0.45702362060546875 	 0.4591081142425537 	 
2025-08-06 08:53:23.006824 test begin: paddle.nn.functional.celu(x=Tensor([2, 3175201, 4],"float64"), )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(x=Tensor([2, 3175201, 4],"float64"), ) 	 25401608 	 32447 	 9.95935606956482 	 9.860759019851685 	 0.3137063980102539 	 0.3103487491607666 	 14.510310411453247 	 14.576727628707886 	 0.45702409744262695 	 0.45911669731140137 	 
2025-08-06 08:54:13.048570 test begin: paddle.nn.functional.celu(x=Tensor([2, 4, 3175201],"float64"), )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(x=Tensor([2, 4, 3175201],"float64"), ) 	 25401608 	 32447 	 11.16676115989685 	 9.878382921218872 	 0.3137381076812744 	 0.31043434143066406 	 14.510326623916626 	 14.576430559158325 	 0.4570910930633545 	 0.4591691493988037 	 
2025-08-06 08:55:07.511313 test begin: paddle.nn.functional.channel_shuffle(Tensor([176401, 4, 4, 9],"float64"), 3, "NHWC", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([176401, 4, 4, 9],"float64"), 3, "NHWC", ) 	 25401744 	 31913 	 10.061893224716187 	 9.487884998321533 	 0.3221595287322998 	 0.3036668300628662 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([176401, 4, 4, 9]) and output[0] has a shape of torch.Size([176401, 9, 4, 4]).
2025-08-06 08:55:39.704281 test begin: paddle.nn.functional.channel_shuffle(Tensor([176401, 4, 4, 9],"float64"), 3, "NHWC", None, )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([176401, 4, 4, 9],"float64"), 3, "NHWC", None, ) 	 25401744 	 31913 	 12.11690640449524 	 9.482455253601074 	 0.3222498893737793 	 0.30364322662353516 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([176401, 4, 4, 9]) and output[0] has a shape of torch.Size([176401, 9, 4, 4]).
2025-08-06 08:56:13.068072 test begin: paddle.nn.functional.channel_shuffle(Tensor([176401, 9, 4, 4],"float64"), 3, "NCHW", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([176401, 9, 4, 4],"float64"), 3, "NCHW", ) 	 25401744 	 31913 	 10.008812189102173 	 11.515531063079834 	 0.3205418586730957 	 0.30909109115600586 	 10.008606672286987 	 9.6576669216156 	 0.32047343254089355 	 0.30925679206848145 	 
2025-08-06 08:56:56.630879 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 352801, 4, 9],"float64"), 3, "NHWC", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 352801, 4, 9],"float64"), 3, "NHWC", ) 	 25401672 	 31913 	 10.042483806610107 	 41.60435128211975 	 0.32158327102661133 	 1.332397699356079 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 352801, 4, 9]) and output[0] has a shape of torch.Size([2, 9, 352801, 4]).
2025-08-06 08:57:59.447052 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 352801, 4, 9],"float64"), 3, "NHWC", None, )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 352801, 4, 9],"float64"), 3, "NHWC", None, ) 	 25401672 	 31913 	 10.04257607460022 	 41.88322186470032 	 0.32154226303100586 	 1.6114068031311035 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 352801, 4, 9]) and output[0] has a shape of torch.Size([2, 9, 352801, 4]).
2025-08-06 08:59:02.568479 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 4, 352801, 9],"float64"), 3, "NHWC", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 4, 352801, 9],"float64"), 3, "NHWC", ) 	 25401672 	 31913 	 10.044519186019897 	 41.604045152664185 	 0.32154321670532227 	 1.3324358463287354 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 4, 352801, 9]) and output[0] has a shape of torch.Size([2, 9, 4, 352801]).
2025-08-06 09:00:05.415397 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 4, 352801, 9],"float64"), 3, "NHWC", None, )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 4, 352801, 9],"float64"), 3, "NHWC", None, ) 	 25401672 	 31913 	 10.042313575744629 	 41.99907159805298 	 0.3215601444244385 	 1.7272586822509766 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 4, 352801, 9]) and output[0] has a shape of torch.Size([2, 9, 4, 352801]).
2025-08-06 09:01:10.323821 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 9, 352801, 4],"float64"), 3, "NCHW", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 9, 352801, 4],"float64"), 3, "NCHW", ) 	 25401672 	 31913 	 10.017904281616211 	 9.647782802581787 	 0.32069826126098633 	 0.3089585304260254 	 10.036895036697388 	 9.646437168121338 	 0.32146787643432617 	 0.3089110851287842 	 
2025-08-06 09:01:52.084150 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 9, 4, 352801],"float64"), 3, "NCHW", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 9, 4, 352801],"float64"), 3, "NCHW", ) 	 25401672 	 31913 	 10.013279676437378 	 9.648786306381226 	 0.3208138942718506 	 0.31020355224609375 	 10.037160396575928 	 9.646218538284302 	 0.32142138481140137 	 0.30891919136047363 	 
2025-08-06 09:02:32.686108 test begin: paddle.nn.functional.conv1d(Tensor([16, 125, 25500],"float32"), Tensor([1, 125, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
W0806 09:02:36.047390 133269 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([16, 125, 25500],"float32"), Tensor([1, 125, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 51000126 	 30827 	 10.19117021560669 	 5.003488540649414 	 0.11011004447937012 	 0.08293867111206055 	 25.529507637023926 	 10.297636985778809 	 0.14133620262145996 	 0.06809115409851074 	 
2025-08-06 09:03:29.149087 test begin: paddle.nn.functional.conv1d(Tensor([16, 64, 49613],"float32"), Tensor([1, 64, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([16, 64, 49613],"float32"), Tensor([1, 64, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50803777 	 30827 	 10.392577171325684 	 5.180184841156006 	 0.11478209495544434 	 0.08584976196289062 	 22.66945481300354 	 11.086413383483887 	 0.12528204917907715 	 0.07335853576660156 	 
2025-08-06 09:04:20.212772 test begin: paddle.nn.functional.conv1d(Tensor([28, 32, 58081],"float32"), Tensor([1, 32, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([28, 32, 58081],"float32"), Tensor([1, 32, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 52040609 	 30827 	 12.166459560394287 	 5.680485963821411 	 0.13439249992370605 	 0.09413981437683105 	 23.791079998016357 	 12.239780187606812 	 0.13150501251220703 	 0.0809934139251709 	 
2025-08-06 09:05:17.583448 test begin: paddle.nn.functional.conv1d(Tensor([28, 32, 58081],"float32"), Tensor([32, 32, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([28, 32, 58081],"float32"), Tensor([32, 32, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 52041632 	 30827 	 20.882827758789062 	 30.786870002746582 	 0.17334723472595215 	 0.5103507041931152 	 41.80740547180176 	 395.1184661388397 	 0.17339396476745605 	 2.615549325942993 	 
2025-08-06 09:13:28.857319 test begin: paddle.nn.functional.conv1d(Tensor([32, 32, 49613],"float32"), Tensor([1, 32, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([32, 32, 49613],"float32"), Tensor([1, 32, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50803745 	 30827 	 11.84047245979309 	 5.491964817047119 	 0.13089823722839355 	 0.09103989601135254 	 22.782641649246216 	 11.39607286453247 	 0.12591814994812012 	 0.07539224624633789 	 
2025-08-06 09:14:21.520348 test begin: paddle.nn.functional.conv1d(Tensor([32, 32, 49613],"float32"), Tensor([32, 32, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([32, 32, 49613],"float32"), Tensor([32, 32, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50804768 	 30827 	 20.243563890457153 	 29.151672840118408 	 0.16803932189941406 	 0.4832000732421875 	 40.861071825027466 	 342.5489716529846 	 0.16944456100463867 	 2.2604126930236816 	 
2025-08-06 09:21:38.520296 test begin: paddle.nn.functional.conv1d(Tensor([32, 64, 25500],"float32"), Tensor([1, 64, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([32, 64, 25500],"float32"), Tensor([1, 64, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 52224065 	 30827 	 10.679370403289795 	 5.2699689865112305 	 0.11761045455932617 	 0.08733963966369629 	 19.658170223236084 	 10.647326231002808 	 0.08152556419372559 	 0.07041049003601074 	 
2025-08-06 09:22:28.250347 test begin: paddle.nn.functional.conv1d_transpose(Tensor([1, 24807, 7],"float32"), Tensor([24807, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([1, 24807, 7],"float32"), Tensor([24807, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50978641 	 14413 	 32.453574419021606 	 34.04093074798584 	 0.7677245140075684 	 0.8053483963012695 	 18.816860675811768 	 15.42621397972107 	 0.2641777992248535 	 0.2735016345977783 	 
2025-08-06 09:24:09.862460 test begin: paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 7],"float32"), Tensor([512, 12404, 8],"float32"), bias=Tensor([12404],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 7],"float32"), Tensor([512, 12404, 8],"float32"), bias=Tensor([12404],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50822772 	 14413 	 3.830927848815918 	 3.8670971393585205 	 0.09055423736572266 	 0.09147477149963379 	 302.7431125640869 	 138.3738534450531 	 4.286576747894287 	 2.448167562484741 	 
2025-08-06 09:31:43.067299 test begin: paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 7],"float32"), Tensor([512, 256, 388],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2b8b0f7f40>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754444504 (unix time) try "date -d @1754444504" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x203a2) received by PID 132002 (TID 0x7f2b8b079640) from PID 132002 ***]

2025-08-06 09:41:52.882372 test begin: paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 99226],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0806 09:41:54.008618 134771 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 09:41:54.033361 134771 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fec5b1aac50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:52:03.747298 test begin: paddle.nn.functional.conv1d_transpose(Tensor([14176, 512, 7],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0806 09:52:04.747002 135243 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 09:52:04.760594 135243 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcd989caf20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:02:29.257750 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 24807, 7],"float32"), Tensor([24807, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0806 10:02:30.281222 135629 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 10:02:30.335462 135629 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 24807, 7],"float32"), Tensor([24807, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 51152290 	 14413 	 31.87096333503723 	 33.614402055740356 	 0.7541608810424805 	 0.7953677177429199 	 18.630762100219727 	 23.06965136528015 	 0.26088881492614746 	 0.4092719554901123 	 
2025-08-06 10:04:18.418239 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 256, 28],"float32"), Tensor([256, 128, 1551],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc14c1aeb00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:14:30.486539 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 256, 28],"float32"), Tensor([256, 24807, 8],"float32"), bias=Tensor([24807],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0806 10:14:31.385288 136187 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 10:14:31.436041 136187 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f28f20caa70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:24:44.769758 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 256, 99226],"float32"), Tensor([256, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0806 10:24:45.718457 136795 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 10:24:45.723052 136795 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdb56162c50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:34:51.166367 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 49613, 28],"float32"), Tensor([49613, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0806 10:34:52.225569 137495 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 10:34:52.277695 137495 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 49613, 28],"float32"), Tensor([49613, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 53582168 	 14413 	 63.845149993896484 	 66.8462610244751 	 1.510803461074829 	 1.5818724632263184 	 26.59664511680603 	 27.173617124557495 	 0.3144853115081787 	 0.48184895515441895 	 
2025-08-06 10:37:57.462283 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 49613],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdbf7783010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:48:06.651765 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 7],"float32"), Tensor([512, 12404, 8],"float32"), bias=Tensor([12404],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0806 10:48:07.561749 138232 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 10:48:07.617604 138232 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 7],"float32"), Tensor([512, 12404, 8],"float32"), bias=Tensor([12404],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50826356 	 14413 	 6.85172963142395 	 12.741271734237671 	 0.1620182991027832 	 0.30142903327941895 	 303.05306792259216 	 138.4466314315796 	 4.291784048080444 	 2.4506919384002686 	 
2025-08-06 10:55:49.506927 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 7],"float32"), Tensor([512, 256, 388],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5a5f4f6b30>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754449553 (unix time) try "date -d @1754449553" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x21ba6) received by PID 138150 (TID 0x7f5a5aab7640) from PID 138150 ***]

2025-08-06 11:06:00.696978 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 907201, 28],"float32"), Tensor([907201, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0806 11:06:14.860303 139067 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 11:06:15.712018 139067 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6e0f432c50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 11:16:32.674434 test begin: paddle.nn.functional.conv1d_transpose(Tensor([7088, 256, 28],"float32"), Tensor([256, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
W0806 11:16:33.641033 139541 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 11:16:33.646438 139541 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f593b54aef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 11:26:48.041404 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 191, 260],"float32"), Tensor([1, 1, 191, 4],"float32"), )
W0806 11:26:49.095317 139836 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 11:26:49.099809 139836 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff80134ef50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 11:37:16.229707 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 191, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), )
W0806 11:37:17.239737 140154 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 11:37:17.244315 140154 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 191, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50851856 	 9096 	 9.980215072631836 	 10.047900438308716 	 1.1205430030822754 	 1.128915548324585 	 107.19305777549744 	 119.13618516921997 	 4.010684251785278 	 4.4647438526153564 	 
2025-08-06 11:41:25.149095 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 192, 259],"float32"), Tensor([1, 1, 192, 4],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7faeeed12950>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 11:51:53.355556 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 192, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), )
W0806 11:51:54.581493 140665 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 11:51:54.586205 140665 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 192, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50921488 	 9096 	 10.000659227371216 	 11.609580993652344 	 1.1235945224761963 	 1.130598545074463 	 107.5199203491211 	 119.37209224700928 	 4.032915830612183 	 4.473720550537109 	 
2025-08-06 11:56:06.782705 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 193, 258],"float32"), Tensor([1, 1, 193, 4],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa317ac2b30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 12:06:35.197150 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 193, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), )
W0806 12:06:41.647857 141266 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 12:06:41.662248 141266 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 193, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50989072 	 9096 	 10.013290882110596 	 10.091381311416626 	 1.1246135234832764 	 1.132784128189087 	 107.70287132263184 	 119.4683780670166 	 4.033120155334473 	 4.4766764640808105 	 
2025-08-06 12:10:51.767034 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 193],"float32"), Tensor([1, 1, 4, 193],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7daeb1eb30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 12:21:17.191789 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 193],"float32"), Tensor([1, 1, 4, 4],"float32"), )
W0806 12:21:18.226850 141711 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 12:21:18.231334 141711 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 193],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50989072 	 9096 	 10.072153329849243 	 10.150867938995361 	 1.1315667629241943 	 1.1392385959625244 	 108.76252222061157 	 119.91377115249634 	 4.081363916397095 	 4.492762327194214 	 
2025-08-06 12:25:30.502025 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 68161552 	 9096 	 14.159190893173218 	 13.528677463531494 	 1.5109896659851074 	 1.5184581279754639 	 145.7986285686493 	 160.69887256622314 	 5.48358416557312 	 6.022313356399536 	 
2025-08-06 12:31:12.543539 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 192],"float32"), Tensor([1, 1, 4, 192],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5a7516fe50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 12:41:43.205993 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 192],"float32"), Tensor([1, 1, 4, 4],"float32"), )
W0806 12:41:44.474375 142162 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 12:41:44.488824 142162 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 192],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50921488 	 9096 	 10.071956634521484 	 10.140602111816406 	 1.1307048797607422 	 1.139214277267456 	 108.54010891914368 	 119.64840316772461 	 4.067395210266113 	 4.4776904582977295 	 
2025-08-06 12:45:55.139710 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 68690960 	 9096 	 13.503236770629883 	 14.798678398132324 	 1.5169177055358887 	 1.52809739112854 	 146.26564025878906 	 161.96215963363647 	 5.484817743301392 	 6.17494797706604 	 
2025-08-06 12:51:38.160061 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 191],"float32"), Tensor([1, 1, 4, 191],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9630b26950>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 13:01:46.537371 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 191],"float32"), Tensor([1, 1, 4, 4],"float32"), )
W0806 13:01:47.492666 142571 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 13:01:47.502563 142571 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 191],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50851856 	 9096 	 10.027195692062378 	 10.102741241455078 	 1.1265175342559814 	 1.1352014541625977 	 108.41346001625061 	 119.51554465293884 	 4.0511016845703125 	 4.479555606842041 	 
2025-08-06 13:05:58.811077 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 69222416 	 9096 	 13.60501766204834 	 13.745374917984009 	 1.5278198719024658 	 1.5438950061798096 	 147.94616222381592 	 163.00580859184265 	 5.531836986541748 	 6.100424289703369 	 
2025-08-06 13:11:41.973253 test begin: paddle.nn.functional.conv2d(Tensor([752, 1, 260, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([752, 1, 260, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50835216 	 9096 	 10.006896734237671 	 10.081445932388306 	 1.1234474182128906 	 1.1318964958190918 	 114.97944664955139 	 115.9574134349823 	 4.308196067810059 	 4.337928295135498 	 
2025-08-06 13:15:54.686598 test begin: paddle.nn.functional.conv2d(Tensor([758, 1, 259, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([758, 1, 259, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50847414 	 9096 	 10.008979558944702 	 10.082607984542847 	 1.1243376731872559 	 1.1329655647277832 	 114.47493243217468 	 115.39379572868347 	 4.291177988052368 	 4.32474422454834 	 
2025-08-06 13:20:06.357044 test begin: paddle.nn.functional.conv2d(Tensor([764, 1, 258, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([764, 1, 258, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50854912 	 9096 	 10.794461488723755 	 10.100558996200562 	 1.1259706020355225 	 1.1330277919769287 	 113.67746090888977 	 114.90179920196533 	 4.259544372558594 	 4.305222034454346 	 
2025-08-06 13:24:20.351416 test begin: paddle.nn.functional.conv2d_transpose(Tensor([16, 32, 320, 320],"float32"), Tensor([32, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([16, 32, 320, 320],"float32"), Tensor([32, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 52428929 	 19463 	 12.182995319366455 	 12.243069648742676 	 0.21337342262268066 	 0.21486806869506836 	 13.044453382492065 	 12.079983234405518 	 0.09780287742614746 	 0.1054680347442627 	 
2025-08-06 13:25:11.119067 test begin: paddle.nn.functional.conv2d_transpose(Tensor([16, 64, 156, 320],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([16, 64, 156, 320],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 51118337 	 19463 	 10.082592487335205 	 10.154505491256714 	 0.17661499977111816 	 0.17789721488952637 	 10.92746901512146 	 10.448992252349854 	 0.08179736137390137 	 0.0912466049194336 	 
2025-08-06 13:25:54.677048 test begin: paddle.nn.functional.conv2d_transpose(Tensor([16, 64, 320, 156],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([16, 64, 320, 156],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 51118337 	 19463 	 10.090114116668701 	 10.163694381713867 	 0.17674803733825684 	 0.17816829681396484 	 10.73507046699524 	 10.45348858833313 	 0.08048129081726074 	 0.09135866165161133 	 
2025-08-06 13:26:38.689338 test begin: paddle.nn.functional.conv2d_transpose(Tensor([4, 56, 480, 480],"float32"), Tensor([56, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([4, 56, 480, 480],"float32"), Tensor([56, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 51609825 	 19463 	 10.424869060516357 	 10.452630043029785 	 0.1828618049621582 	 0.18301057815551758 	 11.938667058944702 	 11.831861972808838 	 0.08939623832702637 	 0.10334897041320801 	 
2025-08-06 13:27:24.285963 test begin: paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 414, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 414, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 50872577 	 19463 	 10.000198125839233 	 10.068714380264282 	 0.17514467239379883 	 0.17638182640075684 	 11.294124126434326 	 11.142540454864502 	 0.08464980125427246 	 0.0973045825958252 	 
2025-08-06 13:28:08.034558 test begin: paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 414],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 414],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 50872577 	 19463 	 10.001150846481323 	 10.069575548171997 	 0.17505788803100586 	 0.1763918399810791 	 11.297608375549316 	 11.151331424713135 	 0.08469200134277344 	 0.09737753868103027 	 
2025-08-06 13:28:51.448792 test begin: paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 58982657 	 19463 	 11.562215089797974 	 11.652634620666504 	 0.20253324508666992 	 0.20383024215698242 	 12.914259195327759 	 12.805284023284912 	 0.09675979614257812 	 0.11185669898986816 	 
2025-08-06 13:29:43.524142 test begin: paddle.nn.functional.conv2d_transpose(Tensor([8, 28, 480, 480],"float32"), Tensor([28, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([8, 28, 480, 480],"float32"), Tensor([28, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 51609713 	 19463 	 13.624287128448486 	 13.694137334823608 	 0.238631010055542 	 0.23983216285705566 	 13.534976482391357 	 12.877586126327515 	 0.10149312019348145 	 0.11241698265075684 	 
2025-08-06 13:30:40.047580 test begin: paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 207, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 207, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 50872577 	 19463 	 9.998653888702393 	 10.070814847946167 	 0.17511940002441406 	 0.1763763427734375 	 10.931322574615479 	 10.756704330444336 	 0.07830262184143066 	 0.09398078918457031 	 
2025-08-06 13:31:22.700889 test begin: paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 320, 320],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 320, 320],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 52429057 	 19463 	 10.312506437301636 	 10.368731498718262 	 0.18065667152404785 	 0.1816103458404541 	 11.022804021835327 	 10.938575983047485 	 0.08263230323791504 	 0.09551143646240234 	 
2025-08-06 13:32:06.266394 test begin: paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 480, 207],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 480, 207],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 50872577 	 19463 	 9.998270750045776 	 10.070955991744995 	 0.17512130737304688 	 0.1763927936553955 	 10.835756063461304 	 10.758181810379028 	 0.08126091957092285 	 0.09395265579223633 	 
2025-08-06 13:32:49.625870 test begin: paddle.nn.functional.conv3d(Tensor([16538, 6, 8, 8, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([16538, 6, 8, 8, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", ) 	 50805060 	 1559 	 9.935993194580078 	 2.3064393997192383 	 6.512892723083496 	 1.5117285251617432 	 68.60717630386353 	 28.73580265045166 	 22.492549657821655 	 9.55100703239441 	 
2025-08-06 13:34:41.675901 test begin: paddle.nn.functional.conv3d(Tensor([16538, 6, 8, 8, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([16538, 6, 8, 8, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50805392 	 1559 	 18.75938582420349 	 18.843014001846313 	 12.286343097686768 	 12.332742691040039 	 126.58435010910034 	 35.77061676979065 	 20.786166429519653 	 5.861750364303589 	 
2025-08-06 13:38:06.855963 test begin: paddle.nn.functional.conv3d(Tensor([33076, 3, 8, 8, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([33076, 3, 8, 8, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", ) 	 50805146 	 1559 	 29.601406574249268 	 41.07824492454529 	 9.70285701751709 	 13.463899374008179 	 375.5013909339905 	 73.6695818901062 	 35.17123198509216 	 6.897610902786255 	 
2025-08-06 13:46:51.716385 test begin: paddle.nn.functional.conv3d(Tensor([4, 3, 66151, 8, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc1347528c0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 13:57:47.676344 test begin: paddle.nn.functional.conv3d(Tensor([4, 3, 8, 66151, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
W0806 13:57:48.615759 143974 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 13:57:48.630877 143974 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc3b88d2d40>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   c10::TensorImpl::~TensorImpl()
1   c10::TensorImpl::~TensorImpl()
2   torch::autograd::deleteNode(torch::autograd::Node*)
3   torch::autograd::SavedVariable::reset_data()
4   c10::TensorImpl::~TensorImpl()
5   c10::TensorImpl::~TensorImpl()
6   c10::impl::PyObjectSlot::~PyObjectSlot()
7   c10::impl::PyObjectSlot::maybe_destroy_pyobj()

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754460467 (unix time) try "date -d @1754460467" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23210) received by PID 143888 (TID 0x7fc3af9f8640) from PID 143888 ***]

2025-08-06 14:07:56.786342 test begin: paddle.nn.functional.conv3d(Tensor([4, 3, 8, 8, 66151],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
W0806 14:07:57.843825 144369 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 14:07:57.848475 144369 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9632baed40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 14:18:45.089985 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 33076, 8, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
W0806 14:18:46.050191 149376 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 14:18:46.059438 149376 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 33076, 8, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", ) 	 50805060 	 1559 	 12.97422981262207 	 3.048743486404419 	 8.504783391952515 	 1.9977545738220215 	 92.10226845741272 	 35.25658941268921 	 30.190009117126465 	 12.001972436904907 	 
2025-08-06 14:21:12.312636 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 33076, 8, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 33076, 8, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50805392 	 1559 	 19.027447938919067 	 19.105979919433594 	 12.462781429290771 	 12.513273477554321 	 139.04806876182556 	 136.2489664554596 	 18.28516387939453 	 22.291890859603882 	 
2025-08-06 14:26:28.024984 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 8, 33076, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 8, 33076, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", ) 	 50805060 	 1559 	 13.34970998764038 	 3.0597493648529053 	 8.750277042388916 	 2.0059614181518555 	 91.17096495628357 	 19.119256734848022 	 29.884100914001465 	 6.320594310760498 	 
2025-08-06 14:28:38.669821 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 8, 33076, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 8, 33076, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50805392 	 1559 	 18.97740411758423 	 18.80745244026184 	 12.280123710632324 	 12.314209461212158 	 137.85373973846436 	 143.78879284858704 	 18.140547275543213 	 31.36710834503174 	 
2025-08-06 14:34:02.336754 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 33076],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 33076],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", ) 	 50805060 	 1559 	 13.263786554336548 	 3.1392085552215576 	 8.69461178779602 	 2.057488441467285 	 93.37246060371399 	 20.6747407913208 	 30.605059385299683 	 6.775173664093018 	 
2025-08-06 14:36:14.713048 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 33076],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 33076],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50805392 	 1559 	 18.641908884048462 	 18.715487241744995 	 12.20927095413208 	 12.256639003753662 	 147.20689988136292 	 152.77051281929016 	 19.348748445510864 	 33.27586030960083 	 
2025-08-06 14:41:54.406923 test begin: paddle.nn.functional.conv3d_transpose(Tensor([2, 2451, 2, 2, 2],"float32"), Tensor([2451, 12, 12, 12, 12],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d_transpose 	 paddle.nn.functional.conv3d_transpose(Tensor([2, 2451, 2, 2, 2],"float32"), Tensor([2451, 12, 12, 12, 12],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", ) 	 50863164 	 3204 	 10.001485586166382 	 2.0335569381713867 	 1.0638463497161865 	 0.21620512008666992 	 14.51833701133728 	 14.55783224105835 	 0.7719511985778809 	 1.2248423099517822 	 
2025-08-06 14:42:38.147653 test begin: paddle.nn.functional.conv3d_transpose(Tensor([24807, 4, 8, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d_transpose 	 paddle.nn.functional.conv3d_transpose(Tensor([24807, 4, 8, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50805066 	 3204 	 50.92563438415527 	 86.37160539627075 	 7.999776601791382 	 13.774721145629883 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([24807, 6, 8, 6, 8]) and output[0] has a shape of torch.Size([24807, 6, 10, 10, 10]).
2025-08-06 14:51:26.360177 test begin: paddle.nn.functional.conv3d_transpose(Tensor([24807, 4, 8, 8, 8],"float32"), Tensor([4, 4, 3, 3, 3],"float32"), Tensor([4],"float32"), output_size=tuple(10,17,10,), padding="valid", stride=tuple(1,2,1,), dilation=1, groups=1, data_format="NCDHW", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f96d131e9b0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 15:01:36.113144 test begin: paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 49613, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
W0806 15:01:41.398903 30940 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 15:01:42.809727 30940 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv3d_transpose 	 paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 49613, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50804042 	 3204 	 18.264276027679443 	 100.76936650276184 	 2.9075775146484375 	 16.080048084259033 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([4, 6, 49613, 6, 8]) and output[0] has a shape of torch.Size([4, 6, 49615, 10, 10]).
2025-08-06 15:11:08.027563 test begin: paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 49613, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d_transpose 	 paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 49613, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50804042 	 3204 	 24.294095993041992 	 91.92562413215637 	 3.787564992904663 	 14.992250919342041 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([4, 6, 8, 49611, 8]) and output[0] has a shape of torch.Size([4, 6, 10, 49615, 10]).
2025-08-06 15:20:41.638675 test begin: paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 49613],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d_transpose 	 paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 49613],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50804042 	 3204 	 17.997295379638672 	 98.10832834243774 	 2.8697447776794434 	 15.652278900146484 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([4, 6, 8, 6, 49613]) and output[0] has a shape of torch.Size([4, 6, 10, 10, 49615]).
2025-08-06 15:29:04.497361 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), Tensor([10],"int64"), margin=0.5, reduction="mean", name=None, )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), Tensor([10],"int64"), margin=0.5, reduction="mean", name=None, ) 	 101606430 	 5778 	 10.020706176757812 	 9.433888912200928 	 0.06746172904968262 	 0.06627035140991211 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:29:47.767414 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([16934401, 3],"float32"), Tensor([16934401, 3],"float32"), Tensor([16934401],"int64"), margin=0.5, reduction="mean", name=None, )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([16934401, 3],"float32"), Tensor([16934401, 3],"float32"), Tensor([16934401],"int64"), margin=0.5, reduction="mean", name=None, ) 	 118540807 	 5778 	 22.88608980178833 	 22.624574184417725 	 0.16791033744812012 	 0.17335844039916992 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:31:10.153380 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([25401601, 3],"float32"), Tensor([25401601, 3],"float32"), Tensor([25401601],"int64"), margin=0.5, reduction="mean", name=None, )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([25401601, 3],"float32"), Tensor([25401601, 3],"float32"), Tensor([25401601],"int64"), margin=0.5, reduction="mean", name=None, ) 	 177811207 	 5778 	 34.027482748031616 	 33.696361780166626 	 0.251070499420166 	 0.258026123046875 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:33:13.434506 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5],"int32"), margin=0.5, reduction="mean", )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5],"int32"), margin=0.5, reduction="mean", ) 	 50803215 	 5778 	 10.564333438873291 	 9.158963203430176 	 0.0711820125579834 	 0.06429815292358398 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:33:54.158628 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5],"int32"), margin=0.5, reduction="none", )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5],"int32"), margin=0.5, reduction="none", ) 	 50803215 	 5778 	 10.668980836868286 	 9.123582363128662 	 0.07489800453186035 	 0.06633901596069336 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:34:34.729003 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([50803201, 3],"float64"), Tensor([50803201, 3],"float64"), Tensor([50803201],"int32"), margin=0.5, reduction="mean", )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([50803201, 3],"float64"), Tensor([50803201, 3],"float64"), Tensor([50803201],"int32"), margin=0.5, reduction="mean", ) 	 355622407 	 5778 	 112.15613675117493 	 112.24221277236938 	 0.821570873260498 	 0.8598098754882812 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:41:19.669328 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([50803201, 3],"float64"), Tensor([50803201, 3],"float64"), Tensor([50803201],"int32"), margin=0.5, reduction="none", )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([50803201, 3],"float64"), Tensor([50803201, 3],"float64"), Tensor([50803201],"int32"), margin=0.5, reduction="none", ) 	 355622407 	 5778 	 110.51061773300171 	 110.56716752052307 	 0.8873844146728516 	 0.9267501831054688 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:47:54.430381 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([8467201, 3],"float64"), Tensor([8467201, 3],"float64"), Tensor([8467201],"int32"), margin=0.5, reduction="mean", )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([8467201, 3],"float64"), Tensor([8467201, 3],"float64"), Tensor([8467201],"int32"), margin=0.5, reduction="mean", ) 	 59270407 	 5778 	 19.263036012649536 	 19.319270372390747 	 0.1411573886871338 	 0.147935152053833 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:49:03.891181 test begin: paddle.nn.functional.cosine_embedding_loss(Tensor([8467201, 3],"float64"), Tensor([8467201, 3],"float64"), Tensor([8467201],"int32"), margin=0.5, reduction="none", )
[Prof] paddle.nn.functional.cosine_embedding_loss 	 paddle.nn.functional.cosine_embedding_loss(Tensor([8467201, 3],"float64"), Tensor([8467201, 3],"float64"), Tensor([8467201],"int32"), margin=0.5, reduction="none", ) 	 59270407 	 5778 	 18.930716514587402 	 18.962883949279785 	 0.15157866477966309 	 0.15887212753295898 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:50:11.219530 test begin: paddle.nn.functional.cosine_similarity(Tensor([10, 12, 423361],"float32"), Tensor([10, 1, 423361],"float32"), axis=2, eps=1e-06, )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([10, 12, 423361],"float32"), Tensor([10, 1, 423361],"float32"), axis=2, eps=1e-06, ) 	 55036930 	 9454 	 10.044600248336792 	 13.325607299804688 	 0.08291101455688477 	 0.11106681823730469 	 29.448352336883545 	 66.81632876396179 	 0.1991565227508545 	 0.2672123908996582 	 
2025-08-06 15:52:12.296575 test begin: paddle.nn.functional.cosine_similarity(Tensor([10, 508033, 10],"float32"), Tensor([10, 1, 10],"float32"), axis=2, eps=1e-06, )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([10, 508033, 10],"float32"), Tensor([10, 1, 10],"float32"), axis=2, eps=1e-06, ) 	 50803400 	 9454 	 13.232961893081665 	 19.893638372421265 	 0.1429140567779541 	 0.1794743537902832 	 72.41937041282654 	 74.26433825492859 	 0.4354691505432129 	 0.3081848621368408 	 
2025-08-06 15:55:15.841188 test begin: paddle.nn.functional.cosine_similarity(Tensor([10, 508033, 10],"float32"), Tensor([10, 508033, 10],"float32"), axis=2, eps=1e-06, )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([10, 508033, 10],"float32"), Tensor([10, 508033, 10],"float32"), axis=2, eps=1e-06, ) 	 101606600 	 9454 	 20.89035964012146 	 24.203184604644775 	 0.22541260719299316 	 0.21820473670959473 	 53.7976176738739 	 73.7913646697998 	 0.4163239002227783 	 0.3320164680480957 	 
2025-08-06 15:58:10.612125 test begin: paddle.nn.functional.cosine_similarity(Tensor([210, 241921],"float32"), Tensor([210, 241921],"float32"), axis=-1, eps=1e-08, )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([210, 241921],"float32"), Tensor([210, 241921],"float32"), axis=-1, eps=1e-08, ) 	 101606820 	 9454 	 15.032898902893066 	 14.755707502365112 	 0.12445712089538574 	 0.12283802032470703 	 49.48417568206787 	 66.31733441352844 	 0.3820006847381592 	 0.275836706161499 	 
2025-08-06 16:00:42.113159 test begin: paddle.nn.functional.cosine_similarity(Tensor([32, 1587601],"float32"), Tensor([32, 1587601],"float32"), )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([32, 1587601],"float32"), Tensor([32, 1587601],"float32"), ) 	 101606464 	 9454 	 14.50431513786316 	 15.31397008895874 	 0.12018156051635742 	 0.12753057479858398 	 49.53604984283447 	 66.69008469581604 	 0.3823103904724121 	 0.2773630619049072 	 
2025-08-06 16:03:10.145979 test begin: paddle.nn.functional.cosine_similarity(Tensor([396901, 128],"float32"), Tensor([396901, 128],"float32"), )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([396901, 128],"float32"), Tensor([396901, 128],"float32"), ) 	 101606656 	 9454 	 22.1558575630188 	 15.057205200195312 	 0.24001502990722656 	 0.1359386444091797 	 49.95839238166809 	 66.70571517944336 	 0.38556361198425293 	 0.30148911476135254 	 
2025-08-06 16:05:47.743468 test begin: paddle.nn.functional.cosine_similarity(Tensor([423361, 12, 10],"float32"), Tensor([423361, 1, 10],"float32"), axis=2, eps=1e-06, )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([423361, 12, 10],"float32"), Tensor([423361, 1, 10],"float32"), axis=2, eps=1e-06, ) 	 55036930 	 9454 	 13.847623586654663 	 20.10147213935852 	 0.1492443084716797 	 0.18126249313354492 	 33.19705319404602 	 74.29497838020325 	 0.22438549995422363 	 0.3210630416870117 	 
2025-08-06 16:08:12.314887 test begin: paddle.nn.functional.cosine_similarity(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float32"), axis=-1, eps=1e-08, )
[Prof] paddle.nn.functional.cosine_similarity 	 paddle.nn.functional.cosine_similarity(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float32"), axis=-1, eps=1e-08, ) 	 101607424 	 9454 	 14.180184364318848 	 14.477957725524902 	 0.1528933048248291 	 0.15670514106750488 	 49.532360792160034 	 66.41833806037903 	 0.38232922554016113 	 0.29883646965026855 	 
2025-08-06 16:10:42.549249 test begin: paddle.nn.functional.cross_entropy(Tensor([1, 1024, 50304],"float32"), Tensor([1, 1024, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdf050729e0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 16:21:21.093469 test begin: paddle.nn.functional.cross_entropy(Tensor([1, 2048, 151936],"float32"), Tensor([1, 2048, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
W0806 16:21:26.000488 10661 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb36bea6e00>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754469081 (unix time) try "date -d @1754469081" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x28ba) received by PID 10426 (TID 0x7fb367699640) from PID 10426 ***]

2025-08-06 16:31:30.766743 test begin: paddle.nn.functional.cross_entropy(Tensor([1, 2048, 24807],"float32"), Tensor([1, 2048, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
W0806 16:31:31.719635 38014 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.cross_entropy 	 paddle.nn.functional.cross_entropy(Tensor([1, 2048, 24807],"float32"), Tensor([1, 2048, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, ) 	 50806784 	 8585 	 4.066800594329834 	 368.98959040641785 	 0.09688901901245117 	 14.645339965820312 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 2048, 1]) and output[0] has a shape of torch.Size([1, 2048]).
2025-08-06 16:37:50.359060 test begin: paddle.nn.functional.cross_entropy(Tensor([1, 335, 151936],"float32"), Tensor([1, 335, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f89a58fea10>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754470070 (unix time) try "date -d @1754470070" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x9346) received by PID 37702 (TID 0x7f899cdfa640) from PID 37702 ***]

2025-08-06 16:47:57.805759 test begin: paddle.nn.functional.cross_entropy(Tensor([1, 4096, 100352],"float32"), Tensor([1, 4096, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
W0806 16:48:03.994683 80567 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2153b1ae00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 16:59:06.736407 test begin: paddle.nn.functional.cross_entropy(Tensor([1, 4096, 12404],"float32"), Tensor([1, 4096, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
W0806 16:59:07.658474 109083 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.cross_entropy 	 paddle.nn.functional.cross_entropy(Tensor([1, 4096, 12404],"float32"), Tensor([1, 4096, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, ) 	 50810880 	 8585 	 3.381270408630371 	 190.73902130126953 	 0.08047080039978027 	 7.575545787811279 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 4096, 1]) and output[0] has a shape of torch.Size([1, 4096]).
2025-08-06 17:02:26.924727 test begin: paddle.nn.functional.cross_entropy(Tensor([1, 507, 100352],"float32"), Tensor([1, 507, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0b131aaa10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 17:13:29.434465 test begin: paddle.nn.functional.cross_entropy(Tensor([8, 1024, 6202],"float32"), Tensor([8, 1024, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
W0806 17:13:30.853338 144936 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.cross_entropy 	 paddle.nn.functional.cross_entropy(Tensor([8, 1024, 6202],"float32"), Tensor([8, 1024, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, ) 	 50814976 	 8585 	 3.7427334785461426 	 99.73839116096497 	 0.08928251266479492 	 3.9604415893554688 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([8, 1024, 1]) and output[0] has a shape of torch.Size([8, 1024]).
2025-08-06 17:15:22.049255 test begin: paddle.nn.functional.cross_entropy(Tensor([8, 127, 50304],"float32"), Tensor([8, 127, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f766503aa10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 17:25:50.852344 test begin: paddle.nn.functional.dropout(Tensor([75760, 13412],"bfloat16"), 0.0, training=True, mode="upscale_in_train", )
W0806 17:26:06.794237 13054 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.dropout 	 paddle.nn.functional.dropout(Tensor([75760, 13412],"bfloat16"), 0.0, training=True, mode="upscale_in_train", ) 	 1016093120 	 66830 	 0.06532144546508789 	 0.5047252178192139 	 1.71661376953125e-05 	 0.000148773193359375 	 2.0566043853759766 	 299.8100459575653 	 5.221366882324219e-05 	 2.292032480239868 	 combined
2025-08-06 17:31:32.372614 test begin: paddle.nn.functional.dropout(Tensor([77120, 13176],"bfloat16"), 0.0, training=True, mode="upscale_in_train", )
[Prof] paddle.nn.functional.dropout 	 paddle.nn.functional.dropout(Tensor([77120, 13176],"bfloat16"), 0.0, training=True, mode="upscale_in_train", ) 	 1016133120 	 66830 	 0.06577467918395996 	 0.48138856887817383 	 1.5974044799804688e-05 	 0.00010037422180175781 	 2.0842113494873047 	 299.99751830101013 	 6.580352783203125e-05 	 2.296099901199341 	 combined
2025-08-06 17:37:08.146136 test begin: paddle.nn.functional.dropout(Tensor([793810, 1280],"bfloat16"), 0.0, training=True, mode="upscale_in_train", )
[Prof] paddle.nn.functional.dropout 	 paddle.nn.functional.dropout(Tensor([793810, 1280],"bfloat16"), 0.0, training=True, mode="upscale_in_train", ) 	 1016076800 	 66830 	 0.06887388229370117 	 0.47974610328674316 	 3.8623809814453125e-05 	 7.748603820800781e-05 	 2.102935314178467 	 299.98808121681213 	 5.507469177246094e-05 	 2.2919704914093018 	 combined
2025-08-06 17:42:47.538208 test begin: paddle.nn.functional.dropout(Tensor([81680, 12440],"bfloat16"), 0.0, training=True, mode="upscale_in_train", )
[Prof] paddle.nn.functional.dropout 	 paddle.nn.functional.dropout(Tensor([81680, 12440],"bfloat16"), 0.0, training=True, mode="upscale_in_train", ) 	 1016099200 	 66830 	 0.06594038009643555 	 0.4652109146118164 	 2.3126602172851562e-05 	 0.0001780986785888672 	 2.1007802486419678 	 299.74447560310364 	 5.8650970458984375e-05 	 2.29439640045166 	 combined
2025-08-06 17:48:24.315592 test begin: paddle.nn.functional.elu(Tensor([1, 21504, 2363],"float32"), )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([1, 21504, 2363],"float32"), ) 	 50813952 	 33842 	 10.017034530639648 	 10.131208896636963 	 0.3022899627685547 	 0.3057382106781006 	 15.244895219802856 	 15.190608501434326 	 0.4602499008178711 	 0.45967650413513184 	 
2025-08-06 17:49:16.688271 test begin: paddle.nn.functional.elu(Tensor([1, 25401601, 2],"float32"), )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([1, 25401601, 2],"float32"), ) 	 50803202 	 33842 	 10.012185335159302 	 10.137957096099854 	 0.30213356018066406 	 0.3056488037109375 	 15.23618459701538 	 15.185408592224121 	 0.4612436294555664 	 0.4582328796386719 	 
2025-08-06 17:50:09.003588 test begin: paddle.nn.functional.elu(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33842 	 10.018435955047607 	 10.375092506408691 	 0.30228543281555176 	 0.30831193923950195 	 15.247306108474731 	 15.180980443954468 	 0.46001100540161133 	 0.4582669734954834 	 
2025-08-06 17:51:04.219954 test begin: paddle.nn.functional.elu(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33842 	 10.008377313613892 	 10.12484335899353 	 0.30215978622436523 	 0.3056306838989258 	 15.241936922073364 	 15.185809135437012 	 0.4610872268676758 	 0.4582383632659912 	 
2025-08-06 17:51:56.543623 test begin: paddle.nn.functional.elu(Tensor([1182, 21504, 2],"float32"), )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([1182, 21504, 2],"float32"), ) 	 50835456 	 33842 	 10.015671730041504 	 10.13666319847107 	 0.30231165885925293 	 0.3058357238769531 	 15.258604288101196 	 15.190961599349976 	 0.4604916572570801 	 0.45852065086364746 	 
2025-08-06 17:52:48.831278 test begin: paddle.nn.functional.elu(Tensor([15, 3386881],"float32"), 1.0, )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([15, 3386881],"float32"), 1.0, ) 	 50803215 	 33842 	 10.011868476867676 	 10.12908935546875 	 0.30216550827026367 	 0.3056199550628662 	 15.239228248596191 	 15.181891441345215 	 0.4598724842071533 	 0.45825886726379395 	 
2025-08-06 17:53:41.865596 test begin: paddle.nn.functional.elu(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33842 	 10.01285171508789 	 10.132047176361084 	 0.30206727981567383 	 0.30562591552734375 	 15.238662242889404 	 15.182620763778687 	 0.45983242988586426 	 0.4583115577697754 	 
2025-08-06 17:54:34.137393 test begin: paddle.nn.functional.elu(Tensor([2540161, 20],"float32"), 1.0, )
[Prof] paddle.nn.functional.elu 	 paddle.nn.functional.elu(Tensor([2540161, 20],"float32"), 1.0, ) 	 50803220 	 33842 	 10.008158683776855 	 10.128737211227417 	 0.30205488204956055 	 0.30565905570983887 	 15.242905616760254 	 15.185487747192383 	 0.4612157344818115 	 0.4582188129425049 	 
2025-08-06 17:55:28.846891 test begin: paddle.nn.functional.embedding(Tensor([1, 4097],"int64"), weight=Tensor([12404, 8192],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, )
W0806 17:55:41.390787 87384 backward.cc:462] While running Node (EmbeddingGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.nn.functional.embedding 	 paddle.nn.functional.embedding(Tensor([1, 4097],"int64"), weight=Tensor([12404, 8192],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, ) 	 101617665 	 66306 	 10.155325174331665 	 26.401155710220337 	 0.1565237045288086 	 0.4077568054199219 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:56:08.459068 test begin: paddle.nn.functional.embedding(Tensor([101, 1024],"int64"), weight=Tensor([151936, 669],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, )
W0806 17:56:35.698007 88910 backward.cc:462] While running Node (EmbeddingGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.nn.functional.embedding 	 paddle.nn.functional.embedding(Tensor([101, 1024],"int64"), weight=Tensor([151936, 669],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, ) 	 101748608 	 66306 	 24.76085615158081 	 62.534008264541626 	 0.38129711151123047 	 0.940514087677002 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:57:40.705675 test begin: paddle.nn.functional.embedding(Tensor([101, 1024],"int64"), weight=Tensor([24807, 4096],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, )
W0806 17:59:46.836074 93016 backward.cc:462] While running Node (EmbeddingGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.nn.functional.embedding 	 paddle.nn.functional.embedding(Tensor([101, 1024],"int64"), weight=Tensor([24807, 4096],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, ) 	 101712896 	 66306 	 118.47158479690552 	 365.48005843162537 	 1.8248794078826904 	 5.6324849128723145 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 18:05:53.311962 test begin: paddle.nn.functional.embedding(Tensor([101, 4097],"int64"), weight=Tensor([100352, 1013],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, )
W0806 18:08:05.805912 113830 backward.cc:462] While running Node (EmbeddingGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

[Prof] paddle.nn.functional.embedding 	 paddle.nn.functional.embedding(Tensor([101, 4097],"int64"), weight=Tensor([100352, 1013],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, ) 	 102070373 	 66306 	 124.92234468460083 	 367.0343041419983 	 1.923389196395874 	 5.658085107803345 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 18:14:13.855997 test begin: paddle.nn.functional.embedding(Tensor([8, 1024],"int64"), weight=Tensor([24807, 4096],"float16"), padding_idx=None, sparse=False, name=None, )
[Prof] paddle.nn.functional.embedding 	 paddle.nn.functional.embedding(Tensor([8, 1024],"int64"), weight=Tensor([24807, 4096],"float16"), padding_idx=None, sparse=False, name=None, ) 	 101617664 	 66306 	 10.024391174316406 	 29.25178813934326 	 0.15435004234313965 	 0.45035386085510254 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 18:15:16.843674 test begin: paddle.nn.functional.embedding(Tensor([801, 1024],"int64"), weight=Tensor([50304, 2020],"float16"), padding_idx=None, sparse=False, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe67484beb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 18:25:25.750806 test begin: paddle.nn.functional.gather_tree(Tensor([100, 4, 8],"int64"), Tensor([100, 4, 8],"int64"), )
W0806 18:25:26.036895 12012 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([100, 4, 8],"int64"), Tensor([100, 4, 8],"int64"), ) 	 6400 	 1000 	 0.01153564453125 	 219.69607591629028 	 1.4781951904296875e-05 	 0.00028896331787109375 	 None 	 None 	 None 	 None 	 combined
2025-08-06 18:29:06.108490 test begin: paddle.nn.functional.gather_tree(Tensor([100, 8, 4],"int64"), Tensor([100, 8, 4],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([100, 8, 4],"int64"), Tensor([100, 8, 4],"int64"), ) 	 6400 	 1000 	 0.011373043060302734 	 208.42362189292908 	 1.2159347534179688e-05 	 0.0002429485321044922 	 None 	 None 	 None 	 None 	 combined
2025-08-06 18:32:34.800068 test begin: paddle.nn.functional.gather_tree(Tensor([20, 28, 8],"int64"), Tensor([20, 28, 8],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([20, 28, 8],"int64"), Tensor([20, 28, 8],"int64"), ) 	 8960 	 1000 	 0.016086339950561523 	 291.8872275352478 	 5.078315734863281e-05 	 0.0003135204315185547 	 None 	 None 	 None 	 None 	 combined
2025-08-06 18:37:27.020345 test begin: paddle.nn.functional.gather_tree(Tensor([20, 30, 4],"int64"), Tensor([20, 30, 4],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([20, 30, 4],"int64"), Tensor([20, 30, 4],"int64"), ) 	 4800 	 1000 	 0.018549680709838867 	 152.9709975719452 	 3.0517578125e-05 	 0.0002734661102294922 	 None 	 None 	 None 	 None 	 combined
2025-08-06 18:40:00.247740 test begin: paddle.nn.functional.gather_tree(Tensor([20, 4, 57],"int64"), Tensor([20, 4, 57],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([20, 4, 57],"int64"), Tensor([20, 4, 57],"int64"), ) 	 9120 	 1000 	 0.011635541915893555 	 292.43984150886536 	 2.6941299438476562e-05 	 0.0002455711364746094 	 None 	 None 	 None 	 None 	 combined
2025-08-06 18:44:53.008826 test begin: paddle.nn.functional.gather_tree(Tensor([20, 57, 4],"int64"), Tensor([20, 57, 4],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([20, 57, 4],"int64"), Tensor([20, 57, 4],"int64"), ) 	 9120 	 1000 	 0.011227846145629883 	 291.5291006565094 	 1.5020370483398438e-05 	 0.0002422332763671875 	 None 	 None 	 None 	 None 	 combined
2025-08-06 18:49:45.107909 test begin: paddle.nn.functional.gather_tree(Tensor([20, 8, 15],"int64"), Tensor([20, 8, 15],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([20, 8, 15],"int64"), Tensor([20, 8, 15],"int64"), ) 	 4800 	 1000 	 0.018173933029174805 	 154.36614394187927 	 2.3126602172851562e-05 	 0.00026535987854003906 	 None 	 None 	 None 	 None 	 combined
2025-08-06 18:52:20.887670 test begin: paddle.nn.functional.gather_tree(Tensor([200, 4, 4],"int64"), Tensor([200, 4, 4],"int64"), )
[Prof] paddle.nn.functional.gather_tree 	 paddle.nn.functional.gather_tree(Tensor([200, 4, 4],"int64"), Tensor([200, 4, 4],"int64"), ) 	 6400 	 1000 	 0.011379718780517578 	 210.34640192985535 	 1.811981201171875e-05 	 0.0002295970916748047 	 None 	 None 	 None 	 None 	 combined
2025-08-06 18:55:51.573356 test begin: paddle.nn.functional.gelu(Tensor([11, 96, 96, 512],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([11, 96, 96, 512],"float32"), False, None, ) 	 51904512 	 29045 	 10.09391975402832 	 9.740979671478271 	 0.35602736473083496 	 0.31087493896484375 	 13.314610719680786 	 13.349768161773682 	 0.46932554244995117 	 0.4692845344543457 	 
2025-08-06 18:56:41.296357 test begin: paddle.nn.functional.gelu(Tensor([124, 9, 96, 512],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([124, 9, 96, 512],"float32"), False, None, ) 	 54853632 	 29045 	 10.659260511398315 	 9.340012311935425 	 0.37571191787719727 	 0.32814908027648926 	 14.064501523971558 	 14.100355625152588 	 0.4942326545715332 	 0.49704861640930176 	 
2025-08-06 18:57:31.571155 test begin: paddle.nn.functional.gelu(Tensor([124, 96, 9, 512],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([124, 96, 9, 512],"float32"), False, None, ) 	 54853632 	 29045 	 10.652764558792114 	 9.343022346496582 	 0.37572240829467773 	 0.3281571865081787 	 14.036161422729492 	 14.107417821884155 	 0.4932703971862793 	 0.4957277774810791 	 
2025-08-06 18:58:21.544830 test begin: paddle.nn.functional.gelu(Tensor([124, 96, 96, 45],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([124, 96, 96, 45],"float32"), False, None, ) 	 51425280 	 29045 	 9.998045444488525 	 8.766841173171997 	 0.35142087936401367 	 0.3093533515930176 	 13.19181489944458 	 13.234769582748413 	 0.46646785736083984 	 0.4663667678833008 	 
2025-08-06 18:59:08.487888 test begin: paddle.nn.functional.gelu(Tensor([128, 6, 96, 768],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([128, 6, 96, 768],"float32"), False, None, ) 	 56623104 	 29045 	 10.991831064224243 	 9.647573471069336 	 0.3861846923828125 	 0.33981871604919434 	 14.513529539108276 	 14.555609464645386 	 0.5102436542510986 	 0.5114405155181885 	 
2025-08-06 19:00:00.844847 test begin: paddle.nn.functional.gelu(Tensor([128, 9, 96, 512],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([128, 9, 96, 512],"float32"), False, None, ) 	 56623104 	 29045 	 10.990087270736694 	 9.647801876068115 	 0.38623929023742676 	 0.338651180267334 	 14.522866487503052 	 14.545604944229126 	 0.5118961334228516 	 0.5114090442657471 	 
2025-08-06 19:00:52.533613 test begin: paddle.nn.functional.gelu(Tensor([128, 96, 6, 768],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([128, 96, 6, 768],"float32"), False, None, ) 	 56623104 	 29045 	 10.991871356964111 	 9.636329650878906 	 0.38625288009643555 	 0.33868408203125 	 14.517885208129883 	 14.551521062850952 	 0.5107812881469727 	 0.511420726776123 	 
2025-08-06 19:01:44.170342 test begin: paddle.nn.functional.gelu(Tensor([128, 96, 9, 512],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([128, 96, 9, 512],"float32"), False, None, ) 	 56623104 	 29045 	 10.992475509643555 	 9.636126041412354 	 0.387467622756958 	 0.338604211807251 	 14.524891138076782 	 14.554547548294067 	 0.510565996170044 	 0.511404275894165 	 
2025-08-06 19:02:39.527630 test begin: paddle.nn.functional.gelu(Tensor([128, 96, 96, 44],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([128, 96, 96, 44],"float32"), False, None, ) 	 51904512 	 29045 	 10.092803955078125 	 8.848970890045166 	 0.3558051586151123 	 0.31089019775390625 	 13.317602396011353 	 13.357790470123291 	 0.46799707412719727 	 0.47190237045288086 	 
2025-08-06 19:03:27.002207 test begin: paddle.nn.functional.gelu(Tensor([8, 96, 96, 768],"float32"), False, None, )
[Prof] paddle.nn.functional.gelu 	 paddle.nn.functional.gelu(Tensor([8, 96, 96, 768],"float32"), False, None, ) 	 56623104 	 29045 	 10.98863959312439 	 9.63574743270874 	 0.38628411293029785 	 0.3386507034301758 	 14.526453018188477 	 14.555251598358154 	 0.5118987560272217 	 0.5114028453826904 	 
2025-08-06 19:04:18.899056 test begin: paddle.nn.functional.glu(Tensor([200, 498, 512],"float32"), -1, None, )
[Prof] paddle.nn.functional.glu 	 paddle.nn.functional.glu(Tensor([200, 498, 512],"float32"), -1, None, ) 	 50995200 	 13909 	 10.008975744247437 	 3.430619478225708 	 0.24474024772644043 	 0.2508370876312256 	 14.96439266204834 	 5.290111541748047 	 0.36606526374816895 	 0.388232946395874 	 
2025-08-06 19:04:54.040316 test begin: paddle.nn.functional.glu(Tensor([209, 477, 512],"float32"), -1, None, )
[Prof] paddle.nn.functional.glu 	 paddle.nn.functional.glu(Tensor([209, 477, 512],"float32"), -1, None, ) 	 51042816 	 13909 	 10.018245220184326 	 3.4232053756713867 	 0.2461087703704834 	 0.25235795974731445 	 14.978525876998901 	 5.294640064239502 	 0.36621642112731934 	 0.3885481357574463 	 
2025-08-06 19:05:29.243680 test begin: paddle.nn.functional.glu(Tensor([218, 457, 512],"float32"), -1, None, )
[Prof] paddle.nn.functional.glu 	 paddle.nn.functional.glu(Tensor([218, 457, 512],"float32"), -1, None, ) 	 51008512 	 13909 	 10.00779414176941 	 3.4181578159332275 	 0.24448204040527344 	 0.25090503692626953 	 14.9790940284729 	 5.292265176773071 	 0.36774325370788574 	 0.3882739543914795 	 
2025-08-06 19:06:04.490891 test begin: paddle.nn.functional.glu(Tensor([30, 3308, 512],"float32"), -1, None, )
[Prof] paddle.nn.functional.glu 	 paddle.nn.functional.glu(Tensor([30, 3308, 512],"float32"), -1, None, ) 	 50810880 	 13909 	 9.97753620147705 	 3.4078574180603027 	 0.24399781227111816 	 0.25017881393432617 	 14.919326305389404 	 5.269073247909546 	 0.3661231994628906 	 0.3867988586425781 	 
2025-08-06 19:06:40.087309 test begin: paddle.nn.functional.glu(Tensor([30, 457, 3706],"float32"), -1, None, )
[Prof] paddle.nn.functional.glu 	 paddle.nn.functional.glu(Tensor([30, 457, 3706],"float32"), -1, None, ) 	 50809260 	 13909 	 10.361075401306152 	 3.4742848873138428 	 0.2545285224914551 	 0.25472474098205566 	 15.705446720123291 	 5.319555282592773 	 0.3840053081512451 	 0.3903336524963379 	 
2025-08-06 19:07:16.384583 test begin: paddle.nn.functional.grid_sample(Tensor([100, 1, 768, 768],"float32"), Tensor([100, 1, 254017, 2],"float32"), align_corners=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6b9d962ec0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 19:17:21.930653 test begin: paddle.nn.functional.grid_sample(Tensor([100, 1, 768, 768],"float32"), Tensor([100, 21, 12544, 2],"float32"), align_corners=False, )
W0806 19:17:23.846354 79196 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb4d026f0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 19:27:27.370933 test begin: paddle.nn.functional.grid_sample(Tensor([100, 21, 768, 768],"float32"), Tensor([100, 21, 12544, 2],"float32"), align_corners=False, )
W0806 19:27:54.615357 123434 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8d9944aad0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 19:37:32.273947 test begin: paddle.nn.functional.grid_sample(Tensor([1000, 1, 662, 768],"float32"), Tensor([1000, 1, 12544, 2],"float32"), align_corners=False, )
W0806 19:37:41.036264  3619 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6edb03b0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 19:47:43.083304 test begin: paddle.nn.functional.grid_sample(Tensor([1000, 1, 662, 768],"float32"), Tensor([1000, 1, 662, 2],"float32"), align_corners=False, )
W0806 19:47:51.468108 48096 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(Tensor([1000, 1, 662, 768],"float32"), Tensor([1000, 1, 662, 2],"float32"), align_corners=False, ) 	 509740000 	 97873 	 10.036908388137817 	 9.628804683685303 	 0.10460758209228516 	 0.10042190551757812 	 175.16384887695312 	 156.1338620185852 	 0.9138226509094238 	 0.8151793479919434 	 
2025-08-06 19:53:48.350965 test begin: paddle.nn.functional.grid_sample(Tensor([1000, 1, 768, 662],"float32"), Tensor([1000, 1, 12544, 2],"float32"), align_corners=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe38e6d7d60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 20:03:53.987756 test begin: paddle.nn.functional.grid_sample(Tensor([1000, 1, 768, 768],"float32"), Tensor([1000, 1, 12544, 2],"float32"), align_corners=False, )
W0806 20:04:03.604127 127533 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f59a021eef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 20:13:59.719657 test begin: paddle.nn.functional.grid_sample(Tensor([1720, 1, 544, 544],"float32"), Tensor([1720, 1, 12544, 2],"float32"), align_corners=False, )
W0806 20:14:11.122431 16632 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f20f6293130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 20:24:05.878021 test begin: paddle.nn.functional.grid_sample(Tensor([200, 1, 544, 544],"float32"), Tensor([200, 1, 127009, 2],"float32"), align_corners=False, )
W0806 20:24:07.757989 69749 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc48ceb3130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 20:34:11.461526 test begin: paddle.nn.functional.grid_sample(Tensor([200, 1, 544, 544],"float32"), Tensor([200, 11, 12544, 2],"float32"), align_corners=False, )
W0806 20:34:13.528355 122828 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f00a5c9b130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 20:44:17.237058 test begin: paddle.nn.functional.grid_sample(Tensor([200, 11, 544, 544],"float32"), Tensor([200, 11, 12544, 2],"float32"), align_corners=False, )
W0806 20:44:29.978475 12398 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1899d1acb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 20:54:22.385573 test begin: paddle.nn.functional.grid_sample(Tensor([2000, 1, 467, 544],"float32"), Tensor([2000, 1, 12544, 2],"float32"), align_corners=False, )
W0806 20:54:32.087961 65688 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdf7092aef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 21:04:30.715337 test begin: paddle.nn.functional.grid_sample(Tensor([2000, 1, 467, 544],"float32"), Tensor([2000, 1, 467, 2],"float32"), align_corners=False, )
W0806 21:04:42.132542 117282 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(Tensor([2000, 1, 467, 544],"float32"), Tensor([2000, 1, 467, 2],"float32"), align_corners=False, ) 	 509964000 	 97873 	 13.237196207046509 	 12.577249526977539 	 0.13802766799926758 	 0.13111042976379395 	 184.10881447792053 	 164.80229091644287 	 0.9594197273254395 	 0.8587851524353027 	 
2025-08-06 21:10:58.280641 test begin: paddle.nn.functional.grid_sample(Tensor([2000, 1, 544, 467],"float32"), Tensor([2000, 1, 12544, 2],"float32"), align_corners=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f924f84eb60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 21:21:04.394258 test begin: paddle.nn.functional.grid_sample(Tensor([2000, 1, 544, 544],"float32"), Tensor([2000, 1, 12544, 2],"float32"), align_corners=False, )
W0806 21:21:17.709676 38749 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6f1d78b0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 21:31:12.193448 test begin: paddle.nn.functional.grid_sample(Tensor([2026, 1, 544, 544],"float32"), Tensor([2026, 1, 12544, 2],"float32"), align_corners=False, )
W0806 21:31:25.912314 91284 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f69882bf130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 21:41:17.946789 test begin: paddle.nn.functional.grid_sample(Tensor([2026, 1, 768, 768],"float32"), Tensor([2026, 1, 12544, 2],"float32"), align_corners=False, )
W0806 21:41:41.989830 143528 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5740326ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 21:51:23.397268 test begin: paddle.nn.functional.grid_sample(Tensor([870, 1, 768, 768],"float32"), Tensor([870, 1, 12544, 2],"float32"), align_corners=False, )
W0806 21:51:31.737807 33440 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe4f887b130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 22:01:29.202514 test begin: paddle.nn.functional.grid_sample(x=Tensor([1, 64, 80, 94, 311],"float32"), grid=Tensor([1, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
W0806 22:01:31.855913 31462 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f956284b160>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 22:11:34.102268 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 6, 80, 94, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
W0806 22:11:41.017081 83109 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f863944b130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 22:21:43.100250 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 6, 80, 94, 311],"float32"), grid=Tensor([4, 6, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
W0806 22:21:44.451246 135012 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.grid_sample 	 paddle.nn.functional.grid_sample(x=Tensor([4, 6, 80, 94, 311],"float32"), grid=Tensor([4, 6, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 	 56806080 	 97873 	 13.198101282119751 	 9.609031438827515 	 0.13754487037658691 	 0.10015749931335449 	 44.54926776885986 	 45.0902156829834 	 0.233551025390625 	 0.23500466346740723 	 
2025-08-06 22:23:40.078438 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 7, 94, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f554267bf10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 22:34:23.558416 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 7, 94, 311],"float32"), grid=Tensor([4, 280, 7, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
W0806 22:34:24.545737 37061 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f832b1af2e0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 22:44:28.719251 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 8, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
W0806 22:44:30.250267 95272 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2c85d77340>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 22:54:33.505196 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 8, 311],"float32"), grid=Tensor([4, 280, 376, 8, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
W0806 22:54:34.737776 157318 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7ba76cb340>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 23:04:42.357914 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 27],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
W0806 23:04:44.073966 56796 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1d57a23160>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 23:14:49.546985 test begin: paddle.nn.functional.label_smooth(label=Tensor([48, 32, 33712],"float32"), epsilon=0.1, )
W0806 23:14:50.629377 119422 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.label_smooth 	 paddle.nn.functional.label_smooth(label=Tensor([48, 32, 33712],"float32"), epsilon=0.1, ) 	 51781632 	 33539 	 10.135613918304443 	 20.711464881896973 	 0.30829453468322754 	 0.2112734317779541 	 10.136426210403442 	 10.186930179595947 	 0.3094797134399414 	 0.3123910427093506 	 combined
2025-08-06 23:15:43.320303 test begin: paddle.nn.functional.label_smooth(label=Tensor([76, 20, 33712],"float32"), epsilon=0.1, )
[Prof] paddle.nn.functional.label_smooth 	 paddle.nn.functional.label_smooth(label=Tensor([76, 20, 33712],"float32"), epsilon=0.1, ) 	 51242240 	 33539 	 10.021789312362671 	 20.50334906578064 	 0.3049173355102539 	 0.2077186107635498 	 10.028454065322876 	 10.086500406265259 	 0.30507898330688477 	 0.3068251609802246 	 combined
2025-08-06 23:16:39.600413 test begin: paddle.nn.functional.layer_norm(Tensor([115, 435, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
[Prof] paddle.nn.functional.layer_norm 	 paddle.nn.functional.layer_norm(Tensor([115, 435, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, ) 	 51227648 	 33222 	 10.062992811203003 	 10.533583402633667 	 0.30997657775878906 	 0.3245971202850342 	 15.71789288520813 	 33.81925296783447 	 0.2427365779876709 	 0.519228458404541 	 
2025-08-06 23:17:51.501585 test begin: paddle.nn.functional.layer_norm(Tensor([174, 286, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
[Prof] paddle.nn.functional.layer_norm 	 paddle.nn.functional.layer_norm(Tensor([174, 286, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, ) 	 50960384 	 33222 	 10.00621771812439 	 10.474979877471924 	 0.30726075172424316 	 0.32167959213256836 	 15.691046953201294 	 33.64236783981323 	 0.24086856842041016 	 0.5165657997131348 	 
2025-08-06 23:19:03.308307 test begin: paddle.nn.functional.layer_norm(Tensor([226, 220, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
[Prof] paddle.nn.functional.layer_norm 	 paddle.nn.functional.layer_norm(Tensor([226, 220, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, ) 	 50915328 	 33222 	 9.993207693099976 	 10.465088129043579 	 0.30937814712524414 	 0.32271528244018555 	 15.605435132980347 	 33.623894691467285 	 0.23953461647033691 	 0.5163073539733887 	 
2025-08-06 23:20:15.516487 test begin: paddle.nn.functional.layer_norm(Tensor([7, 7088, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
[Prof] paddle.nn.functional.layer_norm 	 paddle.nn.functional.layer_norm(Tensor([7, 7088, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, ) 	 50808832 	 33222 	 10.433678150177002 	 10.45204496383667 	 0.3063220977783203 	 0.3209347724914551 	 15.577759981155396 	 33.499457120895386 	 0.23919177055358887 	 0.5143318176269531 	 
2025-08-06 23:21:27.813660 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 26, 304, 544],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 26, 304, 544],"float32"), 0.1, ) 	 51597312 	 33695 	 10.151726961135864 	 10.206555604934692 	 0.307187557220459 	 0.30895423889160156 	 15.436473608016968 	 15.315517902374268 	 0.4672262668609619 	 0.4636499881744385 	 
2025-08-06 23:22:20.951490 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 32, 122, 1088],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 32, 122, 1088],"float32"), 0.1, ) 	 50970624 	 33695 	 10.025813102722168 	 10.083396673202515 	 0.3049466609954834 	 0.3052952289581299 	 15.239222049713135 	 15.130331754684448 	 0.46415209770202637 	 0.4580817222595215 	 
2025-08-06 23:23:13.163079 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 32, 608, 218],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 32, 608, 218],"float32"), 0.1, ) 	 50896896 	 33695 	 10.01727294921875 	 10.078438758850098 	 0.3042938709259033 	 0.3060727119445801 	 15.210081577301025 	 15.111649751663208 	 0.4608006477355957 	 0.4588205814361572 	 
2025-08-06 23:24:05.971382 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 64, 122, 544],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 64, 122, 544],"float32"), 0.1, ) 	 50970624 	 33695 	 10.028376579284668 	 10.087303876876831 	 0.30355381965637207 	 0.3052845001220703 	 15.245424032211304 	 15.129752159118652 	 0.4614543914794922 	 0.458085298538208 	 
2025-08-06 23:24:58.315691 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 64, 304, 218],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 64, 304, 218],"float32"), 0.1, ) 	 50896896 	 33695 	 10.01143741607666 	 10.068637609481812 	 0.3030858039855957 	 0.3048093318939209 	 15.224687814712524 	 15.101117134094238 	 0.46353769302368164 	 0.45740795135498047 	 
2025-08-06 23:25:51.498868 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 7, 608, 1088],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 7, 608, 1088],"float32"), 0.1, ) 	 55566336 	 33695 	 10.926352500915527 	 10.978349924087524 	 0.3305704593658447 	 0.33368754386901855 	 16.605640411376953 	 16.471681356430054 	 0.5027356147766113 	 0.49881887435913086 	 
2025-08-06 23:26:49.804812 test begin: paddle.nn.functional.leaky_relu(Tensor([13, 64, 256, 256],"float32"), 0.1, None, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([13, 64, 256, 256],"float32"), 0.1, None, ) 	 54525952 	 33695 	 10.709155559539795 	 10.775465726852417 	 0.324293851852417 	 0.3277854919433594 	 16.303030967712402 	 16.160505533218384 	 0.49347448348999023 	 0.48948192596435547 	 
2025-08-06 23:27:45.595847 test begin: paddle.nn.functional.leaky_relu(Tensor([3, 32, 608, 1088],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([3, 32, 608, 1088],"float32"), 0.1, ) 	 63504384 	 33695 	 12.464601039886475 	 12.526718854904175 	 0.37733888626098633 	 0.38179922103881836 	 18.954760551452637 	 18.8017315864563 	 0.5739388465881348 	 0.5693995952606201 	 
2025-08-06 23:28:50.569227 test begin: paddle.nn.functional.leaky_relu(Tensor([5, 64, 304, 544],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([5, 64, 304, 544],"float32"), 0.1, ) 	 52920320 	 33695 	 10.398127555847168 	 10.466796159744263 	 0.3160696029663086 	 0.3193020820617676 	 15.817527532577515 	 15.691253662109375 	 0.4790799617767334 	 0.47496509552001953 	 
2025-08-06 23:29:44.804397 test begin: paddle.nn.functional.leaky_relu(Tensor([64, 13, 256, 256],"float32"), 0.1, None, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([64, 13, 256, 256],"float32"), 0.1, None, ) 	 54525952 	 33695 	 10.714656829833984 	 11.224555969238281 	 0.3256189823150635 	 0.32750678062438965 	 16.297751665115356 	 16.158987760543823 	 0.49477529525756836 	 0.4907059669494629 	 
2025-08-06 23:30:43.145349 test begin: paddle.nn.functional.leaky_relu(Tensor([64, 64, 256, 49],"float32"), 0.1, None, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([64, 64, 256, 49],"float32"), 0.1, None, ) 	 51380224 	 33695 	 10.107733726501465 	 10.403544902801514 	 0.30603718757629395 	 0.30774688720703125 	 15.365939140319824 	 15.241312265396118 	 0.4653177261352539 	 0.4616386890411377 	 
2025-08-06 23:31:39.862062 test begin: paddle.nn.functional.leaky_relu(Tensor([64, 64, 49, 256],"float32"), 0.1, None, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([64, 64, 49, 256],"float32"), 0.1, None, ) 	 51380224 	 33695 	 10.11513876914978 	 10.16443681716919 	 0.30722570419311523 	 0.30770039558410645 	 15.3680419921875 	 15.24121642112732 	 0.46535515785217285 	 0.46170592308044434 	 
2025-08-06 23:32:32.767370 test begin: paddle.nn.functional.linear(x=Tensor([1, 12404],"float32"), weight=Tensor([12404, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, )
Warning: The core code of paddle.nn.functional.linear is too complex.
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([1, 12404],"float32"), weight=Tensor([12404, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, ) 	 50823284 	 62826 	 10.025458097457886 	 9.8192138671875 	 0.054152727127075195 	 0.15963959693908691 	 19.76875352859497 	 19.86344313621521 	 0.08033609390258789 	 0.1075751781463623 	 
2025-08-06 23:33:34.592338 test begin: paddle.nn.functional.linear(x=Tensor([1, 25088],"float32"), weight=Tensor([25088, 2026],"float32"), bias=Tensor([2026],"float32"), name=None, )
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([1, 25088],"float32"), weight=Tensor([25088, 2026],"float32"), bias=Tensor([2026],"float32"), name=None, ) 	 50855402 	 62826 	 10.649118185043335 	 10.02799940109253 	 0.05776476860046387 	 0.16236019134521484 	 20.357056140899658 	 20.2557315826416 	 0.0827183723449707 	 0.10956168174743652 	 
2025-08-06 23:34:39.540529 test begin: paddle.nn.functional.linear(x=Tensor([2, 12404],"float32"), weight=Tensor([12404, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, )
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([2, 12404],"float32"), weight=Tensor([12404, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, ) 	 50835688 	 62826 	 13.528355360031128 	 15.403763055801392 	 0.07309985160827637 	 0.06261610984802246 	 26.89006495475769 	 26.493114233016968 	 0.07304501533508301 	 0.08596944808959961 	 
2025-08-06 23:36:03.623801 test begin: paddle.nn.functional.linear(x=Tensor([2, 25088],"float32"), weight=Tensor([25088, 2026],"float32"), bias=Tensor([2026],"float32"), name=None, )
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([2, 25088],"float32"), weight=Tensor([25088, 2026],"float32"), bias=Tensor([2026],"float32"), name=None, ) 	 50880490 	 62826 	 14.440443515777588 	 19.68553113937378 	 0.07798528671264648 	 0.08004593849182129 	 21.231906414031982 	 21.093329191207886 	 0.08752322196960449 	 0.1140599250793457 	 
2025-08-06 23:37:21.121299 test begin: paddle.nn.functional.linear(x=Tensor([2026, 25088],"float32"), weight=Tensor([25088, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe2082237f0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 23:47:37.680423 test begin: paddle.nn.functional.linear(x=Tensor([4051, 12544],"float32"), weight=Tensor([12544, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, )
Warning: The core code of paddle.nn.functional.linear is too complex.
W0806 23:47:40.377557 151604 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f27f9c3ee00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 23:57:44.423445 test begin: paddle.nn.functional.linear(x=Tensor([4096, 12404],"float32"), weight=Tensor([12404, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, )
Warning: The core code of paddle.nn.functional.linear is too complex.
W0806 23:57:45.670143 50683 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5b87b23040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 00:07:49.748011 test begin: paddle.nn.functional.linear(x=Tensor([4096, 12544],"float32"), weight=Tensor([12544, 4051],"float32"), bias=Tensor([4051],"float32"), name=None, )
Warning: The core code of paddle.nn.functional.linear is too complex.
W0807 00:07:51.436400 113289 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fefa2ebafe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 00:17:54.754743 test begin: paddle.nn.functional.linear(x=Tensor([4096, 49613],"float32"), weight=Tensor([49613, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, )
Warning: The core code of paddle.nn.functional.linear is too complex.
W0807 00:17:59.345502 12043 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f39ec53afe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 00:27:59.571911 test begin: paddle.nn.functional.local_response_norm(Tensor([10585, 3, 40, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, )
W0807 00:28:00.613061 74468 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0807 00:28:00.745659 74468 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(Tensor([10585, 3, 40, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, ) 	 50808000 	 2912 	 12.405004024505615 	 14.869324207305908 	 0.7259612083435059 	 0.6538739204406738 	 25.887070894241333 	 25.265342712402344 	 1.0079476833343506 	 0.5219595432281494 	 
2025-08-07 00:29:21.647829 test begin: paddle.nn.functional.local_response_norm(Tensor([3, 10585, 40, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(Tensor([3, 10585, 40, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, ) 	 50808000 	 2912 	 10.016695022583008 	 13.872613906860352 	 0.5849311351776123 	 0.6124310493469238 	 21.278188705444336 	 22.100203275680542 	 0.8278553485870361 	 0.48398709297180176 	 
2025-08-07 00:30:30.727766 test begin: paddle.nn.functional.local_response_norm(Tensor([3, 3, 141121, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(Tensor([3, 3, 141121, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, ) 	 50803560 	 2912 	 12.630683422088623 	 15.342619895935059 	 0.7370059490203857 	 0.6720876693725586 	 26.5047447681427 	 25.856865882873535 	 1.0345475673675537 	 0.56650710105896 	 
2025-08-07 00:31:53.138987 test begin: paddle.nn.functional.local_response_norm(Tensor([3, 3, 40, 141121],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(Tensor([3, 3, 40, 141121],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, ) 	 50803560 	 2912 	 12.641912698745728 	 12.555336713790894 	 0.73984694480896 	 0.5519804954528809 	 26.415875673294067 	 23.14364719390869 	 1.029707431793213 	 0.5067987442016602 	 
2025-08-07 00:33:09.771136 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3, 40, 47041],"float32"), size=5, data_format="NCDHW", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3, 40, 47041],"float32"), size=5, data_format="NCDHW", ) 	 50804280 	 2912 	 12.627220153808594 	 13.672422647476196 	 0.7391531467437744 	 0.6000649929046631 	 26.219640254974365 	 25.040888786315918 	 1.0198752880096436 	 0.5497589111328125 	 
2025-08-07 00:34:29.199468 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3, 47041, 40],"float32"), size=5, data_format="NCDHW", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3, 47041, 40],"float32"), size=5, data_format="NCDHW", ) 	 50804280 	 2912 	 12.625702619552612 	 13.665415048599243 	 0.7366471290588379 	 0.6000916957855225 	 26.236557245254517 	 25.031421899795532 	 1.0184838771820068 	 0.5484042167663574 	 
2025-08-07 00:35:49.775385 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3529, 40, 40],"float32"), size=5, data_format="NCDHW", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3529, 40, 40],"float32"), size=5, data_format="NCDHW", ) 	 50817600 	 2912 	 12.630128860473633 	 12.403825998306274 	 0.7360131740570068 	 0.5436656475067139 	 26.452394485473633 	 22.878153800964355 	 1.0299272537231445 	 0.5011272430419922 	 
2025-08-07 00:37:07.456711 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 40, 40, 3529],"float32"), size=5, data_format="NDHWC", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 40, 40, 3529],"float32"), size=5, data_format="NDHWC", ) 	 50817600 	 2912 	 20.7858567237854 	 14.050968885421753 	 1.2157807350158691 	 0.6174187660217285 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 3, 40, 40, 3529]) and output[0] has a shape of torch.Size([3, 3529, 3, 40, 40]).
2025-08-07 00:38:23.841965 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 40, 47041, 3],"float32"), size=5, data_format="NDHWC", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 40, 47041, 3],"float32"), size=5, data_format="NDHWC", ) 	 50804280 	 2912 	 23.10726261138916 	 14.556197881698608 	 1.3471603393554688 	 0.6379940509796143 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 3, 40, 47041, 3]) and output[0] has a shape of torch.Size([3, 3, 3, 40, 47041]).
2025-08-07 00:40:17.947834 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 47041, 40, 3],"float32"), size=5, data_format="NDHWC", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 47041, 40, 3],"float32"), size=5, data_format="NDHWC", ) 	 50804280 	 2912 	 23.1145236492157 	 14.558687210083008 	 1.3525137901306152 	 0.639329195022583 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 3, 47041, 40, 3]) and output[0] has a shape of torch.Size([3, 3, 3, 47041, 40]).
2025-08-07 00:42:12.097791 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3529, 3, 40, 40],"float32"), size=5, data_format="NCDHW", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3529, 3, 40, 40],"float32"), size=5, data_format="NCDHW", ) 	 50817600 	 2912 	 10.000444173812866 	 12.36721396446228 	 0.5851056575775146 	 0.5421097278594971 	 21.168362140655518 	 21.412088632583618 	 0.8265423774719238 	 0.4698526859283447 	 
2025-08-07 00:43:19.121744 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3529, 40, 40, 3],"float32"), size=5, data_format="NDHWC", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3529, 40, 40, 3],"float32"), size=5, data_format="NDHWC", ) 	 50817600 	 2912 	 23.102176904678345 	 13.291025638580322 	 1.3489079475402832 	 0.5824828147888184 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 3529, 40, 40, 3]) and output[0] has a shape of torch.Size([3, 3, 3529, 40, 40]).
2025-08-07 00:45:13.217853 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3529, 3, 3, 40, 40],"float32"), size=5, data_format="NCDHW", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3529, 3, 3, 40, 40],"float32"), size=5, data_format="NCDHW", ) 	 50817600 	 2912 	 12.414970397949219 	 13.212419509887695 	 0.7263579368591309 	 0.5814244747161865 	 25.934466123580933 	 24.955535173416138 	 1.0087835788726807 	 0.5476987361907959 	 
2025-08-07 00:46:31.665814 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3529, 3, 40, 40, 3],"float32"), size=5, data_format="NDHWC", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3529, 3, 40, 40, 3],"float32"), size=5, data_format="NDHWC", ) 	 50817600 	 2912 	 23.11246085166931 	 13.26405382156372 	 1.3494749069213867 	 0.5834248065948486 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([3529, 3, 40, 40, 3]) and output[0] has a shape of torch.Size([3529, 3, 3, 40, 40]).
2025-08-07 00:48:24.545133 test begin: paddle.nn.functional.log_loss(Tensor([50803201, 1],"float32"), Tensor([50803201, 1],"float32"), )
[Prof] paddle.nn.functional.log_loss 	 paddle.nn.functional.log_loss(Tensor([50803201, 1],"float32"), Tensor([50803201, 1],"float32"), ) 	 101606402 	 20178 	 9.95873761177063 	 69.08847999572754 	 0.5034046173095703 	 0.3495750427246094 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-07 00:49:59.299042 test begin: paddle.nn.functional.log_loss(Tensor([50803201, 1],"float32"), Tensor([50803201, 1],"float32"), epsilon=1e-07, )
[Prof] paddle.nn.functional.log_loss 	 paddle.nn.functional.log_loss(Tensor([50803201, 1],"float32"), Tensor([50803201, 1],"float32"), epsilon=1e-07, ) 	 101606402 	 20178 	 9.966207027435303 	 69.08067059516907 	 0.5040185451507568 	 0.35094356536865234 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-07 00:51:33.913312 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 10, 254017],"float64"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([10, 10, 254017],"float64"), None, ) 	 25401700 	 33264 	 14.680676698684692 	 15.198469400405884 	 0.4504051208496094 	 0.4661381244659424 	 15.008525371551514 	 15.015105247497559 	 0.4628782272338867 	 0.46187758445739746 	 
2025-08-07 00:52:39.221259 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 10, 508033],"float32"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([10, 10, 508033],"float32"), None, ) 	 50803300 	 33264 	 10.008133172988892 	 9.9351806640625 	 0.3082280158996582 	 0.3059079647064209 	 15.001994132995605 	 14.989801168441772 	 0.46014976501464844 	 0.45992302894592285 	 
2025-08-07 00:53:31.200715 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 254017, 10],"float64"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([10, 254017, 10],"float64"), None, ) 	 25401700 	 33264 	 14.688639402389526 	 15.209898471832275 	 0.4517385959625244 	 0.46614670753479004 	 15.014685869216919 	 15.018203735351562 	 0.46047139167785645 	 0.4617762565612793 	 
2025-08-07 00:54:34.582144 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 508033, 10],"float32"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([10, 508033, 10],"float32"), None, ) 	 50803300 	 33264 	 10.680393695831299 	 9.937248229980469 	 0.30689382553100586 	 0.3046417236328125 	 15.00090503692627 	 14.997458934783936 	 0.4622955322265625 	 0.46109485626220703 	 
2025-08-07 00:55:27.528970 test begin: paddle.nn.functional.log_sigmoid(Tensor([254017, 10, 10],"float64"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([254017, 10, 10],"float64"), None, ) 	 25401700 	 33264 	 14.67908501625061 	 15.202054977416992 	 0.45333051681518555 	 0.4675173759460449 	 15.016737461090088 	 15.014905214309692 	 0.4618659019470215 	 0.4604830741882324 	 
2025-08-07 00:56:28.692773 test begin: paddle.nn.functional.log_sigmoid(Tensor([508033, 10, 10],"float32"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([508033, 10, 10],"float32"), None, ) 	 50803300 	 33264 	 10.002325057983398 	 9.951644659042358 	 0.3068690299987793 	 0.30585813522338867 	 14.994460582733154 	 14.996165037155151 	 0.46008825302124023 	 0.46106910705566406 	 
2025-08-07 00:57:20.553276 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([10, 10, 508033],"float32"), )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(x=Tensor([10, 10, 508033],"float32"), ) 	 50803300 	 33264 	 10.01164984703064 	 9.946706771850586 	 0.30809569358825684 	 0.3047060966491699 	 14.99641728401184 	 15.000954866409302 	 0.4607384204864502 	 0.45989513397216797 	 
2025-08-07 00:58:12.335123 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([10, 508033, 10],"float32"), )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(x=Tensor([10, 508033, 10],"float32"), ) 	 50803300 	 33264 	 10.010514259338379 	 9.943336963653564 	 0.30692148208618164 	 0.30591654777526855 	 14.992469549179077 	 14.987753868103027 	 0.46027493476867676 	 0.45986223220825195 	 
2025-08-07 00:59:04.902823 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([508033, 10, 10],"float32"), )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(x=Tensor([508033, 10, 10],"float32"), ) 	 50803300 	 33264 	 10.010458946228027 	 9.925479412078857 	 0.30821871757507324 	 0.304701566696167 	 14.995392084121704 	 14.997256755828857 	 0.4595494270324707 	 0.46242690086364746 	 
2025-08-07 00:59:56.702057 test begin: paddle.nn.functional.log_softmax(Tensor([128, 396901],"float32"), axis=-1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([128, 396901],"float32"), axis=-1, ) 	 50803328 	 32827 	 23.23879837989807 	 20.61934542655945 	 0.7249503135681152 	 0.6446738243103027 	 45.90507793426514 	 21.3747501373291 	 1.426637887954712 	 0.6658625602722168 	 
2025-08-07 01:01:49.656289 test begin: paddle.nn.functional.log_softmax(Tensor([264, 192612],"float32"), axis=-1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([264, 192612],"float32"), axis=-1, ) 	 50849568 	 32827 	 20.34279155731201 	 21.212218523025513 	 0.6333737373352051 	 0.6586365699768066 	 28.562822580337524 	 21.764673233032227 	 0.8889861106872559 	 0.6764404773712158 	 
2025-08-07 01:03:23.769201 test begin: paddle.nn.functional.log_softmax(Tensor([2944, 17257],"float32"), axis=1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([2944, 17257],"float32"), axis=1, ) 	 50804608 	 32827 	 11.726192235946655 	 11.105967998504639 	 0.33713245391845703 	 0.3449382781982422 	 21.27124786376953 	 17.13089609146118 	 0.6622536182403564 	 0.5324172973632812 	 
2025-08-07 01:04:28.841685 test begin: paddle.nn.functional.log_softmax(Tensor([4224, 12028],"float32"), axis=1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([4224, 12028],"float32"), axis=1, ) 	 50806272 	 32827 	 10.70415711402893 	 10.109044075012207 	 0.3130064010620117 	 0.31417179107666016 	 20.71623420715332 	 14.953378438949585 	 0.6436138153076172 	 0.46457958221435547 	 
2025-08-07 01:05:27.831065 test begin: paddle.nn.functional.log_softmax(Tensor([7664, 6629],"float32"), axis=1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([7664, 6629],"float32"), axis=1, ) 	 50804656 	 32827 	 9.944931268692017 	 9.82377815246582 	 0.30936646461486816 	 0.30525684356689453 	 19.579317092895508 	 16.916921138763428 	 0.6123402118682861 	 0.5269253253936768 	 
2025-08-07 01:06:26.365295 test begin: paddle.nn.functional.lp_pool1d(Tensor([2, 3, 8467201],"float32"), 4.0, 3, 2, 1, False, "NCL", None, )
[Prof] paddle.nn.functional.lp_pool1d 	 paddle.nn.functional.lp_pool1d(Tensor([2, 3, 8467201],"float32"), 4.0, 3, 2, 1, False, "NCL", None, ) 	 50803206 	 10734 	 10.020389556884766 	 20.626365184783936 	 0.9525566101074219 	 0.2449808120727539 	 15.229745626449585 	 53.31922626495361 	 0.7262425422668457 	 0.3170492649078369 	 
2025-08-07 01:08:06.989204 test begin: paddle.nn.functional.lp_pool1d(Tensor([2, 793801, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, )
[Prof] paddle.nn.functional.lp_pool1d 	 paddle.nn.functional.lp_pool1d(Tensor([2, 793801, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, ) 	 50803264 	 10734 	 10.030146837234497 	 20.855226278305054 	 0.9568085670471191 	 0.24717497825622559 	 15.260874032974243 	 55.35038161277771 	 0.7262401580810547 	 0.3320345878601074 	 
2025-08-07 01:09:50.546074 test begin: paddle.nn.functional.lp_pool1d(Tensor([529201, 3, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, )
[Prof] paddle.nn.functional.lp_pool1d 	 paddle.nn.functional.lp_pool1d(Tensor([529201, 3, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, ) 	 50803296 	 10734 	 10.022264242172241 	 20.847338676452637 	 0.9527988433837891 	 0.2471468448638916 	 15.265060663223267 	 55.37803506851196 	 0.7267866134643555 	 0.3295016288757324 	 
2025-08-07 01:11:34.860381 test begin: paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, ) 	 50804736 	 18010 	 39.0760977268219 	 63.17353820800781 	 2.1549460887908936 	 0.44791197776794434 	 40.62016940116882 	 122.97233605384827 	 1.1517386436462402 	 0.4652106761932373 	 
2025-08-07 01:16:03.497373 test begin: paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, ) 	 50804736 	 18010 	 10.034987449645996 	 21.779655933380127 	 0.5696446895599365 	 0.15505409240722656 	 18.009167194366455 	 66.00960803031921 	 0.5098836421966553 	 0.24927139282226562 	 
2025-08-07 01:18:00.752748 test begin: paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, ) 	 50804736 	 18010 	 16.9640109539032 	 14.267944812774658 	 0.9618403911590576 	 0.10179543495178223 	 34.70279335975647 	 59.412580251693726 	 0.9827189445495605 	 0.2259688377380371 	 
2025-08-07 01:20:08.264169 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, ) 	 50804736 	 18010 	 37.95057773590088 	 63.2143669128418 	 2.1578712463378906 	 0.44655895233154297 	 40.61413621902466 	 122.92672348022461 	 1.154219388961792 	 0.4638378620147705 	 
2025-08-07 01:24:34.842831 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, ) 	 50804736 	 18010 	 10.700967073440552 	 21.760315656661987 	 0.5681343078613281 	 0.15366768836975098 	 18.007855892181396 	 65.8706464767456 	 0.511263370513916 	 0.24891257286071777 	 
2025-08-07 01:26:34.315814 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, ) 	 50804736 	 18010 	 17.184958934783936 	 14.259780406951904 	 0.9602909088134766 	 0.10051870346069336 	 34.712525606155396 	 59.52126741409302 	 0.9843459129333496 	 0.22628068923950195 	 
2025-08-07 01:28:42.002229 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 3, 264601, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 3, 264601, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, ) 	 50803392 	 18010 	 39.16445255279541 	 63.440303564071655 	 2.223600149154663 	 0.4482088088989258 	 41.420470237731934 	 125.4091637134552 	 1.174285650253296 	 0.4459517002105713 	 
2025-08-07 01:33:14.192266 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 3, 264601, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 3, 264601, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, ) 	 50803392 	 18010 	 10.024407386779785 	 21.80471682548523 	 0.5677359104156494 	 0.15398812294006348 	 18.014773845672607 	 66.0160722732544 	 0.5102047920227051 	 0.2338416576385498 	 
2025-08-07 01:35:11.425437 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 264601],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 264601],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, ) 	 50803392 	 18010 	 37.76474738121033 	 64.71116399765015 	 2.1426384449005127 	 0.4573252201080322 	 41.403170108795166 	 125.19275689125061 	 1.173919439315796 	 0.44363903999328613 	 
2025-08-07 01:39:43.061521 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 264601],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 264601],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, ) 	 50803392 	 18010 	 10.019920349121094 	 21.887025833129883 	 0.5689969062805176 	 0.1545708179473877 	 17.95308828353882 	 65.13570189476013 	 0.509671688079834 	 0.23134279251098633 	 
2025-08-07 01:41:39.857554 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([1373060, 37],"float32"), Tensor([1373060],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([1373060, 37],"float32"), Tensor([1373060],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, ) 	 52176280 	 3872 	 11.452072381973267 	 6.413471937179565 	 0.33473730087280273 	 0.11247014999389648 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-07 01:42:02.066215 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([25401601, 37],"float16"), Tensor([25401601],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([25401601, 37],"float16"), Tensor([25401601],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, ) 	 965260838 	 3872 	 203.60033679008484 	 75.41359376907349 	 5.972235202789307 	 1.325773000717163 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-07 01:48:08.866839 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([25401601, 37],"float32"), Tensor([25401601],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([25401601, 37],"float32"), Tensor([25401601],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, ) 	 965260838 	 3872 	 213.39346480369568 	 117.04434061050415 	 6.256715297698975 	 1.813098430633545 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-07 01:54:40.474285 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([2746119, 37],"float16"), Tensor([2746119],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([2746119, 37],"float16"), Tensor([2746119],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, ) 	 104352522 	 3872 	 22.297294855117798 	 8.344534635543823 	 0.6552517414093018 	 0.14610934257507324 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-07 01:55:21.482259 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([5, 10160641],"float32"), Tensor([5],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([5, 10160641],"float32"), Tensor([5],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, ) 	 50803210 	 3872 	 271.5863127708435 	 23.956227779388428 	 6.521549940109253 	 0.4200010299682617 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-07 02:00:19.414820 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, return_softmax=True, reduction="mean", )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, return_softmax=True, reduction="mean", ) 	 25401610 	 3872 	 162.51652002334595 	 61.627506494522095 	 3.610377311706543 	 1.09028959274292 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-07 02:04:07.637150 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([686530, 37],"float64"), Tensor([686530],"int64"), margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, return_softmax=True, reduction="mean", )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([686530, 37],"float64"), Tensor([686530],"int64"), margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, return_softmax=True, reduction="mean", ) 	 26088140 	 3872 	 9.645151376724243 	 99.86264610290527 	 0.23092412948608398 	 1.7786149978637695 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-07 02:06:03.936428 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), 0.0, "mean", )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), 0.0, "mean", ) 	 76204830 	 7473 	 10.031369924545288 	 14.493128061294556 	 0.27321958541870117 	 0.2828085422515869 	 13.551084280014038 	 15.792516231536865 	 0.4627249240875244 	 0.2710747718811035 	 
2025-08-07 02:07:01.074780 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), 0.0, "mean", None, )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), 0.0, "mean", None, ) 	 76204830 	 7473 	 10.01860499382019 	 14.496093511581421 	 0.2733280658721924 	 0.28391385078430176 	 13.553637981414795 	 15.79227876663208 	 0.46279311180114746 	 0.2699127197265625 	 
2025-08-07 02:07:57.455638 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), 0.0, "mean", )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), 0.0, "mean", ) 	 76204830 	 7473 	 10.025534629821777 	 14.495710134506226 	 0.27338337898254395 	 0.2827584743499756 	 13.548757553100586 	 15.789995670318604 	 0.4627037048339844 	 0.26976776123046875 	 
2025-08-07 02:08:53.021718 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), 0.0, "mean", None, )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), 0.0, "mean", None, ) 	 76204830 	 7473 	 10.027598142623901 	 14.510942220687866 	 0.27457475662231445 	 0.28391551971435547 	 13.552881002426147 	 15.785137176513672 	 0.46396875381469727 	 0.27108263969421387 	 
2025-08-07 02:09:50.628256 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([50803201],"float32"), Tensor([50803201],"float32"), Tensor([50803201],"float32"), 0.5, "mean", None, )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([50803201],"float32"), Tensor([50803201],"float32"), Tensor([50803201],"float32"), 0.5, "mean", None, ) 	 152409603 	 7473 	 12.306983232498169 	 14.517647504806519 	 0.2413346767425537 	 0.28343963623046875 	 16.8249831199646 	 16.832781076431274 	 0.5773489475250244 	 0.2875959873199463 	 
2025-08-07 02:10:53.658686 test begin: paddle.nn.functional.max_pool1d(Tensor([2, 3, 8467201],"float32"), 2, None, 0, False, False, None, )
[Prof] paddle.nn.functional.max_pool1d 	 paddle.nn.functional.max_pool1d(Tensor([2, 3, 8467201],"float32"), 2, None, 0, False, False, None, ) 	 50803206 	 37061 	 9.962787628173828 	 15.43290662765503 	 0.2753453254699707 	 0.42500829696655273 	 28.93686866760254 	 49.047778606414795 	 0.3982081413269043 	 0.6751470565795898 	 
2025-08-07 02:12:39.786114 test begin: paddle.nn.functional.max_pool1d(Tensor([226801, 32, 7],"float32"), 7, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd4e9b41240>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 02:23:03.035365 test begin: paddle.nn.functional.max_pool1d(Tensor([91, 32, 17447],"float32"), 7, )
W0807 02:23:04.306784 116370 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0807 02:23:04.322860 116370 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.max_pool1d 	 paddle.nn.functional.max_pool1d(Tensor([91, 32, 17447],"float32"), 7, ) 	 50805664 	 37061 	 11.631007432937622 	 8.013898134231567 	 0.3201158046722412 	 0.22169804573059082 	 27.508219957351685 	 47.33438491821289 	 0.37863874435424805 	 0.651390552520752 	 
2025-08-07 02:24:42.053435 test begin: paddle.nn.functional.max_pool2d(Tensor([10, 128, 480, 83],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([10, 128, 480, 83],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 50995200 	 50852 	 10.611035346984863 	 14.017915725708008 	 0.21282148361206055 	 0.28127336502075195 	 34.217631578445435 	 68.66599321365356 	 0.34311842918395996 	 0.690147876739502 	 
2025-08-07 02:26:50.948864 test begin: paddle.nn.functional.max_pool2d(Tensor([10, 128, 83, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([10, 128, 83, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 50995200 	 50852 	 10.681155443191528 	 13.901827335357666 	 0.2007122039794922 	 0.28012919425964355 	 33.839823722839355 	 68.88010263442993 	 0.33931398391723633 	 0.6935923099517822 	 
2025-08-07 02:29:00.291432 test begin: paddle.nn.functional.max_pool2d(Tensor([10, 23, 480, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([10, 23, 480, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 52992000 	 50852 	 10.655374526977539 	 14.665462970733643 	 0.21501374244689941 	 0.294048547744751 	 35.51937484741211 	 71.65630650520325 	 0.35614681243896484 	 0.7186791896820068 	 
2025-08-07 02:31:14.002780 test begin: paddle.nn.functional.max_pool2d(Tensor([1536, 24, 112, 13],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([1536, 24, 112, 13],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 53673984 	 50852 	 18.502739906311035 	 19.458675384521484 	 0.3719816207885742 	 0.3904256820678711 	 25.45513415336609 	 74.96012449264526 	 0.5134117603302002 	 0.7517275810241699 	 
2025-08-07 02:33:39.221676 test begin: paddle.nn.functional.max_pool2d(Tensor([1536, 24, 13, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([1536, 24, 13, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 53673984 	 50852 	 17.185025930404663 	 18.677608966827393 	 0.344620943069458 	 0.3762400150299072 	 24.966390371322632 	 74.2280912399292 	 0.5007476806640625 	 0.7458517551422119 	 
2025-08-07 02:35:55.532405 test begin: paddle.nn.functional.max_pool2d(Tensor([1536, 3, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([1536, 3, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 57802752 	 50852 	 20.55194091796875 	 19.2389018535614 	 0.41223883628845215 	 0.38579249382019043 	 46.4179253578186 	 80.23542857170105 	 0.4655182361602783 	 0.8046691417694092 	 
2025-08-07 02:38:43.307252 test begin: paddle.nn.functional.max_pool2d(Tensor([169, 24, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([169, 24, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 50878464 	 50852 	 15.31271243095398 	 16.970827341079712 	 0.3070979118347168 	 0.340360164642334 	 40.99286437034607 	 70.7887647151947 	 0.4138634204864502 	 0.7101714611053467 	 
2025-08-07 02:41:09.643162 test begin: paddle.nn.functional.max_pool2d(Tensor([2, 128, 480, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([2, 128, 480, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 58982400 	 50852 	 11.692812204360962 	 16.28722858428955 	 0.23705244064331055 	 0.32674431800842285 	 39.359395027160645 	 79.71854639053345 	 0.3947482109069824 	 0.7995507717132568 	 
2025-08-07 02:43:39.617917 test begin: paddle.nn.functional.max_pool2d(Tensor([2, 64, 704, 704],"float32"), kernel_size=3, stride=2, padding=1, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([2, 64, 704, 704],"float32"), kernel_size=3, stride=2, padding=1, ) 	 63438848 	 50852 	 19.51545023918152 	 21.27947211265564 	 0.39139246940612793 	 0.4268677234649658 	 51.2101411819458 	 89.69976139068604 	 0.5148074626922607 	 0.8996503353118896 	 
2025-08-07 02:46:43.359513 test begin: paddle.nn.functional.max_pool2d(Tensor([8, 13, 704, 704],"float32"), kernel_size=3, stride=2, padding=1, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([8, 13, 704, 704],"float32"), kernel_size=3, stride=2, padding=1, ) 	 51544064 	 50852 	 17.95776128768921 	 17.355440378189087 	 0.3615124225616455 	 0.34925198554992676 	 41.86309051513672 	 72.97048020362854 	 0.4227445125579834 	 0.731849193572998 	 
2025-08-07 02:49:14.649120 test begin: paddle.nn.functional.max_pool2d(Tensor([8, 64, 141, 704],"float32"), kernel_size=3, stride=2, padding=1, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([8, 64, 141, 704],"float32"), kernel_size=3, stride=2, padding=1, ) 	 50823168 	 50852 	 15.698659658432007 	 17.16490411758423 	 0.3147737979888916 	 0.34676218032836914 	 41.033902645111084 	 71.91075778007507 	 0.4128990173339844 	 0.7226481437683105 	 
2025-08-07 02:51:41.600316 test begin: paddle.nn.functional.max_pool2d(Tensor([8, 64, 704, 141],"float32"), kernel_size=3, stride=2, padding=1, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([8, 64, 704, 141],"float32"), kernel_size=3, stride=2, padding=1, ) 	 50823168 	 50852 	 15.739661931991577 	 17.23893451690674 	 0.31577610969543457 	 0.3452591896057129 	 41.09665489196777 	 71.00655126571655 	 0.41359519958496094 	 0.7123446464538574 	 
2025-08-07 02:54:09.607390 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 16934401],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f210c4bdcf0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 03:04:14.972320 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 16934401],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0807 03:04:17.542342 39324 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f899fff6e60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 03:14:19.810421 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, )
W0807 03:14:21.557497 99160 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9e534db130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 03:24:24.903467 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0807 03:24:26.141937 158869 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb8764d3040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 03:34:29.779829 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 8467201],"int32"), kernel_size=2, stride=2, )
W0807 03:34:31.009392 56029 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f02d43630d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 03:44:34.780733 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 8467201],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0807 03:44:40.333586 116092 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7a1dfdf0a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 03:54:44.210268 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, )
W0807 03:54:45.530527 12115 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcc4d23aef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 04:04:49.275808 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0807 04:04:50.775127 73053 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff424e4ebf0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 04:14:57.307489 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
W0807 04:14:57.529551 133038 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 2540162448 	 283012 	 11.963820934295654 	 12.38245415687561 	 0.0001423358917236328 	 0.0003275871276855469 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-07 04:15:44.786855 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([211680101, 3, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([211680101, 3, 8],"int32"), kernel_size=2, stride=2, ) 	 5080322448 	 283012 	 10.370407342910767 	 11.67549180984497 	 9.942054748535156e-05 	 0.00021886825561523438 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-07 04:16:29.625283 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([211680101, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([211680101, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 5080322448 	 283012 	 11.030186653137207 	 12.571991443634033 	 0.00012040138244628906 	 0.00021791458129882812 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-07 04:17:16.602345 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7efbf44aab90>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 04:27:21.732886 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0807 04:27:23.206812 45239 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc98d64abf0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 04:37:26.497640 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 3175201, 8],"int32"), kernel_size=2, stride=2, )
W0807 04:37:28.049459 105278 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb1985cb100>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 04:47:31.323453 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 3175201, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0807 04:47:32.542729 163670 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa968f57070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 04:57:36.032467 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, )
W0807 04:57:40.119858 52992 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1105b3aef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 05:07:44.148863 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0807 05:07:45.563112 105572 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f901b1b3040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 05:17:49.008419 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 6350401, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, )
W0807 05:17:51.353336 158200 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f39e63270d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-07 05:27:53.681675 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 6350401, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0807 05:27:57.860312 47030 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f93debb6e60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 21:37:55.776254 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 16934401],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
W0805 21:37:55.991569 108805 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 16934401],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 10262247006 	 283012 	 12.096189498901367 	 12.125516653060913 	 0.000148773193359375 	 0.00028634071350097656 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:38:42.568212 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 7696685406 	 283012 	 16.60385537147522 	 15.565802335739136 	 0.00010776519775390625 	 0.0002129077911376953 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:39:42.623288 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 16934401],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5131125927 	 283012 	 17.471348762512207 	 15.469522953033447 	 0.00010633468627929688 	 0.00021791458129882812 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:40:42.739235 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8467201],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8467201],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5131123806 	 283012 	 12.089812755584717 	 11.654552221298218 	 9.465217590332031e-05 	 0.00020265579223632812 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:41:29.741494 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([101, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 2565564327 	 283012 	 12.037757635116577 	 11.643562316894531 	 0.00012636184692382812 	 0.0002048015594482422 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:42:15.770522 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 2565564832 	 283012 	 17.055128574371338 	 15.3233163356781 	 9.799003601074219e-05 	 0.0001804828643798828 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:43:14.836421 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 25404048 	 283012 	 12.08194088935852 	 11.546507358551025 	 8.511543273925781e-05 	 9.608268737792969e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:44:00.601823 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 3, 16934401],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 3, 16934401],"int32"), kernel_size=2, stride=2, ) 	 5131125927 	 283012 	 14.41795039176941 	 16.698673009872437 	 9.012222290039062e-05 	 0.00021076202392578125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:44:59.301396 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 3, 16934401],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 3, 16934401],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 5131125927 	 283012 	 10.489398002624512 	 12.788855791091919 	 0.00010514259338378906 	 0.00019073486328125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:45:44.713628 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 6350401, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 6350401, 8],"int32"), kernel_size=2, stride=2, ) 	 5131126432 	 283012 	 10.451484441757202 	 14.964302062988281 	 4.8160552978515625e-05 	 0.000186920166015625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:46:37.205256 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 6350401, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([101, 6350401, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 5131126432 	 283012 	 10.641291618347168 	 11.221478462219238 	 5.1021575927734375e-05 	 0.0001761913299560547 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:47:21.500329 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, ) 	 50805648 	 283012 	 12.982216358184814 	 11.218748092651367 	 0.0001227855682373047 	 0.0001990795135498047 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:48:11.161654 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 50805648 	 283012 	 10.657610177993774 	 11.236812591552734 	 5.340576171875e-05 	 0.0002009868621826172 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:48:55.240753 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 3175201, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 3175201, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5131124816 	 283012 	 12.448643207550049 	 11.611471176147461 	 0.00011873245239257812 	 0.00020599365234375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:49:42.081792 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5131126432 	 283012 	 15.880358695983887 	 11.592589378356934 	 0.000125885009765625 	 6.985664367675781e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:50:32.324955 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 7696686416 	 283012 	 13.138910055160522 	 11.639807939529419 	 7.724761962890625e-05 	 0.00020575523376464844 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:51:19.681384 test begin: paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 6350401, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([101, 6350401, 8],"float32"), Tensor([101, 6350401, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 10262248016 	 283012 	 11.990712881088257 	 11.65077829360962 	 4.9591064453125e-05 	 0.00019598007202148438 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 21:52:05.512623 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f222c0a3f40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 22:02:10.487075 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0805 22:02:14.538903 110526 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0099bcea10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 22:12:15.028304 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([1058401, 3, 8],"int32"), kernel_size=2, stride=2, )
W0805 22:12:16.273191 110837 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7feeea6e3130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 22:22:19.707277 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([1058401, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0805 22:22:20.939095 111246 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb8eda56e60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 22:32:29.652351 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, )
W0805 22:32:30.887395 111480 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f942a27eef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 22:42:34.188108 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0805 22:42:42.907649 111783 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe41412b040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 22:52:47.647059 test begin: paddle.nn.functional.max_unpool1d(Tensor([105840101, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
W0805 22:52:47.941169 112173 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([105840101, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5080324848 	 283012 	 13.007200479507446 	 15.823494911193848 	 0.0001049041748046875 	 0.00041961669921875 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 22:53:40.970679 test begin: paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 50805648 	 283012 	 11.802287340164185 	 11.679972648620605 	 0.00010156631469726562 	 0.0002014636993408203 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 22:54:26.507278 test begin: paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([105840101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 2590965648 	 283012 	 18.00960397720337 	 15.43694257736206 	 0.00011348724365234375 	 0.0002105236053466797 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 22:55:27.592976 test begin: paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7d81f73100>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 23:05:32.872862 test begin: paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
W0805 23:05:35.010577 112592 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f105a8f2e60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 23:15:37.648032 test begin: paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([1, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
W0805 23:15:41.892992 113089 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([1, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5080322448 	 283012 	 11.950622320175171 	 15.681208848953247 	 0.00014066696166992188 	 0.0003826618194580078 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:16:34.866137 test begin: paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 5105724048 	 283012 	 15.474303722381592 	 12.40523362159729 	 0.00010704994201660156 	 0.0002033710479736328 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:17:24.985341 test begin: paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([211680101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([211680101, 3, 8],"float32"), Tensor([211680101, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 10160644848 	 283012 	 17.147265672683716 	 15.248199224472046 	 0.00011301040649414062 	 0.0002040863037109375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:18:24.169040 test begin: paddle.nn.functional.max_unpool2d(Tensor([1894, 8, 86, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([1894, 8, 86, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 222571440 	 470794 	 27.599917888641357 	 40.07254886627197 	 0.029956579208374023 	 0.04348421096801758 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:20:03.729289 test begin: paddle.nn.functional.max_unpool2d(Tensor([189401, 8, 86, 39],"float32"), Tensor([189401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([189401, 8, 86, 39],"float32"), Tensor([189401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 10164015264 	 470794 	 27.56957983970642 	 40.07402491569519 	 0.029938459396362305 	 0.04342079162597656 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:21:44.176450 test begin: paddle.nn.functional.max_unpool2d(Tensor([189401, 8, 86, 39],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([189401, 8, 86, 39],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5083724880 	 470794 	 27.559903144836426 	 40.86812162399292 	 0.029911518096923828 	 0.043493032455444336 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:23:24.057850 test begin: paddle.nn.functional.max_unpool2d(Tensor([3887, 16, 43, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([3887, 16, 43, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 134484736 	 470794 	 10.141247272491455 	 14.161738634109497 	 0.00014638900756835938 	 0.0001983642578125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:24:16.902727 test begin: paddle.nn.functional.max_unpool2d(Tensor([388701, 16, 43, 19],"float32"), Tensor([388701, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([388701, 16, 43, 19],"float32"), Tensor([388701, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 10162198944 	 470794 	 9.997111797332764 	 14.192386865615845 	 0.010840892791748047 	 0.0002009868621826172 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:25:09.836745 test begin: paddle.nn.functional.max_unpool2d(Tensor([388701, 16, 43, 19],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([388701, 16, 43, 19],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5081936080 	 470794 	 10.122266292572021 	 14.143288373947144 	 8.869171142578125e-05 	 0.00017189979553222656 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:26:02.727375 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([388701, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([388701, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5081936080 	 470794 	 10.067070007324219 	 14.425050973892212 	 0.010900497436523438 	 0.00020265579223632812 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:26:57.944998 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([840101, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([840101, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5081317920 	 470794 	 10.060092687606812 	 15.4898042678833 	 9.012222290039062e-05 	 0.0001990795135498047 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:27:53.677007 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([189401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([189401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5083724880 	 470794 	 27.49949312210083 	 40.196224212646484 	 0.029844999313354492 	 0.04360532760620117 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:29:29.815807 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 2612, 19],"float32"), Tensor([6401, 16, 2612, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 2612, 19],"float32"), Tensor([6401, 16, 2612, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 10165402496 	 470794 	 10.127286672592163 	 14.280471563339233 	 0.010973453521728516 	 0.0002048015594482422 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:30:23.397006 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 2612, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 2612, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5166375120 	 470794 	 11.004043579101562 	 14.198497533798218 	 0.010955333709716797 	 0.00019693374633789062 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:31:18.239593 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 1154],"float32"), Tensor([6401, 16, 43, 1154],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 1154],"float32"), Tensor([6401, 16, 43, 1154],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 10164173504 	 470794 	 11.163317918777466 	 16.030121326446533 	 0.0003726482391357422 	 0.00016570091247558594 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:32:17.672097 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 1154],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 1154],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5165760624 	 470794 	 11.131088733673096 	 14.185372114181519 	 0.00013971328735351562 	 0.00019931793212890625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:33:15.540765 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([3887, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([3887, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 134484736 	 470794 	 10.120917797088623 	 14.372703552246094 	 0.010970592498779297 	 0.00019407272338867188 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:34:08.807353 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 16, 2612, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 16, 2612, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5166375120 	 470794 	 11.107253789901733 	 14.689013242721558 	 4.553794860839844e-05 	 0.0001964569091796875 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:35:06.795006 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 16, 43, 1154],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 16, 43, 1154],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5165760624 	 470794 	 10.109781980514526 	 19.211559772491455 	 0.0002913475036621094 	 0.0005054473876953125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:36:08.183672 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 972, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 16, 43, 19],"float32"), Tensor([6401, 972, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5166861596 	 470794 	 10.878340482711792 	 14.17188549041748 	 0.00011610984802246094 	 0.00048065185546875 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:37:06.532700 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 237, 86, 39],"float32"), Tensor([6401, 237, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 237, 86, 39],"float32"), Tensor([6401, 237, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 10176284196 	 470794 	 27.6706759929657 	 40.31789994239807 	 0.030034542083740234 	 0.0437474250793457 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:38:43.704178 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 237, 86, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 237, 86, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5259893730 	 470794 	 27.734492301940918 	 40.33648371696472 	 0.030087709426879883 	 0.04372859001159668 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:40:25.374397 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 1182],"float32"), Tensor([6401, 32, 21, 1182],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 1182],"float32"), Tensor([6401, 32, 21, 1182],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 10168679808 	 470794 	 11.15975308418274 	 14.430368423461914 	 6.890296936035156e-05 	 0.00019550323486328125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:41:21.380347 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 1182],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 1182],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5123053152 	 470794 	 10.791991233825684 	 14.451703786849976 	 9.322166442871094e-05 	 0.00019741058349609375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:42:14.972348 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 32, 21, 1182],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 32, 21, 1182],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5123053152 	 470794 	 14.414005517959595 	 14.641309261322021 	 0.00012683868408203125 	 0.0002524852752685547 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:43:12.465264 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 32, 2757, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 32, 2757, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5121209664 	 470794 	 13.1100435256958 	 14.589225053787231 	 0.0001049041748046875 	 0.0001919269561767578 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:44:08.611846 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 4201, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([6401, 4201, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5121036837 	 470794 	 9.908912181854248 	 14.463618755340576 	 7.486343383789062e-05 	 0.00019407272338867188 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:45:01.690207 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([8401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 21, 9],"float32"), Tensor([8401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 89522496 	 470794 	 10.52144718170166 	 14.434452772140503 	 0.00014138221740722656 	 0.0001995563507080078 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:45:55.927559 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 2757, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 2757, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5121209664 	 470794 	 9.896381855010986 	 15.767783880233765 	 4.4345855712890625e-05 	 0.00022721290588378906 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:46:49.932466 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 2757, 9],"float32"), Tensor([6401, 32, 2757, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 32, 2757, 9],"float32"), Tensor([6401, 32, 2757, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 10164992832 	 470794 	 10.298484802246094 	 16.179574012756348 	 0.00018453598022460938 	 0.00023365020751953125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:47:48.005441 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 4201, 21, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 4201, 21, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5121036837 	 470794 	 10.017829656600952 	 16.546231269836426 	 0.00010132789611816406 	 0.0005402565002441406 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:48:49.514083 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 4201, 21, 9],"float32"), Tensor([6401, 4201, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 4201, 21, 9],"float32"), Tensor([6401, 4201, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 10164647178 	 470794 	 12.565468311309814 	 20.282288312911987 	 7.62939453125e-05 	 0.0002071857452392578 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:49:56.945603 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 2545, 39],"float32"), Tensor([6401, 8, 2545, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 2545, 39],"float32"), Tensor([6401, 8, 2545, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 10165300080 	 470794 	 27.666001796722412 	 40.318472385406494 	 0.030003786087036133 	 0.04374337196350098 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:51:33.963890 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 2545, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 2545, 39],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5254401672 	 470794 	 27.956292390823364 	 40.783676862716675 	 0.030318021774291992 	 0.04372739791870117 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:53:18.594079 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 1154],"float32"), Tensor([6401, 8, 86, 1154],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 1154],"float32"), Tensor([6401, 8, 86, 1154],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 10164173504 	 470794 	 27.7756404876709 	 40.35163903236389 	 0.030144691467285156 	 0.04374051094055176 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:54:56.904976 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 1154],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 1154],"float32"), Tensor([6401, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5253838384 	 470794 	 27.808164358139038 	 40.38512468338013 	 0.030171632766723633 	 0.04372763633728027 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:56:41.933681 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([1894, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([1894, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 222571440 	 470794 	 27.77931571006775 	 40.32551193237305 	 0.030132293701171875 	 0.043753862380981445 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-05 23:58:22.138438 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 237, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 237, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5259893730 	 470794 	 27.808736562728882 	 40.31453251838684 	 0.03016519546508789 	 0.04375004768371582 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:00:04.045936 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 8, 2545, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 8, 2545, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5254401672 	 470794 	 27.867021799087524 	 40.32204627990723 	 0.030254602432250977 	 0.04375576972961426 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:01:41.049604 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 8, 86, 1154],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 8, 86, 39],"float32"), Tensor([6401, 8, 86, 1154],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 5253838384 	 470794 	 27.434128284454346 	 40.29993414878845 	 0.029760122299194336 	 0.043706655502319336 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:03:24.296171 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 972, 43, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 972, 43, 19],"float32"), Tensor([6401, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 5166861596 	 470794 	 13.720873355865479 	 14.378680229187012 	 0.00010848045349121094 	 0.00019240379333496094 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:04:22.663715 test begin: paddle.nn.functional.max_unpool2d(Tensor([6401, 972, 43, 19],"float32"), Tensor([6401, 972, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([6401, 972, 43, 19],"float32"), Tensor([6401, 972, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 10166375448 	 470794 	 14.41880202293396 	 18.44700527191162 	 8.416175842285156e-05 	 0.00020170211791992188 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:05:30.274020 test begin: paddle.nn.functional.max_unpool2d(Tensor([8401, 32, 21, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([8401, 32, 21, 9],"float32"), Tensor([6401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 89522496 	 470794 	 14.390257835388184 	 17.315373420715332 	 0.00010085105895996094 	 0.00019693374633789062 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:06:38.634762 test begin: paddle.nn.functional.max_unpool2d(Tensor([840101, 32, 21, 9],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([840101, 32, 21, 9],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 5081317920 	 470794 	 10.199039697647095 	 16.167390823364258 	 5.984306335449219e-05 	 0.0001666545867919922 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:07:34.620347 test begin: paddle.nn.functional.max_unpool2d(Tensor([840101, 32, 21, 9],"float32"), Tensor([840101, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([840101, 32, 21, 9],"float32"), Tensor([840101, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 10161861696 	 470794 	 10.181143760681152 	 14.640411853790283 	 3.695487976074219e-05 	 0.0001976490020751953 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 00:08:29.802331 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 211681, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc9183d82b0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 00:18:34.762759 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 211681, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
W0806 00:18:42.638453 115000 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdd204cb040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 00:28:45.667943 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 211681, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0806 00:28:48.924501 115307 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7facdfe73010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 00:38:50.026713 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0806 00:38:53.286896 115712 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f10c06e3400>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 00:48:54.929440 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
W0806 00:48:59.155386 116191 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7facd72bf040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 00:58:59.647675 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0806 00:59:05.395774 116572 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6c276f3400>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 01:09:04.237163 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0806 01:09:07.262756 116873 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f51a8e1b010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 01:19:08.696853 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
W0806 01:19:13.266341 117160 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc23b4e30a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 01:29:13.262256 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([14112101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0806 01:29:13.466377 117442 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([14112101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5080356720 	 349895 	 12.632095575332642 	 12.099586248397827 	 0.00011134147644042969 	 0.0002129077911376953 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 01:30:01.754710 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([14112101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([14112101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5080356720 	 349895 	 9.96871829032898 	 11.35236144065857 	 8.606910705566406e-05 	 0.0001800060272216797 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 01:30:43.250708 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([7056101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([7056101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2540196720 	 349895 	 9.767470359802246 	 11.700706481933594 	 9.703636169433594e-05 	 0.00016736984252929688 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 01:31:24.958046 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 423361, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb094724250>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 01:41:30.736870 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 423361, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
W0806 01:41:42.931730 117852 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6942db70a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 01:51:35.393987 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0806 01:51:41.835069 118238 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131141380 	 349895 	 9.613179206848145 	 12.743389368057251 	 9.918212890625e-05 	 0.00018596649169921875 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 01:52:24.258919 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131141380 	 349895 	 9.822205543518066 	 11.559506893157959 	 4.00543212890625e-05 	 0.0002033710479736328 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 01:53:06.004859 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131141380 	 349895 	 9.80618667602539 	 11.507893323898315 	 8.845329284667969e-05 	 0.00019311904907226562 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 01:53:49.825613 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565607050 	 349895 	 10.022193431854248 	 11.547028303146362 	 8.845329284667969e-05 	 0.00020503997802734375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 01:54:31.407829 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 2565607050 	 349895 	 9.769230365753174 	 11.502346277236938 	 8.487701416015625e-05 	 0.00020384788513183594 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 01:55:12.724300 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565607050 	 349895 	 9.764912843704224 	 11.503087043762207 	 0.0001697540283203125 	 0.0001971721649169922 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 01:55:54.412391 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 7696702980 	 349895 	 9.820202350616455 	 14.169127702713013 	 4.029273986816406e-05 	 0.00022077560424804688 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 01:56:40.213410 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 282241, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 7696702980 	 349895 	 9.794203281402588 	 11.464900732040405 	 3.1948089599609375e-05 	 0.000194549560546875 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 01:57:21.545996 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131137744 	 349895 	 9.814529418945312 	 11.471220970153809 	 7.43865966796875e-05 	 0.00020551681518554688 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 01:58:02.733526 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131137744 	 349895 	 9.77930736541748 	 11.466267824172974 	 8.320808410644531e-05 	 0.00019121170043945312 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 01:58:43.906108 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131137744 	 349895 	 9.783076047897339 	 11.514768362045288 	 7.176399230957031e-05 	 0.00019240379333496094 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 01:59:25.205464 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565605232 	 349895 	 9.879883766174316 	 11.517976999282837 	 8.940696716308594e-05 	 0.00021266937255859375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:00:06.516435 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 2565605232 	 349895 	 9.755742311477661 	 11.517371416091919 	 8.0108642578125e-05 	 0.0001976490020751953 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:00:49.083933 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565605232 	 349895 	 9.741746425628662 	 11.487218618392944 	 3.9577484130859375e-05 	 0.00019931793212890625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:01:31.917333 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 7696699344 	 349895 	 9.901982307434082 	 11.524121522903442 	 0.00013113021850585938 	 0.0001971721649169922 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:02:13.330405 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 352801, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 7696699344 	 349895 	 12.85620403289795 	 14.842407703399658 	 9.202957153320312e-05 	 0.00019860267639160156 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:03:04.654362 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131135320 	 349895 	 12.97864818572998 	 14.804347515106201 	 8.678436279296875e-05 	 0.00020241737365722656 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:03:56.006991 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131135320 	 349895 	 9.807251930236816 	 11.492831230163574 	 8.559226989746094e-05 	 0.00019168853759765625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:04:37.603006 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 423361],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131135320 	 349895 	 12.914440155029297 	 12.358243703842163 	 0.00011205673217773438 	 0.00018358230590820312 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:05:26.474502 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565604020 	 349895 	 9.729242324829102 	 11.525636434555054 	 8.463859558105469e-05 	 0.0002040863037109375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:06:07.619467 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 2565604020 	 349895 	 9.794436931610107 	 11.567721843719482 	 4.363059997558594e-05 	 0.00020313262939453125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:06:49.658780 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565604020 	 349895 	 9.927129745483398 	 11.565242052078247 	 0.00012993812561035156 	 0.0001983642578125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:07:31.074406 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 7696696920 	 349895 	 9.707491397857666 	 11.458992719650269 	 4.410743713378906e-05 	 0.00018262863159179688 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:08:14.193406 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 423361],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 7696696920 	 349895 	 9.68500304222107 	 11.525864839553833 	 5.316734313964844e-05 	 0.00019788742065429688 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:08:55.486629 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 211681, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 211681, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565610080 	 349895 	 9.943403720855713 	 11.503223180770874 	 4.00543212890625e-05 	 0.0002009868621826172 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:09:38.149680 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 282241, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565607050 	 349895 	 9.785903930664062 	 11.50526237487793 	 3.981590270996094e-05 	 0.0002269744873046875 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:10:19.323756 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 352801, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565605232 	 349895 	 9.727667331695557 	 11.549970388412476 	 4.220008850097656e-05 	 0.0002002716064453125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:11:00.516450 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 423361],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 423361],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 2565604020 	 349895 	 9.677978038787842 	 11.54330325126648 	 3.981590270996094e-05 	 0.00019693374633789062 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:11:43.794159 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131165620 	 349895 	 9.952848672866821 	 11.525357484817505 	 6.580352783203125e-05 	 0.00018644332885742188 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:12:25.252823 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131165620 	 349895 	 9.67739486694336 	 11.593761205673218 	 4.482269287109375e-05 	 0.0002067089080810547 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:13:06.612640 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131166832 	 349895 	 9.664921045303345 	 11.500141143798828 	 3.981590270996094e-05 	 0.00018787384033203125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:13:49.604811 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131166832 	 349895 	 9.85251235961914 	 11.53366231918335 	 0.00012969970703125 	 0.00019931793212890625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:14:31.128832 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131168650 	 349895 	 9.688089609146118 	 11.454689264297485 	 3.528594970703125e-05 	 0.00019216537475585938 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:15:12.209201 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131168650 	 349895 	 9.619628429412842 	 11.508094787597656 	 3.7670135498046875e-05 	 0.0001811981201171875 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:15:53.212199 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 5131171680 	 349895 	 9.739184141159058 	 11.50863790512085 	 3.528594970703125e-05 	 0.00020003318786621094 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:16:34.469633 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([101, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 5131171680 	 349895 	 9.652729749679565 	 11.489073753356934 	 3.409385681152344e-05 	 0.00017881393432617188 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:17:15.513487 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50839920 	 349895 	 9.729706764221191 	 11.445923805236816 	 4.9591064453125e-05 	 0.0002048015594482422 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:17:56.679290 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 50839920 	 349895 	 9.693616151809692 	 11.406841278076172 	 4.2438507080078125e-05 	 0.00020194053649902344 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:18:38.625256 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25438320 	 349895 	 9.70476222038269 	 11.45975112915039 	 8.726119995117188e-05 	 0.00019812583923339844 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:19:20.585375 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 846721],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 846721],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 10262258520 	 349895 	 9.957112789154053 	 11.4581778049469 	 8.392333984375e-05 	 0.00019812583923339844 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:20:01.837397 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 846721],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 5, 846721],"float64"), Tensor([101, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 10262258520 	 349895 	 9.705754518508911 	 11.46802282333374 	 8.034706115722656e-05 	 0.00019168853759765625 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:20:42.926733 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 705601, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 705601, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 10262260944 	 349895 	 9.750086784362793 	 11.492206335067749 	 9.512901306152344e-05 	 0.00019860267639160156 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:21:24.717470 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 705601, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 4, 705601, 6],"float64"), Tensor([101, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 10262260944 	 349895 	 10.268267393112183 	 11.489844560623169 	 8.96453857421875e-05 	 0.00018978118896484375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:22:06.389174 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 564481, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 564481, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 10262264580 	 349895 	 9.846431493759155 	 11.48906660079956 	 9.107589721679688e-05 	 0.0001990795135498047 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:22:48.830057 test begin: paddle.nn.functional.max_unpool3d(Tensor([101, 3, 564481, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([101, 3, 564481, 5, 6],"float64"), Tensor([101, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 10262264580 	 349895 	 9.728465557098389 	 11.44658637046814 	 8.249282836914062e-05 	 0.0001919269561767578 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 02:23:29.939299 test begin: paddle.nn.functional.max_unpool3d(Tensor([141121, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f718091bf40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 02:33:35.688525 test begin: paddle.nn.functional.max_unpool3d(Tensor([141121, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
W0806 02:33:47.770344 119242 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe0d514b0a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 02:43:48.840864 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0806 02:43:51.955848 119797 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f24ca5bb460>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 02:53:53.779752 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
W0806 02:53:57.972220 120349 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f373871b040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:03:58.491820 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0806 03:04:01.578531 121059 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff987d0f220>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:14:03.015290 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0806 03:14:06.082777 121550 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7feb3ab0ae30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:24:07.785597 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
W0806 03:24:11.970376 122090 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6d493b3040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:34:12.736014 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0806 03:34:15.733426 122654 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f801c58b070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:44:17.550644 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
W0806 03:44:22.653815 123047 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f34646c3040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:54:22.071705 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
W0806 03:54:25.354763 123443 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fabad2f3010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 04:04:26.822812 test begin: paddle.nn.functional.mish(Tensor([12, 10585, 20, 20],"float32"), )
W0806 04:04:27.903260 123846 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 10585, 20, 20],"float32"), ) 	 50808000 	 32808 	 9.987632274627686 	 9.837169647216797 	 0.3111140727996826 	 0.3064572811126709 	 14.86790156364441 	 14.898353815078735 	 0.46294546127319336 	 0.46410179138183594 	 
2025-08-06 04:05:22.853112 test begin: paddle.nn.functional.mish(Tensor([12, 128, 40, 827],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 128, 40, 827],"float32"), ) 	 50810880 	 32808 	 11.075129270553589 	 9.83444619178772 	 0.31166768074035645 	 0.30632758140563965 	 14.871788740158081 	 14.9041268825531 	 0.463165283203125 	 0.46425437927246094 	 
2025-08-06 04:06:19.509658 test begin: paddle.nn.functional.mish(Tensor([12, 128, 827, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 128, 827, 40],"float32"), ) 	 50810880 	 32808 	 10.000986099243164 	 9.834186315536499 	 0.3115215301513672 	 0.3063209056854248 	 14.87654423713684 	 14.903911590576172 	 0.46346545219421387 	 0.46425795555114746 	 
2025-08-06 04:07:11.278453 test begin: paddle.nn.functional.mish(Tensor([12, 256, 40, 414],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 256, 40, 414],"float32"), ) 	 50872320 	 32808 	 10.000482559204102 	 9.874199151992798 	 0.31140923500061035 	 0.3068556785583496 	 14.889214754104614 	 14.91993498802185 	 0.4636514186859131 	 0.46464109420776367 	 
2025-08-06 04:08:04.148641 test begin: paddle.nn.functional.mish(Tensor([12, 256, 414, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 256, 414, 40],"float32"), ) 	 50872320 	 32808 	 9.998273611068726 	 9.847847700119019 	 0.31147336959838867 	 0.30678319931030273 	 14.891812086105347 	 14.919939756393433 	 0.46393656730651855 	 0.46479225158691406 	 
2025-08-06 04:08:55.500433 test begin: paddle.nn.functional.mish(Tensor([12, 2647, 40, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 2647, 40, 40],"float32"), ) 	 50822400 	 32808 	 10.024130582809448 	 9.836968421936035 	 0.31218695640563965 	 0.30638885498046875 	 14.877604246139526 	 14.905264854431152 	 0.4635450839996338 	 0.4642617702484131 	 
2025-08-06 04:09:46.853613 test begin: paddle.nn.functional.mish(Tensor([12, 512, 20, 414],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 512, 20, 414],"float32"), ) 	 50872320 	 32808 	 10.011107683181763 	 9.872982025146484 	 0.3121359348297119 	 0.3067741394042969 	 14.888757944107056 	 14.919758558273315 	 0.4638791084289551 	 0.46474313735961914 	 
2025-08-06 04:10:42.309447 test begin: paddle.nn.functional.mish(Tensor([12, 512, 414, 20],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 512, 414, 20],"float32"), ) 	 50872320 	 32808 	 10.000402688980103 	 9.856999635696411 	 0.31144237518310547 	 0.3067953586578369 	 14.890490531921387 	 14.919902324676514 	 0.4639880657196045 	 0.46472668647766113 	 
2025-08-06 04:11:34.372151 test begin: paddle.nn.functional.mish(Tensor([125, 256, 40, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([125, 256, 40, 40],"float32"), ) 	 51200000 	 32808 	 10.0693838596344 	 9.90657114982605 	 0.3134794235229492 	 0.3086111545562744 	 14.9786958694458 	 15.01538610458374 	 0.46675562858581543 	 0.4677550792694092 	 
2025-08-06 04:12:29.732859 test begin: paddle.nn.functional.mish(Tensor([249, 128, 40, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([249, 128, 40, 40],"float32"), ) 	 50995200 	 32808 	 11.671201467514038 	 9.868563175201416 	 0.3124721050262451 	 0.3074302673339844 	 14.929365873336792 	 14.955538511276245 	 0.46521782875061035 	 0.4659116268157959 	 
2025-08-06 04:13:24.077458 test begin: paddle.nn.functional.mish(Tensor([249, 512, 20, 20],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([249, 512, 20, 20],"float32"), ) 	 50995200 	 32808 	 10.030951023101807 	 9.868493556976318 	 0.31250715255737305 	 0.3073732852935791 	 14.927461624145508 	 14.957074403762817 	 0.4651510715484619 	 0.4659311771392822 	 
2025-08-06 04:14:16.337641 test begin: paddle.nn.functional.mse_loss(Tensor([24904, 12, 170, 1],"float32"), Tensor([24904, 12, 170, 1],"float32"), "mean", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([24904, 12, 170, 1],"float32"), Tensor([24904, 12, 170, 1],"float32"), "mean", ) 	 101608320 	 13409 	 11.982872247695923 	 8.023817300796509 	 0.22806119918823242 	 0.20360183715820312 	 14.211483478546143 	 15.550310373306274 	 0.36126184463500977 	 0.29619336128234863 	 
2025-08-06 04:15:07.926233 test begin: paddle.nn.functional.mse_loss(Tensor([3548, 12, 1194, 1],"float32"), Tensor([3548, 12, 1194, 1],"float32"), "mean", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([3548, 12, 1194, 1],"float32"), Tensor([3548, 12, 1194, 1],"float32"), "mean", ) 	 101671488 	 13409 	 11.987749814987183 	 8.039634466171265 	 0.22811365127563477 	 0.2038273811340332 	 14.218703269958496 	 15.560165882110596 	 0.3614177703857422 	 0.29645562171936035 	 
2025-08-06 04:16:03.939599 test begin: paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 8],"float32"), "mean", )
/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py:104: UserWarning: Using a target size (torch.Size([3548, 12, 170, 8])) that is different to the input size (torch.Size([3548, 12, 170, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return func(*args, **kwargs)
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 8],"float32"), "mean", ) 	 65141280 	 13409 	 11.588320255279541 	 7.714338541030884 	 0.22051072120666504 	 0.19557547569274902 	 20.716389894485474 	 21.67279815673828 	 0.3947749137878418 	 0.330657958984375 	 
2025-08-06 04:17:10.388890 test begin: paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 1],"float32"), "mean", )
/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py:104: UserWarning: Using a target size (torch.Size([3548, 12, 170, 1])) that is different to the input size (torch.Size([3548, 12, 170, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return func(*args, **kwargs)
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 1],"float32"), "mean", ) 	 65141280 	 13409 	 11.598000526428223 	 7.717788934707642 	 0.22075939178466797 	 0.19539499282836914 	 15.227416515350342 	 21.667141914367676 	 0.3870542049407959 	 0.3305511474609375 	 
2025-08-06 04:18:14.316320 test begin: paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 8],"float32"), "mean", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 8],"float32"), "mean", ) 	 115806720 	 13409 	 13.630118370056152 	 9.510382413864136 	 0.25928354263305664 	 0.2412729263305664 	 16.171056985855103 	 17.683229684829712 	 0.4110105037689209 	 0.3368666172027588 	 
2025-08-06 04:19:15.934455 test begin: paddle.nn.functional.mse_loss(Tensor([3548, 85, 170, 1],"float32"), Tensor([3548, 85, 170, 1],"float32"), "mean", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([3548, 85, 170, 1],"float32"), Tensor([3548, 85, 170, 1],"float32"), "mean", ) 	 102537200 	 13409 	 12.09142541885376 	 8.10155439376831 	 0.23009848594665527 	 0.2053523063659668 	 14.328940868377686 	 15.68610167503357 	 0.36419057846069336 	 0.2987849712371826 	 
2025-08-06 04:20:07.967270 test begin: paddle.nn.functional.mse_loss(Tensor([517, 4, 3, 64, 128],"float32"), Tensor([517, 4, 3, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([517, 4, 3, 64, 128],"float32"), Tensor([517, 4, 3, 64, 128],"float32"), "none", ) 	 101646336 	 13409 	 10.001430034637451 	 7.329269647598267 	 0.38109850883483887 	 0.4564383029937744 	 12.393602848052979 	 19.383771896362305 	 0.47226881980895996 	 0.36928868293762207 	 
2025-08-06 04:21:01.225397 test begin: paddle.nn.functional.mse_loss(Tensor([64, 3, 3, 64, 1379],"float32"), Tensor([64, 3, 3, 64, 1379],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 3, 3, 64, 1379],"float32"), Tensor([64, 3, 3, 64, 1379],"float32"), "none", ) 	 101670912 	 13409 	 10.004883050918579 	 5.9901368618011475 	 0.38129687309265137 	 0.45648932456970215 	 12.39681625366211 	 19.39404582977295 	 0.47246885299682617 	 0.3693234920501709 	 
2025-08-06 04:21:51.833996 test begin: paddle.nn.functional.mse_loss(Tensor([64, 3, 3, 690, 128],"float32"), Tensor([64, 3, 3, 690, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 3, 3, 690, 128],"float32"), Tensor([64, 3, 3, 690, 128],"float32"), "none", ) 	 101744640 	 13409 	 10.009904384613037 	 5.995323419570923 	 0.3814830780029297 	 0.456942081451416 	 12.404759168624878 	 19.405973434448242 	 0.47265100479125977 	 0.36980652809143066 	 
2025-08-06 04:22:42.740562 test begin: paddle.nn.functional.mse_loss(Tensor([64, 3, 33, 64, 128],"float32"), Tensor([64, 3, 33, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 3, 33, 64, 128],"float32"), Tensor([64, 3, 33, 64, 128],"float32"), "none", ) 	 103809024 	 13409 	 10.21143102645874 	 6.115407705307007 	 0.3891141414642334 	 0.46602559089660645 	 12.649389266967773 	 19.789815664291382 	 0.4820830821990967 	 0.3770153522491455 	 
2025-08-06 04:23:34.882241 test begin: paddle.nn.functional.mse_loss(Tensor([64, 33, 3, 64, 128],"float32"), Tensor([64, 33, 3, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 33, 3, 64, 128],"float32"), Tensor([64, 33, 3, 64, 128],"float32"), "none", ) 	 103809024 	 13409 	 10.230394124984741 	 6.116527557373047 	 0.38915371894836426 	 0.4661080837249756 	 12.651830673217773 	 19.789409637451172 	 0.48215484619140625 	 0.3770759105682373 	 
2025-08-06 04:24:30.295711 test begin: paddle.nn.functional.mse_loss(Tensor([64, 4, 25, 64, 128],"float32"), Tensor([64, 4, 25, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 4, 25, 64, 128],"float32"), Tensor([64, 4, 25, 64, 128],"float32"), "none", ) 	 104857600 	 13409 	 10.311940908432007 	 6.179824352264404 	 0.39301419258117676 	 0.47065281867980957 	 12.776553392410278 	 19.98450517654419 	 0.48685526847839355 	 0.38062596321105957 	 
2025-08-06 04:25:22.575069 test begin: paddle.nn.functional.mse_loss(Tensor([64, 4, 3, 517, 128],"float32"), Tensor([64, 4, 3, 517, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 4, 3, 517, 128],"float32"), Tensor([64, 4, 3, 517, 128],"float32"), "none", ) 	 101646336 	 13409 	 10.000422239303589 	 5.988238096237183 	 0.3811030387878418 	 0.4564094543457031 	 12.394841194152832 	 19.382654666900635 	 0.47226738929748535 	 0.3692789077758789 	 
2025-08-06 04:26:13.573535 test begin: paddle.nn.functional.mse_loss(Tensor([64, 4, 3, 64, 1034],"float32"), Tensor([64, 4, 3, 64, 1034],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 4, 3, 64, 1034],"float32"), Tensor([64, 4, 3, 64, 1034],"float32"), "none", ) 	 101646336 	 13409 	 10.001254320144653 	 7.15826940536499 	 0.381084680557251 	 0.4564952850341797 	 12.393692970275879 	 19.383307218551636 	 0.4722440242767334 	 0.36927270889282227 	 
2025-08-06 04:27:06.913971 test begin: paddle.nn.functional.mse_loss(Tensor([690, 3, 3, 64, 128],"float32"), Tensor([690, 3, 3, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([690, 3, 3, 64, 128],"float32"), Tensor([690, 3, 3, 64, 128],"float32"), "none", ) 	 101744640 	 13409 	 10.009675025939941 	 6.025089740753174 	 0.3814527988433838 	 0.4568662643432617 	 12.39481782913208 	 19.407270669937134 	 0.4724576473236084 	 0.3696870803833008 	 
2025-08-06 04:28:00.072651 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), reduction="mean", weight=None, )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), reduction="mean", weight=None, ) 	 50803210 	 2957 	 9.97294545173645 	 9.825817584991455 	 0.3132660388946533 	 0.2825191020965576 	 13.690895557403564 	 12.716602563858032 	 0.3650243282318115 	 0.3386693000793457 	 
2025-08-06 04:28:47.959973 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), reduction="mean", weight=Tensor([5, 5080321],"float64"), )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), reduction="mean", weight=Tensor([5, 5080321],"float64"), ) 	 76204815 	 2957 	 11.281484127044678 	 11.133785247802734 	 0.3243739604949951 	 0.29548001289367676 	 15.89670991897583 	 14.969891548156738 	 0.39243459701538086 	 0.3451511859893799 	 
2025-08-06 04:29:44.215339 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), weight=Tensor([5, 5080321],"float64"), reduction="mean", name=None, )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), weight=Tensor([5, 5080321],"float64"), reduction="mean", name=None, ) 	 76204815 	 2957 	 11.291332960128784 	 11.13224172592163 	 0.3244483470916748 	 0.29554152488708496 	 15.899250745773315 	 14.969073057174683 	 0.3924293518066406 	 0.345125675201416 	 
2025-08-06 04:30:42.898898 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), reduction="mean", weight=None, )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), reduction="mean", weight=None, ) 	 50803210 	 2957 	 10.031558990478516 	 10.305588245391846 	 0.31478238105773926 	 0.29628419876098633 	 13.940486431121826 	 13.17729902267456 	 0.37154459953308105 	 0.35079383850097656 	 
2025-08-06 04:31:32.215354 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), reduction="mean", weight=Tensor([5080321, 5],"float64"), )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), reduction="mean", weight=Tensor([5080321, 5],"float64"), ) 	 76204815 	 2957 	 11.345016956329346 	 11.61879825592041 	 0.326157808303833 	 0.30848002433776855 	 16.15404987335205 	 15.407825946807861 	 0.3986642360687256 	 0.35515761375427246 	 
2025-08-06 04:32:28.482991 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), weight=Tensor([5080321, 5],"float64"), reduction="mean", name=None, )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), weight=Tensor([5080321, 5],"float64"), reduction="mean", name=None, ) 	 76204815 	 2957 	 11.343463659286499 	 11.629574298858643 	 0.32608485221862793 	 0.3087649345397949 	 16.152275323867798 	 15.408015489578247 	 0.3987734317779541 	 0.35518908500671387 	 
2025-08-06 04:33:26.017133 test begin: paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, )
W0806 04:33:26.666929 124573 dygraph_functions.cc:93089] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, ) 	 38102403 	 7942 	 30.695178031921387 	 128.8850338459015 	 0.00019788742065429688 	 5.518249273300171 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:36:41.012068 test begin: paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, ) 	 38102403 	 7942 	 30.0821590423584 	 128.2741837501526 	 0.00012755393981933594 	 16.5078763961792 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:40:00.408588 test begin: paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, ) 	 38102403 	 7942 	 30.763673067092896 	 128.90847635269165 	 0.00019407272338867188 	 5.518855094909668 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:43:18.998168 test begin: paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, ) 	 76204803 	 7942 	 59.75959539413452 	 257.7075319290161 	 0.00041484832763671875 	 11.031760454177856 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:49:44.994118 test begin: paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, ) 	 76204803 	 7942 	 58.616517543792725 	 256.49518060684204 	 0.0002758502960205078 	 33.002321004867554 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:56:08.275640 test begin: paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, ) 	 76204803 	 7942 	 59.75967264175415 	 257.7076849937439 	 0.00041222572326660156 	 11.032204389572144 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 05:02:32.100743 test begin: paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, ) 	 25401610 	 7942 	 10.147609949111938 	 47.86128044128418 	 4.553794860839844e-05 	 3.0798299312591553 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 05:03:49.228370 test begin: paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, ) 	 25401610 	 7942 	 10.249999046325684 	 47.80812954902649 	 6.127357482910156e-05 	 6.154060363769531 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 05:05:06.627427 test begin: paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, ) 	 25401610 	 7942 	 10.177324056625366 	 47.84390068054199 	 1.9311904907226562e-05 	 3.0792641639709473 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 05:06:24.652769 test begin: paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="mean", name=None, )
[Prof] paddle.nn.functional.nll_loss 	 paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="mean", name=None, ) 	 25443143 	 366849 	 10.074708700180054 	 13.754848718643188 	 4.9114227294921875e-05 	 0.00010371208190917969 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 05:07:40.851679 test begin: paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="none", name=None, )
[Prof] paddle.nn.functional.nll_loss 	 paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="none", name=None, ) 	 25443143 	 366849 	 10.361796140670776 	 8.885438919067383 	 4.482269287109375e-05 	 0.00010061264038085938 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 05:08:52.980818 test begin: paddle.nn.functional.normalize(Tensor([2009, 25288],"float32"), )
[Prof] paddle.nn.functional.normalize 	 paddle.nn.functional.normalize(Tensor([2009, 25288],"float32"), ) 	 50803592 	 21309 	 10.098862648010254 	 9.987682104110718 	 0.0969395637512207 	 0.15965509414672852 	 57.169254541397095 	 68.21556997299194 	 0.5492558479309082 	 0.2336723804473877 	 
2025-08-06 05:11:23.000037 test begin: paddle.nn.functional.normalize(Tensor([2081, 24413],"float32"), )
[Prof] paddle.nn.functional.normalize 	 paddle.nn.functional.normalize(Tensor([2081, 24413],"float32"), ) 	 50803453 	 21309 	 10.11882734298706 	 10.012243509292603 	 0.09719538688659668 	 0.1600627899169922 	 57.14076042175293 	 68.2515640258789 	 0.548973798751831 	 0.23379182815551758 	 
2025-08-06 05:13:50.436502 test begin: paddle.nn.functional.normalize(Tensor([2331, 21795],"float32"), )
[Prof] paddle.nn.functional.normalize 	 paddle.nn.functional.normalize(Tensor([2331, 21795],"float32"), ) 	 50804145 	 21309 	 10.034400701522827 	 10.011587858200073 	 0.09639191627502441 	 0.16009306907653809 	 57.3350784778595 	 68.23228812217712 	 0.5508098602294922 	 0.23376870155334473 	 
2025-08-06 05:16:17.744306 test begin: paddle.nn.functional.normalize(Tensor([99226, 512],"float32"), )
[Prof] paddle.nn.functional.normalize 	 paddle.nn.functional.normalize(Tensor([99226, 512],"float32"), ) 	 50803712 	 21309 	 10.397674083709717 	 9.947805404663086 	 0.0998072624206543 	 0.1590576171875 	 57.690982818603516 	 68.45456409454346 	 0.5542964935302734 	 0.23449254035949707 	 
2025-08-06 05:18:45.929243 test begin: paddle.nn.functional.npair_loss(Tensor([18, 2822401],"float32"), positive=Tensor([18, 2822401],"float32"), labels=Tensor([18],"float32"), l2_reg=0.002, )
[Prof] paddle.nn.functional.npair_loss 	 paddle.nn.functional.npair_loss(Tensor([18, 2822401],"float32"), positive=Tensor([18, 2822401],"float32"), labels=Tensor([18],"float32"), l2_reg=0.002, ) 	 101606454 	 6743 	 9.912179708480835 	 9.532991647720337 	 0.06272482872009277 	 0.07203245162963867 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 05:19:27.630232 test begin: paddle.nn.functional.pad(Tensor([7573, 11, 1280],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([7573, 11, 1280],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, ) 	 106627840 	 8486 	 10.47152853012085 	 3.9744646549224854 	 1.2610063552856445 	 0.1595473289489746 	 8.364634990692139 	 6.807290554046631 	 1.0073387622833252 	 0.2730696201324463 	 
2025-08-06 05:20:00.789650 test begin: paddle.nn.functional.pad(Tensor([7573, 8, 1678],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([7573, 8, 1678],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, ) 	 101659952 	 8486 	 9.990333080291748 	 3.7957587242126465 	 1.2031655311584473 	 0.152374267578125 	 7.977313756942749 	 6.494319438934326 	 0.9607524871826172 	 0.26050710678100586 	 
2025-08-06 05:20:32.418438 test begin: paddle.nn.functional.pad(Tensor([7710, 11, 1280],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([7710, 11, 1280],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, ) 	 108556800 	 8486 	 10.658664226531982 	 4.046065330505371 	 1.283297061920166 	 0.16239047050476074 	 8.52094841003418 	 6.9253761768341064 	 1.0262055397033691 	 0.2777729034423828 	 
2025-08-06 05:21:07.381977 test begin: paddle.nn.functional.pad(Tensor([7710, 8, 1648],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([7710, 8, 1648],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, ) 	 101648640 	 8486 	 10.009485006332397 	 3.79470157623291 	 1.2033851146697998 	 0.1523888111114502 	 7.978553533554077 	 6.490422487258911 	 0.9609415531158447 	 0.2604560852050781 	 
2025-08-06 05:21:40.382367 test begin: paddle.nn.functional.pad(Tensor([8162, 10, 1280],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([8162, 10, 1280],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, ) 	 104473600 	 8486 	 10.279319763183594 	 3.897620439529419 	 1.2372956275939941 	 0.15648841857910156 	 8.197307348251343 	 6.675065040588379 	 0.9872400760650635 	 0.26781272888183594 	 
2025-08-06 05:22:12.950802 test begin: paddle.nn.functional.pad(Tensor([8162, 8, 1557],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([8162, 8, 1557],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, ) 	 101665872 	 8486 	 9.994766473770142 	 3.8016197681427 	 1.2036831378936768 	 0.15246248245239258 	 7.981950521469116 	 6.495925426483154 	 0.9613490104675293 	 0.2605857849121094 	 
2025-08-06 05:22:45.930833 test begin: paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, ) 	 101611520 	 8486 	 9.985179424285889 	 3.771268129348755 	 1.2026000022888184 	 0.22708392143249512 	 7.974757671356201 	 6.465460777282715 	 0.9603872299194336 	 0.3893239498138428 	 
2025-08-06 05:23:17.657880 test begin: paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, ) 	 101611520 	 8486 	 9.985493898391724 	 3.7752723693847656 	 1.2025718688964844 	 0.22708940505981445 	 7.974696159362793 	 6.46592116355896 	 0.9603860378265381 	 0.3893272876739502 	 
2025-08-06 05:23:49.899293 test begin: paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, ) 	 101611520 	 8486 	 9.98552656173706 	 3.7720947265625 	 1.2026386260986328 	 0.22719550132751465 	 7.974776268005371 	 6.467226982116699 	 0.9604582786560059 	 0.38942837715148926 	 
2025-08-06 05:24:21.717918 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 508033],"float32"), Tensor([100, 508033],"float32"), -1, 1e-06, False, None, )
[Prof] paddle.nn.functional.pairwise_distance 	 paddle.nn.functional.pairwise_distance(Tensor([100, 508033],"float32"), Tensor([100, 508033],"float32"), -1, 1e-06, False, None, ) 	 101606600 	 8769 	 9.993694305419922 	 9.320244312286377 	 0.19373440742492676 	 0.2713348865509033 	 16.599417209625244 	 24.333449840545654 	 0.9672951698303223 	 0.28362250328063965 	 
2025-08-06 05:25:23.737077 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 508033],"float32"), Tensor([100, 508033],"float32"), -1, 1e-06, True, None, )
[Prof] paddle.nn.functional.pairwise_distance 	 paddle.nn.functional.pairwise_distance(Tensor([100, 508033],"float32"), Tensor([100, 508033],"float32"), -1, 1e-06, True, None, ) 	 101606600 	 8769 	 9.99595594406128 	 9.316521406173706 	 0.1937716007232666 	 0.2712893486022949 	 16.61179804801941 	 24.331475019454956 	 0.968008279800415 	 0.2836310863494873 	 
2025-08-06 05:26:25.761563 test begin: paddle.nn.functional.pairwise_distance(Tensor([508033, 100],"float32"), Tensor([508033, 100],"float32"), -1, 1e-06, False, None, )
[Prof] paddle.nn.functional.pairwise_distance 	 paddle.nn.functional.pairwise_distance(Tensor([508033, 100],"float32"), Tensor([508033, 100],"float32"), -1, 1e-06, False, None, ) 	 101606600 	 8769 	 13.363098859786987 	 14.413172483444214 	 0.31163859367370605 	 0.5600316524505615 	 17.342355251312256 	 24.430203914642334 	 1.0106732845306396 	 0.2847273349761963 	 
2025-08-06 05:27:37.718093 test begin: paddle.nn.functional.pairwise_distance(Tensor([508033, 100],"float32"), Tensor([508033, 100],"float32"), -1, 1e-06, True, None, )
[Prof] paddle.nn.functional.pairwise_distance 	 paddle.nn.functional.pairwise_distance(Tensor([508033, 100],"float32"), Tensor([508033, 100],"float32"), -1, 1e-06, True, None, ) 	 101606600 	 8769 	 13.36598825454712 	 14.412447452545166 	 0.311659574508667 	 0.5600314140319824 	 17.34158444404602 	 24.430716037750244 	 1.0105280876159668 	 0.2847416400909424 	 
2025-08-06 05:28:49.771041 test begin: paddle.nn.functional.pixel_shuffle(Tensor([13, 256, 128, 128],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([13, 256, 128, 128],"float32"), 2, "NCHW", None, ) 	 54525952 	 26616 	 10.805596351623535 	 9.35230302810669 	 0.41492533683776855 	 0.3591470718383789 	 10.485190629959106 	 9.124659299850464 	 0.40258264541625977 	 0.3503091335296631 	 
2025-08-06 05:29:31.456602 test begin: paddle.nn.functional.pixel_shuffle(Tensor([4, 256, 128, 388],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([4, 256, 128, 388],"float32"), 2, "NCHW", None, ) 	 50855936 	 26616 	 10.005025386810303 	 8.86825442314148 	 0.3841392993927002 	 0.3402717113494873 	 10.704356670379639 	 8.645658731460571 	 0.4109630584716797 	 0.33199071884155273 	 
2025-08-06 05:30:11.403707 test begin: paddle.nn.functional.pixel_shuffle(Tensor([4, 256, 388, 128],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([4, 256, 388, 128],"float32"), 2, "NCHW", None, ) 	 50855936 	 26616 	 10.232454061508179 	 8.897297382354736 	 0.3927590847015381 	 0.34096360206604004 	 10.629372358322144 	 8.57557487487793 	 0.40808582305908203 	 0.32935237884521484 	 
2025-08-06 05:30:51.448282 test begin: paddle.nn.functional.pixel_shuffle(Tensor([4, 776, 128, 128],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([4, 776, 128, 128],"float32"), 2, "NCHW", None, ) 	 50855936 	 26616 	 10.06334137916565 	 8.732057809829712 	 0.3864171504974365 	 0.3350815773010254 	 9.784142971038818 	 8.521514654159546 	 0.3757333755493164 	 0.32721614837646484 	 
2025-08-06 05:31:30.974431 test begin: paddle.nn.functional.pixel_shuffle(Tensor([49, 256, 64, 64],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([49, 256, 64, 64],"float32"), 2, "NCHW", None, ) 	 51380224 	 26616 	 10.301326990127563 	 8.7705557346344 	 0.39560961723327637 	 0.336822509765625 	 9.740459203720093 	 8.718480587005615 	 0.3741180896759033 	 0.33478808403015137 	 
2025-08-06 05:32:10.302635 test begin: paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 128, 25],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 128, 25],"float32"), 2, "NCHW", None, ) 	 52428800 	 26616 	 10.260411977767944 	 9.047602415084839 	 0.3938460350036621 	 0.34745097160339355 	 10.395618677139282 	 9.099729537963867 	 0.39928627014160156 	 0.3493962287902832 	 
2025-08-06 05:32:50.850726 test begin: paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 25, 128],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 25, 128],"float32"), 2, "NCHW", None, ) 	 52428800 	 26616 	 10.476352214813232 	 9.007460832595825 	 0.4023163318634033 	 0.34583425521850586 	 10.098382234573364 	 8.571797609329224 	 0.3877131938934326 	 0.3292112350463867 	 
2025-08-06 05:33:30.845610 test begin: paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 49, 64],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 49, 64],"float32"), 2, "NCHW", None, ) 	 51380224 	 26616 	 10.295042753219604 	 8.947076797485352 	 0.39535999298095703 	 0.34356141090393066 	 9.777123212814331 	 8.479239463806152 	 0.37543416023254395 	 0.3256034851074219 	 
2025-08-06 05:34:10.068254 test begin: paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 64, 49],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 64, 49],"float32"), 2, "NCHW", None, ) 	 51380224 	 26616 	 10.095308780670166 	 8.798374652862549 	 0.38761091232299805 	 0.33780527114868164 	 10.043787479400635 	 8.865083932876587 	 0.38570094108581543 	 0.3404216766357422 	 
2025-08-06 05:34:50.104860 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([176401, 1, 12, 12],"float64"), 3, "NCHW", )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([176401, 1, 12, 12],"float64"), 3, "NCHW", ) 	 25401744 	 31500 	 9.981658458709717 	 9.503378868103027 	 0.3238224983215332 	 0.3083622455596924 	 9.964733123779297 	 9.514012575149536 	 0.3232614994049072 	 0.3086872100830078 	 
2025-08-06 05:35:30.220499 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([176401, 1, 12, 12],"float64"), 3, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([176401, 1, 12, 12],"float64"), 3, "NCHW", None, ) 	 25401744 	 31500 	 9.982020616531372 	 9.505423784255981 	 0.32387542724609375 	 0.3083822727203369 	 9.965246438980103 	 9.514077425003052 	 0.3233609199523926 	 0.3087174892425537 	 
2025-08-06 05:36:11.205644 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([2, 176401, 12, 12],"float32"), 3, "NCHW", )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([2, 176401, 12, 12],"float32"), 3, "NCHW", ) 	 50803488 	 31500 	 11.559166431427002 	 11.054672241210938 	 0.3750476837158203 	 0.33371520042419434 	 11.36549973487854 	 10.171620607376099 	 0.3686683177947998 	 0.3300285339355469 	 
2025-08-06 05:36:58.269019 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([2, 88201, 12, 12],"float64"), 3, "NCHW", )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([2, 88201, 12, 12],"float64"), 3, "NCHW", ) 	 25401888 	 31500 	 9.980444431304932 	 9.503244400024414 	 0.323899507522583 	 0.30838918685913086 	 9.965321779251099 	 9.513951063156128 	 0.32325172424316406 	 0.3086426258087158 	 
2025-08-06 05:37:39.986743 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([2, 88201, 12, 12],"float64"), 3, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([2, 88201, 12, 12],"float64"), 3, "NCHW", None, ) 	 25401888 	 31500 	 9.982393980026245 	 9.50430941581726 	 0.32384300231933594 	 0.308408260345459 	 9.965071678161621 	 9.513803482055664 	 0.3233156204223633 	 0.30867671966552734 	 
2025-08-06 05:38:20.107327 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([352801, 1, 12, 12],"float32"), 3, "NCHW", )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([352801, 1, 12, 12],"float32"), 3, "NCHW", ) 	 50803344 	 31500 	 11.541618347167969 	 10.28508734703064 	 0.37459564208984375 	 0.3337218761444092 	 11.36542272567749 	 10.17190146446228 	 0.36870622634887695 	 0.32996273040771484 	 
2025-08-06 05:39:05.144820 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([16934401, 3, 2],"float32"), Tensor([16934401, 3, 2],"bfloat16"), )
W0806 05:39:08.130467 126082 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([16934401, 3, 2],"float32"), Tensor([16934401, 3, 2],"bfloat16"), ) 	 203212812 	 6305 	 19.855010986328125 	 16.347532510757446 	 0.5356204509735107 	 0.5292582511901855 	 35.603776931762695 	 30.32800269126892 	 0.9614126682281494 	 0.7022278308868408 	 
2025-08-06 05:40:50.699741 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([16934401, 3, 2],"float32"), Tensor([16934401, 3, 2],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([16934401, 3, 2],"float32"), Tensor([16934401, 3, 2],"float16"), ) 	 203212812 	 6305 	 19.862013339996338 	 16.318856954574585 	 0.5356686115264893 	 0.528275728225708 	 35.58791446685791 	 30.305071592330933 	 0.960686206817627 	 0.7017643451690674 	 
2025-08-06 05:42:38.470818 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 12700801, 2],"float32"), Tensor([4, 12700801, 2],"bfloat16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 12700801, 2],"float32"), Tensor([4, 12700801, 2],"bfloat16"), ) 	 203212816 	 6305 	 19.855551958084106 	 16.344447374343872 	 0.5355865955352783 	 0.5290794372558594 	 35.59970164299011 	 30.3251633644104 	 0.9612751007080078 	 0.7021951675415039 	 
2025-08-06 05:44:25.191323 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 12700801, 2],"float32"), Tensor([4, 12700801, 2],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 12700801, 2],"float32"), Tensor([4, 12700801, 2],"float16"), ) 	 203212816 	 6305 	 19.854872703552246 	 16.3193621635437 	 0.5356998443603516 	 0.5283572673797607 	 35.58679533004761 	 30.306695699691772 	 0.9608371257781982 	 0.7017464637756348 	 
2025-08-06 05:46:11.548485 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 2116801],"float32"), Tensor([4, 3, 2116801],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 2116801],"float32"), Tensor([4, 3, 2116801],"float64"), ) 	 50803224 	 6305 	 10.516006231307983 	 6.742628812789917 	 0.2434701919555664 	 0.2180938720703125 	 14.329253196716309 	 13.68324327468872 	 0.3318510055541992 	 0.2773268222808838 	 
2025-08-06 05:46:58.724873 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"bfloat16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"bfloat16"), ) 	 101606424 	 6305 	 10.001128911972046 	 8.27309513092041 	 0.26976728439331055 	 0.26783251762390137 	 17.8889057636261 	 15.259359359741211 	 0.48305678367614746 	 0.35330772399902344 	 
2025-08-06 05:47:53.756182 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float16"), ) 	 101606424 	 6305 	 9.999670267105103 	 8.269508600234985 	 0.2698495388031006 	 0.26772356033325195 	 17.899840831756592 	 15.248274326324463 	 0.483356237411499 	 0.35309600830078125 	 
2025-08-06 05:48:47.735759 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float64"), ) 	 101606424 	 6305 	 20.851959228515625 	 13.25710153579712 	 0.48288464546203613 	 0.42903685569763184 	 28.473277807235718 	 27.10214328765869 	 0.6592745780944824 	 0.5492911338806152 	 
2025-08-06 05:50:24.637657 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 8467201],"float32"), Tensor([4, 3, 8467201],"bfloat16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 8467201],"float32"), Tensor([4, 3, 8467201],"bfloat16"), ) 	 203212824 	 6305 	 19.853987455368042 	 16.3446102142334 	 0.5356130599975586 	 0.5291941165924072 	 35.584633350372314 	 30.327356338500977 	 0.9606881141662598 	 0.7021918296813965 	 
2025-08-06 05:52:10.855348 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 8467201],"float32"), Tensor([4, 3, 8467201],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 8467201],"float32"), Tensor([4, 3, 8467201],"float16"), ) 	 203212824 	 6305 	 19.852547883987427 	 16.31693935394287 	 0.535649299621582 	 0.5282912254333496 	 35.58776068687439 	 30.306119441986084 	 0.960723876953125 	 0.7017683982849121 	 
2025-08-06 05:53:57.306374 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3175201, 2],"float32"), Tensor([4, 3175201, 2],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3175201, 2],"float32"), Tensor([4, 3175201, 2],"float64"), ) 	 50803216 	 6305 	 10.516014337539673 	 6.738550186157227 	 0.2434699535369873 	 0.21819114685058594 	 14.323955774307251 	 13.683774948120117 	 0.33165502548217773 	 0.27742433547973633 	 
2025-08-06 05:54:43.630452 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"bfloat16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"bfloat16"), ) 	 101606416 	 6305 	 10.007673501968384 	 8.284436702728271 	 0.26979660987854004 	 0.2682006359100342 	 17.88965106010437 	 15.259060382843018 	 0.4832007884979248 	 0.35332202911376953 	 
2025-08-06 05:55:38.603828 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float16"), ) 	 101606416 	 6305 	 10.005635738372803 	 8.261780977249146 	 0.2698359489440918 	 0.26746630668640137 	 17.889479398727417 	 15.248099565505981 	 0.4829721450805664 	 0.3530769348144531 	 
2025-08-06 05:56:33.401645 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float64"), ) 	 101606416 	 6305 	 20.854804515838623 	 13.255927562713623 	 0.4829139709472656 	 0.4292278289794922 	 28.45650053024292 	 27.102186679840088 	 0.658940315246582 	 0.5493712425231934 	 
2025-08-06 05:58:05.778272 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4233601, 3, 2],"float32"), Tensor([4233601, 3, 2],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4233601, 3, 2],"float32"), Tensor([4233601, 3, 2],"float64"), ) 	 50803212 	 6305 	 10.514601469039917 	 6.744609117507935 	 0.24351024627685547 	 0.21836185455322266 	 14.323546409606934 	 13.682871103286743 	 0.3316633701324463 	 0.27730631828308105 	 
2025-08-06 05:58:52.494648 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"bfloat16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"bfloat16"), ) 	 101606412 	 6305 	 10.004652976989746 	 8.275234937667847 	 0.26985621452331543 	 0.26785731315612793 	 17.88711977005005 	 15.260797500610352 	 0.4828815460205078 	 0.35340166091918945 	 
2025-08-06 05:59:46.921539 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float16"), ) 	 101606412 	 6305 	 10.001206159591675 	 8.263569116592407 	 0.2698326110839844 	 0.26754331588745117 	 17.889732122421265 	 15.248566150665283 	 0.48277854919433594 	 0.3531033992767334 	 
2025-08-06 06:00:41.772272 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float64"), ) 	 101606412 	 6305 	 20.857067108154297 	 13.26145625114441 	 0.4828939437866211 	 0.4289867877960205 	 28.45547842979431 	 27.104060888290405 	 0.6590361595153809 	 0.5493626594543457 	 
2025-08-06 06:02:19.375919 test begin: paddle.nn.functional.prelu(Tensor([104, 128, 56, 69],"float32"), Tensor([128],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([104, 128, 56, 69],"float32"), Tensor([128],"float32"), data_format="NCHW", ) 	 51437696 	 33188 	 10.101588487625122 	 10.366717100143433 	 0.31104087829589844 	 0.3192429542541504 	 36.58030915260315 	 26.437201976776123 	 0.3752408027648926 	 0.27102112770080566 	 
2025-08-06 06:03:44.644066 test begin: paddle.nn.functional.prelu(Tensor([104, 128, 69, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([104, 128, 69, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", ) 	 51437696 	 33188 	 10.098815441131592 	 10.364824771881104 	 0.31099462509155273 	 0.31917524337768555 	 36.50433874130249 	 26.41387128829956 	 0.3742103576660156 	 0.27085208892822266 	 
2025-08-06 06:05:10.650309 test begin: paddle.nn.functional.prelu(Tensor([104, 156, 56, 56],"float32"), Tensor([156],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([104, 156, 56, 56],"float32"), Tensor([156],"float32"), data_format="NCHW", ) 	 50878620 	 33188 	 9.991714477539062 	 10.252427101135254 	 0.3076958656311035 	 0.31574225425720215 	 35.91969895362854 	 26.331493616104126 	 0.3682088851928711 	 0.2698860168457031 	 
2025-08-06 06:06:39.620831 test begin: paddle.nn.functional.prelu(Tensor([127, 128, 56, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([127, 128, 56, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", ) 	 50978944 	 33188 	 10.5576810836792 	 10.272634029388428 	 0.3082702159881592 	 0.3163464069366455 	 36.112449645996094 	 26.140055179595947 	 0.3702051639556885 	 0.2679920196533203 	 
2025-08-06 06:08:06.330505 test begin: paddle.nn.functional.prelu(Tensor([128, 128, 56, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([128, 128, 56, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", ) 	 51380352 	 33188 	 10.088440656661987 	 10.370279550552368 	 0.31063079833984375 	 0.3188331127166748 	 36.30107092857361 	 26.347306966781616 	 0.3721616268157959 	 0.2700660228729248 	 
2025-08-06 06:09:32.353580 test begin: paddle.nn.functional.prelu(Tensor([128, 256, 28, 56],"float32"), Tensor([256],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([128, 256, 28, 56],"float32"), Tensor([256],"float32"), data_format="NCHW", ) 	 51380480 	 33188 	 10.08564019203186 	 10.35025954246521 	 0.3106215000152588 	 0.3187379837036133 	 36.27152109146118 	 26.34466052055359 	 0.37172532081604004 	 0.2700841426849365 	 
2025-08-06 06:10:57.156601 test begin: paddle.nn.functional.prelu(Tensor([128, 256, 56, 28],"float32"), Tensor([256],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([128, 256, 56, 28],"float32"), Tensor([256],"float32"), data_format="NCHW", ) 	 51380480 	 33188 	 10.08594274520874 	 10.354311466217041 	 0.3105504512786865 	 0.3188283443450928 	 36.271804094314575 	 26.36465287208557 	 0.37197232246398926 	 0.27036595344543457 	 
2025-08-06 06:12:23.315599 test begin: paddle.nn.functional.prelu(Tensor([128, 507, 28, 28],"float32"), Tensor([507],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([128, 507, 28, 28],"float32"), Tensor([507],"float32"), data_format="NCHW", ) 	 50878971 	 33188 	 9.993898630142212 	 10.253997087478638 	 0.30774593353271484 	 0.31575846672058105 	 35.65784430503845 	 25.897382020950317 	 0.3655548095703125 	 0.39859509468078613 	 
2025-08-06 06:13:46.798885 test begin: paddle.nn.functional.prelu(Tensor([254, 256, 28, 28],"float32"), Tensor([256],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([254, 256, 28, 28],"float32"), Tensor([256],"float32"), data_format="NCHW", ) 	 50979072 	 33188 	 10.012211322784424 	 10.282933950424194 	 0.30831193923950195 	 0.31635355949401855 	 36.12962198257446 	 26.19302010536194 	 0.37046170234680176 	 0.2684919834136963 	 
2025-08-06 06:15:11.108277 test begin: paddle.nn.functional.relu(Tensor([10, 128, 480, 83],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([10, 128, 480, 83],"float32"), None, ) 	 50995200 	 33682 	 9.991726636886597 	 10.075169324874878 	 0.3031761646270752 	 0.30520057678222656 	 15.217745542526245 	 15.093365669250488 	 0.46172404289245605 	 0.457927942276001 	 
2025-08-06 06:16:04.228410 test begin: paddle.nn.functional.relu(Tensor([10, 128, 83, 480],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([10, 128, 83, 480],"float32"), None, ) 	 50995200 	 33682 	 9.9919273853302 	 10.05909252166748 	 0.30324792861938477 	 0.30522632598876953 	 15.218099355697632 	 15.093361139297485 	 0.46170616149902344 	 0.4579794406890869 	 
2025-08-06 06:16:56.661396 test begin: paddle.nn.functional.relu(Tensor([10, 23, 480, 480],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([10, 23, 480, 480],"float32"), None, ) 	 52992000 	 33682 	 10.962748289108276 	 10.450471878051758 	 0.3148183822631836 	 0.3171253204345703 	 15.806915998458862 	 15.679482221603394 	 0.47957563400268555 	 0.4756758213043213 	 
2025-08-06 06:17:52.589726 test begin: paddle.nn.functional.relu(Tensor([2, 128, 480, 480],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([2, 128, 480, 480],"float32"), None, ) 	 58982400 	 33682 	 11.53457236289978 	 11.607531070709229 	 0.35004210472106934 	 0.35222935676574707 	 17.57759737968445 	 17.43741512298584 	 0.5333003997802734 	 0.5290951728820801 	 
2025-08-06 06:18:52.698924 test begin: paddle.nn.functional.relu(Tensor([2, 256, 352, 352],"float32"), )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([2, 256, 352, 352],"float32"), ) 	 63438848 	 33682 	 12.404776334762573 	 12.477274417877197 	 0.3764212131500244 	 0.378525972366333 	 18.889981031417847 	 18.73915195465088 	 0.5732874870300293 	 0.5685880184173584 	 
2025-08-06 06:19:57.415741 test begin: paddle.nn.functional.relu(Tensor([64, 64, 112, 112],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([64, 64, 112, 112],"float32"), None, ) 	 51380224 	 33682 	 10.067775011062622 	 10.135254621505737 	 0.30550241470336914 	 0.3074958324432373 	 15.329476118087769 	 15.205405235290527 	 0.46512746810913086 	 0.4613780975341797 	 
2025-08-06 06:20:50.714881 test begin: paddle.nn.functional.relu(Tensor([640, 64, 112, 12],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([640, 64, 112, 12],"float32"), None, ) 	 55050240 	 33682 	 10.779324054718018 	 10.844971895217896 	 0.32691526412963867 	 0.3290550708770752 	 16.415759325027466 	 16.282341480255127 	 0.49811220169067383 	 0.49402713775634766 	 
2025-08-06 06:21:46.956285 test begin: paddle.nn.functional.relu(Tensor([640, 64, 12, 112],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([640, 64, 12, 112],"float32"), None, ) 	 55050240 	 33682 	 10.773065328598022 	 10.845318794250488 	 0.326890230178833 	 0.3291137218475342 	 16.415635347366333 	 16.28353714942932 	 0.49813199043273926 	 0.4940483570098877 	 
2025-08-06 06:22:43.405635 test begin: paddle.nn.functional.relu(Tensor([640, 7, 112, 112],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([640, 7, 112, 112],"float32"), None, ) 	 56197120 	 33682 	 11.621675968170166 	 11.073267459869385 	 0.3335855007171631 	 0.335798978805542 	 16.752959966659546 	 16.622186422348022 	 0.5083138942718506 	 0.5043649673461914 	 
2025-08-06 06:23:44.403191 test begin: paddle.nn.functional.relu(Tensor([8, 256, 352, 71],"float32"), )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([8, 256, 352, 71],"float32"), ) 	 51183616 	 33682 	 10.03305721282959 	 10.096627712249756 	 0.304323673248291 	 0.30645322799682617 	 15.271089553833008 	 15.147894144058228 	 0.46332216262817383 	 0.4596443176269531 	 
2025-08-06 06:24:40.001519 test begin: paddle.nn.functional.relu(Tensor([8, 256, 71, 352],"float32"), )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([8, 256, 71, 352],"float32"), ) 	 51183616 	 33682 	 11.397723197937012 	 10.102181196212769 	 0.3042740821838379 	 0.30634140968322754 	 15.270737409591675 	 15.14787244796753 	 0.46335792541503906 	 0.459611177444458 	 
2025-08-06 06:25:35.237765 test begin: paddle.nn.functional.relu(Tensor([8, 52, 352, 352],"float32"), )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([8, 52, 352, 352],"float32"), ) 	 51544064 	 33682 	 11.273924112319946 	 10.167228937149048 	 0.3064382076263428 	 0.3084731101989746 	 15.376633167266846 	 15.251221895217896 	 0.46655750274658203 	 0.462754487991333 	 
2025-08-06 06:26:30.014597 test begin: paddle.nn.functional.relu6(Tensor([128, 144, 112, 25],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([128, 144, 112, 25],"float32"), ) 	 51609600 	 33340 	 10.007732391357422 	 10.077589988708496 	 0.3068270683288574 	 0.3089406490325928 	 15.237984657287598 	 15.119385004043579 	 0.46710848808288574 	 0.4633903503417969 	 
2025-08-06 06:27:24.200401 test begin: paddle.nn.functional.relu6(Tensor([128, 144, 25, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([128, 144, 25, 112],"float32"), ) 	 51609600 	 33340 	 11.207574605941772 	 10.077516794204712 	 0.3068053722381592 	 0.3089268207550049 	 15.239601612091064 	 15.117997646331787 	 0.4670848846435547 	 0.4634382724761963 	 
2025-08-06 06:28:20.326874 test begin: paddle.nn.functional.relu6(Tensor([128, 192, 112, 19],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([128, 192, 112, 19],"float32"), ) 	 52297728 	 33340 	 10.133438110351562 	 10.225630521774292 	 0.31066060066223145 	 0.3129599094390869 	 15.440764427185059 	 15.314836740493774 	 0.4733724594116211 	 0.4694225788116455 	 
2025-08-06 06:29:14.187376 test begin: paddle.nn.functional.relu6(Tensor([128, 192, 19, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([128, 192, 19, 112],"float32"), ) 	 52297728 	 33340 	 10.133488416671753 	 10.218451023101807 	 0.31067442893981934 	 0.31293678283691406 	 15.441380262374878 	 15.315377950668335 	 0.4732491970062256 	 0.4694492816925049 	 
2025-08-06 06:30:09.233456 test begin: paddle.nn.functional.relu6(Tensor([128, 32, 112, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([128, 32, 112, 112],"float32"), ) 	 51380224 	 33340 	 9.964717149734497 	 10.979100465774536 	 0.3054485321044922 	 0.3075249195098877 	 15.171884059906006 	 15.052136421203613 	 0.4651525020599365 	 0.4613516330718994 	 
2025-08-06 06:31:03.806906 test begin: paddle.nn.functional.relu6(Tensor([22, 192, 112, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([22, 192, 112, 112],"float32"), ) 	 52985856 	 33340 	 10.785355806350708 	 10.343518733978271 	 0.31469154357910156 	 0.31711578369140625 	 15.642378091812134 	 15.515857934951782 	 0.4795064926147461 	 0.4756026268005371 	 
2025-08-06 06:31:59.165319 test begin: paddle.nn.functional.relu6(Tensor([256, 16, 112, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([256, 16, 112, 112],"float32"), ) 	 51380224 	 33340 	 9.971917390823364 	 10.031874895095825 	 0.30550622940063477 	 0.30756282806396484 	 15.171836376190186 	 15.0521719455719 	 0.4650881290435791 	 0.46138882637023926 	 
2025-08-06 06:32:51.600891 test begin: paddle.nn.functional.relu6(Tensor([256, 96, 112, 19],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([256, 96, 112, 19],"float32"), ) 	 52297728 	 33340 	 10.133703708648682 	 11.331480026245117 	 0.3106248378753662 	 0.3129587173461914 	 15.440991878509521 	 15.315200567245483 	 0.4733750820159912 	 0.46947789192199707 	 
2025-08-06 06:33:47.398092 test begin: paddle.nn.functional.relu6(Tensor([256, 96, 19, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([256, 96, 19, 112],"float32"), ) 	 52297728 	 33340 	 10.13399052619934 	 10.208312273025513 	 0.3106062412261963 	 0.31290721893310547 	 15.441139936447144 	 15.315535306930542 	 0.4731762409210205 	 0.4695003032684326 	 
2025-08-06 06:34:42.932921 test begin: paddle.nn.functional.relu6(Tensor([29, 144, 112, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([29, 144, 112, 112],"float32"), ) 	 52383744 	 33340 	 10.15750503540039 	 10.22478985786438 	 0.3112204074859619 	 0.3134341239929199 	 15.468552589416504 	 15.3407142162323 	 0.4741818904876709 	 0.4702427387237549 	 
2025-08-06 06:35:39.062189 test begin: paddle.nn.functional.relu6(Tensor([43, 96, 112, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([43, 96, 112, 112],"float32"), ) 	 51781632 	 33340 	 10.57237458229065 	 10.110729455947876 	 0.3077561855316162 	 0.3099226951599121 	 15.290689706802368 	 15.167639255523682 	 0.46866655349731445 	 0.46493983268737793 	 
2025-08-06 06:36:33.710800 test begin: paddle.nn.functional.rrelu(Tensor([1, 2, 3, 4233601],"float64"), 0.05, 0.25, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([1, 2, 3, 4233601],"float64"), 0.05, 0.25, training=False, ) 	 25401606 	 20818 	 10.017369508743286 	 6.245108604431152 	 0.4916036128997803 	 0.3049321174621582 	 12.163357496261597 	 9.224239349365234 	 0.5971131324768066 	 0.45291852951049805 	 
2025-08-06 06:37:14.076453 test begin: paddle.nn.functional.rrelu(Tensor([1, 2, 3175201, 4],"float64"), 0.05, 0.25, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([1, 2, 3175201, 4],"float64"), 0.05, 0.25, training=False, ) 	 25401608 	 20818 	 10.017882108688354 	 6.228352308273315 	 0.49158716201782227 	 0.30500054359436035 	 12.162802696228027 	 9.223932027816772 	 0.5971143245697021 	 0.4528312683105469 	 
2025-08-06 06:37:54.698530 test begin: paddle.nn.functional.rrelu(Tensor([1, 2116801, 3, 4],"float64"), 0.05, 0.25, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([1, 2116801, 3, 4],"float64"), 0.05, 0.25, training=False, ) 	 25401612 	 20818 	 10.017411470413208 	 6.213228702545166 	 0.4915928840637207 	 0.30504679679870605 	 12.163466453552246 	 9.224519729614258 	 0.5971331596374512 	 0.4528930187225342 	 
2025-08-06 06:38:33.451981 test begin: paddle.nn.functional.rrelu(Tensor([1058401, 2, 3, 4],"float64"), 0.05, 0.25, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([1058401, 2, 3, 4],"float64"), 0.05, 0.25, training=False, ) 	 25401624 	 20818 	 10.014015197753906 	 6.213299036026001 	 0.49166321754455566 	 0.30507898330688477 	 12.163565397262573 	 9.225178718566895 	 0.5971593856811523 	 0.45287394523620605 	 
2025-08-06 06:39:12.200293 test begin: paddle.nn.functional.rrelu(Tensor([2, 1270081, 4, 5],"float32"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 1270081, 4, 5],"float32"), 0.1, 0.3, training=False, ) 	 50803240 	 20818 	 10.158407211303711 	 6.213988304138184 	 0.4986860752105713 	 0.30421018600463867 	 12.561663389205933 	 9.296493530273438 	 0.6166563034057617 	 0.45639777183532715 	 
2025-08-06 06:39:54.220667 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 1693441, 5],"float32"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 3, 1693441, 5],"float32"), 0.1, 0.3, training=False, ) 	 50803230 	 20818 	 10.158523321151733 	 6.196346282958984 	 0.49867820739746094 	 0.3042144775390625 	 12.561876773834229 	 9.296544313430786 	 0.6167151927947998 	 0.45641207695007324 	 
2025-08-06 06:40:34.143185 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 1058401],"float64"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 3, 4, 1058401],"float64"), 0.1, 0.3, training=False, ) 	 25401624 	 20818 	 10.016560792922974 	 6.213344097137451 	 0.4915900230407715 	 0.3050878047943115 	 12.163509130477905 	 9.22464370727539 	 0.597135066986084 	 0.45284461975097656 	 
2025-08-06 06:41:14.632512 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 2116801],"float32"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 3, 4, 2116801],"float32"), 0.1, 0.3, training=False, ) 	 50803224 	 20818 	 10.1648268699646 	 6.219000577926636 	 0.4987187385559082 	 0.3041954040527344 	 12.561740636825562 	 9.296364545822144 	 0.6166715621948242 	 0.456371545791626 	 
2025-08-06 06:41:57.098685 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 846721, 5],"float64"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 3, 846721, 5],"float64"), 0.1, 0.3, training=False, ) 	 25401630 	 20818 	 10.013895750045776 	 6.21303391456604 	 0.49160027503967285 	 0.30505847930908203 	 12.16337776184082 	 9.224649429321289 	 0.5971593856811523 	 0.4528834819793701 	 
2025-08-06 06:42:38.898037 test begin: paddle.nn.functional.rrelu(Tensor([2, 635041, 4, 5],"float64"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 635041, 4, 5],"float64"), 0.1, 0.3, training=False, ) 	 25401640 	 20818 	 11.402124881744385 	 6.213303327560425 	 0.4912853240966797 	 0.3049960136413574 	 12.162943363189697 	 9.22506046295166 	 0.5971205234527588 	 0.45291852951049805 	 
2025-08-06 06:43:21.232713 test begin: paddle.nn.functional.rrelu(Tensor([423361, 3, 4, 5],"float64"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([423361, 3, 4, 5],"float64"), 0.1, 0.3, training=False, ) 	 25401660 	 20818 	 10.006189346313477 	 6.2265191078186035 	 0.4911916255950928 	 0.305009126663208 	 12.162862539291382 	 9.225225687026978 	 0.5971333980560303 	 0.45289087295532227 	 
2025-08-06 06:44:01.681743 test begin: paddle.nn.functional.rrelu(Tensor([846721, 3, 4, 5],"float32"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([846721, 3, 4, 5],"float32"), 0.1, 0.3, training=False, ) 	 50803260 	 20818 	 11.58565902709961 	 6.196303844451904 	 0.4987218379974365 	 0.30422091484069824 	 12.561649560928345 	 9.296964406967163 	 0.616671085357666 	 0.4564964771270752 	 
2025-08-06 06:44:45.732710 test begin: paddle.nn.functional.selu(Tensor([101607, 5, 5, 10],"float64"), 1.5, 2.0, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([101607, 5, 5, 10],"float64"), 1.5, 2.0, ) 	 25401750 	 33360 	 9.990831136703491 	 60.92353630065918 	 0.3060133457183838 	 0.31111812591552734 	 14.931448459625244 	 70.83569312095642 	 0.45757007598876953 	 0.2714874744415283 	 
2025-08-06 06:47:28.258052 test begin: paddle.nn.functional.selu(Tensor([101607, 5, 5, 10],"float64"), 1.5, 2.0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([101607, 5, 5, 10],"float64"), 1.5, 2.0, None, ) 	 25401750 	 33360 	 10.572524070739746 	 60.90739941596985 	 0.3060019016265869 	 0.31116390228271484 	 14.929945945739746 	 70.83517456054688 	 0.45745015144348145 	 0.2714970111846924 	 
2025-08-06 06:50:08.562191 test begin: paddle.nn.functional.selu(Tensor([2822401, 3, 3],"float64"), 1.0507009873554805, 0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([2822401, 3, 3],"float64"), 1.0507009873554805, 0, None, ) 	 25401609 	 33360 	 9.990515947341919 	 60.907902240753174 	 0.30607151985168457 	 0.3110992908477783 	 14.930164813995361 	 70.83642959594727 	 0.45748043060302734 	 0.27143311500549316 	 
2025-08-06 06:52:48.680624 test begin: paddle.nn.functional.selu(Tensor([3, 169345, 5, 10],"float64"), 1.5, 2.0, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 169345, 5, 10],"float64"), 1.5, 2.0, ) 	 25401750 	 33360 	 9.990719318389893 	 60.90825629234314 	 0.3060116767883301 	 0.3111603260040283 	 14.930806398391724 	 70.83545541763306 	 0.45757460594177246 	 0.2715189456939697 	 
2025-08-06 06:55:26.541335 test begin: paddle.nn.functional.selu(Tensor([3, 169345, 5, 10],"float64"), 1.5, 2.0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 169345, 5, 10],"float64"), 1.5, 2.0, None, ) 	 25401750 	 33360 	 9.990174055099487 	 60.91660261154175 	 0.30596446990966797 	 0.3110837936401367 	 14.931301832199097 	 70.83397793769836 	 0.45757055282592773 	 0.27143311500549316 	 
2025-08-06 06:58:08.646662 test begin: paddle.nn.functional.selu(Tensor([3, 2822401, 3],"float64"), 1.0507009873554805, 0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 2822401, 3],"float64"), 1.0507009873554805, 0, None, ) 	 25401609 	 33360 	 9.991935014724731 	 60.90456962585449 	 0.3061258792877197 	 0.31113290786743164 	 14.93093991279602 	 70.83398938179016 	 0.45735859870910645 	 0.2714991569519043 	 
2025-08-06 07:00:48.931447 test begin: paddle.nn.functional.selu(Tensor([3, 3, 2822401],"float64"), 1.0507009873554805, 0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 3, 2822401],"float64"), 1.0507009873554805, 0, None, ) 	 25401609 	 33360 	 9.990590333938599 	 60.916441440582275 	 0.30608582496643066 	 0.31106996536254883 	 14.930870294570923 	 70.83524513244629 	 0.4574282169342041 	 0.2714352607727051 	 
2025-08-06 07:03:27.628872 test begin: paddle.nn.functional.selu(Tensor([3, 5, 169345, 10],"float64"), 1.5, 2.0, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 5, 169345, 10],"float64"), 1.5, 2.0, ) 	 25401750 	 33360 	 9.988680839538574 	 60.91301727294922 	 0.3060719966888428 	 0.31119370460510254 	 14.931063890457153 	 70.83644223213196 	 0.45742011070251465 	 0.271481990814209 	 
2025-08-06 07:06:05.869735 test begin: paddle.nn.functional.selu(Tensor([3, 5, 169345, 10],"float64"), 1.5, 2.0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 5, 169345, 10],"float64"), 1.5, 2.0, None, ) 	 25401750 	 33360 	 9.988446235656738 	 60.909159421920776 	 0.3060035705566406 	 0.3111598491668701 	 14.930750608444214 	 70.83543801307678 	 0.4574904441833496 	 0.2714548110961914 	 
2025-08-06 07:08:44.788151 test begin: paddle.nn.functional.selu(Tensor([3, 5, 5, 338689],"float64"), 1.5, 2.0, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 5, 5, 338689],"float64"), 1.5, 2.0, ) 	 25401675 	 33360 	 10.014015674591064 	 60.904590129852295 	 0.306607723236084 	 0.3111839294433594 	 14.931211233139038 	 70.83126997947693 	 0.4575355052947998 	 0.2714409828186035 	 
2025-08-06 07:11:23.768396 test begin: paddle.nn.functional.selu(Tensor([3, 5, 5, 338689],"float64"), 1.5, 2.0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 5, 5, 338689],"float64"), 1.5, 2.0, None, ) 	 25401675 	 33360 	 10.013731718063354 	 60.901585817337036 	 0.3066542148590088 	 0.31110191345214844 	 14.930869579315186 	 70.8321943283081 	 0.4571652412414551 	 0.2714085578918457 	 
2025-08-06 07:14:01.795514 test begin: paddle.nn.functional.sequence_mask(Tensor([2, 2, 3, 3, 705601],"float64"), maxlen=5, dtype=type(numpy.int32), )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([2, 2, 3, 3, 705601],"float64"), maxlen=5, dtype=type(numpy.int32), ) 	 25401636 	 12839 	 9.990786790847778 	 19.01129937171936 	 0.7952821254730225 	 0.50490403175354 	 None 	 None 	 None 	 None 	 combined
2025-08-06 07:14:31.435209 test begin: paddle.nn.functional.sequence_mask(Tensor([2, 2, 3, 705601, 3],"float64"), maxlen=5, dtype=type(numpy.int32), )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([2, 2, 3, 705601, 3],"float64"), maxlen=5, dtype=type(numpy.int32), ) 	 25401636 	 12839 	 9.990694522857666 	 19.031487464904785 	 0.7952666282653809 	 0.5049448013305664 	 None 	 None 	 None 	 None 	 combined
2025-08-06 07:15:06.383339 test begin: paddle.nn.functional.sequence_mask(Tensor([2, 2, 705601, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([2, 2, 705601, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), ) 	 25401636 	 12839 	 9.991960048675537 	 19.01170825958252 	 0.7952742576599121 	 0.5049788951873779 	 None 	 None 	 None 	 None 	 combined
2025-08-06 07:15:39.379837 test begin: paddle.nn.functional.sequence_mask(Tensor([2, 470401, 3, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([2, 470401, 3, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), ) 	 25401654 	 12839 	 11.247042894363403 	 19.011690139770508 	 0.795325517654419 	 0.5048961639404297 	 None 	 None 	 None 	 None 	 combined
2025-08-06 07:16:11.718641 test begin: paddle.nn.functional.sequence_mask(Tensor([470401, 2, 3, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([470401, 2, 3, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), ) 	 25401654 	 12839 	 10.721996545791626 	 19.011873245239258 	 0.7952742576599121 	 0.5049281120300293 	 None 	 None 	 None 	 None 	 combined
2025-08-06 07:16:45.387726 test begin: paddle.nn.functional.sequence_mask(Tensor([50803201],"int32"), maxlen=4, dtype="float32", )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([50803201],"int32"), maxlen=4, dtype="float32", ) 	 50803201 	 12839 	 15.534234046936035 	 31.111663579940796 	 1.2362616062164307 	 0.8262848854064941 	 None 	 None 	 None 	 None 	 combined
2025-08-06 07:17:33.204049 test begin: paddle.nn.functional.sigmoid(Tensor([10, 32, 400, 400],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([10, 32, 400, 400],"float32"), ) 	 51200000 	 33850 	 10.05421757698059 	 10.167742252349854 	 0.30354785919189453 	 0.30696654319763184 	 15.35081434249878 	 15.226469039916992 	 0.46346020698547363 	 0.45972704887390137 	 
2025-08-06 07:18:25.896283 test begin: paddle.nn.functional.sigmoid(Tensor([364, 304, 460],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([364, 304, 460],"float32"), ) 	 50901760 	 33850 	 9.996342658996582 	 10.109692811965942 	 0.3018181324005127 	 0.3052709102630615 	 15.261958122253418 	 15.140620470046997 	 0.46071362495422363 	 0.45708131790161133 	 
2025-08-06 07:19:18.227523 test begin: paddle.nn.functional.sigmoid(Tensor([364, 416, 336],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([364, 416, 336],"float32"), ) 	 50878464 	 33850 	 9.992292642593384 	 11.772475957870483 	 0.30159497261047363 	 0.305095911026001 	 15.255405187606812 	 15.131836175918579 	 0.4605588912963867 	 0.45681047439575195 	 
2025-08-06 07:20:13.257176 test begin: paddle.nn.functional.sigmoid(Tensor([372, 304, 450],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([372, 304, 450],"float32"), ) 	 50889600 	 33850 	 9.99577808380127 	 10.120587825775146 	 0.30178213119506836 	 0.30511951446533203 	 15.257816791534424 	 15.13565969467163 	 0.4607057571411133 	 0.4570655822753906 	 
2025-08-06 07:21:07.102424 test begin: paddle.nn.functional.sigmoid(Tensor([372, 407, 336],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([372, 407, 336],"float32"), ) 	 50871744 	 33850 	 10.482820749282837 	 10.10949420928955 	 0.301563024520874 	 0.30504894256591797 	 15.254810810089111 	 15.132375955581665 	 0.4605412483215332 	 0.45685338973999023 	 
2025-08-06 07:22:00.377342 test begin: paddle.nn.functional.sigmoid(Tensor([498, 304, 336],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([498, 304, 336],"float32"), ) 	 50867712 	 33850 	 9.989430904388428 	 10.102917432785034 	 0.30155491828918457 	 0.3050355911254883 	 15.253781795501709 	 15.130569219589233 	 0.4605851173400879 	 0.4568483829498291 	 
2025-08-06 07:22:52.754204 test begin: paddle.nn.functional.sigmoid(Tensor([8, 32, 400, 497],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([8, 32, 400, 497],"float32"), ) 	 50892800 	 33850 	 9.99205470085144 	 10.126436948776245 	 0.3017101287841797 	 0.3052208423614502 	 15.2614107131958 	 15.138009548187256 	 0.46083521842956543 	 0.456989049911499 	 
2025-08-06 07:23:47.286424 test begin: paddle.nn.functional.sigmoid(Tensor([8, 32, 497, 400],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([8, 32, 497, 400],"float32"), ) 	 50892800 	 33850 	 9.991873502731323 	 10.108955383300781 	 0.3016934394836426 	 0.30515217781066895 	 15.260255575180054 	 15.137839794158936 	 0.4606449604034424 	 0.4570486545562744 	 
2025-08-06 07:24:42.394347 test begin: paddle.nn.functional.sigmoid(Tensor([8, 40, 400, 400],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([8, 40, 400, 400],"float32"), ) 	 51200000 	 33850 	 10.05594539642334 	 10.167261362075806 	 0.3035454750061035 	 0.30697059631347656 	 15.351218223571777 	 15.228569984436035 	 0.4634666442871094 	 0.4598112106323242 	 
2025-08-06 07:25:38.095736 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803248 	 1450 	 10.020386934280396 	 8.176321268081665 	 0.0006859302520751953 	 0.3032839298248291 	 13.425912141799927 	 13.290303230285645 	 0.4116780757904053 	 0.3747272491455078 	 combined
2025-08-06 07:26:27.172399 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803249 	 1450 	 10.438887357711792 	 8.612862348556519 	 0.0009741783142089844 	 0.30306029319763184 	 14.521089792251587 	 15.895849227905273 	 0.3936502933502197 	 0.3503866195678711 	 combined
2025-08-06 07:27:17.735075 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", ) 	 50803249 	 1450 	 10.43675422668457 	 8.613356590270996 	 0.0009710788726806641 	 0.303051233291626 	 14.513793706893921 	 15.896474838256836 	 0.3934977054595947 	 0.350325345993042 	 combined
2025-08-06 07:28:08.217583 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803320 	 1450 	 10.003836154937744 	 8.176172971725464 	 0.0006844997406005859 	 0.3034830093383789 	 13.423568487167358 	 13.291323184967041 	 0.4116203784942627 	 0.37476181983947754 	 combined
2025-08-06 07:28:54.251595 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803321 	 1450 	 10.435219526290894 	 8.612839221954346 	 0.00096893310546875 	 0.3030507564544678 	 14.518069744110107 	 15.897232055664062 	 0.3935110569000244 	 0.35027170181274414 	 combined
2025-08-06 07:29:44.760217 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", ) 	 50803321 	 1450 	 10.453012704849243 	 8.619524717330933 	 0.0009303092956542969 	 0.30312371253967285 	 14.517310380935669 	 15.896306276321411 	 0.3935739994049072 	 0.35043954849243164 	 combined
2025-08-06 07:30:41.405240 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803360 	 1450 	 10.013993501663208 	 8.18161392211914 	 0.0006506443023681641 	 0.30332255363464355 	 13.42090392112732 	 13.291239738464355 	 0.41160035133361816 	 0.37476301193237305 	 combined
2025-08-06 07:31:29.660685 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803361 	 1450 	 10.454315423965454 	 8.61701226234436 	 0.0009241104125976562 	 0.3031117916107178 	 14.51996636390686 	 15.895785093307495 	 0.3936431407928467 	 0.35033130645751953 	 combined
2025-08-06 07:32:24.254246 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", ) 	 50803361 	 1450 	 10.452426433563232 	 8.626345157623291 	 0.0009672641754150391 	 0.30310845375061035 	 14.515578031539917 	 15.895655870437622 	 0.3935070037841797 	 0.35033202171325684 	 combined
2025-08-06 07:33:18.182735 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803440 	 1450 	 10.000607013702393 	 8.176656484603882 	 0.0006854534149169922 	 0.30324673652648926 	 13.422666549682617 	 13.290769100189209 	 0.41164493560791016 	 0.3747429847717285 	 combined
2025-08-06 07:34:05.012035 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803441 	 1450 	 10.44544768333435 	 8.613068580627441 	 0.0009729862213134766 	 0.30304622650146484 	 14.50504755973816 	 15.89733624458313 	 0.39327073097229004 	 0.35035204887390137 	 combined
2025-08-06 07:34:55.595605 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", ) 	 50803441 	 1450 	 10.430214643478394 	 8.61296010017395 	 0.0009729862213134766 	 0.30313611030578613 	 14.50644302368164 	 15.919867992401123 	 0.3933424949645996 	 0.350322961807251 	 combined
2025-08-06 07:35:46.110274 test begin: paddle.nn.functional.silu(Tensor([128, 128, 128, 25],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 128, 128, 25],"float32"), None, ) 	 52428800 	 33834 	 10.298489093780518 	 10.396741390228271 	 0.31104493141174316 	 0.3140714168548584 	 15.716641426086426 	 15.706865787506104 	 0.4747276306152344 	 0.4744536876678467 	 
2025-08-06 07:36:41.753747 test begin: paddle.nn.functional.silu(Tensor([128, 128, 25, 128],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 128, 25, 128],"float32"), None, ) 	 52428800 	 33834 	 10.297382593154907 	 10.39650273323059 	 0.31093645095825195 	 0.3140542507171631 	 15.716814041137695 	 15.706732034683228 	 0.4748680591583252 	 0.4744112491607666 	 
2025-08-06 07:37:37.779909 test begin: paddle.nn.functional.silu(Tensor([128, 25, 128, 128],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 25, 128, 128],"float32"), None, ) 	 52428800 	 33834 	 11.364788293838501 	 10.39767861366272 	 0.3109769821166992 	 0.3140530586242676 	 15.715060949325562 	 15.706981897354126 	 0.47458744049072266 	 0.4744410514831543 	 
2025-08-06 07:38:33.435235 test begin: paddle.nn.functional.silu(Tensor([128, 256, 25, 64],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 256, 25, 64],"float32"), None, ) 	 52428800 	 33834 	 10.29496455192566 	 10.396782398223877 	 0.3109602928161621 	 0.3140559196472168 	 15.716557025909424 	 15.706575870513916 	 0.47471117973327637 	 0.47444629669189453 	 
2025-08-06 07:39:27.326379 test begin: paddle.nn.functional.silu(Tensor([128, 256, 64, 25],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 256, 64, 25],"float32"), None, ) 	 52428800 	 33834 	 10.294067144393921 	 10.397600650787354 	 0.31099939346313477 	 0.314011812210083 	 15.716933250427246 	 15.706412315368652 	 0.4747762680053711 	 0.47443056106567383 	 
2025-08-06 07:40:21.850796 test begin: paddle.nn.functional.silu(Tensor([128, 64, 128, 49],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 64, 128, 49],"float32"), None, ) 	 51380224 	 33834 	 10.097054719924927 	 10.198445081710815 	 0.3049640655517578 	 0.30796313285827637 	 15.407354593276978 	 15.398396015167236 	 0.46537280082702637 	 0.46515417098999023 	 
2025-08-06 07:41:14.695485 test begin: paddle.nn.functional.silu(Tensor([128, 64, 49, 128],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 64, 49, 128],"float32"), None, ) 	 51380224 	 33834 	 10.097133159637451 	 10.206817626953125 	 0.3049783706665039 	 0.3079183101654053 	 15.40710163116455 	 15.398549556732178 	 0.46535515785217285 	 0.46512293815612793 	 
2025-08-06 07:42:08.230412 test begin: paddle.nn.functional.silu(Tensor([128, 97, 64, 64],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 97, 64, 64],"float32"), None, ) 	 50855936 	 33834 	 9.99361276626587 	 10.09682846069336 	 0.3018636703491211 	 0.3048982620239258 	 15.2511465549469 	 15.244985342025757 	 0.46068239212036133 	 0.46048569679260254 	 
2025-08-06 07:43:00.533344 test begin: paddle.nn.functional.silu(Tensor([25, 128, 128, 128],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([25, 128, 128, 128],"float32"), None, ) 	 52428800 	 33834 	 10.293941736221313 	 10.396414041519165 	 0.3109712600708008 	 0.31409502029418945 	 15.716825723648071 	 15.706783056259155 	 0.4747345447540283 	 0.47443485260009766 	 
2025-08-06 07:43:54.479465 test begin: paddle.nn.functional.silu(Tensor([49, 256, 64, 64],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([49, 256, 64, 64],"float32"), None, ) 	 51380224 	 33834 	 10.09741759300232 	 10.193790912628174 	 0.3049626350402832 	 0.30788755416870117 	 15.407350063323975 	 15.398779392242432 	 0.4654233455657959 	 0.46515488624572754 	 
2025-08-06 07:44:47.385584 test begin: paddle.nn.functional.silu(Tensor([49, 64, 128, 128],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([49, 64, 128, 128],"float32"), None, ) 	 51380224 	 33834 	 10.097168922424316 	 10.193614721298218 	 0.30501723289489746 	 0.30791449546813965 	 15.407192468643188 	 15.400190353393555 	 0.46541476249694824 	 0.4650425910949707 	 
2025-08-06 07:45:41.408107 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([1016065, 50],"float32"), Tensor([1016065, 50],"float32"), reduction="none", )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([1016065, 50],"float32"), Tensor([1016065, 50],"float32"), reduction="none", ) 	 101606500 	 12325 	 10.01441740989685 	 5.510977506637573 	 0.4152247905731201 	 0.45651721954345703 	 20.039583206176758 	 17.828413009643555 	 0.41539931297302246 	 0.36945366859436035 	 
2025-08-06 07:46:39.298964 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([1017, 50000],"float32"), Tensor([1017, 50000],"float32"), reduction="mean", delta=1.0, name=None, )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([1017, 50000],"float32"), Tensor([1017, 50000],"float32"), reduction="mean", delta=1.0, name=None, ) 	 101700000 	 12325 	 11.91700029373169 	 7.383181095123291 	 0.24645614624023438 	 0.20382142066955566 	 21.741103410720825 	 14.314180612564087 	 0.360609769821167 	 0.2965967655181885 	 
2025-08-06 07:47:38.360235 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([1914, 26543],"float32"), Tensor([1914, 26543],"float32"), reduction="none", )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([1914, 26543],"float32"), Tensor([1914, 26543],"float32"), reduction="none", ) 	 101606604 	 12325 	 10.001847982406616 	 5.504844903945923 	 0.414461612701416 	 0.45647740364074707 	 20.026668548583984 	 17.830262422561646 	 0.4150879383087158 	 0.36951208114624023 	 
2025-08-06 07:48:38.066858 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([33960, 187, 8],"float32"), Tensor([33960, 187, 8],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([33960, 187, 8],"float32"), Tensor([33960, 187, 8],"float32"), reduction="sum", ) 	 101608320 	 12325 	 11.903791427612305 	 7.377426862716675 	 0.2463235855102539 	 0.20364761352539062 	 21.705913543701172 	 14.302129745483398 	 0.36014580726623535 	 0.29641222953796387 	 
2025-08-06 07:49:38.069212 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([64, 187, 4245],"float32"), Tensor([64, 187, 4245],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([64, 187, 4245],"float32"), Tensor([64, 187, 4245],"float32"), reduction="sum", ) 	 101608320 	 12325 	 11.901824235916138 	 7.377641439437866 	 0.24628806114196777 	 0.20364904403686523 	 21.69637131690979 	 14.302264213562012 	 0.3598451614379883 	 0.296414852142334 	 
2025-08-06 07:50:38.504733 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([64, 99226, 8],"float32"), Tensor([64, 99226, 8],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([64, 99226, 8],"float32"), Tensor([64, 99226, 8],"float32"), reduction="sum", ) 	 101607424 	 12325 	 11.896169662475586 	 7.377637624740601 	 0.24621248245239258 	 0.20363569259643555 	 21.701249837875366 	 14.303605318069458 	 0.35999178886413574 	 0.29637575149536133 	 
2025-08-06 07:51:38.567387 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([7, 7257601],"float32"), Tensor([7, 7257601],"float32"), reduction="mean", delta=1.0, name=None, )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([7, 7257601],"float32"), Tensor([7, 7257601],"float32"), reduction="mean", delta=1.0, name=None, ) 	 101606414 	 12325 	 11.88523554801941 	 7.378184080123901 	 0.24594974517822266 	 0.2037038803100586 	 21.710166215896606 	 14.3045175075531 	 0.36003565788269043 	 0.2967336177825928 	 
2025-08-06 07:52:38.936375 test begin: paddle.nn.functional.softmax(Tensor([10, 2304, 2304],"float32"), axis=-1, )
W0806 07:52:42.330144 130327 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([10, 2304, 2304],"float32"), axis=-1, ) 	 53084160 	 33703 	 10.622222185134888 	 17.836194038391113 	 0.32207775115966797 	 0.5404071807861328 	 16.498802423477173 	 31.366711378097534 	 0.5003266334533691 	 0.47554492950439453 	 
2025-08-06 07:54:01.618019 test begin: paddle.nn.functional.softmax(Tensor([3840, 1, 144, 144],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([3840, 1, 144, 144],"float32"), -1, name=None, ) 	 79626240 	 33703 	 15.567802429199219 	 16.921977996826172 	 0.471996545791626 	 0.5129544734954834 	 23.615930318832397 	 46.928287744522095 	 0.7161314487457275 	 0.7114570140838623 	 
2025-08-06 07:55:48.122066 test begin: paddle.nn.functional.softmax(Tensor([3840, 4, 144, 23],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([3840, 4, 144, 23],"float32"), -1, name=None, ) 	 50872320 	 33703 	 12.329032182693481 	 16.931198120117188 	 0.3738875389099121 	 0.5134117603302002 	 15.187614917755127 	 30.173322439193726 	 0.4605836868286133 	 0.45751500129699707 	 
2025-08-06 07:57:05.249507 test begin: paddle.nn.functional.softmax(Tensor([3840, 4, 23, 144],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([3840, 4, 23, 144],"float32"), -1, name=None, ) 	 50872320 	 33703 	 9.991667747497559 	 10.898780822753906 	 0.3029968738555908 	 0.33048224449157715 	 15.150326251983643 	 30.070017099380493 	 0.45946264266967773 	 0.45595884323120117 	 
2025-08-06 07:58:14.178073 test begin: paddle.nn.functional.softmax(Tensor([4096, 1, 144, 144],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([4096, 1, 144, 144],"float32"), -1, name=None, ) 	 84934656 	 33703 	 16.59711527824402 	 18.024047136306763 	 0.5032577514648438 	 0.5462925434112549 	 25.1747567653656 	 50.040266036987305 	 0.7633302211761475 	 0.7586617469787598 	 
2025-08-06 08:00:07.241567 test begin: paddle.nn.functional.softmax(Tensor([4096, 4, 144, 22],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([4096, 4, 144, 22],"float32"), -1, name=None, ) 	 51904512 	 33703 	 15.514806509017944 	 18.039148092269897 	 0.47048020362854004 	 0.5466268062591553 	 15.592401266098022 	 30.822153568267822 	 0.472851037979126 	 0.46732139587402344 	 
2025-08-06 08:01:30.133401 test begin: paddle.nn.functional.softmax(Tensor([4096, 4, 22, 144],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([4096, 4, 22, 144],"float32"), -1, name=None, ) 	 51904512 	 33703 	 10.196533679962158 	 11.111103534698486 	 0.3091585636138916 	 0.3369317054748535 	 15.453184604644775 	 30.67797565460205 	 0.46865367889404297 	 0.4651820659637451 	 
2025-08-06 08:02:41.311688 test begin: paddle.nn.functional.softmax(Tensor([60, 2304, 368],"float32"), axis=-1, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([60, 2304, 368],"float32"), axis=-1, ) 	 50872320 	 33703 	 10.13317608833313 	 10.19261884689331 	 0.3072059154510498 	 0.30907249450683594 	 15.172216892242432 	 30.086647510528564 	 0.4600811004638672 	 0.45618271827697754 	 
2025-08-06 08:03:48.647333 test begin: paddle.nn.functional.softmax(Tensor([60, 368, 2304],"float32"), axis=-1, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([60, 368, 2304],"float32"), axis=-1, ) 	 50872320 	 33703 	 10.184774160385132 	 17.091145277023315 	 0.3088395595550537 	 0.5182368755340576 	 15.798853158950806 	 30.06778621673584 	 0.4790947437286377 	 0.4558734893798828 	 
2025-08-06 08:05:03.633817 test begin: paddle.nn.functional.softmax(Tensor([613, 4, 144, 144],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([613, 4, 144, 144],"float32"), -1, name=None, ) 	 50844672 	 33703 	 9.988466024398804 	 10.894927501678467 	 0.3029186725616455 	 0.33036017417907715 	 15.14284610748291 	 30.05757212638855 	 0.459214448928833 	 0.455702543258667 	 
2025-08-06 08:06:11.678054 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([1551, 16, 32, 64],"float32"), Tensor([1551, 16, 32, 1],"int64"), axis=3, )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([1551, 16, 32, 64],"float32"), Tensor([1551, 16, 32, 1],"int64"), axis=3, ) 	 51617280 	 29087 	 9.98583722114563 	 37.09341621398926 	 0.35097384452819824 	 0.26006364822387695 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 08:07:18.304689 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 1, 64],"int64"), axis=2, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 1, 64],"int64"), axis=2, ) 	 52394496 	 29087 	 36.99099612236023 	 65.20615005493164 	 0.6498517990112305 	 0.381152868270874 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 08:09:17.494724 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 32, 1],"int64"), axis=-1, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 32, 1],"int64"), axis=-1, ) 	 51600640 	 29087 	 9.983632802963257 	 37.06976771354675 	 0.3507564067840576 	 0.2600381374359131 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 08:10:24.467043 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 32, 1],"int64"), axis=3, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 32, 1],"int64"), axis=3, ) 	 51600640 	 29087 	 9.983210802078247 	 37.067102670669556 	 0.3508110046386719 	 0.2599904537200928 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 08:11:28.643276 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 24807, 64],"float32"), Tensor([2, 16, 1, 64],"int64"), axis=2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6e30653df0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:21:48.554200 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 24807, 64],"float32"), Tensor([2, 16, 24807, 1],"int64"), axis=-1, )
W0806 08:21:49.536561 131682 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 24807, 64],"float32"), Tensor([2, 16, 24807, 1],"int64"), axis=-1, ) 	 51598560 	 29087 	 10.017204284667969 	 37.08741331100464 	 0.3518674373626709 	 0.26013922691345215 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 08:22:53.278778 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 24807, 64],"float32"), Tensor([2, 16, 24807, 1],"int64"), axis=3, )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 24807, 64],"float32"), Tensor([2, 16, 24807, 1],"int64"), axis=3, ) 	 51598560 	 29087 	 10.016293048858643 	 37.07172250747681 	 0.35189032554626465 	 0.2600429058074951 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 08:23:58.782822 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 1, 49613],"int64"), axis=2, )
W0806 08:23:59.557859 131706 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 1, 49613],"int64"), axis=2, ) 	 52391328 	 29087 	 44.716776847839355 	 64.42869472503662 	 0.7854368686676025 	 0.3765401840209961 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 08:26:09.598264 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 32, 1],"int64"), axis=-1, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 32, 1],"int64"), axis=-1, ) 	 50804736 	 29087 	 19.417481660842896 	 31.31429886817932 	 0.6548252105712891 	 0.21957087516784668 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 08:27:16.563443 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 32, 1],"int64"), axis=3, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 32, 1],"int64"), axis=3, ) 	 50804736 	 29087 	 19.187036991119385 	 31.311168909072876 	 0.6548049449920654 	 0.21953988075256348 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 08:28:24.364990 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 793801],"float32"), Tensor([2, 16, 1, 793801],"int64"), axis=2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbe466c2830>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:38:49.894033 test begin: paddle.nn.functional.softplus(Tensor([113401, 7, 64],"float32"), )
W0806 08:38:52.359807 132457 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([113401, 7, 64],"float32"), ) 	 50803648 	 33404 	 10.007323503494263 	 10.012844324111938 	 0.3060445785522461 	 0.3062875270843506 	 15.045653343200684 	 15.043706893920898 	 0.4603235721588135 	 0.46021342277526855 	 
2025-08-06 08:39:44.749225 test begin: paddle.nn.functional.softplus(Tensor([13, 10, 390794],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([13, 10, 390794],"float32"), ) 	 50803220 	 33404 	 10.010812282562256 	 10.009439468383789 	 0.3061697483062744 	 0.3061950206756592 	 15.045644283294678 	 15.046593189239502 	 0.4602677822113037 	 0.46030235290527344 	 
2025-08-06 08:40:39.292103 test begin: paddle.nn.functional.softplus(Tensor([13, 1007, 3881],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([13, 1007, 3881],"float32"), ) 	 50806171 	 33404 	 10.728983640670776 	 10.041341543197632 	 0.3060328960418701 	 0.30626916885375977 	 15.049561500549316 	 15.04762864112854 	 0.46044182777404785 	 0.46031618118286133 	 
2025-08-06 08:41:38.379507 test begin: paddle.nn.functional.softplus(Tensor([13, 61062, 64],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([13, 61062, 64],"float32"), ) 	 50803584 	 33404 	 11.558025121688843 	 10.009442806243896 	 0.30771493911743164 	 0.30623555183410645 	 15.046576499938965 	 15.046695232391357 	 0.4603562355041504 	 0.460315465927124 	 
2025-08-06 08:42:32.544899 test begin: paddle.nn.functional.softplus(Tensor([14, 56701, 64],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([14, 56701, 64],"float32"), ) 	 50804096 	 33404 	 10.045735359191895 	 10.014313459396362 	 0.30739259719848633 	 0.30631518363952637 	 15.04775595664978 	 15.047400712966919 	 0.4604003429412842 	 0.4603595733642578 	 
2025-08-06 08:43:24.363560 test begin: paddle.nn.functional.softplus(Tensor([14, 7, 518401],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([14, 7, 518401],"float32"), ) 	 50803298 	 33404 	 10.06260871887207 	 10.012666702270508 	 0.30788373947143555 	 0.30623626708984375 	 15.047598123550415 	 15.047115564346313 	 0.46042776107788086 	 0.4603445529937744 	 
2025-08-06 08:44:16.207989 test begin: paddle.nn.functional.softplus(Tensor([789, 1007, 64],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([789, 1007, 64],"float32"), ) 	 50849472 	 33404 	 10.009665727615356 	 10.034593105316162 	 0.3062429428100586 	 0.30650830268859863 	 15.057311534881592 	 15.05923843383789 	 0.4607532024383545 	 0.4607419967651367 	 
2025-08-06 08:45:09.407517 test begin: paddle.nn.functional.softplus(Tensor([79381, 10, 64],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([79381, 10, 64],"float32"), ) 	 50803840 	 33404 	 10.05875825881958 	 10.656748056411743 	 0.3077700138092041 	 0.30626440048217773 	 15.04902982711792 	 15.047606468200684 	 0.46039557456970215 	 0.46037769317626953 	 
2025-08-06 08:46:05.240538 test begin: paddle.nn.functional.softshrink(Tensor([2822401, 3, 3],"float64"), 0, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([2822401, 3, 3],"float64"), 0, None, ) 	 25401609 	 33823 	 10.081361532211304 	 10.102396011352539 	 0.3046441078186035 	 0.30522894859313965 	 15.15205430984497 	 15.075988054275513 	 0.4576098918914795 	 0.4555397033691406 	 
2025-08-06 08:46:56.893200 test begin: paddle.nn.functional.softshrink(Tensor([2822401, 3, 3],"float64"), 5, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([2822401, 3, 3],"float64"), 5, None, ) 	 25401609 	 33823 	 10.082604885101318 	 10.096110820770264 	 0.30466604232788086 	 0.304995059967041 	 15.151102066040039 	 15.07643461227417 	 0.45801877975463867 	 0.4555234909057617 	 
2025-08-06 08:47:49.031804 test begin: paddle.nn.functional.softshrink(Tensor([3, 2822401, 3],"float64"), 0, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([3, 2822401, 3],"float64"), 0, None, ) 	 25401609 	 33823 	 10.081637382507324 	 11.052916288375854 	 0.30461883544921875 	 0.3052501678466797 	 15.150036096572876 	 15.075739622116089 	 0.4578225612640381 	 0.45551061630249023 	 
2025-08-06 08:48:44.301859 test begin: paddle.nn.functional.softshrink(Tensor([3, 2822401, 3],"float64"), 5, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([3, 2822401, 3],"float64"), 5, None, ) 	 25401609 	 33823 	 10.087782621383667 	 10.096723079681396 	 0.3046107292175293 	 0.3050575256347656 	 15.151131868362427 	 15.076105833053589 	 0.45801877975463867 	 0.4555017948150635 	 
2025-08-06 08:49:39.362972 test begin: paddle.nn.functional.softshrink(Tensor([3, 3, 2822401],"float64"), 0, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([3, 3, 2822401],"float64"), 0, None, ) 	 25401609 	 33823 	 11.136107444763184 	 10.101812839508057 	 0.30460333824157715 	 0.30523085594177246 	 15.147493362426758 	 15.075218915939331 	 0.45746803283691406 	 0.45546483993530273 	 
2025-08-06 08:50:33.032409 test begin: paddle.nn.functional.softshrink(Tensor([3, 3, 2822401],"float64"), 5, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([3, 3, 2822401],"float64"), 5, None, ) 	 25401609 	 33823 	 10.081310033798218 	 10.096588134765625 	 0.3046138286590576 	 0.3051621913909912 	 15.150012016296387 	 15.077375650405884 	 0.4579493999481201 	 0.455578088760376 	 
2025-08-06 08:51:24.733726 test begin: paddle.nn.functional.softshrink(Tensor([32, 15, 207, 8, 32, 2],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([32, 15, 207, 8, 32, 2],"float32"), threshold=0.01, ) 	 50872320 	 33823 	 10.024063110351562 	 10.10256052017212 	 0.30287647247314453 	 0.30516576766967773 	 15.240714073181152 	 15.123531103134155 	 0.4614729881286621 	 0.45690155029296875 	 
2025-08-06 08:52:19.134180 test begin: paddle.nn.functional.softshrink(Tensor([32, 15, 8, 207, 32, 2],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([32, 15, 8, 207, 32, 2],"float32"), threshold=0.01, ) 	 50872320 	 33823 	 10.019882678985596 	 10.09879469871521 	 0.30270957946777344 	 0.3051316738128662 	 15.246622562408447 	 15.123406648635864 	 0.4606301784515381 	 0.4569556713104248 	 
2025-08-06 08:53:11.265094 test begin: paddle.nn.functional.softshrink(Tensor([32, 15, 8, 8, 32, 52],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([32, 15, 8, 8, 32, 52],"float32"), threshold=0.01, ) 	 51118080 	 33823 	 10.060415506362915 	 11.70318078994751 	 0.3039569854736328 	 0.3067185878753662 	 15.313050985336304 	 15.195322751998901 	 0.4627523422241211 	 0.4591705799102783 	 
2025-08-06 08:54:07.927262 test begin: paddle.nn.functional.softshrink(Tensor([32, 15, 8, 8, 827, 2],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([32, 15, 8, 8, 827, 2],"float32"), threshold=0.01, ) 	 50810880 	 33823 	 11.21384048461914 	 10.753884553909302 	 0.3022305965423584 	 0.3047754764556885 	 15.225714683532715 	 15.1050546169281 	 0.4601461887359619 	 0.45644259452819824 	 
2025-08-06 08:55:04.137278 test begin: paddle.nn.functional.softshrink(Tensor([32, 388, 8, 8, 32, 2],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([32, 388, 8, 8, 32, 2],"float32"), threshold=0.01, ) 	 50855936 	 33823 	 10.012417078018188 	 10.095255613327026 	 0.3025343418121338 	 0.30507779121398926 	 15.238489627838135 	 15.117802858352661 	 0.4604988098144531 	 0.4568061828613281 	 
2025-08-06 08:55:56.435671 test begin: paddle.nn.functional.softshrink(Tensor([827, 15, 8, 8, 32, 2],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([827, 15, 8, 8, 32, 2],"float32"), threshold=0.01, ) 	 50810880 	 33823 	 10.003310680389404 	 10.0866117477417 	 0.3022744655609131 	 0.3048067092895508 	 15.225611925125122 	 15.10550856590271 	 0.46013522148132324 	 0.45644164085388184 	 
2025-08-06 08:56:48.690949 test begin: paddle.nn.functional.softsign(Tensor([12404, 4096],"float32"), )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([12404, 4096],"float32"), ) 	 50806784 	 33892 	 10.009445190429688 	 35.34526491165161 	 0.3018057346343994 	 0.35533690452575684 	 15.257271766662598 	 111.12743759155273 	 0.46004295349121094 	 0.41878175735473633 	 
2025-08-06 08:59:44.603628 test begin: paddle.nn.functional.softsign(Tensor([2822401, 3, 3],"float64"), None, )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([2822401, 3, 3],"float64"), None, ) 	 25401609 	 33892 	 10.142049789428711 	 35.361897706985474 	 0.3056607246398926 	 0.3552134037017822 	 15.177213907241821 	 110.81003522872925 	 0.4576718807220459 	 0.4176292419433594 	 
2025-08-06 09:02:42.278691 test begin: paddle.nn.functional.softsign(Tensor([3, 2822401, 3],"float64"), None, )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([3, 2822401, 3],"float64"), None, ) 	 25401609 	 33892 	 10.146066904067993 	 35.337278604507446 	 0.30570077896118164 	 0.3552086353302002 	 15.181086778640747 	 110.80275297164917 	 0.45790815353393555 	 0.41763925552368164 	 
2025-08-06 09:05:39.119000 test begin: paddle.nn.functional.softsign(Tensor([3, 3, 2822401],"float64"), None, )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([3, 3, 2822401],"float64"), None, ) 	 25401609 	 33892 	 12.204015254974365 	 35.341089963912964 	 0.3057558536529541 	 0.35527658462524414 	 15.179704904556274 	 110.81475639343262 	 0.45767760276794434 	 0.4176211357116699 	 
2025-08-06 09:08:35.089862 test begin: paddle.nn.functional.softsign(Tensor([300, 169345],"float32"), )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([300, 169345],"float32"), ) 	 50803500 	 33892 	 10.51349949836731 	 35.34479522705078 	 0.30141234397888184 	 0.35529565811157227 	 15.258645296096802 	 111.12999510765076 	 0.460097074508667 	 0.418867826461792 	 
2025-08-06 09:11:31.331161 test begin: paddle.nn.functional.softsign(Tensor([32, 1587601],"float32"), )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([32, 1587601],"float32"), ) 	 50803232 	 33892 	 9.998762607574463 	 35.36405801773071 	 0.30155515670776367 	 0.3553032875061035 	 15.25858473777771 	 111.1314845085144 	 0.4600820541381836 	 0.41876935958862305 	 
2025-08-06 09:14:28.215285 test begin: paddle.nn.functional.softsign(Tensor([396901, 128],"float32"), )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([396901, 128],"float32"), ) 	 50803328 	 33892 	 9.993818044662476 	 35.344112396240234 	 0.3012573719024658 	 0.35530734062194824 	 15.256108045578003 	 111.13066291809082 	 0.46001625061035156 	 0.41878843307495117 	 
2025-08-06 09:17:22.581056 test begin: paddle.nn.functional.square_error_cost(Tensor([10161, 100, 100],"float16"), Tensor([10161, 100, 100],"float32"), )
W0806 09:17:25.871439 133867 dygraph_functions.cc:93089] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([10161, 100, 100],"float16"), Tensor([10161, 100, 100],"float32"), ) 	 203220000 	 16765 	 32.909276247024536 	 23.51594305038452 	 0.6688899993896484 	 0.7165372371673584 	 38.41862082481384 	 52.59430432319641 	 0.7806220054626465 	 0.5343160629272461 	 combined
2025-08-06 09:19:58.543065 test begin: paddle.nn.functional.square_error_cost(Tensor([3, 2, 1, 2],"float64"), label=Tensor([3, 2, 2116801, 2],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([3, 2, 1, 2],"float64"), label=Tensor([3, 2, 2116801, 2],"float64"), ) 	 25401624 	 16765 	 10.036478757858276 	 10.03542685508728 	 0.30593061447143555 	 0.3058500289916992 	 69.48358368873596 	 25.423192501068115 	 1.060267686843872 	 0.22135233879089355 	 combined
2025-08-06 09:21:54.653516 test begin: paddle.nn.functional.square_error_cost(Tensor([3, 2, 1, 4233601],"float64"), label=Tensor([3, 2, 1, 4233601],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([3, 2, 1, 4233601],"float64"), label=Tensor([3, 2, 1, 4233601],"float64"), ) 	 50803212 	 16765 	 12.459500789642334 	 12.426503658294678 	 0.37976574897766113 	 0.37882065773010254 	 15.527144193649292 	 22.668648719787598 	 0.4732551574707031 	 0.27660584449768066 	 combined
2025-08-06 09:23:00.953502 test begin: paddle.nn.functional.square_error_cost(Tensor([3, 2, 2116801, 2],"float64"), label=Tensor([3, 2, 1, 2],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([3, 2, 2116801, 2],"float64"), label=Tensor([3, 2, 1, 2],"float64"), ) 	 25401624 	 16765 	 10.005218029022217 	 10.034968137741089 	 0.3049294948577881 	 0.30589842796325684 	 64.30643033981323 	 25.417954206466675 	 1.3077473640441895 	 0.22135257720947266 	 combined
2025-08-06 09:24:51.814772 test begin: paddle.nn.functional.square_error_cost(Tensor([3, 2, 2116801, 2],"float64"), label=Tensor([3, 2, 2116801, 2],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([3, 2, 2116801, 2],"float64"), label=Tensor([3, 2, 2116801, 2],"float64"), ) 	 50803224 	 16765 	 12.4650239944458 	 12.424643993377686 	 0.3799264430999756 	 0.37871313095092773 	 15.527409791946411 	 22.669840335845947 	 0.47329258918762207 	 0.27658867835998535 	 combined
2025-08-06 09:25:56.515860 test begin: paddle.nn.functional.square_error_cost(Tensor([3, 4233601, 1, 2],"float64"), label=Tensor([3, 4233601, 1, 2],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([3, 4233601, 1, 2],"float64"), label=Tensor([3, 4233601, 1, 2],"float64"), ) 	 50803212 	 16765 	 12.46906566619873 	 12.42511248588562 	 0.38006591796875 	 0.37878894805908203 	 15.52889108657837 	 22.67038059234619 	 0.47333526611328125 	 0.2765367031097412 	 combined
2025-08-06 09:27:02.612123 test begin: paddle.nn.functional.square_error_cost(Tensor([5081, 100, 100],"float16"), Tensor([5081, 100, 100],"float32"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([5081, 100, 100],"float16"), Tensor([5081, 100, 100],"float32"), ) 	 101620000 	 16765 	 16.53688335418701 	 11.842737436294556 	 0.3361802101135254 	 0.36087822914123535 	 19.282289266586304 	 26.484115600585938 	 0.39175987243652344 	 0.26903867721557617 	 combined
2025-08-06 09:28:20.940984 test begin: paddle.nn.functional.square_error_cost(Tensor([5081, 100, 100],"float32"), Tensor([5081, 100, 100],"float32"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([5081, 100, 100],"float32"), Tensor([5081, 100, 100],"float32"), ) 	 101620000 	 16765 	 12.498926639556885 	 12.45604133605957 	 0.3810293674468994 	 0.37963366508483887 	 15.49426531791687 	 22.687140941619873 	 0.4721682071685791 	 0.27675914764404297 	 combined
2025-08-06 09:29:27.420357 test begin: paddle.nn.functional.square_error_cost(Tensor([6350401, 2, 1, 2],"float64"), label=Tensor([6350401, 2, 1, 2],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([6350401, 2, 1, 2],"float64"), label=Tensor([6350401, 2, 1, 2],"float64"), ) 	 50803208 	 16765 	 12.467529773712158 	 12.424793004989624 	 0.38004326820373535 	 0.37871813774108887 	 15.528059720993042 	 22.668558597564697 	 0.4732980728149414 	 0.2765500545501709 	 combined
2025-08-06 09:30:33.609264 test begin: paddle.nn.functional.square_error_cost(Tensor([8, 100, 63505],"float32"), Tensor([8, 100, 63505],"float32"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([8, 100, 63505],"float32"), Tensor([8, 100, 63505],"float32"), ) 	 101608000 	 16765 	 12.500643014907837 	 12.453718423843384 	 0.3810145854949951 	 0.37959885597229004 	 15.492295980453491 	 22.685332536697388 	 0.4721381664276123 	 0.27677297592163086 	 combined
2025-08-06 09:31:40.074878 test begin: paddle.nn.functional.square_error_cost(Tensor([8, 63505, 100],"float32"), Tensor([8, 63505, 100],"float32"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([8, 63505, 100],"float32"), Tensor([8, 63505, 100],"float32"), ) 	 101608000 	 16765 	 12.50773286819458 	 12.455225229263306 	 0.38097381591796875 	 0.3795931339263916 	 15.491018295288086 	 22.685375928878784 	 0.47209858894348145 	 0.27677178382873535 	 combined
2025-08-06 09:32:50.714471 test begin: paddle.nn.functional.tanh(Tensor([1016065, 50],"float32"), None, )
[Prof] paddle.nn.functional.tanh 	 paddle.nn.functional.tanh(Tensor([1016065, 50],"float32"), None, ) 	 50803250 	 33853 	 10.006486177444458 	 10.084450006484985 	 0.30208301544189453 	 0.3044588565826416 	 15.239707469940186 	 15.112857580184937 	 0.4601271152496338 	 0.4562375545501709 	 
2025-08-06 09:33:43.159075 test begin: paddle.nn.functional.tanh(Tensor([147015, 346],"float32"), None, )
[Prof] paddle.nn.functional.tanh 	 paddle.nn.functional.tanh(Tensor([147015, 346],"float32"), None, ) 	 50867190 	 33853 	 10.028480529785156 	 10.096842288970947 	 0.3027074337005615 	 0.30484914779663086 	 15.254022121429443 	 15.131674528121948 	 0.4605550765991211 	 0.4568197727203369 	 
2025-08-06 09:34:38.588708 test begin: paddle.nn.functional.tanh(Tensor([282600, 180],"float32"), None, )
[Prof] paddle.nn.functional.tanh 	 paddle.nn.functional.tanh(Tensor([282600, 180],"float32"), None, ) 	 50868000 	 33853 	 10.025267839431763 	 10.100374698638916 	 0.3025336265563965 	 0.30481505393981934 	 15.256710767745972 	 15.132666110992432 	 0.46055126190185547 	 0.45683860778808594 	 
2025-08-06 09:35:33.219424 test begin: paddle.nn.functional.tanh(Tensor([564481, 90],"float32"), None, )
[Prof] paddle.nn.functional.tanh 	 paddle.nn.functional.tanh(Tensor([564481, 90],"float32"), None, ) 	 50803290 	 33853 	 9.996645212173462 	 10.083914995193481 	 0.30184197425842285 	 0.30441975593566895 	 15.23868989944458 	 15.11286997795105 	 0.4601118564605713 	 0.4562497138977051 	 
2025-08-06 09:36:25.337338 test begin: paddle.nn.functional.tanh(Tensor([93401, 544],"float32"), None, )
[Prof] paddle.nn.functional.tanh 	 paddle.nn.functional.tanh(Tensor([93401, 544],"float32"), None, ) 	 50810144 	 33853 	 10.00553274154663 	 10.084489583969116 	 0.30207395553588867 	 0.3044154644012451 	 15.241100072860718 	 15.11492657661438 	 0.4601418972015381 	 0.45625734329223633 	 
2025-08-06 09:37:20.119704 test begin: paddle.nn.functional.tanhshrink(Tensor([2822401, 3, 3],"float64"), None, )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(Tensor([2822401, 3, 3],"float64"), None, ) 	 25401609 	 33845 	 10.10917329788208 	 25.140897512435913 	 0.30527639389038086 	 0.3796427249908447 	 15.159613609313965 	 40.09976053237915 	 0.4578545093536377 	 0.4036991596221924 	 
2025-08-06 09:38:51.880363 test begin: paddle.nn.functional.tanhshrink(Tensor([3, 2822401, 3],"float64"), None, )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(Tensor([3, 2822401, 3],"float64"), None, ) 	 25401609 	 33845 	 10.113917827606201 	 25.274351119995117 	 0.3052964210510254 	 0.37987256050109863 	 15.163187742233276 	 40.102548122406006 	 0.45799946784973145 	 0.4037594795227051 	 
2025-08-06 09:40:23.795051 test begin: paddle.nn.functional.tanhshrink(Tensor([3, 3, 2822401],"float64"), None, )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(Tensor([3, 3, 2822401],"float64"), None, ) 	 25401609 	 33845 	 10.109111785888672 	 25.14712119102478 	 0.30530261993408203 	 0.3797006607055664 	 15.159435987472534 	 40.098421812057495 	 0.45781779289245605 	 0.4036417007446289 	 
2025-08-06 09:41:57.960677 test begin: paddle.nn.functional.tanhshrink(Tensor([50803201],"float32"), None, )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(Tensor([50803201],"float32"), None, ) 	 50803201 	 33845 	 10.741193532943726 	 25.145046949386597 	 0.30188918113708496 	 0.37964391708374023 	 15.231125593185425 	 40.2379035949707 	 0.4598994255065918 	 0.4051363468170166 	 
2025-08-06 09:43:32.071654 test begin: paddle.nn.functional.tanhshrink(x=Tensor([2822401, 3, 3],"float64"), )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(x=Tensor([2822401, 3, 3],"float64"), ) 	 25401609 	 33845 	 10.109541893005371 	 25.14023518562317 	 0.3052990436553955 	 0.3795297145843506 	 15.159096479415894 	 40.09816384315491 	 0.4579505920410156 	 0.4036977291107178 	 
2025-08-06 09:45:03.664050 test begin: paddle.nn.functional.tanhshrink(x=Tensor([3, 2822401, 3],"float64"), )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(x=Tensor([3, 2822401, 3],"float64"), ) 	 25401609 	 33845 	 10.10917067527771 	 25.140138864517212 	 0.30527639389038086 	 0.3795452117919922 	 15.158470630645752 	 40.09864068031311 	 0.45766472816467285 	 0.4036884307861328 	 
2025-08-06 09:46:35.239079 test begin: paddle.nn.functional.tanhshrink(x=Tensor([3, 3, 2822401],"float64"), )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(x=Tensor([3, 3, 2822401],"float64"), ) 	 25401609 	 33845 	 10.629878759384155 	 25.139822959899902 	 0.305328369140625 	 0.3795807361602783 	 15.158442258834839 	 40.09733462333679 	 0.45788121223449707 	 0.4037508964538574 	 
2025-08-06 09:48:08.612179 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 28225, 3, 3],"float64"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 28225, 3, 3],"float64"), 1.0, 0.0, None, ) 	 25402500 	 33826 	 10.064493417739868 	 10.092880725860596 	 0.30408239364624023 	 0.30473995208740234 	 15.161402225494385 	 15.07991099357605 	 0.45815467834472656 	 0.45560479164123535 	 
2025-08-06 09:49:01.077402 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4, 21169, 3],"float64"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 4, 21169, 3],"float64"), 1.0, 0.0, None, ) 	 25402800 	 33826 	 10.079872131347656 	 10.081590175628662 	 0.3045156002044678 	 0.3046388626098633 	 15.155839920043945 	 15.079502582550049 	 0.4577755928039551 	 0.45569348335266113 	 
2025-08-06 09:49:52.610785 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3, 21169],"float64"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3, 21169],"float64"), 1.0, 0.0, None, ) 	 25402800 	 33826 	 10.079856395721436 	 10.081326961517334 	 0.3045003414154053 	 0.30454587936401367 	 15.157902002334595 	 15.079557657241821 	 0.4580655097961426 	 0.4556422233581543 	 
2025-08-06 09:50:44.955291 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3, 42337],"float32"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3, 42337],"float32"), 1.0, 0.0, None, ) 	 50804400 	 33826 	 10.008694648742676 	 10.0933678150177 	 0.30211400985717773 	 0.3043670654296875 	 15.220277070999146 	 15.103963851928711 	 0.45986461639404297 	 0.4563267230987549 	 
2025-08-06 09:51:39.475984 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4, 42337, 3],"float32"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 4, 42337, 3],"float32"), 1.0, 0.0, None, ) 	 50804400 	 33826 	 11.055317640304565 	 10.073307037353516 	 0.30208349227905273 	 0.3043398857116699 	 15.220088243484497 	 15.104090690612793 	 0.45984840393066406 	 0.45644378662109375 	 
2025-08-06 09:52:33.732677 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 56449, 3, 3],"float32"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 56449, 3, 3],"float32"), 1.0, 0.0, None, ) 	 50804100 	 33826 	 10.000740051269531 	 10.073393821716309 	 0.3021104335784912 	 0.3043355941772461 	 15.22130298614502 	 15.104835748672485 	 0.45981717109680176 	 0.45632481575012207 	 
2025-08-06 09:53:25.828880 test begin: paddle.nn.functional.thresholded_relu(Tensor([1411201, 4, 3, 3],"float32"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([1411201, 4, 3, 3],"float32"), 1.0, 0.0, None, ) 	 50803236 	 33826 	 10.003347396850586 	 10.073014974594116 	 0.30222177505493164 	 0.3043351173400879 	 15.226287841796875 	 15.105117559432983 	 0.46007442474365234 	 0.4564089775085449 	 
2025-08-06 09:54:18.012956 test begin: paddle.nn.functional.thresholded_relu(Tensor([705601, 4, 3, 3],"float64"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([705601, 4, 3, 3],"float64"), 1.0, 0.0, None, ) 	 25401636 	 33826 	 10.07667851448059 	 11.56567931175232 	 0.304445743560791 	 0.30461859703063965 	 15.155693054199219 	 15.078498601913452 	 0.4579486846923828 	 0.4555809497833252 	 
2025-08-06 09:55:11.418892 test begin: paddle.nn.functional.thresholded_relu(x=Tensor([100, 4, 3, 42337],"float32"), )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(x=Tensor([100, 4, 3, 42337],"float32"), ) 	 50804400 	 33826 	 10.00123143196106 	 10.085216999053955 	 0.3020968437194824 	 0.30437326431274414 	 15.222060918807983 	 15.103923797607422 	 0.4597597122192383 	 0.4562969207763672 	 
2025-08-06 09:56:05.459939 test begin: paddle.nn.functional.thresholded_relu(x=Tensor([100, 4, 42337, 3],"float32"), )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(x=Tensor([100, 4, 42337, 3],"float32"), ) 	 50804400 	 33826 	 10.004595756530762 	 10.07365345954895 	 0.30211400985717773 	 0.30429697036743164 	 15.218967199325562 	 15.104244470596313 	 0.45978283882141113 	 0.4563765525817871 	 
2025-08-06 09:56:57.692502 test begin: paddle.nn.functional.thresholded_relu(x=Tensor([100, 56449, 3, 3],"float32"), )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(x=Tensor([100, 56449, 3, 3],"float32"), ) 	 50804100 	 33826 	 9.997961521148682 	 10.073110580444336 	 0.30207228660583496 	 0.30432963371276855 	 15.220284461975098 	 15.105242013931274 	 0.45983409881591797 	 0.45642876625061035 	 
2025-08-06 09:57:50.645769 test begin: paddle.nn.functional.thresholded_relu(x=Tensor([1411201, 4, 3, 3],"float32"), )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(x=Tensor([1411201, 4, 3, 3],"float32"), ) 	 50803236 	 33826 	 10.002968788146973 	 10.089703559875488 	 0.3022277355194092 	 0.30431175231933594 	 15.226237773895264 	 15.105059146881104 	 0.4599893093109131 	 0.4563639163970947 	 
2025-08-06 09:58:44.487699 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="mean", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="mean", name=None, ) 	 76204815 	 4341 	 10.07553744316101 	 7.903658866882324 	 4.076957702636719e-05 	 0.1547393798828125 	 17.662052154541016 	 12.443875312805176 	 0.4638652801513672 	 0.18344736099243164 	 
2025-08-06 09:59:34.860109 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="none", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="none", name=None, ) 	 76204815 	 4341 	 10.120506525039673 	 7.88328218460083 	 4.553794860839844e-05 	 0.16846370697021484 	 17.652297735214233 	 12.42850399017334 	 0.520453691482544 	 0.19542455673217773 	 
2025-08-06 10:00:27.268182 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="sum", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="sum", name=None, ) 	 76204815 	 4341 	 10.131346464157104 	 7.8989646434783936 	 4.4345855712890625e-05 	 0.15462923049926758 	 17.667139530181885 	 12.42875337600708 	 0.4638633728027344 	 0.1953747272491455 	 
2025-08-06 10:01:20.263185 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="mean", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="mean", name=None, ) 	 76204815 	 4341 	 12.071819067001343 	 9.692720413208008 	 0.00017523765563964844 	 0.20724129676818848 	 19.17719554901123 	 14.133805751800537 	 0.5033512115478516 	 0.20834088325500488 	 
2025-08-06 10:02:16.941869 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="none", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="none", name=None, ) 	 76204815 	 4341 	 11.911679744720459 	 9.546941041946411 	 0.0001544952392578125 	 0.2490687370300293 	 19.036218881607056 	 13.991710662841797 	 0.5611729621887207 	 0.21988797187805176 	 
2025-08-06 10:03:14.006510 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="sum", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="sum", name=None, ) 	 76204815 	 4341 	 12.05890703201294 	 9.696808338165283 	 0.00017380714416503906 	 0.20723843574523926 	 19.176494359970093 	 13.8907630443573 	 0.5034000873565674 	 0.21831512451171875 	 
2025-08-06 10:04:10.413327 test begin: paddle.nn.functional.unfold(Tensor([10, 1241, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 1241, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), ) 	 50831360 	 5718 	 9.990765810012817 	 15.08741283416748 	 0.17857837677001953 	 0.245072603225708 	 21.34636664390564 	 40.98008370399475 	 0.34703636169433594 	 7.32564902305603 	 
2025-08-06 10:05:45.880454 test begin: paddle.nn.functional.unfold(Tensor([10, 1241, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 1241, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, ) 	 50831360 	 5718 	 9.989484548568726 	 15.086915254592896 	 0.17855310440063477 	 0.245042085647583 	 21.347670793533325 	 40.97818565368652 	 0.347043514251709 	 7.324175834655762 	 
2025-08-06 10:07:21.481074 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 26461, 64],"float32"), 3, 1, 1, tuple(1,1,), )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 3, 26461, 64],"float32"), 3, 1, 1, tuple(1,1,), ) 	 50805120 	 5718 	 10.440840721130371 	 15.13519024848938 	 0.18657875061035156 	 0.22542357444763184 	 21.572552919387817 	 41.24918508529663 	 0.350726842880249 	 7.372259616851807 	 
2025-08-06 10:08:58.918734 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 26461, 64],"float32"), 3, 1, tuple(1,1,), 1, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 3, 26461, 64],"float32"), 3, 1, tuple(1,1,), 1, ) 	 50805120 	 5718 	 10.44052004814148 	 15.144096612930298 	 0.1866309642791748 	 0.22547674179077148 	 21.571967124938965 	 41.24731087684631 	 0.3507518768310547 	 7.372304201126099 	 
2025-08-06 10:10:38.082815 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 64, 26461],"float32"), 3, 1, 1, tuple(1,1,), )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 3, 64, 26461],"float32"), 3, 1, 1, tuple(1,1,), ) 	 50805120 	 5718 	 13.323077917098999 	 17.056488752365112 	 0.22030138969421387 	 0.2540907859802246 	 21.188393354415894 	 40.52389192581177 	 0.344498872756958 	 7.242951154708862 	 
2025-08-06 10:12:18.743557 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 64, 26461],"float32"), 3, 1, tuple(1,1,), 1, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 3, 64, 26461],"float32"), 3, 1, tuple(1,1,), 1, ) 	 50805120 	 5718 	 12.325852394104004 	 17.056634426116943 	 0.22036433219909668 	 0.25403547286987305 	 21.187885284423828 	 40.5235071182251 	 0.3444998264312744 	 7.242915868759155 	 
2025-08-06 10:13:59.059743 test begin: paddle.nn.functional.unfold(Tensor([338, 3, 224, 224],"float32"), 16, 16, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([338, 3, 224, 224],"float32"), 16, 16, ) 	 50878464 	 5718 	 138.5218768119812 	 138.09500217437744 	 0.07323598861694336 	 0.07275390625 	 18.78914213180542 	 12.051876783370972 	 0.009898662567138672 	 2.1540889739990234 	 
2025-08-06 10:19:10.134234 test begin: paddle.nn.functional.unfold(Tensor([4135, 3, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff71e8d67d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:29:15.191333 test begin: paddle.nn.functional.unfold(Tensor([4135, 3, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, )
W0806 10:29:16.192797 137175 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc60689b070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:39:19.928327 test begin: paddle.nn.functional.unfold(Tensor([64, 16, 224, 224],"float32"), 16, 16, )
W0806 10:39:20.926205 137795 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([64, 16, 224, 224],"float32"), 16, 16, ) 	 51380224 	 5718 	 87.94788336753845 	 45.15240025520325 	 0.24562525749206543 	 0.12608122825622559 	 10.71496295928955 	 12.168012142181396 	 0.029359102249145508 	 2.174957275390625 	 
2025-08-06 10:41:58.258130 test begin: paddle.nn.functional.unfold(Tensor([64, 3, 1182, 224],"float32"), 16, 16, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([64, 3, 1182, 224],"float32"), 16, 16, ) 	 50835456 	 5718 	 87.76015615463257 	 46.583176374435425 	 0.24509930610656738 	 0.1255495548248291 	 10.648905277252197 	 13.754971981048584 	 0.029192209243774414 	 1.2292284965515137 	 
2025-08-06 10:44:41.118604 test begin: paddle.nn.functional.unfold(Tensor([64, 3, 224, 1182],"float32"), 16, 16, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([64, 3, 224, 1182],"float32"), 16, 16, ) 	 50835456 	 5718 	 90.14053535461426 	 45.558581590652466 	 0.25171971321105957 	 0.12723708152770996 	 10.671817779541016 	 13.768212080001831 	 0.02925276756286621 	 1.2303657531738281 	 
2025-08-06 10:47:22.966819 test begin: paddle.nn.functional.zeropad2d(Tensor([338, 3, 224, 224],"float32"), list[2,2,2,2,], )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:130: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.common.zeropad2d" is deprecated since 3.0.0, and will be removed in future versions. Please use "paddle.nn.ZeroPad2D" instead.
    Reason: Please use class ZeroPad2D [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:148: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.common.zeropad2d" is deprecated since 3.0.0, and will be removed in future versions. Please use "paddle.nn.ZeroPad2D" instead.
    Reason: Please use class ZeroPad2D [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([338, 3, 224, 224],"float32"), list[2,2,2,2,], ) 	 50878464 	 31313 	 19.686031818389893 	 14.658791065216064 	 0.6424763202667236 	 0.23920249938964844 	 23.486896991729736 	 9.753786325454712 	 0.3830575942993164 	 0.3183131217956543 	 combined
2025-08-06 10:48:32.337676 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 127, 224, 224],"float64"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 127, 224, 224],"float64"), list[2,2,2,2,], ) 	 25489408 	 31313 	 10.137139320373535 	 14.372443675994873 	 0.33086490631103516 	 0.23448777198791504 	 14.397341251373291 	 9.513116359710693 	 0.23477435111999512 	 0.3104557991027832 	 combined
2025-08-06 10:49:21.910518 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 127, 224, 224],"int64"), Tensor([4],"int32"), )
W0806 10:49:34.390527 138334 backward.cc:462] While running Node (Pad3dGradNode) raises an EnforceNotMet exception
[Error] (NotFound) The kernel with key (GPU, Undefined(AnyLayout), int64) of kernel `pad3d_grad` is not registered and fail to fallback to CPU one. Selected wrong DataType `int64`. Paddle support following DataTypes: float64, complex128, float16, float32, complex64, bfloat16.
  [Hint: Expected kernel_iter != iter->second.end(), but received kernel_iter == iter->second.end().] (at ../paddle/phi/core/kernel_factory.cc:380)

[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 127, 224, 224],"int64"), Tensor([4],"int32"), ) 	 25489412 	 31313 	 11.748111724853516 	 16.458919763565063 	 0.0003192424774169922 	 0.0004341602325439453 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 10:49:51.070727 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 254, 224, 224],"float32"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 254, 224, 224],"float32"), list[2,2,2,2,], ) 	 50978816 	 31313 	 19.72010636329651 	 14.69163703918457 	 0.64365553855896 	 0.2396090030670166 	 23.52980399131775 	 9.775073051452637 	 0.38405394554138184 	 0.31905364990234375 	 combined
2025-08-06 10:51:02.958987 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 3, 18901, 224],"float32"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 3, 18901, 224],"float32"), list[2,2,2,2,], ) 	 50805888 	 31313 	 19.339406967163086 	 14.554570198059082 	 0.6312310695648193 	 0.23747587203979492 	 23.451332569122314 	 9.738014698028564 	 0.3825531005859375 	 0.3178248405456543 	 combined
2025-08-06 10:52:11.837548 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 18901],"float32"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 18901],"float32"), list[2,2,2,2,], ) 	 50805888 	 31313 	 19.299352645874023 	 13.917569398880005 	 0.6298191547393799 	 0.22709226608276367 	 23.47375202178955 	 9.657321214675903 	 0.38297390937805176 	 0.3151359558105469 	 combined
2025-08-06 10:53:20.000847 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 9451],"float64"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 9451],"float64"), list[2,2,2,2,], ) 	 25404288 	 31313 	 9.995858669281006 	 13.78447961807251 	 0.326246976852417 	 0.2249610424041748 	 14.310678720474243 	 9.393137693405151 	 0.23348546028137207 	 0.3065662384033203 	 combined
2025-08-06 10:54:08.642423 test begin: paddle.nonzero(Tensor([510, 128, 28, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([510, 128, 28, 28],"float32"), ) 	 51179520 	 1360 	 10.060979127883911 	 3.1651523113250732 	 0.005443096160888672 	 0.0021245479583740234 	 None 	 None 	 None 	 None 	 
2025-08-06 10:54:22.749587 test begin: paddle.nonzero(Tensor([510, 80, 28, 45],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([510, 80, 28, 45],"float32"), ) 	 51408000 	 1360 	 10.117000341415405 	 3.186868667602539 	 0.00546717643737793 	 0.0021736621856689453 	 None 	 None 	 None 	 None 	 
2025-08-06 10:54:38.936829 test begin: paddle.nonzero(Tensor([510, 80, 45, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([510, 80, 45, 28],"float32"), ) 	 51408000 	 1360 	 10.991444826126099 	 3.1937997341156006 	 0.0054852962493896484 	 0.0021445751190185547 	 None 	 None 	 None 	 None 	 
2025-08-06 10:54:56.620984 test begin: paddle.nonzero(Tensor([511, 127, 28, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([511, 127, 28, 28],"float32"), ) 	 50879248 	 1360 	 10.073920965194702 	 3.174215793609619 	 0.0054438114166259766 	 0.0021746158599853516 	 None 	 None 	 None 	 None 	 
2025-08-06 10:55:10.734238 test begin: paddle.nonzero(Tensor([511, 80, 28, 45],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([511, 80, 28, 45],"float32"), ) 	 51508800 	 1360 	 10.12097978591919 	 3.1939289569854736 	 0.005507946014404297 	 0.0021317005157470703 	 None 	 None 	 None 	 None 	 
2025-08-06 10:55:24.891875 test begin: paddle.nonzero(Tensor([511, 80, 45, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([511, 80, 45, 28],"float32"), ) 	 51508800 	 1360 	 10.12851333618164 	 3.210838794708252 	 0.005474567413330078 	 0.002130746841430664 	 None 	 None 	 None 	 None 	 
2025-08-06 10:55:42.885549 test begin: paddle.nonzero(Tensor([512, 127, 28, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([512, 127, 28, 28],"float32"), ) 	 50978816 	 1360 	 10.102132558822632 	 3.523871421813965 	 0.005435466766357422 	 0.0021080970764160156 	 None 	 None 	 None 	 None 	 
2025-08-06 10:56:01.833706 test begin: paddle.nonzero(Tensor([512, 80, 28, 45],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([512, 80, 28, 45],"float32"), ) 	 51609600 	 1360 	 10.140637159347534 	 3.1931490898132324 	 0.005504608154296875 	 0.0021355152130126953 	 None 	 None 	 None 	 None 	 
2025-08-06 10:56:16.086508 test begin: paddle.nonzero(Tensor([512, 80, 45, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([512, 80, 45, 28],"float32"), ) 	 51609600 	 1360 	 10.157207727432251 	 3.193044662475586 	 0.005507707595825195 	 0.002157926559448242 	 None 	 None 	 None 	 None 	 
2025-08-06 10:56:30.315934 test begin: paddle.nonzero(Tensor([811, 80, 28, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([811, 80, 28, 28],"float32"), ) 	 50865920 	 1360 	 10.025866746902466 	 3.1708767414093018 	 0.005427122116088867 	 0.002147197723388672 	 None 	 None 	 None 	 None 	 
2025-08-06 10:56:46.345398 test begin: paddle.not_equal(Tensor([13, 15266, 16, 4, 1],"int64"), Tensor([13, 15266, 16, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 15266, 16, 4, 1],"int64"), Tensor([13, 15266, 16, 1, 8],"int64"), ) 	 38103936 	 46945 	 25.88729238510132 	 24.172131299972534 	 0.5635509490966797 	 0.5262062549591064 	 None 	 None 	 None 	 None 	 
2025-08-06 10:57:39.642177 test begin: paddle.not_equal(Tensor([13, 2, 122124, 4, 1],"int64"), Tensor([13, 2, 122124, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 122124, 4, 1],"int64"), Tensor([13, 2, 122124, 1, 8],"int64"), ) 	 38102688 	 46945 	 26.317028760910034 	 24.169868230819702 	 0.568842887878418 	 0.5261509418487549 	 None 	 None 	 None 	 None 	 
2025-08-06 10:58:32.693457 test begin: paddle.not_equal(Tensor([13, 2, 16, 4, 15266],"int64"), Tensor([13, 2, 16, 1, 15266],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 16, 4, 15266],"int64"), Tensor([13, 2, 16, 1, 15266],"int64"), ) 	 31753280 	 46945 	 10.01493525505066 	 10.304972410202026 	 0.21801114082336426 	 0.2241532802581787 	 None 	 None 	 None 	 None 	 
2025-08-06 10:58:53.779884 test begin: paddle.not_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 61062],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 61062],"int64"), ) 	 25403456 	 46945 	 26.413491010665894 	 22.132144927978516 	 0.5749833583831787 	 0.48183107376098633 	 None 	 None 	 None 	 None 	 
2025-08-06 10:59:42.828257 test begin: paddle.not_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), ) 	 25405120 	 46945 	 48.83583855628967 	 44.943723917007446 	 1.063051700592041 	 0.9688265323638916 	 None 	 None 	 None 	 None 	 
2025-08-06 11:01:19.600128 test begin: paddle.not_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 61062, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 61062, 8],"int64"), ) 	 228616128 	 46945 	 83.82059216499329 	 73.73106932640076 	 1.824814796447754 	 1.6051113605499268 	 None 	 None 	 None 	 None 	 
2025-08-06 11:04:01.074001 test begin: paddle.not_equal(Tensor([13, 2, 244247, 4, 1],"int64"), Tensor([13, 2, 244247, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 244247, 4, 1],"int64"), Tensor([13, 2, 244247, 1, 8],"int64"), ) 	 76205064 	 46945 	 51.907620906829834 	 48.06555914878845 	 1.1300370693206787 	 1.0461103916168213 	 None 	 None 	 None 	 None 	 
2025-08-06 11:05:43.921811 test begin: paddle.not_equal(Tensor([13, 30531, 16, 4, 1],"int64"), Tensor([13, 30531, 16, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 30531, 16, 4, 1],"int64"), Tensor([13, 30531, 16, 1, 8],"int64"), ) 	 76205376 	 46945 	 51.48922038078308 	 49.2886483669281 	 1.1208603382110596 	 1.0460686683654785 	 None 	 None 	 None 	 None 	 
2025-08-06 11:07:29.679814 test begin: paddle.not_equal(Tensor([198451, 2, 16, 4, 1],"int64"), Tensor([198451, 2, 16, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([198451, 2, 16, 4, 1],"int64"), Tensor([198451, 2, 16, 1, 8],"int64"), ) 	 76205184 	 46945 	 51.468958616256714 	 48.050512075424194 	 1.1204473972320557 	 1.0460872650146484 	 None 	 None 	 None 	 None 	 
2025-08-06 11:09:10.785442 test begin: paddle.not_equal(Tensor([25401601],"int64"), Tensor([25401601],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([25401601],"int64"), Tensor([25401601],"int64"), ) 	 50803202 	 46945 	 14.550898313522339 	 14.692894220352173 	 0.3167564868927002 	 0.3198425769805908 	 None 	 None 	 None 	 None 	 
2025-08-06 11:09:42.692720 test begin: paddle.not_equal(Tensor([99226, 2, 16, 4, 1],"int64"), Tensor([99226, 2, 16, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([99226, 2, 16, 4, 1],"int64"), Tensor([99226, 2, 16, 1, 8],"int64"), ) 	 38102784 	 46945 	 26.07691764831543 	 24.168630599975586 	 0.5675356388092041 	 0.5261640548706055 	 None 	 None 	 None 	 None 	 
2025-08-06 11:10:33.759326 test begin: paddle.numel(Tensor([508032010],"float32"), )
[Prof] paddle.numel 	 paddle.numel(Tensor([508032010],"float32"), ) 	 508032010 	 1155041 	 9.90943694114685 	 31.905421257019043 	 5.602836608886719e-05 	 0.00020170211791992188 	 None 	 None 	 None 	 None 	 
2025-08-06 11:11:25.796206 test begin: paddle.ones_like(Tensor([144, 392, 901],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([144, 392, 901],"float32"), ) 	 50859648 	 74688 	 10.018893718719482 	 10.469237089157104 	 0.13704228401184082 	 0.1372828483581543 	 None 	 None 	 None 	 None 	 
2025-08-06 11:11:49.331363 test begin: paddle.ones_like(Tensor([144, 901, 392],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([144, 901, 392],"float32"), ) 	 50859648 	 74688 	 10.020525217056274 	 10.023849487304688 	 0.13707756996154785 	 0.13721370697021484 	 None 	 None 	 None 	 None 	 
2025-08-06 11:12:10.187037 test begin: paddle.ones_like(Tensor([160, 392, 811],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([160, 392, 811],"float32"), ) 	 50865920 	 74688 	 10.02703595161438 	 10.024200201034546 	 0.13705015182495117 	 0.13712549209594727 	 None 	 None 	 None 	 None 	 
2025-08-06 11:12:31.048045 test begin: paddle.ones_like(Tensor([160, 811, 392],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([160, 811, 392],"float32"), ) 	 50865920 	 74688 	 10.02430510520935 	 10.041125297546387 	 0.1377124786376953 	 0.13714385032653809 	 None 	 None 	 None 	 None 	 
2025-08-06 11:12:53.314769 test begin: paddle.ones_like(Tensor([176, 392, 737],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([176, 392, 737],"float32"), ) 	 50847104 	 74688 	 10.01148533821106 	 10.021186113357544 	 0.13693690299987793 	 0.13708829879760742 	 None 	 None 	 None 	 None 	 
2025-08-06 11:13:14.198415 test begin: paddle.ones_like(Tensor([176, 737, 392],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([176, 737, 392],"float32"), ) 	 50847104 	 74688 	 10.013709306716919 	 10.718601703643799 	 0.13692951202392578 	 0.137054443359375 	 None 	 None 	 None 	 None 	 
2025-08-06 11:13:38.990468 test begin: paddle.ones_like(Tensor([331, 392, 392],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([331, 392, 392],"float32"), ) 	 50862784 	 74688 	 10.017442464828491 	 10.043874740600586 	 0.13705134391784668 	 0.1370840072631836 	 None 	 None 	 None 	 None 	 
2025-08-06 11:13:59.915598 test begin: paddle.outer(Tensor([50803201],"float32"), Tensor([2],"float32"), )
[Prof] paddle.outer 	 paddle.outer(Tensor([50803201],"float32"), Tensor([2],"float32"), ) 	 50803203 	 1897 	 10.021430253982544 	 0.982424259185791 	 0.11017489433288574 	 0.5291764736175537 	 7.099447011947632 	 4.956003427505493 	 1.2740488052368164 	 0.5331745147705078 	 
2025-08-06 11:14:25.567243 test begin: paddle.outer(Tensor([50803201],"float32"), Tensor([3],"float32"), )
[Prof] paddle.outer 	 paddle.outer(Tensor([50803201],"float32"), Tensor([3],"float32"), ) 	 50803204 	 1897 	 10.034962177276611 	 1.3763651847839355 	 0.1103219985961914 	 0.7413361072540283 	 8.690388441085815 	 6.552234888076782 	 1.5595037937164307 	 0.7050321102142334 	 
2025-08-06 11:14:55.923012 test begin: paddle.outer(Tensor([50803201],"float32"), Tensor([4],"float32"), )
[Prof] paddle.outer 	 paddle.outer(Tensor([50803201],"float32"), Tensor([4],"float32"), ) 	 50803205 	 1897 	 10.040723323822021 	 1.7920911312103271 	 0.1103677749633789 	 0.9652817249298096 	 8.368868827819824 	 9.60003137588501 	 1.5008201599121094 	 1.0328443050384521 	 
2025-08-06 11:15:30.198857 test begin: paddle.pdist(Tensor([10, 5080321],"float32"), 0, )
[Prof] paddle.pdist 	 paddle.pdist(Tensor([10, 5080321],"float32"), 0, ) 	 50803210 	 1686 	 9.993268728256226 	 15.191729068756104 	 4.3392181396484375e-05 	 9.207778692245483 	 9.490274429321289 	 0.22617626190185547 	 0.0055370330810546875 	 0.11199641227722168 	 
2025-08-06 11:16:07.467403 test begin: paddle.pdist(Tensor([10, 5080321],"float32"), 1.0, )
[Prof] paddle.pdist 	 paddle.pdist(Tensor([10, 5080321],"float32"), 1.0, ) 	 50803210 	 1686 	 9.967844247817993 	 14.116144895553589 	 4.410743713378906e-05 	 8.556523323059082 	 38.43506336212158 	 22.874741554260254 	 0.022667884826660156 	 6.932105541229248 	 
2025-08-06 11:17:34.798164 test begin: paddle.pdist(Tensor([50, 508033],"float64"), 2.0, )
[Prof] paddle.pdist 	 paddle.pdist(Tensor([50, 508033],"float64"), 2.0, ) 	 25401650 	 1686 	 39.21766710281372 	 3.6171207427978516 	 4.363059997558594e-05 	 2.1924891471862793 	 148.1926875114441 	 66.28026556968689 	 0.08772420883178711 	 4.448455095291138 	 
2025-08-06 11:21:54.158639 test begin: paddle.polar(Tensor([1, 793801, 64],"float32"), Tensor([1, 793801, 64],"float32"), )
[Prof] paddle.polar 	 paddle.polar(Tensor([1, 793801, 64],"float32"), Tensor([1, 793801, 64],"float32"), ) 	 101606528 	 8265 	 17.179619550704956 	 17.167497158050537 	 0.42491793632507324 	 0.4240732192993164 	 38.15463423728943 	 41.62769532203674 	 0.6740126609802246 	 0.46781444549560547 	 combined
2025-08-06 11:23:52.143808 test begin: paddle.polar(Tensor([1, 8192, 6202],"float32"), Tensor([1, 8192, 6202],"float32"), )
[Prof] paddle.polar 	 paddle.polar(Tensor([1, 8192, 6202],"float32"), Tensor([1, 8192, 6202],"float32"), ) 	 101613568 	 8265 	 20.671362161636353 	 17.14163064956665 	 0.42510557174682617 	 0.4241752624511719 	 38.195372104644775 	 41.63380265235901 	 0.6746435165405273 	 0.4678354263305664 	 combined
2025-08-06 11:25:52.562341 test begin: paddle.polar(Tensor([1, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), )
[Prof] paddle.polar 	 paddle.polar(Tensor([1, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), ) 	 51380224 	 8265 	 14.834567070007324 	 14.921838760375977 	 0.3670539855957031 	 0.36879634857177734 	 28.93918228149414 	 38.4372296333313 	 0.39717960357666016 	 0.36542773246765137 	 combined
2025-08-06 11:27:33.198340 test begin: paddle.polar(Tensor([97, 8192, 64],"float32"), Tensor([1, 8192, 64],"float32"), )
[Prof] paddle.polar 	 paddle.polar(Tensor([97, 8192, 64],"float32"), Tensor([1, 8192, 64],"float32"), ) 	 51380224 	 8265 	 9.998994588851929 	 10.032640218734741 	 0.24768519401550293 	 0.24847936630249023 	 21.581836462020874 	 23.92465353012085 	 0.2956423759460449 	 0.22739005088806152 	 combined
2025-08-06 11:28:41.655167 test begin: paddle.polar(Tensor([97, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), )
[Prof] paddle.polar 	 paddle.polar(Tensor([97, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), ) 	 101711872 	 8265 	 19.4552583694458 	 17.17058539390564 	 0.4256880283355713 	 0.4245483875274658 	 38.239869832992554 	 41.67149758338928 	 0.6754021644592285 	 0.46823763847351074 	 combined
2025-08-06 11:30:43.946174 test begin: paddle.polygamma(Tensor([10, 20, 254017],"float32"), 1, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([10, 20, 254017],"float32"), 1, ) 	 50803400 	 1793 	 9.99592638015747 	 1.1258783340454102 	 5.697604417800903 	 0.6403810977935791 	 16.275540590286255 	 18.388378143310547 	 9.277160167694092 	 5.240524768829346 	 
2025-08-06 11:31:33.225291 test begin: paddle.polygamma(Tensor([10, 5080321, 1],"float32"), 1, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([10, 5080321, 1],"float32"), 1, ) 	 50803210 	 1793 	 9.99598479270935 	 1.1237566471099854 	 5.697611331939697 	 0.6404082775115967 	 16.270954132080078 	 18.38942289352417 	 9.27431869506836 	 5.240953207015991 	 
2025-08-06 11:32:20.720868 test begin: paddle.polygamma(Tensor([2, 12700801],"float64"), 1, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([2, 12700801],"float64"), 1, ) 	 25401602 	 1793 	 16.66876459121704 	 1.1479125022888184 	 9.54095196723938 	 0.6542015075683594 	 20.477593660354614 	 38.51856470108032 	 11.672093152999878 	 10.977954626083374 	 
2025-08-06 11:33:41.756966 test begin: paddle.polygamma(Tensor([2, 2, 6350401],"float64"), 2, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([2, 2, 6350401],"float64"), 2, ) 	 25401604 	 1793 	 20.364471435546875 	 37.74759888648987 	 11.607781410217285 	 21.516388654708862 	 16.35282278060913 	 16.63998770713806 	 9.321094751358032 	 4.741297245025635 	 
2025-08-06 11:35:14.059724 test begin: paddle.polygamma(Tensor([2, 2116801, 6],"float64"), 2, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([2, 2116801, 6],"float64"), 2, ) 	 25401612 	 1793 	 20.3669593334198 	 37.748286724090576 	 11.608991146087646 	 21.517237424850464 	 16.352701663970947 	 16.635918855667114 	 9.320730686187744 	 4.739034414291382 	 
2025-08-06 11:36:46.428636 test begin: paddle.polygamma(Tensor([2116801, 2, 6],"float64"), 2, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([2116801, 2, 6],"float64"), 2, ) 	 25401612 	 1793 	 20.36441397666931 	 37.73754930496216 	 11.607714653015137 	 21.50982403755188 	 16.354085683822632 	 16.6413471698761 	 9.32075023651123 	 4.74264121055603 	 
2025-08-06 11:38:18.741229 test begin: paddle.polygamma(Tensor([2540161, 20, 1],"float32"), 1, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([2540161, 20, 1],"float32"), 1, ) 	 50803220 	 1793 	 9.996114015579224 	 1.1237168312072754 	 5.697625398635864 	 0.6404099464416504 	 16.275518894195557 	 18.392312049865723 	 9.276134967803955 	 5.241669178009033 	 
2025-08-06 11:39:07.192084 test begin: paddle.polygamma(Tensor([4233601, 6],"float64"), 1, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([4233601, 6],"float64"), 1, ) 	 25401606 	 1793 	 16.576152324676514 	 1.1476635932922363 	 9.448361158370972 	 0.6540224552154541 	 20.47658348083496 	 38.54120588302612 	 11.671226501464844 	 10.985324382781982 	 
2025-08-06 11:40:25.106374 test begin: paddle.positive(Tensor([100, 5080321],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([100, 5080321],"float32"), ) 	 508032100 	 5479885 	 9.40549898147583 	 1.042182207107544 	 0.00013017654418945312 	 0.00010442733764648438 	 186.748028755188 	 263.5865228176117 	 0.00013065338134765625 	 0.000213623046875 	 combined
2025-08-06 11:48:22.529613 test begin: paddle.positive(Tensor([16934410, 3, 4, 5],"float16"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([16934410, 3, 4, 5],"float16"), ) 	 1016064600 	 5479885 	 13.439031600952148 	 1.0507197380065918 	 0.00013399124145507812 	 5.2928924560546875e-05 	 164.71264672279358 	 268.94411039352417 	 0.00014162063598632812 	 0.0002281665802001953 	 combined
2025-08-06 11:56:30.526174 test begin: paddle.positive(Tensor([20, 1270081, 4, 5],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 1270081, 4, 5],"float32"), ) 	 508032400 	 5479885 	 9.717644214630127 	 1.0462419986724854 	 0.00011181831359863281 	 0.00010085105895996094 	 165.66981172561646 	 270.16276454925537 	 0.00010800361633300781 	 0.0002129077911376953 	 combined
2025-08-06 12:04:15.255160 test begin: paddle.positive(Tensor([20, 2540161, 4, 5],"float16"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 2540161, 4, 5],"float16"), ) 	 1016064400 	 5479885 	 10.172019004821777 	 1.0577378273010254 	 0.0001404285430908203 	 0.0003001689910888672 	 186.42224621772766 	 269.66115832328796 	 0.00010943412780761719 	 0.0002181529998779297 	 combined
2025-08-06 12:12:44.795361 test begin: paddle.positive(Tensor([20, 3, 1693441, 5],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 3, 1693441, 5],"float32"), ) 	 508032300 	 5479885 	 13.669755220413208 	 1.050985336303711 	 0.00013136863708496094 	 0.00023698806762695312 	 179.8177409172058 	 271.95132875442505 	 0.00011157989501953125 	 0.00021457672119140625 	 combined
2025-08-06 12:20:48.592719 test begin: paddle.positive(Tensor([20, 3, 3386881, 5],"float16"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 3, 3386881, 5],"float16"), ) 	 1016064300 	 5479885 	 9.696570873260498 	 1.025221824645996 	 9.799003601074219e-05 	 7.724761962890625e-05 	 161.24363899230957 	 267.4267694950104 	 0.00011229515075683594 	 0.00021338462829589844 	 combined
2025-08-06 12:28:45.876903 test begin: paddle.positive(Tensor([20, 3, 4, 2116801],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 3, 4, 2116801],"float32"), ) 	 508032240 	 5479885 	 9.576998710632324 	 1.034977912902832 	 5.0067901611328125e-05 	 8.96453857421875e-05 	 161.81493043899536 	 270.0922646522522 	 0.00011515617370605469 	 0.00024247169494628906 	 combined
2025-08-06 12:36:24.982549 test begin: paddle.positive(Tensor([20, 3, 4, 4233601],"float16"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([20, 3, 4, 4233601],"float16"), ) 	 1016064240 	 5479885 	 9.486510038375854 	 1.0216736793518066 	 0.00010037422180175781 	 2.47955322265625e-05 	 160.83710193634033 	 267.86000871658325 	 0.00010132789611816406 	 0.00021529197692871094 	 combined
2025-08-06 12:44:22.029720 test begin: paddle.positive(Tensor([496130, 1024],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([496130, 1024],"float32"), ) 	 508037120 	 5479885 	 9.636780261993408 	 1.029771327972412 	 0.0001323223114013672 	 0.0001804828643798828 	 162.2632873058319 	 272.4709312915802 	 0.00014519691467285156 	 0.0002422332763671875 	 combined
2025-08-06 12:52:06.215635 test begin: paddle.positive(Tensor([8467210, 3, 4, 5],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([8467210, 3, 4, 5],"float32"), ) 	 508032600 	 5479885 	 9.72706151008606 	 1.0198044776916504 	 0.0001468658447265625 	 0.00011682510375976562 	 188.61366271972656 	 314.78959131240845 	 0.00037980079650878906 	 0.00023889541625976562 	 combined
2025-08-06 13:00:59.074854 test begin: paddle.pow(Tensor([1024, 1024, 25],"float64"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([1024, 1024, 25],"float64"), 2, ) 	 26214400 	 27143 	 16.024546146392822 	 8.347639322280884 	 0.6035325527191162 	 0.3144092559814453 	 16.906053066253662 	 29.437631607055664 	 0.6365044116973877 	 0.3694913387298584 	 
2025-08-06 13:02:11.731610 test begin: paddle.pow(Tensor([1024, 1024, 49],"float32"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([1024, 1024, 49],"float32"), 2, ) 	 51380224 	 27143 	 10.111390590667725 	 9.209747791290283 	 0.38071131706237793 	 0.3075528144836426 	 12.4128737449646 	 28.88811230659485 	 0.4672665596008301 	 0.36261606216430664 	 
2025-08-06 13:03:15.297601 test begin: paddle.pow(Tensor([1024, 3101, 8],"float64"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([1024, 3101, 8],"float64"), 2, ) 	 25403392 	 27143 	 15.526280641555786 	 8.10637640953064 	 0.5846004486083984 	 0.3047752380371094 	 16.38074803352356 	 28.538248538970947 	 0.6167399883270264 	 0.35823488235473633 	 
2025-08-06 13:04:25.127789 test begin: paddle.pow(Tensor([1024, 6202, 8],"float32"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([1024, 6202, 8],"float32"), 2, ) 	 50806784 	 27143 	 9.99794888496399 	 8.081485033035278 	 0.376467227935791 	 0.30423903465270996 	 12.276498794555664 	 28.572983026504517 	 0.4622189998626709 	 0.35855913162231445 	 
2025-08-06 13:05:28.283212 test begin: paddle.pow(Tensor([22, 81, 94, 311],"float32"), 2.0, )
[Prof] paddle.pow 	 paddle.pow(Tensor([22, 81, 94, 311],"float32"), 2.0, ) 	 52094988 	 27143 	 10.260012865066528 	 8.280672550201416 	 0.3863048553466797 	 0.3118152618408203 	 12.570385217666626 	 29.369337558746338 	 0.4732472896575928 	 0.2766754627227783 	 
2025-08-06 13:06:30.558100 test begin: paddle.pow(Tensor([3101, 1024, 8],"float64"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([3101, 1024, 8],"float64"), 2, ) 	 25403392 	 27143 	 15.52829384803772 	 8.09470796585083 	 0.5851926803588867 	 0.30483317375183105 	 16.41948914527893 	 28.53682279586792 	 0.6182341575622559 	 0.3582179546356201 	 
2025-08-06 13:07:41.230453 test begin: paddle.pow(Tensor([4, 435, 94, 311],"float32"), 2.0, )
[Prof] paddle.pow 	 paddle.pow(Tensor([4, 435, 94, 311],"float32"), 2.0, ) 	 50867160 	 27143 	 10.009011268615723 	 8.090639114379883 	 0.3768630027770996 	 0.3046269416809082 	 12.288840532302856 	 28.687988996505737 	 0.4626729488372803 	 0.27030420303344727 	 
2025-08-06 13:08:42.135902 test begin: paddle.pow(Tensor([4, 81, 505, 311],"float32"), 2.0, )
[Prof] paddle.pow 	 paddle.pow(Tensor([4, 81, 505, 311],"float32"), 2.0, ) 	 50885820 	 27143 	 10.022361755371094 	 8.093509197235107 	 0.3773660659790039 	 0.30474424362182617 	 12.293241500854492 	 28.690687656402588 	 0.462796688079834 	 0.2702617645263672 	 
2025-08-06 13:09:42.968341 test begin: paddle.pow(Tensor([4, 81, 94, 1669],"float32"), 2.0, )
[Prof] paddle.pow 	 paddle.pow(Tensor([4, 81, 94, 1669],"float32"), 2.0, ) 	 50831064 	 27143 	 10.003027200698853 	 8.08463191986084 	 0.37667179107666016 	 0.3044168949127197 	 12.281022787094116 	 28.674481630325317 	 0.4623830318450928 	 0.27011775970458984 	 
2025-08-06 13:10:43.792641 test begin: paddle.pow(Tensor([6202, 1024, 8],"float32"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([6202, 1024, 8],"float32"), 2, ) 	 50806784 	 27143 	 9.997037410736084 	 9.115356206893921 	 0.37641096115112305 	 0.3042161464691162 	 12.276743650436401 	 28.573448657989502 	 0.4622986316680908 	 0.35863161087036133 	 
2025-08-06 13:11:46.270197 test begin: paddle.prod(Tensor([10, 10, 28225, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
Warning: The core code of paddle.prod is too complex.
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 10, 28225, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402502 	 68328 	 379.4019412994385 	 2.3780319690704346 	 0.005441427230834961 	 0.00017261505126953125 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([9]) and output[0] has a shape of torch.Size([1, 1, 1, 9]).
2025-08-06 13:19:08.957032 test begin: paddle.prod(Tensor([10, 10, 9, 28225],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 10, 9, 28225],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402502 	 68328 	 21.9309663772583 	 2.3853719234466553 	 0.00021409988403320312 	 7.390975952148438e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([28225]) and output[0] has a shape of torch.Size([1, 1, 1, 28225]).
2025-08-06 13:20:31.850162 test begin: paddle.prod(Tensor([10, 31361, 9, 9],"float64"), Tensor([2],"int64"), )
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 31361, 9, 9],"float64"), Tensor([2],"int64"), ) 	 25402412 	 68328 	 54.52608299255371 	 1.6205270290374756 	 0.0007565021514892578 	 0.00015664100646972656 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([9, 9]) and output[0] has a shape of torch.Size([1, 1, 9, 9]).
2025-08-06 13:22:29.168900 test begin: paddle.prod(Tensor([10, 31361, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 31361, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402412 	 68328 	 379.5399160385132 	 2.3960084915161133 	 0.005444765090942383 	 7.796287536621094e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([9]) and output[0] has a shape of torch.Size([1, 1, 1, 9]).
2025-08-06 13:29:49.711954 test begin: paddle.prod(Tensor([10, 5, 56449, 9],"float64"), Tensor([2],"int64"), )
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 5, 56449, 9],"float64"), Tensor([2],"int64"), ) 	 25402052 	 68328 	 34.255698442459106 	 1.5904688835144043 	 0.00045180320739746094 	 6.985664367675781e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([5, 9]) and output[0] has a shape of torch.Size([1, 5, 1, 9]).
2025-08-06 13:31:24.152205 test begin: paddle.prod(Tensor([10, 5, 9, 56449],"float64"), Tensor([2],"int64"), )
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 5, 9, 56449],"float64"), Tensor([2],"int64"), ) 	 25402052 	 68328 	 12.963971853256226 	 1.6711516380310059 	 0.00016069412231445312 	 0.00012183189392089844 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([10, 56449]) and output[0] has a shape of torch.Size([10, 1, 1, 56449]).
2025-08-06 13:32:39.835243 test begin: paddle.prod(Tensor([16, 3175201],"float32"), -1, )
[Prof] paddle.prod 	 paddle.prod(Tensor([16, 3175201],"float32"), -1, ) 	 50803216 	 68328 	 12.202130794525146 	 10.42599105834961 	 0.09125876426696777 	 0.07795548439025879 	 54.08242893218994 	 108.00048160552979 	 0.808905839920044 	 0.0006282329559326172 	 
2025-08-06 13:35:45.916638 test begin: paddle.prod(Tensor([31361, 10, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.prod 	 paddle.prod(Tensor([31361, 10, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402412 	 68328 	 22.1390380859375 	 2.409174919128418 	 0.00021314620971679688 	 7.033348083496094e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([9]) and output[0] has a shape of torch.Size([1, 1, 9, 1]).
2025-08-06 13:37:11.838154 test begin: paddle.prod(Tensor([49613, 1024],"float32"), -1, )
[Prof] paddle.prod 	 paddle.prod(Tensor([49613, 1024],"float32"), -1, ) 	 50803712 	 68328 	 10.006966352462769 	 10.060452938079834 	 0.1497046947479248 	 0.15047216415405273 	 54.08247780799866 	 108.87605237960815 	 0.8089382648468018 	 0.0006299018859863281 	 
2025-08-06 13:40:16.639847 test begin: paddle.prod(Tensor([62721, 5, 9, 9],"float64"), Tensor([2],"int64"), )
[Prof] paddle.prod 	 paddle.prod(Tensor([62721, 5, 9, 9],"float64"), Tensor([2],"int64"), ) 	 25402007 	 68328 	 21.50742268562317 	 1.665555477142334 	 0.0002684593200683594 	 7.343292236328125e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([62721, 9]) and output[0] has a shape of torch.Size([62721, 1, 9, 1]).
2025-08-06 13:41:40.610427 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 1016065, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4148727e80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 13:51:46.136277 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([2032129, 5, 5],"float32"), 1, "mul", True, False, )
W0806 13:51:46.392072 143802 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([2032129, 5, 5],"float32"), 1, "mul", True, False, ) 	 50804350 	 77440 	 9.576880931854248 	 2.1177077293395996 	 5.555152893066406e-05 	 0.0003268718719482422 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 13:52:07.298399 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 2032129, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 2032129, 5],"float32"), 1, "mul", True, False, ) 	 50804350 	 77440 	 10.261404991149902 	 2.0555427074432373 	 6.103515625e-05 	 0.00011205673217773438 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 13:52:26.539048 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 2032129],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 2032129],"float32"), 1, "mul", True, False, ) 	 50804350 	 77440 	 9.772606611251831 	 2.054077386856079 	 5.173683166503906e-05 	 7.915496826171875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 13:52:44.967448 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 2032129, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f107a0c3220>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 14:02:49.608961 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([2032129, 5, 5],"int32"), 1, "mul", True, False, )
W0806 14:02:49.809625 144209 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([2032129, 5, 5],"int32"), 1, "mul", True, False, ) 	 50804350 	 77440 	 10.623600721359253 	 2.082368850708008 	 9.441375732421875e-05 	 0.00011277198791503906 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:03:09.813540 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 2032129, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 2032129, 5],"int32"), 1, "mul", True, False, ) 	 50804350 	 77440 	 10.308833599090576 	 2.0039784908294678 	 7.200241088867188e-05 	 0.00016450881958007812 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:03:29.867323 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 2032129],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 2032129],"int32"), 1, "mul", True, False, ) 	 50804350 	 77440 	 10.176410675048828 	 2.0058772563934326 	 5.2928924560546875e-05 	 0.00016808509826660156 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:03:49.016410 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 1016065, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f47a34e72e0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 14:13:53.905291 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([1016065, 5, 5],"int64"), 1, "mul", True, False, )
W0806 14:13:54.302846 144785 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([1016065, 5, 5],"int64"), 1, "mul", True, False, ) 	 25402750 	 77440 	 9.593241691589355 	 2.0405681133270264 	 5.745887756347656e-05 	 0.00022268295288085938 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:14:12.867141 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 1016065, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 1016065, 5],"int64"), 1, "mul", True, False, ) 	 25402750 	 77440 	 9.864487171173096 	 2.222292423248291 	 5.745887756347656e-05 	 0.00016236305236816406 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:14:32.040611 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 1016065],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 1016065],"int64"), 1, "mul", True, False, ) 	 25402750 	 77440 	 9.701406002044678 	 2.170168161392212 	 6.818771362304688e-05 	 0.0001819133758544922 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:14:50.866716 test begin: paddle.put_along_axis(Tensor([10, 10, 254017],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 254017],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, ) 	 25401950 	 77440 	 29.188747882843018 	 48.86245131492615 	 0.0002899169921875 	 0.12867522239685059 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:16:41.193554 test begin: paddle.put_along_axis(Tensor([10, 10, 508033],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 508033],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, ) 	 50803550 	 77440 	 28.932440757751465 	 48.875972747802734 	 0.0002961158752441406 	 0.12872982025146484 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 14:18:38.671579 test begin: paddle.put_along_axis(Tensor([10, 10, 508033],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 508033],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, ) 	 50803550 	 77440 	 29.420621156692505 	 48.855928897857666 	 0.0002913475036621094 	 0.12867188453674316 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:20:37.990301 test begin: paddle.put_along_axis(Tensor([10, 254017, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 254017, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, ) 	 25401950 	 77440 	 29.604929447174072 	 48.82245373725891 	 0.0002887248992919922 	 0.12858057022094727 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:22:31.927119 test begin: paddle.put_along_axis(Tensor([10, 508033, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 508033, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, ) 	 50803550 	 77440 	 29.29303526878357 	 50.09907054901123 	 0.00028014183044433594 	 0.1285386085510254 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 14:24:31.208925 test begin: paddle.put_along_axis(Tensor([10, 508033, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 508033, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, ) 	 50803550 	 77440 	 28.769662141799927 	 48.81740880012512 	 0.0002963542938232422 	 0.12854480743408203 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:26:27.268994 test begin: paddle.put_along_axis(Tensor([254017, 10, 10],"int64"), Tensor([254017, 5, 5],"int64"), Tensor([254017, 5, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([254017, 10, 10],"int64"), Tensor([254017, 5, 5],"int64"), Tensor([254017, 5, 5],"int64"), 1, "mul", True, False, ) 	 38102550 	 77440 	 64.51641464233398 	 79.77523827552795 	 0.0006079673767089844 	 0.21043968200683594 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:31:03.890794 test begin: paddle.put_along_axis(Tensor([254017, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([254017, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, ) 	 25401950 	 77440 	 29.577579736709595 	 48.88234233856201 	 0.00029468536376953125 	 0.12870335578918457 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:32:54.930281 test begin: paddle.put_along_axis(Tensor([508033, 10, 10],"float32"), Tensor([254017, 5, 5],"int64"), Tensor([508033, 5, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([508033, 10, 10],"float32"), Tensor([254017, 5, 5],"int64"), Tensor([508033, 5, 5],"float32"), 1, "mul", True, False, ) 	 69854550 	 77440 	 60.10905885696411 	 74.02314519882202 	 0.0005619525909423828 	 0.19527935981750488 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 14:36:55.732396 test begin: paddle.put_along_axis(Tensor([508033, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([508033, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, ) 	 50803550 	 77440 	 29.276477098464966 	 48.84092736244202 	 0.0002892017364501953 	 0.12861919403076172 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 14:38:51.680218 test begin: paddle.put_along_axis(Tensor([508033, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([508033, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, ) 	 50803550 	 77440 	 28.88144826889038 	 48.84240412712097 	 0.0002925395965576172 	 0.128615140914917 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:40:46.885414 test begin: paddle.put_along_axis(Tensor([508033, 10, 10],"int32"), Tensor([508033, 5, 5],"int32"), Tensor([508033, 5, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([508033, 10, 10],"int32"), Tensor([508033, 5, 5],"int32"), Tensor([508033, 5, 5],"int32"), 1, "mul", True, False, ) 	 76204950 	 77440 	 80.96183681488037 	 97.49532270431519 	 0.0008206367492675781 	 0.2573702335357666 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:46:43.083388 test begin: paddle.rad2deg(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33829 	 10.003808498382568 	 10.069601774215698 	 0.3021063804626465 	 0.30414271354675293 	 10.015836715698242 	 10.071807861328125 	 0.3024938106536865 	 0.30415940284729004 	 
2025-08-06 14:47:25.133010 test begin: paddle.rad2deg(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33829 	 10.017368793487549 	 10.09261441230774 	 0.302476167678833 	 0.3041048049926758 	 10.014585733413696 	 10.070752620697021 	 0.3025200366973877 	 0.3041713237762451 	 
2025-08-06 14:48:07.925252 test begin: paddle.rad2deg(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33829 	 10.020018815994263 	 10.069387674331665 	 0.30258703231811523 	 0.3042120933532715 	 10.01657748222351 	 10.070802450180054 	 0.3025188446044922 	 0.30410122871398926 	 
2025-08-06 14:48:50.180539 test begin: paddle.rad2deg(x=Tensor([1587601, 4, 4],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([1587601, 4, 4],"float64"), ) 	 25401616 	 33829 	 10.080394983291626 	 10.08969259262085 	 0.3043954372406006 	 0.3047187328338623 	 10.077739953994751 	 10.092926740646362 	 0.3043057918548584 	 0.30480194091796875 	 
2025-08-06 14:49:31.653058 test begin: paddle.rad2deg(x=Tensor([396901, 4, 4, 4],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 33829 	 10.078572988510132 	 10.0896737575531 	 0.30443239212036133 	 0.30469441413879395 	 10.076670408248901 	 10.094123601913452 	 0.30431413650512695 	 0.30474305152893066 	 
2025-08-06 14:50:13.121275 test begin: paddle.rad2deg(x=Tensor([4, 1587601, 4],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([4, 1587601, 4],"float64"), ) 	 25401616 	 33829 	 10.079126596450806 	 10.092949867248535 	 0.3044133186340332 	 0.3048431873321533 	 10.077176094055176 	 10.094979047775269 	 0.3043372631072998 	 0.30492615699768066 	 
2025-08-06 14:50:55.210967 test begin: paddle.rad2deg(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 33829 	 10.078577518463135 	 10.089606285095215 	 0.3043653964996338 	 0.30483508110046387 	 10.076798677444458 	 10.093218564987183 	 0.3043227195739746 	 0.3048439025878906 	 
2025-08-06 14:51:37.764848 test begin: paddle.rad2deg(x=Tensor([4, 4, 1587601],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([4, 4, 1587601],"float64"), ) 	 25401616 	 33829 	 10.746927499771118 	 10.0910325050354 	 0.30440831184387207 	 0.30473756790161133 	 10.076756954193115 	 10.093186855316162 	 0.3043358325958252 	 0.3048288822174072 	 
2025-08-06 14:52:20.568491 test begin: paddle.rad2deg(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 33829 	 10.081743001937866 	 10.096240997314453 	 0.30437326431274414 	 0.30473875999450684 	 10.075437307357788 	 10.093358993530273 	 0.3043687343597412 	 0.304868221282959 	 
2025-08-06 14:53:02.078565 test begin: paddle.rad2deg(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 33829 	 10.080128908157349 	 10.090036392211914 	 0.30585789680480957 	 0.3047912120819092 	 10.078106164932251 	 10.093597173690796 	 0.3043336868286133 	 0.30481505393981934 	 
2025-08-06 14:53:43.713711 test begin: paddle.rank(input=Tensor([1270080101, 2],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([1270080101, 2],"float64"), ) 	 2540160202 	 247008 	 9.54297423362732 	 6.859957695007324 	 3.790855407714844e-05 	 6.246566772460938e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:37:56.026494 test begin: paddle.Tensor.__abs__(Tensor([10, 5080321],"float32"), )
W0805 21:37:57.079154 108827 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.__abs__ 	 paddle.Tensor.__abs__(Tensor([10, 5080321],"float32"), ) 	 50803210 	 33766 	 9.993123769760132 	 10.048175573348999 	 0.30240797996520996 	 0.30405282974243164 	 15.197286128997803 	 25.071933269500732 	 0.4600198268890381 	 0.3794276714324951 	 
2025-08-05 21:38:59.364084 test begin: paddle.Tensor.__abs__(Tensor([49613, 1024],"float32"), )
[Prof] paddle.Tensor.__abs__ 	 paddle.Tensor.__abs__(Tensor([49613, 1024],"float32"), ) 	 50803712 	 33766 	 9.995180606842041 	 10.045489311218262 	 0.3025321960449219 	 0.3040497303009033 	 15.198319911956787 	 25.07269525527954 	 0.4602468013763428 	 0.3793926239013672 	 
2025-08-05 21:40:02.572205 test begin: paddle.Tensor.__abs__(Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.__abs__ 	 paddle.Tensor.__abs__(Tensor([50803201],"float32"), ) 	 50803201 	 33766 	 9.989314556121826 	 10.045249223709106 	 0.302349328994751 	 0.3041269779205322 	 15.19548773765564 	 25.072127103805542 	 0.45978665351867676 	 0.3793942928314209 	 
2025-08-05 21:41:06.867580 test begin: paddle.Tensor.__add__(Tensor([1, 32, 388, 4096],"float32"), Tensor([1, 1, 388, 4096],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([1, 32, 388, 4096],"float32"), Tensor([1, 1, 388, 4096],"float32"), ) 	 52445184 	 29759 	 10.01025128364563 	 9.634354829788208 	 0.3437507152557373 	 0.3309037685394287 	 14.177022695541382 	 4.572930574417114 	 0.24341058731079102 	 0.15702271461486816 	 
2025-08-05 21:41:47.714633 test begin: paddle.Tensor.__add__(Tensor([1, 32, 4096, 388],"float32"), Tensor([1, 1, 4096, 388],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([1, 32, 4096, 388],"float32"), Tensor([1, 1, 4096, 388],"float32"), ) 	 52445184 	 29759 	 9.985159635543823 	 10.08380126953125 	 0.3428969383239746 	 0.3308868408203125 	 14.173902034759521 	 4.569482803344727 	 0.2434232234954834 	 0.15691637992858887 	 
2025-08-05 21:42:30.536843 test begin: paddle.Tensor.__add__(Tensor([1, 32, 4096, 4096],"float32"), Tensor([1, 1, 4096, 4096],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([1, 32, 4096, 4096],"float32"), Tensor([1, 1, 4096, 4096],"float32"), ) 	 553648128 	 29759 	 140.26740050315857 	 137.6564953327179 	 4.795872926712036 	 4.727203607559204 	 147.79647874832153 	 46.417720794677734 	 1.6903648376464844 	 1.5940783023834229 	 
2025-08-05 21:50:47.956555 test begin: paddle.Tensor.__add__(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 1, 4096, 4096],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 1, 4096, 4096],"float32"), ) 	 83886080 	 29759 	 17.652665853500366 	 17.34611964225769 	 0.6062803268432617 	 0.5957214832305908 	 19.7359459400177 	 7.2568604946136475 	 0.3388671875 	 0.24922919273376465 	 
2025-08-05 21:51:52.389177 test begin: paddle.Tensor.__add__(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 4, 4096, 4096],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 4, 4096, 4096],"float32"), ) 	 134217728 	 29759 	 17.659042835235596 	 17.497348070144653 	 0.6064956188201904 	 0.6008872985839844 	 18.93643856048584 	 1.768618106842041 	 0.6503503322601318 	 8.273124694824219e-05 	 
2025-08-05 21:52:54.765158 test begin: paddle.Tensor.__add__(Tensor([2, 256, 336, 336],"float16"), Tensor([2, 256, 336, 336],"float32"), )
W0805 21:52:56.600636 110301 dygraph_functions.cc:87088] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([2, 256, 336, 336],"float16"), Tensor([2, 256, 336, 336],"float32"), ) 	 115605504 	 29759 	 23.315813779830933 	 13.888919353485107 	 0.4005610942840576 	 0.47646236419677734 	 23.883445501327515 	 7.739447832107544 	 0.410123348236084 	 0.26575636863708496 	 
2025-08-05 21:54:09.018422 test begin: paddle.Tensor.__add__(Tensor([2, 256, 336, 336],"float32"), Tensor([2, 256, 336, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([2, 256, 336, 336],"float32"), Tensor([2, 256, 336, 336],"float32"), ) 	 115605504 	 29759 	 15.228023529052734 	 15.098999500274658 	 0.5230004787445068 	 0.5185527801513672 	 16.327470064163208 	 1.883772850036621 	 0.5607583522796631 	 0.0002613067626953125 	 
2025-08-05 21:55:00.500162 test begin: paddle.Tensor.__add__(Tensor([4, 256, 336, 336],"float16"), Tensor([4, 256, 336, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([4, 256, 336, 336],"float16"), Tensor([4, 256, 336, 336],"float32"), ) 	 231211008 	 29759 	 46.4844286441803 	 28.367042779922485 	 0.7980647087097168 	 0.9465718269348145 	 47.63122200965881 	 15.333675861358643 	 0.8178133964538574 	 0.5265355110168457 	 
2025-08-05 21:57:26.710275 test begin: paddle.Tensor.__add__(Tensor([8, 113, 336, 336],"float16"), Tensor([8, 113, 336, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([8, 113, 336, 336],"float16"), Tensor([8, 113, 336, 336],"float32"), ) 	 204115968 	 29759 	 41.06203556060791 	 24.35666036605835 	 0.7051706314086914 	 0.8363947868347168 	 42.1167688369751 	 13.555094957351685 	 0.723238468170166 	 0.4654679298400879 	 
2025-08-05 21:59:33.410074 test begin: paddle.Tensor.__add__(Tensor([8, 256, 336, 74],"float32"), Tensor([8, 256, 336, 74],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([8, 256, 336, 74],"float32"), Tensor([8, 256, 336, 74],"float32"), ) 	 101842944 	 29759 	 13.431292295455933 	 13.31679654121399 	 0.4613029956817627 	 0.4572603702545166 	 14.396420240402222 	 2.1566500663757324 	 0.49449872970581055 	 8.153915405273438e-05 	 
2025-08-05 22:00:19.567010 test begin: paddle.Tensor.__add__(Tensor([8, 256, 74, 336],"float32"), Tensor([8, 256, 74, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([8, 256, 74, 336],"float32"), Tensor([8, 256, 74, 336],"float32"), ) 	 101842944 	 29759 	 13.430566787719727 	 13.319693565368652 	 0.4612846374511719 	 0.45726680755615234 	 14.396053314208984 	 2.0973641872406006 	 0.49442195892333984 	 8.749961853027344e-05 	 
2025-08-05 22:01:05.518592 test begin: paddle.Tensor.__add__(Tensor([8, 57, 336, 336],"float16"), Tensor([8, 57, 336, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([8, 57, 336, 336],"float16"), Tensor([8, 57, 336, 336],"float32"), ) 	 102961152 	 29759 	 20.792269706726074 	 12.376107931137085 	 0.3570213317871094 	 0.425018310546875 	 21.3151273727417 	 6.911553859710693 	 0.36604952812194824 	 0.2373039722442627 	 
2025-08-05 22:02:09.507756 test begin: paddle.Tensor.__add__(Tensor([8, 57, 336, 336],"float32"), Tensor([8, 57, 336, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([8, 57, 336, 336],"float32"), Tensor([8, 57, 336, 336],"float32"), ) 	 102961152 	 29759 	 13.58008098602295 	 13.473942041397095 	 0.46633124351501465 	 0.46224021911621094 	 14.550908327102661 	 2.130686044692993 	 0.4997825622558594 	 0.000202178955078125 	 
2025-08-05 22:02:58.080779 test begin: paddle.Tensor.__and__(Tensor([1, 1, 2048, 2048],"bool"), Tensor([1, 13, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 1, 2048, 2048],"bool"), Tensor([1, 13, 2048, 2048],"bool"), ) 	 58720256 	 84955 	 13.67891526222229 	 19.235169410705566 	 0.16455960273742676 	 0.23141765594482422 	 None 	 None 	 None 	 None 	 
2025-08-05 22:03:31.809267 test begin: paddle.Tensor.__and__(Tensor([1, 1, 2048, 2048],"bool"), Tensor([13, 1, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 1, 2048, 2048],"bool"), Tensor([13, 1, 2048, 2048],"bool"), ) 	 58720256 	 84955 	 13.678633451461792 	 19.235629558563232 	 0.16456317901611328 	 0.2314281463623047 	 None 	 None 	 None 	 None 	 
2025-08-05 22:04:05.515797 test begin: paddle.Tensor.__and__(Tensor([1, 1, 2048, 24807],"bool"), Tensor([1, 1, 2048, 24807],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 1, 2048, 24807],"bool"), Tensor([1, 1, 2048, 24807],"bool"), ) 	 101609472 	 84955 	 9.996285438537598 	 9.788069248199463 	 0.12023186683654785 	 0.1177215576171875 	 None 	 None 	 None 	 None 	 
2025-08-05 22:04:28.685273 test begin: paddle.Tensor.__and__(Tensor([1, 1, 24807, 2048],"bool"), Tensor([1, 1, 24807, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 1, 24807, 2048],"bool"), Tensor([1, 1, 24807, 2048],"bool"), ) 	 101609472 	 84955 	 9.996355056762695 	 9.800843954086304 	 0.1202554702758789 	 0.11775946617126465 	 None 	 None 	 None 	 None 	 
2025-08-05 22:04:52.072788 test begin: paddle.Tensor.__and__(Tensor([1, 13, 2048, 2048],"bool"), Tensor([1, 1, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 13, 2048, 2048],"bool"), Tensor([1, 1, 2048, 2048],"bool"), ) 	 58720256 	 84955 	 15.326492309570312 	 19.230685234069824 	 0.18439340591430664 	 0.23137474060058594 	 None 	 None 	 None 	 None 	 
2025-08-05 22:05:27.425648 test begin: paddle.Tensor.__and__(Tensor([1, 13, 2048, 2048],"bool"), Tensor([1, 13, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 13, 2048, 2048],"bool"), Tensor([1, 13, 2048, 2048],"bool"), ) 	 109051904 	 84955 	 10.668208599090576 	 10.476025104522705 	 0.12831449508666992 	 0.12586188316345215 	 None 	 None 	 None 	 None 	 
2025-08-05 22:05:50.493141 test begin: paddle.Tensor.__and__(Tensor([13, 1, 1007, 1007],"bool"), Tensor([13, 4, 1007, 1007],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 1, 1007, 1007],"bool"), Tensor([13, 4, 1007, 1007],"bool"), ) 	 65913185 	 84955 	 15.675380229949951 	 20.27168345451355 	 0.18857669830322266 	 0.24387502670288086 	 None 	 None 	 None 	 None 	 
2025-08-05 22:06:27.376888 test begin: paddle.Tensor.__and__(Tensor([13, 1, 1007, 3881],"bool"), Tensor([13, 1, 1007, 3881],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 1, 1007, 3881],"bool"), Tensor([13, 1, 1007, 3881],"bool"), ) 	 101612342 	 84955 	 10.021638870239258 	 9.833731889724731 	 0.12057709693908691 	 0.11814379692077637 	 None 	 None 	 None 	 None 	 
2025-08-05 22:06:51.731336 test begin: paddle.Tensor.__and__(Tensor([13, 1, 2048, 2048],"bool"), Tensor([1, 1, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 1, 2048, 2048],"bool"), Tensor([1, 1, 2048, 2048],"bool"), ) 	 58720256 	 84955 	 15.325830936431885 	 19.230148553848267 	 0.18439221382141113 	 0.23131155967712402 	 None 	 None 	 None 	 None 	 
2025-08-05 22:07:27.084173 test begin: paddle.Tensor.__and__(Tensor([13, 1, 2048, 2048],"bool"), Tensor([13, 1, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 1, 2048, 2048],"bool"), Tensor([13, 1, 2048, 2048],"bool"), ) 	 109051904 	 84955 	 10.66789698600769 	 10.793840408325195 	 0.12834906578063965 	 0.12581586837768555 	 None 	 None 	 None 	 None 	 
2025-08-05 22:07:52.506195 test begin: paddle.Tensor.__and__(Tensor([13, 1, 3881, 1007],"bool"), Tensor([13, 1, 3881, 1007],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 1, 3881, 1007],"bool"), Tensor([13, 1, 3881, 1007],"bool"), ) 	 101612342 	 84955 	 10.021470069885254 	 9.823958396911621 	 0.12056183815002441 	 0.11819195747375488 	 None 	 None 	 None 	 None 	 
2025-08-05 22:08:13.935174 test begin: paddle.Tensor.__and__(Tensor([13, 4, 1007, 1007],"bool"), Tensor([13, 1, 1007, 1007],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 4, 1007, 1007],"bool"), Tensor([13, 1, 1007, 1007],"bool"), ) 	 65913185 	 84955 	 16.795427560806274 	 20.264293432235718 	 0.2020249366760254 	 0.24382376670837402 	 None 	 None 	 None 	 None 	 
2025-08-05 22:08:51.945486 test begin: paddle.Tensor.__and__(Tensor([13, 4, 1007, 1007],"bool"), Tensor([13, 4, 1007, 1007],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 4, 1007, 1007],"bool"), Tensor([13, 4, 1007, 1007],"bool"), ) 	 105461096 	 84955 	 10.42278003692627 	 10.85390019416809 	 0.12537717819213867 	 0.12308073043823242 	 None 	 None 	 None 	 None 	 
2025-08-05 22:09:16.451495 test begin: paddle.Tensor.__and__(Tensor([194, 1, 512, 512],"bool"), Tensor([194, 1, 512, 512],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([194, 1, 512, 512],"bool"), Tensor([194, 1, 512, 512],"bool"), ) 	 101711872 	 84955 	 9.998944520950317 	 9.794438123703003 	 0.12028336524963379 	 0.11783337593078613 	 None 	 None 	 None 	 None 	 
2025-08-05 22:09:38.696535 test begin: paddle.Tensor.__and__(Tensor([51, 1, 1007, 1007],"bool"), Tensor([51, 1, 1007, 1007],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([51, 1, 1007, 1007],"bool"), Tensor([51, 1, 1007, 1007],"bool"), ) 	 103432998 	 84955 	 10.143945693969727 	 9.987118005752563 	 0.1220240592956543 	 0.12014985084533691 	 None 	 None 	 None 	 None 	 
2025-08-05 22:10:00.343007 test begin: paddle.Tensor.__and__(Tensor([8, 1, 12404, 512],"bool"), Tensor([8, 1, 12404, 512],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([8, 1, 12404, 512],"bool"), Tensor([8, 1, 12404, 512],"bool"), ) 	 101613568 	 84955 	 9.995266914367676 	 10.395562648773193 	 0.1202549934387207 	 0.11778020858764648 	 None 	 None 	 None 	 None 	 
2025-08-05 22:10:24.431884 test begin: paddle.Tensor.__and__(Tensor([8, 1, 512, 12404],"bool"), Tensor([8, 1, 512, 12404],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([8, 1, 512, 12404],"bool"), Tensor([8, 1, 512, 12404],"bool"), ) 	 101613568 	 84955 	 9.99540901184082 	 9.7925386428833 	 0.1202688217163086 	 0.1177678108215332 	 None 	 None 	 None 	 None 	 
2025-08-05 22:10:48.204042 test begin: paddle.Tensor.__and__(Tensor([8, 1, 512, 512],"bool"), Tensor([8, 25, 512, 512],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([8, 1, 512, 512],"bool"), Tensor([8, 25, 512, 512],"bool"), ) 	 54525952 	 84955 	 15.580201864242554 	 19.988949298858643 	 0.1874227523803711 	 0.24047112464904785 	 None 	 None 	 None 	 None 	 
2025-08-05 22:11:27.132462 test begin: paddle.Tensor.__and__(Tensor([8, 25, 512, 512],"bool"), Tensor([8, 1, 512, 512],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([8, 25, 512, 512],"bool"), Tensor([8, 1, 512, 512],"bool"), ) 	 54525952 	 84955 	 16.426145792007446 	 19.990448236465454 	 0.19760870933532715 	 0.24051308631896973 	 None 	 None 	 None 	 None 	 
2025-08-05 22:12:04.431877 test begin: paddle.Tensor.__and__(Tensor([8, 25, 512, 512],"bool"), Tensor([8, 25, 512, 512],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([8, 25, 512, 512],"bool"), Tensor([8, 25, 512, 512],"bool"), ) 	 104857600 	 84955 	 10.27191686630249 	 10.084693193435669 	 0.12354230880737305 	 0.12135672569274902 	 None 	 None 	 None 	 None 	 
2025-08-05 22:12:26.407436 test begin: paddle.Tensor.__div__(Tensor([8, 16, 396901],"float32"), 2, )
[Prof] paddle.Tensor.__div__ 	 paddle.Tensor.__div__(Tensor([8, 16, 396901],"float32"), 2, ) 	 50803328 	 33824 	 9.994603633880615 	 10.065093517303467 	 0.301990270614624 	 0.3040616512298584 	 9.995652914047241 	 10.064069032669067 	 0.3020470142364502 	 0.3040955066680908 	 
2025-08-05 22:13:08.355356 test begin: paddle.Tensor.__div__(Tensor([8, 198451, 32],"float32"), 2, )
[Prof] paddle.Tensor.__div__ 	 paddle.Tensor.__div__(Tensor([8, 198451, 32],"float32"), 2, ) 	 50803456 	 33824 	 10.002209901809692 	 10.06519103050232 	 0.3022584915161133 	 0.30419087409973145 	 10.005195617675781 	 10.064003705978394 	 0.30234217643737793 	 0.30410146713256836 	 
2025-08-05 22:13:53.412203 test begin: paddle.Tensor.__div__(Tensor([99226, 16, 32],"float32"), 2, )
[Prof] paddle.Tensor.__div__ 	 paddle.Tensor.__div__(Tensor([99226, 16, 32],"float32"), 2, ) 	 50803712 	 33824 	 10.001353740692139 	 10.06513237953186 	 0.3021829128265381 	 0.3041954040527344 	 9.999602794647217 	 10.063852787017822 	 0.30211377143859863 	 0.30405116081237793 	 
2025-08-05 22:14:38.893594 test begin: paddle.Tensor.__eq__(Tensor([138, 369303],"float32"), Tensor([138, 1],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([138, 369303],"float32"), Tensor([138, 1],"float32"), ) 	 50963952 	 52174 	 10.013825178146362 	 12.617146492004395 	 0.19619083404541016 	 0.24716472625732422 	 None 	 None 	 None 	 None 	 
2025-08-05 22:15:02.637904 test begin: paddle.Tensor.__eq__(Tensor([146, 349866],"float32"), Tensor([146, 1],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([146, 349866],"float32"), Tensor([146, 1],"float32"), ) 	 51080582 	 52174 	 10.031320571899414 	 12.646435260772705 	 0.19649147987365723 	 0.247725248336792 	 None 	 None 	 None 	 None 	 
2025-08-05 22:15:26.205053 test begin: paddle.Tensor.__eq__(Tensor([49, 1036801],"float32"), Tensor([49, 1036801],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([49, 1036801],"float32"), Tensor([49, 1036801],"float32"), ) 	 101606498 	 52174 	 17.047929525375366 	 17.091296672821045 	 0.3339111804962158 	 0.33473730087280273 	 None 	 None 	 None 	 None 	 
2025-08-05 22:16:02.060266 test begin: paddle.Tensor.__eq__(Tensor([49, 1036801],"float32"), Tensor([49, 1],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([49, 1036801],"float32"), Tensor([49, 1],"float32"), ) 	 50803298 	 52174 	 9.99623155593872 	 12.58198094367981 	 0.19583630561828613 	 0.24642014503479004 	 None 	 None 	 None 	 None 	 
2025-08-05 22:16:25.445322 test begin: paddle.Tensor.__eq__(Tensor([53, 958551],"float32"), Tensor([53, 1],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([53, 958551],"float32"), Tensor([53, 1],"float32"), ) 	 50803256 	 52174 	 9.999309539794922 	 12.61324167251587 	 0.1958925724029541 	 0.2464122772216797 	 None 	 None 	 None 	 None 	 
2025-08-05 22:16:50.944254 test begin: paddle.Tensor.__eq__(Tensor([53, 958551],"float32"), Tensor([53, 958551],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([53, 958551],"float32"), Tensor([53, 958551],"float32"), ) 	 101606406 	 52174 	 17.04803228378296 	 17.089826583862305 	 0.33392977714538574 	 0.33474135398864746 	 None 	 None 	 None 	 None 	 
2025-08-05 22:17:26.767768 test begin: paddle.Tensor.__eq__(Tensor([55, 923695],"float32"), Tensor([55, 1],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([55, 923695],"float32"), Tensor([55, 1],"float32"), ) 	 50803280 	 52174 	 9.995996475219727 	 12.596909046173096 	 0.19582033157348633 	 0.24641823768615723 	 None 	 None 	 None 	 None 	 
2025-08-05 22:17:51.122502 test begin: paddle.Tensor.__eq__(Tensor([55, 923695],"float32"), Tensor([55, 923695],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([55, 923695],"float32"), Tensor([55, 923695],"float32"), ) 	 101606450 	 52174 	 17.04795241355896 	 17.089612245559692 	 0.3339824676513672 	 0.3347640037536621 	 None 	 None 	 None 	 None 	 
2025-08-05 22:18:26.928797 test begin: paddle.Tensor.__floordiv__(Tensor([10, 10160641],"float32"), Tensor([10, 10160641],"float16"), )
W0805 22:18:30.397305 111104 dygraph_functions.cc:89596] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([10, 10160641],"float32"), Tensor([10, 10160641],"float16"), ) 	 203212820 	 32836 	 45.04620957374573 	 39.09844160079956 	 0.7011969089508057 	 1.2168893814086914 	 None 	 None 	 None 	 None 	 
2025-08-05 22:19:55.060674 test begin: paddle.Tensor.__floordiv__(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), ) 	 50803220 	 32836 	 14.692471742630005 	 14.692281246185303 	 0.45728015899658203 	 0.4572324752807617 	 None 	 None 	 None 	 None 	 
2025-08-05 22:20:25.262907 test begin: paddle.Tensor.__floordiv__(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float16"), )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float16"), ) 	 101606420 	 32836 	 22.637329816818237 	 19.700577974319458 	 0.35237836837768555 	 0.6128699779510498 	 None 	 None 	 None 	 None 	 
2025-08-05 22:21:12.911910 test begin: paddle.Tensor.__floordiv__(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), ) 	 50804736 	 32836 	 14.706051111221313 	 14.692902326583862 	 0.4577195644378662 	 0.4573180675506592 	 None 	 None 	 None 	 None 	 
2025-08-05 22:21:44.145276 test begin: paddle.Tensor.__floordiv__(Tensor([4, 6350401],"int64"), 4, )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([4, 6350401],"int64"), 4, ) 	 25401604 	 32836 	 9.997642040252686 	 9.935576677322388 	 0.1555783748626709 	 0.30887937545776367 	 None 	 None 	 None 	 None 	 
2025-08-05 22:22:04.516049 test begin: paddle.Tensor.__floordiv__(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float16"), )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float16"), ) 	 101607424 	 32836 	 22.66570496559143 	 19.69142198562622 	 0.3527414798736572 	 0.6128511428833008 	 None 	 None 	 None 	 None 	 
2025-08-05 22:22:48.872672 test begin: paddle.Tensor.__floordiv__(Tensor([84673, 300],"int64"), 4, )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([84673, 300],"int64"), 4, ) 	 25401900 	 32836 	 10.00214695930481 	 9.924606800079346 	 0.15569424629211426 	 0.30892515182495117 	 None 	 None 	 None 	 None 	 
2025-08-05 22:23:09.573589 test begin: paddle.Tensor.__floordiv__(Tensor([99226, 1024],"float32"), Tensor([99226, 1024],"float16"), )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([99226, 1024],"float32"), Tensor([99226, 1024],"float16"), ) 	 203214848 	 32836 	 45.11896872520447 	 39.12161564826965 	 0.7021336555480957 	 1.2408254146575928 	 None 	 None 	 None 	 None 	 
2025-08-05 22:24:38.500208 test begin: paddle.Tensor.__ge__(Tensor([50803201],"int32"), 0, )
[Prof] paddle.Tensor.__ge__ 	 paddle.Tensor.__ge__(Tensor([50803201],"int32"), 0, ) 	 50803201 	 21328 	 9.99941086769104 	 3.9623892307281494 	 0.23949980735778809 	 0.18989300727844238 	 None 	 None 	 None 	 None 	 
2025-08-05 22:24:53.219616 test begin: paddle.Tensor.__getitem__(Tensor([10, 7576, 12800],"bfloat16"), slice(None,-3,None), )
W0805 22:25:38.699610 111291 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 2715238400, memory's size is 1939456000.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):2715238400 > memory_size():1939456000.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f84b2b030a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 22:34:58.654566 test begin: paddle.Tensor.__getitem__(Tensor([10, 7576, 16770],"bfloat16"), slice(None,-3,None), )
W0805 22:35:17.991662 111600 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 22:35:42.676399 111600 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 3557386560, memory's size is 2540990464.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):3557386560 > memory_size():2540990464.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4163712e00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 22:45:03.930624 test begin: paddle.Tensor.__getitem__(Tensor([10, 7712, 12800],"bfloat16"), slice(None,-2,None), )
W0805 22:45:20.842563 111902 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 22:45:43.130065 111902 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 3158835200, memory's size is 1974272000.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):3158835200 > memory_size():1974272000.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7faa51efae00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 22:55:09.725064 test begin: paddle.Tensor.__getitem__(Tensor([10, 7712, 16470],"bfloat16"), slice(None,-2,None), )
W0805 22:55:29.010001 112299 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 22:56:03.157210 112299 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 4064532480, memory's size is 2540332800.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):4064532480 > memory_size():2540332800.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f74dd7dae00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 23:05:15.328625 test begin: paddle.Tensor.__getitem__(Tensor([10, 8168, 12800],"bfloat16"), slice(None,-6,None), )
W0805 23:05:31.085265 112504 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 23:05:49.429957 112504 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7914672e00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 23:15:25.030822 test begin: paddle.Tensor.__getitem__(Tensor([10, 8168, 15550],"bfloat16"), slice(None,-6,None), )
W0805 23:15:44.179558 112988 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 23:16:02.054453 112988 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f08af28ee60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 23:25:30.186153 test begin: paddle.Tensor.__getitem__(Tensor([10, 9923, 12800],"bfloat16"), slice(None,-2,None), )
W0805 23:25:49.465261 113424 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 23:26:17.401064 113424 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 4064460800, memory's size is 2540288000.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):4064460800 > memory_size():2540288000.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8c6067ec20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 23:35:35.843221 test begin: paddle.Tensor.__getitem__(Tensor([10, 9923, 12800],"bfloat16"), slice(None,-3,None), )
W0805 23:35:55.597858 113754 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 23:36:20.312716 113754 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 3556403200, memory's size is 2540288000.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):3556403200 > memory_size():2540288000.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f499eb56e60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 23:45:44.347285 test begin: paddle.Tensor.__getitem__(Tensor([10, 9923, 12800],"bfloat16"), slice(None,-6,None), )
W0805 23:46:07.031319 114000 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0805 23:46:24.882789 114000 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f86a4c42c20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 23:55:49.552712 test begin: paddle.Tensor.__gt__(Tensor([1, 400, 127009],"float32"), 0, )
W0805 23:55:50.697512 114303 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([1, 400, 127009],"float32"), 0, ) 	 50803600 	 21267 	 9.989183187484741 	 3.954660177230835 	 0.2400195598602295 	 0.1898798942565918 	 None 	 None 	 None 	 None 	 
2025-08-05 23:56:04.890900 test begin: paddle.Tensor.__gt__(Tensor([1, 400, 127009],"float32"), 1e-09, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([1, 400, 127009],"float32"), 1e-09, ) 	 50803600 	 21267 	 9.986352443695068 	 3.9522860050201416 	 0.2399461269378662 	 0.1899399757385254 	 None 	 None 	 None 	 None 	 
2025-08-05 23:56:19.791600 test begin: paddle.Tensor.__gt__(Tensor([1, 772, 65856],"float32"), 0, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([1, 772, 65856],"float32"), 0, ) 	 50840832 	 21267 	 9.992806434631348 	 3.955745220184326 	 0.24004554748535156 	 0.19018173217773438 	 None 	 None 	 None 	 None 	 
2025-08-05 23:56:34.677282 test begin: paddle.Tensor.__gt__(Tensor([1, 772, 65856],"float32"), 1e-09, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([1, 772, 65856],"float32"), 1e-09, ) 	 50840832 	 21267 	 10.439412593841553 	 3.954857587814331 	 0.23997282981872559 	 0.19002652168273926 	 None 	 None 	 None 	 None 	 
2025-08-05 23:56:52.744342 test begin: paddle.Tensor.__gt__(Tensor([2, 400, 65856],"float32"), 0, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([2, 400, 65856],"float32"), 0, ) 	 52684800 	 21267 	 10.340358018875122 	 5.3471455574035645 	 0.24840879440307617 	 0.19667267799377441 	 None 	 None 	 None 	 None 	 
2025-08-05 23:57:10.597134 test begin: paddle.Tensor.__gt__(Tensor([2, 400, 65856],"float32"), 1e-09, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([2, 400, 65856],"float32"), 1e-09, ) 	 52684800 	 21267 	 10.338574171066284 	 4.092410087585449 	 0.24835705757141113 	 0.19665193557739258 	 None 	 None 	 None 	 None 	 
2025-08-05 23:57:25.865652 test begin: paddle.Tensor.__gt__(Tensor([324000, 157],"float32"), 0.0, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([324000, 157],"float32"), 0.0, ) 	 50868000 	 21267 	 9.99143385887146 	 3.9694290161132812 	 0.24002289772033691 	 0.19008731842041016 	 None 	 None 	 None 	 None 	 
2025-08-05 23:57:42.094478 test begin: paddle.Tensor.__gt__(Tensor([635041, 80],"float32"), 0.0, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([635041, 80],"float32"), 0.0, ) 	 50803280 	 21267 	 9.982652425765991 	 3.952390193939209 	 0.2397441864013672 	 0.18990254402160645 	 None 	 None 	 None 	 None 	 
2025-08-05 23:57:56.822925 test begin: paddle.Tensor.__le__(Tensor([243360, 209],"float32"), 0.0, )
[Prof] paddle.Tensor.__le__ 	 paddle.Tensor.__le__(Tensor([243360, 209],"float32"), 0.0, ) 	 50862240 	 21231 	 9.972628831863403 	 4.404016733169556 	 0.23999786376953125 	 0.1900629997253418 	 None 	 None 	 None 	 None 	 
2025-08-05 23:58:14.259861 test begin: paddle.Tensor.__le__(Tensor([282240, 181],"float32"), 0.0, )
[Prof] paddle.Tensor.__le__ 	 paddle.Tensor.__le__(Tensor([282240, 181],"float32"), 0.0, ) 	 51085440 	 21231 	 10.011994361877441 	 3.965574264526367 	 0.24093127250671387 	 0.19086956977844238 	 None 	 None 	 None 	 None 	 
2025-08-05 23:58:29.052022 test begin: paddle.Tensor.__le__(Tensor([324000, 157],"float32"), 0.0, )
[Prof] paddle.Tensor.__le__ 	 paddle.Tensor.__le__(Tensor([324000, 157],"float32"), 0.0, ) 	 50868000 	 21231 	 9.976222276687622 	 3.962197780609131 	 0.24009156227111816 	 0.19014215469360352 	 None 	 None 	 None 	 None 	 
2025-08-05 23:58:45.208182 test begin: paddle.Tensor.__le__(Tensor([635041, 80],"float32"), 0.0, )
[Prof] paddle.Tensor.__le__ 	 paddle.Tensor.__le__(Tensor([635041, 80],"float32"), 0.0, ) 	 50803280 	 21231 	 9.963764429092407 	 3.9535491466522217 	 0.23978304862976074 	 0.1899397373199463 	 None 	 None 	 None 	 None 	 
2025-08-05 23:58:59.930520 test begin: paddle.Tensor.__len__(Tensor([1000, 1352, 376],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([1000, 1352, 376],"float32"), ) 	 508352000 	 2154792 	 13.008790254592896 	 13.91608190536499 	 0.00011086463928222656 	 0.0002658367156982422 	 None 	 None 	 None 	 None 	 
2025-08-05 23:59:36.928770 test begin: paddle.Tensor.__len__(Tensor([1000, 376, 1352],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([1000, 376, 1352],"float32"), ) 	 508352000 	 2154792 	 10.170230627059937 	 10.492591142654419 	 6.771087646484375e-05 	 0.0002422332763671875 	 None 	 None 	 None 	 None 	 
2025-08-06 00:00:05.827169 test begin: paddle.Tensor.__len__(Tensor([1000000, 509],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([1000000, 509],"float32"), ) 	 509000000 	 2154792 	 12.860703945159912 	 13.757416009902954 	 0.00014853477478027344 	 0.00019216537475585938 	 None 	 None 	 None 	 None 	 
2025-08-06 00:00:42.167006 test begin: paddle.Tensor.__len__(Tensor([230, 1501, 1501],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([230, 1501, 1501],"float32"), ) 	 518190230 	 2154792 	 9.806254386901855 	 10.25644850730896 	 6.580352783203125e-05 	 0.0003368854522705078 	 None 	 None 	 None 	 None 	 
2025-08-06 00:01:11.188587 test begin: paddle.Tensor.__len__(Tensor([3600, 376, 376],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([3600, 376, 376],"float32"), ) 	 508953600 	 2154792 	 10.084536790847778 	 10.455327033996582 	 6.270408630371094e-05 	 0.0002491474151611328 	 None 	 None 	 None 	 None 	 
2025-08-06 00:01:40.624980 test begin: paddle.Tensor.__len__(Tensor([500, 1501, 677],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([500, 1501, 677],"float32"), ) 	 508088500 	 2154792 	 10.08547568321228 	 10.281466007232666 	 6.985664367675781e-05 	 0.00025272369384765625 	 None 	 None 	 None 	 None 	 
2025-08-06 00:02:09.448786 test begin: paddle.Tensor.__len__(Tensor([500, 677, 1501],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([500, 677, 1501],"float32"), ) 	 508088500 	 2154792 	 10.20905089378357 	 10.371738910675049 	 6.67572021484375e-05 	 0.00025534629821777344 	 None 	 None 	 None 	 None 	 
2025-08-06 00:02:39.330869 test begin: paddle.Tensor.__len__(Tensor([5080330, 100],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([5080330, 100],"float32"), ) 	 508033000 	 2154792 	 9.987766027450562 	 10.145328044891357 	 0.00011849403381347656 	 0.00011324882507324219 	 None 	 None 	 None 	 None 	 
2025-08-06 00:03:09.869171 test begin: paddle.Tensor.__lshift__(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), ) 	 101607000 	 22338 	 10.051416635513306 	 9.973411798477173 	 0.45984339714050293 	 0.4562819004058838 	 None 	 None 	 None 	 None 	 
2025-08-06 00:03:31.138499 test begin: paddle.Tensor.__lshift__(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), ) 	 101606800 	 22338 	 10.052850484848022 	 9.990293741226196 	 0.4599168300628662 	 0.4562876224517822 	 None 	 None 	 None 	 None 	 
2025-08-06 00:03:55.896331 test begin: paddle.Tensor.__lshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), ) 	 203213200 	 22338 	 10.001526832580566 	 10.051305294036865 	 0.4577765464782715 	 0.45983099937438965 	 None 	 None 	 None 	 None 	 
2025-08-06 00:04:18.059345 test begin: paddle.Tensor.__lshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, ) 	 203213200 	 22338 	 9.999991416931152 	 10.051600933074951 	 0.4576232433319092 	 0.4599032402038574 	 None 	 None 	 None 	 None 	 
2025-08-06 00:04:42.240706 test begin: paddle.Tensor.__lshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), ) 	 203213400 	 22338 	 10.005107879638672 	 10.049209594726562 	 0.4574899673461914 	 0.45981740951538086 	 None 	 None 	 None 	 None 	 
2025-08-06 00:05:04.959292 test begin: paddle.Tensor.__lshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, ) 	 203213400 	 22338 	 10.004024267196655 	 10.06132698059082 	 0.45760631561279297 	 0.4597609043121338 	 None 	 None 	 None 	 None 	 
2025-08-06 00:05:27.255502 test begin: paddle.Tensor.__lt__(Tensor([1034, 3, 64, 128],"float64"), 1, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([1034, 3, 64, 128],"float64"), 1, ) 	 25411584 	 22019 	 10.004230737686157 	 5.345612287521362 	 0.2320702075958252 	 0.17184233665466309 	 None 	 None 	 None 	 None 	 
2025-08-06 00:05:45.182757 test begin: paddle.Tensor.__lt__(Tensor([256, 13, 64, 128],"float64"), 1, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([256, 13, 64, 128],"float64"), 1, ) 	 27262976 	 22019 	 10.695070505142212 	 3.960940361022949 	 0.24813437461853027 	 0.18382954597473145 	 None 	 None 	 None 	 None 	 
2025-08-06 00:06:00.431182 test begin: paddle.Tensor.__lt__(Tensor([256, 3, 259, 128],"float64"), 1, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([256, 3, 259, 128],"float64"), 1, ) 	 25460736 	 22019 	 10.01582384109497 	 3.7286031246185303 	 0.2328965663909912 	 0.17217183113098145 	 None 	 None 	 None 	 None 	 
2025-08-06 00:06:15.872854 test begin: paddle.Tensor.__lt__(Tensor([256, 3, 64, 517],"float64"), 1, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([256, 3, 64, 517],"float64"), 1, ) 	 25411584 	 22019 	 10.00225830078125 	 3.7025887966156006 	 0.23206281661987305 	 0.17181396484375 	 None 	 None 	 None 	 None 	 
2025-08-06 00:06:30.134273 test begin: paddle.Tensor.__lt__(Tensor([4, 157920, 81],"float32"), 0.1111111111111111, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([4, 157920, 81],"float32"), 0.1111111111111111, ) 	 51166080 	 22019 	 10.401318311691284 	 4.134393215179443 	 0.24125933647155762 	 0.1912236213684082 	 None 	 None 	 None 	 None 	 
2025-08-06 00:06:46.782481 test begin: paddle.Tensor.__lt__(Tensor([4, 1814401, 7],"float32"), 0.1111111111111111, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([4, 1814401, 7],"float32"), 0.1111111111111111, ) 	 50803228 	 22019 	 10.326739072799683 	 4.09217095375061 	 0.2395789623260498 	 0.18993186950683594 	 None 	 None 	 None 	 None 	 
2025-08-06 00:07:01.997619 test begin: paddle.Tensor.__lt__(Tensor([46, 157920, 7],"float32"), 0.1111111111111111, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([46, 157920, 7],"float32"), 0.1111111111111111, ) 	 50850240 	 22019 	 10.33777642250061 	 5.955002784729004 	 0.23996829986572266 	 0.1901381015777588 	 None 	 None 	 None 	 None 	 
2025-08-06 00:07:21.611604 test begin: paddle.Tensor.__lt__(Tensor([50803201],"float32"), 0.7, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([50803201],"float32"), 0.7, ) 	 50803201 	 22019 	 10.326041460037231 	 4.092198133468628 	 0.23965907096862793 	 0.18995285034179688 	 None 	 None 	 None 	 None 	 
2025-08-06 00:07:38.254450 test begin: paddle.Tensor.__matmul__(Tensor([10, 2304, 2304],"float32"), Tensor([10, 2304, 64],"float32"), )
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([10, 2304, 2304],"float32"), Tensor([10, 2304, 64],"float32"), ) 	 54558720 	 12494 	 11.613216161727905 	 11.612181901931763 	 0.949946403503418 	 0.949904203414917 	 17.18351459503174 	 17.184786081314087 	 0.7027475833892822 	 0.7028446197509766 	 
2025-08-06 00:08:42.972841 test begin: paddle.Tensor.__matmul__(Tensor([111, 3, 392, 392],"float32"), Tensor([111, 3, 392, 32],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([111, 3, 392, 392],"float32"), Tensor([111, 3, 392, 32],"float32"), ) 	 55347264 	 12494 	 12.916412115097046 	 12.91986894607544 	 1.0565783977508545 	 1.0564749240875244 	 18.120002031326294 	 18.115480661392212 	 0.7411057949066162 	 0.7408225536346436 	 
2025-08-06 00:09:46.232554 test begin: paddle.Tensor.__matmul__(Tensor([1351, 3, 392, 392],"float32"), Tensor([1351, 3, 392, 32],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f13f860bfa0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 00:20:03.965512 test begin: paddle.Tensor.__matmul__(Tensor([176, 2, 392, 392],"float32"), Tensor([176, 2, 392, 32],"float32"), )
W0806 00:20:05.033309 115106 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([176, 2, 392, 392],"float32"), Tensor([176, 2, 392, 32],"float32"), ) 	 58505216 	 12494 	 13.881911993026733 	 13.904160499572754 	 1.1355311870574951 	 1.1356101036071777 	 19.378539562225342 	 19.379695892333984 	 0.7925460338592529 	 0.792583703994751 	 
2025-08-06 00:21:15.808036 test begin: paddle.Tensor.__matmul__(Tensor([176, 24, 392, 392],"float32"), Tensor([176, 24, 392, 32],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fec89e1ae00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 00:31:34.344572 test begin: paddle.Tensor.__matmul__(Tensor([176, 3, 246, 392],"float32"), Tensor([176, 3, 392, 32],"float32"), )
W0806 00:31:42.823215 115431 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([176, 3, 246, 392],"float32"), Tensor([176, 3, 392, 32],"float32"), ) 	 57539328 	 12494 	 9.987203598022461 	 9.984435081481934 	 0.816251277923584 	 0.8164269924163818 	 16.852798461914062 	 16.85423755645752 	 0.6892740726470947 	 0.6892971992492676 	 
2025-08-06 00:32:39.146930 test begin: paddle.Tensor.__matmul__(Tensor([176, 3, 392, 392],"float32"), Tensor([176, 3, 392, 246],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([176, 3, 392, 392],"float32"), Tensor([176, 3, 392, 246],"float32"), ) 	 132050688 	 12494 	 39.53799295425415 	 39.53273892402649 	 3.2340168952941895 	 3.2337238788604736 	 89.5117199420929 	 89.5019166469574 	 3.660804510116577 	 3.660672903060913 	 
2025-08-06 00:37:02.262012 test begin: paddle.Tensor.__matmul__(Tensor([345, 2304, 2304],"float32"), Tensor([345, 2304, 64],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f57bb85e980>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 00:47:27.714355 test begin: paddle.Tensor.__matmul__(Tensor([49, 1024, 1024],"float32"), Tensor([49, 1024, 64],"float32"), )
W0806 00:47:28.705852 116082 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([49, 1024, 1024],"float32"), Tensor([49, 1024, 64],"float32"), ) 	 54591488 	 12494 	 10.35371994972229 	 10.352982759475708 	 0.8469371795654297 	 0.846837043762207 	 15.768044233322144 	 15.767823934555054 	 0.6449368000030518 	 0.6448497772216797 	 
2025-08-06 00:48:23.251455 test begin: paddle.Tensor.__matmul__(Tensor([60, 2304, 2304],"float32"), Tensor([60, 2304, 368],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fed948e2c80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 00:58:45.823955 test begin: paddle.Tensor.__matmul__(Tensor([60, 368, 2304],"float32"), Tensor([60, 2304, 64],"float32"), )
W0806 00:58:46.946628 116484 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([60, 368, 2304],"float32"), Tensor([60, 2304, 64],"float32"), ) 	 59719680 	 12494 	 11.608988761901855 	 11.604820251464844 	 0.9491679668426514 	 0.9491326808929443 	 14.961575269699097 	 14.961397886276245 	 0.6119704246520996 	 0.6118788719177246 	 
2025-08-06 00:59:42.502706 test begin: paddle.Tensor.__matmul__(Tensor([776, 1024, 1024],"float32"), Tensor([776, 1024, 64],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9be0d66c80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 01:09:56.165678 test begin: paddle.Tensor.__matmul__(Tensor([96, 1024, 1024],"float32"), Tensor([96, 1024, 517],"float32"), )
W0806 01:09:58.578876 116968 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([96, 1024, 1024],"float32"), Tensor([96, 1024, 517],"float32"), ) 	 151486464 	 12494 	 92.13985300064087 	 92.12061858177185 	 7.536744594573975 	 7.535143852233887 	 166.78179550170898 	 166.7657654285431 	 6.820953845977783 	 6.8206610679626465 	 
2025-08-06 01:18:40.784228 test begin: paddle.Tensor.__matmul__(Tensor([96, 517, 1024],"float32"), Tensor([96, 1024, 64],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([96, 517, 1024],"float32"), Tensor([96, 1024, 64],"float32"), ) 	 57114624 	 12494 	 12.894827127456665 	 13.34088659286499 	 1.0546975135803223 	 1.5024583339691162 	 17.139094829559326 	 17.141886472702026 	 0.7009396553039551 	 0.7010631561279297 	 
2025-08-06 01:19:42.618284 test begin: paddle.Tensor.__mod__(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), ) 	 50803220 	 22349 	 10.010589599609375 	 10.002009391784668 	 0.45777297019958496 	 0.45731067657470703 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 01:20:20.685722 test begin: paddle.Tensor.__mod__(Tensor([13, 2, 976985],"int64"), 16, )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([13, 2, 976985],"int64"), 16, ) 	 25401610 	 22349 	 13.017680883407593 	 6.694338321685791 	 0.29746389389038086 	 0.3055119514465332 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 01:20:49.533638 test begin: paddle.Tensor.__mod__(Tensor([13, 30531, 64],"int64"), 16, )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([13, 30531, 64],"int64"), 16, ) 	 25401792 	 22349 	 13.013861179351807 	 6.9278013706207275 	 0.29746174812316895 	 0.3056178092956543 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 01:21:18.470419 test begin: paddle.Tensor.__mod__(Tensor([198451, 2, 64],"int64"), 16, )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([198451, 2, 64],"int64"), 16, ) 	 25401728 	 22349 	 13.004093170166016 	 7.840766429901123 	 0.2973151206970215 	 0.30564045906066895 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 01:21:48.624607 test begin: paddle.Tensor.__mod__(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), ) 	 50804736 	 22349 	 10.021889209747314 	 10.002387523651123 	 0.45802927017211914 	 0.45739197731018066 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 01:22:26.648133 test begin: paddle.Tensor.__mod__(Tensor([26, 976985],"int64"), 64, )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([26, 976985],"int64"), 64, ) 	 25401610 	 22349 	 13.013792276382446 	 6.684021472930908 	 0.29746246337890625 	 0.3056671619415283 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 01:22:56.391294 test begin: paddle.Tensor.__mod__(Tensor([396901, 64],"int64"), 64, )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([396901, 64],"int64"), 64, ) 	 25401664 	 22349 	 13.003940105438232 	 6.683135032653809 	 0.2972526550292969 	 0.30558109283447266 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 01:23:23.637208 test begin: paddle.Tensor.__mul__(Tensor([1, 1, 32768, 32768],"float16"), 10000.0, )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([1, 1, 32768, 32768],"float16"), 10000.0, ) 	 1073741824 	 33522 	 104.1053831577301 	 103.62894225120544 	 3.1738908290863037 	 3.159301996231079 	 104.11169910430908 	 103.62136912345886 	 3.1740622520446777 	 3.1593801975250244 	 
2025-08-06 01:30:59.628059 test begin: paddle.Tensor.__mul__(Tensor([108544, 469],"float32"), Tensor([108544, 469],"float32"), )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([108544, 469],"float32"), Tensor([108544, 469],"float32"), ) 	 101814272 	 33522 	 15.1277015209198 	 15.022204637527466 	 0.46115875244140625 	 0.4572560787200928 	 39.099732637405396 	 29.9948947429657 	 1.192091464996338 	 0.45720410346984863 	 
2025-08-06 01:32:42.055292 test begin: paddle.Tensor.__mul__(Tensor([111616, 456],"float32"), Tensor([111616, 456],"float32"), )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([111616, 456],"float32"), Tensor([111616, 456],"float32"), ) 	 101793792 	 33522 	 15.123329401016235 	 15.012418508529663 	 0.46106958389282227 	 0.4571514129638672 	 39.35079836845398 	 29.990967988967896 	 1.4647791385650635 	 0.4570930004119873 	 
2025-08-06 01:34:26.298164 test begin: paddle.Tensor.__mul__(Tensor([14176, 3584],"float32"), Tensor([14176, 3584],"float32"), )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([14176, 3584],"float32"), Tensor([14176, 3584],"float32"), ) 	 101613568 	 33522 	 15.098560333251953 	 14.97120189666748 	 0.46031832695007324 	 0.4563429355621338 	 39.08885931968689 	 29.941357851028442 	 1.2631609439849854 	 0.45634031295776367 	 
2025-08-06 01:36:08.693371 test begin: paddle.Tensor.__mul__(Tensor([2, 1, 1551, 32768],"float16"), 10000.0, )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([2, 1, 1551, 32768],"float16"), 10000.0, ) 	 101646336 	 33522 	 10.000808715820312 	 9.92921233177185 	 0.3049159049987793 	 0.30272483825683594 	 10.000723600387573 	 9.928494691848755 	 0.30489158630371094 	 0.30272769927978516 	 
2025-08-06 01:36:52.618774 test begin: paddle.Tensor.__mul__(Tensor([2, 1, 32768, 1551],"float16"), 10000.0, )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([2, 1, 32768, 1551],"float16"), 10000.0, ) 	 101646336 	 33522 	 10.001035451889038 	 9.92935037612915 	 0.3049430847167969 	 0.30272936820983887 	 10.000814437866211 	 9.92860221862793 	 0.3048417568206787 	 0.30263352394104004 	 
2025-08-06 01:37:39.472644 test begin: paddle.Tensor.__mul__(Tensor([2, 1, 32768, 32768],"float16"), 10000.0, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc60acfaf50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 01:47:46.835075 test begin: paddle.Tensor.__ne__(Tensor([144, 392, 901],"float32"), 0, )
W0806 01:47:47.836462 118104 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([144, 392, 901],"float32"), 0, ) 	 50859648 	 21223 	 9.978210926055908 	 3.9501423835754395 	 0.24020028114318848 	 0.19016599655151367 	 None 	 None 	 None 	 None 	 
2025-08-06 01:48:02.991831 test begin: paddle.Tensor.__ne__(Tensor([144, 901, 392],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([144, 901, 392],"float32"), 0, ) 	 50859648 	 21223 	 10.206313371658325 	 3.9488441944122314 	 0.2402207851409912 	 0.19014286994934082 	 None 	 None 	 None 	 None 	 
2025-08-06 01:48:19.751528 test begin: paddle.Tensor.__ne__(Tensor([160, 392, 811],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([160, 392, 811],"float32"), 0, ) 	 50865920 	 21223 	 9.980989933013916 	 3.949528694152832 	 0.24027419090270996 	 0.1902170181274414 	 None 	 None 	 None 	 None 	 
2025-08-06 01:48:34.648725 test begin: paddle.Tensor.__ne__(Tensor([160, 811, 392],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([160, 811, 392],"float32"), 0, ) 	 50865920 	 21223 	 10.529141902923584 	 3.9497718811035156 	 0.24018311500549316 	 0.1901247501373291 	 None 	 None 	 None 	 None 	 
2025-08-06 01:48:51.625820 test begin: paddle.Tensor.__ne__(Tensor([176, 392, 737],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([176, 392, 737],"float32"), 0, ) 	 50847104 	 21223 	 9.97132658958435 	 3.9478611946105957 	 0.24006056785583496 	 0.19010281562805176 	 None 	 None 	 None 	 None 	 
2025-08-06 01:49:06.417654 test begin: paddle.Tensor.__ne__(Tensor([176, 737, 392],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([176, 737, 392],"float32"), 0, ) 	 50847104 	 21223 	 10.808212757110596 	 3.9477992057800293 	 0.2400052547454834 	 0.19012904167175293 	 None 	 None 	 None 	 None 	 
2025-08-06 01:49:23.670305 test begin: paddle.Tensor.__ne__(Tensor([331, 392, 392],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([331, 392, 392],"float32"), 0, ) 	 50862784 	 21223 	 9.973881959915161 	 3.9508697986602783 	 0.24039649963378906 	 0.19013595581054688 	 None 	 None 	 None 	 None 	 
2025-08-06 01:49:38.826736 test begin: paddle.Tensor.__neg__(Tensor([128, 396901],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([128, 396901],"float32"), ) 	 50803328 	 33840 	 10.499637365341187 	 10.072248697280884 	 0.30207133293151855 	 0.3041965961456299 	 10.019083023071289 	 10.07269287109375 	 0.30257606506347656 	 0.3042268753051758 	 
2025-08-06 01:50:22.043175 test begin: paddle.Tensor.__neg__(Tensor([128, 793801],"float16"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([128, 793801],"float16"), ) 	 101606528 	 33840 	 10.101476907730103 	 10.020333528518677 	 0.3050837516784668 	 0.30260515213012695 	 10.087350130081177 	 10.015413999557495 	 0.3046276569366455 	 0.30243659019470215 	 
2025-08-06 01:51:08.039182 test begin: paddle.Tensor.__neg__(Tensor([22, 81, 94, 311],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([22, 81, 94, 311],"float32"), ) 	 52094988 	 33840 	 10.282926797866821 	 10.323307752609253 	 0.31022071838378906 	 0.3117036819458008 	 10.267619371414185 	 10.32088327407837 	 0.3100757598876953 	 0.3117098808288574 	 
2025-08-06 01:51:53.160200 test begin: paddle.Tensor.__neg__(Tensor([264, 192612],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([264, 192612],"float32"), ) 	 50849568 	 33840 	 10.024040460586548 	 10.081765413284302 	 0.3027045726776123 	 0.30454158782958984 	 10.024009943008423 	 10.081631898880005 	 0.302706241607666 	 0.3045003414154053 	 
2025-08-06 01:52:35.954849 test begin: paddle.Tensor.__neg__(Tensor([4, 435, 94, 311],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([4, 435, 94, 311],"float32"), ) 	 50867160 	 33840 	 10.554702043533325 	 10.085946321487427 	 0.3025057315826416 	 0.304612398147583 	 10.026926755905151 	 10.087595701217651 	 0.30281949043273926 	 0.3046541213989258 	 
2025-08-06 01:53:18.943269 test begin: paddle.Tensor.__neg__(Tensor([4, 81, 505, 311],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([4, 81, 505, 311],"float32"), ) 	 50885820 	 33840 	 10.015476942062378 	 10.09626030921936 	 0.3024404048919678 	 0.30474185943603516 	 10.030368089675903 	 10.090483665466309 	 0.30297017097473145 	 0.3047451972961426 	 
2025-08-06 01:54:01.968828 test begin: paddle.Tensor.__neg__(Tensor([4, 81, 94, 1669],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([4, 81, 94, 1669],"float32"), ) 	 50831064 	 33840 	 10.015160322189331 	 10.07820200920105 	 0.3024425506591797 	 0.30437755584716797 	 10.02058458328247 	 10.078469276428223 	 0.30262017250061035 	 0.3044002056121826 	 
2025-08-06 01:54:44.002557 test begin: paddle.Tensor.__neg__(Tensor([528, 192612],"float16"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([528, 192612],"float16"), ) 	 101699136 	 33840 	 10.101529359817505 	 10.03169584274292 	 0.30484557151794434 	 0.3028857707977295 	 10.10096001625061 	 10.023528814315796 	 0.3050680160522461 	 0.30268120765686035 	 
2025-08-06 01:55:28.275620 test begin: paddle.Tensor.__or__(Tensor([1, 210, 241921],"bool"), Tensor([1, 210, 241921],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 210, 241921],"bool"), Tensor([1, 210, 241921],"bool"), ) 	 101606820 	 85484 	 10.092903852462769 	 9.967393636703491 	 0.12063884735107422 	 0.11910820007324219 	 None 	 None 	 None 	 None 	 
2025-08-06 01:55:50.606295 test begin: paddle.Tensor.__or__(Tensor([1, 210, 75600],"bool"), Tensor([4, 210, 75600],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 210, 75600],"bool"), Tensor([4, 210, 75600],"bool"), ) 	 79380000 	 85484 	 13.838273286819458 	 23.752885818481445 	 0.16543364524841309 	 0.2839682102203369 	 None 	 None 	 None 	 None 	 
2025-08-06 01:56:29.315340 test begin: paddle.Tensor.__or__(Tensor([1, 218, 233043],"bool"), Tensor([1, 218, 233043],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 218, 233043],"bool"), Tensor([1, 218, 233043],"bool"), ) 	 101606748 	 85484 	 10.092798948287964 	 9.97744083404541 	 0.12068390846252441 	 0.11902093887329102 	 None 	 None 	 None 	 None 	 
2025-08-06 01:56:50.841634 test begin: paddle.Tensor.__or__(Tensor([1, 218, 70644],"bool"), Tensor([4, 218, 70644],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 218, 70644],"bool"), Tensor([4, 218, 70644],"bool"), ) 	 77001960 	 85484 	 13.441447973251343 	 23.019115447998047 	 0.16068005561828613 	 0.2751147747039795 	 None 	 None 	 None 	 None 	 
2025-08-06 01:57:29.694605 test begin: paddle.Tensor.__or__(Tensor([1, 400, 127009],"bool"), Tensor([1, 400, 127009],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 400, 127009],"bool"), Tensor([1, 400, 127009],"bool"), ) 	 101607200 	 85484 	 10.09719204902649 	 10.014713764190674 	 0.12077522277832031 	 0.11966228485107422 	 None 	 None 	 None 	 None 	 
2025-08-06 01:57:51.491642 test begin: paddle.Tensor.__or__(Tensor([1, 400, 65856],"bool"), Tensor([2, 400, 65856],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 400, 65856],"bool"), Tensor([2, 400, 65856],"bool"), ) 	 79027200 	 85484 	 11.54746961593628 	 19.808753728866577 	 0.13805222511291504 	 0.23681950569152832 	 None 	 None 	 None 	 None 	 
2025-08-06 01:58:23.962417 test begin: paddle.Tensor.__or__(Tensor([1, 673, 75600],"bool"), Tensor([1, 673, 75600],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 673, 75600],"bool"), Tensor([1, 673, 75600],"bool"), ) 	 101757600 	 85484 	 10.092146396636963 	 9.864100456237793 	 0.12064290046691895 	 0.11774206161499023 	 None 	 None 	 None 	 None 	 
2025-08-06 01:58:47.676170 test begin: paddle.Tensor.__or__(Tensor([1, 720, 70644],"bool"), Tensor([1, 720, 70644],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 720, 70644],"bool"), Tensor([1, 720, 70644],"bool"), ) 	 101727360 	 85484 	 10.089038610458374 	 9.918837547302246 	 0.12065720558166504 	 0.11858963966369629 	 None 	 None 	 None 	 None 	 
2025-08-06 01:59:09.158268 test begin: paddle.Tensor.__or__(Tensor([1, 772, 65856],"bool"), Tensor([1, 772, 65856],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 772, 65856],"bool"), Tensor([1, 772, 65856],"bool"), ) 	 101681664 	 85484 	 10.06523585319519 	 9.833837032318115 	 0.12035655975341797 	 0.11748242378234863 	 None 	 None 	 None 	 None 	 
2025-08-06 01:59:31.856744 test begin: paddle.Tensor.__or__(Tensor([2, 400, 65856],"bool"), Tensor([1, 400, 65856],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([2, 400, 65856],"bool"), Tensor([1, 400, 65856],"bool"), ) 	 79027200 	 85484 	 15.098465204238892 	 19.798985481262207 	 0.1804821491241455 	 0.23670434951782227 	 None 	 None 	 None 	 None 	 
2025-08-06 02:00:07.871722 test begin: paddle.Tensor.__or__(Tensor([2, 400, 65856],"bool"), Tensor([2, 400, 65856],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([2, 400, 65856],"bool"), Tensor([2, 400, 65856],"bool"), ) 	 105369600 	 85484 	 10.44490098953247 	 10.158669233322144 	 0.12477803230285645 	 0.12143540382385254 	 None 	 None 	 None 	 None 	 
2025-08-06 02:00:29.997579 test begin: paddle.rank(input=Tensor([201, 12700801],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([201, 12700801],"float64"), ) 	 2552861001 	 247008 	 12.939440488815308 	 8.81024169921875 	 4.124641418457031e-05 	 8.082389831542969e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:01:53.664302 test begin: paddle.rank(input=Tensor([301, 2, 2, 2116801],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([301, 2, 2, 2116801],"float64"), ) 	 2548628404 	 247008 	 9.785802602767944 	 6.594741582870483 	 4.172325134277344e-05 	 8.106231689453125e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:03:10.126354 test begin: paddle.rank(input=Tensor([301, 2, 2116801, 2],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([301, 2, 2116801, 2],"float64"), ) 	 2548628404 	 247008 	 9.759429931640625 	 6.657199144363403 	 3.8623809814453125e-05 	 7.939338684082031e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:04:26.714767 test begin: paddle.rank(input=Tensor([301, 2116801, 2, 2],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([301, 2116801, 2, 2],"float64"), ) 	 2548628404 	 247008 	 9.797401666641235 	 6.580446243286133 	 3.743171691894531e-05 	 8.034706115722656e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:05:43.224047 test begin: paddle.rank(input=Tensor([317520101, 2, 2, 2],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([317520101, 2, 2, 2],"float64"), ) 	 2540160808 	 247008 	 13.050378799438477 	 8.682350397109985 	 4.1484832763671875e-05 	 8.916854858398438e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:07:06.769247 test begin: paddle.reciprocal(Tensor([125, 1, 640, 640],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([125, 1, 640, 640],"float32"), ) 	 51200000 	 33850 	 10.069470405578613 	 10.16628909111023 	 0.3040168285369873 	 0.3068552017211914 	 15.330897569656372 	 35.482176303863525 	 0.4628753662109375 	 0.35716748237609863 	 
2025-08-06 02:08:19.651362 test begin: paddle.reciprocal(Tensor([16, 1, 4962, 640],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([16, 1, 4962, 640],"float32"), ) 	 50810880 	 33850 	 9.998783826828003 	 10.088206768035889 	 0.30188488960266113 	 0.30455994606018066 	 15.216392755508423 	 35.21899628639221 	 0.4593391418457031 	 0.35448741912841797 	 
2025-08-06 02:09:31.977097 test begin: paddle.reciprocal(Tensor([16, 1, 640, 4962],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([16, 1, 640, 4962],"float32"), ) 	 50810880 	 33850 	 9.999401807785034 	 10.088337659835815 	 0.3018643856048584 	 0.3046598434448242 	 15.216731071472168 	 35.21825695037842 	 0.4594388008117676 	 0.35448408126831055 	 
2025-08-06 02:10:44.342891 test begin: paddle.reciprocal(Tensor([16, 8, 640, 640],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([16, 8, 640, 640],"float32"), ) 	 52428800 	 33850 	 10.314320087432861 	 10.415253639221191 	 0.31139230728149414 	 0.31415438652038574 	 15.697580337524414 	 36.32050347328186 	 0.47390246391296387 	 0.3656039237976074 	 
2025-08-06 02:11:59.343933 test begin: paddle.reciprocal(Tensor([4, 1, 13231, 960],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([4, 1, 13231, 960],"float32"), ) 	 50807040 	 33850 	 10.00238847732544 	 10.088083028793335 	 0.30200695991516113 	 0.30457162857055664 	 15.213331937789917 	 35.21621823310852 	 0.4592423439025879 	 0.3544807434082031 	 
2025-08-06 02:13:12.621044 test begin: paddle.reciprocal(Tensor([4, 1, 960, 13231],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([4, 1, 960, 13231],"float32"), ) 	 50807040 	 33850 	 10.002467393875122 	 10.096299648284912 	 0.3020143508911133 	 0.30455851554870605 	 15.209695100784302 	 35.21532201766968 	 0.45923686027526855 	 0.354447603225708 	 
2025-08-06 02:14:25.850095 test begin: paddle.reciprocal(Tensor([4, 14, 960, 960],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([4, 14, 960, 960],"float32"), ) 	 51609600 	 33850 	 10.148299932479858 	 10.242621421813965 	 0.3064577579498291 	 0.30922913551330566 	 15.452296257019043 	 35.76189184188843 	 0.4665520191192627 	 0.35990095138549805 	 
2025-08-06 02:15:41.385411 test begin: paddle.reciprocal(Tensor([56, 1, 960, 960],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([56, 1, 960, 960],"float32"), ) 	 51609600 	 33850 	 10.148605823516846 	 10.242861032485962 	 0.3064143657684326 	 0.30926513671875 	 15.452571868896484 	 35.76114058494568 	 0.4666004180908203 	 0.35998988151550293 	 
2025-08-06 02:16:54.782687 test begin: paddle.reciprocal(Tensor([8, 1, 6616, 960],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([8, 1, 6616, 960],"float32"), ) 	 50810880 	 33850 	 9.998714923858643 	 10.095125198364258 	 0.30188417434692383 	 0.30458736419677734 	 15.215109586715698 	 35.218764305114746 	 0.4593331813812256 	 0.3544883728027344 	 
2025-08-06 02:18:07.531988 test begin: paddle.reciprocal(Tensor([8, 1, 960, 6616],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([8, 1, 960, 6616],"float32"), ) 	 50810880 	 33850 	 9.998828411102295 	 10.123048067092896 	 0.3019406795501709 	 0.3046243190765381 	 15.216748476028442 	 35.21908235549927 	 0.4594290256500244 	 0.3544778823852539 	 
2025-08-06 02:19:20.189690 test begin: paddle.reciprocal(Tensor([8, 7, 960, 960],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([8, 7, 960, 960],"float32"), ) 	 51609600 	 33850 	 10.148144721984863 	 10.26553463935852 	 0.30640411376953125 	 0.30920886993408203 	 15.454215049743652 	 35.76231789588928 	 0.4665946960449219 	 0.35995984077453613 	 
2025-08-06 02:20:38.846171 test begin: paddle.reduce_as(Tensor([30, 1270081, 40],"float32"), Tensor([1270081, 40],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fec7786eaa0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 02:30:51.916769 test begin: paddle.reduce_as(Tensor([30, 200, 254017],"float32"), Tensor([200, 254017],"float32"), )
W0806 02:31:17.822110 119048 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc4dbba2f50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 02:41:01.710287 test begin: paddle.reduce_as(Tensor([30, 200, 8468],"float32"), Tensor([200, 8468],"float32"), )
W0806 02:41:02.671315 119598 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.reduce_as 	 paddle.reduce_as(Tensor([30, 200, 8468],"float32"), Tensor([200, 8468],"float32"), ) 	 52501600 	 56022 	 9.999164819717407 	 8.705164909362793 	 0.18231821060180664 	 0.15880680084228516 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 02:41:33.186134 test begin: paddle.reduce_as(Tensor([30, 42337, 40],"float32"), Tensor([42337, 40],"float32"), )
[Prof] paddle.reduce_as 	 paddle.reduce_as(Tensor([30, 42337, 40],"float32"), Tensor([42337, 40],"float32"), ) 	 52497880 	 56022 	 10.04807710647583 	 8.753891468048096 	 0.1832132339477539 	 0.15964746475219727 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 02:42:07.624935 test begin: paddle.reduce_as(Tensor([6351, 200, 40],"float32"), Tensor([200, 40],"float32"), )
[Prof] paddle.reduce_as 	 paddle.reduce_as(Tensor([6351, 200, 40],"float32"), Tensor([200, 40],"float32"), ) 	 50816000 	 56022 	 14.508596897125244 	 8.701980352401733 	 0.13231611251831055 	 0.07934355735778809 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 02:42:39.993339 test begin: paddle.remainder(Tensor([1, 2, 1270081, 4, 5],"float32"), Tensor([1, 2, 1270081, 4, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 1270081, 4, 5],"float32"), Tensor([1, 2, 1270081, 4, 5],"float32"), ) 	 101606480 	 33680 	 15.18727159500122 	 15.126396179199219 	 0.4606137275695801 	 0.4589719772338867 	 None 	 None 	 None 	 None 	 
2025-08-06 02:43:16.909615 test begin: paddle.remainder(Tensor([1, 2, 1270081, 4, 5],"int32"), Tensor([1, 2, 1270081, 4, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 1270081, 4, 5],"int32"), Tensor([1, 2, 1270081, 4, 5],"int32"), ) 	 101606480 	 33680 	 15.18397855758667 	 15.13646149635315 	 0.46065616607666016 	 0.4592106342315674 	 None 	 None 	 None 	 None 	 
2025-08-06 02:43:48.446736 test begin: paddle.remainder(Tensor([1, 2, 3, 1693441, 5],"float32"), Tensor([1, 2, 3, 1693441, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 1693441, 5],"float32"), Tensor([1, 2, 3, 1693441, 5],"float32"), ) 	 101606460 	 33680 	 15.184926509857178 	 15.125621795654297 	 0.4607529640197754 	 0.4590644836425781 	 None 	 None 	 None 	 None 	 
2025-08-06 02:44:21.270167 test begin: paddle.remainder(Tensor([1, 2, 3, 1693441, 5],"int32"), Tensor([1, 2, 3, 1693441, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 1693441, 5],"int32"), Tensor([1, 2, 3, 1693441, 5],"int32"), ) 	 101606460 	 33680 	 15.661489009857178 	 15.38018250465393 	 0.46064186096191406 	 0.459212064743042 	 None 	 None 	 None 	 None 	 
2025-08-06 02:44:58.228354 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 1058401],"float64"), Tensor([1, 2, 3, 4, 1058401],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 1058401],"float64"), Tensor([1, 2, 3, 4, 1058401],"float64"), ) 	 50803248 	 33680 	 15.036749839782715 	 15.370289325714111 	 0.4562826156616211 	 0.46640992164611816 	 None 	 None 	 None 	 None 	 
2025-08-06 02:45:29.743936 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 2116801],"float32"), Tensor([1, 2, 3, 4, 2116801],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 2116801],"float32"), Tensor([1, 2, 3, 4, 2116801],"float32"), ) 	 101606448 	 33680 	 15.20175552368164 	 15.126270771026611 	 0.46084046363830566 	 0.4589853286743164 	 None 	 None 	 None 	 None 	 
2025-08-06 02:46:03.677605 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 2116801],"int32"), Tensor([1, 2, 3, 4, 2116801],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 2116801],"int32"), Tensor([1, 2, 3, 4, 2116801],"int32"), ) 	 101606448 	 33680 	 15.180620193481445 	 15.134268283843994 	 0.4606661796569824 	 0.4592628479003906 	 None 	 None 	 None 	 None 	 
2025-08-06 02:46:36.846381 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 5],"float32"), Tensor([423361, 2, 3, 4, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 5],"float32"), Tensor([423361, 2, 3, 4, 5],"float32"), ) 	 50803440 	 33680 	 11.029814958572388 	 11.153218269348145 	 0.30396270751953125 	 0.33841729164123535 	 None 	 None 	 None 	 None 	 
2025-08-06 02:47:01.249770 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 5],"float64"), Tensor([211681, 2, 3, 4, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 5],"float64"), Tensor([211681, 2, 3, 4, 5],"float64"), ) 	 25401840 	 33680 	 14.673289775848389 	 12.455901861190796 	 0.4452478885650635 	 0.3779478073120117 	 None 	 None 	 None 	 None 	 
2025-08-06 02:47:28.956183 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 5],"int32"), Tensor([423361, 2, 3, 4, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 5],"int32"), Tensor([423361, 2, 3, 4, 5],"int32"), ) 	 50803440 	 33680 	 10.104361772537231 	 12.483179330825806 	 0.3065781593322754 	 0.3503909111022949 	 None 	 None 	 None 	 None 	 
2025-08-06 02:47:54.533845 test begin: paddle.remainder(Tensor([1, 2, 3, 846721, 5],"float64"), Tensor([1, 2, 3, 846721, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 846721, 5],"float64"), Tensor([1, 2, 3, 846721, 5],"float64"), ) 	 50803260 	 33680 	 15.034956693649292 	 15.371458768844604 	 0.4561891555786133 	 0.4663677215576172 	 None 	 None 	 None 	 None 	 
2025-08-06 02:48:26.083204 test begin: paddle.remainder(Tensor([1, 2, 635041, 4, 5],"float64"), Tensor([1, 2, 635041, 4, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 635041, 4, 5],"float64"), Tensor([1, 2, 635041, 4, 5],"float64"), ) 	 50803280 	 33680 	 15.038406133651733 	 15.380314826965332 	 0.4562711715698242 	 0.4666147232055664 	 None 	 None 	 None 	 None 	 
2025-08-06 02:48:59.040958 test begin: paddle.remainder(Tensor([1, 423361, 3, 4, 5],"float64"), Tensor([1, 423361, 3, 4, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 423361, 3, 4, 5],"float64"), Tensor([1, 423361, 3, 4, 5],"float64"), ) 	 50803320 	 33680 	 15.264824867248535 	 15.371912717819214 	 0.45624637603759766 	 0.46649813652038574 	 None 	 None 	 None 	 None 	 
2025-08-06 02:49:33.479036 test begin: paddle.remainder(Tensor([1, 846721, 3, 4, 5],"float32"), Tensor([1, 846721, 3, 4, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 846721, 3, 4, 5],"float32"), Tensor([1, 846721, 3, 4, 5],"float32"), ) 	 101606520 	 33680 	 15.183793544769287 	 15.126335144042969 	 0.4607510566711426 	 0.4590144157409668 	 None 	 None 	 None 	 None 	 
2025-08-06 02:50:06.812191 test begin: paddle.remainder(Tensor([1, 846721, 3, 4, 5],"int32"), Tensor([1, 846721, 3, 4, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 846721, 3, 4, 5],"int32"), Tensor([1, 846721, 3, 4, 5],"int32"), ) 	 101606520 	 33680 	 15.188941240310669 	 15.13392448425293 	 0.460604190826416 	 0.45928263664245605 	 None 	 None 	 None 	 None 	 
2025-08-06 02:50:43.020693 test begin: paddle.remainder(Tensor([211681, 2, 3, 4, 5],"float64"), Tensor([1, 2, 3, 4, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([211681, 2, 3, 4, 5],"float64"), Tensor([1, 2, 3, 4, 5],"float64"), ) 	 25401840 	 33680 	 13.869653940200806 	 12.537635087966919 	 0.42076563835144043 	 0.38040876388549805 	 None 	 None 	 None 	 None 	 
2025-08-06 02:51:13.193908 test begin: paddle.remainder(Tensor([211681, 2, 3, 4, 5],"float64"), Tensor([211681, 2, 3, 4, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([211681, 2, 3, 4, 5],"float64"), Tensor([211681, 2, 3, 4, 5],"float64"), ) 	 50803440 	 33680 	 15.277738571166992 	 15.371931314468384 	 0.4562678337097168 	 0.46636509895324707 	 None 	 None 	 None 	 None 	 
2025-08-06 02:51:47.728439 test begin: paddle.remainder(Tensor([423361, 2, 3, 4, 5],"float32"), Tensor([1, 2, 3, 4, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([423361, 2, 3, 4, 5],"float32"), Tensor([1, 2, 3, 4, 5],"float32"), ) 	 50803440 	 33680 	 9.98993730545044 	 11.163028001785278 	 0.3031041622161865 	 0.3387267589569092 	 None 	 None 	 None 	 None 	 
2025-08-06 02:52:09.737346 test begin: paddle.remainder(Tensor([423361, 2, 3, 4, 5],"float32"), Tensor([423361, 2, 3, 4, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([423361, 2, 3, 4, 5],"float32"), Tensor([423361, 2, 3, 4, 5],"float32"), ) 	 101606640 	 33680 	 15.18230152130127 	 15.366071939468384 	 0.4606363773345947 	 0.45900988578796387 	 None 	 None 	 None 	 None 	 
2025-08-06 02:52:44.343059 test begin: paddle.remainder(Tensor([423361, 2, 3, 4, 5],"int32"), Tensor([1, 2, 3, 4, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([423361, 2, 3, 4, 5],"int32"), Tensor([1, 2, 3, 4, 5],"int32"), ) 	 50803440 	 33680 	 10.0388503074646 	 11.376619100570679 	 0.304607629776001 	 0.34522032737731934 	 None 	 None 	 None 	 None 	 
2025-08-06 02:53:06.383179 test begin: paddle.remainder(Tensor([423361, 2, 3, 4, 5],"int32"), Tensor([423361, 2, 3, 4, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([423361, 2, 3, 4, 5],"int32"), Tensor([423361, 2, 3, 4, 5],"int32"), ) 	 101606640 	 33680 	 15.184262037277222 	 15.134034395217896 	 0.4607722759246826 	 0.459291934967041 	 None 	 None 	 None 	 None 	 
2025-08-06 02:53:39.830322 test begin: paddle.renorm(Tensor([10, 20, 254017],"float32"), 1.0, -1, 2.05, )
[Prof] paddle.renorm 	 paddle.renorm(Tensor([10, 20, 254017],"float32"), 1.0, -1, 2.05, ) 	 50803400 	 3503 	 9.97105598449707 	 1.6846487522125244 	 0.726118803024292 	 0.1638338565826416 	 19.43387770652771 	 10.220459699630737 	 1.4173741340637207 	 0.22912168502807617 	 
2025-08-06 02:54:23.028638 test begin: paddle.repeat_interleave(Tensor([1, 1500, 33869],"float32"), 5, axis=0, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([1, 1500, 33869],"float32"), 5, axis=0, ) 	 50803500 	 9813 	 18.19873285293579 	 14.800697088241577 	 0.9476213455200195 	 1.5413997173309326 	 23.66802668571472 	 8.579361200332642 	 0.8222446441650391 	 0.8935141563415527 	 
2025-08-06 02:55:33.970518 test begin: paddle.repeat_interleave(Tensor([1, 39691, 1280],"float32"), 5, axis=0, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([1, 39691, 1280],"float32"), 5, axis=0, ) 	 50804480 	 9813 	 18.122007846832275 	 14.644444227218628 	 0.9436168670654297 	 1.5252435207366943 	 23.56283688545227 	 8.464911699295044 	 0.8186438083648682 	 0.881601095199585 	 
2025-08-06 02:56:43.842398 test begin: paddle.repeat_interleave(Tensor([14, 1, 384, 9451],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([14, 1, 384, 9451],"float32"), repeats=3, axis=1, ) 	 50808576 	 9813 	 10.747937440872192 	 8.32633113861084 	 0.5596029758453369 	 0.8670456409454346 	 11.98799729347229 	 5.755966901779175 	 0.41643738746643066 	 0.599463939666748 	 
2025-08-06 02:57:24.520476 test begin: paddle.repeat_interleave(Tensor([14, 1, 9451, 384],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([14, 1, 9451, 384],"float32"), repeats=3, axis=1, ) 	 50808576 	 9813 	 10.747223138809204 	 8.325570821762085 	 0.5596036911010742 	 0.8670353889465332 	 11.994224548339844 	 5.757002592086792 	 0.4166123867034912 	 0.59940505027771 	 
2025-08-06 02:58:06.561942 test begin: paddle.repeat_interleave(Tensor([14, 25, 384, 384],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([14, 25, 384, 384],"float32"), repeats=3, axis=1, ) 	 51609600 	 9813 	 10.174723625183105 	 7.165348768234253 	 0.5297598838806152 	 0.7462179660797119 	 11.773319959640503 	 5.912013292312622 	 0.4089846611022949 	 0.6157925128936768 	 
2025-08-06 02:58:45.059561 test begin: paddle.repeat_interleave(Tensor([27, 1500, 1280],"float32"), 5, axis=0, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([27, 1500, 1280],"float32"), 5, axis=0, ) 	 51840000 	 9813 	 16.447945833206177 	 10.921000719070435 	 0.8564550876617432 	 1.1882901191711426 	 18.25328254699707 	 8.640259504318237 	 0.6341512203216553 	 0.8998010158538818 	 
2025-08-06 02:59:44.505157 test begin: paddle.repeat_interleave(Tensor([345, 1, 384, 384],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([345, 1, 384, 384],"float32"), repeats=3, axis=1, ) 	 50872320 	 9813 	 10.026884078979492 	 7.065492630004883 	 0.5221197605133057 	 0.7358338832855225 	 11.60358214378357 	 5.8291826248168945 	 0.403092622756958 	 0.6070718765258789 	 
2025-08-06 03:00:22.448106 test begin: paddle.repeat_interleave(Tensor([5, 1, 13231, 768],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([5, 1, 13231, 768],"float32"), repeats=3, axis=1, ) 	 50807040 	 9813 	 10.907247543334961 	 8.870064973831177 	 0.5679454803466797 	 0.9233675003051758 	 14.6966392993927 	 5.757951021194458 	 0.5105066299438477 	 0.5996303558349609 	 
2025-08-06 03:01:08.196844 test begin: paddle.repeat_interleave(Tensor([5, 1, 768, 13231],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([5, 1, 768, 13231],"float32"), repeats=3, axis=1, ) 	 50807040 	 9813 	 11.711091041564941 	 9.806404829025269 	 0.5679440498352051 	 0.9234733581542969 	 14.69646430015564 	 5.757523536682129 	 0.5105478763580322 	 0.5996284484863281 	 
2025-08-06 03:01:55.923168 test begin: paddle.repeat_interleave(Tensor([5, 18, 768, 768],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([5, 18, 768, 768],"float32"), repeats=3, axis=1, ) 	 53084160 	 9813 	 10.276726007461548 	 6.891101360321045 	 0.5350980758666992 	 0.7176344394683838 	 12.169078588485718 	 6.133742570877075 	 0.42270636558532715 	 0.6388375759124756 	 
2025-08-06 03:02:36.506720 test begin: paddle.repeat_interleave(Tensor([87, 1, 768, 768],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([87, 1, 768, 768],"float32"), repeats=3, axis=1, ) 	 51314688 	 9813 	 10.74875020980835 	 6.659708499908447 	 0.517024040222168 	 0.6935694217681885 	 11.765309572219849 	 5.93351411819458 	 0.40869998931884766 	 0.6180057525634766 	 
2025-08-06 03:03:15.814779 test begin: paddle.reshape(Tensor([141760, 7168],"bfloat16"), list[-1,7168,], )
[Prof] paddle.reshape 	 paddle.reshape(Tensor([141760, 7168],"bfloat16"), list[-1,7168,], ) 	 1016135680 	 66714 	 0.37965989112854004 	 0.25247693061828613 	 7.128715515136719e-05 	 7.653236389160156e-05 	 3.0638043880462646 	 299.97615218162537 	 5.269050598144531e-05 	 2.2977311611175537 	 
2025-08-06 03:08:54.668905 test begin: paddle.reverse(Tensor([12, 132301, 16],"float64"), axis=list[0,], )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([12, 132301, 16],"float64"), axis=list[0,], ) 	 25401792 	 19922 	 9.866066932678223 	 6.049455881118774 	 0.5061507225036621 	 0.31038737297058105 	 9.874907493591309 	 6.04899525642395 	 0.5065906047821045 	 0.3102707862854004 	 
2025-08-06 03:09:27.705783 test begin: paddle.reverse(Tensor([12, 264601, 8],"float64"), axis=0, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([12, 264601, 8],"float64"), axis=0, ) 	 25401696 	 19922 	 9.879072189331055 	 6.050459861755371 	 0.5068066120147705 	 0.3103671073913574 	 9.904260396957397 	 6.0497283935546875 	 0.508084774017334 	 0.3103358745574951 	 
2025-08-06 03:10:00.708859 test begin: paddle.reverse(Tensor([12, 4, 529201],"float64"), axis=0, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([12, 4, 529201],"float64"), axis=0, ) 	 25401648 	 19922 	 9.877973556518555 	 6.050018787384033 	 0.5067973136901855 	 0.31035804748535156 	 9.897377729415894 	 6.049653768539429 	 0.50773024559021 	 0.31032299995422363 	 
2025-08-06 03:10:33.709239 test begin: paddle.reverse(Tensor([12, 4, 529201],"float64"), axis=list[0,], )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([12, 4, 529201],"float64"), axis=list[0,], ) 	 25401648 	 19922 	 9.877562284469604 	 6.052903413772583 	 0.5067293643951416 	 0.3103759288787842 	 9.897359848022461 	 6.049526214599609 	 0.5077531337738037 	 0.30977630615234375 	 
2025-08-06 03:11:08.136187 test begin: paddle.reverse(Tensor([396901, 4, 16],"float64"), axis=list[0,], )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([396901, 4, 16],"float64"), axis=list[0,], ) 	 25401664 	 19922 	 9.782324314117432 	 6.025009870529175 	 0.5018408298492432 	 0.3091120719909668 	 9.81020188331604 	 6.023994445800781 	 0.5032432079315186 	 0.30901122093200684 	 
2025-08-06 03:11:41.289161 test begin: paddle.reverse(Tensor([4, 12, 529201],"float64"), axis=1, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([4, 12, 529201],"float64"), axis=1, ) 	 25401648 	 19922 	 9.874808311462402 	 6.085322856903076 	 0.506511926651001 	 0.31204867362976074 	 9.89903998374939 	 6.085574150085449 	 0.5077435970306396 	 0.3121814727783203 	 
2025-08-06 03:12:14.376356 test begin: paddle.reverse(Tensor([4, 198451, 32],"float64"), axis=1, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([4, 198451, 32],"float64"), axis=1, ) 	 25401728 	 19922 	 9.811603307723999 	 6.046414613723755 	 0.5033190250396729 	 0.3096959590911865 	 9.785285711288452 	 6.037821292877197 	 0.5019934177398682 	 0.30971384048461914 	 
2025-08-06 03:12:49.052844 test begin: paddle.reverse(Tensor([66151, 12, 32],"float64"), axis=1, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([66151, 12, 32],"float64"), axis=1, ) 	 25401984 	 19922 	 9.81510615348816 	 6.039038896560669 	 0.5034594535827637 	 0.3097832202911377 	 9.788323163986206 	 6.040103435516357 	 0.5021133422851562 	 0.309842586517334 	 
2025-08-06 03:13:21.868910 test begin: paddle.reverse(Tensor([793801, 4, 8],"float64"), axis=0, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([793801, 4, 8],"float64"), axis=0, ) 	 25401632 	 19922 	 9.799194097518921 	 6.040092945098877 	 0.5026633739471436 	 0.30928468704223633 	 9.807288408279419 	 6.029400825500488 	 0.5031230449676514 	 0.30931615829467773 	 
2025-08-06 03:13:55.458744 test begin: paddle.roll(Tensor([128, 37, 56, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 37, 56, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 	 50921472 	 18327 	 9.9642653465271 	 14.411505699157715 	 0.5558209419250488 	 0.4018261432647705 	 9.963212728500366 	 14.362327098846436 	 0.5555896759033203 	 0.4004359245300293 	 
2025-08-06 03:14:45.870493 test begin: paddle.roll(Tensor([128, 37, 56, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 37, 56, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 	 50921472 	 18327 	 9.96299433708191 	 14.370813369750977 	 0.5556221008300781 	 0.40068531036376953 	 9.96161150932312 	 14.396695137023926 	 0.555518388748169 	 0.4014303684234619 	 
2025-08-06 03:15:37.613691 test begin: paddle.roll(Tensor([128, 56, 37, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 56, 37, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 	 50921472 	 18327 	 9.96137523651123 	 14.463028192520142 	 0.5554978847503662 	 0.4031546115875244 	 9.96297311782837 	 14.424238681793213 	 0.5555589199066162 	 0.40217065811157227 	 
2025-08-06 03:16:28.160068 test begin: paddle.roll(Tensor([128, 56, 37, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 56, 37, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 	 50921472 	 18327 	 9.961872577667236 	 14.421329736709595 	 0.5555245876312256 	 0.4020843505859375 	 9.959713697433472 	 14.449913501739502 	 0.5553970336914062 	 0.40288472175598145 	 
2025-08-06 03:17:19.267661 test begin: paddle.roll(Tensor([128, 56, 56, 127],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 56, 56, 127],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 	 50978816 	 18327 	 9.969046115875244 	 14.457221746444702 	 0.555955171585083 	 0.4029974937438965 	 9.970496416091919 	 14.404183864593506 	 0.5560147762298584 	 0.40163707733154297 	 
2025-08-06 03:18:09.794730 test begin: paddle.roll(Tensor([128, 56, 56, 127],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 56, 56, 127],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 	 50978816 	 18327 	 9.971120119094849 	 14.40678071975708 	 0.5560295581817627 	 0.40172648429870605 	 9.97151494026184 	 14.449292182922363 	 0.5560579299926758 	 0.40286946296691895 	 
2025-08-06 03:19:00.305359 test begin: paddle.roll(Tensor([44, 96, 96, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([44, 96, 96, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 	 51904512 	 18327 	 10.140044927597046 	 14.61580491065979 	 0.5654804706573486 	 0.40753960609436035 	 10.141028642654419 	 14.56757140159607 	 0.565507173538208 	 0.406170129776001 	 
2025-08-06 03:19:51.525839 test begin: paddle.roll(Tensor([64, 65, 96, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([64, 65, 96, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 	 51118080 	 18327 	 9.990339756011963 	 14.39830231666565 	 0.5570943355560303 	 0.4014441967010498 	 9.989159345626831 	 14.350767374038696 	 0.5570700168609619 	 0.40013551712036133 	 
2025-08-06 03:20:43.307434 test begin: paddle.roll(Tensor([64, 96, 65, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([64, 96, 65, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 	 51118080 	 18327 	 9.992556810379028 	 14.416409969329834 	 0.5572326183319092 	 0.4019789695739746 	 9.989764928817749 	 14.366995334625244 	 0.5570793151855469 	 0.4005732536315918 	 
2025-08-06 03:21:33.826335 test begin: paddle.roll(Tensor([64, 96, 96, 87],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([64, 96, 96, 87],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 	 51314688 	 18327 	 10.03856897354126 	 14.518892526626587 	 0.5598335266113281 	 0.40468811988830566 	 10.037680625915527 	 14.459428787231445 	 0.559659481048584 	 0.4036839008331299 	 
2025-08-06 03:22:24.609654 test begin: paddle.roll(Tensor([85, 56, 56, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([85, 56, 56, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 	 51179520 	 18327 	 10.010566234588623 	 14.489188194274902 	 0.5582334995269775 	 0.403822660446167 	 10.013181686401367 	 14.441467761993408 	 0.5584609508514404 	 0.40242552757263184 	 
2025-08-06 03:23:15.328140 test begin: paddle.roll(Tensor([85, 56, 56, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([85, 56, 56, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 	 51179520 	 18327 	 10.015038251876831 	 14.458424806594849 	 0.5584011077880859 	 0.4029510021209717 	 10.01320767402649 	 14.490339040756226 	 0.5582787990570068 	 0.4040493965148926 	 
2025-08-06 03:24:06.706942 test begin: paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 19442 	 9.84277606010437 	 5.893988609313965 	 0.5179886817932129 	 0.3098111152648926 	 15.872827053070068 	 5.9019410610198975 	 0.41725707054138184 	 0.3102405071258545 	 
2025-08-06 03:24:45.341672 test begin: paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 19442 	 15.781583070755005 	 5.895277738571167 	 0.41472887992858887 	 0.3098256587982178 	 9.859099864959717 	 5.892125129699707 	 0.518242359161377 	 0.3096930980682373 	 
2025-08-06 03:25:23.880789 test begin: paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 19442 	 15.700764417648315 	 5.893903970718384 	 0.41268181800842285 	 0.3097989559173584 	 9.859163284301758 	 5.885195970535278 	 0.5182709693908691 	 0.30941081047058105 	 
2025-08-06 03:26:02.366540 test begin: paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 19442 	 9.878171920776367 	 5.908376932144165 	 0.5192780494689941 	 0.31060338020324707 	 15.87415623664856 	 5.878991365432739 	 0.41716742515563965 	 0.30904579162597656 	 
2025-08-06 03:26:41.038247 test begin: paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 19442 	 18.341229915618896 	 5.908553838729858 	 0.4820704460144043 	 0.31058788299560547 	 9.922819137573242 	 5.910475015640259 	 0.5216720104217529 	 0.31067585945129395 	 
2025-08-06 03:27:22.245263 test begin: paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 19442 	 15.70059871673584 	 5.893959283828735 	 0.4126393795013428 	 0.3097844123840332 	 9.857877492904663 	 5.885274648666382 	 0.5181751251220703 	 0.30935120582580566 	 
2025-08-06 03:28:00.693308 test begin: paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 19442 	 9.905048370361328 	 5.910473823547363 	 0.5207300186157227 	 0.31071043014526367 	 15.894843101501465 	 5.901932239532471 	 0.4177730083465576 	 0.3102543354034424 	 
2025-08-06 03:28:40.714265 test begin: paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 19442 	 16.005788564682007 	 5.910290241241455 	 0.4206721782684326 	 0.3107640743255615 	 9.858190774917603 	 5.892282962799072 	 0.5181796550750732 	 0.30971813201904297 	 
2025-08-06 03:29:19.494993 test begin: paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 19442 	 15.763284683227539 	 5.911260366439819 	 0.4142947196960449 	 0.31073570251464844 	 9.912233829498291 	 5.9386961460113525 	 0.5210318565368652 	 0.31218814849853516 	 
2025-08-06 03:29:58.148657 test begin: paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 19442 	 9.90404462814331 	 5.918044328689575 	 0.5206561088562012 	 0.31067967414855957 	 15.893714427947998 	 5.901940107345581 	 0.4177436828613281 	 0.31026625633239746 	 
2025-08-06 03:30:37.661841 test begin: paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 19442 	 15.929090976715088 	 5.910388708114624 	 0.4186387062072754 	 0.3107004165649414 	 9.915488719940186 	 5.9440600872039795 	 0.5212421417236328 	 0.3124687671661377 	 
2025-08-06 03:31:16.494882 test begin: paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 19442 	 15.611435890197754 	 5.957492828369141 	 0.41031646728515625 	 0.3126842975616455 	 9.85666561126709 	 5.886099338531494 	 0.5181171894073486 	 0.30941247940063477 	 
2025-08-06 03:31:55.924214 test begin: paddle.round(Tensor([128, 396901],"float32"), )
[Prof] paddle.round 	 paddle.round(Tensor([128, 396901],"float32"), ) 	 50803328 	 33846 	 10.0000159740448 	 10.072275876998901 	 0.30315423011779785 	 0.3040952682495117 	 4.5332677364349365 	 4.534456253051758 	 0.1368091106414795 	 0.13682794570922852 	 
2025-08-06 03:32:26.799262 test begin: paddle.round(Tensor([16, 1587601],"float64"), )
[Prof] paddle.round 	 paddle.round(Tensor([16, 1587601],"float64"), ) 	 25401616 	 33846 	 10.312750816345215 	 10.0916748046875 	 0.31148624420166016 	 0.30467939376831055 	 4.5327887535095215 	 4.553616285324097 	 0.13671207427978516 	 0.13734030723571777 	 
2025-08-06 03:32:57.414713 test begin: paddle.round(Tensor([396901, 128],"float32"), )
[Prof] paddle.round 	 paddle.round(Tensor([396901, 128],"float32"), ) 	 50803328 	 33846 	 9.999183177947998 	 10.072309732437134 	 0.3018958568572998 	 0.3041656017303467 	 4.533213138580322 	 4.534475803375244 	 0.13678526878356934 	 0.13686442375183105 	 
2025-08-06 03:33:28.308356 test begin: paddle.round(Tensor([99226, 256],"float64"), )
[Prof] paddle.round 	 paddle.round(Tensor([99226, 256],"float64"), ) 	 25401856 	 33846 	 10.27965760231018 	 10.091841697692871 	 0.31063008308410645 	 0.304826021194458 	 4.53159761428833 	 4.55233907699585 	 0.13671064376831055 	 0.13746881484985352 	 
2025-08-06 03:33:59.012801 test begin: paddle.round(x=Tensor([3, 3, 5644801],"float32"), )
[Prof] paddle.round 	 paddle.round(x=Tensor([3, 3, 5644801],"float32"), ) 	 50803209 	 33846 	 10.001740217208862 	 10.072475671768188 	 0.3020455837249756 	 0.30413222312927246 	 4.534038782119751 	 4.534667253494263 	 0.1368560791015625 	 0.13683509826660156 	 
2025-08-06 03:34:29.975849 test begin: paddle.round(x=Tensor([3, 5644801, 3],"float32"), )
[Prof] paddle.round 	 paddle.round(x=Tensor([3, 5644801, 3],"float32"), ) 	 50803209 	 33846 	 10.001769542694092 	 10.072249412536621 	 0.30208635330200195 	 0.30417609214782715 	 4.53434157371521 	 4.53478479385376 	 0.1368579864501953 	 0.13689160346984863 	 
2025-08-06 03:35:00.913535 test begin: paddle.round(x=Tensor([5644801, 3, 3],"float32"), )
[Prof] paddle.round 	 paddle.round(x=Tensor([5644801, 3, 3],"float32"), ) 	 50803209 	 33846 	 10.001789093017578 	 10.076805830001831 	 0.3020339012145996 	 0.30414748191833496 	 4.534253120422363 	 4.5348591804504395 	 0.13684964179992676 	 0.13687396049499512 	 
2025-08-06 03:35:31.752352 test begin: paddle.row_stack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], ) 	 76204872 	 32497 	 30.580841541290283 	 31.94854426383972 	 0.16026759147644043 	 0.9420812129974365 	 30.90644335746765 	 2.5454702377319336 	 0.1619863510131836 	 8.082389831542969e-05 	 
2025-08-06 03:37:14.862038 test begin: paddle.row_stack(list[Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2, 1058401],"float64"),], ) 	 25401624 	 32497 	 10.230435848236084 	 10.967589139938354 	 0.16087126731872559 	 0.15982627868652344 	 10.253880262374878 	 1.8343729972839355 	 0.1612224578857422 	 0.00017261505126953125 	 
2025-08-06 03:37:50.728097 test begin: paddle.row_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401880 	 32497 	 10.176137208938599 	 10.499658107757568 	 0.08016085624694824 	 0.33002448081970215 	 10.458876609802246 	 2.2276570796966553 	 0.08234691619873047 	 9.369850158691406e-05 	 
2025-08-06 03:38:25.296392 test begin: paddle.row_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401880 	 32497 	 10.236804723739624 	 10.405113220214844 	 0.08034276962280273 	 0.3269655704498291 	 10.558768510818481 	 2.2028067111968994 	 0.08285641670227051 	 7.414817810058594e-05 	 
2025-08-06 03:38:59.941097 test begin: paddle.row_stack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], ) 	 76204836 	 32497 	 30.5711567401886 	 29.71576690673828 	 0.1602327823638916 	 0.9345004558563232 	 30.4882550239563 	 2.2032206058502197 	 0.15978145599365234 	 9.512901306152344e-05 	 
2025-08-06 03:40:38.056849 test begin: paddle.row_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], ) 	 25401656 	 32497 	 10.42215609550476 	 10.45645546913147 	 0.08205652236938477 	 0.3286280632019043 	 10.266082525253296 	 2.2080540657043457 	 0.08084583282470703 	 0.00010085105895996094 	 
2025-08-06 03:41:12.642759 test begin: paddle.row_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401656 	 32497 	 10.209108352661133 	 10.353736639022827 	 0.08013176918029785 	 0.3253169059753418 	 10.236961603164673 	 2.9211628437042236 	 0.08028340339660645 	 0.00011134147644042969 	 
2025-08-06 03:41:47.543698 test begin: paddle.row_stack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 76204980 	 32497 	 30.491589069366455 	 29.84432363510132 	 0.1598377227783203 	 0.9387474060058594 	 30.41737413406372 	 2.555403470993042 	 0.15940093994140625 	 0.00010704994201660156 	 
2025-08-06 03:43:24.260229 test begin: paddle.row_stack(list[Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401660 	 32497 	 9.967939615249634 	 10.168879270553589 	 0.15674686431884766 	 0.15982699394226074 	 9.98831057548523 	 2.308216094970703 	 0.15702199935913086 	 0.00011849403381347656 	 
2025-08-06 03:43:57.871184 test begin: paddle.row_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 76204818 	 32497 	 30.6965548992157 	 29.891578435897827 	 0.16088032722473145 	 0.9400782585144043 	 30.956783771514893 	 2.7881882190704346 	 0.16222381591796875 	 0.0002307891845703125 	 
2025-08-06 03:45:37.702975 test begin: paddle.row_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 76204890 	 32497 	 30.614900827407837 	 30.222574472427368 	 0.16045451164245605 	 0.9494326114654541 	 30.792689085006714 	 2.203092098236084 	 0.16144156455993652 	 9.512901306152344e-05 	 
2025-08-06 03:47:16.302969 test begin: paddle.row_stack(list[Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401630 	 32497 	 10.23047947883606 	 10.174853801727295 	 0.16088271141052246 	 0.1598196029663086 	 10.253849506378174 	 1.7887306213378906 	 0.16121220588684082 	 0.00017309188842773438 	 
2025-08-06 03:47:51.394613 test begin: paddle.row_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401656 	 32497 	 10.383780241012573 	 10.014851331710815 	 0.08147740364074707 	 0.3148224353790283 	 10.393796682357788 	 2.1858811378479004 	 0.0815579891204834 	 0.00010275840759277344 	 
2025-08-06 03:48:25.515118 test begin: paddle.row_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], ) 	 76204824 	 32497 	 30.602113008499146 	 29.738534688949585 	 0.1603994369506836 	 0.9353406429290771 	 30.91675329208374 	 2.1876251697540283 	 0.16203880310058594 	 7.486343383789062e-05 	 
2025-08-06 03:50:02.846657 test begin: paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401880 	 32497 	 10.149914026260376 	 10.106393337249756 	 0.07963252067565918 	 0.3178229331970215 	 10.371057510375977 	 2.209559202194214 	 0.08138370513916016 	 8.344650268554688e-05 	 
2025-08-06 03:50:38.850288 test begin: paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 76204920 	 32497 	 30.476319551467896 	 29.979326248168945 	 0.1597602367401123 	 0.9422948360443115 	 30.648170471191406 	 2.2759573459625244 	 0.16062641143798828 	 0.0002040863037109375 	 
2025-08-06 03:52:16.525174 test begin: paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401640 	 32497 	 9.968568325042725 	 10.183835983276367 	 0.15674781799316406 	 0.15984153747558594 	 9.987359762191772 	 1.7619619369506836 	 0.15702009201049805 	 7.772445678710938e-05 	 
2025-08-06 03:52:51.354215 test begin: paddle.rsqrt(Tensor([10000, 1694, 3],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([10000, 1694, 3],"float32"), ) 	 50820000 	 33838 	 10.001911163330078 	 10.074045896530151 	 0.3021054267883301 	 0.30425000190734863 	 15.224702835083008 	 35.206134557724 	 0.45981931686401367 	 0.35439515113830566 	 
2025-08-06 03:54:03.681009 test begin: paddle.rsqrt(Tensor([10000, 2, 1271],"float64"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([10000, 2, 1271],"float64"), ) 	 25420000 	 33838 	 10.101257801055908 	 10.122200727462769 	 0.305095911026001 	 0.3056004047393799 	 15.175328254699707 	 35.19064712524414 	 0.4583163261413574 	 0.354306697845459 	 
2025-08-06 03:55:17.553549 test begin: paddle.rsqrt(Tensor([10000, 2, 2541],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([10000, 2, 2541],"float32"), ) 	 50820000 	 33838 	 10.00203561782837 	 10.081522226333618 	 0.3021364212036133 	 0.30423903465270996 	 15.22407054901123 	 35.20497465133667 	 0.45983386039733887 	 0.35453271865844727 	 
2025-08-06 03:56:30.971876 test begin: paddle.rsqrt(Tensor([10000, 847, 3],"float64"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([10000, 847, 3],"float64"), ) 	 25410000 	 33838 	 10.098561763763428 	 10.112727642059326 	 0.3049783706665039 	 0.3054313659667969 	 15.16087794303894 	 35.17659592628479 	 0.4578418731689453 	 0.3541693687438965 	 
2025-08-06 03:57:43.536928 test begin: paddle.rsqrt(Tensor([13, 1007, 3881],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([13, 1007, 3881],"float32"), ) 	 50806171 	 33838 	 10.00231146812439 	 10.071120262145996 	 0.3021361827850342 	 0.30423617362976074 	 15.220835208892822 	 35.198607444763184 	 0.4597005844116211 	 0.3544046878814697 	 
2025-08-06 03:58:57.157400 test begin: paddle.rsqrt(Tensor([13, 3907939, 1],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([13, 3907939, 1],"float32"), ) 	 50803207 	 33838 	 10.00158405303955 	 10.070662260055542 	 0.3020346164703369 	 0.304152250289917 	 15.221799612045288 	 35.194504737854004 	 0.4598362445831299 	 0.3543713092803955 	 
2025-08-06 04:00:10.950572 test begin: paddle.rsqrt(Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([4233601, 2, 3],"float64"), ) 	 25401606 	 33838 	 10.09401822090149 	 10.119922637939453 	 0.30486154556274414 	 0.30536890029907227 	 15.15723967552185 	 35.166614055633545 	 0.4578118324279785 	 0.35407495498657227 	 
2025-08-06 04:01:26.735190 test begin: paddle.rsqrt(Tensor([50451, 1007, 1],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([50451, 1007, 1],"float32"), ) 	 50804157 	 33838 	 10.001587152481079 	 10.089636325836182 	 0.30212831497192383 	 0.3041963577270508 	 15.222301959991455 	 35.19518208503723 	 0.45980191230773926 	 0.3543686866760254 	 
2025-08-06 04:02:42.839986 test begin: paddle.rsqrt(Tensor([8467201, 2, 3],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([8467201, 2, 3],"float32"), ) 	 50803206 	 33838 	 10.001964569091797 	 10.070398569107056 	 0.302093505859375 	 0.3041799068450928 	 15.22030520439148 	 35.19453167915344 	 0.4596712589263916 	 0.354337215423584 	 
2025-08-06 04:03:55.335051 test begin: paddle.scale(Tensor([2, 256, 256, 388],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([2, 256, 256, 388],"float32"), scale=1.1111111111111112, ) 	 50855936 	 33791 	 10.011416673660278 	 20.147135496139526 	 0.302746057510376 	 0.3044552803039551 	 10.005153894424438 	 10.066205263137817 	 0.30260610580444336 	 0.3044736385345459 	 combined
2025-08-06 04:04:48.843501 test begin: paddle.scale(Tensor([2, 256, 388, 256],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([2, 256, 388, 256],"float32"), scale=1.1111111111111112, ) 	 50855936 	 33791 	 10.009996891021729 	 20.132323741912842 	 0.3027181625366211 	 0.3045084476470947 	 10.004941701889038 	 10.06629467010498 	 0.3026289939880371 	 0.3043959140777588 	 combined
2025-08-06 04:05:43.601732 test begin: paddle.scale(Tensor([2, 388, 256, 256],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([2, 388, 256, 256],"float32"), scale=1.1111111111111112, ) 	 50855936 	 33791 	 10.010188102722168 	 20.131959438323975 	 0.30277252197265625 	 0.30450868606567383 	 10.004894256591797 	 10.06628131866455 	 0.3026082515716553 	 0.3044412136077881 	 combined
2025-08-06 04:06:35.569883 test begin: paddle.scale(Tensor([4, 194, 256, 256],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 194, 256, 256],"float32"), scale=1.1111111111111112, ) 	 50855936 	 33791 	 10.010275840759277 	 20.1368510723114 	 0.3028078079223633 	 0.3044424057006836 	 10.00479531288147 	 10.06613564491272 	 0.30255818367004395 	 0.30442118644714355 	 combined
2025-08-06 04:07:27.504038 test begin: paddle.scale(Tensor([4, 256, 194, 256],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 256, 194, 256],"float32"), scale=1.1111111111111112, ) 	 50855936 	 33791 	 10.01001262664795 	 20.17111325263977 	 0.3027505874633789 	 0.3044769763946533 	 10.005114078521729 	 10.066004514694214 	 0.30260205268859863 	 0.3045053482055664 	 combined
2025-08-06 04:08:19.957036 test begin: paddle.scale(Tensor([4, 256, 256, 194],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 256, 256, 194],"float32"), scale=1.1111111111111112, ) 	 50855936 	 33791 	 10.010225296020508 	 20.139871835708618 	 0.3027472496032715 	 0.3044290542602539 	 10.004971027374268 	 10.066149473190308 	 0.30257153511047363 	 0.3044614791870117 	 combined
2025-08-06 04:09:14.113839 test begin: paddle.scale(Tensor([4, 256, 256, 256],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 256, 256, 256],"float32"), scale=1.1111111111111112, ) 	 67108864 	 33791 	 13.167266368865967 	 26.47195339202881 	 0.39827609062194824 	 0.4001917839050293 	 13.172416687011719 	 13.22834324836731 	 0.39837074279785156 	 0.4000589847564697 	 combined
2025-08-06 04:10:25.043204 test begin: paddle.scale(Tensor([4, 256, 256, 388],"float16"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 256, 256, 388],"float16"), scale=1.1111111111111112, ) 	 101711872 	 33791 	 10.084729671478271 	 20.0229389667511 	 0.30501413345336914 	 0.3027842044830322 	 10.079188346862793 	 10.015661001205444 	 0.30482006072998047 	 0.30288004875183105 	 combined
2025-08-06 04:11:19.708684 test begin: paddle.scale(Tensor([4, 256, 388, 256],"float16"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 256, 388, 256],"float16"), scale=1.1111111111111112, ) 	 101711872 	 33791 	 10.084593057632446 	 20.0283145904541 	 0.30499815940856934 	 0.3028132915496826 	 10.079239845275879 	 10.015472412109375 	 0.3048257827758789 	 0.3028831481933594 	 combined
2025-08-06 04:12:13.776391 test begin: paddle.scale(Tensor([4, 388, 256, 256],"float16"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 388, 256, 256],"float16"), scale=1.1111111111111112, ) 	 101711872 	 33791 	 10.084543704986572 	 20.052122354507446 	 0.3050248622894287 	 0.3028450012207031 	 10.079414129257202 	 10.015478610992432 	 0.30483579635620117 	 0.3029294013977051 	 combined
2025-08-06 04:13:09.848708 test begin: paddle.scale(Tensor([7, 256, 256, 256],"float16"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([7, 256, 256, 256],"float16"), scale=1.1111111111111112, ) 	 117440512 	 33791 	 11.61452841758728 	 23.075207710266113 	 0.3513000011444092 	 0.3490004539489746 	 11.612748146057129 	 11.539194107055664 	 0.3512265682220459 	 0.34891438484191895 	 combined
2025-08-06 04:14:12.675429 test begin: paddle.scatter(Tensor([262144, 194],"float32"), Tensor([197],"int32"), Tensor([197, 194],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 194],"float32"), Tensor([197],"int32"), Tensor([197, 194],"float32"), overwrite=True, ) 	 50894351 	 14665 	 4.64497184753418 	 96.66566491127014 	 0.16187071800231934 	 0.0002543926239013672 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:16:00.312969 test begin: paddle.scatter(Tensor([262144, 194],"float32"), Tensor([205],"int32"), Tensor([205, 194],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 194],"float32"), Tensor([205],"int32"), Tensor([205, 194],"float32"), overwrite=True, ) 	 50895911 	 14665 	 4.640660762786865 	 99.9404284954071 	 0.16170430183410645 	 0.000225067138671875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:17:51.181980 test begin: paddle.scatter(Tensor([262144, 194],"float32"), Tensor([219],"int32"), Tensor([219, 194],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 194],"float32"), Tensor([219],"int32"), Tensor([219, 194],"float32"), overwrite=True, ) 	 50898641 	 14665 	 4.649118423461914 	 107.80566334724426 	 0.1620171070098877 	 0.00024056434631347656 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:19:50.100570 test begin: paddle.scatter(Tensor([262144, 2314],"float32"), Tensor([219],"int32"), Tensor([219, 2314],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 2314],"float32"), Tensor([219],"int32"), Tensor([219, 2314],"float32"), overwrite=True, ) 	 607108201 	 14665 	 53.76474976539612 	 109.36924052238464 	 1.246492624282837 	 0.0005540847778320312 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:23:47.861455 test begin: paddle.scatter(Tensor([262144, 2476],"float32"), Tensor([205],"int32"), Tensor([205, 2476],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 2476],"float32"), Tensor([205],"int32"), Tensor([205, 2476],"float32"), overwrite=True, ) 	 649576329 	 14665 	 57.50173282623291 	 103.7441246509552 	 1.333118200302124 	 0.00023317337036132812 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:27:48.971248 test begin: paddle.scatter(Tensor([262144, 2569],"float32"), Tensor([197],"int32"), Tensor([197, 2569],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 2569],"float32"), Tensor([197],"int32"), Tensor([197, 2569],"float32"), overwrite=True, ) 	 673954226 	 14665 	 59.62340807914734 	 97.00426936149597 	 1.3822886943817139 	 0.0006530284881591797 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:31:49.193503 test begin: paddle.scatter(Tensor([262144, 64],"float32"), Tensor([197],"int32"), Tensor([7938, 64],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 64],"float32"), Tensor([197],"int32"), Tensor([7938, 64],"float32"), overwrite=True, ) 	 17285445 	 14665 	 1.6485090255737305 	 102.91202902793884 	 0.05736851692199707 	 0.00026607513427734375 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:33:38.633308 test begin: paddle.scatter(Tensor([262144, 64],"float32"), Tensor([205],"int32"), Tensor([7938, 64],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 64],"float32"), Tensor([205],"int32"), Tensor([7938, 64],"float32"), overwrite=True, ) 	 17285453 	 14665 	 1.6468486785888672 	 117.88044047355652 	 0.05740475654602051 	 0.0005953311920166016 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:35:41.145494 test begin: paddle.scatter(Tensor([262144, 64],"float32"), Tensor([219],"int32"), Tensor([7938, 64],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([262144, 64],"float32"), Tensor([219],"int32"), Tensor([7938, 64],"float32"), overwrite=True, ) 	 17285467 	 14665 	 1.647458553314209 	 107.65112400054932 	 0.0573878288269043 	 0.000293731689453125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:37:32.687932 test begin: paddle.scatter(Tensor([793801, 64],"float32"), Tensor([197],"int32"), Tensor([197, 64],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([793801, 64],"float32"), Tensor([197],"int32"), Tensor([197, 64],"float32"), overwrite=True, ) 	 50816069 	 14665 	 4.720329284667969 	 97.68664288520813 	 0.10946178436279297 	 0.00022602081298828125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:39:21.738228 test begin: paddle.scatter(Tensor([793801, 64],"float32"), Tensor([205],"int32"), Tensor([205, 64],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([793801, 64],"float32"), Tensor([205],"int32"), Tensor([205, 64],"float32"), overwrite=True, ) 	 50816589 	 14665 	 4.718364477157593 	 100.92141127586365 	 0.10938692092895508 	 0.00024962425231933594 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:41:15.308686 test begin: paddle.scatter(Tensor([793801, 64],"float32"), Tensor([219],"int32"), Tensor([219, 64],"float32"), overwrite=True, )
[Prof] paddle.scatter 	 paddle.scatter(Tensor([793801, 64],"float32"), Tensor([219],"int32"), Tensor([219, 64],"float32"), overwrite=True, ) 	 50817499 	 14665 	 4.709778308868408 	 108.2073483467102 	 0.10922980308532715 	 0.00022602081298828125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:43:14.777228 test begin: paddle.scatter_nd(Tensor([1280, 2],"int64"), Tensor([1280, 9, 10],"float32"), list[3,5,9,10,], )
[Prof] paddle.scatter_nd 	 paddle.scatter_nd(Tensor([1280, 2],"int64"), Tensor([1280, 9, 10],"float32"), list[3,5,9,10,], ) 	 117760 	 1000 	 0.03745412826538086 	 196.80516386032104 	 1.6450881958007812e-05 	 0.00021648406982421875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:46:31.899961 test begin: paddle.scatter_nd_add(Tensor([1, 14176, 7168],"bfloat16"), Tensor([585, 2],"int64"), Tensor([585, 7168],"bfloat16"), )
[Prof] paddle.scatter_nd_add 	 paddle.scatter_nd_add(Tensor([1, 14176, 7168],"bfloat16"), Tensor([585, 2],"int64"), Tensor([585, 7168],"bfloat16"), ) 	 105808018 	 1437 	 0.575819730758667 	 100.90669441223145 	 0.20476770401000977 	 0.0002810955047607422 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:48:19.374638 test begin: paddle.scatter_nd_add(Tensor([1, 14176, 7168],"bfloat16"), Tensor([595, 2],"int64"), Tensor([595, 7168],"bfloat16"), )
[Prof] paddle.scatter_nd_add 	 paddle.scatter_nd_add(Tensor([1, 14176, 7168],"bfloat16"), Tensor([595, 2],"int64"), Tensor([595, 7168],"bfloat16"), ) 	 105879718 	 1437 	 0.5774381160736084 	 101.97879528999329 	 0.20535635948181152 	 0.0005764961242675781 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:50:09.149184 test begin: paddle.scatter_nd_add(Tensor([1, 8192, 12404],"bfloat16"), Tensor([587, 2],"int64"), Tensor([587, 12404],"bfloat16"), )
[Prof] paddle.scatter_nd_add 	 paddle.scatter_nd_add(Tensor([1, 8192, 12404],"bfloat16"), Tensor([587, 2],"int64"), Tensor([587, 12404],"bfloat16"), ) 	 108895890 	 1437 	 0.7785773277282715 	 101.11714172363281 	 0.2768871784210205 	 0.0002429485321044922 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:51:55.924182 test begin: paddle.scatter_nd_add(Tensor([1, 8192, 17069],"bfloat16"), Tensor([595, 2],"int64"), Tensor([595, 17069],"bfloat16"), )
[Prof] paddle.scatter_nd_add 	 paddle.scatter_nd_add(Tensor([1, 8192, 17069],"bfloat16"), Tensor([595, 2],"int64"), Tensor([595, 17069],"bfloat16"), ) 	 149986493 	 1437 	 1.2980926036834717 	 102.5826735496521 	 0.4616100788116455 	 0.00030422210693359375 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:53:45.970219 test begin: paddle.scatter_nd_add(Tensor([2, 8192, 7168],"bfloat16"), Tensor([585, 2],"int64"), Tensor([585, 7168],"bfloat16"), )
[Prof] paddle.scatter_nd_add 	 paddle.scatter_nd_add(Tensor([2, 8192, 7168],"bfloat16"), Tensor([585, 2],"int64"), Tensor([585, 7168],"bfloat16"), ) 	 121634962 	 1437 	 0.642681360244751 	 101.22109937667847 	 0.22851967811584473 	 0.0005598068237304688 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:55:33.123617 test begin: paddle.scatter_nd_add(Tensor([2, 8192, 7168],"bfloat16"), Tensor([587, 2],"int64"), Tensor([587, 7168],"bfloat16"), )
[Prof] paddle.scatter_nd_add 	 paddle.scatter_nd_add(Tensor([2, 8192, 7168],"bfloat16"), Tensor([587, 2],"int64"), Tensor([587, 7168],"bfloat16"), ) 	 121649302 	 1437 	 0.6436221599578857 	 101.16086292266846 	 0.2288813591003418 	 0.0002048015594482422 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:57:21.442521 test begin: paddle.scatter_nd_add(Tensor([2, 8192, 7168],"bfloat16"), Tensor([595, 2],"int64"), Tensor([595, 7168],"bfloat16"), )
[Prof] paddle.scatter_nd_add 	 paddle.scatter_nd_add(Tensor([2, 8192, 7168],"bfloat16"), Tensor([595, 2],"int64"), Tensor([595, 7168],"bfloat16"), ) 	 121706662 	 1437 	 0.6437900066375732 	 104.41216015815735 	 0.22890400886535645 	 0.0002052783966064453 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 04:59:11.279943 test begin: paddle.searchsorted(Tensor([1024],"float32"), Tensor([50803201],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff33de2d390>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 05:09:16.827318 test begin: paddle.searchsorted(Tensor([1024],"float64"), Tensor([25401601],"float64"), )
W0806 05:09:17.537252 125361 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe1fd636d10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 05:19:21.296469 test begin: paddle.searchsorted(Tensor([1024],"int32"), Tensor([50803201],"int32"), )
W0806 05:19:21.987527 125646 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7f5db2ef50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 05:29:25.906074 test begin: paddle.searchsorted(Tensor([2540160101],"float64"), Tensor([512],"float64"), )
W0806 05:30:12.382330 125948 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([2540160101],"float64"), Tensor([512],"float64"), ) 	 2540160613 	 998667 	 9.861360549926758 	 11.063600063323975 	 0.010089397430419922 	 7.128715515136719e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 05:30:40.171733 test begin: paddle.searchsorted(Tensor([25401601],"float64"), Tensor([25401601],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1a364627a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 05:40:47.453422 test begin: paddle.searchsorted(Tensor([25401601],"float64"), Tensor([51201],"float64"), )
W0806 05:40:48.189366 126189 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([25401601],"float64"), Tensor([51201],"float64"), ) 	 25452802 	 998667 	 14.39415717124939 	 17.80038094520569 	 0.00010752677917480469 	 0.00025653839111328125 	 None 	 None 	 None 	 None 	 
2025-08-06 05:41:21.461812 test begin: paddle.searchsorted(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7547a22980>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 05:51:31.602566 test begin: paddle.searchsorted(Tensor([50803201],"float32"), Tensor([51201],"float32"), )
W0806 05:51:33.023335 126496 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([50803201],"float32"), Tensor([51201],"float32"), ) 	 50854402 	 998667 	 11.067846536636353 	 10.93878698348999 	 0.011314630508422852 	 0.00013947486877441406 	 None 	 None 	 None 	 None 	 
2025-08-06 05:51:55.289918 test begin: paddle.searchsorted(Tensor([50803201],"int32"), Tensor([50803201],"int32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fadcca4e980>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 06:02:03.236061 test begin: paddle.searchsorted(Tensor([50803201],"int32"), Tensor([51201],"int32"), )
W0806 06:02:03.987998 126990 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([50803201],"int32"), Tensor([51201],"int32"), ) 	 50854402 	 998667 	 11.02938437461853 	 10.987654685974121 	 0.011282682418823242 	 0.00023126602172851562 	 None 	 None 	 None 	 None 	 
2025-08-06 06:02:26.226217 test begin: paddle.select_scatter(Tensor([12700801, 3, 4],"float32"), Tensor([12700801, 4],"float32"), 1, 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdd68dfab00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 06:12:31.043921 test begin: paddle.select_scatter(Tensor([1693441, 3, 4, 5],"float64"), Tensor([1693441, 3, 5],"float64"), 2, 1, )
W0806 06:12:33.541216 127322 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f074361f0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 06:22:35.844646 test begin: paddle.select_scatter(Tensor([2, 211681, 4, 5, 6],"int32"), Tensor([2, 211681, 5, 6],"int32"), 2, 1, )
W0806 06:22:43.849368 127631 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 211681, 4, 5, 6],"int32"), Tensor([2, 211681, 5, 6],"int32"), 2, 1, ) 	 63504300 	 315923 	 56.260135650634766 	 155.38274836540222 	 0.18189263343811035 	 0.16735601425170898 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 06:31:20.505349 test begin: paddle.select_scatter(Tensor([2, 2540161, 4, 5],"float64"), Tensor([2, 2540161, 5],"float64"), 2, 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd6d1ba2bc0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 06:41:25.245998 test begin: paddle.select_scatter(Tensor([2, 3, 25401601],"float32"), Tensor([2, 25401601],"float32"), 1, 1, )
W0806 06:41:28.435098 128074 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdd19fcb130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 06:51:29.862413 test begin: paddle.select_scatter(Tensor([2, 3, 4, 1411201, 6],"int32"), Tensor([2, 3, 1411201, 6],"int32"), 2, 1, )
W0806 06:51:32.564525 128393 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8eef39ae60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 07:01:34.555912 test begin: paddle.select_scatter(Tensor([2, 3, 4, 352801, 6],"int32"), Tensor([2, 3, 352801, 6],"int32"), 2, 1, )
W0806 07:01:40.027156 128807 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 3, 4, 352801, 6],"int32"), Tensor([2, 3, 352801, 6],"int32"), 2, 1, ) 	 63504180 	 315923 	 43.66265535354614 	 123.85645294189453 	 0.1412336826324463 	 0.1333484649658203 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 07:09:01.599923 test begin: paddle.select_scatter(Tensor([2, 3, 4, 4233601],"float64"), Tensor([2, 3, 4233601],"float64"), 2, 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6acf4f6bc0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 07:19:06.579082 test begin: paddle.select_scatter(Tensor([2, 3, 4, 5, 1693441],"int32"), Tensor([2, 3, 5, 1693441],"int32"), 2, 1, )
W0806 07:19:09.391597 129300 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7feffa6af130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 07:29:11.173684 test begin: paddle.select_scatter(Tensor([2, 3, 4, 5, 423361],"int32"), Tensor([2, 3, 5, 423361],"int32"), 2, 1, )
W0806 07:29:12.011145 129528 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 3, 4, 5, 423361],"int32"), Tensor([2, 3, 5, 423361],"int32"), 2, 1, ) 	 63504150 	 315923 	 43.647915840148926 	 123.85769605636597 	 0.1411890983581543 	 0.13337922096252441 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 07:36:31.940076 test begin: paddle.select_scatter(Tensor([2, 3, 8467201],"float32"), Tensor([2, 8467201],"float32"), 1, 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f189beeaa40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 07:46:37.491445 test begin: paddle.select_scatter(Tensor([2, 635041, 4, 5],"float64"), Tensor([2, 635041, 5],"float64"), 2, 1, )
W0806 07:46:43.001111 130252 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fef7232eef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 07:56:44.258709 test begin: paddle.select_scatter(Tensor([2, 846721, 4, 5, 6],"int32"), Tensor([2, 846721, 5, 6],"int32"), 2, 1, )
W0806 07:56:46.953555 130715 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f393631f040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:06:48.876253 test begin: paddle.select_scatter(Tensor([20, 3, 282241, 5, 6],"int32"), Tensor([20, 3, 5, 6],"int32"), 2, 1, )
W0806 08:06:54.392926 131169 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f12ed3d6f50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:16:53.567442 test begin: paddle.select_scatter(Tensor([20, 3, 4, 1058401],"float64"), Tensor([20, 3, 1058401],"float64"), 2, 1, )
W0806 08:17:01.431349 131543 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdc35637130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:26:58.436726 test begin: paddle.select_scatter(Tensor([20, 3, 846721, 5],"float64"), Tensor([20, 3, 5],"float64"), 2, 1, )
W0806 08:27:03.272830 131906 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbfd667af50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:37:03.124545 test begin: paddle.select_scatter(Tensor([20, 635040, 4],"float32"), Tensor([20, 4],"float32"), 1, 1, )
W0806 08:37:04.097179 132351 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([20, 635040, 4],"float32"), Tensor([20, 4],"float32"), 1, 1, ) 	 50803280 	 315923 	 6.191769361495972 	 99.81990003585815 	 6.461143493652344e-05 	 0.10744261741638184 	 103.71777844429016 	 100.53759217262268 	 0.04168820381164551 	 0.08109116554260254 	 
2025-08-06 08:42:15.773920 test begin: paddle.select_scatter(Tensor([4233601, 3, 4],"float32"), Tensor([4233601, 4],"float32"), 1, 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7e312e6980>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:52:20.329684 test begin: paddle.select_scatter(Tensor([423361, 3, 4, 5],"float64"), Tensor([423361, 3, 5],"float64"), 2, 1, )
W0806 08:52:21.165246 132882 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdf21e73130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:02:25.063698 test begin: paddle.sgn(Tensor([12, 1058401, 2],"float64"), )
W0806 09:02:25.711771 133266 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.sgn 	 paddle.sgn(Tensor([12, 1058401, 2],"float64"), ) 	 25401624 	 32402 	 10.013221025466919 	 9.65942668914795 	 0.3157615661621094 	 0.3046550750732422 	 9.640883445739746 	 1.8579325675964355 	 0.30405330657958984 	 0.0001690387725830078 	 
2025-08-06 09:02:59.193584 test begin: paddle.sgn(Tensor([12, 20, 105841],"float64"), )
[Prof] paddle.sgn 	 paddle.sgn(Tensor([12, 20, 105841],"float64"), ) 	 25401840 	 32402 	 10.017579317092896 	 9.658183097839355 	 0.3158853054046631 	 0.3046391010284424 	 9.648792266845703 	 1.7304410934448242 	 0.30434513092041016 	 0.00015974044799804688 	 
2025-08-06 09:03:31.362076 test begin: paddle.sgn(Tensor([12, 20, 211681],"float32"), )
[Prof] paddle.sgn 	 paddle.sgn(Tensor([12, 20, 211681],"float32"), ) 	 50803440 	 32402 	 11.173555135726929 	 9.658971071243286 	 0.3524470329284668 	 0.3039889335632324 	 9.586007595062256 	 1.689953327178955 	 0.3023669719696045 	 7.605552673339844e-05 	 
2025-08-06 09:04:06.230904 test begin: paddle.sgn(Tensor([12, 2116801, 2],"float32"), )
[Prof] paddle.sgn 	 paddle.sgn(Tensor([12, 2116801, 2],"float32"), ) 	 50803224 	 32402 	 11.17350435256958 	 9.639447212219238 	 0.35238194465637207 	 0.30405139923095703 	 9.586114406585693 	 1.778592586517334 	 0.3023390769958496 	 6.604194641113281e-05 	 
2025-08-06 09:04:43.349469 test begin: paddle.sgn(Tensor([1270081, 20, 2],"float32"), )
[Prof] paddle.sgn 	 paddle.sgn(Tensor([1270081, 20, 2],"float32"), ) 	 50803240 	 32402 	 11.174759149551392 	 9.639437198638916 	 0.35222911834716797 	 0.30401062965393066 	 9.586090326309204 	 1.7617206573486328 	 0.3022935390472412 	 8.320808410644531e-05 	 
2025-08-06 09:05:17.865130 test begin: paddle.sgn(Tensor([635041, 20, 2],"float64"), )
[Prof] paddle.sgn 	 paddle.sgn(Tensor([635041, 20, 2],"float64"), ) 	 25401640 	 32402 	 9.992839813232422 	 9.667182207107544 	 0.3151073455810547 	 0.3045940399169922 	 9.634614706039429 	 1.7965738773345947 	 0.3038966655731201 	 0.0001010894775390625 	 
2025-08-06 09:05:50.645810 test begin: paddle.shape(Tensor([10, 1600, 376, 280],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([10, 1600, 376, 280],"float32"), ) 	 1684480000 	 2368860 	 10.417876243591309 	 72.12802815437317 	 7.200241088867188e-05 	 0.00021529197692871094 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:07:42.034587 test begin: paddle.shape(Tensor([130, 128, 256, 256],"float16"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([130, 128, 256, 256],"float16"), ) 	 1090519040 	 2368860 	 10.517527341842651 	 71.52848172187805 	 0.00013685226440429688 	 0.00020956993103027344 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:09:28.849934 test begin: paddle.shape(Tensor([40, 121, 376, 280],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 121, 376, 280],"float32"), ) 	 509555200 	 2368860 	 10.54507565498352 	 72.08223533630371 	 8.869171142578125e-05 	 0.0002193450927734375 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:11:01.271526 test begin: paddle.shape(Tensor([40, 128, 256, 388],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 128, 256, 388],"float32"), ) 	 508559360 	 2368860 	 10.53925347328186 	 71.75832843780518 	 8.440017700195312e-05 	 0.0002162456512451172 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:12:31.854082 test begin: paddle.shape(Tensor([40, 128, 256, 776],"float16"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 128, 256, 776],"float16"), ) 	 1017118720 	 2368860 	 10.527549266815186 	 71.76056098937988 	 7.295608520507812e-05 	 0.0002117156982421875 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:14:15.228831 test begin: paddle.shape(Tensor([40, 128, 388, 256],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 128, 388, 256],"float32"), ) 	 508559360 	 2368860 	 10.489243030548096 	 72.10531306266785 	 8.821487426757812e-05 	 0.0002243518829345703 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:15:46.191639 test begin: paddle.shape(Tensor([40, 128, 776, 256],"float16"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 128, 776, 256],"float16"), ) 	 1017118720 	 2368860 	 10.5750892162323 	 72.04933476448059 	 7.677078247070312e-05 	 0.00021123886108398438 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:17:27.558816 test begin: paddle.shape(Tensor([40, 1600, 29, 280],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 1600, 29, 280],"float32"), ) 	 519680000 	 2368860 	 10.498631238937378 	 72.16713619232178 	 8.630752563476562e-05 	 0.0002231597900390625 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:19:00.858787 test begin: paddle.shape(Tensor([40, 1600, 376, 22],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 1600, 376, 22],"float32"), ) 	 529408000 	 2368860 	 10.491369009017944 	 72.03197503089905 	 8.893013000488281e-05 	 0.00021696090698242188 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:20:31.937777 test begin: paddle.shape(Tensor([40, 194, 256, 256],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 194, 256, 256],"float32"), ) 	 508559360 	 2368860 	 10.510267734527588 	 77.84453582763672 	 8.440017700195312e-05 	 0.00021696090698242188 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:22:08.544664 test begin: paddle.shape(Tensor([40, 388, 256, 256],"float16"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([40, 388, 256, 256],"float16"), ) 	 1017118720 	 2368860 	 10.496038913726807 	 72.05298066139221 	 8.511543273925781e-05 	 0.00021767616271972656 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:23:51.477447 test begin: paddle.shape(Tensor([70, 128, 256, 256],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([70, 128, 256, 256],"float32"), ) 	 587202560 	 2368860 	 10.467557668685913 	 72.01998591423035 	 9.465217590332031e-05 	 0.00022101402282714844 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:25:23.508940 test begin: paddle.shard_index(input=Tensor([12700801, 2, 1],"int64"), index_num=20, nshards=4, shard_id=1, )
[Prof] paddle.shard_index 	 paddle.shard_index(input=Tensor([12700801, 2, 1],"int64"), index_num=20, nshards=4, shard_id=1, ) 	 25401602 	 32350 	 10.010236501693726 	 65.83109545707703 	 0.31623220443725586 	 0.0006539821624755859 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:26:43.023566 test begin: paddle.shard_index(input=Tensor([12700801, 2, 1],"int64"), index_num=20, nshards=4, shard_id=1, ignore_value=16, )
[Prof] paddle.shard_index 	 paddle.shard_index(input=Tensor([12700801, 2, 1],"int64"), index_num=20, nshards=4, shard_id=1, ignore_value=16, ) 	 25401602 	 32350 	 10.011647462844849 	 65.94357562065125 	 0.3162691593170166 	 0.0006527900695800781 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:27:59.684534 test begin: paddle.shard_index(input=Tensor([25401601, 1],"int64"), index_num=13, nshards=3, shard_id=0, )
[Prof] paddle.shard_index 	 paddle.shard_index(input=Tensor([25401601, 1],"int64"), index_num=13, nshards=3, shard_id=0, ) 	 25401601 	 32350 	 10.026707172393799 	 72.34176135063171 	 0.31677889823913574 	 0.0007479190826416016 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:29:23.829530 test begin: paddle.shard_index(input=Tensor([4, 6350401, 1],"int64"), index_num=20, nshards=4, shard_id=1, )
[Prof] paddle.shard_index 	 paddle.shard_index(input=Tensor([4, 6350401, 1],"int64"), index_num=20, nshards=4, shard_id=1, ) 	 25401604 	 32350 	 10.010401010513306 	 66.14497637748718 	 0.31624794006347656 	 0.0007987022399902344 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:30:42.583456 test begin: paddle.shard_index(input=Tensor([4, 6350401, 1],"int64"), index_num=20, nshards=4, shard_id=1, ignore_value=16, )
[Prof] paddle.shard_index 	 paddle.shard_index(input=Tensor([4, 6350401, 1],"int64"), index_num=20, nshards=4, shard_id=1, ignore_value=16, ) 	 25401604 	 32350 	 10.009972095489502 	 66.11999011039734 	 0.31630611419677734 	 0.0006427764892578125 	 None 	 None 	 None 	 None 	 combined
2025-08-06 09:31:59.350223 test begin: paddle.sign(Tensor([12404, 32, 128],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([12404, 32, 128],"float32"), ) 	 50806784 	 32506 	 11.241382360458374 	 9.672296285629272 	 0.3534207344055176 	 0.3040812015533447 	 9.617813110351562 	 2.2268197536468506 	 0.3023655414581299 	 0.00020074844360351562 	 
2025-08-06 09:32:34.091828 test begin: paddle.sign(Tensor([32, 12404, 128],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([32, 12404, 128],"float32"), ) 	 50806784 	 32506 	 11.241570711135864 	 9.672496795654297 	 0.35347461700439453 	 0.30411624908447266 	 9.617852210998535 	 1.8936495780944824 	 0.30242371559143066 	 6.914138793945312e-05 	 
2025-08-06 09:33:08.253238 test begin: paddle.sign(Tensor([32, 32, 49613],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([32, 32, 49613],"float32"), ) 	 50803712 	 32506 	 11.210264205932617 	 9.67188286781311 	 0.352522611618042 	 0.3040761947631836 	 9.611427545547485 	 2.2057909965515137 	 0.3021526336669922 	 0.00023412704467773438 	 
2025-08-06 09:33:42.778633 test begin: paddle.sign(Tensor([64, 1, 28, 28351],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([64, 1, 28, 28351],"float32"), ) 	 50804992 	 32506 	 11.21362018585205 	 9.671904563903809 	 0.35263609886169434 	 0.30403614044189453 	 9.612281560897827 	 1.8056559562683105 	 0.3023054599761963 	 0.0001926422119140625 	 
2025-08-06 09:34:16.910169 test begin: paddle.sign(Tensor([64, 1, 28351, 28],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([64, 1, 28351, 28],"float32"), ) 	 50804992 	 32506 	 11.213748693466187 	 10.333027601242065 	 0.3525092601776123 	 0.30414485931396484 	 9.612231492996216 	 1.7258763313293457 	 0.30222249031066895 	 0.0001971721649169922 	 
2025-08-06 09:34:52.771391 test begin: paddle.sign(Tensor([64, 1013, 28, 28],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([64, 1013, 28, 28],"float32"), ) 	 50828288 	 32506 	 11.212949991226196 	 9.676578998565674 	 0.3525667190551758 	 0.30422091484069824 	 9.616379022598267 	 1.731015682220459 	 0.3023371696472168 	 9.751319885253906e-05 	 
2025-08-06 09:35:26.788364 test begin: paddle.sign(Tensor([64801, 1, 28, 28],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([64801, 1, 28, 28],"float32"), ) 	 50803984 	 32506 	 11.195189714431763 	 9.67196273803711 	 0.3520660400390625 	 0.3040950298309326 	 9.612079620361328 	 1.7809369564056396 	 0.30220699310302734 	 0.00010180473327636719 	 
2025-08-06 09:36:00.867174 test begin: paddle.sign(Tensor([66151, 1, 384],"int64"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([66151, 1, 384],"int64"), ) 	 25401984 	 32506 	 10.016743183135986 	 9.69636344909668 	 0.3150064945220947 	 0.30484938621520996 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 09:36:31.157492 test begin: paddle.sign(Tensor([7, 1, 3628801],"int64"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([7, 1, 3628801],"int64"), ) 	 25401607 	 32506 	 10.006052732467651 	 9.696704626083374 	 0.31460118293762207 	 0.3049042224884033 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 09:37:01.450882 test begin: paddle.sign(Tensor([7, 9451, 384],"int64"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([7, 9451, 384],"int64"), ) 	 25404288 	 32506 	 10.013402223587036 	 9.721595525741577 	 0.31490397453308105 	 0.3048248291015625 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 09:37:33.734192 test begin: paddle.signal.stft(Tensor([16, 3175201],"float32"), 1024, 120, 600, window=Tensor([600],"float32"), center=True, pad_mode="reflect", )
[Prof] paddle.signal.stft 	 paddle.signal.stft(Tensor([16, 3175201],"float32"), 1024, 120, 600, window=Tensor([600],"float32"), center=True, pad_mode="reflect", ) 	 50803816 	 1000 	 19.410813570022583 	 4.784717559814453 	 2.4840636253356934 	 0.9755313396453857 	 43.31166124343872 	 33.876681089401245 	 2.9459893703460693 	 1.7280430793762207 	 
2025-08-06 09:39:23.153267 test begin: paddle.signal.stft(Tensor([16, 3175201],"float32"), 2048, 240, 1200, window=Tensor([1200],"float32"), center=True, pad_mode="reflect", )
[Prof] paddle.signal.stft 	 paddle.signal.stft(Tensor([16, 3175201],"float32"), 2048, 240, 1200, window=Tensor([1200],"float32"), center=True, pad_mode="reflect", ) 	 50804416 	 1000 	 19.79651403427124 	 4.984906911849976 	 2.533450126647949 	 1.0250160694122314 	 43.15819811820984 	 32.352535247802734 	 2.9351539611816406 	 1.650256633758545 	 
2025-08-06 09:41:13.322546 test begin: paddle.signal.stft(Tensor([1993, 25500],"float32"), 1024, 120, 600, window=Tensor([600],"float32"), center=True, pad_mode="reflect", )
[Prof] paddle.signal.stft 	 paddle.signal.stft(Tensor([1993, 25500],"float32"), 1024, 120, 600, window=Tensor([600],"float32"), center=True, pad_mode="reflect", ) 	 50822100 	 1000 	 17.680163860321045 	 4.7848875522613525 	 2.2623026371002197 	 0.9805493354797363 	 42.47296357154846 	 31.10478711128235 	 2.8889007568359375 	 1.587003469467163 	 
2025-08-06 09:42:57.719497 test begin: paddle.signal.stft(Tensor([1993, 25500],"float32"), 2048, 240, 1200, window=Tensor([1200],"float32"), center=True, pad_mode="reflect", )
[Prof] paddle.signal.stft 	 paddle.signal.stft(Tensor([1993, 25500],"float32"), 2048, 240, 1200, window=Tensor([1200],"float32"), center=True, pad_mode="reflect", ) 	 50822700 	 1000 	 18.144152879714966 	 5.0664286613464355 	 2.32169246673584 	 1.0364904403686523 	 42.79541492462158 	 31.599485874176025 	 2.9105849266052246 	 1.7180027961730957 	 
2025-08-06 09:44:41.854365 test begin: paddle.signbit(Tensor([11, 17, 271],"int32"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([11, 17, 271],"int32"), ) 	 50677 	 4599 	 9.957216501235962 	 0.04895830154418945 	 4.6253204345703125e-05 	 4.00543212890625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 09:44:52.016465 test begin: paddle.signbit(Tensor([11, 17, 543],"int16"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([11, 17, 543],"int16"), ) 	 101541 	 4599 	 18.9899799823761 	 0.04881787300109863 	 4.601478576660156e-05 	 7.43865966796875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 09:45:11.186488 test begin: paddle.signbit(Tensor([11, 461, 10],"int32"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([11, 461, 10],"int32"), ) 	 50710 	 4599 	 9.990544319152832 	 0.04737257957458496 	 9.703636169433594e-05 	 3.7670135498046875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 09:45:21.483165 test begin: paddle.signbit(Tensor([11, 923, 10],"int16"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([11, 923, 10],"int16"), ) 	 101530 	 4599 	 18.9585120677948 	 0.0479130744934082 	 4.363059997558594e-05 	 6.937980651855469e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 09:45:41.883678 test begin: paddle.signbit(Tensor([12, 20, 211],"float32"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([12, 20, 211],"float32"), ) 	 50640 	 4599 	 10.062958002090454 	 0.046094655990600586 	 3.933906555175781e-05 	 3.314018249511719e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 09:45:52.118346 test begin: paddle.signbit(Tensor([12, 2116, 2],"float32"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([12, 2116, 2],"float32"), ) 	 50784 	 4599 	 10.061437845230103 	 0.04593300819396973 	 1.9311904907226562e-05 	 4.9114227294921875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 09:46:02.351307 test begin: paddle.signbit(Tensor([1270, 20, 2],"float32"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([1270, 20, 2],"float32"), ) 	 50800 	 4599 	 10.12607741355896 	 0.046051740646362305 	 4.1961669921875e-05 	 3.4332275390625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 09:46:12.646738 test begin: paddle.signbit(Tensor([298, 17, 10],"int32"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([298, 17, 10],"int32"), ) 	 50660 	 4599 	 10.007103443145752 	 0.06009936332702637 	 2.3365020751953125e-05 	 7.128715515136719e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 09:46:22.838974 test begin: paddle.signbit(Tensor([597, 17, 10],"int16"), )
[Prof] paddle.signbit 	 paddle.signbit(Tensor([597, 17, 10],"int16"), ) 	 101490 	 4599 	 19.076282739639282 	 0.04702591896057129 	 4.1961669921875e-05 	 4.1961669921875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 09:46:42.204152 test begin: paddle.sin(Tensor([128512, 396],"float32"), )
[Prof] paddle.sin 	 paddle.sin(Tensor([128512, 396],"float32"), ) 	 50890752 	 33844 	 10.008856773376465 	 10.099279642105103 	 0.3022425174713135 	 0.30496859550476074 	 15.262268781661987 	 25.19842505455017 	 0.4608442783355713 	 0.38046979904174805 	 
2025-08-06 09:47:44.977678 test begin: paddle.sin(Tensor([254017, 200],"float32"), )
[Prof] paddle.sin 	 paddle.sin(Tensor([254017, 200],"float32"), ) 	 50803400 	 33844 	 10.000272750854492 	 10.083378314971924 	 0.302004337310791 	 0.3045015335083008 	 15.243937492370605 	 25.149322986602783 	 0.46024179458618164 	 0.3797032833099365 	 
2025-08-06 09:48:47.153970 test begin: paddle.sin(Tensor([50000, 1017],"float32"), )
[Prof] paddle.sin 	 paddle.sin(Tensor([50000, 1017],"float32"), ) 	 50850000 	 33844 	 10.004169940948486 	 10.102679014205933 	 0.30211353302001953 	 0.3047816753387451 	 15.255318880081177 	 25.172535181045532 	 0.46071815490722656 	 0.38009095191955566 	 
2025-08-06 09:49:49.418017 test begin: paddle.sin(Tensor([508033, 100],"float32"), )
[Prof] paddle.sin 	 paddle.sin(Tensor([508033, 100],"float32"), ) 	 50803300 	 33844 	 9.998222827911377 	 10.084527730941772 	 0.3018989562988281 	 0.3044929504394531 	 15.23265552520752 	 25.14896249771118 	 0.45995402336120605 	 0.37970852851867676 	 
2025-08-06 09:50:51.606192 test begin: paddle.sin(Tensor([68608, 741],"float32"), )
[Prof] paddle.sin 	 paddle.sin(Tensor([68608, 741],"float32"), ) 	 50838528 	 33844 	 10.005642890930176 	 10.09572958946228 	 0.30214643478393555 	 0.304659366607666 	 15.253030776977539 	 25.166224241256714 	 0.4606001377105713 	 0.37996888160705566 	 
2025-08-06 09:51:54.452559 test begin: paddle.sinc(Tensor([16, 1587601],"float64"), )
[Prof] paddle.sinc 	 paddle.sinc(Tensor([16, 1587601],"float64"), ) 	 25401616 	 3389 	 9.97566819190979 	 1.0280640125274658 	 0.25074291229248047 	 0.3081068992614746 	 8.777927160263062 	 12.756260395050049 	 0.4408855438232422 	 0.32038283348083496 	 
2025-08-06 09:52:28.503886 test begin: paddle.sinc(Tensor([396901, 64],"float64"), )
[Prof] paddle.sinc 	 paddle.sinc(Tensor([396901, 64],"float64"), ) 	 25401664 	 3389 	 9.98054313659668 	 1.02146315574646 	 0.2508504390716553 	 0.30803656578063965 	 8.780757427215576 	 12.7559232711792 	 0.44103288650512695 	 0.32041096687316895 	 
2025-08-06 09:53:02.173694 test begin: paddle.sinh(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.sinh 	 paddle.sinh(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33851 	 9.991442441940308 	 10.958439350128174 	 0.3016364574432373 	 0.30449604988098145 	 15.229558944702148 	 25.161895513534546 	 0.4598109722137451 	 0.3798258304595947 	 
2025-08-06 09:54:07.627496 test begin: paddle.sinh(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.sinh 	 paddle.sinh(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33851 	 9.997628927230835 	 10.089725255966187 	 0.30183959007263184 	 0.3044567108154297 	 15.239490747451782 	 25.162228107452393 	 0.4600954055786133 	 0.3798215389251709 	 
2025-08-06 09:55:09.847441 test begin: paddle.sinh(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.sinh 	 paddle.sinh(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33851 	 9.994439840316772 	 10.100112199783325 	 0.3017725944519043 	 0.3045337200164795 	 15.240312814712524 	 25.161924600601196 	 0.46010613441467285 	 0.3797769546508789 	 
2025-08-06 09:56:13.889442 test begin: paddle.slice(Tensor([653440, 1555],"bfloat16"), axes=list[0,], starts=list[0,], ends=list[8168,], )
W0806 09:56:39.177373 135385 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f08e5ebef50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:06:22.575293 test begin: paddle.slice(Tensor([653440, 1555],"bfloat16"), axes=list[0,], starts=list[16336,], ends=list[24504,], )
W0806 10:06:42.724604 135848 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 10:07:00.565428 135848 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd2d739b010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:16:30.345192 test begin: paddle.slice(Tensor([653440, 1555],"bfloat16"), axes=list[0,], starts=list[24504,], ends=list[32672,], )
W0806 10:16:45.742770 136291 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 10:17:03.642051 136291 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2f1123f010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:26:36.811038 test begin: paddle.slice(Tensor([793810, 1280],"bfloat16"), axes=list[0,], starts=list[0,], ends=list[8168,], )
W0806 10:26:52.056304 136896 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 10:27:02.186607 136896 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f09b9e1b070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:36:46.314807 test begin: paddle.slice(Tensor([793810, 1280],"bfloat16"), axes=list[0,], starts=list[16336,], ends=list[24504,], )
W0806 10:37:01.556881 137595 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 10:37:12.108372 137595 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5ffb0db010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:46:54.053218 test begin: paddle.slice(Tensor([793810, 1280],"bfloat16"), axes=list[0,], starts=list[24504,], ends=list[32672,], )
W0806 10:47:09.379201 138131 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 10:47:19.666581 138131 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f48a3827070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:56:59.020433 test begin: paddle.slice_scatter(Tensor([8, 1058401, 3, 9],"float32"), Tensor([8, 1058401, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
W0806 10:57:06.486595 138688 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcfadf73100>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 11:07:03.988890 test begin: paddle.slice_scatter(Tensor([8, 117601, 3, 9],"float64"), Tensor([8, 117601, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
W0806 11:07:04.732546 139161 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 117601, 3, 9],"float64"), Tensor([8, 117601, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 31046664 	 54545 	 12.748294830322266 	 30.02188515663147 	 0.2387526035308838 	 0.1873767375946045 	 45.15622806549072 	 42.22132992744446 	 0.14097309112548828 	 0.15813851356506348 	 combined
2025-08-06 11:09:16.010606 test begin: paddle.slice_scatter(Tensor([8, 235201, 3, 9],"float32"), Tensor([8, 235201, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 235201, 3, 9],"float32"), Tensor([8, 235201, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 62093064 	 54545 	 17.906772136688232 	 34.97265839576721 	 0.3354811668395996 	 0.2183220386505127 	 50.61052894592285 	 47.1995108127594 	 0.15793085098266602 	 0.1768801212310791 	 combined
2025-08-06 11:11:52.605177 test begin: paddle.slice_scatter(Tensor([8, 529201, 3, 9],"float64"), Tensor([8, 529201, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 529201, 3, 9],"float64"), Tensor([8, 529201, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 139709064 	 54545 	 57.69564127922058 	 133.87784695625305 	 1.0811338424682617 	 0.8355147838592529 	 198.16247129440308 	 184.92003083229065 	 0.6183590888977051 	 0.6929361820220947 	 combined
2025-08-06 11:21:34.385763 test begin: paddle.slice_scatter(Tensor([8, 6, 117601, 9],"float32"), Tensor([8, 6, 117601, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 117601, 9],"float32"), Tensor([8, 6, 117601, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 62093328 	 54545 	 17.901968240737915 	 34.97895908355713 	 0.3354368209838867 	 0.21831464767456055 	 50.794585943222046 	 47.1868782043457 	 0.15853047370910645 	 0.17684578895568848 	 combined
2025-08-06 11:24:09.925831 test begin: paddle.slice_scatter(Tensor([8, 6, 211681, 5],"float32"), Tensor([8, 2, 211681, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 211681, 5],"float32"), Tensor([8, 2, 211681, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 67737920 	 54545 	 9.99443769454956 	 22.82586407661438 	 0.1872568130493164 	 0.14222168922424316 	 42.151161432266235 	 31.210448741912842 	 0.13158822059631348 	 0.11685037612915039 	 combined
2025-08-06 11:26:01.879405 test begin: paddle.slice_scatter(Tensor([8, 6, 264601, 9],"float64"), Tensor([8, 6, 264601, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 264601, 9],"float64"), Tensor([8, 6, 264601, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 139709328 	 54545 	 57.719218492507935 	 133.87839651107788 	 1.0817105770111084 	 0.8357341289520264 	 197.8344738483429 	 184.92023491859436 	 0.617384672164917 	 0.6926796436309814 	 combined
2025-08-06 11:35:43.250356 test begin: paddle.slice_scatter(Tensor([8, 6, 3, 1058401],"float32"), Tensor([8, 2, 3, 1058401],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 3, 1058401],"float32"), Tensor([8, 2, 3, 1058401],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 203212992 	 54545 	 29.490078926086426 	 67.04744601249695 	 0.5525202751159668 	 0.4182398319244385 	 124.23748898506165 	 91.24164271354675 	 0.38794398307800293 	 0.3416562080383301 	 combined
2025-08-06 11:41:01.162193 test begin: paddle.slice_scatter(Tensor([8, 6, 3, 352801],"float32"), Tensor([8, 2, 3, 352801],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 3, 352801],"float32"), Tensor([8, 2, 3, 352801],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 67737792 	 54545 	 9.990349531173706 	 22.79597783088684 	 0.18721723556518555 	 0.1421980857849121 	 42.2521767616272 	 31.204946756362915 	 0.13192343711853027 	 0.11685442924499512 	 combined
2025-08-06 11:42:49.398530 test begin: paddle.slice_scatter(Tensor([8, 6, 529201, 9],"float32"), Tensor([8, 6, 529201, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1e73116b30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 11:52:54.312267 test begin: paddle.slice_scatter(Tensor([8, 6, 58801, 9],"float64"), Tensor([8, 6, 58801, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
W0806 11:52:55.042207 140844 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 58801, 9],"float64"), Tensor([8, 6, 58801, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 31046928 	 54545 	 12.77167558670044 	 30.041551113128662 	 0.23926830291748047 	 0.18753433227539062 	 45.178438901901245 	 42.23374652862549 	 0.14100980758666992 	 0.15822815895080566 	 combined
2025-08-06 11:55:08.912522 test begin: paddle.slice_scatter(Tensor([8, 6, 635041, 5],"float32"), Tensor([8, 2, 635041, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 635041, 5],"float32"), Tensor([8, 2, 635041, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 203213120 	 54545 	 29.505385637283325 	 67.05797386169434 	 0.5528507232666016 	 0.41829991340637207 	 124.40781426429749 	 91.25980877876282 	 0.38842105865478516 	 0.3417091369628906 	 combined
2025-08-06 12:00:27.151267 test begin: paddle.slice_scatter(Tensor([80, 423361, 3, 5],"float32"), Tensor([80, 2, 3, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([80, 423361, 3, 5],"float32"), Tensor([80, 2, 3, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 508035600 	 54545 	 0.8535919189453125 	 167.34484124183655 	 4.458427429199219e-05 	 1.0432195663452148 	 167.66664457321167 	 167.66806840896606 	 0.5225489139556885 	 0.6270179748535156 	 combined
2025-08-06 12:09:08.922913 test begin: paddle.slice_scatter(Tensor([80, 6, 3, 176401],"float64"), Tensor([80, 6, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([80, 6, 3, 176401],"float64"), Tensor([80, 6, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 254020320 	 54545 	 0.8567357063293457 	 167.3710901737213 	 2.765655517578125e-05 	 1.0435471534729004 	 167.7789661884308 	 167.70868062973022 	 0.5228936672210693 	 0.6272163391113281 	 combined
2025-08-06 12:17:43.689617 test begin: paddle.slice_scatter(Tensor([80, 6, 3, 352801],"float32"), Tensor([80, 6, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([80, 6, 3, 352801],"float32"), Tensor([80, 6, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 508036320 	 54545 	 1.3259589672088623 	 167.60680651664734 	 9.250640869140625e-05 	 1.2897403240203857 	 167.75923919677734 	 167.7045693397522 	 0.5228629112243652 	 0.6271786689758301 	 combined
2025-08-06 12:26:27.118420 test begin: paddle.sqrt(Tensor([128, 396901],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([128, 396901],"float32"), ) 	 50803328 	 33937 	 9.995460748672485 	 10.149171113967896 	 0.3010060787200928 	 0.30531740188598633 	 15.291113376617432 	 25.354235410690308 	 0.4604325294494629 	 0.3816854953765869 	 
2025-08-06 12:27:29.677954 test begin: paddle.sqrt(Tensor([18, 15, 3, 256, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([18, 15, 3, 256, 256],"float32"), ) 	 53084160 	 33937 	 10.438318252563477 	 10.594393968582153 	 0.31435561180114746 	 0.31873655319213867 	 15.963109970092773 	 26.472284078598022 	 0.48086118698120117 	 0.3985559940338135 	 
2025-08-06 12:28:38.248408 test begin: paddle.sqrt(Tensor([259, 3, 256, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([259, 3, 256, 256],"float32"), ) 	 50921472 	 33937 	 10.020646095275879 	 11.133157014846802 	 0.3017771244049072 	 0.30591917037963867 	 15.322556495666504 	 25.413024425506592 	 0.46141862869262695 	 0.3826451301574707 	 
2025-08-06 12:29:43.594808 test begin: paddle.sqrt(Tensor([4, 15, 13, 256, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([4, 15, 13, 256, 256],"float32"), ) 	 51118080 	 33937 	 10.05963683128357 	 10.199424266815186 	 0.3030130863189697 	 0.3070354461669922 	 15.378238439559937 	 25.509079694747925 	 0.46312689781188965 	 0.38408732414245605 	 
2025-08-06 12:30:47.142038 test begin: paddle.sqrt(Tensor([4, 15, 3, 1103, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([4, 15, 3, 1103, 256],"float32"), ) 	 50826240 	 33937 	 9.999085664749146 	 10.142109155654907 	 0.301131010055542 	 0.3054380416870117 	 15.294711589813232 	 25.364923238754272 	 0.46057891845703125 	 0.3819866180419922 	 
2025-08-06 12:31:49.779869 test begin: paddle.sqrt(Tensor([4, 15, 3, 256, 1103],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([4, 15, 3, 256, 1103],"float32"), ) 	 50826240 	 33937 	 9.999963521957397 	 10.142042398452759 	 0.3010988235473633 	 0.3053734302520752 	 15.295247793197632 	 25.36472988128662 	 0.46060609817504883 	 0.38192200660705566 	 
2025-08-06 12:32:52.427447 test begin: paddle.sqrt(Tensor([4, 65, 3, 256, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([4, 65, 3, 256, 256],"float32"), ) 	 51118080 	 33937 	 10.059118747711182 	 10.195222854614258 	 0.3028984069824219 	 0.3070042133331299 	 15.378603458404541 	 25.50951337814331 	 0.46312665939331055 	 0.3840665817260742 	 
2025-08-06 12:33:55.399222 test begin: paddle.sqrt(Tensor([544, 93431],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([544, 93431],"float32"), ) 	 50826464 	 33937 	 10.002440690994263 	 10.14749813079834 	 0.3012514114379883 	 0.3054380416870117 	 15.297514200210571 	 25.36536955833435 	 0.4606456756591797 	 0.3819253444671631 	 
2025-08-06 12:34:58.052831 test begin: paddle.sqrt(Tensor([64, 13, 256, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([64, 13, 256, 256],"float32"), ) 	 54525952 	 33937 	 10.71993112564087 	 10.866505146026611 	 0.32282304763793945 	 0.327239990234375 	 16.39485263824463 	 27.18552827835083 	 0.49376416206359863 	 0.4093475341796875 	 
2025-08-06 12:36:05.738787 test begin: paddle.sqrt(Tensor([64, 3, 1034, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([64, 3, 1034, 256],"float32"), ) 	 50823168 	 33937 	 10.003817796707153 	 10.14185643196106 	 0.30127930641174316 	 0.3054373264312744 	 15.292387962341309 	 25.363418340682983 	 0.4605288505554199 	 0.38190722465515137 	 
2025-08-06 12:37:09.808352 test begin: paddle.sqrt(Tensor([64, 3, 256, 1034],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([64, 3, 256, 1034],"float32"), ) 	 50823168 	 33937 	 10.003989458084106 	 10.146988868713379 	 0.3012816905975342 	 0.30539965629577637 	 15.292674541473389 	 25.36321520805359 	 0.46055078506469727 	 0.38190340995788574 	 
2025-08-06 12:38:14.339561 test begin: paddle.square(Tensor([104, 488493],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([104, 488493],"float32"), ) 	 50803272 	 33810 	 9.995325088500977 	 10.07001280784607 	 0.30210375785827637 	 0.3041801452636719 	 15.208509683609009 	 35.69630432128906 	 0.4597508907318115 	 0.26993870735168457 	 
2025-08-06 12:39:27.909275 test begin: paddle.square(Tensor([128, 396901],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([128, 396901],"float32"), ) 	 50803328 	 33810 	 9.99567723274231 	 10.063274383544922 	 0.3021852970123291 	 0.3041667938232422 	 15.208781480789185 	 35.69530725479126 	 0.45977187156677246 	 0.26998114585876465 	 
2025-08-06 12:40:41.602651 test begin: paddle.square(Tensor([24904, 12, 170, 1],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([24904, 12, 170, 1],"float32"), ) 	 50804160 	 33810 	 9.991920471191406 	 10.065947771072388 	 0.30202579498291016 	 0.304171085357666 	 15.206567764282227 	 35.69686794281006 	 0.45970654487609863 	 0.26993823051452637 	 
2025-08-06 12:41:54.816082 test begin: paddle.square(Tensor([3548, 12, 1194, 1],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([3548, 12, 1194, 1],"float32"), ) 	 50835744 	 33810 	 10.007655143737793 	 10.070109367370605 	 0.3025345802307129 	 0.30440831184387207 	 15.219578266143799 	 35.719208002090454 	 0.46004271507263184 	 0.2700626850128174 	 
2025-08-06 12:43:07.732200 test begin: paddle.square(Tensor([3548, 12, 170, 8],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([3548, 12, 170, 8],"float32"), ) 	 57903360 	 33810 	 11.376809358596802 	 13.124865531921387 	 0.3438684940338135 	 0.34592747688293457 	 17.31453514099121 	 40.60483527183533 	 0.5233111381530762 	 0.3070237636566162 	 
2025-08-06 12:44:37.914731 test begin: paddle.square(Tensor([3548, 85, 170, 1],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([3548, 85, 170, 1],"float32"), ) 	 51268600 	 33810 	 10.08704662322998 	 11.66408896446228 	 0.3049345016479492 	 0.3068554401397705 	 15.347640991210938 	 36.004019021987915 	 0.4639136791229248 	 0.2722752094268799 	 
2025-08-06 12:45:54.081993 test begin: paddle.square(Tensor([544, 93431],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([544, 93431],"float32"), ) 	 50826464 	 33810 	 10.001728534698486 	 10.067443132400513 	 0.302340030670166 	 0.3043098449707031 	 15.215446710586548 	 35.709402322769165 	 0.4598691463470459 	 0.270064115524292 	 
2025-08-06 12:47:06.905787 test begin: paddle.squeeze(Tensor([100, 512, 1, 100, 100],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([100, 512, 1, 100, 100],"float32"), axis=list[2,], ) 	 512000000 	 2700427 	 13.077037334442139 	 13.211927890777588 	 0.0003204345703125 	 0.00011849403381347656 	 114.85736322402954 	 155.58312964439392 	 0.0001220703125 	 0.0002551078796386719 	 
2025-08-06 12:52:21.335314 test begin: paddle.squeeze(Tensor([1053440, 483],"float32"), )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([1053440, 483],"float32"), ) 	 508811520 	 2700427 	 10.806186437606812 	 11.155450105667114 	 0.0001506805419921875 	 0.0007054805755615234 	 128.05676913261414 	 140.29812097549438 	 0.00015306472778320312 	 0.00025773048400878906 	 
2025-08-06 12:57:29.607353 test begin: paddle.squeeze(Tensor([3969010, 128],"float32"), )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([3969010, 128],"float32"), ) 	 508033280 	 2700427 	 16.084511756896973 	 17.862661600112915 	 0.00018453598022460938 	 0.0001327991485595703 	 132.75022768974304 	 149.75184679031372 	 0.0002262592315673828 	 0.0006487369537353516 	 
2025-08-06 13:03:04.930627 test begin: paddle.squeeze(Tensor([4211200, 25, 5],"float32"), axis=-1, )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([4211200, 25, 5],"float32"), axis=-1, ) 	 526400000 	 2700427 	 13.023305892944336 	 10.516637563705444 	 0.00029921531677246094 	 0.0007157325744628906 	 115.07476115226746 	 136.9855761528015 	 0.00012254714965820312 	 0.00023102760314941406 	 
2025-08-06 13:08:00.770118 test begin: paddle.squeeze(Tensor([4211200, 31, 4],"float32"), axis=-1, )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([4211200, 31, 4],"float32"), axis=-1, ) 	 522188800 	 2700427 	 15.672219514846802 	 10.557009220123291 	 0.00015664100646972656 	 0.0008115768432617188 	 116.51062035560608 	 149.44488310813904 	 0.0001983642578125 	 0.000240325927734375 	 
2025-08-06 13:13:12.628446 test begin: paddle.squeeze(Tensor([5080330, 25, 4],"float32"), axis=-1, )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([5080330, 25, 4],"float32"), axis=-1, ) 	 508033000 	 2700427 	 12.812564611434937 	 10.63126015663147 	 0.00036907196044921875 	 0.0007293224334716797 	 113.02125358581543 	 163.81510543823242 	 0.00015497207641601562 	 0.0002913475036621094 	 
2025-08-06 13:18:31.082750 test begin: paddle.squeeze(Tensor([80, 512, 1, 100, 125],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([80, 512, 1, 100, 125],"float32"), axis=list[2,], ) 	 512000000 	 2700427 	 13.07435417175293 	 13.495422601699829 	 0.0001583099365234375 	 0.000152587890625 	 122.24372553825378 	 181.93529152870178 	 0.00010609626770019531 	 0.0002396106719970703 	 
2025-08-06 13:24:19.743928 test begin: paddle.squeeze(Tensor([80, 512, 1, 125, 100],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([80, 512, 1, 125, 100],"float32"), axis=list[2,], ) 	 512000000 	 2700427 	 12.951090097427368 	 13.32796859741211 	 0.0001239776611328125 	 0.0001614093780517578 	 114.32759809494019 	 186.2365963459015 	 0.00012731552124023438 	 0.00023412704467773438 	 
2025-08-06 13:30:04.767688 test begin: paddle.squeeze(Tensor([80, 512, 2, 100, 100],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([80, 512, 2, 100, 100],"float32"), axis=list[2,], ) 	 819200000 	 2700427 	 13.065615177154541 	 14.097697257995605 	 0.00013899803161621094 	 0.0003871917724609375 	 114.3071219921112 	 169.97331643104553 	 0.00010895729064941406 	 0.00024199485778808594 	 
2025-08-06 13:35:43.785148 test begin: paddle.squeeze(Tensor([80, 636, 1, 100, 100],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([80, 636, 1, 100, 100],"float32"), axis=list[2,], ) 	 508800000 	 2700427 	 14.539147853851318 	 13.440146684646606 	 0.0001633167266845703 	 0.0001163482666015625 	 113.7908763885498 	 144.38264298439026 	 0.00013327598571777344 	 0.0006339550018310547 	 
2025-08-06 13:40:48.078001 test begin: paddle.stack(list[Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),], axis=-2, ) 	 259269120 	 5521 	 10.007869958877563 	 39.323530435562134 	 1.8527770042419434 	 7.278899669647217 	 11.18419885635376 	 0.5865688323974609 	 2.0702059268951416 	 7.843971252441406e-05 	 
2025-08-06 13:41:59.814738 test begin: paddle.stack(list[Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),], axis=-2, ) 	 270103680 	 5521 	 10.464903116226196 	 40.978049993515015 	 1.9371602535247803 	 7.5852415561676025 	 11.739790916442871 	 0.6738474369049072 	 2.173123598098755 	 9.34600830078125e-05 	 
2025-08-06 13:43:13.021343 test begin: paddle.stack(list[Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),], axis=0, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),], axis=0, ) 	 609681408 	 5521 	 16.30908441543579 	 14.250088453292847 	 3.017307996749878 	 2.6360626220703125 	 25.222498178482056 	 14.915249586105347 	 4.66891074180603 	 1.3803739547729492 	 
2025-08-06 13:44:44.829087 test begin: paddle.stack(list[Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),], axis=0, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),], axis=0, ) 	 609681408 	 5521 	 16.302467346191406 	 14.240265369415283 	 3.017392873764038 	 2.6360785961151123 	 25.222377061843872 	 14.915580987930298 	 4.668869972229004 	 1.3806798458099365 	 
2025-08-06 13:46:20.301612 test begin: paddle.stack(list[Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),], axis=-2, ) 	 282839040 	 5521 	 11.635921716690063 	 45.12301540374756 	 2.6765739917755127 	 8.469166994094849 	 12.233551979064941 	 0.591428279876709 	 2.264523506164551 	 7.534027099609375e-05 	 
2025-08-06 13:47:39.827175 test begin: paddle.stack(list[Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),], axis=-2, ) 	 294658560 	 5521 	 11.489442586898804 	 46.88129663467407 	 2.1289186477661133 	 8.678054571151733 	 12.725787878036499 	 0.5894584655761719 	 2.3557002544403076 	 7.700920104980469e-05 	 
2025-08-06 13:49:01.316511 test begin: paddle.stack(list[Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),], axis=-2, ) 	 254018560 	 5521 	 9.830990314483643 	 38.532450914382935 	 1.819234848022461 	 7.131464004516602 	 10.964798212051392 	 0.5900506973266602 	 2.029633045196533 	 8.058547973632812e-05 	 
2025-08-06 13:50:12.620177 test begin: paddle.stack(list[Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),], axis=-2, ) 	 257826240 	 5521 	 9.968609094619751 	 39.1068058013916 	 1.8454480171203613 	 7.238717317581177 	 11.30686092376709 	 0.4436516761779785 	 2.172746419906616 	 7.581710815429688e-05 	 
2025-08-06 13:51:23.289165 test begin: paddle.stack(list[Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),], axis=-2, ) 	 259269120 	 5521 	 10.009782075881958 	 39.321470737457275 	 1.8530035018920898 	 7.278835296630859 	 11.19034218788147 	 0.6176674365997314 	 2.0711913108825684 	 0.0002079010009765625 	 
2025-08-06 13:52:33.042672 test begin: paddle.stanh(x=Tensor([12700801, 2],"float64"), scale_a=6.42, scale_b=3.58, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([12700801, 2],"float64"), scale_a=6.42, scale_b=3.58, ) 	 25401602 	 33926 	 10.37676191329956 	 10.405277490615845 	 0.31260013580322266 	 0.3134610652923584 	 15.12937617301941 	 25.145506620407104 	 0.4556708335876465 	 0.37872743606567383 	 
2025-08-06 13:53:37.803083 test begin: paddle.stanh(x=Tensor([2, 12700801],"float64"), scale_a=6.42, scale_b=3.58, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2, 12700801],"float64"), scale_a=6.42, scale_b=3.58, ) 	 25401602 	 33926 	 10.380837440490723 	 10.40548586845398 	 0.31285977363586426 	 0.31346654891967773 	 15.130134344100952 	 25.143688440322876 	 0.45571303367614746 	 0.3787703514099121 	 
2025-08-06 13:54:41.950810 test begin: paddle.stanh(x=Tensor([2, 25401601],"float32"), scale_a=6.42, scale_b=3.58, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2, 25401601],"float32"), scale_a=6.42, scale_b=3.58, ) 	 50803202 	 33926 	 9.99988055229187 	 10.143517255783081 	 0.3012275695800781 	 0.305311918258667 	 15.281010389328003 	 25.20334482192993 	 0.46030163764953613 	 0.37963271141052246 	 
2025-08-06 13:55:46.137117 test begin: paddle.stanh(x=Tensor([2, 3, 2, 2116801],"float64"), scale_a=0.67, scale_b=1.72, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2, 3, 2, 2116801],"float64"), scale_a=0.67, scale_b=1.72, ) 	 25401612 	 33926 	 10.154615640640259 	 10.199187278747559 	 0.30590200424194336 	 0.3068256378173828 	 15.186391592025757 	 25.14345645904541 	 0.4576690196990967 	 0.3786742687225342 	 
2025-08-06 13:56:49.753571 test begin: paddle.stanh(x=Tensor([2, 3, 2116801, 2],"float64"), scale_a=0.67, scale_b=1.72, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2, 3, 2116801, 2],"float64"), scale_a=0.67, scale_b=1.72, ) 	 25401612 	 33926 	 10.154632568359375 	 10.184249877929688 	 0.30590248107910156 	 0.3068222999572754 	 15.184826612472534 	 25.144693851470947 	 0.45752930641174316 	 0.3786749839782715 	 
2025-08-06 13:57:51.649609 test begin: paddle.stanh(x=Tensor([2, 3175201, 2, 2],"float64"), scale_a=0.67, scale_b=1.72, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2, 3175201, 2, 2],"float64"), scale_a=0.67, scale_b=1.72, ) 	 25401608 	 33926 	 10.154680728912354 	 10.186927318572998 	 0.3059113025665283 	 0.3068506717681885 	 15.186340808868408 	 25.143823385238647 	 0.45729517936706543 	 0.37863636016845703 	 
2025-08-06 13:58:53.457351 test begin: paddle.stanh(x=Tensor([2116801, 3, 2, 2],"float64"), scale_a=0.67, scale_b=1.72, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2116801, 3, 2, 2],"float64"), scale_a=0.67, scale_b=1.72, ) 	 25401612 	 33926 	 10.154782772064209 	 10.184027910232544 	 0.3059089183807373 	 0.3068087100982666 	 15.186864376068115 	 25.144240856170654 	 0.457409143447876 	 0.37866997718811035 	 
2025-08-06 13:59:55.377499 test begin: paddle.stanh(x=Tensor([25401601, 2],"float32"), scale_a=6.42, scale_b=3.58, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([25401601, 2],"float32"), scale_a=6.42, scale_b=3.58, ) 	 50803202 	 33926 	 9.999898672103882 	 10.133396863937378 	 0.3012235164642334 	 0.3052370548248291 	 15.281198024749756 	 25.20366072654724 	 0.4603877067565918 	 0.3795793056488037 	 
2025-08-06 14:01:00.468171 test begin: paddle.std(Tensor([1, 1270081, 4, 10],"float32"), list[1,3,], True, False, )
W0806 14:01:01.322471 144099 dygraph_functions.cc:88394] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.std 	 paddle.std(Tensor([1, 1270081, 4, 10],"float32"), list[1,3,], True, False, ) 	 50803240 	 9126 	 11.828163146972656 	 2.107797861099243 	 5.8650970458984375e-05 	 0.11802911758422852 	 13.056319236755371 	 7.32392430305481 	 0.18306946754455566 	 0.09138035774230957 	 
2025-08-06 14:01:37.723923 test begin: paddle.std(Tensor([1, 3, 1693441, 10],"float32"), list[1,3,], True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([1, 3, 1693441, 10],"float32"), list[1,3,], True, False, ) 	 50803230 	 9126 	 13.995681285858154 	 7.239976406097412 	 4.863739013671875e-05 	 0.8107864856719971 	 15.03670620918274 	 9.803202390670776 	 0.24058079719543457 	 0.13745474815368652 	 
2025-08-06 14:02:24.730170 test begin: paddle.std(Tensor([1, 3, 4, 2116801],"float64"), 2, True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([1, 3, 4, 2116801],"float64"), 2, True, False, ) 	 25401612 	 9126 	 15.107155323028564 	 2.038154125213623 	 0.00010132789611816406 	 0.23033714294433594 	 18.45174264907837 	 13.564535856246948 	 0.29513025283813477 	 0.18981242179870605 	 
2025-08-06 14:03:14.661970 test begin: paddle.std(Tensor([1, 3, 4, 4233601],"float32"), list[1,3,], True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([1, 3, 4, 4233601],"float32"), list[1,3,], True, False, ) 	 50803212 	 9126 	 11.434178590774536 	 2.1452815532684326 	 5.793571472167969e-05 	 0.12015390396118164 	 12.86452054977417 	 7.290489673614502 	 0.18041634559631348 	 0.09103727340698242 	 
2025-08-06 14:03:50.773448 test begin: paddle.std(Tensor([1, 3, 846721, 10],"float64"), 2, True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([1, 3, 846721, 10],"float64"), 2, True, False, ) 	 25401630 	 9126 	 73.03548669815063 	 1.6925678253173828 	 5.173683166503906e-05 	 0.09476685523986816 	 43.9142382144928 	 7.109036445617676 	 0.615929365158081 	 0.08872175216674805 	 
2025-08-06 14:05:57.093345 test begin: paddle.std(Tensor([1, 635041, 4, 10],"float64"), 2, True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([1, 635041, 4, 10],"float64"), 2, True, False, ) 	 25401640 	 9126 	 16.06749725341797 	 1.8110320568084717 	 0.00010776519775390625 	 0.20272374153137207 	 17.697169303894043 	 11.566465854644775 	 0.2831249237060547 	 0.1620163917541504 	 
2025-08-06 14:06:47.063310 test begin: paddle.std(Tensor([1587601, 32],"float32"), )
[Prof] paddle.std 	 paddle.std(Tensor([1587601, 32],"float32"), ) 	 50803232 	 9126 	 10.199194192886353 	 1.519604206085205 	 8.20159912109375e-05 	 0.08514642715454102 	 12.210817813873291 	 7.082640886306763 	 0.17122697830200195 	 0.08840703964233398 	 
2025-08-06 14:07:18.998421 test begin: paddle.std(Tensor([211681, 3, 4, 10],"float64"), 2, True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([211681, 3, 4, 10],"float64"), 2, True, False, ) 	 25401720 	 9126 	 16.13044571876526 	 1.813889980316162 	 0.00010085105895996094 	 0.20272541046142578 	 17.698525428771973 	 11.566052675247192 	 0.28313660621643066 	 0.16200900077819824 	 
2025-08-06 14:08:06.936074 test begin: paddle.std(Tensor([32, 1587601],"float32"), )
[Prof] paddle.std 	 paddle.std(Tensor([32, 1587601],"float32"), ) 	 50803232 	 9126 	 10.012884855270386 	 1.5196986198425293 	 6.29425048828125e-05 	 0.08508944511413574 	 12.209460496902466 	 7.083299160003662 	 0.17119526863098145 	 0.08843708038330078 	 
2025-08-06 14:08:40.903419 test begin: paddle.std(Tensor([423361, 3, 4, 10],"float32"), list[1,3,], True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([423361, 3, 4, 10],"float32"), list[1,3,], True, False, ) 	 50803320 	 9126 	 14.130621194839478 	 7.5332348346710205 	 5.412101745605469e-05 	 0.8436069488525391 	 14.915045976638794 	 10.059032678604126 	 0.23862314224243164 	 0.14101624488830566 	 
2025-08-06 14:09:28.546153 test begin: paddle.strided_slice(x=Tensor([301, 4, 3528, 6],"float64"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
[Prof] paddle.strided_slice 	 paddle.strided_slice(x=Tensor([301, 4, 3528, 6],"float64"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], ) 	 25486272 	 476161 	 2.814072608947754 	 102.2599515914917 	 6.818771362304688e-05 	 0.0002162456512451172 	 71.51373553276062 	 120.74974346160889 	 0.07680153846740723 	 0.008886098861694336 	 combined
2025-08-06 14:14:26.692082 test begin: paddle.subtract(Tensor([24904, 12, 170, 1],"float32"), Tensor([24904, 12, 170, 1],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([24904, 12, 170, 1],"float32"), Tensor([24904, 12, 170, 1],"float32"), ) 	 101608320 	 28013 	 12.614389419555664 	 12.511775493621826 	 0.4601435661315918 	 0.4563016891479492 	 13.350072383880615 	 8.340978622436523 	 0.4869682788848877 	 0.30417776107788086 	 
2025-08-06 14:15:16.323325 test begin: paddle.subtract(Tensor([3548, 12, 1194, 1],"float32"), Tensor([3548, 12, 1194, 1],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([3548, 12, 1194, 1],"float32"), Tensor([3548, 12, 1194, 1],"float32"), ) 	 101671488 	 28013 	 12.623541355133057 	 12.527008533477783 	 0.46033740043640137 	 0.4566349983215332 	 13.357931852340698 	 8.345232248306274 	 0.48714780807495117 	 0.3043501377105713 	 
2025-08-06 14:16:06.008525 test begin: paddle.subtract(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 8],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 8],"float32"), ) 	 65141280 	 28013 	 10.018807649612427 	 10.366895198822021 	 0.3653533458709717 	 0.3777294158935547 	 24.849435091018677 	 25.171780586242676 	 0.4531440734863281 	 0.4589989185333252 	 
2025-08-06 14:17:19.374430 test begin: paddle.subtract(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 1],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 1],"float32"), ) 	 65141280 	 28013 	 10.005549430847168 	 10.36226773262024 	 0.3649601936340332 	 0.3776853084564209 	 23.023685216903687 	 25.170316696166992 	 0.27985715866088867 	 0.4589521884918213 	 
2025-08-06 14:18:30.322236 test begin: paddle.subtract(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 8],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 8],"float32"), ) 	 115806720 	 28013 	 14.36634612083435 	 14.253786563873291 	 0.5240597724914551 	 0.519474983215332 	 15.14797067642212 	 9.487073183059692 	 0.5525310039520264 	 0.3473551273345947 	 
2025-08-06 14:19:28.556245 test begin: paddle.subtract(Tensor([3548, 85, 170, 1],"float32"), Tensor([3548, 85, 170, 1],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([3548, 85, 170, 1],"float32"), Tensor([3548, 85, 170, 1],"float32"), ) 	 102537200 	 28013 	 12.805835247039795 	 12.626245260238647 	 0.4644625186920166 	 0.46047544479370117 	 13.422144174575806 	 8.413439750671387 	 0.48949551582336426 	 0.3068678379058838 	 
2025-08-06 14:20:18.657746 test begin: paddle.subtract(Tensor([517, 4, 3, 64, 128],"float32"), Tensor([517, 4, 3, 64, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([517, 4, 3, 64, 128],"float32"), Tensor([517, 4, 3, 64, 128],"float32"), ) 	 101646336 	 28013 	 12.62249231338501 	 12.51694917678833 	 0.46041417121887207 	 0.4565286636352539 	 13.307196378707886 	 8.342434644699097 	 0.4853854179382324 	 0.30434608459472656 	 
2025-08-06 14:21:08.755059 test begin: paddle.subtract(Tensor([64, 3, 3, 64, 1379],"float32"), Tensor([64, 3, 3, 64, 1379],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 3, 3, 64, 1379],"float32"), Tensor([64, 3, 3, 64, 1379],"float32"), ) 	 101670912 	 28013 	 12.626315116882324 	 14.025926351547241 	 0.46065330505371094 	 0.45655059814453125 	 13.315603256225586 	 8.34556531906128 	 0.48568248748779297 	 0.30434751510620117 	 
2025-08-06 14:22:01.279469 test begin: paddle.subtract(Tensor([64, 3, 3, 690, 128],"float32"), Tensor([64, 3, 3, 690, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 3, 3, 690, 128],"float32"), Tensor([64, 3, 3, 690, 128],"float32"), ) 	 101744640 	 28013 	 12.634615659713745 	 12.525699853897095 	 0.4607398509979248 	 0.4568774700164795 	 13.330072402954102 	 8.351863622665405 	 0.48625969886779785 	 0.3045628070831299 	 
2025-08-06 14:22:51.551097 test begin: paddle.subtract(Tensor([64, 3, 33, 64, 128],"float32"), Tensor([64, 3, 33, 64, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 3, 33, 64, 128],"float32"), Tensor([64, 3, 33, 64, 128],"float32"), ) 	 103809024 	 28013 	 12.8903489112854 	 12.77966570854187 	 0.4701259136199951 	 0.46611976623535156 	 13.618504047393799 	 8.514400005340576 	 0.4965791702270508 	 0.31055283546447754 	 
2025-08-06 14:23:42.887922 test begin: paddle.subtract(Tensor([64, 33, 3, 64, 128],"float32"), Tensor([64, 33, 3, 64, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 33, 3, 64, 128],"float32"), Tensor([64, 33, 3, 64, 128],"float32"), ) 	 103809024 	 28013 	 12.88901400566101 	 12.77930998802185 	 0.47010278701782227 	 0.46613383293151855 	 13.618940353393555 	 8.5156409740448 	 0.4967684745788574 	 0.3105590343475342 	 
2025-08-06 14:24:33.663728 test begin: paddle.subtract(Tensor([64, 4, 25, 64, 128],"float32"), Tensor([64, 4, 25, 64, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 4, 25, 64, 128],"float32"), Tensor([64, 4, 25, 64, 128],"float32"), ) 	 104857600 	 28013 	 13.01937222480774 	 12.90906810760498 	 0.47503089904785156 	 0.4707956314086914 	 13.683161735534668 	 8.603039026260376 	 0.4989748001098633 	 0.31376099586486816 	 
2025-08-06 14:25:24.849228 test begin: paddle.subtract(Tensor([64, 4, 3, 517, 128],"float32"), Tensor([64, 4, 3, 517, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 4, 3, 517, 128],"float32"), Tensor([64, 4, 3, 517, 128],"float32"), ) 	 101646336 	 28013 	 12.62164831161499 	 12.520787954330444 	 0.46036624908447266 	 0.45641446113586426 	 13.307861328125 	 8.343634128570557 	 0.48539209365844727 	 0.3043231964111328 	 
2025-08-06 14:26:15.345586 test begin: paddle.subtract(Tensor([64, 4, 3, 64, 1034],"float32"), Tensor([64, 4, 3, 64, 1034],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 4, 3, 64, 1034],"float32"), Tensor([64, 4, 3, 64, 1034],"float32"), ) 	 101646336 	 28013 	 12.622705936431885 	 12.514351606369019 	 0.46042966842651367 	 0.45648980140686035 	 13.307504415512085 	 8.343677043914795 	 0.4853823184967041 	 0.3056771755218506 	 
2025-08-06 14:27:04.801718 test begin: paddle.subtract(Tensor([690, 3, 3, 64, 128],"float32"), Tensor([690, 3, 3, 64, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([690, 3, 3, 64, 128],"float32"), Tensor([690, 3, 3, 64, 128],"float32"), ) 	 101744640 	 28013 	 12.636195421218872 	 12.525465965270996 	 0.46083879470825195 	 0.4569120407104492 	 13.331627607345581 	 8.351842403411865 	 0.4862525463104248 	 0.30459165573120117 	 
2025-08-06 14:27:54.255178 test begin: paddle.sum(Tensor([3544, 32, 896],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([3544, 32, 896],"bfloat16"), axis=1, keepdim=False, ) 	 101613568 	 58698 	 10.00368046760559 	 9.382120132446289 	 0.17412853240966797 	 0.16335725784301758 	 15.760532140731812 	 4.940403938293457 	 0.2743220329284668 	 0.00015926361083984375 	 
2025-08-06 14:28:38.643530 test begin: paddle.sum(Tensor([6017, 19, 896],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6017, 19, 896],"bfloat16"), axis=1, keepdim=False, ) 	 102433408 	 58698 	 10.23239541053772 	 10.294996500015259 	 0.17812180519104004 	 0.17922186851501465 	 15.891664743423462 	 5.66614031791687 	 0.277935266494751 	 0.000194549560546875 	 
2025-08-06 14:29:23.628726 test begin: paddle.sum(Tensor([6017, 32, 528],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6017, 32, 528],"bfloat16"), axis=1, keepdim=False, ) 	 101663232 	 58698 	 10.737275838851929 	 9.534862279891968 	 0.18690824508666992 	 0.1660139560699463 	 15.778570413589478 	 5.632639646530151 	 0.27468109130859375 	 0.00021219253540039062 	 
2025-08-06 14:30:07.071607 test begin: paddle.sum(Tensor([6036, 19, 896],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6036, 19, 896],"bfloat16"), axis=1, keepdim=False, ) 	 102756864 	 58698 	 10.273184537887573 	 10.342359066009521 	 0.17884349822998047 	 0.17972612380981445 	 15.938122510910034 	 5.2129294872283936 	 0.2774665355682373 	 0.00021505355834960938 	 
2025-08-06 14:30:50.643231 test begin: paddle.sum(Tensor([6036, 32, 527],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6036, 32, 527],"bfloat16"), axis=1, keepdim=False, ) 	 101791104 	 58698 	 10.87137222290039 	 10.549443244934082 	 0.1892232894897461 	 0.18366241455078125 	 15.800376653671265 	 5.140177488327026 	 0.2762761116027832 	 8.225440979003906e-05 	 
2025-08-06 14:31:35.305120 test begin: paddle.sum(Tensor([6078, 19, 896],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6078, 19, 896],"bfloat16"), axis=1, keepdim=False, ) 	 103471872 	 58698 	 10.35792851448059 	 10.40231728553772 	 0.18034720420837402 	 0.18108415603637695 	 16.052200317382812 	 4.959269762039185 	 0.2793855667114258 	 8.678436279296875e-05 	 
2025-08-06 14:32:18.895034 test begin: paddle.sum(Tensor([6078, 32, 523],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6078, 32, 523],"bfloat16"), axis=1, keepdim=False, ) 	 101721408 	 58698 	 10.643325328826904 	 10.336570501327515 	 0.18529152870178223 	 0.17995500564575195 	 15.78683352470398 	 5.436279058456421 	 0.27481865882873535 	 8.273124694824219e-05 	 
2025-08-06 14:33:05.856177 test begin: paddle.t(Tensor([100, 5080321],"float32"), )
[Prof] paddle.t 	 paddle.t(Tensor([100, 5080321],"float32"), ) 	 508032100 	 2515777 	 10.861452341079712 	 10.560504913330078 	 0.00014901161193847656 	 0.00028443336486816406 	 102.31281304359436 	 162.11149740219116 	 0.00011348724365234375 	 0.0002295970916748047 	 
2025-08-06 14:38:08.576939 test begin: paddle.t(Tensor([200, 2540161],"float32"), )
[Prof] paddle.t 	 paddle.t(Tensor([200, 2540161],"float32"), ) 	 508032200 	 2515777 	 10.670475959777832 	 9.321171522140503 	 0.00016355514526367188 	 0.00012731552124023438 	 101.32269883155823 	 160.0308964252472 	 0.00012302398681640625 	 0.00024390220642089844 	 
2025-08-06 14:43:08.352818 test begin: paddle.t(Tensor([25401610, 20],"float32"), )
[Prof] paddle.t 	 paddle.t(Tensor([25401610, 20],"float32"), ) 	 508032200 	 2515777 	 10.865252256393433 	 9.151633024215698 	 0.00016117095947265625 	 0.00010967254638671875 	 101.59486150741577 	 160.14913392066956 	 0.0001404285430908203 	 0.0002503395080566406 	 
2025-08-06 14:48:06.917190 test begin: paddle.t(Tensor([496130, 512],"int64"), )
[Prof] paddle.t 	 paddle.t(Tensor([496130, 512],"int64"), ) 	 254018560 	 2515777 	 10.84067702293396 	 9.285056591033936 	 8.678436279296875e-05 	 0.0001068115234375 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:50:17.770661 test begin: paddle.t(Tensor([50803210, 10],"float32"), )
[Prof] paddle.t 	 paddle.t(Tensor([50803210, 10],"float32"), ) 	 508032100 	 2515777 	 11.690601110458374 	 9.278532266616821 	 0.00013899803161621094 	 0.00022792816162109375 	 102.07066535949707 	 160.09179782867432 	 0.0001380443572998047 	 0.0002636909484863281 	 
2025-08-06 14:55:19.290791 test begin: paddle.t(Tensor([5120, 49613],"int64"), )
[Prof] paddle.t 	 paddle.t(Tensor([5120, 49613],"int64"), ) 	 254018560 	 2515777 	 11.033724069595337 	 10.092634439468384 	 7.677078247070312e-05 	 0.00030541419982910156 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:57:34.206230 test begin: paddle.take(Tensor([12700801, 4],"float32"), Tensor([2, 12700801],"int64"), mode="raise", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd3c1e76a10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 15:07:43.924194 test begin: paddle.take(Tensor([12700801, 4],"float32"), Tensor([201, 3],"int64"), mode="raise", )
W0806 15:07:44.974869 41583 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.take 	 paddle.take(Tensor([12700801, 4],"float32"), Tensor([201, 3],"int64"), mode="raise", ) 	 50803807 	 172659 	 15.30069613456726 	 22.369107723236084 	 6.222724914550781e-05 	 0.0002541542053222656 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:08:51.324630 test begin: paddle.take(Tensor([12700801, 4],"float32"), Tensor([6350401, 3],"int64"), mode="raise", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb4cfeaab00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 15:18:57.169173 test begin: paddle.take(Tensor([3, 16934401],"float32"), Tensor([2, 3],"int64"), mode="raise", )
W0806 15:18:58.325448 61329 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.take 	 paddle.take(Tensor([3, 16934401],"float32"), Tensor([2, 3],"int64"), mode="raise", ) 	 50803209 	 172659 	 15.114275217056274 	 29.826987266540527 	 5.5789947509765625e-05 	 0.00024390220642089844 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:20:13.995167 test begin: paddle.take(Tensor([3, 16934401],"float32"), Tensor([2, 8467201],"int64"), mode="raise", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f17cdcbaaa0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 15:30:19.765511 test begin: paddle.take(Tensor([3, 16934401],"float32"), Tensor([201, 3],"int64"), mode="raise", )
W0806 15:30:20.720650 80926 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.take 	 paddle.take(Tensor([3, 16934401],"float32"), Tensor([201, 3],"int64"), mode="raise", ) 	 50803806 	 172659 	 14.583552837371826 	 25.829797983169556 	 5.340576171875e-05 	 0.00044846534729003906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:31:31.063124 test begin: paddle.take(Tensor([3, 4],"float32"), Tensor([2, 12700801],"int64"), mode="raise", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fae5e022b60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 15:41:46.359955 test begin: paddle.take(Tensor([3, 4],"float32"), Tensor([8467201, 3],"int64"), mode="raise", )
W0806 15:41:47.313760 99918 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2e3f397010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 15:51:50.912146 test begin: paddle.take(Tensor([3, 4],"float64"), Tensor([3175201, 8],"int64"), mode="clip", )
W0806 15:51:51.540987 116588 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f197f753010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 16:01:55.444147 test begin: paddle.take(Tensor([3, 4],"float64"), Tensor([3175201, 8],"int64"), mode="wrap", )
W0806 16:01:59.082430 133636 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f769a45f010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 16:12:00.102574 test begin: paddle.take(Tensor([3, 4],"float64"), Tensor([5, 5080321],"int64"), mode="clip", )
W0806 16:12:00.752683 150941 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4d3fb0afb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 16:22:04.694746 test begin: paddle.take(Tensor([3, 4],"float64"), Tensor([5, 5080321],"int64"), mode="wrap", )
W0806 16:22:05.339601 12700 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3379c46fb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 16:32:09.311627 test begin: paddle.take(Tensor([3, 8467201],"float64"), Tensor([5, 8467201],"int64"), mode="clip", )
W0806 16:32:10.822063 39888 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7683eff0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 16:42:14.240002 test begin: paddle.take(Tensor([3, 8467201],"float64"), Tensor([5, 8467201],"int64"), mode="wrap", )
W0806 16:42:15.638870 65937 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0ea0806ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 16:52:19.215926 test begin: paddle.take(Tensor([3, 8467201],"float64"), Tensor([5, 8],"int64"), mode="wrap", )
W0806 16:52:19.917546 91484 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.take 	 paddle.take(Tensor([3, 8467201],"float64"), Tensor([5, 8],"int64"), mode="wrap", ) 	 25401643 	 172659 	 25.359484434127808 	 12.916286706924438 	 7.581710815429688e-05 	 0.00011777877807617188 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 16:53:24.253304 test begin: paddle.take(Tensor([3, 8467201],"float64"), Tensor([501, 8],"int64"), mode="clip", )
[Prof] paddle.take 	 paddle.take(Tensor([3, 8467201],"float64"), Tensor([501, 8],"int64"), mode="clip", ) 	 25405611 	 172659 	 9.776723146438599 	 7.480628252029419 	 8.463859558105469e-05 	 0.00015401840209960938 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 16:54:09.129913 test begin: paddle.take(Tensor([6350401, 4],"float64"), Tensor([5, 8],"int64"), mode="wrap", )
[Prof] paddle.take 	 paddle.take(Tensor([6350401, 4],"float64"), Tensor([5, 8],"int64"), mode="wrap", ) 	 25401644 	 172659 	 24.941831350326538 	 12.36978530883789 	 6.29425048828125e-05 	 0.00014781951904296875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 16:55:13.009780 test begin: paddle.take(Tensor([6350401, 4],"float64"), Tensor([6350401, 8],"int64"), mode="clip", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff80b54ad10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 17:05:19.789790 test begin: paddle.take(Tensor([6350401, 4],"float64"), Tensor([6350401, 8],"int64"), mode="wrap", )
W0806 17:05:21.425583 124705 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd1e021b130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 17:15:24.673674 test begin: paddle.take_along_axis(Tensor([1024, 384],"float32"), Tensor([1024, 24807],"int64"), axis=-1, )
W0806 17:15:27.052091 150077 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([1024, 384],"float32"), Tensor([1024, 24807],"int64"), axis=-1, ) 	 25795584 	 32853 	 19.07583498954773 	 7.939600706100464 	 0.19750595092773438 	 0.24649333953857422 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:16:43.322457 test begin: paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 24807],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 24807],"int64"), axis=-1, ) 	 76206080 	 32853 	 32.97878336906433 	 14.479301929473877 	 0.3431239128112793 	 0.4498603343963623 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:18:13.309364 test begin: paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 7],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 7],"int64"), axis=-1, ) 	 50810880 	 32853 	 10.015916585922241 	 0.5513689517974854 	 0.10385894775390625 	 8.845329284667969e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:18:42.050555 test begin: paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 8],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 8],"int64"), axis=-1, ) 	 50811904 	 32853 	 10.016749620437622 	 0.5532753467559814 	 0.1038825511932373 	 8.726119995117188e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:19:09.750621 test begin: paddle.take_along_axis(Tensor([1051, 63, 768],"float32"), axis=1, indices=Tensor([1051, 7, 768],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([1051, 63, 768],"float32"), axis=1, indices=Tensor([1051, 7, 768],"int64"), ) 	 56501760 	 32853 	 18.16872239112854 	 10.065658807754517 	 0.1883397102355957 	 0.3129463195800781 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:20:03.576105 test begin: paddle.take_along_axis(Tensor([132301, 384],"float32"), Tensor([132301, 7],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([132301, 384],"float32"), Tensor([132301, 7],"int64"), axis=-1, ) 	 51729691 	 32853 	 11.978378057479858 	 1.905245304107666 	 0.12422585487365723 	 0.05926394462585449 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:20:40.381182 test begin: paddle.take_along_axis(Tensor([132301, 384],"float32"), Tensor([132301, 8],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([132301, 384],"float32"), Tensor([132301, 8],"int64"), axis=-1, ) 	 51861992 	 32853 	 12.213402032852173 	 2.087996482849121 	 0.12661528587341309 	 0.06428313255310059 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:21:15.519319 test begin: paddle.take_along_axis(Tensor([3175201, 384],"float32"), Tensor([3175201, 8],"int64"), axis=-1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f61c2c4e9e0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 17:31:23.140729 test begin: paddle.take_along_axis(Tensor([3628801, 384],"float32"), Tensor([3628801, 7],"int64"), axis=-1, )
W0806 17:31:47.004984 27866 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb905992c20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 17:41:27.950010 test begin: paddle.take_along_axis(Tensor([4726, 63, 768],"float32"), axis=1, indices=Tensor([4726, 7, 768],"int64"), )
W0806 17:41:34.911325 52925 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([4726, 63, 768],"float32"), axis=1, indices=Tensor([4726, 7, 768],"int64"), ) 	 254069760 	 32853 	 79.9174313545227 	 45.35551643371582 	 0.8284573554992676 	 1.412809133529663 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:45:29.571198 test begin: paddle.take_along_axis(Tensor([8, 63, 100801],"float32"), axis=1, indices=Tensor([8, 7, 100801],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([8, 63, 100801],"float32"), axis=1, indices=Tensor([8, 7, 100801],"int64"), ) 	 56448560 	 32853 	 18.786022186279297 	 10.141518831253052 	 0.19481372833251953 	 0.3159666061401367 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:46:31.636615 test begin: paddle.take_along_axis(Tensor([8, 63, 453601],"float32"), axis=1, indices=Tensor([8, 7, 453601],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([8, 63, 453601],"float32"), axis=1, indices=Tensor([8, 7, 453601],"int64"), ) 	 254016560 	 32853 	 99.8375473022461 	 52.205116987228394 	 1.0348658561706543 	 1.6225578784942627 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:52:26.647795 test begin: paddle.take_along_axis(Tensor([8, 63, 768],"float32"), axis=1, indices=Tensor([8, 4135, 768],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([8, 63, 768],"float32"), axis=1, indices=Tensor([8, 4135, 768],"int64"), ) 	 25792512 	 32853 	 19.738622426986694 	 9.359941005706787 	 0.20448660850524902 	 0.2923116683959961 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:53:20.023072 test begin: paddle.take_along_axis(Tensor([8, 8269, 768],"float32"), axis=1, indices=Tensor([8, 4135, 768],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([8, 8269, 768],"float32"), axis=1, indices=Tensor([8, 4135, 768],"int64"), ) 	 76210176 	 32853 	 44.410566091537476 	 31.265109300613403 	 0.4602651596069336 	 0.9717621803283691 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:55:31.636925 test begin: paddle.take_along_axis(Tensor([8, 8269, 768],"float32"), axis=1, indices=Tensor([8, 7, 768],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([8, 8269, 768],"float32"), axis=1, indices=Tensor([8, 7, 768],"int64"), ) 	 50847744 	 32853 	 10.10913610458374 	 0.5637640953063965 	 0.1062006950378418 	 8.106231689453125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:55:58.523144 test begin: paddle.tan(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.tan 	 paddle.tan(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 33898 	 10.004278182983398 	 10.101945638656616 	 0.30140113830566406 	 0.3044149875640869 	 15.313620567321777 	 35.286986112594604 	 0.4613761901855469 	 0.3544142246246338 	 
2025-08-06 17:57:12.576882 test begin: paddle.tan(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.tan 	 paddle.tan(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 33898 	 10.005112171173096 	 10.13846206665039 	 0.30143189430236816 	 0.3044400215148926 	 15.320801496505737 	 35.29172897338867 	 0.461529016494751 	 0.3543813228607178 	 
2025-08-06 17:58:27.391438 test begin: paddle.tan(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.tan 	 paddle.tan(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 33898 	 10.004688262939453 	 10.105975866317749 	 0.3014383316040039 	 0.3058931827545166 	 15.319356918334961 	 35.283018589019775 	 0.4615671634674072 	 0.35442256927490234 	 
2025-08-06 17:59:41.693608 test begin: paddle.tanh(Tensor([16, 125, 25500],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([16, 125, 25500],"float32"), ) 	 51000000 	 33846 	 10.052757024765015 	 10.128973007202148 	 0.30313658714294434 	 0.3055698871612549 	 15.304859161376953 	 15.183189868927002 	 0.4618096351623535 	 0.45943260192871094 	 
2025-08-06 18:00:34.773110 test begin: paddle.tanh(Tensor([16, 64, 49613],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([16, 64, 49613],"float32"), ) 	 50803712 	 33846 	 10.635971069335938 	 10.0851891040802 	 0.30187392234802246 	 0.3044142723083496 	 15.243438482284546 	 15.119918823242188 	 0.4598410129547119 	 0.4562492370605469 	 
2025-08-06 18:01:28.220107 test begin: paddle.tanh(Tensor([28, 32, 241, 241],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([28, 32, 241, 241],"float32"), ) 	 52040576 	 33846 	 10.252415895462036 	 10.331001281738281 	 0.30942440032958984 	 0.31176161766052246 	 15.616257190704346 	 15.487183570861816 	 0.4725608825683594 	 0.4673268795013428 	 
2025-08-06 18:02:22.864363 test begin: paddle.tanh(Tensor([32, 64, 25500],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([32, 64, 25500],"float32"), ) 	 52224000 	 33846 	 10.36861801147461 	 10.366623163223267 	 0.31063389778137207 	 0.31284594535827637 	 15.666771411895752 	 15.540632247924805 	 0.4727051258087158 	 0.47047972679138184 	 
2025-08-06 18:03:19.641553 test begin: paddle.tanh(Tensor([64, 26, 512, 1, 60],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([64, 26, 512, 1, 60],"float32"), ) 	 51118080 	 33846 	 10.075518608093262 	 10.151150465011597 	 0.3040440082550049 	 0.3062601089477539 	 15.337736129760742 	 15.213579177856445 	 0.4656703472137451 	 0.45903992652893066 	 
2025-08-06 18:04:12.670465 test begin: paddle.tanh(Tensor([64, 26, 512, 2, 40],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([64, 26, 512, 2, 40],"float32"), ) 	 68157440 	 33846 	 13.395612955093384 	 13.472712278366089 	 0.4039320945739746 	 0.40647339820861816 	 20.391834497451782 	 20.23691201210022 	 0.6152553558349609 	 0.6104676723480225 	 
2025-08-06 18:05:22.567578 test begin: paddle.tanh(Tensor([64, 26, 764, 1, 40],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([64, 26, 764, 1, 40],"float32"), ) 	 50851840 	 33846 	 10.027646541595459 	 10.566289186477661 	 0.3025083541870117 	 0.30491113662719727 	 15.25495982170105 	 15.13657021522522 	 0.4616734981536865 	 0.45669007301330566 	 
2025-08-06 18:06:16.699168 test begin: paddle.tanh(Tensor([64, 39, 512, 1, 40],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([64, 39, 512, 1, 40],"float32"), ) 	 51118080 	 33846 	 10.071508646011353 	 10.151346683502197 	 0.3038802146911621 	 0.3062877655029297 	 15.336699485778809 	 15.215830326080322 	 0.4627983570098877 	 0.4590604305267334 	 
2025-08-06 18:07:09.205990 test begin: paddle.tanh(Tensor([8, 110, 241, 241],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([8, 110, 241, 241],"float32"), ) 	 51111280 	 33846 	 10.057063579559326 	 10.166728734970093 	 0.3033945560455322 	 0.3063347339630127 	 15.338892459869385 	 15.210132122039795 	 0.4627647399902344 	 0.4590175151824951 	 
2025-08-06 18:08:04.263737 test begin: paddle.tanh(Tensor([8, 32, 241, 824],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([8, 32, 241, 824],"float32"), ) 	 50837504 	 33846 	 10.023817300796509 	 10.094015836715698 	 0.3023800849914551 	 0.3047630786895752 	 15.257708311080933 	 15.136599779129028 	 0.461712121963501 	 0.4580070972442627 	 
2025-08-06 18:08:56.517742 test begin: paddle.tanh(Tensor([8, 32, 824, 241],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([8, 32, 824, 241],"float32"), ) 	 50837504 	 33846 	 10.019972085952759 	 10.098544120788574 	 0.3023874759674072 	 0.3047020435333252 	 15.256391525268555 	 15.12981915473938 	 0.46027255058288574 	 0.45658349990844727 	 
2025-08-06 18:09:48.728317 test begin: paddle.tanh(Tensor([96, 26, 512, 1, 40],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([96, 26, 512, 1, 40],"float32"), ) 	 51118080 	 33846 	 10.07137393951416 	 10.1612069606781 	 0.3038451671600342 	 0.30634117126464844 	 15.337881565093994 	 15.209761619567871 	 0.46281909942626953 	 0.4590473175048828 	 
2025-08-06 18:10:41.601209 test begin: paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), list[2,3,], axis=3, )
W0806 18:11:01.317790 125993 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), list[2,3,], axis=3, ) 	 254017120 	 428549 	 12.493714094161987 	 3.4887592792510986 	 0.00011467933654785156 	 8.58306884765625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 18:11:10.912112 test begin: paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), list[2,4,6,], axis=3, )
W0806 18:11:32.066424 127269 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), list[2,4,6,], axis=3, ) 	 254017120 	 428549 	 14.07714033126831 	 4.09947657585144 	 0.000118255615234375 	 0.00011348724365234375 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 18:11:39.999178 test begin: paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), tuple(2,6,), axis=3, )
W0806 18:11:57.769186 128462 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([2268010, 4, 4, 7],"int64"), tuple(2,6,), axis=3, ) 	 254017120 	 428549 	 10.746349573135376 	 3.508866786956787 	 0.0001239776611328125 	 9.679794311523438e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 18:12:03.144283 test begin: paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), list[2,3,], axis=3, )
W0806 18:12:20.623569 129576 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), list[2,3,], axis=3, ) 	 254017120 	 428549 	 10.520809650421143 	 3.5257022380828857 	 9.489059448242188e-05 	 9.393692016601562e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 18:12:28.038361 test begin: paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), list[2,4,6,], axis=3, )
W0806 18:12:48.942457 130704 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), list[2,4,6,], axis=3, ) 	 254017120 	 428549 	 13.823854207992554 	 6.314279556274414 	 0.0001125335693359375 	 0.00013446807861328125 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 18:12:56.743058 test begin: paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), tuple(2,6,), axis=3, )
W0806 18:13:14.475503 131827 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 226801, 4, 7],"int64"), tuple(2,6,), axis=3, ) 	 254017120 	 428549 	 10.712631225585938 	 3.489206552505493 	 0.00011873245239257812 	 8.726119995117188e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 18:13:19.644732 test begin: paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), list[2,3,], axis=3, )
W0806 18:13:37.471897 132945 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), list[2,3,], axis=3, ) 	 254017120 	 428549 	 10.547825574874878 	 3.489551067352295 	 8.58306884765625e-05 	 9.751319885253906e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 18:13:42.629192 test begin: paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), list[2,4,6,], axis=3, )
W0806 18:14:03.312122 133830 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), list[2,4,6,], axis=3, ) 	 254017120 	 428549 	 13.692477941513062 	 4.204115867614746 	 0.0001232624053955078 	 8.869171142578125e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 18:14:09.325221 test begin: paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), tuple(2,6,), axis=3, )
W0806 18:14:27.014585 135169 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 226801, 7],"int64"), tuple(2,6,), axis=3, ) 	 254017120 	 428549 	 10.739508867263794 	 3.571425199508667 	 0.00011157989501953125 	 9.274482727050781e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 18:14:32.290980 test begin: paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), list[2,3,], axis=3, )
W0806 18:14:50.014848 136290 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), list[2,3,], axis=3, ) 	 254016640 	 428549 	 10.708280324935913 	 3.503884792327881 	 0.00010704994201660156 	 0.00026535987854003906 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 18:14:55.100987 test begin: paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), list[2,4,6,], axis=3, )
W0806 18:15:15.919196 136967 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), list[2,4,6,], axis=3, ) 	 254016640 	 428549 	 13.88951325416565 	 4.163506507873535 	 0.00011444091796875 	 0.0002372264862060547 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 18:15:21.568742 test begin: paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), tuple(2,6,), axis=3, )
W0806 18:15:42.044533 138310 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.tensor_split 	 paddle.tensor_split(Tensor([40, 4, 4, 396901],"int64"), tuple(2,6,), axis=3, ) 	 254016640 	 428549 	 13.220952987670898 	 3.5020954608917236 	 0.00011301040649414062 	 8.320808410644531e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-05 21:37:57.093461 test begin: paddle.Tensor.__or__(Tensor([4, 210, 75600],"bool"), Tensor([1, 210, 75600],"bool"), )
W0805 21:37:58.543613 108934 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([4, 210, 75600],"bool"), Tensor([1, 210, 75600],"bool"), ) 	 79380000 	 85484 	 17.95298409461975 	 23.735048055648804 	 0.2146298885345459 	 0.28377199172973633 	 None 	 None 	 None 	 None 	 
2025-08-05 21:38:44.384589 test begin: paddle.Tensor.__or__(Tensor([4, 210, 75600],"bool"), Tensor([4, 210, 75600],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([4, 210, 75600],"bool"), Tensor([4, 210, 75600],"bool"), ) 	 127008000 	 85484 	 12.617435455322266 	 12.237100601196289 	 0.1480393409729004 	 0.1462845802307129 	 None 	 None 	 None 	 None 	 
2025-08-05 21:39:13.684946 test begin: paddle.Tensor.__or__(Tensor([4, 218, 70644],"bool"), Tensor([1, 218, 70644],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([4, 218, 70644],"bool"), Tensor([1, 218, 70644],"bool"), ) 	 77001960 	 85484 	 17.49419379234314 	 22.985816717147827 	 0.20914435386657715 	 0.2747781276702881 	 None 	 None 	 None 	 None 	 
2025-08-05 21:39:59.612500 test begin: paddle.Tensor.__or__(Tensor([4, 218, 70644],"bool"), Tensor([4, 218, 70644],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([4, 218, 70644],"bool"), Tensor([4, 218, 70644],"bool"), ) 	 123203136 	 85484 	 12.071326494216919 	 11.978756189346313 	 0.14434361457824707 	 0.14316320419311523 	 None 	 None 	 None 	 None 	 
2025-08-05 21:40:25.552790 test begin: paddle.Tensor.__pow__(Tensor([23, 17, 256, 256],"float64"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([23, 17, 256, 256],"float64"), 2, ) 	 25624576 	 27052 	 15.621524810791016 	 8.131186962127686 	 0.5900821685791016 	 0.307112455368042 	 16.655672311782837 	 28.669824600219727 	 0.6301305294036865 	 0.36099934577941895 	 
2025-08-05 21:41:40.039524 test begin: paddle.Tensor.__pow__(Tensor([24, 17, 244, 256],"float64"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([24, 17, 244, 256],"float64"), 2, ) 	 25485312 	 27052 	 16.918373107910156 	 8.108500719070435 	 0.5868501663208008 	 0.3055107593536377 	 16.396337747573853 	 28.516947507858276 	 0.6194398403167725 	 0.3591463565826416 	 
2025-08-05 21:42:52.952496 test begin: paddle.Tensor.__pow__(Tensor([24, 17, 256, 244],"float64"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([24, 17, 256, 244],"float64"), 2, ) 	 25485312 	 27052 	 15.53908371925354 	 8.094060182571411 	 0.5871529579162598 	 0.3055245876312256 	 16.39766526222229 	 28.516610383987427 	 0.6195163726806641 	 0.3591766357421875 	 
2025-08-05 21:44:03.121486 test begin: paddle.Tensor.__pow__(Tensor([24, 17, 256, 256],"float64"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([24, 17, 256, 256],"float64"), 2, ) 	 26738688 	 27052 	 16.296967029571533 	 8.511647701263428 	 0.6156919002532959 	 0.32041478157043457 	 17.230749368667603 	 29.915061712265015 	 0.6525857448577881 	 0.37676239013671875 	 
2025-08-05 21:45:19.437500 test begin: paddle.Tensor.__pow__(Tensor([259, 3, 256, 256],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([259, 3, 256, 256],"float32"), 2, ) 	 50921472 	 27052 	 9.999507427215576 	 8.077675819396973 	 0.37791895866394043 	 0.3048405647277832 	 12.260486364364624 	 28.528153896331787 	 0.46303534507751465 	 0.35932135581970215 	 
2025-08-05 21:46:21.665688 test begin: paddle.Tensor.__pow__(Tensor([28, 32, 241, 241],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([28, 32, 241, 241],"float32"), 2, ) 	 52040576 	 27052 	 10.219468116760254 	 8.241429328918457 	 0.3860495090484619 	 0.31136012077331543 	 12.519429206848145 	 29.22593641281128 	 0.4730527400970459 	 0.2762258052825928 	 
2025-08-05 21:47:23.684943 test begin: paddle.Tensor.__pow__(Tensor([64, 13, 256, 256],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([64, 13, 256, 256],"float32"), 2, ) 	 54525952 	 27052 	 10.698564767837524 	 8.635804414749146 	 0.404160737991333 	 0.3260059356689453 	 13.112678527832031 	 30.53027606010437 	 0.4952876567840576 	 0.38448667526245117 	 
2025-08-05 21:48:29.346043 test begin: paddle.Tensor.__pow__(Tensor([64, 3, 1034, 256],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([64, 3, 1034, 256],"float32"), 2, ) 	 50823168 	 27052 	 9.983748435974121 	 8.05704927444458 	 0.37713003158569336 	 0.30414462089538574 	 12.236826658248901 	 28.465478658676147 	 0.4622764587402344 	 0.3585085868835449 	 
2025-08-05 21:49:30.456945 test begin: paddle.Tensor.__pow__(Tensor([64, 3, 256, 1034],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([64, 3, 256, 1034],"float32"), 2, ) 	 50823168 	 27052 	 9.983710527420044 	 8.052365779876709 	 0.37717747688293457 	 0.30421876907348633 	 12.237303018569946 	 28.46498155593872 	 0.4622344970703125 	 0.35843920707702637 	 
2025-08-05 21:50:32.366884 test begin: paddle.Tensor.__pow__(Tensor([8, 110, 241, 241],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([8, 110, 241, 241],"float32"), 2, ) 	 51111280 	 27052 	 11.370703220367432 	 8.099476337432861 	 0.3792548179626465 	 0.30600571632385254 	 12.303218126296997 	 28.71269154548645 	 0.4647054672241211 	 0.27134251594543457 	 
2025-08-05 21:51:38.685456 test begin: paddle.Tensor.__pow__(Tensor([8, 32, 241, 824],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([8, 32, 241, 824],"float32"), 2, ) 	 50837504 	 27052 	 10.505043506622314 	 8.054720640182495 	 0.37716221809387207 	 0.3043675422668457 	 12.24542236328125 	 28.473135232925415 	 0.4625875949859619 	 0.3585999011993408 	 
2025-08-05 21:52:44.080583 test begin: paddle.Tensor.__pow__(Tensor([8, 32, 824, 241],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([8, 32, 824, 241],"float32"), 2, ) 	 50837504 	 27052 	 10.000111818313599 	 8.054927587509155 	 0.3779745101928711 	 0.304293155670166 	 12.234601974487305 	 28.47349524497986 	 0.4620213508605957 	 0.35853075981140137 	 
2025-08-05 21:53:45.781640 test begin: paddle.Tensor.__radd__(Tensor([192, 104, 32, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 104, 32, 160],"float16"), 0, ) 	 102236160 	 33511 	 10.050661563873291 	 9.979604721069336 	 0.3064908981323242 	 0.3043632507324219 	 10.043102025985718 	 1.6518232822418213 	 0.30628442764282227 	 7.700920104980469e-05 	 
2025-08-05 21:54:21.353868 test begin: paddle.Tensor.__radd__(Tensor([192, 128, 16, 259],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 128, 16, 259],"float16"), 0, ) 	 101842944 	 33511 	 10.00823450088501 	 9.9497389793396 	 0.30515623092651367 	 0.3031802177429199 	 10.007797479629517 	 1.7778334617614746 	 0.3051881790161133 	 8.20159912109375e-05 	 
2025-08-05 21:54:57.073288 test begin: paddle.Tensor.__radd__(Tensor([192, 128, 26, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 128, 26, 160],"float16"), 0, ) 	 102236160 	 33511 	 10.050461769104004 	 9.980091333389282 	 0.3065042495727539 	 0.30437302589416504 	 10.043419599533081 	 1.7239325046539307 	 0.3062417507171631 	 6.604194641113281e-05 	 
2025-08-05 21:55:33.161273 test begin: paddle.Tensor.__radd__(Tensor([192, 207, 16, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 207, 16, 160],"float16"), 0, ) 	 101744640 	 33511 	 9.997459411621094 	 9.93038558959961 	 0.3049147129058838 	 0.302807092666626 	 9.998602628707886 	 1.7014119625091553 	 0.3049430847167969 	 0.0001838207244873047 	 
2025-08-05 21:56:08.563228 test begin: paddle.Tensor.__radd__(Tensor([192, 240, 16, 138],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 240, 16, 138],"float16"), 0, ) 	 101744640 	 33511 	 9.996921300888062 	 9.93014669418335 	 0.3049013614654541 	 0.3028085231781006 	 9.998112916946411 	 1.7508196830749512 	 0.30487537384033203 	 8.916854858398438e-05 	 
2025-08-05 21:56:44.522328 test begin: paddle.Tensor.__radd__(Tensor([192, 240, 28, 80],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 240, 28, 80],"float16"), 0, ) 	 103219200 	 33511 	 10.140013456344604 	 10.071120023727417 	 0.3092479705810547 	 0.30713891983032227 	 10.136613845825195 	 1.7361280918121338 	 0.30914831161499023 	 7.486343383789062e-05 	 
2025-08-05 21:57:20.389826 test begin: paddle.Tensor.__radd__(Tensor([192, 414, 16, 80],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 414, 16, 80],"float16"), 0, ) 	 101744640 	 33511 	 9.996678113937378 	 9.936450719833374 	 0.3048670291900635 	 0.3028695583343506 	 9.998207807540894 	 1.6377015113830566 	 0.3048980236053467 	 9.942054748535156e-05 	 
2025-08-05 21:57:56.547436 test begin: paddle.Tensor.__radd__(Tensor([192, 64, 32, 259],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 64, 32, 259],"float16"), 0, ) 	 101842944 	 33511 	 10.006113767623901 	 9.939601421356201 	 0.30515551567077637 	 0.30316972732543945 	 10.006859540939331 	 1.6965599060058594 	 0.30520057678222656 	 8.606910705566406e-05 	 
2025-08-05 21:58:31.967015 test begin: paddle.Tensor.__radd__(Tensor([192, 64, 52, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 64, 52, 160],"float16"), 0, ) 	 102236160 	 33511 	 10.050271987915039 	 9.979796171188354 	 0.30652451515197754 	 0.3043389320373535 	 10.042989492416382 	 1.6554346084594727 	 0.3062615394592285 	 7.891654968261719e-05 	 
2025-08-05 21:59:08.574754 test begin: paddle.Tensor.__radd__(Tensor([311, 128, 16, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([311, 128, 16, 160],"float16"), 0, ) 	 101908480 	 33511 	 10.01338267326355 	 9.94580626487732 	 0.305356502532959 	 0.3033406734466553 	 10.012810707092285 	 1.7082269191741943 	 0.3053557872772217 	 7.867813110351562e-05 	 
2025-08-05 21:59:44.051303 test begin: paddle.Tensor.__radd__(Tensor([311, 64, 32, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([311, 64, 32, 160],"float16"), 0, ) 	 101908480 	 33511 	 10.012555599212646 	 9.94608473777771 	 0.3053891658782959 	 0.3032722473144531 	 10.013054370880127 	 1.6503040790557861 	 0.3053615093231201 	 8.106231689453125e-05 	 
2025-08-05 22:00:20.082090 test begin: paddle.Tensor.__radd__(Tensor([331, 240, 16, 80],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([331, 240, 16, 80],"float16"), 0, ) 	 101683200 	 33511 	 9.990857362747192 	 9.939255952835083 	 0.30471062660217285 	 0.30266237258911133 	 9.992010354995728 	 1.6790764331817627 	 0.30468320846557617 	 7.534027099609375e-05 	 
2025-08-05 22:00:55.434631 test begin: paddle.Tensor.__rlshift__(Tensor([169345, 300],"int32"), -223, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([169345, 300],"int32"), -223, ) 	 50803500 	 33460 	 9.98717999458313 	 11.54006052017212 	 0.15249967575073242 	 0.3041067123413086 	 None 	 None 	 None 	 None 	 
2025-08-05 22:01:20.257097 test begin: paddle.Tensor.__rlshift__(Tensor([200, 254017],"int32"), -223, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([200, 254017],"int32"), -223, ) 	 50803400 	 33460 	 9.993024349212646 	 9.957186222076416 	 0.1526041030883789 	 0.3041539192199707 	 None 	 None 	 None 	 None 	 
2025-08-05 22:01:43.574376 test begin: paddle.Tensor.__rlshift__(Tensor([200, 508033],"int16"), -212, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([200, 508033],"int16"), -212, ) 	 101606600 	 33460 	 11.299302816390991 	 9.861114263534546 	 0.00029587745666503906 	 0.30119800567626953 	 None 	 None 	 None 	 None 	 
2025-08-05 22:02:05.745653 test begin: paddle.Tensor.__rlshift__(Tensor([200, 508033],"int16"), 63, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([200, 508033],"int16"), 63, ) 	 101606600 	 33460 	 11.137824058532715 	 9.872655868530273 	 0.0003006458282470703 	 0.30117177963256836 	 None 	 None 	 None 	 None 	 
2025-08-05 22:02:28.301262 test begin: paddle.Tensor.__rlshift__(Tensor([338689, 300],"int16"), -212, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([338689, 300],"int16"), -212, ) 	 101606700 	 33460 	 11.1966073513031 	 10.195362091064453 	 0.0002651214599609375 	 0.301206111907959 	 None 	 None 	 None 	 None 	 
2025-08-05 22:02:52.250686 test begin: paddle.Tensor.__rlshift__(Tensor([338689, 300],"int16"), 63, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([338689, 300],"int16"), 63, ) 	 101606700 	 33460 	 11.314849376678467 	 9.860933542251587 	 0.0002932548522949219 	 0.30117249488830566 	 None 	 None 	 None 	 None 	 
2025-08-05 22:03:14.441561 test begin: paddle.Tensor.__rmatmul__(Tensor([10160641, 5],"float32"), Tensor([2, 10160641],"float32"), )
[Prof] paddle.Tensor.__rmatmul__ 	 paddle.Tensor.__rmatmul__(Tensor([10160641, 5],"float32"), Tensor([2, 10160641],"float32"), ) 	 71124487 	 13088 	 18.146032571792603 	 18.10729479789734 	 0.7084450721740723 	 0.7069423198699951 	 27.408854722976685 	 27.395844221115112 	 0.10697293281555176 	 0.10692310333251953 	 
2025-08-05 22:04:46.811635 test begin: paddle.Tensor.__rmatmul__(Tensor([25401601, 5],"float32"), Tensor([2, 25401601],"float32"), )
[Prof] paddle.Tensor.__rmatmul__ 	 paddle.Tensor.__rmatmul__(Tensor([25401601, 5],"float32"), Tensor([2, 25401601],"float32"), ) 	 177811207 	 13088 	 44.513139963150024 	 44.418760538101196 	 1.7378926277160645 	 1.7341997623443604 	 68.51785087585449 	 68.49556112289429 	 0.1069486141204834 	 0.10692000389099121 	 
2025-08-05 22:08:38.715272 test begin: paddle.Tensor.__rmatmul__(Tensor([3, 16934401],"float32"), Tensor([2, 3],"float32"), )
[Prof] paddle.Tensor.__rmatmul__ 	 paddle.Tensor.__rmatmul__(Tensor([3, 16934401],"float32"), Tensor([2, 3],"float32"), ) 	 50803209 	 13088 	 10.726083040237427 	 10.001956939697266 	 0.04581952095031738 	 0.04591178894042969 	 53.65529155731201 	 53.606945514678955 	 0.21874260902404785 	 0.22074484825134277 	 
2025-08-05 22:10:49.125312 test begin: paddle.Tensor.__rmatmul__(Tensor([3, 5],"float32"), Tensor([16934401, 3],"float32"), )
[Prof] paddle.Tensor.__rmatmul__ 	 paddle.Tensor.__rmatmul__(Tensor([3, 5],"float32"), Tensor([16934401, 3],"float32"), ) 	 50803218 	 13088 	 23.35590171813965 	 23.359758138656616 	 0.10716128349304199 	 0.1071782112121582 	 54.764551401138306 	 54.73974657058716 	 0.22550010681152344 	 0.22324919700622559 	 
2025-08-05 22:13:27.592882 test begin: paddle.Tensor.__rmod__(Tensor([2, 3, 8467201],"float32"), Tensor([2, 3, 8467201],"float32"), )
[Prof] paddle.Tensor.__rmod__ 	 paddle.Tensor.__rmod__(Tensor([2, 3, 8467201],"float32"), Tensor([2, 3, 8467201],"float32"), ) 	 101606412 	 22183 	 9.994361639022827 	 9.959264278411865 	 0.4604949951171875 	 0.45879292488098145 	 24.74934673309326 	 26.544344663619995 	 1.1401758193969727 	 0.40756773948669434 	 
2025-08-05 22:14:43.394457 test begin: paddle.Tensor.__rmod__(Tensor([2, 6350401, 4],"float32"), Tensor([2, 6350401, 4],"float32"), )
[Prof] paddle.Tensor.__rmod__ 	 paddle.Tensor.__rmod__(Tensor([2, 6350401, 4],"float32"), Tensor([2, 6350401, 4],"float32"), ) 	 101606416 	 22183 	 9.998101711273193 	 9.963371753692627 	 0.4606199264526367 	 0.4588150978088379 	 24.750202894210815 	 26.54422116279602 	 1.140279769897461 	 0.4075798988342285 	 
2025-08-05 22:15:58.616607 test begin: paddle.Tensor.__rmod__(Tensor([4233601, 3, 4],"float32"), Tensor([4233601, 3, 4],"float32"), )
[Prof] paddle.Tensor.__rmod__ 	 paddle.Tensor.__rmod__(Tensor([4233601, 3, 4],"float32"), Tensor([4233601, 3, 4],"float32"), ) 	 101606424 	 22183 	 9.993459701538086 	 10.297733068466187 	 0.46031951904296875 	 0.4587433338165283 	 24.764309406280518 	 26.54329252243042 	 1.1532535552978516 	 0.4075291156768799 	 
2025-08-05 22:17:15.802175 test begin: paddle.Tensor.__rmul__(Tensor([176, 392, 737],"float32"), -100.0, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([176, 392, 737],"float32"), -100.0, ) 	 50847104 	 33795 	 10.017208099365234 	 10.078419208526611 	 0.30270934104919434 	 0.3043496608734131 	 10.012067317962646 	 10.061948776245117 	 0.3027775287628174 	 0.30425405502319336 	 
2025-08-05 22:17:58.750300 test begin: paddle.Tensor.__rmul__(Tensor([176, 737, 392],"float32"), -100.0, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([176, 737, 392],"float32"), -100.0, ) 	 50847104 	 33795 	 10.009164094924927 	 10.065029382705688 	 0.30272579193115234 	 0.30442309379577637 	 10.01202392578125 	 10.061794519424438 	 0.30278801918029785 	 0.30426597595214844 	 
2025-08-05 22:18:42.906136 test begin: paddle.Tensor.__rmul__(Tensor([324000, 157],"float32"), 0.75, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([324000, 157],"float32"), 0.75, ) 	 50868000 	 33795 	 10.022665023803711 	 10.071276903152466 	 0.30286645889282227 	 0.30456972122192383 	 10.015621900558472 	 10.067713975906372 	 0.30289387702941895 	 0.30445384979248047 	 
2025-08-05 22:19:24.806452 test begin: paddle.Tensor.__rmul__(Tensor([324000, 157],"float32"), 1.0, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([324000, 157],"float32"), 1.0, ) 	 50868000 	 33795 	 10.016249418258667 	 10.078899145126343 	 0.3029024600982666 	 0.30454039573669434 	 10.016046524047852 	 10.06804347038269 	 0.302903413772583 	 0.30439305305480957 	 
2025-08-05 22:20:07.716872 test begin: paddle.Tensor.__rmul__(Tensor([331, 392, 392],"float32"), -100.0, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([331, 392, 392],"float32"), -100.0, ) 	 50862784 	 33795 	 9.99806261062622 	 10.070419788360596 	 0.30232882499694824 	 0.30452537536621094 	 10.01449704170227 	 10.066681385040283 	 0.30286550521850586 	 0.3043787479400635 	 
2025-08-05 22:20:50.750280 test begin: paddle.Tensor.__rmul__(Tensor([635041, 80],"float32"), 0.75, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([635041, 80],"float32"), 0.75, ) 	 50803280 	 33795 	 10.615118026733398 	 10.060954093933105 	 0.3021092414855957 	 0.3041393756866455 	 10.000176906585693 	 10.054199695587158 	 0.3024115562438965 	 0.3040790557861328 	 
2025-08-05 22:21:34.093086 test begin: paddle.Tensor.__rmul__(Tensor([635041, 80],"float32"), 1.0, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([635041, 80],"float32"), 1.0, ) 	 50803280 	 33795 	 9.991302728652954 	 12.491949558258057 	 0.30212926864624023 	 0.30411839485168457 	 9.999963521957397 	 10.05547833442688 	 0.3023808002471924 	 0.3040595054626465 	 
2025-08-05 22:22:19.375797 test begin: paddle.Tensor.__ror__(Tensor([2, 3, 8467201],"int32"), 5, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([2, 3, 8467201],"int32"), 5, ) 	 50803206 	 33465 	 9.987087965011597 	 9.957006454467773 	 0.15251612663269043 	 0.30413079261779785 	 None 	 None 	 None 	 None 	 
2025-08-05 22:22:43.280285 test begin: paddle.Tensor.__ror__(Tensor([2, 3, 8467201],"int32"), True, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([2, 3, 8467201],"int32"), True, ) 	 50803206 	 33465 	 9.990638971328735 	 9.96753740310669 	 0.1525287628173828 	 0.30405521392822266 	 None 	 None 	 None 	 None 	 
2025-08-05 22:23:03.858419 test begin: paddle.Tensor.__ror__(Tensor([2, 5080321, 5],"int32"), 5, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([2, 5080321, 5],"int32"), 5, ) 	 50803210 	 33465 	 9.987207412719727 	 9.968221664428711 	 0.15249037742614746 	 0.3040645122528076 	 None 	 None 	 None 	 None 	 
2025-08-05 22:23:24.362259 test begin: paddle.Tensor.__ror__(Tensor([2, 5080321, 5],"int32"), True, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([2, 5080321, 5],"int32"), True, ) 	 50803210 	 33465 	 9.98713493347168 	 9.95729374885559 	 0.15251445770263672 	 0.3040940761566162 	 None 	 None 	 None 	 None 	 
2025-08-05 22:23:44.865060 test begin: paddle.Tensor.__ror__(Tensor([3386881, 3, 5],"int32"), 5, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([3386881, 3, 5],"int32"), 5, ) 	 50803215 	 33465 	 9.986991167068481 	 9.957161903381348 	 0.1525411605834961 	 0.30407071113586426 	 None 	 None 	 None 	 None 	 
2025-08-05 22:24:05.353290 test begin: paddle.Tensor.__ror__(Tensor([3386881, 3, 5],"int32"), True, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([3386881, 3, 5],"int32"), True, ) 	 50803215 	 33465 	 9.987322330474854 	 10.287990808486938 	 0.1525118350982666 	 0.30406928062438965 	 None 	 None 	 None 	 None 	 
2025-08-05 22:24:27.584383 test begin: paddle.Tensor.__rpow__(Tensor([50803201],"float32"), 10000, )
[Prof] paddle.Tensor.__rpow__ 	 paddle.Tensor.__rpow__(Tensor([50803201],"float32"), 10000, ) 	 50803201 	 17155 	 9.997929096221924 	 10.808035850524902 	 0.29790496826171875 	 0.32195591926574707 	 12.016582012176514 	 12.741039991378784 	 0.7158670425415039 	 0.37952542304992676 	 
2025-08-05 22:25:14.923345 test begin: paddle.Tensor.__rpow__(Tensor([50803201],"float32"), 10000.0, )
[Prof] paddle.Tensor.__rpow__ 	 paddle.Tensor.__rpow__(Tensor([50803201],"float32"), 10000.0, ) 	 50803201 	 17155 	 9.999758243560791 	 11.053004741668701 	 0.29833078384399414 	 0.3290889263153076 	 12.011884689331055 	 12.741076469421387 	 0.7155804634094238 	 0.37947988510131836 	 
2025-08-05 22:26:02.673534 test begin: paddle.Tensor.__rrshift__(Tensor([169345, 300],"int32"), 232, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([169345, 300],"int32"), 232, ) 	 50803500 	 33481 	 9.989187240600586 	 9.961469173431396 	 0.1523885726928711 	 0.3040497303009033 	 None 	 None 	 None 	 None 	 
2025-08-05 22:26:23.181305 test begin: paddle.Tensor.__rrshift__(Tensor([200, 254017],"int32"), 232, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([200, 254017],"int32"), 232, ) 	 50803400 	 33481 	 9.98485279083252 	 9.969535112380981 	 0.15239286422729492 	 0.3041064739227295 	 None 	 None 	 None 	 None 	 
2025-08-05 22:26:44.237252 test begin: paddle.Tensor.__rrshift__(Tensor([200, 508033],"int16"), -255, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([200, 508033],"int16"), -255, ) 	 101606600 	 33481 	 11.256948232650757 	 9.902610063552856 	 0.0003120899200439453 	 0.30222558975219727 	 None 	 None 	 None 	 None 	 
2025-08-05 22:27:06.393047 test begin: paddle.Tensor.__rrshift__(Tensor([200, 508033],"int16"), 11, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([200, 508033],"int16"), 11, ) 	 101606600 	 33481 	 11.249443054199219 	 9.902291536331177 	 0.0003077983856201172 	 0.30225229263305664 	 None 	 None 	 None 	 None 	 
2025-08-05 22:27:28.538541 test begin: paddle.Tensor.__rrshift__(Tensor([338689, 300],"int16"), -255, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([338689, 300],"int16"), -255, ) 	 101606700 	 33481 	 11.243215322494507 	 9.915404796600342 	 0.0003070831298828125 	 0.30222272872924805 	 None 	 None 	 None 	 None 	 
2025-08-05 22:27:51.609553 test begin: paddle.Tensor.__rrshift__(Tensor([338689, 300],"int16"), 11, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([338689, 300],"int16"), 11, ) 	 101606700 	 33481 	 11.26458215713501 	 9.902446508407593 	 0.00030875205993652344 	 0.3022911548614502 	 None 	 None 	 None 	 None 	 
2025-08-05 22:28:13.781539 test begin: paddle.Tensor.__rshift__(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), ) 	 101607000 	 22342 	 10.060636520385742 	 9.974494695663452 	 0.46026039123535156 	 0.4562647342681885 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:28:34.947492 test begin: paddle.Tensor.__rshift__(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), ) 	 101606800 	 22342 	 10.395848035812378 	 9.975042581558228 	 0.46011853218078613 	 0.45630669593811035 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:28:59.228049 test begin: paddle.Tensor.__rshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), ) 	 203213200 	 22342 	 9.994404315948486 	 10.051057815551758 	 0.45723986625671387 	 0.45979952812194824 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:29:21.284756 test begin: paddle.Tensor.__rshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, ) 	 203213200 	 22342 	 10.009648323059082 	 69.54086256027222 	 0.4578824043273926 	 0.3536415100097656 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:30:43.095310 test begin: paddle.Tensor.__rshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), ) 	 203213400 	 22342 	 10.002041578292847 	 10.050424814224243 	 0.4572722911834717 	 0.45974159240722656 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:31:05.035327 test begin: paddle.Tensor.__rshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, ) 	 203213400 	 22342 	 10.027908325195312 	 69.54761123657227 	 0.45841026306152344 	 0.35362935066223145 	 None 	 None 	 None 	 None 	 combined
2025-08-05 22:32:28.295614 test begin: paddle.Tensor.__rsub__(Tensor([2, 1, 12404, 4096],"float16"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([2, 1, 12404, 4096],"float16"), 1, ) 	 101613568 	 33801 	 10.071746349334717 	 10.003649473190308 	 0.3044877052307129 	 0.3024449348449707 	 10.07445764541626 	 10.003448247909546 	 0.3045840263366699 	 0.30244922637939453 	 
2025-08-05 22:33:12.472662 test begin: paddle.Tensor.__rsub__(Tensor([2, 1, 4096, 12404],"float16"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([2, 1, 4096, 12404],"float16"), 1, ) 	 101613568 	 33801 	 10.074605703353882 	 10.016700744628906 	 0.304518461227417 	 0.3025319576263428 	 10.077126741409302 	 10.003658056259155 	 0.30469703674316406 	 0.302473783493042 	 
2025-08-05 22:33:56.391620 test begin: paddle.Tensor.__rsub__(Tensor([2, 4, 4096, 4096],"float16"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([2, 4, 4096, 4096],"float16"), 1, ) 	 134217728 	 33801 	 13.258257150650024 	 13.174365043640137 	 0.400881290435791 	 0.3983588218688965 	 13.25633716583252 	 13.176336526870728 	 0.40079593658447266 	 0.39835572242736816 	 
2025-08-05 22:34:54.311886 test begin: paddle.Tensor.__rsub__(Tensor([2944, 17257],"float32"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([2944, 17257],"float32"), 1, ) 	 50804608 	 33801 	 9.993529319763184 	 10.059270143508911 	 0.30210018157958984 	 0.3041675090789795 	 10.001698732376099 	 10.055542230606079 	 0.3023531436920166 	 0.304088830947876 	 
2025-08-05 22:35:38.259743 test begin: paddle.Tensor.__rsub__(Tensor([4224, 12028],"float32"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([4224, 12028],"float32"), 1, ) 	 50806272 	 33801 	 11.11792802810669 	 10.067007064819336 	 0.3023662567138672 	 0.3041565418243408 	 10.002203226089478 	 10.056572437286377 	 0.30242395401000977 	 0.3040120601654053 	 
2025-08-05 22:36:21.998166 test begin: paddle.Tensor.__rsub__(Tensor([7, 1, 4096, 4096],"float16"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([7, 1, 4096, 4096],"float16"), 1, ) 	 117440512 	 33801 	 11.611749172210693 	 11.540220975875854 	 0.3510606288909912 	 0.34894347190856934 	 11.612853050231934 	 11.539278030395508 	 0.3511052131652832 	 0.3488881587982178 	 
2025-08-05 22:37:13.090352 test begin: paddle.Tensor.__rsub__(Tensor([7664, 6629],"float32"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([7664, 6629],"float32"), 1, ) 	 50804656 	 33801 	 9.998100996017456 	 10.059468269348145 	 0.3022651672363281 	 0.3041055202484131 	 10.001950740814209 	 10.055763959884644 	 0.3024170398712158 	 0.30403780937194824 	 
2025-08-05 22:37:54.901057 test begin: paddle.Tensor.__rtruediv__(Tensor([15548, 3268],"float32"), 1.0, )
[Prof] paddle.Tensor.__rtruediv__ 	 paddle.Tensor.__rtruediv__(Tensor([15548, 3268],"float32"), 1.0, ) 	 50810864 	 17130 	 9.995474100112915 	 10.208606481552124 	 0.29810452461242676 	 0.3043785095214844 	 10.032062768936157 	 22.91930866241455 	 0.5984845161437988 	 0.34192895889282227 	 
2025-08-05 22:38:50.800202 test begin: paddle.Tensor.__rtruediv__(Tensor([16773, 3029],"float32"), 1.0, )
[Prof] paddle.Tensor.__rtruediv__ 	 paddle.Tensor.__rtruediv__(Tensor([16773, 3029],"float32"), 1.0, ) 	 50805417 	 17130 	 9.997792720794678 	 10.201761484146118 	 0.29823780059814453 	 0.3042922019958496 	 10.030937433242798 	 22.91709542274475 	 0.5984675884246826 	 0.34183335304260254 	 
2025-08-05 22:39:47.713750 test begin: paddle.Tensor.__rtruediv__(Tensor([26736, 1901],"float32"), 1.0, )
[Prof] paddle.Tensor.__rtruediv__ 	 paddle.Tensor.__rtruediv__(Tensor([26736, 1901],"float32"), 1.0, ) 	 50825136 	 17130 	 10.000792980194092 	 10.20622444152832 	 0.2982821464538574 	 0.30443692207336426 	 10.034474611282349 	 22.924550533294678 	 0.5986776351928711 	 0.34195828437805176 	 
2025-08-05 22:40:42.781370 test begin: paddle.Tensor.__rtruediv__(Tensor([37411, 1358],"float32"), 1.0, )
[Prof] paddle.Tensor.__rtruediv__ 	 paddle.Tensor.__rtruediv__(Tensor([37411, 1358],"float32"), 1.0, ) 	 50804138 	 17130 	 10.005353927612305 	 10.201310873031616 	 0.2981569766998291 	 0.30432796478271484 	 10.03039264678955 	 22.916232585906982 	 0.5984160900115967 	 0.3418588638305664 	 
2025-08-05 22:41:38.050050 test begin: paddle.Tensor.__rtruediv__(Tensor([6684, 7601],"float32"), 1.0, )
[Prof] paddle.Tensor.__rtruediv__ 	 paddle.Tensor.__rtruediv__(Tensor([6684, 7601],"float32"), 1.0, ) 	 50805084 	 17130 	 10.011224269866943 	 10.201810836791992 	 0.2982492446899414 	 0.30437636375427246 	 10.028562307357788 	 22.918006896972656 	 0.5983254909515381 	 0.34189939498901367 	 
2025-08-05 22:42:33.997275 test begin: paddle.Tensor.__rxor__(Tensor([2, 3, 8467201],"int32"), 5, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([2, 3, 8467201],"int32"), 5, ) 	 50803206 	 33457 	 9.986198425292969 	 9.95490837097168 	 0.15252685546875 	 0.30408263206481934 	 None 	 None 	 None 	 None 	 
2025-08-05 22:42:56.247361 test begin: paddle.Tensor.__rxor__(Tensor([2, 3, 8467201],"int32"), True, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([2, 3, 8467201],"int32"), True, ) 	 50803206 	 33457 	 9.993202209472656 	 9.95479130744934 	 0.15253663063049316 	 0.3041081428527832 	 None 	 None 	 None 	 None 	 
2025-08-05 22:43:16.811259 test begin: paddle.Tensor.__rxor__(Tensor([2, 5080321, 5],"int32"), 5, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([2, 5080321, 5],"int32"), 5, ) 	 50803210 	 33457 	 9.985862493515015 	 9.954675674438477 	 0.15250754356384277 	 0.304063081741333 	 None 	 None 	 None 	 None 	 
2025-08-05 22:43:38.885534 test begin: paddle.Tensor.__rxor__(Tensor([2, 5080321, 5],"int32"), True, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([2, 5080321, 5],"int32"), True, ) 	 50803210 	 33457 	 10.427974700927734 	 9.954853534698486 	 0.15250706672668457 	 0.30410218238830566 	 None 	 None 	 None 	 None 	 
2025-08-05 22:44:04.799578 test begin: paddle.Tensor.__rxor__(Tensor([3386881, 3, 5],"int32"), 5, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([3386881, 3, 5],"int32"), 5, ) 	 50803215 	 33457 	 9.985836267471313 	 9.954476118087769 	 0.15248823165893555 	 0.3040745258331299 	 None 	 None 	 None 	 None 	 
2025-08-05 22:44:25.359534 test begin: paddle.Tensor.__rxor__(Tensor([3386881, 3, 5],"int32"), True, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([3386881, 3, 5],"int32"), True, ) 	 50803215 	 33457 	 9.986022233963013 	 10.413300037384033 	 0.15250730514526367 	 0.30408143997192383 	 None 	 None 	 None 	 None 	 
2025-08-05 22:44:48.916261 test begin: paddle.Tensor.__sub__(Tensor([1, 1, 32768, 32768],"float16"), 1, )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([1, 1, 32768, 32768],"float16"), 1, ) 	 1073741824 	 33533 	 104.1074206829071 	 103.64363527297974 	 3.1729249954223633 	 3.1587507724761963 	 104.14179992675781 	 1.8387978076934814 	 3.173935890197754 	 7.176399230957031e-05 	 
2025-08-05 22:50:43.129965 test begin: paddle.Tensor.__sub__(Tensor([128, 192, 22, 96],"float32"), Tensor([128, 1, 22, 96],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([128, 192, 22, 96],"float32"), Tensor([128, 1, 22, 96],"float32"), ) 	 52174848 	 33533 	 10.188264846801758 	 10.62373948097229 	 0.3104894161224365 	 0.32256150245666504 	 17.956321239471436 	 15.266482830047607 	 0.27364563941955566 	 0.23264145851135254 	 
2025-08-05 22:51:39.961731 test begin: paddle.Tensor.__sub__(Tensor([128, 192, 96, 22],"float32"), Tensor([128, 1, 96, 22],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([128, 192, 96, 22],"float32"), Tensor([128, 1, 96, 22],"float32"), ) 	 52174848 	 33533 	 10.187649726867676 	 10.586761474609375 	 0.3104865550994873 	 0.3225405216217041 	 17.955912828445435 	 15.267303466796875 	 0.2736175060272217 	 0.2326524257659912 	 
2025-08-05 22:52:39.272310 test begin: paddle.Tensor.__sub__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 1, 96, 96],"float32"), ) 	 53084160 	 33533 	 10.247112274169922 	 10.696021795272827 	 0.3122673034667969 	 0.32599663734436035 	 16.28593373298645 	 15.490156650543213 	 0.24817371368408203 	 0.2360224723815918 	 
2025-08-05 22:53:33.768493 test begin: paddle.Tensor.__sub__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 44, 96, 96],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 44, 96, 96],"float32"), ) 	 103809024 	 33533 	 15.423238515853882 	 15.28768253326416 	 0.47010374069213867 	 0.46597886085510254 	 16.24339509010315 	 10.190546751022339 	 0.49503588676452637 	 0.31055426597595215 	 
2025-08-05 22:54:33.471424 test begin: paddle.Tensor.__sub__(Tensor([2, 1, 1551, 32768],"float16"), 1, )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([2, 1, 1551, 32768],"float16"), 1, ) 	 101646336 	 33533 	 9.997562170028687 	 9.931885480880737 	 0.30467653274536133 	 0.3025379180908203 	 9.995805263519287 	 1.822606086730957 	 0.3046267032623291 	 8.749961853027344e-05 	 
2025-08-05 22:55:09.063662 test begin: paddle.Tensor.__sub__(Tensor([2, 1, 32768, 1551],"float16"), 1, )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([2, 1, 32768, 1551],"float16"), 1, ) 	 101646336 	 33533 	 9.997659921646118 	 9.929787874221802 	 0.30471348762512207 	 0.302532434463501 	 9.995718955993652 	 1.8057491779327393 	 0.30465245246887207 	 9.059906005859375e-05 	 
2025-08-05 22:55:46.367042 test begin: paddle.Tensor.__sub__(Tensor([2, 1, 32768, 32768],"float16"), 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa180454400>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-05 23:05:53.801586 test begin: paddle.Tensor.__sub__(Tensor([26736, 3029, 1],"float32"), Tensor([26736, 3029, 1],"float32"), )
W0805 23:05:56.371989 112767 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([26736, 3029, 1],"float32"), Tensor([26736, 3029, 1],"float32"), ) 	 161966688 	 33533 	 23.966249227523804 	 23.768018007278442 	 0.7305107116699219 	 0.7243921756744385 	 25.330087184906006 	 15.804808378219604 	 0.7720198631286621 	 0.4816403388977051 	 
2025-08-05 23:07:27.458271 test begin: paddle.Tensor.__sub__(Tensor([26736, 3029, 1],"float32"), Tensor([26736, 3029, 2],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([26736, 3029, 1],"float32"), Tensor([26736, 3029, 2],"float32"), ) 	 242950032 	 33533 	 39.41782546043396 	 40.13970351219177 	 1.2014594078063965 	 1.2231507301330566 	 79.75025868415833 	 82.72024297714233 	 1.2152025699615479 	 1.2605223655700684 	 
2025-08-05 23:11:39.182185 test begin: paddle.Tensor.__sub__(Tensor([26736, 3029, 2],"float32"), Tensor([26736, 3029, 1],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([26736, 3029, 2],"float32"), Tensor([26736, 3029, 1],"float32"), ) 	 242950032 	 33533 	 39.402332067489624 	 40.14554810523987 	 1.200727939605713 	 1.223069429397583 	 74.28881216049194 	 82.71883392333984 	 0.7544429302215576 	 1.2605741024017334 	 
2025-08-05 23:15:45.032187 test begin: paddle.Tensor.__sub__(Tensor([26736, 951, 2],"float32"), Tensor([26736, 951, 2],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([26736, 951, 2],"float32"), Tensor([26736, 951, 2],"float32"), ) 	 101703744 	 33533 	 15.114651679992676 	 14.98176908493042 	 0.4606308937072754 	 0.4566326141357422 	 15.912284851074219 	 9.990779638290405 	 0.4849522113800049 	 0.30448222160339355 	 
2025-08-05 23:16:43.746241 test begin: paddle.Tensor.__sub__(Tensor([29, 192, 96, 96],"float32"), Tensor([29, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([29, 192, 96, 96],"float32"), Tensor([29, 1, 96, 96],"float32"), ) 	 51581952 	 33533 	 10.057848930358887 	 10.453266382217407 	 0.3063981533050537 	 0.3185768127441406 	 16.03810954093933 	 15.085208415985107 	 0.2443833351135254 	 0.22978878021240234 	 
2025-08-05 23:17:38.422497 test begin: paddle.Tensor.__sub__(Tensor([8387, 3029, 2],"float32"), Tensor([8387, 3029, 2],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([8387, 3029, 2],"float32"), Tensor([8387, 3029, 2],"float32"), ) 	 101616892 	 33533 	 15.625714302062988 	 14.96909785270691 	 0.4601120948791504 	 0.4562361240386963 	 15.898041725158691 	 9.982062101364136 	 0.4844822883605957 	 0.3042433261871338 	 
2025-08-05 23:18:39.775538 test begin: paddle.Tensor.__truediv__(Tensor([124, 128, 34, 96],"float32"), Tensor([124, 1, 34, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([124, 128, 34, 96],"float32"), Tensor([124, 1, 34, 96],"float32"), ) 	 52210944 	 33318 	 11.007885456085205 	 10.787307739257812 	 0.31058669090270996 	 0.3307778835296631 	 27.03341794013977 	 62.83095407485962 	 0.41453051567077637 	 0.32103610038757324 	 
2025-08-05 23:20:38.929524 test begin: paddle.Tensor.__truediv__(Tensor([124, 128, 96, 34],"float32"), Tensor([124, 1, 96, 34],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([124, 128, 96, 34],"float32"), Tensor([124, 1, 96, 34],"float32"), ) 	 52210944 	 33318 	 11.063502550125122 	 10.79915189743042 	 0.31050992012023926 	 0.330946683883667 	 27.029752016067505 	 62.82683491706848 	 0.4145212173461914 	 0.3210616111755371 	 
2025-08-05 23:22:34.775974 test begin: paddle.Tensor.__truediv__(Tensor([124, 45, 96, 96],"float32"), Tensor([124, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([124, 45, 96, 96],"float32"), Tensor([124, 1, 96, 96],"float32"), ) 	 52568064 	 33318 	 10.698043823242188 	 10.802480459213257 	 0.30989933013916016 	 0.3311288356781006 	 25.680246829986572 	 62.81884813308716 	 0.3938584327697754 	 0.32097911834716797 	 
2025-08-05 23:24:27.508861 test begin: paddle.Tensor.__truediv__(Tensor([124, 45, 96, 96],"float32"), Tensor([124, 45, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([124, 45, 96, 96],"float32"), Tensor([124, 45, 96, 96],"float32"), ) 	 102850560 	 33318 	 15.189522981643677 	 15.154909372329712 	 0.4657893180847168 	 0.46483492851257324 	 38.37758922576904 	 70.51904463768005 	 1.1771738529205322 	 0.4326632022857666 	 
2025-08-05 23:26:52.210740 test begin: paddle.Tensor.__truediv__(Tensor([128, 128, 33, 96],"float32"), Tensor([128, 1, 33, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 128, 33, 96],"float32"), Tensor([128, 1, 33, 96],"float32"), ) 	 52310016 	 33318 	 10.153675079345703 	 10.811878681182861 	 0.3114144802093506 	 0.33174777030944824 	 27.079288244247437 	 62.96531701087952 	 0.41533446311950684 	 0.3217203617095947 	 
2025-08-05 23:28:46.540169 test begin: paddle.Tensor.__truediv__(Tensor([128, 128, 96, 33],"float32"), Tensor([128, 1, 96, 33],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 128, 96, 33],"float32"), Tensor([128, 1, 96, 33],"float32"), ) 	 52310016 	 33318 	 10.145215034484863 	 10.828860521316528 	 0.31107020378112793 	 0.331787109375 	 27.090569734573364 	 62.97126293182373 	 0.41547203063964844 	 0.3217899799346924 	 
2025-08-05 23:30:43.934295 test begin: paddle.Tensor.__truediv__(Tensor([128, 192, 22, 96],"float32"), Tensor([128, 1, 22, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 192, 22, 96],"float32"), Tensor([128, 1, 22, 96],"float32"), ) 	 52174848 	 33318 	 10.134503841400146 	 10.79257345199585 	 0.3107013702392578 	 0.3310527801513672 	 27.483160495758057 	 62.85517191886902 	 0.4215083122253418 	 0.321195125579834 	 
2025-08-05 23:32:38.436906 test begin: paddle.Tensor.__truediv__(Tensor([128, 192, 96, 22],"float32"), Tensor([128, 1, 96, 22],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 192, 96, 22],"float32"), Tensor([128, 1, 96, 22],"float32"), ) 	 52174848 	 33318 	 10.355619430541992 	 10.795992851257324 	 0.3107030391693115 	 0.3312098979949951 	 27.48352360725403 	 62.86480474472046 	 0.4214901924133301 	 0.32120823860168457 	 
2025-08-05 23:34:34.535584 test begin: paddle.Tensor.__truediv__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 1, 96, 96],"float32"), ) 	 53084160 	 33318 	 11.036193132400513 	 10.893941879272461 	 0.31291675567626953 	 0.3338901996612549 	 25.97735857963562 	 63.32581400871277 	 0.39832592010498047 	 0.32351064682006836 	 
2025-08-05 23:36:30.502334 test begin: paddle.Tensor.__truediv__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 44, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 44, 96, 96],"float32"), ) 	 103809024 	 33318 	 15.330225467681885 	 15.294524669647217 	 0.4703097343444824 	 0.46914172172546387 	 38.737314224243164 	 71.1690583229065 	 1.188338041305542 	 0.43665409088134766 	 
2025-08-05 23:38:55.143831 test begin: paddle.Tensor.__truediv__(Tensor([29, 192, 96, 96],"float32"), Tensor([29, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([29, 192, 96, 96],"float32"), Tensor([29, 1, 96, 96],"float32"), ) 	 51581952 	 33318 	 9.992024660110474 	 10.668723106384277 	 0.3064839839935303 	 0.3272378444671631 	 25.471887826919556 	 62.137325048446655 	 0.39061570167541504 	 0.31754350662231445 	 
2025-08-05 23:40:45.375452 test begin: paddle.Tensor.__truediv__(Tensor([44, 128, 96, 96],"float32"), Tensor([44, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([44, 128, 96, 96],"float32"), Tensor([44, 1, 96, 96],"float32"), ) 	 52310016 	 33318 	 10.123366832733154 	 10.806069135665894 	 0.31050729751586914 	 0.3315260410308838 	 25.83874773979187 	 62.90311861038208 	 0.39622020721435547 	 0.321439266204834 	 
2025-08-05 23:42:38.577530 test begin: paddle.Tensor.__xor__(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 85302 	 38.26172876358032 	 38.39012908935547 	 0.4586448669433594 	 0.45987749099731445 	 None 	 None 	 None 	 None 	 
2025-08-05 23:44:01.002099 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 85302 	 38.28023934364319 	 38.38412070274353 	 0.4584658145904541 	 0.45992302894592285 	 None 	 None 	 None 	 None 	 
2025-08-05 23:45:20.067766 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), ) 	 203214240 	 85302 	 38.2344446182251 	 38.3827440738678 	 0.4577951431274414 	 0.45986294746398926 	 None 	 None 	 None 	 None 	 
2025-08-05 23:46:40.449759 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), ) 	 203213880 	 85302 	 38.27110290527344 	 38.38378024101257 	 0.45876646041870117 	 0.4598269462585449 	 None 	 None 	 None 	 None 	 
2025-08-05 23:48:00.516786 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), ) 	 101607264 	 85302 	 10.058668851852417 	 9.9796462059021 	 0.1204836368560791 	 0.11950802803039551 	 None 	 None 	 None 	 None 	 
2025-08-05 23:48:21.967341 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), ) 	 101607264 	 85302 	 38.41020345687866 	 38.0793833732605 	 0.46016359329223633 	 0.45621490478515625 	 None 	 None 	 None 	 None 	 
2025-08-05 23:49:41.508084 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), ) 	 203213664 	 85302 	 38.250322103500366 	 38.385820388793945 	 0.4575498104095459 	 0.459918737411499 	 None 	 None 	 None 	 None 	 
2025-08-05 23:51:00.227481 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 50807520 	 85302 	 14.34213137626648 	 19.37049150466919 	 0.17185258865356445 	 0.2320239543914795 	 None 	 None 	 None 	 None 	 
2025-08-05 23:51:35.082086 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 101610720 	 85302 	 27.163268089294434 	 40.87079977989197 	 0.3213052749633789 	 0.4896817207336426 	 None 	 None 	 None 	 None 	 
2025-08-05 23:52:46.017910 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 50807520 	 85302 	 25.281626224517822 	 26.273783922195435 	 0.30284905433654785 	 0.31478190422058105 	 None 	 None 	 None 	 None 	 
2025-08-05 23:53:39.866123 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), ) 	 101608560 	 85302 	 10.058185577392578 	 9.883944034576416 	 0.12047410011291504 	 0.11843061447143555 	 None 	 None 	 None 	 None 	 
2025-08-05 23:54:01.372378 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), ) 	 101608560 	 85302 	 38.412187576293945 	 38.09063267707825 	 0.4602322578430176 	 0.45624828338623047 	 None 	 None 	 None 	 None 	 
2025-08-05 23:55:20.231689 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), ) 	 203214960 	 85302 	 38.26719856262207 	 38.389357805252075 	 0.4585714340209961 	 0.45990729331970215 	 None 	 None 	 None 	 None 	 
2025-08-05 23:56:41.927739 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 50807520 	 85302 	 15.079162120819092 	 19.355560541152954 	 0.18065524101257324 	 0.2319025993347168 	 None 	 None 	 None 	 None 	 
2025-08-05 23:57:17.127632 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 101610720 	 85302 	 10.052491664886475 	 9.891063928604126 	 0.12045598030090332 	 0.11847639083862305 	 None 	 None 	 None 	 None 	 
2025-08-05 23:57:41.466964 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50807520 	 85302 	 25.25415802001953 	 26.683762073516846 	 0.30254340171813965 	 0.3140830993652344 	 None 	 None 	 None 	 None 	 
2025-08-05 23:58:38.538285 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 101610720 	 85302 	 38.417266845703125 	 38.10111689567566 	 0.4602396488189697 	 0.45619893074035645 	 None 	 None 	 None 	 None 	 
2025-08-05 23:59:59.999427 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101610720 	 85302 	 29.407269954681396 	 40.746323108673096 	 0.3521261215209961 	 0.48818302154541016 	 None 	 None 	 None 	 None 	 
2025-08-06 00:01:11.444813 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 203217120 	 85302 	 38.257142543792725 	 38.38678240776062 	 0.45772624015808105 	 0.45992088317871094 	 None 	 None 	 None 	 None 	 
2025-08-06 00:02:30.171806 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), ) 	 101607480 	 85302 	 10.037299633026123 	 10.591874837875366 	 0.12024092674255371 	 0.11758613586425781 	 None 	 None 	 None 	 None 	 
2025-08-06 00:02:53.818461 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), ) 	 101607480 	 85302 	 38.41498064994812 	 38.07745313644409 	 0.46025514602661133 	 0.45624303817749023 	 None 	 None 	 None 	 None 	 
2025-08-06 00:04:11.576915 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), ) 	 101607840 	 85302 	 10.036796808242798 	 9.82323956489563 	 0.12023520469665527 	 0.11766529083251953 	 None 	 None 	 None 	 None 	 
2025-08-06 00:04:32.881727 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), ) 	 101607840 	 85302 	 38.40963268280029 	 38.07703256607056 	 0.4600999355316162 	 0.456204891204834 	 None 	 None 	 None 	 None 	 
2025-08-06 00:05:50.638372 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 85302 	 10.035781383514404 	 9.829320430755615 	 0.1202387809753418 	 0.11775922775268555 	 None 	 None 	 None 	 None 	 
2025-08-06 00:06:12.156213 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 85302 	 38.40954113006592 	 38.07744264602661 	 0.46010923385620117 	 0.4562077522277832 	 None 	 None 	 None 	 None 	 
2025-08-06 00:07:29.929094 test begin: paddle.Tensor.__xor__(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 85302 	 10.035999774932861 	 9.831859111785889 	 0.12026858329772949 	 0.1177835464477539 	 None 	 None 	 None 	 None 	 
2025-08-06 00:07:51.261171 test begin: paddle.Tensor.__xor__(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 85302 	 38.408665895462036 	 38.07714915275574 	 0.46021389961242676 	 0.4561917781829834 	 None 	 None 	 None 	 None 	 
2025-08-06 00:09:09.004547 test begin: paddle.Tensor.__xor__(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101608560 	 85302 	 10.042725801467896 	 9.882014751434326 	 0.12031269073486328 	 0.11838507652282715 	 None 	 None 	 None 	 None 	 
2025-08-06 00:09:33.168144 test begin: paddle.Tensor.__xor__(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101608560 	 85302 	 38.41681623458862 	 38.08076524734497 	 0.4602046012878418 	 0.45627355575561523 	 None 	 None 	 None 	 None 	 
2025-08-06 00:10:53.623830 test begin: paddle.Tensor.__xor__(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214960 	 85302 	 38.2894287109375 	 38.38852000236511 	 0.45889854431152344 	 0.4599471092224121 	 None 	 None 	 None 	 None 	 
2025-08-06 00:12:12.415805 test begin: paddle.Tensor.abs(Tensor([243360, 209],"float32"), )
[Prof] paddle.Tensor.abs 	 paddle.Tensor.abs(Tensor([243360, 209],"float32"), ) 	 50862240 	 33826 	 10.023614883422852 	 11.036959886550903 	 0.30282068252563477 	 0.304440975189209 	 15.242699146270752 	 25.149407863616943 	 0.4604940414428711 	 0.3799760341644287 	 
2025-08-06 00:13:17.539245 test begin: paddle.Tensor.abs(Tensor([282240, 181],"float32"), )
[Prof] paddle.Tensor.abs 	 paddle.Tensor.abs(Tensor([282240, 181],"float32"), ) 	 51085440 	 33826 	 10.065189838409424 	 10.118510484695435 	 0.30409741401672363 	 0.3057091236114502 	 15.303598165512085 	 25.258368015289307 	 0.4625072479248047 	 0.3815631866455078 	 
2025-08-06 00:14:20.029388 test begin: paddle.Tensor.abs(Tensor([324000, 157],"float32"), )
[Prof] paddle.Tensor.abs 	 paddle.Tensor.abs(Tensor([324000, 157],"float32"), ) 	 50868000 	 33826 	 10.013235092163086 	 10.093860864639282 	 0.3025491237640381 	 0.3044872283935547 	 15.248197793960571 	 25.153281688690186 	 0.46068787574768066 	 0.379974365234375 	 
2025-08-06 00:15:24.862595 test begin: paddle.Tensor.abs(Tensor([635041, 80],"float32"), )
[Prof] paddle.Tensor.abs 	 paddle.Tensor.abs(Tensor([635041, 80],"float32"), ) 	 50803280 	 33826 	 10.004385232925415 	 10.066378593444824 	 0.3022751808166504 	 0.3040773868560791 	 15.226913928985596 	 25.117116928100586 	 0.4601304531097412 	 0.37946343421936035 	 
2025-08-06 00:16:28.430944 test begin: paddle.Tensor.add(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.add 	 paddle.Tensor.add(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 22210 	 10.47698712348938 	 9.914432525634766 	 0.46011877059936523 	 0.4561648368835449 	 10.72071647644043 	 1.2866308689117432 	 0.49329209327697754 	 7.271766662597656e-05 	 
2025-08-06 00:17:05.077077 test begin: paddle.Tensor.all(Tensor([10, 1, 2048, 24807],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([10, 1, 2048, 24807],"bool"), ) 	 508047360 	 21540 	 10.007449388504028 	 10.941251039505005 	 0.2374420166015625 	 0.25960326194763184 	 None 	 None 	 None 	 None 	 
2025-08-06 00:17:35.480094 test begin: paddle.Tensor.all(Tensor([10, 1, 24807, 2048],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([10, 1, 24807, 2048],"bool"), ) 	 508047360 	 21540 	 10.007121801376343 	 10.9451425075531 	 0.23738789558410645 	 0.25969767570495605 	 None 	 None 	 None 	 None 	 
2025-08-06 00:18:05.130811 test begin: paddle.Tensor.all(Tensor([10, 13, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([10, 13, 2048, 2048],"bool"), ) 	 545259520 	 21540 	 10.72971224784851 	 11.745245456695557 	 0.2543916702270508 	 0.27866148948669434 	 None 	 None 	 None 	 None 	 
2025-08-06 00:18:35.085665 test begin: paddle.Tensor.all(Tensor([130, 1, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([130, 1, 2048, 2048],"bool"), ) 	 545259520 	 21540 	 10.727072715759277 	 11.747655630111694 	 0.25441765785217285 	 0.2785806655883789 	 None 	 None 	 None 	 None 	 
2025-08-06 00:19:05.195353 test begin: paddle.Tensor.all(Tensor([1590, 10, 32000],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([1590, 10, 32000],"bool"), ) 	 508800000 	 21540 	 10.049869775772095 	 10.958545207977295 	 0.2383558750152588 	 0.259929895401001 	 None 	 None 	 None 	 None 	 
2025-08-06 00:19:33.315712 test begin: paddle.Tensor.all(Tensor([20, 10, 2540161],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([20, 10, 2540161],"bool"), ) 	 508032200 	 21540 	 10.07580018043518 	 10.933226585388184 	 0.2389841079711914 	 0.2593669891357422 	 None 	 None 	 None 	 None 	 
2025-08-06 00:20:01.686209 test begin: paddle.Tensor.all(Tensor([20, 100, 256000],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([20, 100, 256000],"bool"), ) 	 512000000 	 21540 	 10.076891899108887 	 11.162886381149292 	 0.239091157913208 	 0.2648797035217285 	 None 	 None 	 None 	 None 	 
2025-08-06 00:20:30.170988 test begin: paddle.Tensor.all(Tensor([20, 794, 32000],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([20, 794, 32000],"bool"), ) 	 508160000 	 21540 	 10.029020547866821 	 10.903714418411255 	 0.23789334297180176 	 0.25873398780822754 	 None 	 None 	 None 	 None 	 
2025-08-06 00:20:58.090119 test begin: paddle.Tensor.all(Tensor([200, 10, 256000],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([200, 10, 256000],"bool"), ) 	 512000000 	 21540 	 10.077434778213501 	 11.162754535675049 	 0.239091157913208 	 0.26482605934143066 	 None 	 None 	 None 	 None 	 
2025-08-06 00:21:27.194918 test begin: paddle.Tensor.amax(Tensor([1270081, 2, 4, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([1270081, 2, 4, 5],"float32"), axis=-1, keepdim=True, ) 	 50803240 	 65883 	 29.8679301738739 	 31.043150663375854 	 0.463238000869751 	 0.48148036003112793 	 86.59088277816772 	 106.47228193283081 	 0.33574366569519043 	 0.3300750255584717 	 
2025-08-06 00:25:45.699181 test begin: paddle.Tensor.amax(Tensor([1270081, 2, 5, 4],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([1270081, 2, 5, 4],"float32"), axis=2, keepdim=True, ) 	 50803240 	 65883 	 33.80778455734253 	 12.08979845046997 	 0.5243868827819824 	 0.18753743171691895 	 91.33260011672974 	 100.79617762565613 	 0.35426926612854004 	 0.3125925064086914 	 
2025-08-06 00:29:47.683102 test begin: paddle.Tensor.amax(Tensor([1270081, 2, 5, 4],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([1270081, 2, 5, 4],"float32"), axis=None, keepdim=False, ) 	 50803240 	 65883 	 9.988929986953735 	 10.065859079360962 	 0.07749247550964355 	 0.07809042930603027 	 68.74684190750122 	 82.20013451576233 	 0.21344971656799316 	 0.2123262882232666 	 
2025-08-06 00:32:42.654866 test begin: paddle.Tensor.amax(Tensor([3, 2, 1693441, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 2, 1693441, 5],"float32"), axis=-1, keepdim=True, ) 	 50803230 	 65883 	 29.865292072296143 	 31.04913640022278 	 0.4632711410522461 	 0.4814143180847168 	 86.63374543190002 	 106.5233006477356 	 0.33577632904052734 	 0.33006906509399414 	 
2025-08-06 00:36:59.263334 test begin: paddle.Tensor.amax(Tensor([3, 2, 2116801, 4],"float32"), axis=2, keepdim=True, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f348c086890>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 00:47:04.617393 test begin: paddle.Tensor.amax(Tensor([3, 2, 2116801, 4],"float32"), axis=None, keepdim=False, )
W0806 00:47:05.586575 115991 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 2, 2116801, 4],"float32"), axis=None, keepdim=False, ) 	 50803224 	 65883 	 9.984394550323486 	 10.066149473190308 	 0.07744526863098145 	 0.0780646800994873 	 68.78101992607117 	 82.179270029068 	 0.2135777473449707 	 0.21237945556640625 	 
2025-08-06 00:49:57.239181 test begin: paddle.Tensor.amax(Tensor([3, 2, 4, 2116801],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 2, 4, 2116801],"float32"), axis=-1, keepdim=True, ) 	 50803224 	 65883 	 11.541081428527832 	 10.177303314208984 	 0.08953356742858887 	 0.07894611358642578 	 70.18626070022583 	 83.93192863464355 	 0.21785187721252441 	 0.21688580513000488 	 
2025-08-06 00:52:53.945324 test begin: paddle.Tensor.amax(Tensor([3, 2, 5, 1693441],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 2, 5, 1693441],"float32"), axis=2, keepdim=True, ) 	 50803230 	 65883 	 13.70688796043396 	 14.282109022140503 	 0.21268105506896973 	 0.22171545028686523 	 83.26464653015137 	 100.48204112052917 	 0.3229215145111084 	 0.3116030693054199 	 
2025-08-06 00:56:26.718534 test begin: paddle.Tensor.amax(Tensor([3, 2, 5, 1693441],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 2, 5, 1693441],"float32"), axis=None, keepdim=False, ) 	 50803230 	 65883 	 9.996286869049072 	 10.068120241165161 	 0.07751893997192383 	 0.07812380790710449 	 68.77042388916016 	 82.17484545707703 	 0.21351218223571777 	 0.21232271194458008 	 
2025-08-06 00:59:19.288142 test begin: paddle.Tensor.amax(Tensor([3, 846721, 4, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 846721, 4, 5],"float32"), axis=-1, keepdim=True, ) 	 50803260 	 65883 	 29.862879514694214 	 31.075917959213257 	 0.4632275104522705 	 0.48174023628234863 	 86.60804033279419 	 106.539705991745 	 0.33581018447875977 	 0.33069539070129395 	 
2025-08-06 01:03:37.510471 test begin: paddle.Tensor.amax(Tensor([3, 846721, 5, 4],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 846721, 5, 4],"float32"), axis=2, keepdim=True, ) 	 50803260 	 65883 	 34.835238218307495 	 12.08878469467163 	 0.52443528175354 	 0.18749451637268066 	 91.29865598678589 	 100.82489490509033 	 0.35422778129577637 	 0.3127632141113281 	 
2025-08-06 01:07:41.162677 test begin: paddle.Tensor.amax(Tensor([3, 846721, 5, 4],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 846721, 5, 4],"float32"), axis=None, keepdim=False, ) 	 50803260 	 65883 	 9.998355150222778 	 10.069300651550293 	 0.07752227783203125 	 0.07813262939453125 	 68.77220559120178 	 82.17101645469666 	 0.21356725692749023 	 0.21233820915222168 	 
2025-08-06 01:10:33.028420 test begin: paddle.Tensor.amin(Tensor([1270081, 2, 4, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([1270081, 2, 4, 5],"float32"), axis=-1, keepdim=True, ) 	 50803240 	 65865 	 29.862956285476685 	 31.055441856384277 	 0.4631986618041992 	 0.4825303554534912 	 86.60874795913696 	 106.52836775779724 	 0.3357858657836914 	 0.3300178050994873 	 
2025-08-06 01:14:48.180309 test begin: paddle.Tensor.amin(Tensor([1270081, 2, 5, 4],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([1270081, 2, 5, 4],"float32"), axis=2, keepdim=True, ) 	 50803240 	 65865 	 33.79100179672241 	 12.088115215301514 	 0.5242965221405029 	 0.18753433227539062 	 91.28939008712769 	 100.76320052146912 	 0.35379719734191895 	 0.3127903938293457 	 
2025-08-06 01:18:47.139971 test begin: paddle.Tensor.amin(Tensor([1270081, 2, 5, 4],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([1270081, 2, 5, 4],"float32"), axis=None, keepdim=False, ) 	 50803240 	 65865 	 9.993817567825317 	 10.065372705459595 	 0.07753491401672363 	 0.07810282707214355 	 68.7543785572052 	 82.15063142776489 	 0.21351265907287598 	 0.21234631538391113 	 
2025-08-06 01:21:40.897227 test begin: paddle.Tensor.amin(Tensor([3, 2, 1693441, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 2, 1693441, 5],"float32"), axis=-1, keepdim=True, ) 	 50803230 	 65865 	 30.531194925308228 	 31.057449102401733 	 0.46335363388061523 	 0.4816000461578369 	 86.57679653167725 	 106.43051075935364 	 0.3358180522918701 	 0.3301084041595459 	 
2025-08-06 01:25:58.316556 test begin: paddle.Tensor.amin(Tensor([3, 2, 2116801, 4],"float32"), axis=2, keepdim=True, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6d6b49fa60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 01:36:03.431797 test begin: paddle.Tensor.amin(Tensor([3, 2, 2116801, 4],"float32"), axis=None, keepdim=False, )
W0806 01:36:04.408985 117694 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 2, 2116801, 4],"float32"), axis=None, keepdim=False, ) 	 50803224 	 65865 	 9.979169130325317 	 10.063239812850952 	 0.07740187644958496 	 0.07810115814208984 	 68.76012921333313 	 82.15854907035828 	 0.21352696418762207 	 0.21233797073364258 	 
2025-08-06 01:38:57.618270 test begin: paddle.Tensor.amin(Tensor([3, 2, 4, 2116801],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 2, 4, 2116801],"float32"), axis=-1, keepdim=True, ) 	 50803224 	 65865 	 11.530962944030762 	 10.17524003982544 	 0.08945488929748535 	 0.07895541191101074 	 70.16533493995667 	 83.91547727584839 	 0.21789026260375977 	 0.21694254875183105 	 
2025-08-06 01:41:54.269182 test begin: paddle.Tensor.amin(Tensor([3, 2, 5, 1693441],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 2, 5, 1693441],"float32"), axis=2, keepdim=True, ) 	 50803230 	 65865 	 13.699369668960571 	 14.274661302566528 	 0.2126481533050537 	 0.2214500904083252 	 83.23677349090576 	 100.46685576438904 	 0.3228945732116699 	 0.3116333484649658 	 
2025-08-06 01:45:26.968315 test begin: paddle.Tensor.amin(Tensor([3, 2, 5, 1693441],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 2, 5, 1693441],"float32"), axis=None, keepdim=False, ) 	 50803230 	 65865 	 9.991017580032349 	 10.065215349197388 	 0.07752156257629395 	 0.07808399200439453 	 68.7493953704834 	 82.15673279762268 	 0.21346020698547363 	 0.21235251426696777 	 
2025-08-06 01:48:21.255466 test begin: paddle.Tensor.amin(Tensor([3, 846721, 4, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 846721, 4, 5],"float32"), axis=-1, keepdim=True, ) 	 50803260 	 65865 	 29.867642641067505 	 32.97257328033447 	 0.46347784996032715 	 0.48166465759277344 	 86.57969903945923 	 106.44000720977783 	 0.33585309982299805 	 0.3301718235015869 	 
2025-08-06 01:52:39.578714 test begin: paddle.Tensor.amin(Tensor([3, 846721, 5, 4],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 846721, 5, 4],"float32"), axis=2, keepdim=True, ) 	 50803260 	 65865 	 33.834267139434814 	 12.084970235824585 	 0.5246102809906006 	 0.18749356269836426 	 91.22824716567993 	 100.72334814071655 	 0.3538320064544678 	 0.31240177154541016 	 
2025-08-06 01:56:40.264089 test begin: paddle.Tensor.amin(Tensor([3, 846721, 5, 4],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 846721, 5, 4],"float32"), axis=None, keepdim=False, ) 	 50803260 	 65865 	 9.991820573806763 	 10.065365314483643 	 0.07753562927246094 	 0.07812142372131348 	 68.74899768829346 	 82.1543984413147 	 0.21350550651550293 	 0.21234917640686035 	 
2025-08-06 01:59:32.095733 test begin: paddle.Tensor.any(Tensor([10, 1379, 192, 192],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 1379, 192, 192],"bool"), axis=list[2,3,], ) 	 508354560 	 20191 	 10.001730918884277 	 11.133041620254517 	 0.25313448905944824 	 0.563554048538208 	 None 	 None 	 None 	 None 	 
2025-08-06 02:00:01.492510 test begin: paddle.Tensor.any(Tensor([10, 1501, 184, 184],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 1501, 184, 184],"bool"), axis=list[2,3,], ) 	 508178560 	 20191 	 10.219963550567627 	 11.487061262130737 	 0.25868701934814453 	 0.5814836025238037 	 None 	 None 	 None 	 None 	 
2025-08-06 02:00:30.149058 test begin: paddle.Tensor.any(Tensor([10, 300, 184, 921],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 300, 184, 921],"bool"), axis=list[2,3,], ) 	 508392000 	 20191 	 12.887701511383057 	 10.640483140945435 	 0.3261525630950928 	 0.5385887622833252 	 None 	 None 	 None 	 None 	 
2025-08-06 02:01:01.349726 test begin: paddle.Tensor.any(Tensor([10, 300, 192, 883],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 300, 192, 883],"bool"), axis=list[2,3,], ) 	 508608000 	 20191 	 10.948706865310669 	 10.6099534034729 	 0.2770426273345947 	 0.5370988845825195 	 None 	 None 	 None 	 None 	 
2025-08-06 02:01:31.687501 test begin: paddle.Tensor.any(Tensor([10, 300, 883, 192],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 300, 883, 192],"bool"), axis=list[2,3,], ) 	 508608000 	 20191 	 10.947595834732056 	 10.603702783584595 	 0.27704572677612305 	 0.5367310047149658 	 None 	 None 	 None 	 None 	 
2025-08-06 02:02:00.722842 test begin: paddle.Tensor.any(Tensor([10, 300, 921, 184],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 300, 921, 184],"bool"), axis=list[2,3,], ) 	 508392000 	 20191 	 12.88034701347351 	 10.646090507507324 	 0.325991153717041 	 0.5388948917388916 	 None 	 None 	 None 	 None 	 
2025-08-06 02:02:31.269735 test begin: paddle.Tensor.any(Tensor([100, 300, 136, 136],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([100, 300, 136, 136],"bool"), axis=list[2,3,], ) 	 554880000 	 20191 	 10.858368873596191 	 14.407570123672485 	 0.5495941638946533 	 0.7293217182159424 	 None 	 None 	 None 	 None 	 
2025-08-06 02:03:05.509092 test begin: paddle.Tensor.any(Tensor([20, 1374, 136, 136],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([20, 1374, 136, 136],"bool"), axis=list[2,3,], ) 	 508270080 	 20191 	 9.989993572235107 	 13.213418006896973 	 0.5056533813476562 	 0.6688210964202881 	 None 	 None 	 None 	 None 	 
2025-08-06 02:03:36.789159 test begin: paddle.Tensor.any(Tensor([20, 300, 136, 623],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([20, 300, 136, 623],"bool"), axis=list[2,3,], ) 	 508368000 	 20191 	 12.523146867752075 	 10.794310331344604 	 0.31690049171447754 	 0.5463802814483643 	 None 	 None 	 None 	 None 	 
2025-08-06 02:04:07.099724 test begin: paddle.Tensor.any(Tensor([20, 300, 623, 136],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([20, 300, 623, 136],"bool"), axis=list[2,3,], ) 	 508368000 	 20191 	 12.5228910446167 	 10.7944655418396 	 0.3169221878051758 	 0.5464599132537842 	 None 	 None 	 None 	 None 	 
2025-08-06 02:04:37.590497 test begin: paddle.Tensor.any(Tensor([50, 300, 192, 192],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([50, 300, 192, 192],"bool"), axis=list[2,3,], ) 	 552960000 	 20191 	 10.832533359527588 	 12.090937614440918 	 0.27414393424987793 	 0.612025260925293 	 None 	 None 	 None 	 None 	 
2025-08-06 02:05:09.010926 test begin: paddle.Tensor.any(Tensor([60, 300, 184, 184],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([60, 300, 184, 184],"bool"), axis=list[2,3,], ) 	 609408000 	 20191 	 12.187485694885254 	 13.724165439605713 	 0.3084135055541992 	 0.6947264671325684 	 None 	 None 	 None 	 None 	 
2025-08-06 02:05:43.426666 test begin: paddle.Tensor.argmax(Tensor([13, 498, 8000],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([13, 498, 8000],"float32"), axis=2, ) 	 51792000 	 36546 	 10.162506818771362 	 6.199467182159424 	 0.284177303314209 	 0.1733384132385254 	 None 	 None 	 None 	 None 	 
2025-08-06 02:06:00.658229 test begin: paddle.Tensor.argmax(Tensor([14, 457, 8000],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([14, 457, 8000],"float32"), axis=2, ) 	 51184000 	 36546 	 10.046163082122803 	 6.136754274368286 	 0.2809417247772217 	 0.17160844802856445 	 None 	 None 	 None 	 None 	 
2025-08-06 02:06:17.682077 test begin: paddle.Tensor.argmax(Tensor([14, 477, 8000],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([14, 477, 8000],"float32"), axis=2, ) 	 53424000 	 36546 	 10.476334571838379 	 6.339749336242676 	 0.29297900199890137 	 0.17723965644836426 	 None 	 None 	 None 	 None 	 
2025-08-06 02:06:37.150932 test begin: paddle.Tensor.argmax(Tensor([30, 212, 8000],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([30, 212, 8000],"float32"), axis=2, ) 	 50880000 	 36546 	 9.993565082550049 	 6.111733913421631 	 0.27946901321411133 	 0.1709580421447754 	 None 	 None 	 None 	 None 	 
2025-08-06 02:06:54.192893 test begin: paddle.Tensor.argmax(Tensor([30, 457, 3706],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([30, 457, 3706],"float32"), axis=2, ) 	 50809260 	 36546 	 12.723510503768921 	 5.945039749145508 	 0.3558018207550049 	 0.16623616218566895 	 None 	 None 	 None 	 None 	 
2025-08-06 02:07:16.093648 test begin: paddle.Tensor.argmax(Tensor([30, 477, 3551],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([30, 477, 3551],"float32"), axis=2, ) 	 50814810 	 36546 	 13.099869966506958 	 6.1135194301605225 	 0.36634373664855957 	 0.17092394828796387 	 None 	 None 	 None 	 None 	 
2025-08-06 02:07:36.814880 test begin: paddle.Tensor.argmax(Tensor([30, 498, 3401],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([30, 498, 3401],"float32"), axis=2, ) 	 50810940 	 36546 	 13.515118837356567 	 6.03344464302063 	 0.3779573440551758 	 0.16874265670776367 	 None 	 None 	 None 	 None 	 
2025-08-06 02:07:57.216636 test begin: paddle.Tensor.astype(Tensor([10, 32, 388, 4096],"float32"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([10, 32, 388, 4096],"float32"), "float32", ) 	 508559360 	 3208126 	 16.06494402885437 	 11.055315017700195 	 0.00013971328735351562 	 0.0001277923583984375 	 110.55210709571838 	 143.67057037353516 	 0.0001087188720703125 	 0.00021529197692871094 	 
2025-08-06 02:12:58.453425 test begin: paddle.Tensor.astype(Tensor([10, 32, 4096, 388],"float32"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([10, 32, 4096, 388],"float32"), "float32", ) 	 508559360 	 3208126 	 15.829444646835327 	 10.948757410049438 	 0.00012731552124023438 	 0.0007233619689941406 	 97.06600522994995 	 181.87492895126343 	 0.0001430511474609375 	 0.00027942657470703125 	 
2025-08-06 02:18:23.384135 test begin: paddle.Tensor.astype(Tensor([10, 4, 4096, 4096],"float32"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([10, 4, 4096, 4096],"float32"), "float32", ) 	 671088640 	 3208126 	 15.948400020599365 	 11.055787563323975 	 0.0001709461212158203 	 0.00021576881408691406 	 110.38956618309021 	 146.78217911720276 	 0.0001285076141357422 	 0.0004799365997314453 	 
2025-08-06 02:23:32.468220 test begin: paddle.Tensor.astype(Tensor([100352, 1013],"bfloat16"), "float32", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7febe892af80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 02:33:41.401046 test begin: paddle.Tensor.astype(Tensor([1013, 100352],"bfloat16"), "float32", )
W0806 02:33:46.498560 119253 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6a4703f010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 02:43:48.972517 test begin: paddle.Tensor.astype(Tensor([12404, 8192],"bfloat16"), "float32", )
W0806 02:43:50.730142 119805 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb83284b070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 02:53:53.348765 test begin: paddle.Tensor.astype(Tensor([8192, 12404],"bfloat16"), "float32", )
W0806 02:53:55.054554 120340 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1fe011b010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:03:58.068411 test begin: paddle.Tensor.atanh(Tensor([1, 16934401, 3],"float32"), )
W0806 03:03:59.077004 121050 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([1, 16934401, 3],"float32"), ) 	 50803203 	 33658 	 9.989665508270264 	 10.032787322998047 	 0.30329346656799316 	 0.30459094047546387 	 15.150306463241577 	 54.61414623260498 	 0.4601156711578369 	 0.33173584938049316 	 
2025-08-06 03:05:32.796990 test begin: paddle.Tensor.atanh(Tensor([1, 2, 12700801],"float64"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([1, 2, 12700801],"float64"), ) 	 25401602 	 33658 	 14.90554404258728 	 13.702512979507446 	 0.45255517959594727 	 0.4160299301147461 	 15.088337182998657 	 54.53710651397705 	 0.4585154056549072 	 0.3312668800354004 	 
2025-08-06 03:07:12.186555 test begin: paddle.Tensor.atanh(Tensor([1, 2, 25401601],"float32"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([1, 2, 25401601],"float32"), ) 	 50803202 	 33658 	 9.987739086151123 	 12.110561847686768 	 0.30335497856140137 	 0.30453920364379883 	 15.150938272476196 	 54.61242485046387 	 0.46002721786499023 	 0.3317220211029053 	 
2025-08-06 03:08:46.647187 test begin: paddle.Tensor.atanh(Tensor([1, 8467201, 3],"float64"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([1, 8467201, 3],"float64"), ) 	 25401603 	 33658 	 14.904646635055542 	 13.701086282730103 	 0.4524710178375244 	 0.4159970283508301 	 15.08863878250122 	 54.53571701049805 	 0.4576106071472168 	 0.33119750022888184 	 
2025-08-06 03:10:26.010936 test begin: paddle.Tensor.atanh(Tensor([2, 12700801],"float64"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([2, 12700801],"float64"), ) 	 25401602 	 33658 	 15.431912899017334 	 13.69979476928711 	 0.45249152183532715 	 0.4159855842590332 	 15.085256814956665 	 54.539469480514526 	 0.4577915668487549 	 0.33124423027038574 	 
2025-08-06 03:12:06.557245 test begin: paddle.Tensor.atanh(Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([4233601, 2, 3],"float64"), ) 	 25401606 	 33658 	 14.90326452255249 	 13.713615417480469 	 0.45249056816101074 	 0.4159884452819824 	 15.088836431503296 	 54.53730225563049 	 0.458479642868042 	 0.3312509059906006 	 
2025-08-06 03:13:47.017968 test begin: paddle.Tensor.atanh(Tensor([6350401, 4],"float64"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([6350401, 4],"float64"), ) 	 25401604 	 33658 	 14.904828071594238 	 13.704766511917114 	 0.45250701904296875 	 0.41603875160217285 	 15.086217164993286 	 54.53653025627136 	 0.4576599597930908 	 0.3312869071960449 	 
2025-08-06 03:15:26.402822 test begin: paddle.Tensor.atanh(Tensor([8467201, 2, 3],"float32"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([8467201, 2, 3],"float32"), ) 	 50803206 	 33658 	 9.989552736282349 	 10.032812356948853 	 0.303281307220459 	 0.30453014373779297 	 15.151565790176392 	 54.613155364990234 	 0.46007347106933594 	 0.3317074775695801 	 
2025-08-06 03:16:58.564071 test begin: paddle.Tensor.bmm(Tensor([1, 16934401, 3],"float32"), Tensor([1, 3, 2],"float32"), )
[Prof] paddle.Tensor.bmm 	 paddle.Tensor.bmm(Tensor([1, 16934401, 3],"float32"), Tensor([1, 3, 2],"float32"), ) 	 50803209 	 42736 	 76.08182120323181 	 76.09162497520447 	 0.10693120956420898 	 0.10695409774780273 	 176.99759769439697 	 176.87280678749084 	 0.22318196296691895 	 0.2208704948425293 	 
2025-08-06 03:25:26.198737 test begin: paddle.Tensor.bmm(Tensor([1, 170476, 299],"float32"), Tensor([1, 299, 2],"float32"), )
[Prof] paddle.Tensor.bmm 	 paddle.Tensor.bmm(Tensor([1, 170476, 299],"float32"), Tensor([1, 299, 2],"float32"), ) 	 50972922 	 42736 	 10.227525234222412 	 10.225436925888062 	 0.24454021453857422 	 0.2445235252380371 	 18.103681325912476 	 18.070953607559204 	 0.14427924156188965 	 0.1439824104309082 	 
2025-08-06 03:26:23.928225 test begin: paddle.Tensor.bmm(Tensor([1, 179876, 283],"float32"), Tensor([1, 283, 2],"float32"), )
[Prof] paddle.Tensor.bmm 	 paddle.Tensor.bmm(Tensor([1, 179876, 283],"float32"), Tensor([1, 283, 2],"float32"), ) 	 50905474 	 42736 	 10.305037498474121 	 10.300305128097534 	 0.2464001178741455 	 0.24630379676818848 	 17.840887308120728 	 17.80046033859253 	 0.14217734336853027 	 0.1418764591217041 	 
2025-08-06 03:27:21.098366 test begin: paddle.Tensor.bmm(Tensor([1, 191277, 266],"float32"), Tensor([1, 266, 2],"float32"), )
[Prof] paddle.Tensor.bmm 	 paddle.Tensor.bmm(Tensor([1, 191277, 266],"float32"), Tensor([1, 266, 2],"float32"), ) 	 50880214 	 42736 	 9.991500854492188 	 9.991254806518555 	 0.23891687393188477 	 0.23888134956359863 	 17.989962577819824 	 17.96979069709778 	 0.14340829849243164 	 0.14309453964233398 	 
2025-08-06 03:28:17.894484 test begin: paddle.Tensor.bmm(Tensor([100, 170476, 3],"float32"), Tensor([100, 3, 2],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe78cd62aa0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:38:47.668851 test begin: paddle.Tensor.bmm(Tensor([89, 191277, 3],"float32"), Tensor([89, 3, 2],"float32"), )
W0806 03:38:48.732760 122796 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2aa821efb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:48:52.135067 test begin: paddle.Tensor.bmm(Tensor([95, 179876, 3],"float32"), Tensor([95, 3, 2],"float32"), )
W0806 03:48:53.250119 123199 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5d1021ed70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:58:57.302974 test begin: paddle.Tensor.cast(Tensor([128256, 793],"float16"), Dtype(float16), )
W0806 03:58:59.396008 123591 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([128256, 793],"float16"), Dtype(float16), ) 	 101707008 	 32559 	 10.200408220291138 	 0.07456111907958984 	 0.16013026237487793 	 2.1457672119140625e-05 	 10.19752311706543 	 1.5301129817962646 	 0.1600322723388672 	 9.512901306152344e-05 	 combined
2025-08-06 03:59:23.796692 test begin: paddle.Tensor.cast(Tensor([152064, 669],"float16"), Dtype(float16), )
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([152064, 669],"float16"), Dtype(float16), ) 	 101730816 	 32559 	 10.1983962059021 	 0.06721973419189453 	 0.1600487232208252 	 1.9311904907226562e-05 	 10.200488328933716 	 1.4434535503387451 	 0.16005277633666992 	 0.00018453598022460938 	 combined
2025-08-06 03:59:50.133505 test begin: paddle.Tensor.cast(Tensor([24807, 4096],"float16"), Dtype(float16), )
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([24807, 4096],"float16"), Dtype(float16), ) 	 101609472 	 32559 	 10.131858587265015 	 0.09417247772216797 	 0.31803464889526367 	 5.936622619628906e-05 	 10.090471982955933 	 1.4517598152160645 	 0.3167397975921631 	 0.00011610984802246094 	 combined
2025-08-06 04:00:15.704944 test begin: paddle.Tensor.cast(Tensor([28351, 3584],"float16"), Dtype(float16), )
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([28351, 3584],"float16"), Dtype(float16), ) 	 101609984 	 32559 	 10.184425354003906 	 0.09242749214172363 	 0.15983796119689941 	 2.3126602172851562e-05 	 10.187023878097534 	 1.4700517654418945 	 0.1598649024963379 	 7.200241088867188e-05 	 combined
2025-08-06 04:00:42.782358 test begin: paddle.Tensor.cast(Tensor([3584, 28351],"float16"), Dtype(float16), )
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([3584, 28351],"float16"), Dtype(float16), ) 	 101609984 	 32559 	 11.394034147262573 	 0.06782317161560059 	 0.15984272956848145 	 6.556510925292969e-05 	 10.187110900878906 	 1.4745597839355469 	 0.15989136695861816 	 9.441375732421875e-05 	 combined
2025-08-06 04:01:13.528202 test begin: paddle.Tensor.cast(Tensor([669, 152064],"float16"), Dtype(float16), )
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([669, 152064],"float16"), Dtype(float16), ) 	 101730816 	 32559 	 10.19778823852539 	 0.0675666332244873 	 0.1600346565246582 	 2.384185791015625e-05 	 10.200128316879272 	 1.4563384056091309 	 0.1600487232208252 	 7.486343383789062e-05 	 combined
2025-08-06 04:01:42.316048 test begin: paddle.Tensor.ceil(Tensor([1, 50803201],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([1, 50803201],"float32"), ) 	 50803201 	 33828 	 10.009334564208984 	 10.098494529724121 	 0.3023238182067871 	 0.3042640686035156 	 4.533510684967041 	 4.534426212310791 	 0.13686537742614746 	 0.13691043853759766 	 
2025-08-06 04:02:15.516913 test begin: paddle.Tensor.ceil(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33828 	 10.009147644042969 	 10.070531845092773 	 0.3024117946624756 	 0.3042576313018799 	 4.532297134399414 	 4.542850971221924 	 0.13686800003051758 	 0.13698554039001465 	 
2025-08-06 04:02:46.343377 test begin: paddle.Tensor.ceil(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33828 	 10.007189273834229 	 10.31324553489685 	 0.30228328704833984 	 0.30423855781555176 	 4.533241271972656 	 4.537768363952637 	 0.13690543174743652 	 0.13702940940856934 	 
2025-08-06 04:03:19.042620 test begin: paddle.Tensor.ceil(Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([10, 5080321],"float32"), ) 	 50803210 	 33828 	 10.004582405090332 	 10.07073426246643 	 0.30229902267456055 	 0.30435729026794434 	 4.533969163894653 	 4.538385391235352 	 0.13698768615722656 	 0.137007474899292 	 
2025-08-06 04:03:50.021444 test begin: paddle.Tensor.ceil(Tensor([25401601, 2],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([25401601, 2],"float32"), ) 	 50803202 	 33828 	 10.006013870239258 	 10.070676326751709 	 0.3022911548614502 	 0.3043179512023926 	 4.533185958862305 	 4.542779207229614 	 0.13691067695617676 	 0.13702940940856934 	 
2025-08-06 04:04:20.973786 test begin: paddle.Tensor.ceil(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33828 	 10.00621747970581 	 10.076536655426025 	 0.30222201347351074 	 0.3042032718658447 	 4.5339624881744385 	 4.53816294670105 	 0.1369032859802246 	 0.13701462745666504 	 
2025-08-06 04:04:54.163969 test begin: paddle.Tensor.ceil(Tensor([2540161, 20],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([2540161, 20],"float32"), ) 	 50803220 	 33828 	 10.004555940628052 	 10.07037878036499 	 0.3022429943084717 	 0.3042912483215332 	 4.533183813095093 	 4.5418267250061035 	 0.13689732551574707 	 0.13704180717468262 	 
2025-08-06 04:05:27.016878 test begin: paddle.Tensor.chunk(Tensor([1034, 32, 64, 48],"float16"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([1034, 32, 64, 48],"float16"), 2, axis=1, ) 	 101646336 	 29186 	 13.380868673324585 	 0.20237088203430176 	 0.46854400634765625 	 5.6743621826171875e-05 	 8.99447250366211 	 13.195903062820435 	 0.3150010108947754 	 0.46211957931518555 	 
2025-08-06 04:06:06.607122 test begin: paddle.Tensor.chunk(Tensor([128, 2068, 192],"float32"), 3, axis=-1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([128, 2068, 192],"float32"), 3, axis=-1, ) 	 50823168 	 29186 	 10.057322978973389 	 0.2324380874633789 	 0.35222959518432617 	 3.0517578125e-05 	 9.00633716583252 	 9.014536142349243 	 0.31525754928588867 	 0.31569695472717285 	 
2025-08-06 04:06:40.281985 test begin: paddle.Tensor.chunk(Tensor([512, 32, 130, 48],"float16"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([512, 32, 130, 48],"float16"), 2, axis=1, ) 	 102236160 	 29186 	 13.297795534133911 	 0.20343279838562012 	 0.46553921699523926 	 3.647804260253906e-05 	 9.132896423339844 	 13.276982545852661 	 0.31981968879699707 	 0.464946985244751 	 
2025-08-06 04:07:20.097076 test begin: paddle.Tensor.chunk(Tensor([512, 32, 64, 49],"float32"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([512, 32, 64, 49],"float32"), 2, axis=1, ) 	 51380224 	 29186 	 10.250085353851318 	 0.2065896987915039 	 0.3589193820953369 	 7.677078247070312e-05 	 9.177942514419556 	 9.140022039413452 	 0.32139062881469727 	 0.3200652599334717 	 
2025-08-06 04:07:55.888332 test begin: paddle.Tensor.chunk(Tensor([512, 32, 64, 97],"float16"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([512, 32, 64, 97],"float16"), 2, axis=1, ) 	 101711872 	 29186 	 13.271042108535767 	 0.20224595069885254 	 0.4641754627227783 	 2.6941299438476562e-05 	 9.097270727157593 	 13.195350408554077 	 0.31847500801086426 	 0.4621272087097168 	 
2025-08-06 04:08:40.555891 test begin: paddle.Tensor.chunk(Tensor([512, 32, 65, 48],"float32"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([512, 32, 65, 48],"float32"), 2, axis=1, ) 	 51118080 	 29186 	 10.25162124633789 	 0.20529866218566895 	 0.3582770824432373 	 3.4332275390625e-05 	 9.130002975463867 	 9.088049411773682 	 0.3197762966156006 	 0.31827569007873535 	 
2025-08-06 04:09:13.962512 test begin: paddle.Tensor.chunk(Tensor([517, 32, 64, 48],"float32"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([517, 32, 64, 48],"float32"), 2, axis=1, ) 	 50823168 	 29186 	 10.203470706939697 	 0.20802927017211914 	 0.3572525978088379 	 0.00012826919555664062 	 9.00672459602356 	 9.086784362792969 	 0.31533169746398926 	 0.3181459903717041 	 
2025-08-06 04:09:45.246315 test begin: paddle.Tensor.chunk(Tensor([85, 3136, 192],"float32"), 3, axis=-1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([85, 3136, 192],"float32"), 3, axis=-1, ) 	 51179520 	 29186 	 10.108347415924072 	 0.23153924942016602 	 0.35317206382751465 	 3.361701965332031e-05 	 9.063833236694336 	 9.074097633361816 	 0.31737589836120605 	 0.31772470474243164 	 
2025-08-06 04:10:18.205686 test begin: paddle.Tensor.clip(Tensor([1, 386, 65856, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([1, 386, 65856, 2],"float32"), 0, ) 	 50840832 	 33863 	 10.013314485549927 	 10.088330268859863 	 0.30220913887023926 	 0.30445265769958496 	 15.25877833366394 	 20.175737619400024 	 0.4605278968811035 	 0.20313572883605957 	 
2025-08-06 04:11:19.695931 test begin: paddle.Tensor.clip(Tensor([1, 400, 63505, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([1, 400, 63505, 2],"float32"), 0, ) 	 50804000 	 33863 	 11.083979606628418 	 10.085497617721558 	 0.3018989562988281 	 0.304241418838501 	 15.246963739395142 	 20.163957357406616 	 0.46023988723754883 	 0.2030477523803711 	 
2025-08-06 04:12:19.584411 test begin: paddle.Tensor.clip(Tensor([1, 400, 65856, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([1, 400, 65856, 2],"float32"), 0, ) 	 52684800 	 33863 	 10.38207221031189 	 10.446546077728271 	 0.3133220672607422 	 0.3152806758880615 	 15.802120447158813 	 20.873638153076172 	 0.4769876003265381 	 0.21019744873046875 	 
2025-08-06 04:13:19.838741 test begin: paddle.Tensor.clip(Tensor([2100, 12096, 3],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([2100, 12096, 3],"float32"), 0, ) 	 76204800 	 33863 	 14.955002784729004 	 15.032002925872803 	 0.45130085945129395 	 0.4536550045013428 	 22.784162044525146 	 29.955257415771484 	 0.6878013610839844 	 0.30161166191101074 	 
2025-08-06 04:14:45.917540 test begin: paddle.Tensor.clip(Tensor([2100, 12097, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([2100, 12097, 2],"float32"), 0, ) 	 50807400 	 33863 	 10.008232355117798 	 10.081321239471436 	 0.3020601272583008 	 0.30420494079589844 	 15.241351127624512 	 20.158108234405518 	 0.4600372314453125 	 0.20296907424926758 	 
2025-08-06 04:15:43.182266 test begin: paddle.Tensor.clip(Tensor([2101, 12096, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([2101, 12096, 2],"float32"), 0, ) 	 50827392 	 33863 	 10.009236335754395 	 10.08530855178833 	 0.3019859790802002 	 0.3043797016143799 	 15.246383428573608 	 20.162931203842163 	 0.4600379467010498 	 0.20304560661315918 	 
2025-08-06 04:16:40.535509 test begin: paddle.Tensor.clip(Tensor([4, 525, 12096, 3],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([4, 525, 12096, 3],"float32"), 0, ) 	 76204800 	 33863 	 14.952003955841064 	 15.030604362487793 	 0.4513416290283203 	 0.4536144733428955 	 22.77762770652771 	 29.953317165374756 	 0.6874372959136963 	 0.30159568786621094 	 
2025-08-06 04:18:08.307711 test begin: paddle.Tensor.clip(Tensor([4, 525, 12097, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([4, 525, 12097, 2],"float32"), 0, ) 	 50807400 	 33863 	 10.008329391479492 	 11.441680431365967 	 0.30205488204956055 	 0.3042144775390625 	 15.239198446273804 	 20.16242003440857 	 0.4599268436431885 	 0.2029564380645752 	 
2025-08-06 04:19:07.621768 test begin: paddle.Tensor.clip(Tensor([4, 526, 12096, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([4, 526, 12096, 2],"float32"), 0, ) 	 50899968 	 33863 	 10.033723592758179 	 10.101312160491943 	 0.3028099536895752 	 0.3048245906829834 	 15.272197723388672 	 20.19071102142334 	 0.46085691452026367 	 0.20296359062194824 	 
2025-08-06 04:20:05.516835 test begin: paddle.Tensor.clip(Tensor([5, 525, 12096, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([5, 525, 12096, 2],"float32"), 0, ) 	 63504000 	 33863 	 12.465864419937134 	 14.009082317352295 	 0.37627220153808594 	 0.3789951801300049 	 19.009357690811157 	 25.053155183792114 	 0.57364821434021 	 0.252241849899292 	 
2025-08-06 04:21:20.004204 test begin: paddle.Tensor.clone(Tensor([3544, 32, 896],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([3544, 32, 896],"bfloat16"), ) 	 101613568 	 32273 	 10.055317401885986 	 10.00715947151184 	 0.3184034824371338 	 0.3167903423309326 	 19.846491813659668 	 14.636392831802368 	 0.6284947395324707 	 0.46335721015930176 	 
2025-08-06 04:22:18.058973 test begin: paddle.Tensor.clone(Tensor([6017, 19, 896],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6017, 19, 896],"bfloat16"), ) 	 102433408 	 32273 	 10.252576112747192 	 10.179135084152222 	 0.16231584548950195 	 0.16115856170654297 	 20.108267784118652 	 14.753556966781616 	 0.31836891174316406 	 0.46718907356262207 	 
2025-08-06 04:23:16.763748 test begin: paddle.Tensor.clone(Tensor([6017, 32, 528],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6017, 32, 528],"bfloat16"), ) 	 101663232 	 32273 	 10.101370811462402 	 10.095289468765259 	 0.15991926193237305 	 0.15982890129089355 	 19.95341157913208 	 14.642283916473389 	 0.31595373153686523 	 0.4636704921722412 	 
2025-08-06 04:24:14.903043 test begin: paddle.Tensor.clone(Tensor([6036, 19, 896],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6036, 19, 896],"bfloat16"), ) 	 102756864 	 32273 	 10.208004474639893 	 10.203612089157104 	 0.16164159774780273 	 0.16154718399047852 	 20.172887325286865 	 14.799309968948364 	 0.3193843364715576 	 0.4686615467071533 	 
2025-08-06 04:25:13.888487 test begin: paddle.Tensor.clone(Tensor([6036, 32, 527],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6036, 32, 527],"bfloat16"), ) 	 101791104 	 32273 	 10.186840534210205 	 10.120870590209961 	 0.16118431091308594 	 0.16025567054748535 	 19.9819552898407 	 14.662244319915771 	 0.3163869380950928 	 0.4643254280090332 	 
2025-08-06 04:26:14.861049 test begin: paddle.Tensor.clone(Tensor([6078, 19, 896],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6078, 19, 896],"bfloat16"), ) 	 103471872 	 32273 	 10.099164962768555 	 10.278537034988403 	 0.1598799228668213 	 0.16274333000183105 	 20.310616731643677 	 14.901720762252808 	 0.3215920925140381 	 0.4718964099884033 	 
2025-08-06 04:27:14.020391 test begin: paddle.Tensor.clone(Tensor([6078, 32, 523],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6078, 32, 523],"bfloat16"), ) 	 101721408 	 32273 	 10.170076847076416 	 10.116304397583008 	 0.16100573539733887 	 0.1601424217224121 	 19.97062063217163 	 14.652347564697266 	 0.3161952495574951 	 0.4640216827392578 	 
2025-08-06 04:28:13.504133 test begin: paddle.Tensor.conj(Tensor([10, 2540161],"float64"), )
[Prof] paddle.Tensor.conj 	 paddle.Tensor.conj(Tensor([10, 2540161],"float64"), ) 	 25401610 	 33610 	 9.999926328659058 	 0.05519819259643555 	 0.30387091636657715 	 2.2649765014648438e-05 	 9.97181510925293 	 1.465684413909912 	 0.3031961917877197 	 7.2479248046875e-05 	 
2025-08-06 04:28:38.507278 test begin: paddle.Tensor.conj(Tensor([1270081, 20],"float64"), )
[Prof] paddle.Tensor.conj 	 paddle.Tensor.conj(Tensor([1270081, 20],"float64"), ) 	 25401620 	 33610 	 11.240600824356079 	 0.05595135688781738 	 0.3038778305053711 	 5.364418029785156e-05 	 9.971801042556763 	 1.4734909534454346 	 0.30325984954833984 	 9.989738464355469e-05 	 
2025-08-06 04:29:02.858138 test begin: paddle.Tensor.cos(Tensor([131072, 388],"float32"), )
[Prof] paddle.Tensor.cos 	 paddle.Tensor.cos(Tensor([131072, 388],"float32"), ) 	 50855936 	 33870 	 10.017541885375977 	 10.103337526321411 	 0.302262544631958 	 0.3049156665802002 	 15.267769575119019 	 35.29105734825134 	 0.460801362991333 	 0.3550083637237549 	 
2025-08-06 04:30:17.768284 test begin: paddle.Tensor.cos(Tensor([3175201, 16],"float32"), )
[Prof] paddle.Tensor.cos 	 paddle.Tensor.cos(Tensor([3175201, 16],"float32"), ) 	 50803216 	 33870 	 10.010952711105347 	 10.095986604690552 	 0.30207324028015137 	 0.3045947551727295 	 15.250418901443481 	 35.255497455596924 	 0.4600677490234375 	 0.35462164878845215 	 
2025-08-06 04:31:30.105442 test begin: paddle.Tensor.cos(Tensor([32768, 1551],"float32"), )
[Prof] paddle.Tensor.cos 	 paddle.Tensor.cos(Tensor([32768, 1551],"float32"), ) 	 50823168 	 33870 	 10.01303482055664 	 10.097150802612305 	 0.30213141441345215 	 0.3046741485595703 	 15.259047746658325 	 35.26905083656311 	 0.46036624908447266 	 0.35480284690856934 	 
2025-08-06 04:32:43.831358 test begin: paddle.Tensor.cos(Tensor([396901, 128],"float32"), )
[Prof] paddle.Tensor.cos 	 paddle.Tensor.cos(Tensor([396901, 128],"float32"), ) 	 50803328 	 33870 	 10.006355285644531 	 10.093485593795776 	 0.30196595191955566 	 0.30458879470825195 	 15.247566223144531 	 35.256123542785645 	 0.4599885940551758 	 0.35466766357421875 	 
2025-08-06 04:33:56.629575 test begin: paddle.Tensor.cumprod(Tensor([25401601],"float64"), -1, )
[Prof] paddle.Tensor.cumprod 	 paddle.Tensor.cumprod(Tensor([25401601],"float64"), -1, ) 	 25401601 	 30751 	 9.998529195785522 	 10.069685459136963 	 0.16607069969177246 	 0.16721224784851074 	 86.10477304458618 	 63.13912010192871 	 0.000293731689453125 	 0.001302480697631836 	 
2025-08-06 04:36:47.877159 test begin: paddle.Tensor.cumprod(Tensor([50803201],"float32"), -1, )
[Prof] paddle.Tensor.cumprod 	 paddle.Tensor.cumprod(Tensor([50803201],"float32"), -1, ) 	 50803201 	 30751 	 10.026153564453125 	 10.146288871765137 	 0.1665651798248291 	 0.1683802604675293 	 96.13450503349304 	 65.06182622909546 	 0.0002932548522949219 	 0.0013275146484375 	 
2025-08-06 04:39:52.931251 test begin: paddle.Tensor.cumsum(Tensor([1, 144, 352801],"float32"), 1, )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([1, 144, 352801],"float32"), 1, ) 	 50803344 	 28938 	 36.25774145126343 	 10.742333173751831 	 0.42679357528686523 	 0.3611264228820801 	 185.06684231758118 	 28.281067371368408 	 1.3674144744873047 	 0.33289456367492676 	 
2025-08-06 04:44:16.528004 test begin: paddle.Tensor.cumsum(Tensor([1, 144, 352801],"float32"), 2, )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([1, 144, 352801],"float32"), 2, ) 	 50803344 	 28938 	 25.691882610321045 	 29.41129779815674 	 0.907181978225708 	 1.0386979579925537 	 48.74374747276306 	 47.48906064033508 	 0.5736067295074463 	 0.5593166351318359 	 
2025-08-06 04:46:49.708824 test begin: paddle.Tensor.cumsum(Tensor([1, 254017, 200],"float32"), 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f561fc0e980>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 04:59:01.639406 test begin: paddle.Tensor.cumsum(Tensor([1, 254017, 200],"float32"), 2, )
W0806 04:59:03.443756 125147 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([1, 254017, 200],"float32"), 2, ) 	 50803400 	 28938 	 11.715741872787476 	 81.5836443901062 	 0.413646936416626 	 2.881307601928711 	 120.35922694206238 	 99.62092852592468 	 1.4163234233856201 	 1.1735568046569824 	 
2025-08-06 05:04:21.183648 test begin: paddle.Tensor.cumsum(Tensor([1765, 144, 200],"float32"), 1, )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([1765, 144, 200],"float32"), 1, ) 	 50832000 	 28938 	 37.25245547294617 	 10.47632884979248 	 0.43851161003112793 	 0.3699159622192383 	 186.31434559822083 	 28.749019145965576 	 1.316446304321289 	 0.33847856521606445 	 
2025-08-06 05:08:45.723049 test begin: paddle.Tensor.cumsum(Tensor([1765, 144, 200],"float32"), 2, )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([1765, 144, 200],"float32"), 2, ) 	 50832000 	 28938 	 11.71710467338562 	 81.63700199127197 	 0.41356754302978516 	 2.8835325241088867 	 120.25561690330505 	 99.74367094039917 	 1.4150505065917969 	 1.1750388145446777 	 
2025-08-06 05:14:02.245495 test begin: paddle.Tensor.cumsum(Tensor([211681, 120],"int64"), )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([211681, 120],"int64"), ) 	 25401720 	 28938 	 9.984721899032593 	 9.461961269378662 	 9.274482727050781e-05 	 0.16709494590759277 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:14:34.145077 test begin: paddle.Tensor.cumsum(Tensor([300, 84673],"int64"), )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([300, 84673],"int64"), ) 	 25401900 	 28938 	 10.563532829284668 	 9.461867094039917 	 9.894371032714844e-05 	 0.16710472106933594 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:15:08.049478 test begin: paddle.Tensor.detach(Tensor([1003520, 1013],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([1003520, 1013],"bfloat16"), ) 	 1016565760 	 12614448 	 9.458468198776245 	 36.134161949157715 	 0.00015044212341308594 	 0.0003037452697753906 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:21:53.286343 test begin: paddle.Tensor.detach(Tensor([10130, 100352],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([10130, 100352],"bfloat16"), ) 	 1016565760 	 12614448 	 9.54785442352295 	 36.47752332687378 	 7.867813110351562e-05 	 0.0002846717834472656 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:28:42.583856 test begin: paddle.Tensor.detach(Tensor([124040, 8192],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([124040, 8192],"bfloat16"), ) 	 1016135680 	 12614448 	 9.252235412597656 	 36.66008543968201 	 6.198883056640625e-05 	 0.0002865791320800781 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:35:30.216902 test begin: paddle.Tensor.detach(Tensor([17720, 57344],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([17720, 57344],"bfloat16"), ) 	 1016135680 	 12614448 	 9.377800464630127 	 36.437039375305176 	 6.985664367675781e-05 	 0.0002777576446533203 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:42:20.124954 test begin: paddle.Tensor.detach(Tensor([81920, 12404],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([81920, 12404],"bfloat16"), ) 	 1016135680 	 12614448 	 9.302860736846924 	 36.10726523399353 	 8.463859558105469e-05 	 0.00028443336486816406 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 05:49:06.673823 test begin: paddle.Tensor.diag_embed(Tensor([1, 25401601, 2],"float32"), )
[Prof] paddle.Tensor.diag_embed 	 paddle.Tensor.diag_embed(Tensor([1, 25401601, 2],"float32"), ) 	 50803202 	 5658 	 18.995420694351196 	 5.733147859573364 	 9.34600830078125e-05 	 0.5177953243255615 	 None 	 None 	 None 	 None 	 
2025-08-06 05:49:32.259576 test begin: paddle.Tensor.diag_embed(Tensor([25401601, 1, 2],"float32"), )
[Prof] paddle.Tensor.diag_embed 	 paddle.Tensor.diag_embed(Tensor([25401601, 1, 2],"float32"), ) 	 50803202 	 5658 	 20.94709324836731 	 5.739196538925171 	 9.298324584960938e-05 	 0.5177216529846191 	 None 	 None 	 None 	 None 	 
2025-08-06 05:49:59.806579 test begin: paddle.Tensor.diagonal(Tensor([301, 84672],"float64"), axis1=-2, axis2=-1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcd3c10aa70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 06:00:04.496515 test begin: paddle.Tensor.diagonal(Tensor([8467201, 3],"float64"), axis1=-2, axis2=-1, )
W0806 06:00:05.232015 126791 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5f44277010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 06:10:09.090014 test begin: paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), )
W0806 06:10:09.784507 127199 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 12395 	 11.705461263656616 	 3.240191698074341 	 0.3217477798461914 	 0.267031192779541 	 None 	 None 	 None 	 None 	 
2025-08-06 06:10:24.986952 test begin: paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=-2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=-2, ) 	 25401664 	 12395 	 11.69615364074707 	 3.259671211242676 	 0.32144689559936523 	 0.2671058177947998 	 None 	 None 	 None 	 None 	 
2025-08-06 06:10:41.990940 test begin: paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=2, ) 	 25401664 	 12395 	 11.705195426940918 	 3.245570182800293 	 0.32144594192504883 	 0.26709771156311035 	 None 	 None 	 None 	 None 	 
2025-08-06 06:10:57.580253 test begin: paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 12395 	 10.794859647750854 	 3.257619857788086 	 0.2966752052307129 	 0.2671034336090088 	 None 	 None 	 None 	 None 	 
2025-08-06 06:11:13.730452 test begin: paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=-2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=-2, ) 	 25401664 	 12395 	 10.8040931224823 	 3.2396888732910156 	 0.29671382904052734 	 0.2671816349029541 	 None 	 None 	 None 	 None 	 
2025-08-06 06:11:29.652603 test begin: paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=2, ) 	 25401664 	 12395 	 10.795153856277466 	 3.2494215965270996 	 0.2967257499694824 	 0.26711487770080566 	 None 	 None 	 None 	 None 	 
2025-08-06 06:11:44.820610 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 12395 	 10.79443359375 	 3.2433505058288574 	 0.2967360019683838 	 0.2671067714691162 	 None 	 None 	 None 	 None 	 
2025-08-06 06:11:59.432295 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=-2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=-2, ) 	 25401664 	 12395 	 13.276730298995972 	 3.7086267471313477 	 0.36492323875427246 	 0.3058280944824219 	 None 	 None 	 None 	 None 	 
2025-08-06 06:12:18.104175 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=2, ) 	 25401664 	 12395 	 13.279989957809448 	 3.709014415740967 	 0.3648543357849121 	 0.3057746887207031 	 None 	 None 	 None 	 None 	 
2025-08-06 06:12:37.877752 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 12395 	 13.31458830833435 	 3.7090749740600586 	 0.3648569583892822 	 0.3059091567993164 	 None 	 None 	 None 	 None 	 
2025-08-06 06:12:58.106628 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=-2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=-2, ) 	 25401664 	 12395 	 10.004507780075073 	 3.2553818225860596 	 0.27505946159362793 	 0.268402099609375 	 None 	 None 	 None 	 None 	 
2025-08-06 06:13:11.982627 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=2, ) 	 25401664 	 12395 	 10.003586053848267 	 3.255413770675659 	 0.27489137649536133 	 0.26844096183776855 	 None 	 None 	 None 	 None 	 
2025-08-06 06:13:26.500479 test begin: paddle.Tensor.digamma(Tensor([4, 6350401],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([4, 6350401],"float64"), ) 	 25401604 	 8544 	 10.018595933914185 	 9.763325452804565 	 1.2186791896820068 	 1.1677672863006592 	 73.10268688201904 	 9.270546913146973 	 8.743966341018677 	 0.5543508529663086 	 
2025-08-06 06:15:10.739808 test begin: paddle.Tensor.digamma(Tensor([453601, 7, 8],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([453601, 7, 8],"float64"), ) 	 25401656 	 8544 	 9.993745565414429 	 9.767557382583618 	 1.1954576969146729 	 1.1676788330078125 	 73.09554505348206 	 9.26938009262085 	 8.74336290359497 	 0.5543618202209473 	 
2025-08-06 06:16:56.663802 test begin: paddle.Tensor.digamma(Tensor([45361, 7, 8, 10],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([45361, 7, 8, 10],"float64"), ) 	 25402160 	 8544 	 9.997219562530518 	 9.756562232971191 	 1.1967957019805908 	 1.1670811176300049 	 73.11891269683838 	 9.267272233963013 	 8.746486902236938 	 0.5542619228363037 	 
2025-08-06 06:18:44.008322 test begin: paddle.Tensor.digamma(Tensor([5, 635041, 8],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5, 635041, 8],"float64"), ) 	 25401640 	 8544 	 9.992907524108887 	 9.758446455001831 	 1.1948373317718506 	 1.16731858253479 	 73.09936952590942 	 9.268069982528687 	 8.744544267654419 	 0.5543017387390137 	 
2025-08-06 06:20:29.452464 test begin: paddle.Tensor.digamma(Tensor([5, 63505, 8, 10],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5, 63505, 8, 10],"float64"), ) 	 25402000 	 8544 	 9.991097211837769 	 9.759040117263794 	 1.1951067447662354 	 1.1668555736541748 	 73.10426831245422 	 9.266330003738403 	 8.745418548583984 	 0.5541801452636719 	 
2025-08-06 06:22:13.314523 test begin: paddle.Tensor.digamma(Tensor([5, 7, 725761],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5, 7, 725761],"float64"), ) 	 25401635 	 8544 	 9.992258071899414 	 10.375643730163574 	 1.1945698261260986 	 1.1670169830322266 	 73.08566355705261 	 9.269139289855957 	 8.74221420288086 	 0.5551278591156006 	 
2025-08-06 06:23:59.647350 test begin: paddle.Tensor.digamma(Tensor([5, 7, 72577, 10],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5, 7, 72577, 10],"float64"), ) 	 25401950 	 8544 	 10.002887964248657 	 9.754474878311157 	 1.1953327655792236 	 1.1667957305908203 	 73.06970834732056 	 9.272611618041992 	 8.73985767364502 	 0.5555558204650879 	 
2025-08-06 06:25:43.367004 test begin: paddle.Tensor.digamma(Tensor([5, 7, 8, 90721],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5, 7, 8, 90721],"float64"), ) 	 25401880 	 8544 	 9.993915796279907 	 9.75676703453064 	 1.1944329738616943 	 1.1668906211853027 	 73.07017374038696 	 9.270236730575562 	 8.740793943405151 	 0.5542280673980713 	 
2025-08-06 06:27:28.339233 test begin: paddle.Tensor.digamma(Tensor([5080321, 5],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5080321, 5],"float64"), ) 	 25401605 	 8544 	 9.996659994125366 	 9.76157546043396 	 1.195246696472168 	 1.1667780876159668 	 73.13023066520691 	 9.266307830810547 	 8.747463464736938 	 0.554185152053833 	 
2025-08-06 06:29:12.203112 test begin: paddle.Tensor.dim(Tensor([1116160, 911],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([1116160, 911],"bfloat16"), ) 	 1016821760 	 14433255 	 10.274211406707764 	 25.299463987350464 	 0.0001583099365234375 	 0.00035071372985839844 	 None 	 None 	 None 	 None 	 
2025-08-06 06:30:07.898437 test begin: paddle.Tensor.dim(Tensor([124040, 8192],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([124040, 8192],"bfloat16"), ) 	 1016135680 	 14433255 	 10.139782905578613 	 25.43810749053955 	 6.580352783203125e-05 	 0.00035381317138671875 	 None 	 None 	 None 	 None 	 
2025-08-06 06:31:02.141961 test begin: paddle.Tensor.dim(Tensor([141760, 7168],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([141760, 7168],"bfloat16"), ) 	 1016135680 	 14433255 	 10.076135873794556 	 22.692360162734985 	 0.00014901161193847656 	 0.00029277801513671875 	 None 	 None 	 None 	 None 	 
2025-08-06 06:31:54.490487 test begin: paddle.Tensor.dim(Tensor([71680, 14176],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([71680, 14176],"bfloat16"), ) 	 1016135680 	 14433255 	 10.051365852355957 	 23.32931685447693 	 9.036064147949219e-05 	 0.0003001689910888672 	 None 	 None 	 None 	 None 	 
2025-08-06 06:32:47.110714 test begin: paddle.Tensor.dim(Tensor([9110, 111616],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([9110, 111616],"bfloat16"), ) 	 1016821760 	 14433255 	 11.083329916000366 	 23.22282099723816 	 8.320808410644531e-05 	 0.0003044605255126953 	 None 	 None 	 None 	 None 	 
2025-08-06 06:33:41.978739 test begin: paddle.Tensor.dim(Tensor([958720, 1060],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([958720, 1060],"bfloat16"), ) 	 1016243200 	 14433255 	 10.148173332214355 	 23.0345196723938 	 6.246566772460938e-05 	 0.0002942085266113281 	 None 	 None 	 None 	 None 	 
2025-08-06 06:34:34.463890 test begin: paddle.Tensor.dot(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
Warning: The core code of paddle.Tensor.dot is too complex.
[Prof] paddle.Tensor.dot 	 paddle.Tensor.dot(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 33757 	 9.983102321624756 	 9.907751560211182 	 0.30219006538391113 	 0.15001678466796875 	 23.890944004058838 	 20.381807565689087 	 0.3615734577178955 	 0.3085300922393799 	 
2025-08-06 06:35:42.719034 test begin: paddle.Tensor.equal(Tensor([128, 198451],"int64"), Tensor([128, 198451],"int64"), )
[Prof] paddle.Tensor.equal 	 paddle.Tensor.equal(Tensor([128, 198451],"int64"), Tensor([128, 198451],"int64"), ) 	 50803456 	 56162 	 17.32876205444336 	 17.58548092842102 	 0.3153417110443115 	 0.31981444358825684 	 None 	 None 	 None 	 None 	 
2025-08-06 06:36:18.774300 test begin: paddle.Tensor.equal(Tensor([198451, 128],"int64"), Tensor([198451, 128],"int64"), )
[Prof] paddle.Tensor.equal 	 paddle.Tensor.equal(Tensor([198451, 128],"int64"), Tensor([198451, 128],"int64"), ) 	 50803456 	 56162 	 17.32826328277588 	 19.081104278564453 	 0.3153214454650879 	 0.31981778144836426 	 None 	 None 	 None 	 None 	 
2025-08-06 06:36:58.621676 test begin: paddle.Tensor.equal(Tensor([2, 12700801],"int64"), 3, )
[Prof] paddle.Tensor.equal 	 paddle.Tensor.equal(Tensor([2, 12700801],"int64"), 3, ) 	 25401602 	 56162 	 10.012634992599487 	 9.440296173095703 	 0.09106087684631348 	 0.1717522144317627 	 None 	 None 	 None 	 None 	 
2025-08-06 06:37:18.618182 test begin: paddle.Tensor.equal(Tensor([2540161, 10],"int64"), 3, )
[Prof] paddle.Tensor.equal 	 paddle.Tensor.equal(Tensor([2540161, 10],"int64"), 3, ) 	 25401610 	 56162 	 10.012609004974365 	 9.443122148513794 	 0.09108686447143555 	 0.17178869247436523 	 None 	 None 	 None 	 None 	 
2025-08-06 06:37:39.112342 test begin: paddle.Tensor.equal_all(Tensor([25401601],"int64"), Tensor([25401601],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([25401601],"int64"), Tensor([25401601],"int64"), ) 	 50803202 	 612074 	 209.44794487953186 	 232.17725110054016 	 0.11635589599609375 	 0.00029850006103515625 	 None 	 None 	 None 	 None 	 
2025-08-06 06:45:02.333688 test begin: paddle.Tensor.equal_all(Tensor([25401601],"int64"), Tensor([801],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([25401601],"int64"), Tensor([801],"int64"), ) 	 25402402 	 612074 	 13.749630689620972 	 2.9154088497161865 	 0.00011301040649414062 	 0.00015616416931152344 	 None 	 None 	 None 	 None 	 
2025-08-06 06:45:19.611167 test begin: paddle.Tensor.equal_all(Tensor([8, 3175201],"int64"), Tensor([8, 3175201],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8, 3175201],"int64"), Tensor([8, 3175201],"int64"), ) 	 50803216 	 612074 	 209.44264912605286 	 231.54374146461487 	 0.11636734008789062 	 0.00028204917907714844 	 None 	 None 	 None 	 None 	 
2025-08-06 06:52:43.479145 test begin: paddle.Tensor.equal_all(Tensor([801, 3175201],"int64"), Tensor([801, 3],"int64"), )
[Error] CUDA out of memory. Tried to allocate 18.95 GiB. GPU 0 has a total capacity of 39.39 GiB of which 18.03 GiB is free. Process 150701 has 21.35 GiB memory in use. Of the allocated memory 8.12 MiB is allocated by PyTorch, and 11.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-06 06:53:33.173469 test begin: paddle.Tensor.equal_all(Tensor([801, 3],"int64"), Tensor([801, 3175201],"int64"), )
W0806 06:54:03.453260 128600 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([801, 3],"int64"), Tensor([801, 3175201],"int64"), ) 	 2543338404 	 612074 	 10.123789548873901 	 1.8648080825805664 	 7.176399230957031e-05 	 8.320808410644531e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 06:54:26.494911 test begin: paddle.Tensor.equal_all(Tensor([801, 3],"int64"), Tensor([8467201, 3],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([801, 3],"int64"), Tensor([8467201, 3],"int64"), ) 	 25404006 	 612074 	 10.02568769454956 	 1.6638169288635254 	 0.0001671314239501953 	 8.58306884765625e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 06:54:40.303854 test begin: paddle.Tensor.equal_all(Tensor([801],"int64"), Tensor([25401601],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([801],"int64"), Tensor([25401601],"int64"), ) 	 25402402 	 612074 	 10.09318494796753 	 1.652707576751709 	 0.00010156631469726562 	 7.605552673339844e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 06:54:54.465830 test begin: paddle.Tensor.equal_all(Tensor([8467201, 3],"int64"), Tensor([801, 3],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8467201, 3],"int64"), Tensor([801, 3],"int64"), ) 	 25404006 	 612074 	 10.069708824157715 	 1.6492080688476562 	 0.00011801719665527344 	 0.00011515617370605469 	 None 	 None 	 None 	 None 	 
2025-08-06 06:55:06.675976 test begin: paddle.Tensor.equal_all(Tensor([8467201, 3],"int64"), Tensor([8467201, 3],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8467201, 3],"int64"), Tensor([8467201, 3],"int64"), ) 	 50803206 	 612074 	 208.47383427619934 	 230.82822799682617 	 0.11582446098327637 	 0.000240325927734375 	 None 	 None 	 None 	 None 	 
2025-08-06 07:02:29.970321 test begin: paddle.Tensor.equal_all(Tensor([846720101, 3],"int64"), Tensor([8, 3],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([846720101, 3],"int64"), Tensor([8, 3],"int64"), ) 	 2540160327 	 612074 	 10.091681241989136 	 1.7063279151916504 	 4.5299530029296875e-05 	 7.748603820800781e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 07:03:22.004042 test begin: paddle.Tensor.equal_all(Tensor([8],"int64"), Tensor([2540160101],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8],"int64"), Tensor([2540160101],"int64"), ) 	 2540160109 	 612074 	 10.044186115264893 	 1.6517298221588135 	 6.747245788574219e-05 	 7.557868957519531e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 07:04:16.561981 test begin: paddle.Tensor.erfinv(x=Tensor([211681, 2, 3, 5, 4],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([211681, 2, 3, 5, 4],"float64"), ) 	 25401720 	 30679 	 10.202757120132446 	 9.535585403442383 	 0.3419616222381592 	 0.3174436092376709 	 13.747422933578491 	 50.38639736175537 	 0.4579033851623535 	 0.3357517719268799 	 
2025-08-06 07:05:45.019248 test begin: paddle.Tensor.erfinv(x=Tensor([4, 105841, 3, 5, 4],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 105841, 3, 5, 4],"float64"), ) 	 25401840 	 30679 	 10.318232297897339 	 9.595298528671265 	 0.3445453643798828 	 0.3201279640197754 	 13.746156692504883 	 50.39443874359131 	 0.45793890953063965 	 0.33615565299987793 	 
2025-08-06 07:07:10.238680 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 158761, 5, 4],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 158761, 5, 4],"float64"), ) 	 25401760 	 30679 	 10.337761640548706 	 9.620986223220825 	 0.34508252143859863 	 0.3188323974609375 	 13.73200011253357 	 50.42059898376465 	 0.4574000835418701 	 0.3358907699584961 	 
2025-08-06 07:08:39.655549 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 1058401],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 1058401],"float64"), ) 	 25401624 	 30679 	 10.329721689224243 	 9.601215124130249 	 0.3444802761077881 	 0.3207101821899414 	 13.732326984405518 	 50.42681932449341 	 0.45748353004455566 	 0.3358769416809082 	 
2025-08-06 07:10:08.167478 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 264601, 4],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 264601, 4],"float64"), ) 	 25401696 	 30679 	 10.445834875106812 	 9.717122316360474 	 0.34822583198547363 	 0.32416653633117676 	 13.729387760162354 	 50.42634391784668 	 0.45737504959106445 	 0.3360602855682373 	 
2025-08-06 07:11:34.784971 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 5, 211681],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 5, 211681],"float64"), ) 	 25401720 	 30679 	 10.444486618041992 	 9.682137489318848 	 0.3455212116241455 	 0.3227388858795166 	 13.747353792190552 	 50.4205276966095 	 0.4579472541809082 	 0.3359642028808594 	 
2025-08-06 07:13:00.247072 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 3175201],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 3175201],"float64"), ) 	 25401608 	 30679 	 10.514375686645508 	 9.701038599014282 	 0.3491666316986084 	 0.323606014251709 	 13.732322692871094 	 50.42298102378845 	 0.45740842819213867 	 0.3360469341278076 	 
2025-08-06 07:14:30.030405 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 635041, 5],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 635041, 5],"float64"), ) 	 25401640 	 30679 	 10.43385362625122 	 9.702585697174072 	 0.34583115577697754 	 0.3230905532836914 	 13.734976291656494 	 50.42446184158325 	 0.4575026035308838 	 0.3360316753387451 	 
2025-08-06 07:15:56.836438 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2116801, 3],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2116801, 3],"float64"), ) 	 25401612 	 30679 	 10.485704898834229 	 9.690863132476807 	 0.3481616973876953 	 0.3229694366455078 	 13.732479572296143 	 50.4197199344635 	 0.4574880599975586 	 0.3360280990600586 	 
2025-08-06 07:17:22.343928 test begin: paddle.Tensor.erfinv(x=Tensor([4, 423361, 3, 5],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 423361, 3, 5],"float64"), ) 	 25401660 	 30679 	 10.489301443099976 	 9.746964931488037 	 0.3484628200531006 	 0.32441258430480957 	 13.734939336776733 	 50.420767068862915 	 0.45754075050354004 	 0.33597230911254883 	 
2025-08-06 07:18:47.927342 test begin: paddle.Tensor.erfinv(x=Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4233601, 2, 3],"float64"), ) 	 25401606 	 30679 	 10.436673402786255 	 9.655864715576172 	 0.3466663360595703 	 0.3224658966064453 	 13.73310375213623 	 50.425562381744385 	 0.4574286937713623 	 0.3360128402709961 	 
2025-08-06 07:20:13.340129 test begin: paddle.Tensor.erfinv(x=Tensor([846721, 2, 3, 5],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([846721, 2, 3, 5],"float64"), ) 	 25401630 	 30679 	 10.408456563949585 	 10.738354921340942 	 0.3472151756286621 	 0.3196558952331543 	 13.73252010345459 	 50.431519508361816 	 0.4575035572052002 	 0.33613061904907227 	 
2025-08-06 07:21:43.272243 test begin: paddle.Tensor.exp(Tensor([1000000, 26],"float64"), )
[Prof] paddle.Tensor.exp 	 paddle.Tensor.exp(Tensor([1000000, 26],"float64"), ) 	 26000000 	 33844 	 10.357119083404541 	 10.41873836517334 	 0.31278085708618164 	 0.31398677825927734 	 15.531982898712158 	 15.386466264724731 	 0.46913814544677734 	 0.4646146297454834 	 
2025-08-06 07:22:39.011810 test begin: paddle.Tensor.exp(Tensor([2540161, 20],"float32"), )
[Prof] paddle.Tensor.exp 	 paddle.Tensor.exp(Tensor([2540161, 20],"float32"), ) 	 50803220 	 33844 	 9.998367547988892 	 10.071013450622559 	 0.30194544792175293 	 0.3041098117828369 	 15.21527361869812 	 15.10929012298584 	 0.45938634872436523 	 0.45621156692504883 	 
2025-08-06 07:23:31.172053 test begin: paddle.Tensor.exp(Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.exp 	 paddle.Tensor.exp(Tensor([50803201],"float32"), ) 	 50803201 	 33844 	 9.998302459716797 	 10.071299314498901 	 0.30197930335998535 	 0.30412888526916504 	 15.216593265533447 	 15.109222650527954 	 0.4595048427581787 	 0.4562258720397949 	 
2025-08-06 07:24:25.309021 test begin: paddle.Tensor.exp(Tensor([6350401, 4],"float64"), )
[Prof] paddle.Tensor.exp 	 paddle.Tensor.exp(Tensor([6350401, 4],"float64"), ) 	 25401604 	 33844 	 10.131664276123047 	 10.169819831848145 	 0.3059866428375244 	 0.30687737464904785 	 15.165711879730225 	 15.037510395050049 	 0.4579455852508545 	 0.4541008472442627 	 
2025-08-06 07:25:16.966685 test begin: paddle.Tensor.exp(Tensor([64, 793801],"float32"), )
[Prof] paddle.Tensor.exp 	 paddle.Tensor.exp(Tensor([64, 793801],"float32"), ) 	 50803264 	 33844 	 9.998368501663208 	 10.073787927627563 	 0.301922082901001 	 0.30413222312927246 	 15.215098142623901 	 15.108962535858154 	 0.4594612121582031 	 0.45627832412719727 	 
2025-08-06 07:26:09.309350 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 266, 477, 401],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 266, 477, 401],"float32"), ) 	 50879683 	 74165 	 10.024530172348022 	 0.5731191635131836 	 0.1381077766418457 	 0.00010275840759277344 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:26:32.677756 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 283, 466, 386],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 283, 466, 386],"float32"), ) 	 50904909 	 74165 	 10.03182077407837 	 0.5736606121063232 	 0.13808202743530273 	 5.269050598144531e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:26:56.088841 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 299, 391, 436],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 299, 391, 436],"float32"), ) 	 50972325 	 74165 	 10.027862071990967 	 0.307330846786499 	 0.13944196701049805 	 4.363059997558594e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:27:19.208852 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 38841, 436],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 38841, 436],"float32"), ) 	 50804029 	 74165 	 10.002276420593262 	 0.3064005374908447 	 0.13783979415893555 	 3.218650817871094e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:27:42.286962 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 391, 43311],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 391, 43311],"float32"), ) 	 50803804 	 74165 	 10.00655746459961 	 0.30558061599731445 	 0.13783597946166992 	 6.67572021484375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:28:05.420686 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 42231, 401],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 42231, 401],"float32"), ) 	 50803894 	 74165 	 10.002597093582153 	 0.3056340217590332 	 0.13786673545837402 	 0.0001552104949951172 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:28:28.565121 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 43872, 386],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 43872, 386],"float32"), ) 	 50803777 	 74165 	 9.998528957366943 	 0.5685522556304932 	 0.13774442672729492 	 4.57763671875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:28:51.955417 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 466, 36340],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 466, 36340],"float32"), ) 	 50803321 	 74165 	 10.008401155471802 	 0.5829863548278809 	 0.1378040313720703 	 7.939338684082031e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:29:15.513790 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 477, 35502],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 477, 35502],"float32"), ) 	 50803363 	 74165 	 9.998865127563477 	 0.4882848262786865 	 0.13777470588684082 	 0.0002315044403076172 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:29:42.461702 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([100, 3, 391, 436],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([100, 3, 391, 436],"float32"), ) 	 51142801 	 74165 	 10.072258710861206 	 0.31418275833129883 	 0.13873910903930664 	 7.462501525878906e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:30:08.340007 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([89, 3, 477, 401],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([89, 3, 477, 401],"float32"), ) 	 51070960 	 74165 	 10.061375141143799 	 0.3037564754486084 	 0.13857150077819824 	 2.4080276489257812e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:30:31.536737 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([95, 3, 466, 386],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([95, 3, 466, 386],"float32"), ) 	 51264661 	 74165 	 10.09501838684082 	 0.3099629878997803 	 0.1389920711517334 	 5.054473876953125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 07:30:54.861481 test begin: paddle.Tensor.fill_(Tensor([50803201],"float32"), 0, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(Tensor([50803201],"float32"), 0, ) 	 50803201 	 68510 	 9.921207189559937 	 9.193871259689331 	 0.14788818359375 	 0.1369800567626953 	 None 	 None 	 None 	 None 	 
2025-08-06 07:31:20.792091 test begin: paddle.Tensor.fill_(Tensor([659782, 77],"float32"), value=-math.inf, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(Tensor([659782, 77],"float32"), value=-math.inf, ) 	 50803214 	 68510 	 9.920811176300049 	 9.194028854370117 	 0.14805960655212402 	 0.13696742057800293 	 None 	 None 	 None 	 None 	 
2025-08-06 07:31:43.925256 test begin: paddle.Tensor.fill_(Tensor([77, 659782],"float32"), value=-math.inf, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(Tensor([77, 659782],"float32"), value=-math.inf, ) 	 50803214 	 68510 	 9.922782182693481 	 9.185094356536865 	 0.1480114459991455 	 0.13696956634521484 	 None 	 None 	 None 	 None 	 
2025-08-06 07:32:07.063399 test begin: paddle.Tensor.fill_(x=Tensor([10, 158761, 16],"float64"), value=41.2, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(x=Tensor([10, 158761, 16],"float64"), value=41.2, ) 	 25401760 	 68510 	 10.375245094299316 	 9.220743417739868 	 0.15460634231567383 	 0.13739013671875 	 None 	 None 	 None 	 None 	 
2025-08-06 07:32:30.097758 test begin: paddle.Tensor.fill_(x=Tensor([10, 16, 158761],"float64"), value=41.2, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(x=Tensor([10, 16, 158761],"float64"), value=41.2, ) 	 25401760 	 68510 	 10.37263035774231 	 9.212310075759888 	 0.15479445457458496 	 0.13743877410888672 	 None 	 None 	 None 	 None 	 
2025-08-06 07:32:53.161035 test begin: paddle.Tensor.fill_(x=Tensor([99226, 16, 16],"float64"), value=41.2, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(x=Tensor([99226, 16, 16],"float64"), value=41.2, ) 	 25401856 	 68510 	 10.192211627960205 	 9.211705446243286 	 0.15203166007995605 	 0.13738369941711426 	 None 	 None 	 None 	 None 	 
2025-08-06 07:33:15.962324 test begin: paddle.Tensor.fill_diagonal_(Tensor([1280, 396901],"float32"), 0, wrap=False, )
[Prof] paddle.Tensor.fill_diagonal_ 	 paddle.Tensor.fill_diagonal_(Tensor([1280, 396901],"float32"), 0, wrap=False, ) 	 508033280 	 432242 	 9.749254703521729 	 4.6434526443481445 	 9.417533874511719e-05 	 7.200241088867188e-05 	 14.171513557434082 	 18.65148901939392 	 0.00010895729064941406 	 0.000217437744140625 	 combined
2025-08-06 07:34:20.076777 test begin: paddle.Tensor.fill_diagonal_(Tensor([3969010, 128],"float32"), 0, wrap=False, )
[Prof] paddle.Tensor.fill_diagonal_ 	 paddle.Tensor.fill_diagonal_(Tensor([3969010, 128],"float32"), 0, wrap=False, ) 	 508033280 	 432242 	 9.703539848327637 	 4.599483966827393 	 8.654594421386719e-05 	 6.890296936035156e-05 	 14.266777992248535 	 18.756645917892456 	 9.870529174804688e-05 	 0.00021028518676757812 	 combined
2025-08-06 07:35:24.497492 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([12700801, 4, 7],"int32"), Tensor([12700801, 4],"int32"), 0, 1, 2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc4a26af100>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 07:45:29.289927 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([1814401, 4, 7],"int32"), Tensor([1814401, 4],"int32"), 0, 1, 2, )
W0806 07:45:30.090118 130158 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([1814401, 4, 7],"int32"), Tensor([1814401, 4],"int32"), 0, 1, 2, ) 	 58060832 	 31092 	 144.56652092933655 	 19.587241888046265 	 0.0015172958374023438 	 0.21450352668762207 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 07:50:41.327917 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4, 3175201],"int64"), Tensor([2, 4],"int64"), 0, 1, 2, )
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4, 3175201],"int64"), Tensor([2, 4],"int64"), 0, 1, 2, ) 	 25401616 	 31092 	 9.917848825454712 	 9.834318161010742 	 0.08128809928894043 	 0.10754704475402832 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 07:51:12.250082 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4, 6350401],"int32"), Tensor([2, 4],"int32"), 0, 1, 2, )
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4, 6350401],"int32"), Tensor([2, 4],"int32"), 0, 1, 2, ) 	 50803216 	 31092 	 9.91079568862915 	 9.83413314819336 	 0.0813145637512207 	 0.10752606391906738 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 07:51:43.347915 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4233601, 3, 2],"int32"), Tensor([2, 2, 3],"int32"), offset=0, dim1=1, dim2=2, )
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4233601, 3, 2],"int32"), Tensor([2, 2, 3],"int32"), offset=0, dim1=1, dim2=2, ) 	 50803224 	 31092 	 9.911051988601685 	 9.835040807723999 	 0.08126068115234375 	 0.1075143814086914 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 07:52:14.495129 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([6350401, 4, 7],"int64"), Tensor([6350401, 4],"int64"), 0, 1, 2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f15fe44b100>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 08:02:19.240442 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([907201, 4, 7],"int64"), Tensor([907201, 4],"int64"), 0, 1, 2, )
W0806 08:02:21.801916 130861 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([907201, 4, 7],"int64"), Tensor([907201, 4],"int64"), 0, 1, 2, ) 	 29030432 	 31092 	 71.2076268196106 	 16.10312533378601 	 0.0007393360137939453 	 0.17636489868164062 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 08:05:04.057217 test begin: paddle.Tensor.flatten(Tensor([10, 64, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([10, 64, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, ) 	 1684480000 	 1816345 	 14.949182271957397 	 10.349711179733276 	 0.00014972686767578125 	 0.0001266002655029297 	 92.51499915122986 	 119.91645407676697 	 0.0001423358917236328 	 0.0006892681121826172 	 
2025-08-06 08:10:10.557928 test begin: paddle.Tensor.flatten(Tensor([1280, 127, 56, 56],"float32"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 127, 56, 56],"float32"), 2, ) 	 509788160 	 1816345 	 10.90178894996643 	 7.850767135620117 	 0.0003046989440917969 	 0.00024080276489257812 	 78.30306148529053 	 96.54279518127441 	 0.0001227855682373047 	 0.0005764961242675781 	 
2025-08-06 08:13:42.854495 test begin: paddle.Tensor.flatten(Tensor([1280, 254, 56, 56],"float16"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 254, 56, 56],"float16"), 2, ) 	 1019576320 	 1816345 	 9.725275754928589 	 7.815966367721558 	 9.942054748535156e-05 	 0.00012969970703125 	 77.95917677879333 	 95.64865303039551 	 0.0001010894775390625 	 0.0002231597900390625 	 
2025-08-06 08:17:34.162101 test begin: paddle.Tensor.flatten(Tensor([1280, 512, 14, 56],"float32"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 512, 14, 56],"float32"), 2, ) 	 513802240 	 1816345 	 9.71175765991211 	 8.592774629592896 	 0.0001201629638671875 	 0.00015687942504882812 	 77.86751413345337 	 97.88282871246338 	 0.00011086463928222656 	 0.00022101402282714844 	 
2025-08-06 08:21:07.291207 test begin: paddle.Tensor.flatten(Tensor([1280, 512, 28, 56],"float16"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 512, 28, 56],"float16"), 2, ) 	 1027604480 	 1816345 	 9.782025337219238 	 7.89236044883728 	 0.00011444091796875 	 0.00013113021850585938 	 77.79754185676575 	 96.20348739624023 	 0.00010991096496582031 	 0.00022077560424804688 	 
2025-08-06 08:25:02.637096 test begin: paddle.Tensor.flatten(Tensor([1280, 512, 56, 14],"float32"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 512, 56, 14],"float32"), 2, ) 	 513802240 	 1816345 	 18.832826852798462 	 7.926194906234741 	 0.0003428459167480469 	 0.00011157989501953125 	 78.62678170204163 	 100.6353394985199 	 0.00011229515075683594 	 0.00025153160095214844 	 
2025-08-06 08:28:46.944237 test begin: paddle.Tensor.flatten(Tensor([1280, 512, 56, 28],"float16"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1280, 512, 56, 28],"float16"), 2, ) 	 1027604480 	 1816345 	 9.795363664627075 	 7.942538261413574 	 0.00011467933654785156 	 0.00028014183044433594 	 78.42991638183594 	 96.55961918830872 	 0.0001163482666015625 	 0.00022649765014648438 	 
2025-08-06 08:32:38.200387 test begin: paddle.Tensor.flatten(Tensor([320, 512, 56, 56],"float32"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([320, 512, 56, 56],"float32"), 2, ) 	 513802240 	 1816345 	 18.923511743545532 	 11.516352891921997 	 0.000301361083984375 	 0.0002655982971191406 	 91.85557460784912 	 122.44541716575623 	 0.0001239776611328125 	 0.0005562305450439453 	 
2025-08-06 08:36:59.479891 test begin: paddle.Tensor.flatten(Tensor([40, 5, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([40, 5, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, ) 	 526400000 	 1816345 	 10.579874038696289 	 8.17691946029663 	 0.0002703666687011719 	 0.00011944770812988281 	 78.26122093200684 	 98.19185781478882 	 0.0001049041748046875 	 0.0006139278411865234 	 
2025-08-06 08:40:33.690441 test begin: paddle.Tensor.flatten(Tensor([40, 64, 2, 376, 280],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([40, 64, 2, 376, 280],"float32"), start_axis=1, stop_axis=2, ) 	 539033600 	 1816345 	 18.439498901367188 	 8.2289719581604 	 0.00015020370483398438 	 0.00027298927307128906 	 77.92455530166626 	 116.93484091758728 	 0.00018715858459472656 	 0.0006592273712158203 	 
2025-08-06 08:44:35.354856 test begin: paddle.Tensor.flatten(Tensor([40, 64, 25, 29, 280],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([40, 64, 25, 29, 280],"float32"), start_axis=1, stop_axis=2, ) 	 519680000 	 1816345 	 11.454363107681274 	 8.19973635673523 	 0.00027179718017578125 	 0.00011515617370605469 	 81.68523097038269 	 97.88218951225281 	 0.00012731552124023438 	 0.0006368160247802734 	 
2025-08-06 08:48:12.788894 test begin: paddle.Tensor.flatten(Tensor([40, 64, 25, 376, 22],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([40, 64, 25, 376, 22],"float32"), start_axis=1, stop_axis=2, ) 	 529408000 	 1816345 	 11.527389764785767 	 8.229573488235474 	 0.00010752677917480469 	 8.7738037109375e-05 	 79.77328944206238 	 98.02113318443298 	 0.00010085105895996094 	 0.00022101402282714844 	 
2025-08-06 08:51:53.137343 test begin: paddle.Tensor.flatten(Tensor([640, 512, 56, 56],"float16"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([640, 512, 56, 56],"float16"), 2, ) 	 1027604480 	 1816345 	 10.882467269897461 	 7.857343912124634 	 0.0001316070556640625 	 8.58306884765625e-05 	 77.14332556724548 	 95.97141885757446 	 9.942054748535156e-05 	 0.00022172927856445312 	 
2025-08-06 08:55:49.538326 test begin: paddle.Tensor.flip(Tensor([16, 3, 224, 4726],"float32"), 0, )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([16, 3, 224, 4726],"float32"), 0, ) 	 50813952 	 11097 	 10.700312852859497 	 3.4539995193481445 	 0.9854609966278076 	 0.3180525302886963 	 10.678436994552612 	 3.4527528285980225 	 0.983405590057373 	 0.3179800510406494 	 
2025-08-06 08:56:19.571649 test begin: paddle.Tensor.flip(Tensor([16, 3, 4726, 224],"float32"), 0, )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([16, 3, 4726, 224],"float32"), 0, ) 	 50813952 	 11097 	 10.699122667312622 	 3.476158380508423 	 0.9855461120605469 	 0.31804943084716797 	 10.676471471786499 	 3.453096866607666 	 0.9832556247711182 	 0.3180396556854248 	 
2025-08-06 08:56:50.449067 test begin: paddle.Tensor.flip(Tensor([16, 64, 224, 224],"float32"), 0, )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([16, 64, 224, 224],"float32"), 0, ) 	 51380224 	 11097 	 10.804232835769653 	 3.4957997798919678 	 0.995011568069458 	 0.3219282627105713 	 10.783965826034546 	 3.4956743717193604 	 0.9930810928344727 	 0.3219447135925293 	 
2025-08-06 08:57:20.743525 test begin: paddle.Tensor.flip(Tensor([3, 400, 42337],"float32"), axis=list[-1,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([3, 400, 42337],"float32"), axis=list[-1,], ) 	 50804400 	 11097 	 10.052512168884277 	 3.5035383701324463 	 0.9257843494415283 	 0.3193228244781494 	 9.998813152313232 	 3.4664785861968994 	 0.9208676815032959 	 0.31925439834594727 	 
2025-08-06 08:57:56.312620 test begin: paddle.Tensor.flip(Tensor([3, 400, 42337],"float32"), axis=list[-2,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([3, 400, 42337],"float32"), axis=list[-2,], ) 	 50804400 	 11097 	 10.052087545394897 	 3.5083324909210205 	 0.9257850646972656 	 0.3228914737701416 	 9.999396562576294 	 3.5055432319641113 	 0.9209249019622803 	 0.32281947135925293 	 
2025-08-06 08:58:25.331532 test begin: paddle.Tensor.flip(Tensor([3, 56449, 300],"float32"), axis=list[-1,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([3, 56449, 300],"float32"), axis=list[-1,], ) 	 50804100 	 11097 	 10.053986310958862 	 3.4773800373077393 	 0.925990104675293 	 0.3193995952606201 	 10.000728607177734 	 3.468038320541382 	 0.9210186004638672 	 0.31937479972839355 	 
2025-08-06 08:58:54.061808 test begin: paddle.Tensor.flip(Tensor([3, 56449, 300],"float32"), axis=list[-2,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([3, 56449, 300],"float32"), axis=list[-2,], ) 	 50804100 	 11097 	 10.055307626724243 	 3.5026352405548096 	 0.926093339920044 	 0.32261061668395996 	 10.000187397003174 	 3.501631736755371 	 0.9209690093994141 	 0.3224642276763916 	 
2025-08-06 08:59:22.944345 test begin: paddle.Tensor.flip(Tensor([338, 3, 224, 224],"float32"), 0, )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([338, 3, 224, 224],"float32"), 0, ) 	 50878464 	 11097 	 10.70411205291748 	 3.471745252609253 	 0.9858226776123047 	 0.3188588619232178 	 10.677955865859985 	 3.4620516300201416 	 0.9833366870880127 	 0.31884169578552246 	 
2025-08-06 08:59:53.129774 test begin: paddle.Tensor.flip(Tensor([424, 400, 300],"float32"), axis=list[-1,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([424, 400, 300],"float32"), axis=list[-1,], ) 	 50880000 	 11097 	 10.07533597946167 	 3.5076160430908203 	 0.9279091358184814 	 0.3198812007904053 	 10.007001399993896 	 3.4733529090881348 	 0.9216411113739014 	 0.319744348526001 	 
2025-08-06 09:00:24.605703 test begin: paddle.Tensor.flip(Tensor([424, 400, 300],"float32"), axis=list[-2,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([424, 400, 300],"float32"), axis=list[-2,], ) 	 50880000 	 11097 	 10.076571702957153 	 3.5148422718048096 	 0.9280331134796143 	 0.3234739303588867 	 10.008971452713013 	 3.5117299556732178 	 0.921821117401123 	 0.3233640193939209 	 
2025-08-06 09:00:53.619470 test begin: paddle.Tensor.floor(Tensor([12700801, 4],"float32"), )
[Prof] paddle.Tensor.floor 	 paddle.Tensor.floor(Tensor([12700801, 4],"float32"), ) 	 50803204 	 33839 	 10.003988027572632 	 10.07858681678772 	 0.30217432975769043 	 0.30434083938598633 	 4.534831285476685 	 4.542918920516968 	 0.13786840438842773 	 0.1369624137878418 	 
2025-08-06 09:01:27.663501 test begin: paddle.Tensor.floor(Tensor([1857, 27358],"float32"), )
[Prof] paddle.Tensor.floor 	 paddle.Tensor.floor(Tensor([1857, 27358],"float32"), ) 	 50803806 	 33839 	 9.99669885635376 	 10.088441133499146 	 0.30197739601135254 	 0.30439281463623047 	 4.535423517227173 	 4.539129972457886 	 0.13799500465393066 	 0.13699054718017578 	 
2025-08-06 09:01:59.287705 test begin: paddle.Tensor.floor(Tensor([1872, 27139],"float32"), )
[Prof] paddle.Tensor.floor 	 paddle.Tensor.floor(Tensor([1872, 27139],"float32"), ) 	 50804208 	 33839 	 10.007820129394531 	 10.077847719192505 	 0.30226922035217285 	 0.3043801784515381 	 4.53127121925354 	 4.538041591644287 	 0.1367628574371338 	 0.13696885108947754 	 
2025-08-06 09:02:32.454837 test begin: paddle.Tensor.floor(Tensor([1915, 26530],"float32"), )
[Prof] paddle.Tensor.floor 	 paddle.Tensor.floor(Tensor([1915, 26530],"float32"), ) 	 50804950 	 33839 	 10.010376214981079 	 10.076460838317871 	 0.3023033142089844 	 0.3043091297149658 	 4.529460906982422 	 4.53874945640564 	 0.13679933547973633 	 0.13697457313537598 	 
2025-08-06 09:03:03.357898 test begin: paddle.Tensor.gather(Tensor([40, 12700801],"float32"), Tensor([40, 1],"int64"), 1, )
[Prof] paddle.Tensor.gather 	 paddle.Tensor.gather(Tensor([40, 12700801],"float32"), Tensor([40, 1],"int64"), 1, ) 	 508032080 	 72474 	 0.7122955322265625 	 94.5001528263092 	 7.843971252441406e-05 	 0.0005846023559570312 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 09:06:41.809214 test begin: paddle.Tensor.gather(Tensor([400, 1270080],"float32"), Tensor([400, 1],"int64"), 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f220c4eceb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:16:47.085522 test begin: paddle.Tensor.gather(Tensor([4000, 127008],"float32"), Tensor([4000, 1],"int64"), 1, )
W0806 09:16:54.947889 133861 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f590b84eef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:26:51.741803 test begin: paddle.Tensor.gather_nd(Tensor([11, 53, 8],"float32"), Tensor([40, 50, 2],"int64"), )
W0806 09:26:51.962409 134233 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([11, 53, 8],"float32"), Tensor([40, 50, 2],"int64"), ) 	 8664 	 1730 	 0.01843571662902832 	 287.2406313419342 	 1.5020370483398438e-05 	 0.0003428459167480469 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 09:31:40.013575 test begin: paddle.Tensor.gather_nd(Tensor([16, 3, 15, 80, 8],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([16, 3, 15, 80, 8],"float32"), Tensor([516, 4],"int64"), ) 	 462864 	 1730 	 0.018218994140625 	 133.0708417892456 	 1.3113021850585938e-05 	 0.00022721290588378906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 09:33:58.669206 test begin: paddle.Tensor.gather_nd(Tensor([16, 3, 80, 156, 85],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([16, 3, 80, 156, 85],"float32"), Tensor([516, 4],"int64"), ) 	 50920464 	 1730 	 0.03160381317138672 	 136.53024172782898 	 1.2874603271484375e-05 	 0.00021910667419433594 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 09:36:16.556275 test begin: paddle.Tensor.gather_nd(Tensor([16, 3, 80, 80, 166],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([16, 3, 80, 80, 166],"float32"), Tensor([516, 4],"int64"), ) 	 50997264 	 1730 	 0.01812577247619629 	 145.57641291618347 	 1.2159347534179688e-05 	 0.0006690025329589844 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 09:38:43.328063 test begin: paddle.Tensor.gather_nd(Tensor([16, 6, 80, 80, 85],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([16, 6, 80, 80, 85],"float32"), Tensor([516, 4],"int64"), ) 	 52226064 	 1730 	 0.0182955265045166 	 131.82231664657593 	 1.5497207641601562e-05 	 0.0006237030029296875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 09:40:56.409052 test begin: paddle.Tensor.gather_nd(Tensor([32, 3, 80, 80, 85],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([32, 3, 80, 80, 85],"float32"), Tensor([385, 4],"int64"), ) 	 52225540 	 1730 	 0.03127598762512207 	 120.37612056732178 	 1.239776611328125e-05 	 0.00018978118896484375 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 09:42:58.060617 test begin: paddle.Tensor.gather_nd(Tensor([32, 3, 80, 80, 85],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([32, 3, 80, 80, 85],"float32"), Tensor([516, 4],"int64"), ) 	 52226064 	 1730 	 0.03139925003051758 	 138.3020794391632 	 1.3828277587890625e-05 	 0.0002231597900390625 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 09:45:17.662441 test begin: paddle.Tensor.gather_nd(Tensor([8, 12, 80, 80, 85],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([8, 12, 80, 80, 85],"float32"), Tensor([385, 4],"int64"), ) 	 52225540 	 1730 	 0.03213763236999512 	 98.50610756874084 	 2.1696090698242188e-05 	 0.00021791458129882812 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 09:47:00.075206 test begin: paddle.Tensor.gather_nd(Tensor([8, 3, 312, 80, 85],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([8, 3, 312, 80, 85],"float32"), Tensor([385, 4],"int64"), ) 	 50919940 	 1730 	 0.03219270706176758 	 110.84931659698486 	 1.5020370483398438e-05 	 9.179115295410156e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 09:48:54.387899 test begin: paddle.Tensor.gather_nd(Tensor([8, 3, 80, 312, 85],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([8, 3, 80, 312, 85],"float32"), Tensor([385, 4],"int64"), ) 	 50919940 	 1730 	 0.018331050872802734 	 98.66177034378052 	 1.1205673217773438e-05 	 0.0002110004425048828 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 09:50:34.423045 test begin: paddle.Tensor.gather_nd(Tensor([8, 3, 80, 80, 331],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([8, 3, 80, 80, 331],"float32"), Tensor([385, 4],"int64"), ) 	 50843140 	 1730 	 0.032230377197265625 	 106.96966671943665 	 1.430511474609375e-05 	 0.0002181529998779297 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-08-06 09:52:28.443093 test begin: paddle.Tensor.gcd(x=Tensor([127008, 2, 4, 5],"int32"), y=Tensor([127008, 2, 4, 5],"int32"), )
W0806 09:52:41.283560 135249 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.Tensor.gcd 	 paddle.Tensor.gcd(x=Tensor([127008, 2, 4, 5],"int32"), y=Tensor([127008, 2, 4, 5],"int32"), ) 	 10160640 	 1000 	 12.43468713760376 	 0.1741635799407959 	 4.4345855712890625e-05 	 0.14080429077148438 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 09:52:43.766229 test begin: paddle.Tensor.gcd(x=Tensor([2, 4, 635040],"int32"), y=Tensor([2, 4, 635040],"int32"), )
W0806 09:52:56.765503 135256 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.Tensor.gcd 	 paddle.Tensor.gcd(x=Tensor([2, 4, 635040],"int32"), y=Tensor([2, 4, 635040],"int32"), ) 	 10160640 	 1000 	 12.454864263534546 	 0.15851950645446777 	 4.982948303222656e-05 	 0.14126300811767578 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 09:52:56.987132 test begin: paddle.Tensor.gcd(x=Tensor([2, 508032, 5],"int32"), y=Tensor([2, 508032, 5],"int32"), )
W0806 09:53:10.148942 135257 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.Tensor.gcd 	 paddle.Tensor.gcd(x=Tensor([2, 508032, 5],"int32"), y=Tensor([2, 508032, 5],"int32"), ) 	 10160640 	 1000 	 12.99696683883667 	 0.158829927444458 	 5.53131103515625e-05 	 0.14146852493286133 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 09:53:10.362860 test begin: paddle.Tensor.gcd(x=Tensor([254016, 1, 4, 5],"int32"), y=Tensor([2, 1, 5],"int32"), )
W0806 09:53:31.337548 135263 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.Tensor.gcd 	 paddle.Tensor.gcd(x=Tensor([254016, 1, 4, 5],"int32"), y=Tensor([2, 1, 5],"int32"), ) 	 5080330 	 1000 	 20.797300100326538 	 0.44078898429870605 	 5.054473876953125e-05 	 0.4125251770019531 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 09:53:32.770313 test begin: paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([13001],"int64"), )
[Prof] paddle.Tensor.index_select 	 paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([13001],"int64"), ) 	 50816225 	 1057032 	 9.737354040145874 	 13.799731492996216 	 0.00010228157043457031 	 7.82012939453125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 09:56:40.573114 test begin: paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([18201],"int64"), )
[Prof] paddle.Tensor.index_select 	 paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([18201],"int64"), ) 	 50821425 	 1057032 	 17.342997074127197 	 13.910669565200806 	 0.00011277198791503906 	 0.00024437904357910156 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 09:59:56.575984 test begin: paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([9101],"int64"), )
[Prof] paddle.Tensor.index_select 	 paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([9101],"int64"), ) 	 50812325 	 1057032 	 9.929626941680908 	 13.886114358901978 	 9.751319885253906e-05 	 0.00010395050048828125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 10:03:01.618867 test begin: paddle.Tensor.index_select(Tensor([4004, 12689],"float32"), axis=0, index=Tensor([18201],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8e656bfa90>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:13:11.175453 test begin: paddle.Tensor.index_select(Tensor([4004, 24],"float32"), axis=0, index=Tensor([25401601],"int64"), )
W0806 10:13:11.716209 136089 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa07bb1afb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:23:16.881767 test begin: paddle.Tensor.index_select(Tensor([454, 111902],"float32"), axis=0, index=Tensor([130],"int64"), )
W0806 10:23:17.827119 136695 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7aaf79f010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:33:22.801613 test begin: paddle.Tensor.index_select(Tensor([454, 111902],"float32"), axis=0, index=Tensor([91],"int64"), )
W0806 10:33:23.776072 137394 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.index_select 	 paddle.Tensor.index_select(Tensor([454, 111902],"float32"), axis=0, index=Tensor([91],"int64"), ) 	 50803599 	 1057032 	 141.0810375213623 	 128.26589131355286 	 0.13637518882751465 	 0.1239621639251709 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 10:42:03.724574 test begin: paddle.Tensor.index_select(Tensor([454, 24],"float32"), axis=0, index=Tensor([25401601],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7efb4cd42980>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:52:15.576504 test begin: paddle.Tensor.inner(x=Tensor([2, 1058401, 3, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), )
W0806 10:52:16.279039 138450 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([2, 1058401, 3, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), ) 	 25401744 	 51721 	 100.91728806495667 	 100.88975644111633 	 0.28484439849853516 	 0.28475522994995117 	 174.09087944030762 	 167.65786337852478 	 0.3819875717163086 	 0.36739039421081543 	 
2025-08-06 11:01:25.703399 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 2, 1058401, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 2, 1058401, 4],"float64"), ) 	 25401744 	 51721 	 75.06599497795105 	 75.22847557067871 	 0.2118682861328125 	 0.212371826171875 	 210.61534762382507 	 210.37580609321594 	 0.4619622230529785 	 0.46166443824768066 	 
2025-08-06 11:11:05.356010 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 423361, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 423361, 5, 4],"float64"), ) 	 25401780 	 51721 	 94.34154534339905 	 93.07035851478577 	 0.26634836196899414 	 0.26268959045410156 	 176.3854718208313 	 188.84752559661865 	 0.38665223121643066 	 0.4146716594696045 	 
2025-08-06 11:20:29.247626 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([635041, 2, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([635041, 2, 5, 4],"float64"), ) 	 25401760 	 51721 	 75.02312016487122 	 75.19194102287292 	 0.211745023727417 	 0.21223187446594238 	 210.54434990882874 	 210.21602272987366 	 0.46175599098205566 	 0.46148109436035156 	 
2025-08-06 11:30:05.044840 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 846721],"float64"), y=Tensor([3, 2, 5, 846721],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([2, 5, 3, 846721],"float64"), y=Tensor([3, 2, 5, 846721],"float64"), ) 	 50803260 	 51721 	 17.315765619277954 	 17.30630660057068 	 0.17107820510864258 	 0.17089557647705078 	 36.377904415130615 	 38.84762215614319 	 0.35933756828308105 	 0.3836839199066162 	 
2025-08-06 11:31:56.613195 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 635041, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([2, 5, 635041, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), ) 	 25401760 	 51721 	 100.84283900260925 	 101.25233030319214 	 0.2846357822418213 	 0.28577518463134766 	 173.96571969985962 	 167.6893208026886 	 0.38170742988586426 	 0.3679065704345703 	 
2025-08-06 11:41:08.928753 test begin: paddle.Tensor.inner(x=Tensor([2116801, 3, 4],"float64"), y=Tensor([2, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([2116801, 3, 4],"float64"), y=Tensor([2, 5, 4],"float64"), ) 	 25401652 	 51721 	 97.27689719200134 	 95.79632186889648 	 0.27244067192077637 	 0.2703726291656494 	 120.48605036735535 	 120.38754916191101 	 0.2644996643066406 	 0.2639944553375244 	 
2025-08-06 11:48:29.252457 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 2, 1058401, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 2, 1058401, 4],"float64"), ) 	 25401636 	 51721 	 70.33110761642456 	 70.71778345108032 	 0.1985466480255127 	 0.1996145248413086 	 137.2007658481598 	 136.30051279067993 	 0.3010222911834717 	 0.2991218566894531 	 
2025-08-06 11:55:25.504523 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 423361, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 423361, 5, 4],"float64"), ) 	 25401672 	 51721 	 71.25011324882507 	 71.76294922828674 	 0.2011241912841797 	 0.2025606632232666 	 122.46336913108826 	 126.83290719985962 	 0.26856255531311035 	 0.2783968448638916 	 
2025-08-06 12:01:59.389517 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([635041, 2, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([635041, 2, 5, 4],"float64"), ) 	 25401652 	 51721 	 70.7239031791687 	 71.12088751792908 	 0.19961953163146973 	 0.2007296085357666 	 137.19557666778564 	 136.2926061153412 	 0.3009483814239502 	 0.2991211414337158 	 
2025-08-06 12:08:56.456345 test begin: paddle.Tensor.inner(x=Tensor([3, 8467201],"float64"), y=Tensor([3, 2, 5, 8467201],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb1f7ea6cb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 12:19:01.065617 test begin: paddle.Tensor.inner(x=Tensor([3, 846721],"float64"), y=Tensor([3, 2, 5, 846721],"float64"), )
W0806 12:19:01.713645 141597 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([3, 846721],"float64"), y=Tensor([3, 2, 5, 846721],"float64"), ) 	 27941793 	 51721 	 9.952994346618652 	 9.956998348236084 	 0.09830069541931152 	 0.09838461875915527 	 21.443797826766968 	 21.841920614242554 	 0.21181821823120117 	 0.21579623222351074 	 
2025-08-06 12:20:05.529112 test begin: paddle.Tensor.inner(x=Tensor([423361, 5, 3, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([423361, 5, 3, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), ) 	 25401780 	 51721 	 100.72554683685303 	 101.17541241645813 	 0.2843749523162842 	 0.2853538990020752 	 174.222754240036 	 167.99015712738037 	 0.38253283500671387 	 0.36830615997314453 	 
2025-08-06 12:29:14.383308 test begin: paddle.Tensor.inner(x=Tensor([5, 1270081, 4],"float64"), y=Tensor([2, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([5, 1270081, 4],"float64"), y=Tensor([2, 5, 4],"float64"), ) 	 25401660 	 51721 	 96.37965750694275 	 95.71131372451782 	 0.2720513343811035 	 0.270153284072876 	 115.95399236679077 	 117.71113538742065 	 0.25459837913513184 	 0.2581503391265869 	 
2025-08-06 12:36:22.095235 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 1693441],"float64"), y=Tensor([2, 5, 1693441],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([5, 3, 1693441],"float64"), y=Tensor([2, 5, 1693441],"float64"), ) 	 42336025 	 51721 	 15.365356922149658 	 15.040201187133789 	 0.15175771713256836 	 0.14856553077697754 	 42.355982065200806 	 44.8829665184021 	 0.20914506912231445 	 0.22167468070983887 	 
2025-08-06 12:38:22.278393 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 2540161],"float64"), y=Tensor([2, 5, 2540161],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([5, 3, 2540161],"float64"), y=Tensor([2, 5, 2540161],"float64"), ) 	 63504025 	 51721 	 22.624999046325684 	 22.110890865325928 	 0.22355318069458008 	 0.21841120719909668 	 64.08331108093262 	 68.03509378433228 	 0.21094655990600586 	 0.22396492958068848 	 
2025-08-06 12:41:20.587008 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([1270081, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([1270081, 5, 4],"float64"), ) 	 25401680 	 51721 	 80.61579155921936 	 79.66685366630554 	 0.22756099700927734 	 0.22487211227416992 	 127.88685774803162 	 138.54421257972717 	 0.2804746627807617 	 0.30408596992492676 	 
2025-08-06 12:48:29.937506 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([2, 3175201, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([2, 3175201, 4],"float64"), ) 	 25401668 	 51721 	 80.35943555831909 	 79.53069877624512 	 0.22684288024902344 	 0.22452425956726074 	 144.2718334197998 	 144.65679597854614 	 0.3164205551147461 	 0.3179788589477539 	 
2025-08-06 12:56:01.561770 test begin: paddle.Tensor.inner(x=Tensor([6350401, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), )
[Prof] paddle.Tensor.inner 	 paddle.Tensor.inner(x=Tensor([6350401, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), ) 	 25401724 	 51721 	 100.71153259277344 	 101.16691756248474 	 0.28555846214294434 	 0.2855722904205322 	 174.2431480884552 	 167.96742725372314 	 0.3829066753387451 	 0.3675568103790283 	 
2025-08-06 13:05:11.399340 test begin: paddle.Tensor.inverse(Tensor([4, 39690, 4, 4],"float64"), )
[Prof] paddle.Tensor.inverse 	 paddle.Tensor.inverse(Tensor([4, 39690, 4, 4],"float64"), ) 	 2540160 	 2707 	 20.31417989730835 	 0.9471838474273682 	 0.00011110305786132812 	 7.176399230957031e-05 	 14.598034858703613 	 5.306959867477417 	 0.9195272922515869 	 0.2861912250518799 	 
2025-08-06 13:05:52.844712 test begin: paddle.Tensor.inverse(Tensor([70560, 6, 6],"float64"), )
[Prof] paddle.Tensor.inverse 	 paddle.Tensor.inverse(Tensor([70560, 6, 6],"float64"), ) 	 2540160 	 2707 	 9.487031936645508 	 1.0912559032440186 	 0.00012731552124023438 	 0.00019860267639160156 	 5.949175119400024 	 4.524576187133789 	 0.5622789859771729 	 0.34102487564086914 	 
2025-08-06 13:06:13.998882 test begin: paddle.Tensor.inverse(Tensor([79380, 2, 4, 4],"float64"), )
[Prof] paddle.Tensor.inverse 	 paddle.Tensor.inverse(Tensor([79380, 2, 4, 4],"float64"), ) 	 2540160 	 2707 	 20.271716356277466 	 0.9170866012573242 	 0.00010371208190917969 	 0.0001049041748046875 	 14.592108249664307 	 5.321976900100708 	 0.9191617965698242 	 0.2870168685913086 	 
2025-08-06 13:06:55.242476 test begin: paddle.Tensor.is_complex(Tensor([201, 3, 100, 42337],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3, 100, 42337],"float64"), ) 	 2552921100 	 2814024 	 13.051533222198486 	 4.539393424987793 	 0.00011301040649414062 	 8.797645568847656e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 13:08:26.294317 test begin: paddle.Tensor.is_complex(Tensor([201, 3, 105841, 40],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3, 105841, 40],"float64"), ) 	 2552884920 	 2814024 	 10.351035594940186 	 4.489749431610107 	 4.458427429199219e-05 	 9.441375732421875e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 13:09:45.951147 test begin: paddle.Tensor.is_complex(Tensor([201, 3, 40, 105841],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3, 40, 105841],"float64"), ) 	 2552884920 	 2814024 	 13.811635732650757 	 5.610272645950317 	 4.9114227294921875e-05 	 9.012222290039062e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 13:10:58.997288 test begin: paddle.Tensor.is_complex(Tensor([201, 3, 42337, 100],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3, 42337, 100],"float64"), ) 	 2552921100 	 2814024 	 10.319873809814453 	 5.511738538742065 	 4.0531158447265625e-05 	 9.655952453613281e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 13:12:24.403213 test begin: paddle.Tensor.is_complex(Tensor([201, 3176, 100, 40],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3176, 100, 40],"float64"), ) 	 2553504000 	 2814024 	 10.302148342132568 	 4.521158933639526 	 4.744529724121094e-05 	 8.320808410644531e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 13:13:33.102289 test begin: paddle.Tensor.is_complex(Tensor([201, 3176, 40, 100],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([201, 3176, 40, 100],"float64"), ) 	 2553504000 	 2814024 	 10.27150559425354 	 4.471659898757935 	 4.38690185546875e-05 	 8.440017700195312e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 13:14:41.162252 test begin: paddle.Tensor.is_complex(Tensor([211701, 3, 100, 40],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([211701, 3, 100, 40],"float64"), ) 	 2540412000 	 2814024 	 10.406789779663086 	 5.606616497039795 	 3.7670135498046875e-05 	 8.869171142578125e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 13:15:49.680964 test begin: paddle.Tensor.is_complex(Tensor([211701, 3, 40, 100],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([211701, 3, 40, 100],"float64"), ) 	 2540412000 	 2814024 	 10.362323522567749 	 4.4131739139556885 	 4.863739013671875e-05 	 8.368492126464844e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 13:16:58.176097 test begin: paddle.Tensor.is_complex(Tensor([301, 100, 84673],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([301, 100, 84673],"float64"), ) 	 2548657300 	 2814024 	 10.369832038879395 	 4.572686433792114 	 3.981590270996094e-05 	 9.369850158691406e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 13:18:06.026671 test begin: paddle.Tensor.is_complex(Tensor([301, 211681, 40],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([301, 211681, 40],"float64"), ) 	 2548639240 	 2814024 	 10.347745656967163 	 4.40397310256958 	 4.601478576660156e-05 	 8.511543273925781e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 13:19:15.706130 test begin: paddle.Tensor.is_complex(Tensor([635101, 100, 40],"float64"), )
[Prof] paddle.Tensor.is_complex 	 paddle.Tensor.is_complex(Tensor([635101, 100, 40],"float64"), ) 	 2540404000 	 2814024 	 10.420928955078125 	 4.401573896408081 	 4.38690185546875e-05 	 8.726119995117188e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 13:20:23.570723 test begin: paddle.Tensor.isclose(x=Tensor([1270081, 4, 5],"float64"), y=Tensor([1270081, 4, 5],"float64"), )
[Prof] paddle.Tensor.isclose 	 paddle.Tensor.isclose(x=Tensor([1270081, 4, 5],"float64"), y=Tensor([1270081, 4, 5],"float64"), ) 	 50803240 	 27577 	 10.000208616256714 	 84.98196005821228 	 0.3705785274505615 	 0.24183344841003418 	 None 	 None 	 None 	 None 	 
2025-08-06 13:21:59.758778 test begin: paddle.Tensor.isclose(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), )
[Prof] paddle.Tensor.isclose 	 paddle.Tensor.isclose(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), ) 	 50803202 	 27577 	 10.003358364105225 	 84.98119020462036 	 0.3706479072570801 	 0.2417595386505127 	 None 	 None 	 None 	 None 	 
2025-08-06 13:23:38.398879 test begin: paddle.Tensor.isclose(x=Tensor([3, 1693441, 5],"float64"), y=Tensor([3, 1693441, 5],"float64"), )
[Prof] paddle.Tensor.isclose 	 paddle.Tensor.isclose(x=Tensor([3, 1693441, 5],"float64"), y=Tensor([3, 1693441, 5],"float64"), ) 	 50803230 	 27577 	 10.004983186721802 	 84.97966456413269 	 0.37079286575317383 	 0.24177908897399902 	 None 	 None 	 None 	 None 	 
2025-08-06 13:25:14.535234 test begin: paddle.Tensor.isclose(x=Tensor([3, 4, 2116801],"float64"), y=Tensor([3, 4, 2116801],"float64"), )
[Prof] paddle.Tensor.isclose 	 paddle.Tensor.isclose(x=Tensor([3, 4, 2116801],"float64"), y=Tensor([3, 4, 2116801],"float64"), ) 	 50803224 	 27577 	 10.002016067504883 	 84.9808361530304 	 0.37062525749206543 	 0.24179911613464355 	 None 	 None 	 None 	 None 	 
2025-08-06 13:26:50.671852 test begin: paddle.Tensor.isclose(x=Tensor([50803201],"float32"), y=Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.isclose 	 paddle.Tensor.isclose(x=Tensor([50803201],"float32"), y=Tensor([50803201],"float32"), ) 	 101606402 	 27577 	 11.720562219619751 	 91.32618379592896 	 0.4342770576477051 	 0.2599301338195801 	 None 	 None 	 None 	 None 	 
2025-08-06 13:28:37.467443 test begin: paddle.Tensor.isnan(Tensor([25401601],"float64"), )
[Prof] paddle.Tensor.isnan 	 paddle.Tensor.isnan(Tensor([25401601],"float64"), ) 	 25401601 	 55184 	 10.012644052505493 	 9.308721542358398 	 0.18542146682739258 	 0.17230701446533203 	 None 	 None 	 None 	 None 	 
2025-08-06 13:28:57.358689 test begin: paddle.Tensor.isnan(Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.isnan 	 paddle.Tensor.isnan(Tensor([50803201],"float32"), ) 	 50803201 	 55184 	 12.897098302841187 	 10.260193586349487 	 0.2388932704925537 	 0.19007658958435059 	 None 	 None 	 None 	 None 	 
2025-08-06 13:29:21.363548 test begin: paddle.Tensor.item(Tensor([201, 1, 12700801],"int64"), 0, )
[Prof] paddle.Tensor.item 	 paddle.Tensor.item(Tensor([201, 1, 12700801],"int64"), 0, ) 	 2552861001 	 533796 	 9.80292534828186 	 14.159905433654785 	 4.291534423828125e-05 	 6.437301635742188e-05 	 None 	 None 	 None 	 None 	 combined
2025-08-06 13:30:25.615923 test begin: paddle.Tensor.item(Tensor([201, 12700801, 1],"int64"), 0, )
[Prof] paddle.Tensor.item 	 paddle.Tensor.item(Tensor([201, 12700801, 1],"int64"), 0, ) 	 2552861001 	 533796 	 9.905489921569824 	 14.215501308441162 	 0.00010609626770019531 	 0.000232696533203125 	 None 	 None 	 None 	 None 	 combined
2025-08-06 13:31:35.889061 test begin: paddle.Tensor.item(Tensor([2540160101, 1, 1],"int64"), 0, )
[Prof] paddle.Tensor.item 	 paddle.Tensor.item(Tensor([2540160101, 1, 1],"int64"), 0, ) 	 2540160101 	 533796 	 9.803968906402588 	 14.239499807357788 	 4.38690185546875e-05 	 6.794929504394531e-05 	 None 	 None 	 None 	 None 	 combined
2025-08-06 13:32:41.167607 test begin: paddle.Tensor.kthvalue(Tensor([2, 200, 127009],"float32"), k=200, axis=1, )
[Prof] paddle.Tensor.kthvalue 	 paddle.Tensor.kthvalue(Tensor([2, 200, 127009],"float32"), k=200, axis=1, ) 	 50803600 	 1495 	 9.982494115829468 	 17.37636375427246 	 1.7027924060821533 	 11.872360467910767 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:33:16.365216 test begin: paddle.Tensor.kthvalue(Tensor([2, 2540161, 10],"float32"), k=200, axis=1, )
[Prof] paddle.Tensor.kthvalue 	 paddle.Tensor.kthvalue(Tensor([2, 2540161, 10],"float32"), k=200, axis=1, ) 	 50803220 	 1495 	 56.67892408370972 	 51.38158583641052 	 9.666786670684814 	 35.12535881996155 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:35:11.703141 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 317521],"float64"), y=Tensor([4, 5, 4, 317521],"float64"), weight=0.0, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 5, 4, 317521],"float64"), y=Tensor([4, 5, 4, 317521],"float64"), weight=0.0, ) 	 50803360 	 22191 	 9.996875047683716 	 9.870950698852539 	 0.23012924194335938 	 0.4546537399291992 	 10.657687425613403 	 13.233225345611572 	 0.4908602237701416 	 0.3046863079071045 	 
2025-08-06 13:35:59.597133 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 317521],"float64"), y=Tensor([4, 5, 4, 317521],"float64"), weight=0.5, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 5, 4, 317521],"float64"), y=Tensor([4, 5, 4, 317521],"float64"), weight=0.5, ) 	 50803360 	 22191 	 9.997196435928345 	 9.869587182998657 	 0.23025202751159668 	 0.45452260971069336 	 10.658264398574829 	 13.233267545700073 	 0.4908437728881836 	 0.30474376678466797 	 
2025-08-06 13:36:45.065401 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 317521],"float64"), y=Tensor([4, 5, 4, 317521],"float64"), weight=1.0, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 5, 4, 317521],"float64"), y=Tensor([4, 5, 4, 317521],"float64"), weight=1.0, ) 	 50803360 	 22191 	 9.996127128601074 	 9.871413707733154 	 0.2301158905029297 	 0.4545621871948242 	 10.658605098724365 	 13.233359575271606 	 0.4908757209777832 	 0.304767370223999 	 
2025-08-06 13:37:30.769431 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 423361, 3],"float64"), y=Tensor([4, 5, 423361, 3],"float64"), weight=0.0, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 5, 423361, 3],"float64"), y=Tensor([4, 5, 423361, 3],"float64"), weight=0.0, ) 	 50803320 	 22191 	 9.997698068618774 	 9.869822263717651 	 0.23020243644714355 	 0.4545562267303467 	 10.67527437210083 	 13.233817338943481 	 0.49164700508117676 	 0.304654598236084 	 
2025-08-06 13:38:17.960930 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 423361, 3],"float64"), y=Tensor([4, 5, 423361, 3],"float64"), weight=0.5, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 5, 423361, 3],"float64"), y=Tensor([4, 5, 423361, 3],"float64"), weight=0.5, ) 	 50803320 	 22191 	 9.998070240020752 	 9.871046781539917 	 0.2303473949432373 	 0.4545280933380127 	 10.6753568649292 	 13.232598304748535 	 0.49164557456970215 	 0.3046693801879883 	 
2025-08-06 13:39:03.585951 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 423361, 3],"float64"), y=Tensor([4, 5, 423361, 3],"float64"), weight=1.0, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 5, 423361, 3],"float64"), y=Tensor([4, 5, 423361, 3],"float64"), weight=1.0, ) 	 50803320 	 22191 	 9.998207569122314 	 9.869436979293823 	 0.23029565811157227 	 0.4544808864593506 	 10.675413608551025 	 13.233325004577637 	 0.4916386604309082 	 0.30463504791259766 	 
2025-08-06 13:39:49.495455 test begin: paddle.Tensor.lerp(x=Tensor([4, 529201, 4, 3],"float64"), y=Tensor([4, 529201, 4, 3],"float64"), weight=0.0, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 529201, 4, 3],"float64"), y=Tensor([4, 529201, 4, 3],"float64"), weight=0.0, ) 	 50803296 	 22191 	 9.996075630187988 	 9.869456768035889 	 0.23023700714111328 	 0.454545259475708 	 10.675578594207764 	 13.23378610610962 	 0.4916088581085205 	 0.30472898483276367 	 
2025-08-06 13:40:34.962320 test begin: paddle.Tensor.lerp(x=Tensor([4, 529201, 4, 3],"float64"), y=Tensor([4, 529201, 4, 3],"float64"), weight=0.5, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 529201, 4, 3],"float64"), y=Tensor([4, 529201, 4, 3],"float64"), weight=0.5, ) 	 50803296 	 22191 	 9.998294115066528 	 9.869329929351807 	 0.23015713691711426 	 0.4545748233795166 	 10.675692081451416 	 13.232189416885376 	 0.49166107177734375 	 0.3046555519104004 	 
2025-08-06 13:41:20.951111 test begin: paddle.Tensor.lerp(x=Tensor([4, 529201, 4, 3],"float64"), y=Tensor([4, 529201, 4, 3],"float64"), weight=1.0, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([4, 529201, 4, 3],"float64"), y=Tensor([4, 529201, 4, 3],"float64"), weight=1.0, ) 	 50803296 	 22191 	 9.998754978179932 	 9.869486570358276 	 0.2302989959716797 	 0.45452356338500977 	 10.675832986831665 	 13.232491731643677 	 0.4917135238647461 	 0.3046901226043701 	 
2025-08-06 13:42:06.430843 test begin: paddle.Tensor.lerp(x=Tensor([423361, 5, 4, 3],"float64"), y=Tensor([423361, 5, 4, 3],"float64"), weight=0.0, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([423361, 5, 4, 3],"float64"), y=Tensor([423361, 5, 4, 3],"float64"), weight=0.0, ) 	 50803320 	 22191 	 9.998171329498291 	 9.869548082351685 	 0.23026204109191895 	 0.45459747314453125 	 10.675384044647217 	 13.232758283615112 	 0.4916541576385498 	 0.304645299911499 	 
2025-08-06 13:42:52.178888 test begin: paddle.Tensor.lerp(x=Tensor([423361, 5, 4, 3],"float64"), y=Tensor([423361, 5, 4, 3],"float64"), weight=0.5, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([423361, 5, 4, 3],"float64"), y=Tensor([423361, 5, 4, 3],"float64"), weight=0.5, ) 	 50803320 	 22191 	 9.998103141784668 	 9.871431827545166 	 0.2301788330078125 	 0.45450448989868164 	 10.674216270446777 	 13.231774806976318 	 0.4915604591369629 	 0.3047020435333252 	 
2025-08-06 13:43:38.372844 test begin: paddle.Tensor.lerp(x=Tensor([423361, 5, 4, 3],"float64"), y=Tensor([423361, 5, 4, 3],"float64"), weight=1.0, )
[Prof] paddle.Tensor.lerp 	 paddle.Tensor.lerp(x=Tensor([423361, 5, 4, 3],"float64"), y=Tensor([423361, 5, 4, 3],"float64"), weight=1.0, ) 	 50803320 	 22191 	 9.999377727508545 	 9.869697332382202 	 0.23032283782958984 	 0.45450782775878906 	 10.67484450340271 	 13.232774496078491 	 0.4915957450866699 	 0.30467820167541504 	 
2025-08-06 13:44:23.894673 test begin: paddle.Tensor.less(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.less 	 paddle.Tensor.less(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), ) 	 101606420 	 30607 	 9.999728679656982 	 10.373277187347412 	 0.3339061737060547 	 0.3348691463470459 	 None 	 None 	 None 	 None 	 
2025-08-06 13:44:48.523061 test begin: paddle.Tensor.less(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float32"), )
[Prof] paddle.Tensor.less 	 paddle.Tensor.less(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float32"), ) 	 101607424 	 30607 	 10.008487701416016 	 10.04092001914978 	 0.33422064781188965 	 0.3348655700683594 	 None 	 None 	 None 	 None 	 
2025-08-06 13:45:12.160760 test begin: paddle.Tensor.lgamma(Tensor([100, 100, 2541],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([100, 100, 2541],"float64"), ) 	 25410000 	 14030 	 9.99639892578125 	 9.678030490875244 	 0.7281286716461182 	 0.7049956321716309 	 19.428749561309814 	 22.249536991119385 	 1.4152886867523193 	 0.8103489875793457 	 
2025-08-06 13:46:14.790575 test begin: paddle.Tensor.lgamma(Tensor([100, 2541, 100],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([100, 2541, 100],"float64"), ) 	 25410000 	 14030 	 9.99570631980896 	 9.677343606948853 	 0.7281560897827148 	 0.7050714492797852 	 19.428526639938354 	 22.249694347381592 	 1.4152660369873047 	 0.8103122711181641 	 
2025-08-06 13:47:18.014803 test begin: paddle.Tensor.lgamma(Tensor([2541, 100, 100],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([2541, 100, 100],"float64"), ) 	 25410000 	 14030 	 9.999176740646362 	 9.678836822509766 	 0.728405237197876 	 0.7049417495727539 	 19.42839527130127 	 22.250101804733276 	 1.4152519702911377 	 0.8104345798492432 	 
2025-08-06 13:48:20.511598 test begin: paddle.Tensor.lgamma(Tensor([453601, 7, 8],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([453601, 7, 8],"float64"), ) 	 25401656 	 14030 	 9.99356460571289 	 9.676514625549316 	 0.7280097007751465 	 0.7048749923706055 	 19.371875762939453 	 22.245970010757446 	 1.4111888408660889 	 0.8102288246154785 	 
2025-08-06 13:49:22.962162 test begin: paddle.Tensor.lgamma(Tensor([45361, 7, 8, 10],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([45361, 7, 8, 10],"float64"), ) 	 25402160 	 14030 	 9.993405103683472 	 9.675939559936523 	 0.7279791831970215 	 0.7049083709716797 	 19.36999273300171 	 22.24544930458069 	 1.4108846187591553 	 0.8102409839630127 	 
2025-08-06 13:50:25.385404 test begin: paddle.Tensor.lgamma(Tensor([5, 635041, 8],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([5, 635041, 8],"float64"), ) 	 25401640 	 14030 	 9.99743938446045 	 9.679879665374756 	 0.7282724380493164 	 0.7051045894622803 	 19.378892183303833 	 22.25109100341797 	 1.4116275310516357 	 0.8104109764099121 	 
2025-08-06 13:51:28.395580 test begin: paddle.Tensor.lgamma(Tensor([5, 63505, 8, 10],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([5, 63505, 8, 10],"float64"), ) 	 25402000 	 14030 	 9.99739694595337 	 9.676798582077026 	 0.7282764911651611 	 0.7049047946929932 	 19.379611253738403 	 22.24676513671875 	 1.411628007888794 	 0.8102645874023438 	 
2025-08-06 13:52:30.853321 test begin: paddle.Tensor.lgamma(Tensor([5, 7, 725761],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([5, 7, 725761],"float64"), ) 	 25401635 	 14030 	 9.997769594192505 	 9.676828622817993 	 0.7283012866973877 	 0.7048728466033936 	 19.371766805648804 	 22.245827198028564 	 1.411125659942627 	 0.8102350234985352 	 
2025-08-06 13:53:33.473503 test begin: paddle.Tensor.lgamma(Tensor([5, 7, 72577, 10],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([5, 7, 72577, 10],"float64"), ) 	 25401950 	 14030 	 9.993520021438599 	 9.675710678100586 	 0.7280058860778809 	 0.7048394680023193 	 19.395208597183228 	 22.244744777679443 	 1.4128258228302002 	 0.8101658821105957 	 
2025-08-06 13:54:39.026507 test begin: paddle.Tensor.lgamma(Tensor([5, 7, 8, 90721],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([5, 7, 8, 90721],"float64"), ) 	 25401880 	 14030 	 9.997313976287842 	 9.697232723236084 	 0.7283134460449219 	 0.7052690982818604 	 19.401837825775146 	 22.252873182296753 	 1.4133763313293457 	 0.8104839324951172 	 
2025-08-06 13:55:43.021918 test begin: paddle.Tensor.log(Tensor([100, 200, 1271],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([100, 200, 1271],"float64"), ) 	 25420000 	 33839 	 10.346526384353638 	 10.365624189376831 	 0.3125190734863281 	 0.31307029724121094 	 15.154794931411743 	 15.192579507827759 	 0.457721471786499 	 0.45883893966674805 	 
2025-08-06 13:56:38.105915 test begin: paddle.Tensor.log(Tensor([100, 2541, 100],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([100, 2541, 100],"float64"), ) 	 25410000 	 33839 	 10.336979866027832 	 10.361318349838257 	 0.31215739250183105 	 0.312969446182251 	 15.150404214859009 	 15.187426805496216 	 0.4576542377471924 	 0.45870351791381836 	 
2025-08-06 13:57:30.326605 test begin: paddle.Tensor.log(Tensor([10000, 5, 509],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([10000, 5, 509],"float64"), ) 	 25450000 	 33839 	 10.356169939041138 	 10.376063585281372 	 0.3127765655517578 	 0.3133864402770996 	 15.179823160171509 	 15.20940637588501 	 0.4582805633544922 	 0.45926904678344727 	 
2025-08-06 13:58:22.613443 test begin: paddle.Tensor.log(Tensor([10000, 847, 3],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([10000, 847, 3],"float64"), ) 	 25410000 	 33839 	 10.337878227233887 	 10.361095190048218 	 0.31227540969848633 	 0.3128514289855957 	 15.1502525806427 	 15.187239170074463 	 0.45774006843566895 	 0.4587106704711914 	 
2025-08-06 13:59:14.791321 test begin: paddle.Tensor.log(Tensor([1271, 200, 100],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([1271, 200, 100],"float64"), ) 	 25420000 	 33839 	 10.346116065979004 	 10.37959098815918 	 0.31244707107543945 	 0.3130457401275635 	 15.152762174606323 	 15.192918062210083 	 0.45776844024658203 	 0.45882368087768555 	 
2025-08-06 14:00:08.188328 test begin: paddle.Tensor.log(Tensor([1693441, 5, 3],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([1693441, 5, 3],"float64"), ) 	 25401615 	 33839 	 10.33455514907837 	 10.358224868774414 	 0.3120872974395752 	 0.31288981437683105 	 15.146986722946167 	 15.181902408599854 	 0.4575507640838623 	 0.45851802825927734 	 
2025-08-06 14:01:00.473234 test begin: paddle.Tensor.log(Tensor([4800, 10585],"float32"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([4800, 10585],"float32"), ) 	 50808000 	 33839 	 10.002132892608643 	 10.064618825912476 	 0.3020446300506592 	 0.30400967597961426 	 15.232804775238037 	 15.211351871490479 	 0.46003031730651855 	 0.4594552516937256 	 
2025-08-06 14:01:52.748587 test begin: paddle.Tensor.log(Tensor([503002, 101],"float32"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([503002, 101],"float32"), ) 	 50803202 	 33839 	 10.003648042678833 	 10.06389832496643 	 0.3021516799926758 	 0.3039736747741699 	 15.222535133361816 	 15.210105180740356 	 0.4599480628967285 	 0.4593772888183594 	 
2025-08-06 14:02:44.977970 test begin: paddle.Tensor.log10(Tensor([101811, 499],"float32"), )
[Prof] paddle.Tensor.log10 	 paddle.Tensor.log10(Tensor([101811, 499],"float32"), ) 	 50803689 	 33813 	 9.997636318206787 	 11.615897417068481 	 0.3021073341369629 	 0.30393242835998535 	 15.212097406387329 	 25.21272349357605 	 0.4598109722137451 	 0.38103222846984863 	 
2025-08-06 14:03:49.627425 test begin: paddle.Tensor.log10(Tensor([80, 635041],"float32"), )
[Prof] paddle.Tensor.log10 	 paddle.Tensor.log10(Tensor([80, 635041],"float32"), ) 	 50803280 	 33813 	 9.98947286605835 	 10.055978775024414 	 0.3019261360168457 	 0.3039393424987793 	 15.197680234909058 	 25.21176242828369 	 0.4593510627746582 	 0.3810262680053711 	 
2025-08-06 14:04:52.215838 test begin: paddle.Tensor.log1p(Tensor([16934401, 3],"float32"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([16934401, 3],"float32"), ) 	 50803203 	 33826 	 9.999040842056274 	 10.106316089630127 	 0.3021118640899658 	 0.30525755882263184 	 15.212299585342407 	 25.221768617630005 	 0.45961427688598633 	 0.3811042308807373 	 
2025-08-06 14:05:54.477876 test begin: paddle.Tensor.log1p(Tensor([2, 25401601],"float32"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([2, 25401601],"float32"), ) 	 50803202 	 33826 	 9.999595880508423 	 10.103853940963745 	 0.3021671772003174 	 0.3052794933319092 	 15.21303677558899 	 25.222154140472412 	 0.45957040786743164 	 0.3810157775878906 	 
2025-08-06 14:06:56.846472 test begin: paddle.Tensor.log1p(Tensor([2, 3, 4233601],"float64"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([2, 3, 4233601],"float64"), ) 	 25401606 	 33826 	 10.324130296707153 	 11.35496973991394 	 0.3118889331817627 	 0.3430929183959961 	 15.140779733657837 	 25.19247341156006 	 0.4571878910064697 	 0.38054323196411133 	 
2025-08-06 14:08:00.067928 test begin: paddle.Tensor.log1p(Tensor([2, 6350401, 2],"float64"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([2, 6350401, 2],"float64"), ) 	 25401604 	 33826 	 10.323900938034058 	 11.356517553329468 	 0.3119332790374756 	 0.3431217670440674 	 15.1422438621521 	 25.191091299057007 	 0.4588472843170166 	 0.3805274963378906 	 
2025-08-06 14:09:03.431846 test begin: paddle.Tensor.log1p(Tensor([25401601],"float64"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([25401601],"float64"), ) 	 25401601 	 33826 	 10.324352264404297 	 11.357728004455566 	 0.3119945526123047 	 0.3428955078125 	 15.143451690673828 	 25.191640853881836 	 0.45758819580078125 	 0.3805198669433594 	 
2025-08-06 14:10:08.680299 test begin: paddle.Tensor.log1p(Tensor([4233601, 3, 2],"float64"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([4233601, 3, 2],"float64"), ) 	 25401606 	 33826 	 10.325261116027832 	 11.364705085754395 	 0.3119480609893799 	 0.3430812358856201 	 15.140607118606567 	 25.190858364105225 	 0.4573836326599121 	 0.3805525302886963 	 
2025-08-06 14:11:13.082075 test begin: paddle.Tensor.logical_and(Tensor([50803201],"bool"), Tensor([50803201],"bool"), )
[Prof] paddle.Tensor.logical_and 	 paddle.Tensor.logical_and(Tensor([50803201],"bool"), Tensor([50803201],"bool"), ) 	 101606402 	 84274 	 9.98843765258789 	 9.768728017807007 	 0.12111592292785645 	 0.11844086647033691 	 None 	 None 	 None 	 None 	 
2025-08-06 14:11:34.293084 test begin: paddle.Tensor.logical_not(Tensor([508032010],"bool"), )
[Prof] paddle.Tensor.logical_not 	 paddle.Tensor.logical_not(Tensor([508032010],"bool"), ) 	 508032010 	 12687 	 9.998251676559448 	 9.47349214553833 	 0.8054416179656982 	 0.7631101608276367 	 None 	 None 	 None 	 None 	 
2025-08-06 14:12:00.811898 test begin: paddle.Tensor.logical_or(Tensor([50803201],"bool"), Tensor([50803201],"bool"), )
[Prof] paddle.Tensor.logical_or 	 paddle.Tensor.logical_or(Tensor([50803201],"bool"), Tensor([50803201],"bool"), ) 	 101606402 	 84635 	 9.990583896636963 	 9.786590814590454 	 0.1206357479095459 	 0.11818695068359375 	 None 	 None 	 None 	 None 	 
2025-08-06 14:12:22.031147 test begin: paddle.Tensor.logit(x=Tensor([4, 3, 423361, 5],"float64"), eps=0.2, )
[Prof] paddle.Tensor.logit 	 paddle.Tensor.logit(x=Tensor([4, 3, 423361, 5],"float64"), eps=0.2, ) 	 25401660 	 30800 	 10.007441759109497 	 9.322406530380249 	 0.33186960220336914 	 0.30935168266296387 	 13.64677095413208 	 13.819970846176147 	 0.45281195640563965 	 0.4585916996002197 	 
2025-08-06 14:13:09.967624 test begin: paddle.Tensor.logit(x=Tensor([4, 635041, 2, 5],"float64"), eps=0.2, )
[Prof] paddle.Tensor.logit 	 paddle.Tensor.logit(x=Tensor([4, 635041, 2, 5],"float64"), eps=0.2, ) 	 25401640 	 30800 	 10.001164197921753 	 9.3229238986969 	 0.3318488597869873 	 0.3093447685241699 	 13.646937370300293 	 13.822917222976685 	 0.45284390449523926 	 0.4586019515991211 	 
2025-08-06 14:13:57.881477 test begin: paddle.Tensor.logit(x=Tensor([846721, 3, 2, 5],"float64"), eps=0.2, )
[Prof] paddle.Tensor.logit 	 paddle.Tensor.logit(x=Tensor([846721, 3, 2, 5],"float64"), eps=0.2, ) 	 25401630 	 30800 	 10.02479362487793 	 9.324951648712158 	 0.33199119567871094 	 0.309342622756958 	 13.680331707000732 	 13.823137283325195 	 0.45380735397338867 	 0.4585299491882324 	 
2025-08-06 14:14:45.850900 test begin: paddle.Tensor.lu(Tensor([1693, 300],"float32"), )
/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:924: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
LU, pivots = torch.lu(A, compute_pivots)
should be replaced with
LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
and
LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
should be replaced with
LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2055.)
  LU, pivots, infos = torch._lu_with_info(
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7efad89cc7c0>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception
2025-08-06 14:24:54.855266 test begin: paddle.Tensor.lu(Tensor([216, 3, 2, 2],"float64"), )
W0806 14:24:55.092424 154794 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2c93967070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 14:35:02.222499 test begin: paddle.Tensor.lu(Tensor([3, 3, 422],"float64"), )
W0806 14:35:02.716130 163634 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:924: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
LU, pivots = torch.lu(A, compute_pivots)
should be replaced with
LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
and
LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
should be replaced with
LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2055.)
  LU, pivots, infos = torch._lu_with_info(
[Prof] paddle.Tensor.lu 	 paddle.Tensor.lu(Tensor([3, 3, 422],"float64"), ) 	 3798 	 64078 	 9.97559905052185 	 8.17620062828064 	 0.00013113021850585938 	 0.00026798248291015625 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:35:48.859043 test begin: paddle.Tensor.lu(Tensor([301, 1193],"float32"), )
[Prof] paddle.Tensor.lu 	 paddle.Tensor.lu(Tensor([301, 1193],"float32"), ) 	 359093 	 64078 	 88.43433618545532 	 325.5667338371277 	 0.00023174285888671875 	 0.0006225109100341797 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 14:43:19.259065 test begin: paddle.Tensor.lu(Tensor([301, 422, 3],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f92880ebf40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 14:53:24.485481 test begin: paddle.Tensor.lu(Tensor([4, 187, 2, 2],"float64"), )
W0806 14:53:24.686892 16962 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8930957070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 15:03:29.170452 test begin: paddle.Tensor.lu(Tensor([4, 3, 158, 2],"float64"), )
W0806 15:03:29.372303 34397 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:924: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
LU, pivots = torch.lu(A, compute_pivots)
should be replaced with
LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
and
LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
should be replaced with
LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2055.)
  LU, pivots, infos = torch._lu_with_info(
[Prof] paddle.Tensor.lu 	 paddle.Tensor.lu(Tensor([4, 3, 158, 2],"float64"), ) 	 3792 	 64078 	 17.70476245880127 	 6.6006364822387695 	 0.00010561943054199219 	 0.00014138221740722656 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:04:21.825356 test begin: paddle.Tensor.lu(Tensor([4, 3, 2, 158],"float64"), )
[Prof] paddle.Tensor.lu 	 paddle.Tensor.lu(Tensor([4, 3, 2, 158],"float64"), ) 	 3792 	 64078 	 23.833141326904297 	 8.6952223777771 	 9.489059448242188e-05 	 8.7738037109375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:05:21.849922 test begin: paddle.Tensor.lu(Tensor([522, 3, 3],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f85ee5d3f70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 15:15:26.601776 test begin: paddle.Tensor.masked_fill(Tensor([1, 198451, 256],"float32"), Tensor([1, 198451, 1],"bool"), 0.0, )
W0806 15:15:27.561779 55191 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 198451, 256],"float32"), Tensor([1, 198451, 1],"bool"), 0.0, ) 	 51001907 	 70418 	 10.080788135528564 	 43.546804428100586 	 0.04895305633544922 	 0.210435152053833 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:16:33.215947 test begin: paddle.Tensor.masked_fill(Tensor([1, 36828, 1380],"float32"), Tensor([1, 36828, 1380],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 36828, 1380],"float32"), Tensor([1, 36828, 1380],"bool"), 0.0, ) 	 101645280 	 70418 	 26.508222818374634 	 45.835688829422 	 0.09624671936035156 	 0.22142529487609863 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:18:09.682422 test begin: paddle.Tensor.masked_fill(Tensor([1, 36828, 1380],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 36828, 1380],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, ) 	 50859468 	 70418 	 9.993568897247314 	 43.7322461605072 	 0.04835367202758789 	 0.21137142181396484 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:19:14.878959 test begin: paddle.Tensor.masked_fill(Tensor([1, 38367, 1325],"float32"), Tensor([1, 38367, 1325],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 38367, 1325],"float32"), Tensor([1, 38367, 1325],"bool"), 0.0, ) 	 101672550 	 70418 	 26.50077176094055 	 45.89284133911133 	 0.09619832038879395 	 0.22189903259277344 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:20:49.514290 test begin: paddle.Tensor.masked_fill(Tensor([1, 38367, 1325],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 38367, 1325],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, ) 	 50874642 	 70418 	 17.348228931427002 	 43.74550986289978 	 0.08391475677490234 	 0.2114734649658203 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:22:12.541216 test begin: paddle.Tensor.masked_fill(Tensor([1, 8550, 5942],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 8550, 5942],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, ) 	 50812650 	 70418 	 9.98250937461853 	 43.403000354766846 	 0.04831242561340332 	 0.2111961841583252 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:23:17.226705 test begin: paddle.Tensor.masked_fill(Tensor([1, 8550, 5942],"float32"), Tensor([1, 8550, 5942],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 8550, 5942],"float32"), Tensor([1, 8550, 5942],"bool"), 0.0, ) 	 101608200 	 70418 	 26.509297132492065 	 45.81589317321777 	 0.09621620178222656 	 0.22137951850891113 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:24:51.335301 test begin: paddle.Tensor.masked_fill(Tensor([24, 8550, 256],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([24, 8550, 256],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, ) 	 52539750 	 70418 	 32.182276248931885 	 44.91364073753357 	 0.11690163612365723 	 0.3258180618286133 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:26:40.939480 test begin: paddle.Tensor.masked_fill(Tensor([24, 8550, 256],"float32"), Tensor([24, 8550, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([24, 8550, 256],"float32"), Tensor([24, 8550, 1],"bool"), 0.0, ) 	 52736400 	 70418 	 10.361884593963623 	 44.846954107284546 	 0.05014300346374512 	 0.32658958435058594 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:27:48.002989 test begin: paddle.Tensor.masked_fill(Tensor([6, 36828, 256],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([6, 36828, 256],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, ) 	 56604636 	 70418 	 34.515687704086304 	 48.488818645477295 	 0.1253964900970459 	 0.35157299041748047 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:29:45.955855 test begin: paddle.Tensor.masked_fill(Tensor([6, 36828, 256],"float32"), Tensor([6, 36828, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([6, 36828, 256],"float32"), Tensor([6, 36828, 1],"bool"), 0.0, ) 	 56788776 	 70418 	 11.103888988494873 	 48.235206842422485 	 0.05374455451965332 	 0.3497896194458008 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:30:58.450774 test begin: paddle.Tensor.masked_fill(Tensor([6, 38367, 256],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([6, 38367, 256],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, ) 	 58970079 	 70418 	 36.015477657318115 	 50.63042593002319 	 0.13079833984375 	 0.24471139907836914 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:33:05.480402 test begin: paddle.Tensor.masked_fill(Tensor([6, 38367, 256],"float32"), Tensor([6, 38367, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([6, 38367, 256],"float32"), Tensor([6, 38367, 1],"bool"), 0.0, ) 	 59161914 	 70418 	 11.600282907485962 	 50.38366174697876 	 0.056130170822143555 	 0.24344968795776367 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:34:21.643886 test begin: paddle.Tensor.masked_select(Tensor([1016065, 50],"float32"), Tensor([1016065, 50],"bool"), )
[Prof] paddle.Tensor.masked_select 	 paddle.Tensor.masked_select(Tensor([1016065, 50],"float32"), Tensor([1016065, 50],"bool"), ) 	 101606500 	 7203 	 18.887592315673828 	 17.609766244888306 	 0.0016345977783203125 	 0.002331256866455078 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:35:21.293628 test begin: paddle.Tensor.masked_select(Tensor([15000, 3387],"float32"), Tensor([15000, 3387],"bool"), )
[Prof] paddle.Tensor.masked_select 	 paddle.Tensor.masked_select(Tensor([15000, 3387],"float32"), Tensor([15000, 3387],"bool"), ) 	 101610000 	 7203 	 9.886782169342041 	 17.679205656051636 	 0.0008459091186523438 	 0.002343893051147461 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:36:01.526553 test begin: paddle.Tensor.masked_select(Tensor([50803201],"float32"), Tensor([50803201],"bool"), )
[Prof] paddle.Tensor.masked_select 	 paddle.Tensor.masked_select(Tensor([50803201],"float32"), Tensor([50803201],"bool"), ) 	 101606402 	 7203 	 34.672239780426025 	 8.148143291473389 	 0.002934694290161133 	 0.001032114028930664 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:37:21.566473 test begin: paddle.Tensor.masked_select(Tensor([60000, 847],"float32"), Tensor([60000, 847],"bool"), )
[Prof] paddle.Tensor.masked_select 	 paddle.Tensor.masked_select(Tensor([60000, 847],"float32"), Tensor([60000, 847],"bool"), ) 	 101640000 	 7203 	 9.912945032119751 	 17.554210662841797 	 0.0008504390716552734 	 0.002317667007446289 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 15:38:01.768363 test begin: paddle.Tensor.matmul(Tensor([110, 12, 197, 197],"float32"), Tensor([110, 12, 197, 64],"float32"), )
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([110, 12, 197, 197],"float32"), Tensor([110, 12, 197, 64],"float32"), ) 	 67870440 	 9746 	 10.024285078048706 	 10.170772790908813 	 1.050577163696289 	 1.1994719505310059 	 17.110950708389282 	 17.10677409172058 	 0.896559476852417 	 0.896453857421875 	 
2025-08-06 15:38:59.444144 test begin: paddle.Tensor.matmul(Tensor([124, 16, 100, 257],"float32"), Tensor([124, 16, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([124, 16, 100, 257],"float32"), Tensor([124, 16, 257, 64],"float32"), ) 	 83621632 	 9746 	 10.007802248001099 	 10.21763563156128 	 1.0488066673278809 	 1.2646286487579346 	 20.06821870803833 	 20.05903387069702 	 1.0515742301940918 	 1.0511114597320557 	 
2025-08-06 15:40:03.549101 test begin: paddle.Tensor.matmul(Tensor([124, 16, 257, 257],"float32"), Tensor([124, 16, 257, 100],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([124, 16, 257, 257],"float32"), Tensor([124, 16, 257, 100],"float32"), ) 	 182030016 	 9746 	 29.288565397262573 	 29.281206369400024 	 3.0705783367156982 	 3.0703678131103516 	 65.22270011901855 	 65.22108387947083 	 3.4224157333374023 	 3.419495105743408 	 
2025-08-06 15:43:19.283770 test begin: paddle.Tensor.matmul(Tensor([124, 25, 257, 257],"float32"), Tensor([124, 25, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([124, 25, 257, 257],"float32"), Tensor([124, 25, 257, 64],"float32"), ) 	 255740700 	 9746 	 45.380011320114136 	 45.373722076416016 	 4.759341478347778 	 4.758973121643066 	 82.36597323417664 	 82.52079200744629 	 4.318750858306885 	 4.478406190872192 	 
2025-08-06 15:47:42.235380 test begin: paddle.Tensor.matmul(Tensor([124, 7, 257, 257],"float32"), Tensor([124, 7, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([124, 7, 257, 257],"float32"), Tensor([124, 7, 257, 64],"float32"), ) 	 71607396 	 9746 	 13.102456092834473 	 13.104641199111938 	 1.373401403427124 	 1.3733477592468262 	 23.545811891555786 	 23.551048278808594 	 1.2340118885040283 	 1.2355084419250488 	 
2025-08-06 15:48:57.279909 test begin: paddle.Tensor.matmul(Tensor([128, 11, 197, 197],"float32"), Tensor([128, 11, 197, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 11, 197, 197],"float32"), Tensor([128, 11, 197, 64],"float32"), ) 	 72395136 	 9746 	 10.804141283035278 	 10.81354546546936 	 1.1322996616363525 	 1.1419155597686768 	 18.34306573867798 	 18.340617179870605 	 0.961310863494873 	 0.9625072479248047 	 
2025-08-06 15:49:57.213935 test begin: paddle.Tensor.matmul(Tensor([128, 12, 168, 197],"float32"), Tensor([128, 12, 197, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 12, 168, 197],"float32"), Tensor([128, 12, 197, 64],"float32"), ) 	 70201344 	 9746 	 11.599512100219727 	 11.600467443466187 	 1.2158188819885254 	 1.2157566547393799 	 18.052178621292114 	 18.06301784515381 	 0.9457857608795166 	 0.9493162631988525 	 
2025-08-06 15:51:00.081044 test begin: paddle.Tensor.matmul(Tensor([128, 12, 197, 197],"float32"), Tensor([128, 12, 197, 168],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 12, 197, 197],"float32"), Tensor([128, 12, 197, 168],"float32"), ) 	 110446080 	 9746 	 22.729616403579712 	 22.73088049888611 	 2.385066509246826 	 2.3821933269500732 	 41.98690700531006 	 41.98399019241333 	 2.2003042697906494 	 2.201674699783325 	 
2025-08-06 15:53:12.250885 test begin: paddle.Tensor.matmul(Tensor([128, 16, 257, 257],"float32"), Tensor([128, 16, 257, 97],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 16, 257, 257],"float32"), Tensor([128, 16, 257, 97],"float32"), ) 	 186322944 	 9746 	 29.833364248275757 	 29.828104734420776 	 3.129703998565674 	 3.1265740394592285 	 66.91223454475403 	 66.90703225135803 	 3.5088651180267334 	 3.5090270042419434 	 
2025-08-06 15:56:29.726395 test begin: paddle.Tensor.matmul(Tensor([128, 16, 97, 257],"float32"), Tensor([128, 16, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 16, 97, 257],"float32"), Tensor([128, 16, 257, 64],"float32"), ) 	 84740096 	 9746 	 10.009116649627686 	 10.013004541397095 	 1.0493507385253906 	 1.0494420528411865 	 20.55073618888855 	 20.549254894256592 	 1.0769391059875488 	 1.0768425464630127 	 
2025-08-06 15:57:32.559958 test begin: paddle.Tensor.matmul(Tensor([128, 25, 257, 257],"float32"), Tensor([128, 25, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 25, 257, 257],"float32"), Tensor([128, 25, 257, 64],"float32"), ) 	 263990400 	 9746 	 46.86152696609497 	 46.466293811798096 	 4.8730244636535645 	 4.869945764541626 	 84.63552975654602 	 84.63346123695374 	 4.439135313034058 	 4.438514471054077 	 
2025-08-06 16:02:05.719657 test begin: paddle.Tensor.matmul(Tensor([128, 32, 197, 197],"float32"), Tensor([128, 32, 197, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 32, 197, 197],"float32"), Tensor([128, 32, 197, 64],"float32"), ) 	 210604032 	 9746 	 30.274634838104248 	 30.275654315948486 	 3.172982931137085 	 3.1760149002075195 	 51.98814558982849 	 51.99223256111145 	 2.727125644683838 	 2.7272276878356934 	 
2025-08-06 16:04:56.425862 test begin: paddle.Tensor.matmul(Tensor([128, 7, 257, 257],"float32"), Tensor([128, 7, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 7, 257, 257],"float32"), Tensor([128, 7, 257, 64],"float32"), ) 	 73917312 	 9746 	 13.126049757003784 	 13.1227867603302 	 1.3755238056182861 	 1.376981258392334 	 23.899975061416626 	 23.906644821166992 	 1.2538490295410156 	 1.2539994716644287 	 
2025-08-06 16:06:12.217589 test begin: paddle.Tensor.matmul(Tensor([194, 16, 257, 257],"float32"), Tensor([194, 16, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([194, 16, 257, 257],"float32"), Tensor([194, 16, 257, 64],"float32"), ) 	 256070688 	 9746 	 45.395925760269165 	 45.38852334022522 	 4.760157823562622 	 4.760563850402832 	 82.43563842773438 	 82.43911576271057 	 4.321369647979736 	 4.322619915008545 	 
2025-08-06 16:10:33.080316 test begin: paddle.Tensor.matmul(Tensor([336, 12, 197, 197],"float32"), Tensor([336, 12, 197, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([336, 12, 197, 197],"float32"), Tensor([336, 12, 197, 64],"float32"), ) 	 207313344 	 9746 	 31.11556124687195 	 29.871662139892578 	 3.1307268142700195 	 3.1333658695220947 	 51.259684801101685 	 51.24198055267334 	 2.6887428760528564 	 2.6881778240203857 	 
2025-08-05 21:38:00.820153 test begin: paddle.Tensor.matmul(Tensor([49, 16, 257, 257],"float32"), Tensor([49, 16, 257, 64],"float32"), )
W0805 21:38:02.117919 109028 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([49, 16, 257, 257],"float32"), Tensor([49, 16, 257, 64],"float32"), ) 	 64677648 	 9746 	 11.547179698944092 	 11.602681159973145 	 1.2107479572296143 	 1.2573132514953613 	 20.984713554382324 	 20.973902702331543 	 1.1051955223083496 	 1.099675178527832 	 
2025-08-05 21:39:10.017710 test begin: paddle.Tensor.max(Tensor([1, 400, 127009],"float32"), -2, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([1, 400, 127009],"float32"), -2, ) 	 50803600 	 61050 	 15.97619366645813 	 11.816377878189087 	 0.26738953590393066 	 0.19757699966430664 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 21:40:50.947890 test begin: paddle.Tensor.max(Tensor([1, 400, 127009],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([1, 400, 127009],"float32"), axis=-1, keepdim=True, ) 	 50803600 	 61050 	 10.934568643569946 	 9.742836236953735 	 0.09152960777282715 	 0.16306853294372559 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 21:42:21.301325 test begin: paddle.Tensor.max(Tensor([1, 772, 65856],"float32"), -2, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([1, 772, 65856],"float32"), -2, ) 	 50840832 	 61050 	 18.889166355133057 	 9.77965259552002 	 0.15815162658691406 	 0.1636953353881836 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 21:44:05.746663 test begin: paddle.Tensor.max(Tensor([1, 772, 65856],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([1, 772, 65856],"float32"), axis=-1, keepdim=True, ) 	 50840832 	 61050 	 9.994733572006226 	 9.611284971237183 	 0.08368682861328125 	 0.160888671875 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 21:45:30.644787 test begin: paddle.Tensor.max(Tensor([2, 400, 65856],"float32"), -2, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([2, 400, 65856],"float32"), -2, ) 	 52684800 	 61050 	 15.745826959609985 	 10.271466255187988 	 0.26352906227111816 	 0.1720433235168457 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 21:47:10.258214 test begin: paddle.Tensor.max(Tensor([2, 400, 65856],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([2, 400, 65856],"float32"), axis=-1, keepdim=True, ) 	 52684800 	 61050 	 10.149587154388428 	 9.897878170013428 	 0.08493518829345703 	 0.16565227508544922 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 21:48:38.603249 test begin: paddle.Tensor.max(Tensor([324000, 157],"float32"), axis=1, keepdim=True, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([324000, 157],"float32"), axis=1, keepdim=True, ) 	 50868000 	 61050 	 35.29016351699829 	 25.350893020629883 	 0.5907752513885498 	 0.4201793670654297 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 21:50:57.443059 test begin: paddle.Tensor.max(Tensor([635041, 80],"float32"), axis=1, keepdim=True, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([635041, 80],"float32"), axis=1, keepdim=True, ) 	 50803280 	 61050 	 32.91503024101257 	 32.930917263031006 	 0.5510921478271484 	 0.5517048835754395 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 21:53:20.010940 test begin: paddle.Tensor.mean(Tensor([124, 128, 34, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([124, 128, 34, 96],"float32"), 1, keepdim=True, ) 	 51806208 	 60303 	 12.274070024490356 	 9.280893325805664 	 0.208021879196167 	 0.1548013687133789 	 8.927104234695435 	 11.895553350448608 	 0.15154123306274414 	 0.20160913467407227 	 
2025-08-05 21:54:04.720337 test begin: paddle.Tensor.mean(Tensor([124, 128, 96, 34],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([124, 128, 96, 34],"float32"), 1, keepdim=True, ) 	 51806208 	 60303 	 12.274420261383057 	 9.128686428070068 	 0.20801424980163574 	 0.15468525886535645 	 8.908107042312622 	 11.909618377685547 	 0.15096521377563477 	 0.20113611221313477 	 
2025-08-05 21:54:48.249843 test begin: paddle.Tensor.mean(Tensor([124, 45, 96, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([124, 45, 96, 96],"float32"), 1, keepdim=True, ) 	 51425280 	 60303 	 10.005101680755615 	 9.486019134521484 	 0.16946125030517578 	 0.16072392463684082 	 8.914401531219482 	 12.817242860794067 	 0.15028142929077148 	 0.2168886661529541 	 
2025-08-05 21:55:31.615415 test begin: paddle.Tensor.mean(Tensor([128, 128, 33, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([128, 128, 33, 96],"float32"), 1, keepdim=True, ) 	 51904512 	 60303 	 12.352129697799683 	 9.20318341255188 	 0.20936989784240723 	 0.1558690071105957 	 8.843594074249268 	 11.93662142753601 	 0.15121746063232422 	 0.20250320434570312 	 
2025-08-05 21:56:14.833318 test begin: paddle.Tensor.mean(Tensor([128, 128, 96, 33],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([128, 128, 96, 33],"float32"), 1, keepdim=True, ) 	 51904512 	 60303 	 12.352149248123169 	 10.859428644180298 	 0.20937895774841309 	 0.15588617324829102 	 8.853224754333496 	 11.940104961395264 	 0.1511216163635254 	 0.20230484008789062 	 
2025-08-05 21:57:00.588613 test begin: paddle.Tensor.mean(Tensor([128, 192, 22, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([128, 192, 22, 96],"float32"), 1, keepdim=True, ) 	 51904512 	 60303 	 13.160230159759521 	 9.071484804153442 	 0.2230837345123291 	 0.1537020206451416 	 8.871275663375854 	 11.854287385940552 	 0.15050220489501953 	 0.20090985298156738 	 
2025-08-05 21:57:45.603856 test begin: paddle.Tensor.mean(Tensor([128, 192, 96, 22],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([128, 192, 96, 22],"float32"), 1, keepdim=True, ) 	 51904512 	 60303 	 13.159197568893433 	 9.07133674621582 	 0.22310853004455566 	 0.1537306308746338 	 8.838072776794434 	 11.854492425918579 	 0.14993691444396973 	 0.20085835456848145 	 
2025-08-05 21:58:29.511720 test begin: paddle.Tensor.mean(Tensor([128, 44, 96, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([128, 44, 96, 96],"float32"), 1, keepdim=True, ) 	 51904512 	 60303 	 10.10068964958191 	 9.488183736801147 	 0.1712038516998291 	 0.16078448295593262 	 8.975977897644043 	 12.975261449813843 	 0.15256166458129883 	 0.2198808193206787 	 
2025-08-05 21:59:14.247554 test begin: paddle.Tensor.mean(Tensor([29, 192, 96, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([29, 192, 96, 96],"float32"), 1, keepdim=True, ) 	 51314688 	 60303 	 9.99618148803711 	 8.929549217224121 	 0.16941165924072266 	 0.15129303932189941 	 8.682774305343628 	 11.584523916244507 	 0.1481459140777588 	 0.19633126258850098 	 
2025-08-05 21:59:56.219637 test begin: paddle.Tensor.mean(Tensor([44, 128, 96, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([44, 128, 96, 96],"float32"), 1, keepdim=True, ) 	 51904512 	 60303 	 11.286406993865967 	 9.091153383255005 	 0.17119646072387695 	 0.15407490730285645 	 8.797701597213745 	 12.164535760879517 	 0.1488184928894043 	 0.2061309814453125 	 
2025-08-05 22:00:39.081100 test begin: paddle.Tensor.min(Tensor([1, 193, 65856, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([1, 193, 65856, 4],"float32"), axis=-1, ) 	 50840832 	 45506 	 25.63126564025879 	 39.495808839797974 	 0.54972243309021 	 0.8860149383544922 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 22:02:53.394565 test begin: paddle.Tensor.min(Tensor([1, 400, 31753, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([1, 400, 31753, 4],"float32"), axis=-1, ) 	 50804800 	 45506 	 24.47894048690796 	 39.47485613822937 	 0.5501666069030762 	 0.8865180015563965 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 22:05:04.613329 test begin: paddle.Tensor.min(Tensor([1, 400, 65856, 2],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([1, 400, 65856, 2],"float32"), axis=-1, ) 	 52684800 	 45506 	 23.530306339263916 	 37.93413066864014 	 0.5284850597381592 	 0.839627742767334 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 22:07:20.399330 test begin: paddle.Tensor.min(Tensor([1, 400, 65856, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([1, 400, 65856, 4],"float32"), axis=-1, ) 	 105369600 	 45506 	 50.51480197906494 	 81.56425142288208 	 1.134467601776123 	 1.8318884372711182 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 22:11:45.903696 test begin: paddle.Tensor.min(Tensor([15661, 4, 811],"float32"), axis=1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([15661, 4, 811],"float32"), axis=1, ) 	 50804284 	 45506 	 10.002212047576904 	 13.583453178405762 	 0.2246570587158203 	 0.30509090423583984 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 22:13:06.136054 test begin: paddle.Tensor.min(Tensor([24565, 3, 811],"float32"), axis=1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([24565, 3, 811],"float32"), axis=1, ) 	 59766645 	 45506 	 13.895474672317505 	 18.482460498809814 	 0.31217288970947266 	 0.4149818420410156 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 22:14:50.921042 test begin: paddle.Tensor.min(Tensor([24565, 4, 518],"float32"), axis=1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([24565, 4, 518],"float32"), axis=1, ) 	 50898680 	 45506 	 10.315613031387329 	 12.085132598876953 	 0.23170042037963867 	 0.27141475677490234 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 22:16:10.779397 test begin: paddle.Tensor.min(Tensor([3, 525, 12096, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([3, 525, 12096, 4],"float32"), axis=-1, ) 	 76204800 	 45506 	 36.63968849182129 	 59.073513984680176 	 0.8227453231811523 	 1.3264169692993164 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 22:19:25.750721 test begin: paddle.Tensor.min(Tensor([4, 263, 12096, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([4, 263, 12096, 4],"float32"), axis=-1, ) 	 50899968 	 45506 	 24.546464204788208 	 39.53401279449463 	 0.55133056640625 	 0.8869974613189697 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 22:21:35.089866 test begin: paddle.Tensor.min(Tensor([4, 525, 12096, 3],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([4, 525, 12096, 3],"float32"), axis=-1, ) 	 76204800 	 45506 	 24.493589878082275 	 40.44810175895691 	 0.5381810665130615 	 0.9082422256469727 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 22:24:14.320761 test begin: paddle.Tensor.min(Tensor([4, 525, 6049, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([4, 525, 6049, 4],"float32"), axis=-1, ) 	 50811600 	 45506 	 24.490422010421753 	 39.42454552650452 	 0.5500550270080566 	 0.8854336738586426 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-08-05 22:26:25.025172 test begin: paddle.Tensor.mm(Tensor([10, 10],"float32"), Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.mm 	 paddle.Tensor.mm(Tensor([10, 10],"float32"), Tensor([10, 5080321],"float32"), ) 	 50803310 	 15341 	 10.03584098815918 	 10.017148971557617 	 0.1337110996246338 	 0.13344168663024902 	 22.17656970024109 	 22.13711142539978 	 0.21103668212890625 	 0.21064496040344238 	 
2025-08-05 22:27:33.381441 test begin: paddle.Tensor.mm(Tensor([5080321, 10],"float32"), Tensor([10, 10],"float32"), )
[Prof] paddle.Tensor.mm 	 paddle.Tensor.mm(Tensor([5080321, 10],"float32"), Tensor([10, 10],"float32"), ) 	 50803310 	 15341 	 9.995080471038818 	 9.984151363372803 	 0.13313603401184082 	 0.13298797607421875 	 21.700702905654907 	 21.662900686264038 	 0.20648431777954102 	 0.20609045028686523 	 
2025-08-05 22:28:42.331726 test begin: paddle.Tensor.mode(Tensor([3, 2, 4233601],"float64"), )
[Prof] paddle.Tensor.mode 	 paddle.Tensor.mode(Tensor([3, 2, 4233601],"float64"), ) 	 25401606 	 1000 	 50.30563282966614 	 9.400331020355225 	 0.00025773048400878906 	 0.0005950927734375 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 22:29:48.512276 test begin: paddle.Tensor.mode(Tensor([3, 2, 4233601],"float64"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.mode 	 paddle.Tensor.mode(Tensor([3, 2, 4233601],"float64"), axis=2, keepdim=True, ) 	 25401606 	 1000 	 60.953391551971436 	 9.494344472885132 	 0.00012946128845214844 	 0.0006313323974609375 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 22:31:02.004253 test begin: paddle.Tensor.mode(Tensor([3, 2822401, 3],"float64"), axis=1, keepdim=False, )
[Prof] paddle.Tensor.mode 	 paddle.Tensor.mode(Tensor([3, 2822401, 3],"float64"), axis=1, keepdim=False, ) 	 25401609 	 1000 	 73.61777973175049 	 10.724910259246826 	 0.0001418590545654297 	 0.0002415180206298828 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-05 22:32:32.998642 test begin: paddle.Tensor.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=0, destination=2, ) 	 254018100 	 1457722 	 17.639769077301025 	 12.498253583908081 	 0.00017642974853515625 	 0.000274658203125 	 69.152907371521 	 81.17913866043091 	 0.00012803077697753906 	 0.000232696533203125 	 
2025-08-05 22:35:46.982173 test begin: paddle.Tensor.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([1209610, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254018100 	 1457722 	 11.140780687332153 	 9.556818008422852 	 0.00013780593872070312 	 0.0002951622009277344 	 58.806654930114746 	 80.62479162216187 	 0.00011777877807617188 	 0.0002269744873046875 	 
2025-08-05 22:38:39.174781 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 1058401],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 1058401],"float64"), source=0, destination=2, ) 	 254016240 	 1457722 	 10.121206760406494 	 6.680440902709961 	 0.00012731552124023438 	 0.00026226043701171875 	 58.945061445236206 	 78.62896847724915 	 0.00011157989501953125 	 0.0002238750457763672 	 
2025-08-05 22:41:24.174107 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=0, destination=2, ) 	 254017680 	 1457722 	 16.483880519866943 	 6.709892749786377 	 0.00015807151794433594 	 9.441375732421875e-05 	 60.985920429229736 	 79.50160813331604 	 0.00012755393981933594 	 0.00023126602172851562 	 
2025-08-05 22:44:21.177490 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 151201, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254017680 	 1457722 	 12.444952011108398 	 8.315762281417847 	 0.00030541419982910156 	 0.0006718635559082031 	 58.855650424957275 	 100.42411541938782 	 0.00011229515075683594 	 0.0002460479736328125 	 
2025-08-05 22:47:33.407880 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=0, destination=2, ) 	 254017200 	 1457722 	 11.042941093444824 	 6.670679092407227 	 0.00013494491577148438 	 0.00029587745666503906 	 58.36994934082031 	 101.67862153053284 	 0.00011658668518066406 	 0.00021839141845703125 	 
2025-08-05 22:50:43.086667 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 3, 5, 211681],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254017200 	 1457722 	 11.114331722259521 	 8.311313390731812 	 0.0001232624053955078 	 0.000274658203125 	 58.864331007003784 	 101.51893281936646 	 0.00010657310485839844 	 0.00022721290588378906 	 
2025-08-05 22:53:55.948993 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 635041, 5],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 635041, 5],"float64"), source=0, destination=2, ) 	 254016400 	 1457722 	 9.953558444976807 	 6.72553825378418 	 7.343292236328125e-05 	 0.00014328956604003906 	 58.498422145843506 	 93.4285933971405 	 0.00011372566223144531 	 0.00021982192993164062 	 
2025-08-05 22:56:55.455973 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=0, destination=2, ) 	 254018800 	 1457722 	 11.212499380111694 	 6.800904035568237 	 0.0001323223114013672 	 0.00012493133544921875 	 59.00809955596924 	 78.78053426742554 	 0.00010371208190917969 	 0.00025653839111328125 	 
2025-08-05 22:59:43.727781 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 2, 90721, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254018800 	 1457722 	 10.923455238342285 	 8.290775775909424 	 0.00013113021850585938 	 8.535385131835938e-05 	 59.03368926048279 	 79.56265449523926 	 0.00010728836059570312 	 0.00021123886108398438 	 
2025-08-05 23:02:35.845713 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 423361, 3, 5],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 423361, 3, 5],"float64"), source=0, destination=2, ) 	 254016600 	 1457722 	 10.00174617767334 	 6.715398550033569 	 0.0001633167266845703 	 0.0006628036499023438 	 58.955495834350586 	 92.15339040756226 	 0.0002799034118652344 	 0.00022554397583007812 	 
2025-08-05 23:05:34.827253 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=0, destination=2, ) 	 254020200 	 1457722 	 10.53796672821045 	 6.721175909042358 	 0.00015616416931152344 	 0.00024962425231933594 	 59.362828969955444 	 101.58576250076294 	 0.00019431114196777344 	 0.0005662441253662109 	 
2025-08-05 23:08:46.657836 test begin: paddle.Tensor.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([40, 60481, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 254020200 	 1457722 	 12.180265665054321 	 8.374134063720703 	 0.00013113021850585938 	 0.00015473365783691406 	 60.224040031433105 	 102.36968278884888 	 0.0001163482666015625 	 0.00024366378784179688 	 
2025-08-05 23:12:01.940482 test begin: paddle.Tensor.moveaxis(x=Tensor([8467210, 2, 3, 5],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([8467210, 2, 3, 5],"float64"), source=0, destination=2, ) 	 254016300 	 1457722 	 9.973675727844238 	 6.803235292434692 	 0.00013399124145507812 	 0.0001513957977294922 	 59.648898124694824 	 101.68129014968872 	 0.00010776519775390625 	 0.00023818016052246094 	 
2025-08-05 23:15:11.243610 test begin: paddle.Tensor.multiply(Tensor([132301, 768],"float16"), Tensor([132301, 1],"float32"), )
W0805 23:15:13.673285 112895 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([132301, 768],"float16"), Tensor([132301, 1],"float32"), ) 	 101739469 	 33843 	 36.09596824645996 	 24.625251293182373 	 0.5450890064239502 	 0.7436423301696777 	 64.52255511283875 	 71.99361968040466 	 0.6489953994750977 	 0.5431714057922363 	 
2025-08-05 23:18:32.786289 test begin: paddle.Tensor.multiply(Tensor([160, 317521],"float32"), Tensor([160, 1],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([160, 317521],"float32"), Tensor([160, 1],"float32"), ) 	 50803520 	 33843 	 10.030310153961182 	 10.261246681213379 	 0.302844762802124 	 0.3098876476287842 	 26.107703924179077 	 31.216725826263428 	 0.26247477531433105 	 0.2353353500366211 	 
2025-08-05 23:19:52.141480 test begin: paddle.Tensor.multiply(Tensor([160, 317521],"float32"), Tensor([160, 317521],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([160, 317521],"float32"), Tensor([160, 317521],"float32"), ) 	 101606720 	 33843 	 15.239461898803711 	 15.115417003631592 	 0.4602055549621582 	 0.4564495086669922 	 36.353554010391235 	 30.222352504730225 	 1.097679615020752 	 0.45636820793151855 	 
2025-08-05 23:21:31.642990 test begin: paddle.Tensor.multiply(Tensor([160, 635041],"float16"), Tensor([160, 1],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([160, 635041],"float16"), Tensor([160, 1],"float32"), ) 	 101606720 	 33843 	 36.13935351371765 	 23.52184820175171 	 0.5457777976989746 	 0.7106552124023438 	 65.01854276657104 	 72.30095362663269 	 0.4903111457824707 	 0.43613243103027344 	 
2025-08-05 23:24:53.983292 test begin: paddle.Tensor.multiply(Tensor([16538, 3072],"float32"), Tensor([16538, 1],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([16538, 3072],"float32"), Tensor([16538, 1],"float32"), ) 	 50821274 	 33843 	 9.998405456542969 	 10.411590814590454 	 0.30193424224853516 	 0.31444692611694336 	 24.916881561279297 	 30.47041368484497 	 0.37616658210754395 	 0.3066120147705078 	 
2025-08-05 23:26:12.137252 test begin: paddle.Tensor.multiply(Tensor([33076, 3072],"float16"), Tensor([33076, 1],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([33076, 3072],"float16"), Tensor([33076, 1],"float32"), ) 	 101642548 	 33843 	 36.14426827430725 	 24.481581687927246 	 0.5456926822662354 	 0.739316463470459 	 64.36450123786926 	 72.11055660247803 	 0.6473321914672852 	 0.5440866947174072 	 
2025-08-05 23:29:32.821666 test begin: paddle.Tensor.multiply(Tensor([512, 198451],"float16"), Tensor([512, 1],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([512, 198451],"float16"), Tensor([512, 1],"float32"), ) 	 101607424 	 33843 	 36.22884392738342 	 23.541603803634644 	 0.5469582080841064 	 0.7109096050262451 	 65.82791066169739 	 72.95030379295349 	 0.496443510055542 	 0.5504107475280762 	 
2025-08-05 23:32:54.995839 test begin: paddle.Tensor.nansum(Tensor([105841, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([105841, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401840 	 10691 	 11.385929584503174 	 2.0239906311035156 	 0.27230405807495117 	 0.19345927238464355 	 5.64109992980957 	 4.720527172088623 	 0.26961660385131836 	 0.15038371086120605 	 
2025-08-05 23:33:19.679148 test begin: paddle.Tensor.nansum(Tensor([2822401, 3, 3],"float64"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([2822401, 3, 3],"float64"), ) 	 25401609 	 10691 	 9.99876070022583 	 1.6014018058776855 	 0.19093108177185059 	 0.07658171653747559 	 4.969261646270752 	 4.417142152786255 	 0.23745441436767578 	 0.1407005786895752 	 
2025-08-05 23:33:43.251473 test begin: paddle.Tensor.nansum(Tensor([3, 2, 105841, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2, 105841, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401840 	 10691 	 11.385605096817017 	 2.273406982421875 	 0.2723510265350342 	 0.19347095489501953 	 5.640845775604248 	 4.720752716064453 	 0.26962757110595703 	 0.15036749839782715 	 
2025-08-05 23:34:10.506850 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 141121, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2, 3, 141121, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401780 	 10691 	 60.42645049095154 	 1.8393161296844482 	 1.157968282699585 	 0.08794999122619629 	 4.977754354476929 	 4.462961673736572 	 0.237900972366333 	 0.14215469360351562 	 
2025-08-05 23:35:22.799944 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 176401, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 176401, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401744 	 10691 	 10.415391206741333 	 3.081376075744629 	 0.2504589557647705 	 0.19411683082580566 	 5.467558145523071 	 4.740269184112549 	 0.261307954788208 	 0.1510157585144043 	 
2025-08-05 23:35:49.277394 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 1, 70561],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 1, 70561],"float64"), axis=3, keepdim=True, ) 	 25401960 	 10691 	 10.42465853691101 	 2.0335512161254883 	 0.24927592277526855 	 0.19437909126281738 	 5.4703428745269775 	 4.745549440383911 	 0.26146674156188965 	 0.1511228084564209 	 
2025-08-05 23:36:12.649417 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 35281, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 35281, 2],"float64"), axis=3, keepdim=True, ) 	 25402320 	 10691 	 10.415180444717407 	 2.0304434299468994 	 0.2489948272705078 	 0.19403767585754395 	 5.468700170516968 	 4.759193420410156 	 0.26140594482421875 	 0.15157341957092285 	 
2025-08-05 23:36:38.888308 test begin: paddle.Tensor.nansum(Tensor([3, 2822401, 3],"float64"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2822401, 3],"float64"), ) 	 25401609 	 10691 	 9.99976921081543 	 1.601088285446167 	 0.19094586372375488 	 0.07648015022277832 	 4.968231201171875 	 4.4159815311431885 	 0.23747563362121582 	 0.14069390296936035 	 
2025-08-05 23:37:00.448082 test begin: paddle.Tensor.nansum(Tensor([3, 3, 2822401],"float64"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 3, 2822401],"float64"), ) 	 25401609 	 10691 	 9.998671531677246 	 1.600949764251709 	 0.19095301628112793 	 0.0765371322631836 	 4.968062400817871 	 4.416191101074219 	 0.23741817474365234 	 0.1406385898590088 	 
2025-08-05 23:37:22.033416 test begin: paddle.Tensor.nansum(Tensor([3, 3, 5644801],"float32"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 3, 5644801],"float32"), ) 	 50803209 	 10691 	 10.792948246002197 	 1.6309483051300049 	 0.20619702339172363 	 0.07797718048095703 	 5.971724033355713 	 6.300533294677734 	 0.2854146957397461 	 0.20071172714233398 	 
2025-08-05 23:37:49.380061 test begin: paddle.Tensor.nansum(Tensor([3, 5644801, 3],"float32"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 5644801, 3],"float32"), ) 	 50803209 	 10691 	 10.79156756401062 	 1.6309845447540283 	 0.20618605613708496 	 0.07798624038696289 	 5.970751762390137 	 6.302940130233765 	 0.2853710651397705 	 0.2008039951324463 	 
2025-08-05 23:38:14.940533 test begin: paddle.Tensor.nansum(Tensor([3, 70561, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 70561, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401960 	 10691 	 11.37974214553833 	 2.023984909057617 	 0.27223801612854004 	 0.19345617294311523 	 5.668142557144165 	 4.7247583866119385 	 0.27086925506591797 	 0.15047979354858398 	 
2025-08-05 23:38:43.652017 test begin: paddle.Tensor.nansum(Tensor([5644801, 3, 3],"float32"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([5644801, 3, 3],"float32"), ) 	 50803209 	 10691 	 10.79316782951355 	 1.6305935382843018 	 0.20609593391418457 	 0.07794427871704102 	 5.973150014877319 	 6.3076183795928955 	 0.285595178604126 	 0.200944185256958 	 
2025-08-05 23:39:09.399210 test begin: paddle.Tensor.neg(Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.neg 	 paddle.Tensor.neg(Tensor([50803201],"float32"), ) 	 50803201 	 33807 	 9.995673894882202 	 10.064117431640625 	 0.3021667003631592 	 0.30426955223083496 	 9.998630285263062 	 10.064186334609985 	 0.3023090362548828 	 0.3043065071105957 	 
2025-08-05 23:39:53.081165 test begin: paddle.Tensor.nonzero(Tensor([3628801, 14],"bool"), )
[Prof] paddle.Tensor.nonzero 	 paddle.Tensor.nonzero(Tensor([3628801, 14],"bool"), ) 	 50803214 	 1684 	 10.015019655227661 	 2.384366273880005 	 0.004055023193359375 	 0.0013132095336914062 	 None 	 None 	 None 	 None 	 
2025-08-05 23:40:06.306110 test begin: paddle.Tensor.nonzero(Tensor([3907939, 13],"bool"), )
[Prof] paddle.Tensor.nonzero 	 paddle.Tensor.nonzero(Tensor([3907939, 13],"bool"), ) 	 50803207 	 1684 	 9.991406202316284 	 2.37559175491333 	 0.004055023193359375 	 0.001317739486694336 	 None 	 None 	 None 	 None 	 
2025-08-05 23:40:19.379134 test begin: paddle.Tensor.nonzero(Tensor([4233601, 12],"bool"), )
[Prof] paddle.Tensor.nonzero 	 paddle.Tensor.nonzero(Tensor([4233601, 12],"bool"), ) 	 50803212 	 1684 	 10.016295671463013 	 2.387157440185547 	 0.004061460494995117 	 0.001306772232055664 	 None 	 None 	 None 	 None 	 
2025-08-05 23:40:32.534240 test begin: paddle.Tensor.nonzero(Tensor([52640, 966],"bool"), )
[Prof] paddle.Tensor.nonzero 	 paddle.Tensor.nonzero(Tensor([52640, 966],"bool"), ) 	 50850240 	 1684 	 10.000320196151733 	 2.390061378479004 	 0.0040683746337890625 	 0.001316070556640625 	 None 	 None 	 None 	 None 	 
2025-08-05 23:40:45.644134 test begin: paddle.Tensor.norm(Tensor([100352, 507],"float32"), )
[Prof] paddle.Tensor.norm 	 paddle.Tensor.norm(Tensor([100352, 507],"float32"), ) 	 50878464 	 65593 	 10.023472309112549 	 9.982029676437378 	 0.051980018615722656 	 0.07782483100891113 	 65.39984488487244 	 59.81888270378113 	 1.0188806056976318 	 0.23312616348266602 	 
2025-08-05 23:43:13.699675 test begin: paddle.Tensor.norm(Tensor([507, 100352],"float32"), )
[Prof] paddle.Tensor.norm 	 paddle.Tensor.norm(Tensor([507, 100352],"float32"), ) 	 50878464 	 65593 	 10.024030685424805 	 9.98375678062439 	 0.05197024345397949 	 0.07780885696411133 	 65.39260721206665 	 59.81761336326599 	 1.0187935829162598 	 0.23311138153076172 	 
2025-08-05 23:45:41.491391 test begin: paddle.Tensor.norm(Tensor([6202, 8192],"float32"), )
[Prof] paddle.Tensor.norm 	 paddle.Tensor.norm(Tensor([6202, 8192],"float32"), ) 	 50806784 	 65593 	 9.995582342147827 	 9.96985387802124 	 0.051822662353515625 	 0.07766914367675781 	 65.33120059967041 	 59.70675706863403 	 1.0178401470184326 	 0.23269009590148926 	 
2025-08-05 23:48:07.584131 test begin: paddle.Tensor.norm(Tensor([8192, 6202],"float32"), )
[Prof] paddle.Tensor.norm 	 paddle.Tensor.norm(Tensor([8192, 6202],"float32"), ) 	 50806784 	 65593 	 9.996182918548584 	 9.969436168670654 	 0.05182147026062012 	 0.07764673233032227 	 65.33210372924805 	 59.706209897994995 	 1.017869472503662 	 0.23270773887634277 	 
2025-08-05 23:50:33.578625 test begin: paddle.Tensor.norm(Tensor([886, 57344],"float32"), )
[Prof] paddle.Tensor.norm 	 paddle.Tensor.norm(Tensor([886, 57344],"float32"), ) 	 50806784 	 65593 	 9.996612787246704 	 9.969958543777466 	 0.05183005332946777 	 0.0776824951171875 	 65.33368515968323 	 59.7057089805603 	 1.0178885459899902 	 0.23268890380859375 	 
2025-08-05 23:52:59.558273 test begin: paddle.Tensor.not_equal(Tensor([128, 198451],"int64"), Tensor([128, 198451],"int64"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([128, 198451],"int64"), Tensor([128, 198451],"int64"), ) 	 50803456 	 72509 	 22.412041425704956 	 22.695252656936646 	 0.31589794158935547 	 0.3198566436767578 	 None 	 None 	 None 	 None 	 
2025-08-05 23:53:45.834155 test begin: paddle.Tensor.not_equal(Tensor([13, 1953970],"int64"), Tensor([1],"int64"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([13, 1953970],"int64"), Tensor([1],"int64"), ) 	 25401611 	 72509 	 12.772869348526001 	 13.059356927871704 	 0.18004369735717773 	 0.18400788307189941 	 None 	 None 	 None 	 None 	 
2025-08-05 23:54:12.128362 test begin: paddle.Tensor.not_equal(Tensor([13, 3907939],"bool"), Tensor([1],"bool"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([13, 3907939],"bool"), Tensor([1],"bool"), ) 	 50803208 	 72509 	 9.983827352523804 	 14.361973762512207 	 0.14071035385131836 	 0.20241999626159668 	 None 	 None 	 None 	 None 	 
2025-08-05 23:54:38.822379 test begin: paddle.Tensor.not_equal(Tensor([1814401, 14],"int64"), Tensor([1],"int64"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([1814401, 14],"int64"), Tensor([1],"int64"), ) 	 25401615 	 72509 	 12.77342176437378 	 13.05860710144043 	 0.18004274368286133 	 0.18399930000305176 	 None 	 None 	 None 	 None 	 
2025-08-05 23:55:05.148149 test begin: paddle.Tensor.not_equal(Tensor([198451, 128],"int64"), Tensor([198451, 128],"int64"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([198451, 128],"int64"), Tensor([198451, 128],"int64"), ) 	 50803456 	 72509 	 22.410664319992065 	 22.6964373588562 	 0.31587886810302734 	 0.31987547874450684 	 None 	 None 	 None 	 None 	 
2025-08-05 23:55:51.387004 test begin: paddle.Tensor.not_equal(Tensor([3628801, 14],"bool"), Tensor([1],"bool"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([3628801, 14],"bool"), Tensor([1],"bool"), ) 	 50803215 	 72509 	 9.996031045913696 	 14.368059635162354 	 0.14085674285888672 	 0.2026071548461914 	 None 	 None 	 None 	 None 	 
2025-08-05 23:56:16.451731 test begin: paddle.Tensor.outer(x=Tensor([12700801, 2],"float64"), y=Tensor([2, 3, 4],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([12700801, 2],"float64"), y=Tensor([2, 3, 4],"float64"), ) 	 25401626 	 6606 	 25.615681171417236 	 25.210892915725708 	 0.1584792137145996 	 0.9747776985168457 	 49.47492027282715 	 151.06958985328674 	 2.550359010696411 	 1.167159080505371 	 
2025-08-06 00:00:41.837488 test begin: paddle.Tensor.outer(x=Tensor([4, 2, 3175201],"float64"), y=Tensor([4, 2, 3],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2, 3175201],"float64"), y=Tensor([4, 2, 3],"float64"), ) 	 25401632 	 6606 	 25.614521026611328 	 25.276044368743896 	 0.1585984230041504 	 0.9776182174682617 	 49.46755814552307 	 150.7007474899292 	 2.5501198768615723 	 1.1640233993530273 	 
2025-08-06 00:05:08.028429 test begin: paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4, 2, 3175201],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4, 2, 3175201],"float64"), ) 	 25401632 	 6606 	 26.691653966903687 	 46.977877378463745 	 0.16518688201904297 	 1.8164641857147217 	 49.15365934371948 	 168.66069078445435 	 2.5337729454040527 	 1.304330587387085 	 
2025-08-06 00:10:16.610386 test begin: paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4, 2116801, 3],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4, 2116801, 3],"float64"), ) 	 25401636 	 6606 	 26.63945245742798 	 46.76089286804199 	 0.1648705005645752 	 1.8085808753967285 	 49.208155155181885 	 168.54356813430786 	 2.53686785697937 	 1.3556270599365234 	 
2025-08-06 00:15:21.604318 test begin: paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4233601, 2, 3],"float64"), ) 	 25401630 	 6606 	 28.7394917011261 	 47.05622982978821 	 0.1702592372894287 	 1.8200292587280273 	 49.221869230270386 	 168.75017666816711 	 2.5372860431671143 	 1.3049976825714111 	 
2025-08-06 00:20:31.364404 test begin: paddle.Tensor.outer(x=Tensor([4, 2116801, 3],"float64"), y=Tensor([4, 2, 3],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2116801, 3],"float64"), y=Tensor([4, 2, 3],"float64"), ) 	 25401636 	 6606 	 25.61647868156433 	 25.296550035476685 	 0.15847468376159668 	 0.9782469272613525 	 49.468817710876465 	 150.6880099773407 	 2.5497355461120605 	 1.1640293598175049 	 
2025-08-06 00:24:56.773131 test begin: paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2, 3, 4233601],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2, 3, 4233601],"float64"), ) 	 25401614 	 6606 	 10.077553987503052 	 15.639061450958252 	 0.06230807304382324 	 2.4186878204345703 	 18.203325033187866 	 55.07481408119202 	 0.938359260559082 	 1.7031593322753906 	 
2025-08-06 00:26:42.606405 test begin: paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2, 3175201, 4],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2, 3175201, 4],"float64"), ) 	 25401616 	 6606 	 10.033718347549438 	 15.58205795288086 	 0.06195831298828125 	 2.410623788833618 	 18.173920154571533 	 55.04912877082825 	 0.9368200302124023 	 1.7023940086364746 	 
2025-08-06 00:28:26.492084 test begin: paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2116801, 3, 4],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2116801, 3, 4],"float64"), ) 	 25401620 	 6606 	 9.98491907119751 	 15.743168115615845 	 0.061769723892211914 	 2.612133264541626 	 18.186749696731567 	 55.03082823753357 	 0.9372360706329346 	 1.7016289234161377 	 
2025-08-06 00:30:10.407661 test begin: paddle.Tensor.outer(x=Tensor([4, 6350401],"float64"), y=Tensor([2, 3, 4],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 6350401],"float64"), y=Tensor([2, 3, 4],"float64"), ) 	 25401628 	 6606 	 25.6156644821167 	 25.33728528022766 	 0.1585226058959961 	 0.978689432144165 	 49.47126317024231 	 150.43302154541016 	 2.549968957901001 	 1.162261724472046 	 
2025-08-06 00:34:39.126723 test begin: paddle.Tensor.outer(x=Tensor([4233601, 2, 3],"float64"), y=Tensor([4, 2, 3],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4233601, 2, 3],"float64"), y=Tensor([4, 2, 3],"float64"), ) 	 25401630 	 6606 	 26.8408465385437 	 25.23540949821472 	 0.15853071212768555 	 0.9760541915893555 	 49.4774432182312 	 150.58022379875183 	 2.5503878593444824 	 1.1633193492889404 	 
2025-08-06 00:39:05.836553 test begin: paddle.Tensor.pow(Tensor([124, 128, 34, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([124, 128, 34, 96],"float32"), 2, ) 	 51806208 	 26857 	 10.383910894393921 	 8.353301286697388 	 0.39618563652038574 	 0.31005167961120605 	 12.319494009017944 	 28.812084674835205 	 0.4687225818634033 	 0.3655674457550049 	 
2025-08-06 00:40:08.941748 test begin: paddle.Tensor.pow(Tensor([124, 128, 96, 34],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([124, 128, 96, 34],"float32"), 2, ) 	 51806208 	 26857 	 10.19709038734436 	 8.147666692733765 	 0.3894774913787842 	 0.31006312370300293 	 12.351442813873291 	 28.81224799156189 	 0.4694805145263672 	 0.36553311347961426 	 
2025-08-06 00:41:12.576408 test begin: paddle.Tensor.pow(Tensor([124, 45, 96, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([124, 45, 96, 96],"float32"), 2, ) 	 51425280 	 26857 	 10.059988260269165 	 8.325493335723877 	 0.3844313621520996 	 0.30785059928894043 	 12.267081022262573 	 28.60283613204956 	 0.46715760231018066 	 0.36286401748657227 	 
2025-08-06 00:42:15.094426 test begin: paddle.Tensor.pow(Tensor([128, 128, 33, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([128, 128, 33, 96],"float32"), 2, ) 	 51904512 	 26857 	 10.189713478088379 	 8.406542539596558 	 0.3901703357696533 	 0.3106710910797119 	 12.373345136642456 	 28.867141723632812 	 0.4702303409576416 	 0.3662550449371338 	 
2025-08-06 00:43:19.151353 test begin: paddle.Tensor.pow(Tensor([128, 128, 96, 33],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([128, 128, 96, 33],"float32"), 2, ) 	 51904512 	 26857 	 10.22509765625 	 8.173145055770874 	 0.39019227027893066 	 0.31066226959228516 	 12.373491048812866 	 28.867050170898438 	 0.47048139572143555 	 0.3662080764770508 	 
2025-08-06 00:44:20.591848 test begin: paddle.Tensor.pow(Tensor([128, 192, 22, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([128, 192, 22, 96],"float32"), 2, ) 	 51904512 	 26857 	 10.140436887741089 	 8.165475130081177 	 0.3886744976043701 	 0.31069135665893555 	 12.392405033111572 	 28.867121696472168 	 0.4715886116027832 	 0.3661386966705322 	 
2025-08-06 00:45:22.456071 test begin: paddle.Tensor.pow(Tensor([128, 192, 96, 22],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([128, 192, 96, 22],"float32"), 2, ) 	 51904512 	 26857 	 10.188325881958008 	 8.164209127426147 	 0.38932204246520996 	 0.3107571601867676 	 12.363399505615234 	 28.8687846660614 	 0.47057342529296875 	 0.3662271499633789 	 
2025-08-06 00:46:25.892710 test begin: paddle.Tensor.pow(Tensor([128, 44, 96, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([128, 44, 96, 96],"float32"), 2, ) 	 51904512 	 26857 	 10.214901685714722 	 8.166451692581177 	 0.39169979095458984 	 0.31067657470703125 	 12.368873119354248 	 28.870434284210205 	 0.47043943405151367 	 0.36621737480163574 	 
2025-08-06 00:47:27.466910 test begin: paddle.Tensor.pow(Tensor([29, 192, 96, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([29, 192, 96, 96],"float32"), 2, ) 	 51314688 	 26857 	 10.121438980102539 	 8.073888063430786 	 0.3867156505584717 	 0.3072338104248047 	 12.234968900680542 	 28.54712986946106 	 0.4653763771057129 	 0.3621220588684082 	 
2025-08-06 00:48:28.234082 test begin: paddle.Tensor.pow(Tensor([44, 128, 96, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([44, 128, 96, 96],"float32"), 2, ) 	 51904512 	 26857 	 10.284647941589355 	 8.16380262374878 	 0.3955094814300537 	 0.3107175827026367 	 12.340112686157227 	 28.867727756500244 	 0.4693186283111572 	 0.36621618270874023 	 
2025-08-06 00:49:29.843503 test begin: paddle.Tensor.prod(Tensor([1, 386, 65856, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([1, 386, 65856, 2],"float32"), -1, ) 	 50840832 	 25773 	 10.0158531665802 	 12.06668472290039 	 0.39731788635253906 	 0.4786078929901123 	 43.22306561470032 	 53.00614142417908 	 1.7135193347930908 	 0.0007014274597167969 	 
2025-08-06 00:51:31.215730 test begin: paddle.Tensor.prod(Tensor([1, 400, 63505, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([1, 400, 63505, 2],"float32"), -1, ) 	 50804000 	 25773 	 10.040503025054932 	 12.067223310470581 	 0.39819765090942383 	 0.47812557220458984 	 43.23482012748718 	 52.96808481216431 	 1.715153694152832 	 0.0007023811340332031 	 
2025-08-06 00:53:31.560275 test begin: paddle.Tensor.prod(Tensor([1, 400, 65856, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([1, 400, 65856, 2],"float32"), -1, ) 	 52684800 	 25773 	 10.37269139289856 	 12.497124910354614 	 0.4112083911895752 	 0.49559855461120605 	 44.80208206176758 	 54.893335819244385 	 1.7771263122558594 	 0.0007281303405761719 	 
2025-08-06 00:55:38.037244 test begin: paddle.Tensor.prod(Tensor([2100, 12096, 3],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([2100, 12096, 3],"float32"), -1, ) 	 76204800 	 25773 	 10.672296285629272 	 13.237282276153564 	 0.4232974052429199 	 0.5238592624664307 	 48.74153518676758 	 70.10276484489441 	 1.9342007637023926 	 0.0009877681732177734 	 
2025-08-06 00:58:03.728400 test begin: paddle.Tensor.prod(Tensor([2100, 12097, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([2100, 12097, 2],"float32"), -1, ) 	 50807400 	 25773 	 10.008921146392822 	 12.059354066848755 	 0.3969705104827881 	 0.47813963890075684 	 32.67337250709534 	 52.95394277572632 	 1.2959012985229492 	 0.0006954669952392578 	 
2025-08-06 00:59:53.505669 test begin: paddle.Tensor.prod(Tensor([2101, 12096, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([2101, 12096, 2],"float32"), -1, ) 	 50827392 	 25773 	 10.014400720596313 	 12.087716102600098 	 0.3971683979034424 	 0.47830986976623535 	 32.67822599411011 	 52.94517946243286 	 1.2956116199493408 	 0.0007011890411376953 	 
2025-08-06 01:01:43.856797 test begin: paddle.Tensor.prod(Tensor([4, 525, 12096, 3],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([4, 525, 12096, 3],"float32"), -1, ) 	 76204800 	 25773 	 10.666444540023804 	 13.202876329421997 	 0.422961950302124 	 0.5238487720489502 	 64.61782932281494 	 70.0787832736969 	 2.5622718334198 	 0.001001119613647461 	 
2025-08-06 01:04:24.480787 test begin: paddle.Tensor.prod(Tensor([4, 525, 12097, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([4, 525, 12097, 2],"float32"), -1, ) 	 50807400 	 25773 	 10.008971929550171 	 12.052425861358643 	 0.39693212509155273 	 0.4779090881347656 	 43.2479088306427 	 52.94318342208862 	 1.7152934074401855 	 0.0007042884826660156 	 
2025-08-06 01:06:24.096203 test begin: paddle.Tensor.prod(Tensor([4, 526, 12096, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([4, 526, 12096, 2],"float32"), -1, ) 	 50899968 	 25773 	 10.026888132095337 	 12.07234263420105 	 0.3976283073425293 	 0.47876453399658203 	 43.32952165603638 	 53.05868697166443 	 1.7179312705993652 	 0.0006995201110839844 	 
2025-08-06 01:08:23.874043 test begin: paddle.Tensor.prod(Tensor([5, 525, 12096, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([5, 525, 12096, 2],"float32"), -1, ) 	 63504000 	 25773 	 12.471848011016846 	 15.037570476531982 	 0.4946098327636719 	 0.5962021350860596 	 53.97126340866089 	 65.67121410369873 	 2.139073371887207 	 0.0008914470672607422 	 
2025-08-06 01:10:52.609959 test begin: paddle.Tensor.quantile(Tensor([3, 405, 3, 4, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 405, 3, 4, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, ) 	 145800 	 17789 	 291.11525416374207 	 3.859544038772583 	 0.42563343048095703 	 7.963180541992188e-05 	 3.4981260299682617 	 4.206146478652954 	 4.506111145019531e-05 	 7.557868957519531e-05 	 
2025-08-06 01:15:55.434085 test begin: paddle.Tensor.quantile(Tensor([3, 405, 3, 4, 2, 5],"float64"), q=0.75, axis=5, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 405, 3, 4, 2, 5],"float64"), q=0.75, axis=5, ) 	 145800 	 17789 	 233.27946281433105 	 3.615016460418701 	 0.3685297966003418 	 0.00023889541625976562 	 3.2028746604919434 	 3.7804951667785645 	 6.365776062011719e-05 	 7.128715515136719e-05 	 
2025-08-06 01:19:59.343847 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 288],"float64"), q=0.75, axis=3, keepdim=True, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 288],"float64"), q=0.75, axis=3, keepdim=True, ) 	 124416 	 17789 	 248.7723822593689 	 3.7824525833129883 	 0.36356496810913086 	 0.0001850128173828125 	 3.4485926628112793 	 4.125297784805298 	 3.409385681152344e-05 	 7.2479248046875e-05 	 
2025-08-06 01:24:19.500129 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 288],"float64"), q=0.75, axis=5, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 288],"float64"), q=0.75, axis=5, ) 	 124416 	 17789 	 8.664470672607422 	 3.477583408355713 	 6.747245788574219e-05 	 0.00025153160095214844 	 3.1484851837158203 	 3.7824761867523193 	 5.030632019042969e-05 	 7.009506225585938e-05 	 
2025-08-06 01:24:38.596454 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 235, 5],"float64"), q=0.75, axis=3, keepdim=True, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 235, 5],"float64"), q=0.75, axis=3, keepdim=True, ) 	 253800 	 17789 	 504.40057730674744 	 3.850558280944824 	 0.7371208667755127 	 0.00022554397583007812 	 3.456937074661255 	 4.146937847137451 	 5.364418029785156e-05 	 7.343292236328125e-05 	 
2025-08-06 01:33:15.895187 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 235, 5],"float64"), q=0.75, axis=5, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 235, 5],"float64"), q=0.75, axis=5, ) 	 253800 	 17789 	 404.1492760181427 	 3.6433863639831543 	 0.6383206844329834 	 0.00010967254638671875 	 3.192251682281494 	 3.9386789798736572 	 5.602836608886719e-05 	 7.462501525878906e-05 	 
2025-08-06 01:40:10.859511 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 470, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 3, 470, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, ) 	 253800 	 17789 	 10.276704549789429 	 3.726628065109253 	 9.942054748535156e-05 	 8.893013000488281e-05 	 3.8740861415863037 	 4.170988321304321 	 7.104873657226562e-05 	 7.62939453125e-05 	 
2025-08-06 01:40:32.921929 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 470, 2, 5],"float64"), q=0.75, axis=5, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 3, 470, 2, 5],"float64"), q=0.75, axis=5, ) 	 253800 	 17789 	 404.1914849281311 	 3.6632936000823975 	 0.6383917331695557 	 0.0002384185791015625 	 3.249626398086548 	 3.7612509727478027 	 6.437301635742188e-05 	 7.271766662597656e-05 	 
2025-08-06 01:47:27.824737 test begin: paddle.Tensor.quantile(Tensor([3, 6, 352, 4, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 352, 4, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, ) 	 253440 	 17789 	 504.0097713470459 	 3.803148031234741 	 0.7365894317626953 	 0.00021719932556152344 	 3.3866212368011475 	 4.455708742141724 	 5.173683166503906e-05 	 7.557868957519531e-05 	 
2025-08-06 01:56:03.527222 test begin: paddle.Tensor.quantile(Tensor([3, 6, 352, 4, 2, 5],"float64"), q=0.75, axis=5, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 352, 4, 2, 5],"float64"), q=0.75, axis=5, ) 	 253440 	 17789 	 403.6516635417938 	 3.5776517391204834 	 0.6374979019165039 	 7.867813110351562e-05 	 3.219691038131714 	 3.789933204650879 	 3.933906555175781e-05 	 7.128715515136719e-05 	 
2025-08-06 02:02:57.803444 test begin: paddle.Tensor.quantile(Tensor([352, 6, 3, 4, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([352, 6, 3, 4, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, ) 	 253440 	 17789 	 504.0293319225311 	 3.7649729251861572 	 0.7365925312042236 	 0.00019049644470214844 	 3.444359302520752 	 4.171803712844849 	 4.887580871582031e-05 	 7.367134094238281e-05 	 
2025-08-06 02:11:33.256464 test begin: paddle.Tensor.quantile(Tensor([352, 6, 3, 4, 2, 5],"float64"), q=0.75, axis=5, )
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([352, 6, 3, 4, 2, 5],"float64"), q=0.75, axis=5, ) 	 253440 	 17789 	 403.6261308193207 	 3.589751720428467 	 0.6375396251678467 	 0.00010991096496582031 	 3.1295552253723145 	 3.759326219558716 	 3.981590270996094e-05 	 7.152557373046875e-05 	 
2025-08-06 02:18:29.584888 test begin: paddle.Tensor.rad2deg(x=Tensor([1587601, 4, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([1587601, 4, 4],"float64"), ) 	 25401616 	 33553 	 10.001126289367676 	 10.014031410217285 	 0.30464863777160645 	 0.3048243522644043 	 10.003378629684448 	 10.005613565444946 	 0.3047056198120117 	 0.30481934547424316 	 
2025-08-06 02:19:10.757337 test begin: paddle.Tensor.rad2deg(x=Tensor([396901, 4, 4, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 33553 	 9.997289419174194 	 10.005087614059448 	 0.30449843406677246 	 0.30480170249938965 	 10.002778053283691 	 10.005213499069214 	 0.3046996593475342 	 0.3047046661376953 	 
2025-08-06 02:19:51.958832 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 1587601, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 1587601, 4],"float64"), ) 	 25401616 	 33553 	 10.001386880874634 	 10.004463195800781 	 0.30466747283935547 	 0.3047456741333008 	 10.003732919692993 	 10.005122900009155 	 0.3047170639038086 	 0.3046834468841553 	 
2025-08-06 02:20:33.126170 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 33553 	 9.998057842254639 	 10.680741548538208 	 0.3045520782470703 	 0.3047475814819336 	 10.003648281097412 	 10.004526138305664 	 0.3046879768371582 	 0.3047192096710205 	 
2025-08-06 02:21:16.230107 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 4, 1587601],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 4, 1587601],"float64"), ) 	 25401616 	 33553 	 10.001717805862427 	 10.24031662940979 	 0.30466556549072266 	 0.30473971366882324 	 10.004082918167114 	 10.005293369293213 	 0.30469512939453125 	 0.304715633392334 	 
2025-08-06 02:21:59.719127 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 33553 	 9.998029470443726 	 10.004964351654053 	 0.30455636978149414 	 0.3047974109649658 	 10.003313541412354 	 10.004548788070679 	 0.3047792911529541 	 0.30382585525512695 	 
2025-08-06 02:22:41.517080 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 33553 	 9.998051881790161 	 10.004823684692383 	 0.30455470085144043 	 0.30473756790161133 	 10.003414154052734 	 10.005020141601562 	 0.3047065734863281 	 0.30475687980651855 	 
2025-08-06 02:23:22.764534 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 6350401],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 6350401],"float64"), ) 	 25401604 	 33553 	 10.001888751983643 	 10.012325286865234 	 0.3046555519104004 	 0.30475950241088867 	 10.004060983657837 	 10.004754066467285 	 0.3047308921813965 	 0.3047642707824707 	 
2025-08-06 02:24:04.074184 test begin: paddle.Tensor.rad2deg(x=Tensor([6350401, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([6350401, 4],"float64"), ) 	 25401604 	 33553 	 10.001832962036133 	 10.005027055740356 	 0.3046863079071045 	 0.30469274520874023 	 10.004178762435913 	 10.004926681518555 	 0.30471134185791016 	 0.30469226837158203 	 
2025-08-06 02:24:45.255315 test begin: paddle.Tensor.rank(Tensor([2560, 1536, 3, 44],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 1536, 3, 44],"float32"), ) 	 519045120 	 242345 	 9.555835008621216 	 7.639551639556885 	 5.936622619628906e-05 	 0.00023746490478515625 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:25:17.323246 test begin: paddle.Tensor.rank(Tensor([2560, 1536, 44, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 1536, 44, 3],"float32"), ) 	 519045120 	 242345 	 10.942069053649902 	 6.979942321777344 	 6.699562072753906e-05 	 0.0002200603485107422 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:25:51.034922 test begin: paddle.Tensor.rank(Tensor([2560, 2048, 3, 33],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 2048, 3, 33],"float32"), ) 	 519045120 	 242345 	 10.092621564865112 	 7.777521848678589 	 4.506111145019531e-05 	 0.00022268295288085938 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:26:24.315739 test begin: paddle.Tensor.rank(Tensor([2560, 2048, 33, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 2048, 33, 3],"float32"), ) 	 519045120 	 242345 	 10.388467073440552 	 6.990176677703857 	 6.437301635742188e-05 	 0.0001590251922607422 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:26:58.129038 test begin: paddle.Tensor.rank(Tensor([2560, 22051, 3, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 22051, 3, 3],"float32"), ) 	 508055040 	 242345 	 9.841443538665771 	 7.045943737030029 	 9.250640869140625e-05 	 0.00022792816162109375 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:27:29.874413 test begin: paddle.Tensor.rank(Tensor([2560, 768, 3, 87],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 768, 3, 87],"float32"), ) 	 513146880 	 242345 	 9.866786003112793 	 6.984597444534302 	 6.127357482910156e-05 	 0.00022482872009277344 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:28:03.339325 test begin: paddle.Tensor.rank(Tensor([2560, 768, 87, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2560, 768, 87, 3],"float32"), ) 	 513146880 	 242345 	 9.732333421707153 	 6.989309310913086 	 6.008148193359375e-05 	 0.00022459030151367188 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:28:34.853018 test begin: paddle.Tensor.rank(Tensor([27570, 2048, 3, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([27570, 2048, 3, 3],"float32"), ) 	 508170240 	 242345 	 9.762039422988892 	 6.9684083461761475 	 6.341934204101562e-05 	 0.000213623046875 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:29:07.456452 test begin: paddle.Tensor.rank(Tensor([36760, 1536, 3, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([36760, 1536, 3, 3],"float32"), ) 	 508170240 	 242345 	 9.74351716041565 	 6.97143030166626 	 6.29425048828125e-05 	 0.00026988983154296875 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:29:38.789613 test begin: paddle.Tensor.rank(Tensor([73510, 768, 3, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([73510, 768, 3, 3],"float32"), ) 	 508101120 	 242345 	 11.011935472488403 	 8.994397640228271 	 7.653236389160156e-05 	 0.0006735324859619141 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 02:30:16.460005 test begin: paddle.Tensor.reciprocal(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33834 	 9.999859094619751 	 10.09260869026184 	 0.3020615577697754 	 0.30453920364379883 	 15.232484340667725 	 35.206305503845215 	 0.46015357971191406 	 0.35451388359069824 	 
2025-08-06 02:31:29.573058 test begin: paddle.Tensor.reciprocal(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33834 	 9.997763395309448 	 10.082269430160522 	 0.3020012378692627 	 0.304567813873291 	 15.224247694015503 	 35.204482555389404 	 0.4598677158355713 	 0.35447096824645996 	 
2025-08-06 02:32:43.886561 test begin: paddle.Tensor.reciprocal(Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([10, 5080321],"float32"), ) 	 50803210 	 33834 	 9.997929573059082 	 10.08246660232544 	 0.3020210266113281 	 0.30456995964050293 	 15.222541332244873 	 35.20789098739624 	 0.4597644805908203 	 0.35445666313171387 	 
2025-08-06 02:33:56.229163 test begin: paddle.Tensor.reciprocal(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33834 	 9.997766256332397 	 10.082350015640259 	 0.3020510673522949 	 0.3045189380645752 	 15.222191572189331 	 35.203797578811646 	 0.45981478691101074 	 0.3544797897338867 	 
2025-08-06 02:35:08.487295 test begin: paddle.Tensor.reciprocal(Tensor([2540161, 20],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([2540161, 20],"float32"), ) 	 50803220 	 33834 	 9.997581005096436 	 10.796708106994629 	 0.3019900321960449 	 0.3045651912689209 	 15.22261929512024 	 35.2044243812561 	 0.4597764015197754 	 0.3545095920562744 	 
2025-08-06 02:36:23.592711 test begin: paddle.Tensor.reciprocal(Tensor([4233601, 12],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([4233601, 12],"float32"), ) 	 50803212 	 33834 	 9.997933864593506 	 10.082318782806396 	 0.30196714401245117 	 0.3046107292175293 	 15.223068237304688 	 35.2039098739624 	 0.4598383903503418 	 0.3544800281524658 	 
2025-08-06 02:37:39.410781 test begin: paddle.Tensor.remainder(Tensor([2, 3, 8467201],"float32"), Tensor([2, 3, 8467201],"float32"), )
[Prof] paddle.Tensor.remainder 	 paddle.Tensor.remainder(Tensor([2, 3, 8467201],"float32"), Tensor([2, 3, 8467201],"float32"), ) 	 101606412 	 22180 	 9.998077392578125 	 9.974239110946655 	 0.46082329750061035 	 0.45903682708740234 	 None 	 None 	 None 	 None 	 
2025-08-06 02:38:01.187856 test begin: paddle.Tensor.remainder(Tensor([2, 6350401, 4],"float32"), Tensor([2, 6350401, 4],"float32"), )
[Prof] paddle.Tensor.remainder 	 paddle.Tensor.remainder(Tensor([2, 6350401, 4],"float32"), Tensor([2, 6350401, 4],"float32"), ) 	 101606416 	 22180 	 9.996681928634644 	 9.961586952209473 	 0.46062612533569336 	 0.4590640068054199 	 None 	 None 	 None 	 None 	 
2025-08-06 02:38:22.749576 test begin: paddle.Tensor.remainder(Tensor([4233601, 3, 4],"float32"), Tensor([4233601, 3, 4],"float32"), )
[Prof] paddle.Tensor.remainder 	 paddle.Tensor.remainder(Tensor([4233601, 3, 4],"float32"), Tensor([4233601, 3, 4],"float32"), ) 	 101606424 	 22180 	 9.996307373046875 	 9.964812517166138 	 0.4607083797454834 	 0.45897889137268066 	 None 	 None 	 None 	 None 	 
2025-08-06 02:38:45.677511 test begin: paddle.Tensor.repeat_interleave(Tensor([1, 1, 198451, 128],"float64"), 3, axis=1, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(Tensor([1, 1, 198451, 128],"float64"), 3, axis=1, ) 	 25401728 	 16163 	 14.498300313949585 	 14.36267876625061 	 0.45837974548339844 	 0.9082341194152832 	 24.005457878112793 	 9.500019788742065 	 0.5062916278839111 	 0.600618839263916 	 
2025-08-06 02:39:50.264339 test begin: paddle.Tensor.repeat_interleave(Tensor([1, 1, 64, 396901],"float64"), 3, axis=1, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(Tensor([1, 1, 64, 396901],"float64"), 3, axis=1, ) 	 25401664 	 16163 	 14.518905401229858 	 14.263777017593384 	 0.45903635025024414 	 0.9019842147827148 	 24.00543451309204 	 9.498742818832397 	 0.5063130855560303 	 0.600621223449707 	 
2025-08-06 02:40:57.067321 test begin: paddle.Tensor.repeat_interleave(Tensor([1, 3101, 64, 128],"float64"), 3, axis=1, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(Tensor([1, 3101, 64, 128],"float64"), 3, axis=1, ) 	 25403392 	 16163 	 11.246159076690674 	 10.305848121643066 	 0.07375526428222656 	 0.6517620086669922 	 14.833468914031982 	 9.606981754302979 	 0.09438824653625488 	 0.6074554920196533 	 
2025-08-06 02:41:45.347280 test begin: paddle.Tensor.repeat_interleave(Tensor([3101, 1, 64, 128],"float64"), 3, axis=1, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(Tensor([3101, 1, 64, 128],"float64"), 3, axis=1, ) 	 25403392 	 16163 	 10.004240989685059 	 10.303991794586182 	 0.3162651062011719 	 0.6516859531402588 	 14.82195496559143 	 9.607189416885376 	 0.31249380111694336 	 0.6074395179748535 	 
2025-08-06 02:42:32.303980 test begin: paddle.Tensor.repeat_interleave(x=Tensor([158761, 2, 4, 4, 5],"float64"), repeats=2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6d3dcc6590>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 02:52:37.976271 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 158761, 4, 5],"float64"), repeats=2, )
W0806 02:52:43.090487 120083 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f763eb52d10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:02:44.006736 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 158761, 5],"float64"), repeats=2, )
W0806 03:02:44.660580 120875 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa248ea2e90>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754421164 (unix time) try "date -d @1754421164" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d7d9) received by PID 120793 (TID 0x7fa2403f9640) from PID 120793 ***]

2025-08-06 03:12:55.519843 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 198451],"float64"), repeats=2, )
W0806 03:12:56.449818 121449 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7fbdd26e90>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 03:23:00.130685 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 79381, 4, 4, 5],"float64"), repeats=2, )
W0806 03:23:00.794310 121907 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5c4df0af50>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754422380 (unix time) try "date -d @1754422380" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1dbe2) received by PID 121826 (TID 0x7f5c44dfa640) from PID 121826 ***]

2025-08-06 03:33:07.001366 test begin: paddle.Tensor.reshape(Tensor([124040, 8192],"bfloat16"), list[-1,8192,], )
W0806 03:33:22.313937 122467 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.reshape 	 paddle.Tensor.reshape(Tensor([124040, 8192],"bfloat16"), list[-1,8192,], ) 	 1016135680 	 66716 	 0.3475642204284668 	 0.535825252532959 	 3.218650817871094e-05 	 0.0003132820129394531 	 3.0252833366394043 	 299.96698236465454 	 7.677078247070312e-05 	 2.2991511821746826 	 
2025-08-06 03:38:46.172188 test begin: paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 19459 	 11.133302211761475 	 5.899992227554321 	 0.5847570896148682 	 0.3098630905151367 	 17.161128520965576 	 5.907910108566284 	 0.4506502151489258 	 0.3102695941925049 	 
2025-08-06 03:39:27.453284 test begin: paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 19459 	 17.09029483795166 	 5.899988651275635 	 0.44871091842651367 	 0.3098945617675781 	 11.163928508758545 	 5.900895833969116 	 0.5863277912139893 	 0.30994367599487305 	 
2025-08-06 03:40:08.658270 test begin: paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 19459 	 17.008889198303223 	 5.902973651885986 	 0.44666409492492676 	 0.30988645553588867 	 11.163306474685669 	 5.893707752227783 	 0.5863046646118164 	 0.3095536231994629 	 
2025-08-06 03:40:50.008075 test begin: paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 19459 	 11.268640756607056 	 5.916531324386597 	 0.5918443202972412 	 0.31073498725891113 	 17.27236032485962 	 5.884439706802368 	 0.45358800888061523 	 0.3090181350708008 	 
2025-08-06 03:41:31.512112 test begin: paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 19459 	 19.750727891921997 	 6.443026304244995 	 0.5187711715698242 	 0.31075525283813477 	 11.330300331115723 	 5.917248010635376 	 0.5949807167053223 	 0.310748815536499 	 
2025-08-06 03:42:17.453279 test begin: paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 19459 	 17.0171115398407 	 5.900055646896362 	 0.4469425678253174 	 0.3098738193511963 	 11.172568798065186 	 5.893595933914185 	 0.5867555141448975 	 0.30951976776123047 	 
2025-08-06 03:42:58.566677 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 19459 	 11.303258419036865 	 5.917105674743652 	 0.5936498641967773 	 0.3107471466064453 	 17.29551649093628 	 5.907802581787109 	 0.4541447162628174 	 0.3102881908416748 	 
2025-08-06 03:43:43.131179 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 19459 	 17.328938484191895 	 5.9184205532073975 	 0.45502305030822754 	 0.3107180595397949 	 11.171793699264526 	 5.900751829147339 	 0.5867118835449219 	 0.30986762046813965 	 
2025-08-06 03:44:25.164894 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 19459 	 17.168121099472046 	 5.917621612548828 	 0.45081377029418945 	 0.3107929229736328 	 11.310444116592407 	 5.945355653762817 	 0.5940115451812744 	 0.31226253509521484 	 
2025-08-06 03:45:06.619754 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 19459 	 11.304489612579346 	 5.9261252880096436 	 0.5937368869781494 	 0.31078600883483887 	 17.29668402671814 	 5.907867670059204 	 0.45422983169555664 	 0.31024956703186035 	 
2025-08-06 03:45:49.564538 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 19459 	 17.354498386383057 	 5.91713285446167 	 0.45577406883239746 	 0.3107597827911377 	 11.314206600189209 	 5.954004287719727 	 0.5939805507659912 	 0.3127586841583252 	 
2025-08-06 03:46:31.212077 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 19459 	 16.9361572265625 	 5.95731782913208 	 0.4447503089904785 	 0.3129119873046875 	 11.167081832885742 	 5.89361572265625 	 0.5865063667297363 	 0.3095111846923828 	 
2025-08-06 03:47:12.283932 test begin: paddle.Tensor.round(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.Tensor.round 	 paddle.Tensor.round(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33823 	 10.001510381698608 	 10.067945718765259 	 0.3022630214691162 	 0.3042113780975342 	 4.528508186340332 	 4.540248870849609 	 0.13676738739013672 	 0.1369626522064209 	 
2025-08-06 03:47:43.182095 test begin: paddle.Tensor.round(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.Tensor.round 	 paddle.Tensor.round(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33823 	 9.999555110931396 	 10.067937135696411 	 0.302135705947876 	 0.3042716979980469 	 4.537261962890625 	 4.535324335098267 	 0.13709092140197754 	 0.13701128959655762 	 
2025-08-06 03:48:14.045735 test begin: paddle.Tensor.round(Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.round 	 paddle.Tensor.round(Tensor([10, 5080321],"float32"), ) 	 50803210 	 33823 	 9.999167442321777 	 10.068063020706177 	 0.3021829128265381 	 0.3042130470275879 	 4.531853914260864 	 4.54014253616333 	 0.13695907592773438 	 0.1369917392730713 	 
2025-08-06 03:48:44.881149 test begin: paddle.Tensor.round(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.Tensor.round 	 paddle.Tensor.round(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33823 	 10.000850439071655 	 10.595613956451416 	 0.30212998390197754 	 0.3042008876800537 	 4.535648822784424 	 4.536057710647583 	 0.13690662384033203 	 0.13698363304138184 	 
2025-08-06 03:49:18.529882 test begin: paddle.Tensor.round(Tensor([2540161, 20],"float32"), )
[Prof] paddle.Tensor.round 	 paddle.Tensor.round(Tensor([2540161, 20],"float32"), ) 	 50803220 	 33823 	 9.99939775466919 	 10.070986032485962 	 0.30213308334350586 	 0.30420732498168945 	 4.532022714614868 	 4.535860538482666 	 0.13689184188842773 	 0.13694000244140625 	 
2025-08-06 03:49:49.360746 test begin: paddle.Tensor.rsqrt(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.Tensor.rsqrt 	 paddle.Tensor.rsqrt(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 33809 	 9.999107837677002 	 10.064722299575806 	 0.3022158145904541 	 0.3042120933532715 	 15.217741966247559 	 35.17227578163147 	 0.45999598503112793 	 0.35445117950439453 	 
2025-08-06 03:51:01.645832 test begin: paddle.Tensor.rsqrt(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.Tensor.rsqrt 	 paddle.Tensor.rsqrt(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 33809 	 9.997411251068115 	 10.064538717269897 	 0.3022127151489258 	 0.30422306060791016 	 15.209102153778076 	 35.172972679138184 	 0.4597353935241699 	 0.3544306755065918 	 
2025-08-06 03:52:13.976419 test begin: paddle.Tensor.rsqrt(Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.rsqrt 	 paddle.Tensor.rsqrt(Tensor([10, 5080321],"float32"), ) 	 50803210 	 33809 	 9.997220277786255 	 10.092352867126465 	 0.302213191986084 	 0.3043055534362793 	 15.20940899848938 	 35.173134565353394 	 0.45973753929138184 	 0.35446834564208984 	 
2025-08-06 03:53:28.520471 test begin: paddle.Tensor.rsqrt(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.Tensor.rsqrt 	 paddle.Tensor.rsqrt(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 33809 	 9.997347831726074 	 10.064635992050171 	 0.3022470474243164 	 0.3042314052581787 	 15.210994720458984 	 35.17438769340515 	 0.45976710319519043 	 0.3544497489929199 	 
2025-08-06 03:54:42.583956 test begin: paddle.Tensor.rsqrt(Tensor([2540161, 20],"float32"), )
[Prof] paddle.Tensor.rsqrt 	 paddle.Tensor.rsqrt(Tensor([2540161, 20],"float32"), ) 	 50803220 	 33809 	 9.997202157974243 	 10.06460976600647 	 0.3022439479827881 	 0.30423808097839355 	 15.20987606048584 	 35.17252802848816 	 0.45972776412963867 	 0.35447216033935547 	 
2025-08-06 03:55:54.808834 test begin: paddle.Tensor.scale(Tensor([100352, 1013],"bfloat16"), 0.006378560586546936, )
[Prof] paddle.Tensor.scale 	 paddle.Tensor.scale(Tensor([100352, 1013],"bfloat16"), 0.006378560586546936, ) 	 101656576 	 33533 	 10.00271987915039 	 19.864415168762207 	 0.3048441410064697 	 0.30265378952026367 	 19.719709157943726 	 25.13796591758728 	 0.600966215133667 	 0.38306736946105957 	 combined
2025-08-06 03:57:13.746798 test begin: paddle.Tensor.scale(Tensor([1013, 100352],"bfloat16"), 0.006378560586546936, )
[Prof] paddle.Tensor.scale 	 paddle.Tensor.scale(Tensor([1013, 100352],"bfloat16"), 0.006378560586546936, ) 	 101656576 	 33533 	 10.002548217773438 	 19.869464635849 	 0.30481815338134766 	 0.30267786979675293 	 19.720367431640625 	 25.13758158683777 	 0.6010782718658447 	 0.3830585479736328 	 combined
2025-08-06 03:58:31.870330 test begin: paddle.Tensor.scale(Tensor([12404, 8192],"bfloat16"), 0.006378560586546936, )
[Prof] paddle.Tensor.scale 	 paddle.Tensor.scale(Tensor([12404, 8192],"bfloat16"), 0.006378560586546936, ) 	 101613568 	 33533 	 9.998032093048096 	 19.85678768157959 	 0.30472469329833984 	 0.3025805950164795 	 19.72731590270996 	 25.127434968948364 	 0.6011807918548584 	 0.3829500675201416 	 combined
2025-08-06 03:59:49.943001 test begin: paddle.Tensor.scale(Tensor([1772, 57344],"bfloat16"), 0.006378560586546936, )
[Prof] paddle.Tensor.scale 	 paddle.Tensor.scale(Tensor([1772, 57344],"bfloat16"), 0.006378560586546936, ) 	 101613568 	 33533 	 9.99814748764038 	 19.857100248336792 	 0.30472278594970703 	 0.3025805950164795 	 19.72615671157837 	 25.126583337783813 	 0.6011991500854492 	 0.3829011917114258 	 combined
2025-08-06 04:01:09.335239 test begin: paddle.Tensor.scale(Tensor([8192, 12404],"bfloat16"), 0.006378560586546936, )
[Prof] paddle.Tensor.scale 	 paddle.Tensor.scale(Tensor([8192, 12404],"bfloat16"), 0.006378560586546936, ) 	 101613568 	 33533 	 9.998197793960571 	 19.86572813987732 	 0.3047497272491455 	 0.30255579948425293 	 19.725944995880127 	 25.127083778381348 	 0.6012377738952637 	 0.3829360008239746 	 combined
2025-08-06 04:02:27.683082 test begin: paddle.Tensor.set_(Tensor([2001],"bool"), Tensor([15, 3386881],"bool"), list[20,], list[2,], 0, )
[Prof] paddle.Tensor.set_ 	 paddle.Tensor.set_(Tensor([2001],"bool"), Tensor([15, 3386881],"bool"), list[20,], list[2,], 0, ) 	 50805216 	 272766 	 12.582786321640015 	 0.630856990814209 	 7.510185241699219e-05 	 0.000110626220703125 	 None 	 None 	 None 	 None 	 
2025-08-06 04:02:51.477475 test begin: paddle.Tensor.set_(Tensor([2001],"bool"), Tensor([16934401, 3],"bool"), list[20,], list[2,], 0, )
[Prof] paddle.Tensor.set_ 	 paddle.Tensor.set_(Tensor([2001],"bool"), Tensor([16934401, 3],"bool"), list[20,], list[2,], 0, ) 	 50805204 	 272766 	 12.5001802444458 	 0.629239559173584 	 9.608268737792969e-05 	 5.841255187988281e-05 	 None 	 None 	 None 	 None 	 
2025-08-06 04:03:14.042504 test begin: paddle.Tensor.set_(Tensor([50803201],"bool"), Tensor([1501, 3],"bool"), list[20,], list[2,], 0, )
[Prof] paddle.Tensor.set_ 	 paddle.Tensor.set_(Tensor([50803201],"bool"), Tensor([1501, 3],"bool"), list[20,], list[2,], 0, ) 	 50807704 	 272766 	 12.5115065574646 	 1.017665147781372 	 0.00010251998901367188 	 0.00018644332885742188 	 None 	 None 	 None 	 None 	 
2025-08-06 04:03:42.360353 test begin: paddle.Tensor.sigmoid(Tensor([1, 1100, 46185],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1, 1100, 46185],"float32"), ) 	 50803500 	 33917 	 9.997430562973022 	 10.113287210464478 	 0.3012251853942871 	 0.3047337532043457 	 15.25924825668335 	 15.145462274551392 	 0.45973920822143555 	 0.45636487007141113 	 
2025-08-06 04:04:34.657764 test begin: paddle.Tensor.sigmoid(Tensor([1, 12700801, 4],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1, 12700801, 4],"float32"), ) 	 50803204 	 33917 	 9.999233722686768 	 10.112957954406738 	 0.30127882957458496 	 0.3047168254852295 	 15.256792068481445 	 15.144109964370728 	 0.45969486236572266 	 0.4563319683074951 	 
2025-08-06 04:05:26.989643 test begin: paddle.Tensor.sigmoid(Tensor([1, 6380, 7963],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1, 6380, 7963],"float32"), ) 	 50803940 	 33917 	 10.000815629959106 	 10.11725401878357 	 0.30134153366088867 	 0.3046755790710449 	 15.266642570495605 	 15.143807649612427 	 0.4601414203643799 	 0.4562866687774658 	 
2025-08-06 04:06:19.280174 test begin: paddle.Tensor.sigmoid(Tensor([1, 8550, 5942],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1, 8550, 5942],"float32"), ) 	 50804100 	 33917 	 9.998915195465088 	 10.113278150558472 	 0.30126237869262695 	 0.3047938346862793 	 15.257218599319458 	 15.143803834915161 	 0.45975518226623535 	 0.45635342597961426 	 
2025-08-06 04:07:11.573446 test begin: paddle.Tensor.sigmoid(Tensor([11547, 1100, 4],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([11547, 1100, 4],"float32"), ) 	 50806800 	 33917 	 10.003451585769653 	 10.12626314163208 	 0.30142951011657715 	 0.3047339916229248 	 15.254684448242188 	 15.14439344406128 	 0.4596738815307617 	 0.45630764961242676 	 
2025-08-06 04:08:04.609652 test begin: paddle.Tensor.sigmoid(Tensor([1486, 8550, 4],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1486, 8550, 4],"float32"), ) 	 50821200 	 33917 	 9.999964237213135 	 10.116445302963257 	 0.30136585235595703 	 0.30484604835510254 	 15.266135454177856 	 15.147998571395874 	 0.46002936363220215 	 0.45647168159484863 	 
2025-08-06 04:08:59.660733 test begin: paddle.Tensor.sigmoid(Tensor([1991, 6380, 4],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1991, 6380, 4],"float32"), ) 	 50810320 	 33917 	 10.003370523452759 	 10.11424732208252 	 0.3014237880706787 	 0.3047945499420166 	 15.26892876625061 	 15.14549469947815 	 0.46007680892944336 	 0.4562997817993164 	 
2025-08-06 04:09:51.932396 test begin: paddle.Tensor.sign(Tensor([1016065, 5, 5],"float64"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([1016065, 5, 5],"float64"), ) 	 25401625 	 32403 	 9.999887943267822 	 9.676170587539673 	 0.3153369426727295 	 0.3047635555267334 	 9.651931762695312 	 4.357216835021973 	 0.30438947677612305 	 0.13738298416137695 	 
2025-08-06 04:10:28.399418 test begin: paddle.Tensor.sign(Tensor([1124, 45199],"float32"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([1124, 45199],"float32"), ) 	 50803676 	 32403 	 11.166244745254517 	 9.643623113632202 	 0.3522191047668457 	 0.3041951656341553 	 9.582198858261108 	 4.345066070556641 	 0.30226755142211914 	 0.13696503639221191 	 
2025-08-06 04:11:04.860160 test begin: paddle.Tensor.sign(Tensor([12700801, 2],"float64"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([12700801, 2],"float64"), ) 	 25401602 	 32403 	 9.999655723571777 	 9.667381763458252 	 0.31536293029785156 	 0.30477476119995117 	 9.6517653465271 	 4.3616907596588135 	 0.3044307231903076 	 0.13742876052856445 	 
2025-08-06 04:11:42.506367 test begin: paddle.Tensor.sign(Tensor([1587601, 32],"float32"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([1587601, 32],"float32"), ) 	 50803232 	 32403 	 11.16781210899353 	 9.643171310424805 	 0.352339506149292 	 0.3041665554046631 	 9.58320665359497 	 4.344287872314453 	 0.3022482395172119 	 0.136976957321167 	 
2025-08-06 04:12:18.973104 test begin: paddle.Tensor.sign(Tensor([50000, 102, 5],"float64"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([50000, 102, 5],"float64"), ) 	 25500000 	 32403 	 10.050123929977417 	 9.707989931106567 	 0.31702709197998047 	 0.30603456497192383 	 9.698289155960083 	 4.373578071594238 	 0.3059272766113281 	 0.13786005973815918 	 
2025-08-06 04:12:54.012927 test begin: paddle.Tensor.sign(Tensor([50000, 5, 102],"float64"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([50000, 5, 102],"float64"), ) 	 25500000 	 32403 	 10.049912929534912 	 9.702940464019775 	 0.3170177936553955 	 0.3060283660888672 	 9.698291063308716 	 4.372786998748779 	 0.3058760166168213 	 0.1378934383392334 	 
2025-08-06 04:13:29.000790 test begin: paddle.Tensor.sign(Tensor([50000, 509],"float64"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([50000, 509],"float64"), ) 	 25450000 	 32403 	 10.017898797988892 	 9.692826271057129 	 0.31607723236083984 	 0.3054478168487549 	 9.67615008354187 	 4.365780353546143 	 0.3052053451538086 	 0.13781976699829102 	 
2025-08-06 04:14:04.082783 test begin: paddle.Tensor.signbit(Tensor([12, 10584, 2],"float64"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([12, 10584, 2],"float64"), ) 	 254016 	 1000 	 10.107264995574951 	 0.010282278060913086 	 4.839897155761719e-05 	 2.4318695068359375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:14:14.300180 test begin: paddle.Tensor.signbit(Tensor([12, 20, 1058],"float64"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([12, 20, 1058],"float64"), ) 	 253920 	 1000 	 10.103583812713623 	 0.01033329963684082 	 4.673004150390625e-05 	 2.5272369384765625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:14:24.472033 test begin: paddle.Tensor.signbit(Tensor([12, 20, 2116],"float32"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([12, 20, 2116],"float32"), ) 	 507840 	 1000 	 19.766697645187378 	 0.010257244110107422 	 4.863739013671875e-05 	 2.9325485229492188e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:14:44.312343 test begin: paddle.Tensor.signbit(Tensor([12, 20, 4233],"int16"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([12, 20, 4233],"int16"), ) 	 1015920 	 1000 	 39.22181487083435 	 0.01067042350769043 	 4.57763671875e-05 	 4.1484832763671875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:15:23.635255 test begin: paddle.Tensor.signbit(Tensor([12, 21168, 2],"float32"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([12, 21168, 2],"float32"), ) 	 508032 	 1000 	 19.841837644577026 	 0.010234594345092773 	 4.458427429199219e-05 	 2.7894973754882812e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:15:43.549074 test begin: paddle.Tensor.signbit(Tensor([12, 42336, 2],"int16"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([12, 42336, 2],"int16"), ) 	 1016064 	 1000 	 39.31894278526306 	 0.010712623596191406 	 5.245208740234375e-05 	 3.9577484130859375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:16:22.966438 test begin: paddle.Tensor.signbit(Tensor([12700, 20, 2],"float32"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([12700, 20, 2],"float32"), ) 	 508000 	 1000 	 19.75744915008545 	 0.0101470947265625 	 5.1021575927734375e-05 	 2.7179718017578125e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:16:42.811562 test begin: paddle.Tensor.signbit(Tensor([25401, 20, 2],"int16"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([25401, 20, 2],"int16"), ) 	 1016040 	 1000 	 39.15756106376648 	 0.012977361679077148 	 4.839897155761719e-05 	 4.076957702636719e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:17:22.074800 test begin: paddle.Tensor.signbit(Tensor([6350, 20, 2],"float64"), )
[Prof] paddle.Tensor.signbit 	 paddle.Tensor.signbit(Tensor([6350, 20, 2],"float64"), ) 	 254000 	 1000 	 10.077489137649536 	 0.010317325592041016 	 4.506111145019531e-05 	 2.9802322387695312e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 04:17:32.210219 test begin: paddle.Tensor.sin(Tensor([131072, 388],"float32"), )
[Prof] paddle.Tensor.sin 	 paddle.Tensor.sin(Tensor([131072, 388],"float32"), ) 	 50855936 	 33845 	 10.00671648979187 	 10.093286514282227 	 0.3021659851074219 	 0.3047611713409424 	 15.252853393554688 	 25.189000129699707 	 0.46056461334228516 	 0.38025736808776855 	 
2025-08-06 04:18:34.481087 test begin: paddle.Tensor.sin(Tensor([3175201, 16],"float32"), )
[Prof] paddle.Tensor.sin 	 paddle.Tensor.sin(Tensor([3175201, 16],"float32"), ) 	 50803216 	 33845 	 10.001517295837402 	 10.083042621612549 	 0.3020308017730713 	 0.3044586181640625 	 15.238162517547607 	 25.162514448165894 	 0.46010756492614746 	 0.37987732887268066 	 
2025-08-06 04:19:38.390499 test begin: paddle.Tensor.sin(Tensor([32768, 1551],"float32"), )
[Prof] paddle.Tensor.sin 	 paddle.Tensor.sin(Tensor([32768, 1551],"float32"), ) 	 50823168 	 33845 	 10.001190662384033 	 10.08667516708374 	 0.3019881248474121 	 0.30457329750061035 	 15.243834018707275 	 25.17233443260193 	 0.460263729095459 	 0.3800222873687744 	 
2025-08-06 04:20:42.258135 test begin: paddle.Tensor.sin(Tensor([396901, 128],"float32"), )
[Prof] paddle.Tensor.sin 	 paddle.Tensor.sin(Tensor([396901, 128],"float32"), ) 	 50803328 	 33845 	 9.996838331222534 	 10.08450984954834 	 0.3018677234649658 	 0.3044562339782715 	 15.232922315597534 	 25.16367483139038 	 0.4600098133087158 	 0.3799154758453369 	 
2025-08-06 04:21:47.483067 test begin: paddle.Tensor.slice(Tensor([127008010, 4],"float32"), list[1,], list[0,], list[1,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f84ede229b0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 04:31:54.297332 test begin: paddle.Tensor.slice(Tensor([40, 12700801],"float32"), list[1,], list[0,], list[1,], )
W0806 04:32:02.105402 124547 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.slice 	 paddle.Tensor.slice(Tensor([40, 12700801],"float32"), list[1,], list[0,], list[1,], ) 	 508032040 	 198496 	 2.492096424102783 	 2.8034238815307617 	 7.939338684082031e-05 	 0.00011992454528808594 	 298.0071256160736 	 261.44687390327454 	 0.7664833068847656 	 0.6728794574737549 	 combined
2025-08-06 04:41:28.229316 test begin: paddle.Tensor.slice_scatter(Tensor([4233601, 6],"float64"), Tensor([4233601, 3],"float64"), list[1,], list[0,], list[6,], list[2,], )
[Prof] paddle.Tensor.slice_scatter 	 paddle.Tensor.slice_scatter(Tensor([4233601, 6],"float64"), Tensor([4233601, 3],"float64"), list[1,], list[0,], list[6,], list[2,], ) 	 38102409 	 32593 	 12.268336772918701 	 22.413362979888916 	 0.38474225997924805 	 0.23416566848754883 	 34.62185287475586 	 24.931880950927734 	 0.18087029457092285 	 0.19542813301086426 	 
2025-08-06 04:43:04.381220 test begin: paddle.Tensor.slice_scatter(Tensor([80, 3175201],"float64"), Tensor([80, 3],"float64"), list[1,], list[0,], list[6,], list[2,], )
[Prof] paddle.Tensor.slice_scatter 	 paddle.Tensor.slice_scatter(Tensor([80, 3175201],"float64"), Tensor([80, 3],"float64"), list[1,], list[0,], list[6,], list[2,], ) 	 254016320 	 32593 	 0.4743916988372803 	 99.98473763465881 	 3.0517578125e-05 	 1.0430092811584473 	 100.05790948867798 	 100.06203889846802 	 0.5218920707702637 	 0.7828555107116699 	 
2025-08-06 04:48:15.942325 test begin: paddle.Tensor.slice_scatter(Tensor([8467201, 6],"float64"), Tensor([8467201, 3],"float64"), list[1,], list[0,], list[6,], list[2,], )
[Prof] paddle.Tensor.slice_scatter 	 paddle.Tensor.slice_scatter(Tensor([8467201, 6],"float64"), Tensor([8467201, 3],"float64"), list[1,], list[0,], list[6,], list[2,], ) 	 76204809 	 32593 	 24.312992811203003 	 44.403188943862915 	 0.7623682022094727 	 0.46396493911743164 	 68.44806241989136 	 49.04483199119568 	 0.3575315475463867 	 0.384354829788208 	 
2025-08-06 04:51:25.568392 test begin: paddle.Tensor.sqrt(Tensor([276, 80, 48, 48],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([276, 80, 48, 48],"float32"), ) 	 50872320 	 33926 	 10.008593082427979 	 10.144406080245972 	 0.3015122413635254 	 0.30556488037109375 	 15.299967050552368 	 25.37476134300232 	 0.4608621597290039 	 0.38222384452819824 	 
2025-08-06 04:52:28.192870 test begin: paddle.Tensor.sqrt(Tensor([329, 80, 44, 44],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([329, 80, 44, 44],"float32"), ) 	 50955520 	 33926 	 10.025318384170532 	 10.160958290100098 	 0.3020443916320801 	 0.3060493469238281 	 15.324111700057983 	 25.414560317993164 	 0.46165895462036133 	 0.38275671005249023 	 
2025-08-06 04:53:31.606910 test begin: paddle.Tensor.sqrt(Tensor([397, 80, 40, 40],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([397, 80, 40, 40],"float32"), ) 	 50816000 	 33926 	 9.995978832244873 	 10.133943557739258 	 0.30117273330688477 	 0.3052830696105957 	 15.283408164978027 	 25.34854221343994 	 0.4604179859161377 	 0.3818349838256836 	 
2025-08-06 04:54:34.289018 test begin: paddle.Tensor.sqrt(Tensor([64, 345, 48, 48],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 345, 48, 48],"float32"), ) 	 50872320 	 33926 	 10.008779764175415 	 10.144261837005615 	 0.30150866508483887 	 0.30564093589782715 	 15.29941177368164 	 25.374738931655884 	 0.4608449935913086 	 0.38220953941345215 	 
2025-08-06 04:55:37.178686 test begin: paddle.Tensor.sqrt(Tensor([64, 411, 44, 44],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 411, 44, 44],"float32"), ) 	 50924544 	 33926 	 10.022905349731445 	 10.15356159210205 	 0.30191850662231445 	 0.3058159351348877 	 15.315775394439697 	 25.400010585784912 	 0.46134257316589355 	 0.3825206756591797 	 
2025-08-06 04:56:40.342109 test begin: paddle.Tensor.sqrt(Tensor([64, 497, 40, 40],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 497, 40, 40],"float32"), ) 	 50892800 	 33926 	 10.013150453567505 	 10.14777946472168 	 0.30164003372192383 	 0.3056671619415283 	 15.30670690536499 	 25.38461947441101 	 0.46112561225891113 	 0.38234972953796387 	 
2025-08-06 04:57:42.945531 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 207, 48],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 207, 48],"float32"), ) 	 50872320 	 33926 	 10.008846044540405 	 10.144423484802246 	 0.3015103340148926 	 0.3055856227874756 	 15.299636840820312 	 25.37420153617859 	 0.4609246253967285 	 0.38219237327575684 	 
2025-08-06 04:58:45.520506 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 226, 44],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 226, 44],"float32"), ) 	 50913280 	 33926 	 10.016916990280151 	 10.151317834854126 	 0.30176877975463867 	 0.3058016300201416 	 15.313668727874756 	 25.393765926361084 	 0.46131110191345215 	 0.38247108459472656 	 
2025-08-06 04:59:48.131174 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 249, 40],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 249, 40],"float32"), ) 	 50995200 	 33926 	 10.022804260253906 	 10.16927719116211 	 0.3019258975982666 	 0.306257963180542 	 15.33790922164917 	 25.434139013290405 	 0.4620094299316406 	 0.38309431076049805 	 
2025-08-06 05:00:50.815958 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 40, 249],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 40, 249],"float32"), ) 	 50995200 	 33926 	 10.022756814956665 	 10.179942846298218 	 0.30193042755126953 	 0.30628252029418945 	 15.33871340751648 	 25.434653759002686 	 0.4621586799621582 	 0.38312721252441406 	 
2025-08-06 05:01:54.756755 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 44, 226],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 44, 226],"float32"), ) 	 50913280 	 33926 	 10.016788482666016 	 10.152637958526611 	 0.30176663398742676 	 0.3057687282562256 	 15.31239366531372 	 25.39427638053894 	 0.4612600803375244 	 0.3825352191925049 	 
2025-08-06 05:02:57.513347 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 48, 207],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 48, 207],"float32"), ) 	 50872320 	 33926 	 10.008874416351318 	 10.14482593536377 	 0.3015167713165283 	 0.30562520027160645 	 15.29900312423706 	 25.374088764190674 	 0.4609200954437256 	 0.38219451904296875 	 
2025-08-06 05:04:00.059344 test begin: paddle.Tensor.square(Tensor([2, 25401601],"float32"), )
[Prof] paddle.Tensor.square 	 paddle.Tensor.square(Tensor([2, 25401601],"float32"), ) 	 50803202 	 33808 	 9.989448547363281 	 10.0596923828125 	 0.30199313163757324 	 0.30413103103637695 	 15.203508853912354 	 35.671222448349 	 0.45963549613952637 	 0.2697725296020508 	 
2025-08-06 05:05:15.293546 test begin: paddle.Tensor.square(Tensor([396901, 128],"float32"), )
[Prof] paddle.Tensor.square 	 paddle.Tensor.square(Tensor([396901, 128],"float32"), ) 	 50803328 	 33808 	 9.995076417922974 	 10.059802532196045 	 0.3021657466888428 	 0.30419373512268066 	 15.205752611160278 	 35.672051191329956 	 0.4596517086029053 	 0.2697627544403076 	 
2025-08-06 05:06:27.960739 test begin: paddle.Tensor.square(Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.square 	 paddle.Tensor.square(Tensor([50803201],"float32"), ) 	 50803201 	 33808 	 9.989596605300903 	 10.3037748336792 	 0.3019983768463135 	 0.3040752410888672 	 15.203922748565674 	 35.67113447189331 	 0.4595968723297119 	 0.2697467803955078 	 
2025-08-06 05:07:42.563439 test begin: paddle.Tensor.square(Tensor([8, 6350401],"float32"), )
[Prof] paddle.Tensor.square 	 paddle.Tensor.square(Tensor([8, 6350401],"float32"), ) 	 50803208 	 33808 	 9.989284038543701 	 10.059430122375488 	 0.30196619033813477 	 0.3040454387664795 	 15.203954935073853 	 35.67258143424988 	 0.45961904525756836 	 0.26978087425231934 	 
2025-08-06 05:08:55.188120 test begin: paddle.Tensor.squeeze(Tensor([10, 2, 3840, 10240],"float32"), 0, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([10, 2, 3840, 10240],"float32"), 0, ) 	 786432000 	 2295230 	 10.419241428375244 	 8.98656439781189 	 4.696846008300781e-05 	 0.0002903938293457031 	 96.0049397945404 	 114.41274547576904 	 0.0001049041748046875 	 0.00022029876708984375 	 
2025-08-06 05:13:12.692721 test begin: paddle.Tensor.squeeze(Tensor([10, 3, 1654, 10240],"float32"), 0, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([10, 3, 1654, 10240],"float32"), 0, ) 	 508108800 	 2295230 	 9.982481241226196 	 9.112807989120483 	 9.179115295410156e-05 	 0.00016450881958007812 	 95.68795967102051 	 114.865079164505 	 0.00010633468627929688 	 0.00020551681518554688 	 
2025-08-06 05:17:19.569617 test begin: paddle.Tensor.squeeze(Tensor([10, 3, 3840, 10240],"float32"), 0, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([10, 3, 3840, 10240],"float32"), 0, ) 	 1179648000 	 2295230 	 10.397551774978638 	 9.040216207504272 	 0.00013494491577148438 	 0.0002391338348388672 	 95.86703133583069 	 114.2392942905426 	 0.00010061264038085938 	 0.00022530555725097656 	 
2025-08-06 05:21:49.639239 test begin: paddle.Tensor.squeeze(Tensor([10, 3, 3840, 4411],"float32"), 0, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([10, 3, 3840, 4411],"float32"), 0, ) 	 508147200 	 2295230 	 10.018365859985352 	 10.158671855926514 	 0.00010347366333007812 	 0.00014495849609375 	 95.84339427947998 	 114.95006012916565 	 0.00011515617370605469 	 0.00023794174194335938 	 
2025-08-06 05:25:57.793658 test begin: paddle.Tensor.squeeze(Tensor([160, 1, 125, 25500],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([160, 1, 125, 25500],"float32"), 1, ) 	 510000000 	 2295230 	 10.234570503234863 	 9.231139659881592 	 0.00012636184692382812 	 0.00028705596923828125 	 97.30807423591614 	 121.91403579711914 	 0.00010633468627929688 	 0.00022220611572265625 	 
2025-08-06 05:30:13.602320 test begin: paddle.Tensor.squeeze(Tensor([160, 1, 80, 39691],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([160, 1, 80, 39691],"float32"), 1, ) 	 508044800 	 2295230 	 10.208682537078857 	 9.504334688186646 	 7.462501525878906e-05 	 0.0002129077911376953 	 96.51479816436768 	 122.67431402206421 	 0.00011658668518066406 	 0.00021958351135253906 	 
2025-08-06 05:34:31.859034 test begin: paddle.Tensor.squeeze(Tensor([160, 2, 80, 25500],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([160, 2, 80, 25500],"float32"), 1, ) 	 652800000 	 2295230 	 10.09535002708435 	 8.930038928985596 	 5.602836608886719e-05 	 0.00013208389282226562 	 95.36861729621887 	 114.22499489784241 	 9.965896606445312e-05 	 0.0002346038818359375 	 
2025-08-06 05:38:43.512685 test begin: paddle.Tensor.squeeze(Tensor([2000, 1, 127009, 2],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([2000, 1, 127009, 2],"float32"), 1, ) 	 508036000 	 2295230 	 10.169719457626343 	 9.110297441482544 	 0.00010275840759277344 	 8.893013000488281e-05 	 96.87270259857178 	 121.42639970779419 	 9.72747802734375e-05 	 0.0002315044403076172 	 
2025-08-06 05:42:58.277249 test begin: paddle.Tensor.squeeze(Tensor([2000, 1, 37632, 7],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([2000, 1, 37632, 7],"float32"), 1, ) 	 526848000 	 2295230 	 10.190547227859497 	 9.046138286590576 	 8.511543273925781e-05 	 0.0001289844512939453 	 95.97407579421997 	 122.01142024993896 	 9.846687316894531e-05 	 0.00021767616271972656 	 
2025-08-06 05:47:12.878246 test begin: paddle.Tensor.squeeze(Tensor([2000, 4, 37632, 2],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([2000, 4, 37632, 2],"float32"), 1, ) 	 602112000 	 2295230 	 10.036096811294556 	 8.876088380813599 	 5.698204040527344e-05 	 0.0001308917999267578 	 106.76014113426208 	 114.28126096725464 	 0.00010156631469726562 	 0.00021696090698242188 	 
2025-08-06 05:51:34.034061 test begin: paddle.Tensor.squeeze(Tensor([250, 1, 80, 25500],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([250, 1, 80, 25500],"float32"), 1, ) 	 510000000 	 2295230 	 10.268239736557007 	 9.075513124465942 	 0.00010418891906738281 	 0.0001842975616455078 	 96.16338205337524 	 120.24648451805115 	 9.274482727050781e-05 	 0.00021910667419433594 	 
2025-08-06 05:55:48.340343 test begin: paddle.Tensor.squeeze(Tensor([6760, 1, 37632, 2],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([6760, 1, 37632, 2],"float32"), 1, ) 	 508784640 	 2295230 	 10.228679418563843 	 8.998061895370483 	 6.508827209472656e-05 	 0.00018262863159179688 	 96.06609797477722 	 119.77098226547241 	 9.5367431640625e-05 	 0.00022554397583007812 	 
2025-08-06 06:00:00.057272 test begin: paddle.Tensor.std(Tensor([1024, 1024, 25],"float64"), )
W0806 06:00:00.557454 126709 dygraph_functions.cc:88394] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([1024, 1024, 25],"float64"), ) 	 26214400 	 9109 	 12.182493209838867 	 1.678987741470337 	 4.887580871582031e-05 	 0.09421372413635254 	 13.939180850982666 	 7.2062788009643555 	 0.19573545455932617 	 0.09014606475830078 	 
2025-08-06 06:00:38.560986 test begin: paddle.Tensor.std(Tensor([1024, 1024, 49],"float32"), )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([1024, 1024, 49],"float32"), ) 	 51380224 	 9109 	 10.0177903175354 	 1.5317068099975586 	 4.267692565917969e-05 	 0.08591723442077637 	 12.325595378875732 	 7.136948585510254 	 0.17314386367797852 	 0.08928227424621582 	 
2025-08-06 06:01:13.218694 test begin: paddle.Tensor.std(Tensor([1024, 3101, 8],"float64"), )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([1024, 3101, 8],"float64"), ) 	 25403392 	 9109 	 12.147829055786133 	 1.6299259662628174 	 4.9591064453125e-05 	 0.09141278266906738 	 13.510475158691406 	 6.999409198760986 	 0.18977713584899902 	 0.08750700950622559 	 
2025-08-06 06:01:50.014623 test begin: paddle.Tensor.std(Tensor([1024, 6202, 8],"float32"), )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([1024, 6202, 8],"float32"), ) 	 50806784 	 9109 	 10.259153127670288 	 1.5162572860717773 	 5.1975250244140625e-05 	 0.08508181571960449 	 12.197489500045776 	 7.0593554973602295 	 0.17135405540466309 	 0.08831000328063965 	 
2025-08-06 06:02:21.913654 test begin: paddle.Tensor.std(Tensor([1444, 35183],"float32"), axis=1, )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([1444, 35183],"float32"), axis=1, ) 	 50804252 	 9109 	 10.576896667480469 	 1.6044166088104248 	 4.935264587402344e-05 	 0.18000483512878418 	 12.397396802902222 	 7.115381240844727 	 0.1741955280303955 	 0.09993720054626465 	 
2025-08-06 06:02:54.491655 test begin: paddle.Tensor.std(Tensor([3101, 1024, 8],"float64"), )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([3101, 1024, 8],"float64"), ) 	 25403392 	 9109 	 12.166621208190918 	 1.6331117153167725 	 5.53131103515625e-05 	 0.09160256385803223 	 13.521716833114624 	 6.997703313827515 	 0.18996262550354004 	 0.0875251293182373 	 
2025-08-06 06:03:29.391867 test begin: paddle.Tensor.std(Tensor([49613, 1024],"float32"), axis=1, )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([49613, 1024],"float32"), axis=1, ) 	 50803712 	 9109 	 10.1885666847229 	 1.5271728038787842 	 4.673004150390625e-05 	 0.17133641242980957 	 12.278493404388428 	 7.1060192584991455 	 0.19680380821228027 	 0.09983205795288086 	 
2025-08-06 06:04:01.390138 test begin: paddle.Tensor.std(Tensor([6202, 1024, 8],"float32"), )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([6202, 1024, 8],"float32"), ) 	 50806784 	 9109 	 10.206387281417847 	 1.515998363494873 	 3.528594970703125e-05 	 0.08507490158081055 	 12.197197198867798 	 7.05896520614624 	 0.17138361930847168 	 0.08829855918884277 	 
2025-08-06 06:04:33.240761 test begin: paddle.Tensor.subtract(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.subtract 	 paddle.Tensor.subtract(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 22223 	 9.994489908218384 	 9.919382810592651 	 0.45960330963134766 	 0.4562058448791504 	 10.547807931900024 	 6.612566232681274 	 0.48504042625427246 	 0.30406832695007324 	 
2025-08-06 06:05:12.983597 test begin: paddle.Tensor.sum(Tensor([106496, 478],"float32"), axis=-1, )
[Prof] paddle.Tensor.sum 	 paddle.Tensor.sum(Tensor([106496, 478],"float32"), axis=-1, ) 	 50905088 	 68402 	 10.261206150054932 	 10.604909658432007 	 0.15337824821472168 	 0.15848994255065918 	 9.448272466659546 	 3.713334560394287 	 0.14113783836364746 	 9.441375732421875e-05 	 
2025-08-06 06:05:47.899490 test begin: paddle.Tensor.sum(Tensor([108544, 469],"float32"), axis=-1, )
[Prof] paddle.Tensor.sum 	 paddle.Tensor.sum(Tensor([108544, 469],"float32"), axis=-1, ) 	 50907136 	 68402 	 10.285362720489502 	 10.597851276397705 	 0.15589570999145508 	 0.15827178955078125 	 9.42517614364624 	 3.614962100982666 	 0.1408219337463379 	 0.00019669532775878906 	 
2025-08-06 06:06:22.684291 test begin: paddle.Tensor.sum(Tensor([111616, 456],"float32"), axis=-1, )
[Prof] paddle.Tensor.sum 	 paddle.Tensor.sum(Tensor([111616, 456],"float32"), axis=-1, ) 	 50896896 	 68402 	 10.398125410079956 	 10.443272113800049 	 0.1554889678955078 	 0.1559748649597168 	 9.456145763397217 	 3.7623324394226074 	 0.1412341594696045 	 9.250640869140625e-05 	 
2025-08-06 06:06:57.922706 test begin: paddle.Tensor.sum(Tensor([14176, 3584],"float32"), axis=-1, )
[Prof] paddle.Tensor.sum 	 paddle.Tensor.sum(Tensor([14176, 3584],"float32"), axis=-1, ) 	 50806784 	 68402 	 9.996165037155151 	 10.687798738479614 	 0.14935040473937988 	 0.15967655181884766 	 9.362444162368774 	 3.596918821334839 	 0.1398487091064453 	 9.179115295410156e-05 	 
2025-08-06 06:07:32.496228 test begin: paddle.Tensor.take_along_axis(Tensor([128, 1000],"float32"), indices=Tensor([128, 396901],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([128, 1000],"float32"), indices=Tensor([128, 396901],"int32"), axis=-1, ) 	 50931328 	 33018 	 25.604219675064087 	 15.597129344940186 	 0.2641584873199463 	 0.48275303840637207 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 06:12:07.974546 test begin: paddle.Tensor.take_along_axis(Tensor([128, 396901],"float32"), indices=Tensor([128, 1],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([128, 396901],"float32"), indices=Tensor([128, 1],"int32"), axis=-1, ) 	 50803456 	 33018 	 9.986365556716919 	 0.5797474384307861 	 0.10313034057617188 	 8.058547973632812e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 06:12:34.567854 test begin: paddle.Tensor.take_along_axis(Tensor([128, 396901],"float32"), indices=Tensor([128, 396901],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([128, 396901],"float32"), indices=Tensor([128, 396901],"int32"), axis=-1, ) 	 101606656 	 33018 	 46.32167720794678 	 24.341657876968384 	 0.4781057834625244 	 0.7535710334777832 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 06:14:43.291905 test begin: paddle.Tensor.take_along_axis(Tensor([50804, 1000],"float32"), indices=Tensor([50804, 1],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([50804, 1000],"float32"), indices=Tensor([50804, 1],"int32"), axis=-1, ) 	 50854804 	 33018 	 10.165021657943726 	 0.8502037525177002 	 0.10492873191833496 	 8.58306884765625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 06:15:10.724034 test begin: paddle.Tensor.take_along_axis(Tensor([80, 1000],"float32"), indices=Tensor([80, 635041],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([80, 1000],"float32"), indices=Tensor([80, 635041],"int32"), axis=-1, ) 	 50883280 	 33018 	 25.20926523208618 	 15.54248309135437 	 0.26004600524902344 	 0.4807920455932617 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 06:19:58.975231 test begin: paddle.Tensor.take_along_axis(Tensor([80, 635041],"float32"), indices=Tensor([80, 1],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([80, 635041],"float32"), indices=Tensor([80, 1],"int32"), axis=-1, ) 	 50803360 	 33018 	 9.984979629516602 	 0.5906124114990234 	 0.10313558578491211 	 6.270408630371094e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 06:20:26.874071 test begin: paddle.Tensor.take_along_axis(Tensor([80, 635041],"float32"), indices=Tensor([80, 635041],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([80, 635041],"float32"), indices=Tensor([80, 635041],"int32"), axis=-1, ) 	 101606560 	 33018 	 46.40129804611206 	 24.599231719970703 	 0.4795668125152588 	 0.7613623142242432 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 06:22:39.122893 test begin: paddle.Tensor.tanh(Tensor([1, 16934401, 3],"float32"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([1, 16934401, 3],"float32"), ) 	 50803203 	 33854 	 9.994444370269775 	 10.082695484161377 	 0.3017585277557373 	 0.3043644428253174 	 15.216691493988037 	 15.110960721969604 	 0.4593026638031006 	 0.4561934471130371 	 
2025-08-06 06:23:31.230021 test begin: paddle.Tensor.tanh(Tensor([1, 2, 12700801],"float64"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([1, 2, 12700801],"float64"), ) 	 25401602 	 33854 	 10.12990951538086 	 10.393873453140259 	 0.3058280944824219 	 0.30670619010925293 	 15.173826932907104 	 15.041016578674316 	 0.45801377296447754 	 0.4540693759918213 	 
2025-08-06 06:24:24.939330 test begin: paddle.Tensor.tanh(Tensor([1, 2, 25401601],"float32"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([1, 2, 25401601],"float32"), ) 	 50803202 	 33854 	 9.994236469268799 	 10.082833528518677 	 0.3017537593841553 	 0.30438232421875 	 15.216548204421997 	 15.111070394515991 	 0.45932912826538086 	 0.4562239646911621 	 
2025-08-06 06:25:17.079902 test begin: paddle.Tensor.tanh(Tensor([1, 8467201, 3],"float64"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([1, 8467201, 3],"float64"), ) 	 25401603 	 33854 	 10.129965543746948 	 10.175371885299683 	 0.30583643913269043 	 0.30672121047973633 	 15.173474073410034 	 15.041343450546265 	 0.45793795585632324 	 0.45409488677978516 	 
2025-08-06 06:26:08.762728 test begin: paddle.Tensor.tanh(Tensor([2, 12700801],"float64"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([2, 12700801],"float64"), ) 	 25401602 	 33854 	 10.129680871963501 	 10.163986444473267 	 0.30581116676330566 	 0.3066871166229248 	 15.173178911209106 	 15.041364431381226 	 0.45823192596435547 	 0.45409679412841797 	 
2025-08-06 06:27:00.411036 test begin: paddle.Tensor.tanh(Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([4233601, 2, 3],"float64"), ) 	 25401606 	 33854 	 10.131349802017212 	 10.183203220367432 	 0.30730462074279785 	 0.30672669410705566 	 15.172229290008545 	 15.041099786758423 	 0.458115816116333 	 0.45398759841918945 	 
2025-08-06 06:27:53.627533 test begin: paddle.Tensor.tanh(Tensor([6350401, 4],"float64"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([6350401, 4],"float64"), ) 	 25401604 	 33854 	 10.129703283309937 	 10.161199569702148 	 0.30579543113708496 	 0.3068108558654785 	 15.174488067626953 	 15.0412437915802 	 0.45810580253601074 	 0.45405149459838867 	 
2025-08-06 06:28:48.004162 test begin: paddle.Tensor.tanh(Tensor([8467201, 2, 3],"float32"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([8467201, 2, 3],"float32"), ) 	 50803206 	 33854 	 9.994210481643677 	 10.083088159561157 	 0.30171942710876465 	 0.30439090728759766 	 15.216700553894043 	 15.111657857894897 	 0.4594156742095947 	 0.4561645984649658 	 
2025-08-06 06:29:40.160110 test begin: paddle.Tensor.tile(Tensor([198451, 1, 256],"float32"), tuple(1,1,1,), )
[Prof] paddle.Tensor.tile 	 paddle.Tensor.tile(Tensor([198451, 1, 256],"float32"), tuple(1,1,1,), ) 	 50803456 	 33795 	 10.003903865814209 	 10.570260286331177 	 0.3025226593017578 	 0.15987682342529297 	 10.628825902938843 	 1.6459453105926514 	 0.1607367992401123 	 7.653236389160156e-05 	 
2025-08-06 06:30:14.741646 test begin: paddle.Tensor.tile(Tensor([36858, 1, 1379],"float32"), tuple(1,1,1,), )
[Prof] paddle.Tensor.tile 	 paddle.Tensor.tile(Tensor([36858, 1, 1379],"float32"), tuple(1,1,1,), ) 	 50827182 	 33795 	 10.009514331817627 	 11.051881313323975 	 0.3027052879333496 	 0.1600961685180664 	 10.69708800315857 	 1.6921675205230713 	 0.16173410415649414 	 0.00010633468627929688 	 
2025-08-06 06:30:51.022746 test begin: paddle.Tensor.tile(Tensor([36858, 6, 256],"float32"), tuple(1,1,1,), )
[Prof] paddle.Tensor.tile 	 paddle.Tensor.tile(Tensor([36858, 6, 256],"float32"), tuple(1,1,1,), ) 	 56613888 	 33795 	 11.127662897109985 	 11.684138059616089 	 0.3365194797515869 	 0.35243892669677734 	 11.670969247817993 	 1.6439344882965088 	 0.35292959213256836 	 9.846687316894531e-05 	 
2025-08-06 06:31:30.909558 test begin: paddle.Tensor.tile(Tensor([38402, 1, 1323],"float32"), tuple(1,1,1,), )
[Prof] paddle.Tensor.tile 	 paddle.Tensor.tile(Tensor([38402, 1, 1323],"float32"), tuple(1,1,1,), ) 	 50805846 	 33795 	 9.994186401367188 	 10.561145782470703 	 0.30223774909973145 	 0.15970230102539062 	 10.391656875610352 	 1.6387841701507568 	 0.15711641311645508 	 7.319450378417969e-05 	 
2025-08-06 06:32:05.232846 test begin: paddle.Tensor.tile(Tensor([38402, 6, 256],"float32"), tuple(1,1,1,), )
[Prof] paddle.Tensor.tile 	 paddle.Tensor.tile(Tensor([38402, 6, 256],"float32"), tuple(1,1,1,), ) 	 58985472 	 33795 	 11.591930150985718 	 12.133756637573242 	 0.35049009323120117 	 0.36693620681762695 	 12.144083261489868 	 1.6491870880126953 	 0.3672299385070801 	 7.581710815429688e-05 	 
2025-08-06 06:32:44.786082 test begin: paddle.Tensor.tolist(Tensor([11, 16, 32, 43],"int64"), )
[Prof] paddle.Tensor.tolist 	 paddle.Tensor.tolist(Tensor([11, 16, 32, 43],"int64"), ) 	 242176 	 1000 	 12.817359924316406 	 17.057552337646484 	 7.796287536621094e-05 	 0.00011706352233886719 	 None 	 None 	 None 	 None 	 
2025-08-06 06:33:14.703800 test begin: paddle.Tensor.tolist(Tensor([11, 25, 21, 43],"int64"), )
[Prof] paddle.Tensor.tolist 	 paddle.Tensor.tolist(Tensor([11, 25, 21, 43],"int64"), ) 	 248325 	 1000 	 12.672268390655518 	 17.112772464752197 	 6.437301635742188e-05 	 0.00011610984802246094 	 None 	 None 	 None 	 None 	 
2025-08-06 06:33:44.529148 test begin: paddle.Tensor.tolist(Tensor([11, 25, 32, 28],"int64"), )
[Prof] paddle.Tensor.tolist 	 paddle.Tensor.tolist(Tensor([11, 25, 32, 28],"int64"), ) 	 246400 	 1000 	 14.101983308792114 	 21.117939710617065 	 7.43865966796875e-05 	 0.00011682510375976562 	 None 	 None 	 None 	 None 	 
2025-08-06 06:34:19.784676 test begin: paddle.Tensor.tolist(Tensor([7, 25, 32, 43],"int64"), )
[Prof] paddle.Tensor.tolist 	 paddle.Tensor.tolist(Tensor([7, 25, 32, 43],"int64"), ) 	 240800 	 1000 	 12.774455785751343 	 16.940986394882202 	 5.14984130859375e-05 	 0.00011754035949707031 	 None 	 None 	 None 	 None 	 
2025-08-06 06:34:50.964116 test begin: paddle.Tensor.topk(Tensor([1, 50803201],"float32"), 5, 1, True, True, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbb5274aa40>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754433891 (unix time) try "date -d @1754433891" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e631) received by PID 124465 (TID 0x7fbb4df3b640) from PID 124465 ***]

2025-08-06 06:45:00.633712 test begin: paddle.Tensor.topk(Tensor([1024, 1034, 48],"float32"), 2, axis=-1, )
W0806 06:45:05.666843 128221 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.topk 	 paddle.Tensor.topk(Tensor([1024, 1034, 48],"float32"), 2, axis=-1, ) 	 50823168 	 26178 	 69.88878750801086 	 291.903906583786 	 2.728520154953003 	 5.6980979442596436 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 06:51:19.911333 test begin: paddle.Tensor.topk(Tensor([1024, 8, 6202],"float32"), 2, axis=-1, )
[Prof] paddle.Tensor.topk 	 paddle.Tensor.topk(Tensor([1024, 8, 6202],"float32"), 2, axis=-1, ) 	 50806784 	 26178 	 9.993162870407104 	 40.31131720542908 	 0.3901336193084717 	 0.08753561973571777 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 06:52:24.132632 test begin: paddle.Tensor.topk(Tensor([128, 396901],"float32"), 5, 1, True, True, )
[Prof] paddle.Tensor.topk 	 paddle.Tensor.topk(Tensor([128, 396901],"float32"), 5, 1, True, True, ) 	 50803328 	 26178 	 39.003323554992676 	 38.85022020339966 	 1.5226712226867676 	 0.08434581756591797 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 06:53:52.336545 test begin: paddle.Tensor.topk(Tensor([132301, 8, 48],"float32"), 2, axis=-1, )
[Prof] paddle.Tensor.topk 	 paddle.Tensor.topk(Tensor([132301, 8, 48],"float32"), 2, axis=-1, ) 	 50803584 	 26178 	 69.73873591423035 	 291.9231188297272 	 2.722660779953003 	 5.696829080581665 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 07:00:09.347091 test begin: paddle.Tensor.topk(Tensor([50804, 1000],"float32"), 5, 1, True, True, )
[Prof] paddle.Tensor.topk 	 paddle.Tensor.topk(Tensor([50804, 1000],"float32"), 5, 1, True, True, ) 	 50804000 	 26178 	 17.058796405792236 	 63.21813368797302 	 0.6661286354064941 	 0.13727807998657227 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 07:01:44.648016 test begin: paddle.Tensor.transpose(Tensor([1064960, 955],"bfloat16"), list[1,0,], )
[Prof] paddle.Tensor.transpose 	 paddle.Tensor.transpose(Tensor([1064960, 955],"bfloat16"), list[1,0,], ) 	 1017036800 	 66690 	 0.21781659126281738 	 0.3023552894592285 	 2.5510787963867188e-05 	 4.267692565917969e-05 	 2.9481396675109863 	 300.09035658836365 	 4.172325134277344e-05 	 2.2995142936706543 	 
2025-08-06 07:07:23.618867 test begin: paddle.Tensor.transpose(Tensor([1085440, 937],"bfloat16"), list[1,0,], )
[Prof] paddle.Tensor.transpose 	 paddle.Tensor.transpose(Tensor([1085440, 937],"bfloat16"), list[1,0,], ) 	 1017057280 	 66690 	 0.22613883018493652 	 0.3194925785064697 	 4.1484832763671875e-05 	 8.58306884765625e-05 	 2.898813486099243 	 300.1012382507324 	 5.078315734863281e-05 	 2.299708843231201 	 
2025-08-06 07:13:01.833152 test begin: paddle.Tensor.transpose(Tensor([1116160, 911],"bfloat16"), list[1,0,], )
[Prof] paddle.Tensor.transpose 	 paddle.Tensor.transpose(Tensor([1116160, 911],"bfloat16"), list[1,0,], ) 	 1016821760 	 66690 	 0.22303318977355957 	 0.312511682510376 	 3.24249267578125e-05 	 8.559226989746094e-05 	 2.9466075897216797 	 300.03877878189087 	 6.461143493652344e-05 	 2.2991201877593994 	 
2025-08-06 07:18:43.103222 test begin: paddle.Tensor.transpose(Tensor([141760, 7168],"bfloat16"), list[1,0,], )
[Prof] paddle.Tensor.transpose 	 paddle.Tensor.transpose(Tensor([141760, 7168],"bfloat16"), list[1,0,], ) 	 1016135680 	 66690 	 0.22248220443725586 	 0.31105995178222656 	 3.743171691894531e-05 	 6.29425048828125e-05 	 2.9766595363616943 	 300.2166187763214 	 4.935264587402344e-05 	 2.6979081630706787 	 
2025-08-06 07:24:25.459651 test begin: paddle.Tensor.tril(Tensor([1, 2, 25401601],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([1, 2, 25401601],"float32"), -1, ) 	 50803202 	 33390 	 9.988202810287476 	 8.900670051574707 	 0.30574631690979004 	 0.27250075340270996 	 9.987967014312744 	 8.89393162727356 	 0.30568456649780273 	 0.27222418785095215 	 
2025-08-06 07:25:04.960980 test begin: paddle.Tensor.tril(Tensor([1, 25401601, 2],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([1, 25401601, 2],"float32"), -1, ) 	 50803202 	 33390 	 14.035019636154175 	 10.74610185623169 	 0.4297354221343994 	 0.3284800052642822 	 13.984400510787964 	 10.743156433105469 	 0.428037166595459 	 0.3289055824279785 	 
2025-08-06 07:25:56.249781 test begin: paddle.Tensor.tril(Tensor([12700801, 2, 2],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([12700801, 2, 2],"float32"), -1, ) 	 50803204 	 33390 	 14.055955648422241 	 10.817511558532715 	 0.43027734756469727 	 0.33115077018737793 	 14.008851289749146 	 10.82484769821167 	 0.4287569522857666 	 0.33130645751953125 	 
2025-08-06 07:26:49.527023 test begin: paddle.Tensor.tril(Tensor([2, 12700801, 2],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([2, 12700801, 2],"float32"), -1, ) 	 50803204 	 33390 	 14.048356533050537 	 10.821836948394775 	 0.4302060604095459 	 0.3311610221862793 	 14.006141424179077 	 10.88840937614441 	 0.4287893772125244 	 0.33672523498535156 	 
2025-08-06 07:27:41.738659 test begin: paddle.Tensor.tril(Tensor([2, 2, 12700801],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([2, 2, 12700801],"float32"), -1, ) 	 50803204 	 33390 	 9.984102249145508 	 8.898769855499268 	 0.3055911064147949 	 0.2723813056945801 	 9.983886480331421 	 8.891887426376343 	 0.3055765628814697 	 0.2721598148345947 	 
2025-08-06 07:28:21.249383 test begin: paddle.Tensor.tril(Tensor([2, 25401601],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([2, 25401601],"float32"), -1, ) 	 50803202 	 33390 	 9.984242916107178 	 6.105563640594482 	 0.3055553436279297 	 0.1865980625152588 	 9.99512529373169 	 6.08895468711853 	 0.3059248924255371 	 0.18636178970336914 	 
2025-08-06 07:28:55.218954 test begin: paddle.Tensor.tril(Tensor([25401601, 2],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([25401601, 2],"float32"), -1, ) 	 50803202 	 33390 	 14.053987741470337 	 10.229669570922852 	 0.4301438331604004 	 0.31312990188598633 	 14.004915714263916 	 10.222975730895996 	 0.4286766052246094 	 0.3128969669342041 	 
2025-08-06 07:29:45.438487 test begin: paddle.Tensor.trunc(Tensor([18144010, 28],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([18144010, 28],"float32"), ) 	 508032280 	 34186 	 117.47708559036255 	 99.98276591300964 	 0.0001380443572998047 	 2.988982677459717 	 84.59389686584473 	 44.84641432762146 	 7.653236389160156e-05 	 1.3398804664611816 	 
2025-08-06 07:35:51.638609 test begin: paddle.Tensor.trunc(Tensor([20, 3175201, 8],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([20, 3175201, 8],"float32"), ) 	 508032160 	 34186 	 117.68513441085815 	 100.04413294792175 	 0.00011205673217773438 	 3.0506248474121094 	 84.62224841117859 	 44.85598707199097 	 7.62939453125e-05 	 1.339766025543213 	 
2025-08-06 07:41:59.271415 test begin: paddle.Tensor.trunc(Tensor([20, 8, 3175201],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([20, 8, 3175201],"float32"), ) 	 508032160 	 34186 	 117.22532773017883 	 101.28686213493347 	 0.00010633468627929688 	 2.9890613555908203 	 84.6214485168457 	 44.855512857437134 	 7.581710815429688e-05 	 1.3398842811584473 	 
2025-08-06 07:48:07.186375 test begin: paddle.Tensor.trunc(Tensor([280, 1814401],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([280, 1814401],"float32"), ) 	 508032280 	 34186 	 117.39115834236145 	 99.98313617706299 	 0.00011563301086425781 	 2.9891107082366943 	 84.58322668075562 	 44.85004639625549 	 8.058547973632812e-05 	 1.3396646976470947 	 
2025-08-06 07:54:13.226709 test begin: paddle.Tensor.trunc(Tensor([63504010, 8],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([63504010, 8],"float32"), ) 	 508032080 	 34186 	 117.3179259300232 	 99.98328065872192 	 0.00011324882507324219 	 2.9890425205230713 	 84.64997458457947 	 44.856239318847656 	 7.700920104980469e-05 	 1.339738130569458 	 
2025-08-06 08:00:19.331096 test begin: paddle.Tensor.trunc(Tensor([7938010, 8, 8],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([7938010, 8, 8],"float32"), ) 	 508032640 	 34186 	 117.16278076171875 	 99.98156547546387 	 0.00011157989501953125 	 2.9889488220214844 	 84.67449903488159 	 44.85290789604187 	 7.343292236328125e-05 	 1.3398070335388184 	 
2025-08-06 08:06:25.414216 test begin: paddle.Tensor.trunc(Tensor([80, 6350401],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([80, 6350401],"float32"), ) 	 508032080 	 34186 	 117.14100360870361 	 99.97984075546265 	 0.00010657310485839844 	 2.988816976547241 	 84.63724493980408 	 44.8400559425354 	 7.224082946777344e-05 	 1.339792013168335 	 
2025-08-06 08:12:31.374753 test begin: paddle.Tensor.unbind(Tensor([30, 115, 2304, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 115, 2304, 64],"float32"), 0, ) 	 508723200 	 85224 	 3.4439666271209717 	 2.867913007736206 	 0.00011086463928222656 	 0.00020074844360351562 	 300.0608112812042 	 253.14758276939392 	 3.5982108116149902 	 3.035778045654297 	 
2025-08-06 08:22:09.947714 test begin: paddle.Tensor.unbind(Tensor([30, 1351, 196, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 1351, 196, 64],"float32"), 0, ) 	 508408320 	 85224 	 3.3577332496643066 	 2.8420586585998535 	 0.00010180473327636719 	 0.0001068115234375 	 300.126345872879 	 253.01010727882385 	 3.599256753921509 	 3.0342049598693848 	 
2025-08-06 08:31:46.300599 test begin: paddle.Tensor.unbind(Tensor([30, 60, 2304, 123],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 60, 2304, 123],"float32"), 0, ) 	 510105600 	 85224 	 3.3842921257019043 	 2.785677671432495 	 0.0001430511474609375 	 0.00013494491577148438 	 300.99891448020935 	 253.86430525779724 	 3.6093716621398926 	 3.044194459915161 	 
2025-08-06 08:41:24.733481 test begin: paddle.Tensor.unbind(Tensor([30, 60, 4411, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 60, 4411, 64],"float32"), 0, ) 	 508147200 	 85224 	 3.3489370346069336 	 2.8297970294952393 	 5.0067901611328125e-05 	 0.00029015541076660156 	 299.9751753807068 	 252.83815217018127 	 3.667330741882324 	 3.032052755355835 	 
2025-08-06 08:51:00.830739 test begin: paddle.Tensor.unbind(Tensor([30, 864, 196, 101],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 864, 196, 101],"float32"), 0, ) 	 513112320 	 85224 	 3.3360087871551514 	 2.7780089378356934 	 0.00011706352233886719 	 9.298324584960938e-05 	 302.6735825538635 	 255.39293456077576 	 3.629756450653076 	 3.0625526905059814 	 
2025-08-06 09:00:44.956082 test begin: paddle.Tensor.unbind(Tensor([30, 864, 307, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 864, 307, 64],"float32"), 0, ) 	 509276160 	 85224 	 3.8378541469573975 	 2.8016297817230225 	 0.0001456737518310547 	 0.00010466575622558594 	 300.92902755737305 	 253.45536279678345 	 3.608454942703247 	 3.039433240890503 	 
2025-08-06 09:10:26.513076 test begin: paddle.Tensor.unbind(Tensor([30, 960, 196, 91],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 960, 196, 91],"float32"), 0, ) 	 513676800 	 85224 	 3.805697202682495 	 2.827225685119629 	 5.364418029785156e-05 	 8.249282836914062e-05 	 303.5652437210083 	 255.72557878494263 	 3.6406466960906982 	 3.0668418407440186 	 
2025-08-06 09:20:09.781888 test begin: paddle.Tensor.unbind(Tensor([30, 960, 276, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([30, 960, 276, 64],"float32"), 0, ) 	 508723200 	 85224 	 3.3448262214660645 	 2.8185360431671143 	 0.00010085105895996094 	 0.00018858909606933594 	 300.02109241485596 	 253.20488667488098 	 3.597839117050171 	 3.0362746715545654 	 
2025-08-06 09:29:46.007982 test begin: paddle.Tensor.unbind(Tensor([50, 864, 196, 64],"float32"), 0, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd0e8f8aad0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:39:53.202800 test begin: paddle.Tensor.unbind(Tensor([50, 960, 196, 64],"float32"), 0, )
W0806 09:40:06.300290 134650 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6e3c91ee00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 09:50:00.836257 test begin: paddle.Tensor.unbind(Tensor([60, 60, 2304, 64],"float32"), 0, )
W0806 09:50:09.927814 135047 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8823eb2e00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 10:00:07.986685 test begin: paddle.Tensor.unique(Tensor([25401601],"int64"), )
W0806 10:00:08.539652 135519 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.unique 	 paddle.Tensor.unique(Tensor([25401601],"int64"), ) 	 25401601 	 1487 	 10.00676703453064 	 4.864617824554443 	 0.00011944770812988281 	 0.00010538101196289062 	 None 	 None 	 None 	 None 	 
2025-08-06 10:00:23.715298 test begin: paddle.Tensor.unsqueeze(Tensor([1720, 544, 544],"float32"), 0, )
Warning: The core code of paddle.Tensor.unsqueeze is too complex.
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([1720, 544, 544],"float32"), 0, ) 	 509009920 	 2409550 	 10.232331037521362 	 9.332915306091309 	 0.00014162063598632812 	 0.0002570152282714844 	 102.20105624198914 	 132.26217555999756 	 0.00011181831359863281 	 0.0002524852752685547 	 
2025-08-06 10:04:55.402572 test begin: paddle.Tensor.unsqueeze(Tensor([1720, 544, 544],"float32"), 1, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([1720, 544, 544],"float32"), 1, ) 	 509009920 	 2409550 	 10.4205482006073 	 9.340120077133179 	 0.00014638900756835938 	 9.369850158691406e-05 	 102.969162940979 	 132.24187302589417 	 0.00010180473327636719 	 0.0002219676971435547 	 
2025-08-06 10:09:27.263374 test begin: paddle.Tensor.unsqueeze(Tensor([20, 3840, 10240],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([20, 3840, 10240],"float32"), 0, ) 	 786432000 	 2409550 	 10.242717027664185 	 9.53424620628357 	 8.940696716308594e-05 	 9.465217590332031e-05 	 102.31546831130981 	 131.05644059181213 	 0.00010848045349121094 	 0.000225067138671875 	 
2025-08-06 10:14:06.933663 test begin: paddle.Tensor.unsqueeze(Tensor([2000, 467, 544],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([2000, 467, 544],"float32"), 0, ) 	 508096000 	 2409550 	 10.347518682479858 	 9.42277216911316 	 0.00014209747314453125 	 0.0002713203430175781 	 102.83586525917053 	 131.35954427719116 	 0.00012564659118652344 	 0.00022363662719726562 	 
2025-08-06 10:18:40.068506 test begin: paddle.Tensor.unsqueeze(Tensor([2000, 467, 544],"float32"), 1, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([2000, 467, 544],"float32"), 1, ) 	 508096000 	 2409550 	 10.566586256027222 	 9.391724348068237 	 8.034706115722656e-05 	 0.00026607513427734375 	 104.01797747612 	 131.4534831047058 	 0.00012230873107910156 	 0.00022840499877929688 	 
2025-08-06 10:23:13.233980 test begin: paddle.Tensor.unsqueeze(Tensor([2000, 544, 467],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([2000, 544, 467],"float32"), 0, ) 	 508096000 	 2409550 	 20.65004587173462 	 9.444212436676025 	 0.0001766681671142578 	 0.00027060508728027344 	 107.13864850997925 	 131.39628100395203 	 0.00012350082397460938 	 0.00023412704467773438 	 
2025-08-06 10:27:59.401620 test begin: paddle.Tensor.unsqueeze(Tensor([2000, 544, 467],"float32"), 1, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([2000, 544, 467],"float32"), 1, ) 	 508096000 	 2409550 	 10.384144306182861 	 9.393580198287964 	 0.00015687942504882812 	 0.0001437664031982422 	 103.9679651260376 	 131.77137327194214 	 0.0001049041748046875 	 0.00019311904907226562 	 
2025-08-06 10:32:31.909427 test begin: paddle.Tensor.unsqueeze(Tensor([30, 1654, 10240],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([30, 1654, 10240],"float32"), 0, ) 	 508108800 	 2409550 	 10.354072570800781 	 9.257417440414429 	 0.00013589859008789062 	 0.00010514259338378906 	 111.83135342597961 	 131.00407934188843 	 0.0001342296600341797 	 0.00022101402282714844 	 
2025-08-06 10:37:11.363112 test begin: paddle.Tensor.unsqueeze(Tensor([30, 3840, 4411],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([30, 3840, 4411],"float32"), 0, ) 	 508147200 	 2409550 	 10.31827998161316 	 9.29465365409851 	 0.0001342296600341797 	 8.940696716308594e-05 	 102.83125567436218 	 131.79391407966614 	 0.00012731552124023438 	 0.0002186298370361328 	 
2025-08-06 10:41:42.967007 test begin: paddle.Tensor.var(Tensor([1000, 50804],"float32"), axis=0, )
W0806 10:41:43.735145 137901 dygraph_functions.cc:88394] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.var 	 paddle.Tensor.var(Tensor([1000, 50804],"float32"), axis=0, ) 	 50804000 	 7832 	 9.935949087142944 	 1.358149766921997 	 4.601478576660156e-05 	 0.17710375785827637 	 11.218685388565063 	 6.004222631454468 	 0.2091364860534668 	 0.19579529762268066 	 
2025-08-06 10:42:12.533900 test begin: paddle.Tensor.var(Tensor([100000, 255],"float64"), axis=0, )
[Prof] paddle.Tensor.var 	 paddle.Tensor.var(Tensor([100000, 255],"float64"), axis=0, ) 	 25500000 	 7832 	 13.64984941482544 	 1.5250661373138428 	 4.00543212890625e-05 	 0.0995328426361084 	 13.379212141036987 	 5.974145174026489 	 0.24939823150634766 	 0.15590167045593262 	 
2025-08-06 10:42:48.645894 test begin: paddle.Tensor.var(Tensor([1000000, 26],"float64"), axis=0, )
[Prof] paddle.Tensor.var 	 paddle.Tensor.var(Tensor([1000000, 26],"float64"), axis=0, ) 	 26000000 	 7832 	 48.29730033874512 	 1.4830701351165771 	 4.982948303222656e-05 	 0.09679269790649414 	 30.77966046333313 	 6.146914482116699 	 0.573775053024292 	 0.16040897369384766 	 
2025-08-06 10:44:15.943673 test begin: paddle.Tensor.var(Tensor([6350401, 4],"float64"), axis=0, )
[Prof] paddle.Tensor.var 	 paddle.Tensor.var(Tensor([6350401, 4],"float64"), axis=0, ) 	 25401604 	 7832 	 90.34045195579529 	 1.9570116996765137 	 4.553794860839844e-05 	 0.12764954566955566 	 51.74956250190735 	 5.989635705947876 	 0.9647715091705322 	 0.15633201599121094 	 
2025-08-06 10:46:51.150240 test begin: paddle.Tensor.var(Tensor([64801, 784],"float32"), axis=0, )
[Prof] paddle.Tensor.var 	 paddle.Tensor.var(Tensor([64801, 784],"float32"), axis=0, ) 	 50803984 	 7832 	 11.016814231872559 	 2.0327188968658447 	 1.9550323486328125e-05 	 0.13264012336730957 	 11.730809926986694 	 6.276288032531738 	 0.21880102157592773 	 0.1637744903564453 	 
2025-08-06 10:47:23.065611 test begin: paddle.Tensor.zero_(Tensor([100352, 507],"float32"), )
[Prof] paddle.Tensor.zero_ 	 paddle.Tensor.zero_(Tensor([100352, 507],"float32"), ) 	 50878464 	 68861 	 9.94338846206665 	 9.25243854522705 	 0.1478126049041748 	 0.13716816902160645 	 None 	 None 	 None 	 None 	 
2025-08-06 10:47:47.178207 test begin: paddle.Tensor.zero_(Tensor([507, 100352],"float32"), )
[Prof] paddle.Tensor.zero_ 	 paddle.Tensor.zero_(Tensor([507, 100352],"float32"), ) 	 50878464 	 68861 	 9.947668313980103 	 9.248039245605469 	 0.14746475219726562 	 0.13721132278442383 	 None 	 None 	 None 	 None 	 
2025-08-06 10:48:10.277818 test begin: paddle.Tensor.zero_(Tensor([6202, 8192],"float32"), )
[Prof] paddle.Tensor.zero_ 	 paddle.Tensor.zero_(Tensor([6202, 8192],"float32"), ) 	 50806784 	 68861 	 10.064631938934326 	 9.233980655670166 	 0.14925050735473633 	 0.1369638442993164 	 None 	 None 	 None 	 None 	 
2025-08-06 10:48:33.524788 test begin: paddle.Tensor.zero_(Tensor([8192, 6202],"float32"), )
[Prof] paddle.Tensor.zero_ 	 paddle.Tensor.zero_(Tensor([8192, 6202],"float32"), ) 	 50806784 	 68861 	 10.067424058914185 	 9.235689640045166 	 0.1494612693786621 	 0.13708806037902832 	 None 	 None 	 None 	 None 	 
2025-08-06 10:48:56.780334 test begin: paddle.Tensor.zero_(Tensor([886, 57344],"float32"), )
[Prof] paddle.Tensor.zero_ 	 paddle.Tensor.zero_(Tensor([886, 57344],"float32"), ) 	 50806784 	 68861 	 10.066943883895874 	 9.234597444534302 	 0.14949512481689453 	 0.1369922161102295 	 None 	 None 	 None 	 None 	 
2025-08-06 10:49:20.012964 test begin: paddle.tensordot(Tensor([406426, 5, 5, 5],"float32"), Tensor([406426, 5, 5, 5],"float32"), list[0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([406426, 5, 5, 5],"float32"), Tensor([406426, 5, 5, 5],"float32"), list[0,], ) 	 101606500 	 20388 	 22.070921659469604 	 17.423629999160767 	 0.37050485610961914 	 0.4367845058441162 	 32.88648295402527 	 32.837531328201294 	 0.8249282836914062 	 0.8206188678741455 	 
2025-08-06 10:51:08.596448 test begin: paddle.tensordot(Tensor([406426, 5, 5, 5],"float32"), Tensor([406426, 5, 5, 5],"float32"), list[3,0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([406426, 5, 5, 5],"float32"), Tensor([406426, 5, 5, 5],"float32"), list[3,0,], ) 	 101606500 	 20388 	 23.65291690826416 	 72.38093900680542 	 0.2961916923522949 	 0.9055783748626709 	 16.380067586898804 	 16.443763256072998 	 0.20542025566101074 	 0.20787715911865234 	 
2025-08-06 10:53:19.197205 test begin: paddle.tensordot(Tensor([5, 406426, 5, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[0,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff1f894cdf0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 11:03:24.231120 test begin: paddle.tensordot(Tensor([5, 406426, 5, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[3,0,], )
W0806 11:03:25.258726 138931 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 406426, 5, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[3,0,], ) 	 50803875 	 20388 	 29.046770572662354 	 14.815641164779663 	 0.3635389804840088 	 0.18559789657592773 	 16.009899377822876 	 16.053121328353882 	 0.20152997970581055 	 0.20123291015625 	 
2025-08-06 11:04:43.924227 test begin: paddle.tensordot(Tensor([5, 5, 406426, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[0,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fce364caaa0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 11:14:49.178004 test begin: paddle.tensordot(Tensor([5, 5, 406426, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[3,0,], )
W0806 11:14:50.204191 139431 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 406426, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[3,0,], ) 	 50803875 	 20388 	 29.036670923233032 	 14.831857204437256 	 0.36362767219543457 	 0.18556714057922363 	 15.9861581325531 	 15.956990480422974 	 0.20028305053710938 	 0.2001667022705078 	 
2025-08-06 11:16:19.074313 test begin: paddle.tensordot(Tensor([5, 5, 5, 406426],"float32"), Tensor([5, 5, 5, 406426],"float32"), list[3,0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 5, 406426],"float32"), Tensor([5, 5, 5, 406426],"float32"), list[3,0,], ) 	 101606500 	 20388 	 42.65121865272522 	 51.273478507995605 	 0.5339415073394775 	 0.6415567398071289 	 16.43124485015869 	 16.221944570541382 	 0.207289457321167 	 0.2049095630645752 	 
2025-08-06 11:18:27.347361 test begin: paddle.tensordot(Tensor([5, 5, 5, 406426],"float32"), Tensor([5, 5, 5, 5],"float32"), list[0,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fde74534bb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 11:28:32.250083 test begin: paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 406426, 5, 5],"float32"), list[0,], )
W0806 11:28:37.332322 139948 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fce011aead0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 11:38:37.902931 test begin: paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 406426, 5, 5],"float32"), list[3,0,], )
W0806 11:38:42.421501 140252 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 406426, 5, 5],"float32"), list[3,0,], ) 	 50803875 	 20388 	 17.723846912384033 	 25.944706916809082 	 0.22316431999206543 	 0.32489562034606934 	 16.40563702583313 	 16.321094751358032 	 0.20664501190185547 	 0.2046184539794922 	 
2025-08-06 11:40:00.455480 test begin: paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 5, 406426, 5],"float32"), list[0,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1bbaac6b60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 11:50:05.395422 test begin: paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 5, 406426, 5],"float32"), list[3,0,], )
W0806 11:50:06.416111 140555 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 5, 406426, 5],"float32"), list[3,0,], ) 	 50803875 	 20388 	 17.701855421066284 	 26.046103715896606 	 0.22211956977844238 	 0.32608771324157715 	 16.41016721725464 	 16.307448387145996 	 0.20626235008239746 	 0.20371031761169434 	 
2025-08-06 11:51:24.590281 test begin: paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 5, 5, 406426],"float32"), list[0,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f94f3732d40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 12:01:29.613774 test begin: paddle.tensordot(x=Tensor([4, 105841, 3, 5, 4],"float64"), y=Tensor([105841, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
W0806 12:01:31.231922 141036 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 105841, 3, 5, 4],"float64"), y=Tensor([105841, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 76205520 	 20388 	 34.43401384353638 	 27.851874113082886 	 0.4313647747039795 	 0.46513986587524414 	 68.18143224716187 	 69.67449116706848 	 0.24417400360107422 	 0.2493877410888672 	 
2025-08-06 12:04:52.371133 test begin: paddle.tensordot(x=Tensor([4, 2, 158761, 5, 4],"float64"), y=Tensor([2, 4, 158761, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 158761, 5, 4],"float64"), y=Tensor([2, 4, 158761, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 76205280 	 20388 	 34.069814682006836 	 27.628499507904053 	 0.42678332328796387 	 0.46142053604125977 	 67.6566276550293 	 69.63153314590454 	 0.2423079013824463 	 0.2492053508758545 	 
2025-08-06 12:08:15.300822 test begin: paddle.tensordot(x=Tensor([4, 2, 3, 132301, 4],"float64"), y=Tensor([2, 4, 3, 132301, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 3, 132301, 4],"float64"), y=Tensor([2, 4, 3, 132301, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 38102688 	 20388 	 17.1584734916687 	 14.091588497161865 	 0.21508336067199707 	 0.23563885688781738 	 33.880776166915894 	 34.95730423927307 	 0.21219730377197266 	 0.21886920928955078 	 
2025-08-06 12:09:56.420320 test begin: paddle.tensordot(x=Tensor([4, 2, 3, 264601, 4],"float64"), y=Tensor([2, 4, 3, 264601, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 3, 264601, 4],"float64"), y=Tensor([2, 4, 3, 264601, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 76205088 	 20388 	 34.086111068725586 	 27.699887990951538 	 0.42716026306152344 	 0.46256351470947266 	 67.67269253730774 	 70.28334999084473 	 0.24229049682617188 	 0.261533260345459 	 
2025-08-06 12:13:18.853920 test begin: paddle.tensordot(x=Tensor([4, 2, 3, 5, 211681],"float64"), y=Tensor([2, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 3, 5, 211681],"float64"), y=Tensor([2, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 25402680 	 20388 	 10.787459373474121 	 4.059277057647705 	 0.18036413192749023 	 0.10158085823059082 	 7.160442590713501 	 7.7403528690338135 	 0.11956787109375 	 0.12915587425231934 	 
2025-08-06 12:13:49.873299 test begin: paddle.tensordot(x=Tensor([4, 2, 3, 5, 4],"float64"), y=Tensor([2, 4, 3, 5, 211681],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 3, 5, 4],"float64"), y=Tensor([2, 4, 3, 5, 211681],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 25402200 	 20388 	 10.130478620529175 	 9.93214225769043 	 0.16916632652282715 	 0.24887466430664062 	 7.687175035476685 	 7.527393579483032 	 0.12832403182983398 	 0.12566113471984863 	 
2025-08-06 12:14:26.094958 test begin: paddle.tensordot(x=Tensor([4, 2, 79381, 5, 4],"float64"), y=Tensor([2, 4, 79381, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 79381, 5, 4],"float64"), y=Tensor([2, 4, 79381, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 38102880 	 20388 	 17.244802474975586 	 14.10514211654663 	 0.21607613563537598 	 0.2356109619140625 	 33.90440607070923 	 34.95277500152588 	 0.21229958534240723 	 0.21890664100646973 	 
2025-08-06 12:16:08.076744 test begin: paddle.tensordot(x=Tensor([4, 52921, 3, 5, 4],"float64"), y=Tensor([52921, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 52921, 3, 5, 4],"float64"), y=Tensor([52921, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 38103120 	 20388 	 17.424967288970947 	 14.171744108200073 	 0.21841073036193848 	 0.23662209510803223 	 34.177794456481934 	 34.92172431945801 	 0.21419191360473633 	 0.21863794326782227 	 
2025-08-06 12:17:49.623066 test begin: paddle.tile(Tensor([102426, 248, 1, 1, 2],"float32"), list[1,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([102426, 248, 1, 1, 2],"float32"), list[1,1,1,1,1,], ) 	 50803296 	 33767 	 10.000978708267212 	 10.57450556755066 	 0.3026998043060303 	 0.1600017547607422 	 10.570905208587646 	 1.7218215465545654 	 0.1599564552307129 	 7.05718994140625e-05 	 
2025-08-06 12:18:24.176960 test begin: paddle.tile(Tensor([1511, 10, 1, 58, 58],"float32"), list[1,1,4,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([1511, 10, 1, 58, 58],"float32"), list[1,1,4,1,1,], ) 	 50830040 	 33767 	 66.5823221206665 	 32.743656158447266 	 1.0076265335083008 	 0.9574034214019775 	 63.74893522262573 	 24.691033601760864 	 1.929997205734253 	 0.7472491264343262 	 
2025-08-06 12:21:38.417403 test begin: paddle.tile(Tensor([16, 1, 1, 3, 16538, 64],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 1, 1, 3, 16538, 64],"float32"), list[1,11,1,1,1,1,], ) 	 50804736 	 33767 	 205.36974430084229 	 92.76672601699829 	 3.1024231910705566 	 1.4037468433380127 	 113.65208745002747 	 58.573460817337036 	 3.4400253295898438 	 0.8864192962646484 	 
2025-08-06 12:29:43.785936 test begin: paddle.tile(Tensor([16, 1, 1, 3, 64, 16538],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 1, 1, 3, 64, 16538],"float32"), list[1,11,1,1,1,1,], ) 	 50804736 	 33767 	 204.9873161315918 	 92.76844096183777 	 3.1020729541778564 	 1.4037718772888184 	 113.63227558135986 	 58.5729706287384 	 3.4411470890045166 	 0.8864223957061768 	 
2025-08-06 12:37:43.858064 test begin: paddle.tile(Tensor([16, 1, 1, 776, 64, 64],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 1, 1, 776, 64, 64],"float32"), list[1,11,1,1,1,1,], ) 	 50855936 	 33767 	 205.35004878044128 	 92.97828197479248 	 3.1078104972839355 	 1.4071235656738281 	 113.70912599563599 	 58.60904598236084 	 3.44067120552063 	 0.8869190216064453 	 
2025-08-06 12:45:44.606742 test begin: paddle.tile(Tensor([16, 1, 259, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 1, 259, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], ) 	 50921472 	 33767 	 205.69674968719482 	 93.12929654121399 	 3.1130616664886475 	 1.409376621246338 	 113.72744560241699 	 58.689515113830566 	 3.441331386566162 	 0.8881888389587402 	 
2025-08-06 12:53:45.794327 test begin: paddle.tile(Tensor([16, 10, 1, 5475, 58],"float32"), list[1,1,4,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 10, 1, 5475, 58],"float32"), list[1,1,4,1,1,], ) 	 50808000 	 33767 	 65.62412977218628 	 29.268309116363525 	 0.9932329654693604 	 0.8858044147491455 	 63.77222919464111 	 25.170172691345215 	 1.9300143718719482 	 0.7617900371551514 	 
2025-08-06 12:56:53.820223 test begin: paddle.tile(Tensor([16, 10, 1, 58, 5475],"float32"), list[1,1,4,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 10, 1, 58, 5475],"float32"), list[1,1,4,1,1,], ) 	 50808000 	 33767 	 65.62129783630371 	 29.28616952896118 	 0.9933357238769531 	 0.8862695693969727 	 63.79179382324219 	 25.17007279396057 	 1.930128574371338 	 0.7617630958557129 	 
2025-08-06 13:00:01.896542 test begin: paddle.tile(Tensor([16, 10, 95, 58, 58],"float32"), list[1,1,4,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 10, 95, 58, 58],"float32"), list[1,1,4,1,1,], ) 	 51132800 	 33767 	 66.00214791297913 	 29.474401473999023 	 0.9991791248321533 	 0.8917911052703857 	 64.16844487190247 	 24.934049606323242 	 1.9418423175811768 	 0.7546563148498535 	 
2025-08-06 13:03:10.755389 test begin: paddle.tile(Tensor([16, 259, 1, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 259, 1, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], ) 	 50921472 	 33767 	 206.2455973625183 	 93.23407506942749 	 3.1090610027313232 	 1.4109017848968506 	 113.70704698562622 	 58.690157651901245 	 3.441185712814331 	 0.8881380558013916 	 
2025-08-06 13:11:15.983372 test begin: paddle.tile(Tensor([16, 944, 1, 58, 58],"float32"), list[1,1,4,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 944, 1, 58, 58],"float32"), list[1,1,4,1,1,], ) 	 50809856 	 33767 	 66.55255961418152 	 31.623213529586792 	 1.0072715282440186 	 0.9571559429168701 	 63.73094820976257 	 24.680256843566895 	 1.929330825805664 	 0.7469751834869385 	 
2025-08-06 13:14:26.832971 test begin: paddle.tile(Tensor([216, 117601, 1, 1, 2],"float32"), list[1,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([216, 117601, 1, 1, 2],"float32"), list[1,1,1,1,1,], ) 	 50803632 	 33767 	 10.684148073196411 	 10.574106216430664 	 0.3027641773223877 	 0.16003727912902832 	 10.570589065551758 	 1.7547941207885742 	 0.15996623039245605 	 7.295608520507812e-05 	 
2025-08-06 13:15:03.072006 test begin: paddle.tile(Tensor([216, 248, 1, 1, 949],"float32"), list[1,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([216, 248, 1, 1, 949],"float32"), list[1,1,1,1,1,], ) 	 50836032 	 33767 	 10.011046886444092 	 10.575799942016602 	 0.30297303199768066 	 0.16004347801208496 	 10.573423624038696 	 1.7444078922271729 	 0.1599743366241455 	 7.605552673339844e-05 	 
2025-08-06 13:15:38.883074 test begin: paddle.tile(Tensor([216, 248, 1, 475, 2],"float32"), list[1,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([216, 248, 1, 475, 2],"float32"), list[1,1,1,1,1,], ) 	 50889600 	 33767 	 10.265809297561646 	 10.58854866027832 	 0.3033771514892578 	 0.1602463722229004 	 10.591436386108398 	 1.7463133335113525 	 0.160264253616333 	 7.510185241699219e-05 	 
2025-08-06 13:16:14.659075 test begin: paddle.tile(Tensor([216, 248, 475, 1, 2],"float32"), list[1,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([216, 248, 475, 1, 2],"float32"), list[1,1,1,1,1,], ) 	 50889600 	 33767 	 10.022282361984253 	 10.5975980758667 	 0.303403377532959 	 0.1602640151977539 	 10.591496706008911 	 2.2044179439544678 	 0.16030406951904297 	 8.940696716308594e-05 	 
2025-08-06 13:16:51.743538 test begin: paddle.tile(Tensor([4135, 1, 1, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([4135, 1, 1, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], ) 	 50810880 	 33767 	 169.109224319458 	 83.09571504592896 	 2.6011905670166016 	 1.2575185298919678 	 113.43204379081726 	 59.01019740104675 	 3.432717800140381 	 0.8930237293243408 	 
2025-08-06 13:24:07.368809 test begin: paddle.tolist(Tensor([10160, 5],"float32"), )
[Prof] paddle.tolist 	 paddle.tolist(Tensor([10160, 5],"float32"), ) 	 50800 	 11272 	 115.82368659973145 	 156.13245964050293 	 0.0001366138458251953 	 0.00016736984252929688 	 None 	 None 	 None 	 None 	 
2025-08-06 13:28:39.821791 test begin: paddle.tolist(Tensor([2, 25400],"float32"), )
[Prof] paddle.tolist 	 paddle.tolist(Tensor([2, 25400],"float32"), ) 	 50800 	 11272 	 10.403700351715088 	 11.846380233764648 	 0.00012421607971191406 	 0.0002422332763671875 	 None 	 None 	 None 	 None 	 
2025-08-06 13:29:02.081272 test begin: paddle.tolist(Tensor([8467, 3],"int64"), )
[Prof] paddle.tolist 	 paddle.tolist(Tensor([8467, 3],"int64"), ) 	 25401 	 11272 	 66.21659541130066 	 116.24642992019653 	 8.153915405273438e-05 	 0.00010514259338378906 	 None 	 None 	 None 	 None 	 
2025-08-06 13:32:04.552472 test begin: paddle.topk(Tensor([138, 369303],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([138, 369303],"float32"), k=1, axis=0, ) 	 50963814 	 4760 	 11.44147777557373 	 40.75521683692932 	 0.6129708290100098 	 8.749659538269043 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:33:27.847895 test begin: paddle.topk(Tensor([146, 349866],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([146, 349866],"float32"), k=1, axis=0, ) 	 51080436 	 4760 	 11.162789583206177 	 52.0560348033905 	 0.5981247425079346 	 11.171454668045044 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:35:01.965522 test begin: paddle.topk(Tensor([148, 343728],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([148, 343728],"float32"), k=1, axis=0, ) 	 50871744 	 4760 	 10.073017835617065 	 44.253018379211426 	 0.5395894050598145 	 9.497651100158691 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:36:28.074462 test begin: paddle.topk(Tensor([49, 1036801],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([49, 1036801],"float32"), k=1, axis=0, ) 	 50803249 	 4760 	 10.55837631225586 	 19.762421131134033 	 0.5656921863555908 	 4.243075132369995 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:37:31.965801 test begin: paddle.topk(Tensor([53, 958551],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([53, 958551],"float32"), k=1, axis=0, ) 	 50803203 	 4760 	 10.620354413986206 	 20.46070885658264 	 0.5685510635375977 	 4.393162727355957 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:38:40.285668 test begin: paddle.topk(Tensor([55, 923695],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([55, 923695],"float32"), k=1, axis=0, ) 	 50803225 	 4760 	 10.484608173370361 	 21.63281536102295 	 0.5614919662475586 	 4.644929885864258 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-08-06 13:39:45.650572 test begin: paddle.trace(x=Tensor([2, 3, 4233601],"float64"), offset=0, axis1=-3, axis2=-2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([2, 3, 4233601],"float64"), offset=0, axis1=-3, axis2=-2, ) 	 25401606 	 153440 	 36.90917253494263 	 12.347900152206421 	 9.202957153320312e-05 	 0.08222556114196777 	 117.42694664001465 	 37.24485206604004 	 7.271766662597656e-05 	 0.12288975715637207 	 combined
2025-08-06 13:43:10.266958 test begin: paddle.trace(x=Tensor([2, 6350401, 2],"float64"), offset=1, axis1=0, axis2=2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([2, 6350401, 2],"float64"), offset=1, axis1=0, axis2=2, ) 	 25401604 	 153440 	 36.623854637145996 	 17.892066955566406 	 9.131431579589844e-05 	 0.11916661262512207 	 117.29407596588135 	 51.30734181404114 	 7.033348083496094e-05 	 0.17080903053283691 	 combined
2025-08-06 13:46:54.115345 test begin: paddle.trace(x=Tensor([20, 3, 42336],"float64"), offset=1, axis1=0, axis2=2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([20, 3, 42336],"float64"), offset=1, axis1=0, axis2=2, ) 	 2540160 	 153440 	 9.673882246017456 	 3.2171621322631836 	 9.107589721679688e-05 	 8.940696716308594e-05 	 25.793849229812622 	 14.155484199523926 	 6.937980651855469e-05 	 0.0002353191375732422 	 combined
2025-08-06 13:47:47.029714 test begin: paddle.trace(x=Tensor([30, 84672],"float64"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([30, 84672],"float64"), offset=0, axis1=0, axis2=1, ) 	 2540160 	 153440 	 9.99853515625 	 3.313823938369751 	 9.369850158691406e-05 	 0.00020122528076171875 	 20.63406729698181 	 15.157774448394775 	 7.319450378417969e-05 	 0.00024509429931640625 	 combined
2025-08-06 13:48:37.495025 test begin: paddle.trace(x=Tensor([4233601, 3, 2],"float64"), offset=0, axis1=-3, axis2=-2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([4233601, 3, 2],"float64"), offset=0, axis1=-3, axis2=-2, ) 	 25401606 	 153440 	 9.692568063735962 	 3.257272958755493 	 9.226799011230469e-05 	 7.605552673339844e-05 	 115.77573728561401 	 21.23349356651306 	 7.009506225585938e-05 	 0.07070088386535645 	 combined
2025-08-06 13:51:08.068433 test begin: paddle.trace(x=Tensor([4233601, 3, 2],"float64"), offset=1, axis1=0, axis2=2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([4233601, 3, 2],"float64"), offset=1, axis1=0, axis2=2, ) 	 25401606 	 153440 	 9.927708387374878 	 3.205963373184204 	 0.00011181831359863281 	 0.00018453598022460938 	 115.9567322731018 	 21.217564344406128 	 0.00019669532775878906 	 0.0706026554107666 	 combined
2025-08-06 13:53:40.295181 test begin: paddle.trace(x=Tensor([6350401, 4],"float64"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([6350401, 4],"float64"), offset=0, axis1=0, axis2=1, ) 	 25401604 	 153440 	 10.99215579032898 	 4.741399049758911 	 0.00010991096496582031 	 0.00012183189392089844 	 93.64283299446106 	 21.223493576049805 	 8.320808410644531e-05 	 0.07062721252441406 	 combined
2025-08-06 13:55:51.610587 test begin: paddle.transpose(Tensor([20, 150, 512, 512],"float32"), list[0,2,3,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([20, 150, 512, 512],"float32"), list[0,2,3,1,], ) 	 786432000 	 2997001 	 10.449178218841553 	 13.634407997131348 	 6.842613220214844e-05 	 0.00010776519775390625 	 121.98818254470825 	 212.48920321464539 	 0.00014162063598632812 	 0.0005009174346923828 	 
2025-08-06 14:02:17.389391 test begin: paddle.transpose(Tensor([20, 7168, 7168],"bfloat16"), list[0,2,1,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f00a49f4c10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 14:12:27.186485 test begin: paddle.transpose(Tensor([40, 150, 166, 512],"float32"), list[0,2,3,1,], )
W0806 14:12:34.988654 144687 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.transpose 	 paddle.transpose(Tensor([40, 150, 166, 512],"float32"), list[0,2,3,1,], ) 	 509952000 	 2997001 	 10.320037841796875 	 16.585821628570557 	 0.0001659393310546875 	 0.00036978721618652344 	 115.75731301307678 	 164.64956307411194 	 0.00010824203491210938 	 0.00023627281188964844 	 
2025-08-06 14:17:52.066071 test begin: paddle.transpose(Tensor([40, 150, 512, 166],"float32"), list[0,2,3,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([40, 150, 512, 166],"float32"), list[0,2,3,1,], ) 	 509952000 	 2997001 	 10.52319049835205 	 15.888503789901733 	 0.0001220703125 	 0.00028586387634277344 	 117.71184134483337 	 159.51400804519653 	 0.00010466575622558594 	 0.00023698806762695312 	 
2025-08-06 14:23:14.325701 test begin: paddle.transpose(Tensor([40, 3584, 7168],"bfloat16"), list[0,2,1,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb385907b80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 14:33:20.662780 test begin: paddle.transpose(Tensor([40, 49, 512, 512],"float32"), list[0,2,3,1,], )
W0806 14:33:31.376701 162027 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.transpose 	 paddle.transpose(Tensor([40, 49, 512, 512],"float32"), list[0,2,3,1,], ) 	 513802240 	 2997001 	 22.268348693847656 	 17.123846530914307 	 0.0002124309539794922 	 0.0001766681671142578 	 118.45376205444336 	 164.0846390724182 	 0.00010609626770019531 	 0.0002772808074951172 	 
2025-08-06 14:39:03.109686 test begin: paddle.transpose(Tensor([60, 2363, 7168],"bfloat16"), list[0,2,1,], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f64a0e72e00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 14:49:09.501276 test begin: paddle.transpose(Tensor([60, 3584, 4726],"bfloat16"), list[0,2,1,], )
W0806 14:49:33.368038 12945 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5e5545f070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 14:59:15.840038 test begin: paddle.transpose(Tensor([60, 7168, 2363],"bfloat16"), list[0,2,1,], )
W0806 14:59:31.703219 26695 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe2656f3070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 15:09:22.144408 test begin: paddle.tril(Tensor([1, 1, 2048, 24807],"bool"), )
W0806 15:09:23.090214 44744 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 1, 2048, 24807],"bool"), ) 	 50804736 	 32473 	 9.985064029693604 	 8.408923864364624 	 0.31400394439697266 	 0.2644674777984619 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:09:53.365802 test begin: paddle.tril(Tensor([1, 1, 2048, 24807],"float32"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 1, 2048, 24807],"float32"), ) 	 50804736 	 32473 	 10.088106870651245 	 10.791776657104492 	 0.3184845447540283 	 0.3392362594604492 	 10.086838960647583 	 10.784926414489746 	 0.3172485828399658 	 0.3392155170440674 	 
2025-08-06 15:10:37.878740 test begin: paddle.tril(Tensor([1, 1, 24807, 2048],"bool"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 1, 24807, 2048],"bool"), ) 	 50804736 	 32473 	 12.086158990859985 	 7.636767864227295 	 0.3802216053009033 	 0.24033570289611816 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:11:11.118782 test begin: paddle.tril(Tensor([1, 1, 24807, 2048],"float32"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 1, 24807, 2048],"float32"), ) 	 50804736 	 32473 	 13.457291603088379 	 12.227495193481445 	 0.426067590713501 	 0.3843531608581543 	 13.456506490707397 	 12.222266912460327 	 0.4233567714691162 	 0.3843865394592285 	 
2025-08-06 15:12:05.565727 test begin: paddle.tril(Tensor([1, 13, 2048, 2048],"bool"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 13, 2048, 2048],"bool"), ) 	 54525952 	 32473 	 12.393696308135986 	 8.171068668365479 	 0.3912694454193115 	 0.25620198249816895 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:12:41.323093 test begin: paddle.tril(Tensor([1, 13, 2048, 2048],"float32"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 13, 2048, 2048],"float32"), ) 	 54525952 	 32473 	 13.482132911682129 	 12.380971670150757 	 0.42393040657043457 	 0.3892936706542969 	 13.470087766647339 	 12.365625858306885 	 0.42356371879577637 	 0.3896763324737549 	 
2025-08-06 15:13:40.425377 test begin: paddle.tril(Tensor([13, 1, 2048, 2048],"bool"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([13, 1, 2048, 2048],"bool"), ) 	 54525952 	 32473 	 12.415908813476562 	 8.159692287445068 	 0.3905212879180908 	 0.25609445571899414 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:14:15.685418 test begin: paddle.tril(Tensor([13, 1, 2048, 2048],"float32"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([13, 1, 2048, 2048],"float32"), ) 	 54525952 	 32473 	 13.481107473373413 	 12.368266582489014 	 0.42374253273010254 	 0.3907599449157715 	 13.465193510055542 	 12.37209415435791 	 0.4231581687927246 	 0.3896758556365967 	 
2025-08-06 15:15:10.636593 test begin: paddle.tril(Tensor([2048, 24807],"bool"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([2048, 24807],"bool"), ) 	 50804736 	 32473 	 9.987791299819946 	 8.38305377960205 	 0.31412601470947266 	 0.26371192932128906 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:15:41.137545 test begin: paddle.tril(Tensor([24807, 2048],"bool"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([24807, 2048],"bool"), ) 	 50804736 	 32473 	 12.08240532875061 	 7.663775444030762 	 0.38002610206604004 	 0.24097800254821777 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 15:16:14.403091 test begin: paddle.triu(Tensor([1, 1, 1024, 99226],"float16"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 1024, 99226],"float16"), diagonal=1, ) 	 101607424 	 30544 	 23.733333826065063 	 16.102794408798218 	 0.7858164310455322 	 0.5384817123413086 	 23.47533416748047 	 16.096287965774536 	 0.7861542701721191 	 0.5383174419403076 	 
2025-08-06 15:17:41.851751 test begin: paddle.triu(Tensor([1, 1, 12404, 4096],"float32"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 12404, 4096],"float32"), diagonal=1, ) 	 50806784 	 30544 	 9.925235033035278 	 10.35132622718811 	 0.3317451477050781 	 0.3476572036743164 	 9.920289516448975 	 10.345622539520264 	 0.33188724517822266 	 0.345905065536499 	 
2025-08-06 15:18:24.095470 test begin: paddle.triu(Tensor([1, 1, 2048, 49613],"float16"), )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 2048, 49613],"float16"), ) 	 101607424 	 30544 	 23.47343420982361 	 16.054807662963867 	 0.7846128940582275 	 0.5382876396179199 	 23.43387484550476 	 16.04996109008789 	 0.7849884033203125 	 0.5384151935577393 	 
2025-08-06 15:19:46.993531 test begin: paddle.triu(Tensor([1, 1, 4096, 12404],"float32"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 4096, 12404],"float32"), diagonal=1, ) 	 50806784 	 30544 	 12.5232412815094 	 11.354421615600586 	 0.4188253879547119 	 0.37946581840515137 	 12.519268989562988 	 11.34787392616272 	 0.42025232315063477 	 0.3796348571777344 	 
2025-08-06 15:20:41.615289 test begin: paddle.triu(Tensor([1, 1, 49613, 2048],"float16"), )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 49613, 2048],"float16"), ) 	 101607424 	 30544 	 17.960145711898804 	 11.164867162704468 	 0.6007077693939209 	 0.3735842704772949 	 17.96343159675598 	 11.16572642326355 	 0.6008038520812988 	 0.37492847442626953 	 
2025-08-06 15:21:43.715161 test begin: paddle.triu(Tensor([1, 1, 99226, 1024],"float16"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 99226, 1024],"float16"), diagonal=1, ) 	 101607424 	 30544 	 17.867972373962402 	 11.129347324371338 	 0.5975775718688965 	 0.37195539474487305 	 17.871877431869507 	 11.11596965789795 	 0.5975985527038574 	 0.37180256843566895 	 
2025-08-06 15:22:45.616369 test begin: paddle.triu(Tensor([1, 25, 2048, 2048],"float16"), )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 25, 2048, 2048],"float16"), ) 	 104857600 	 30544 	 22.647552251815796 	 13.383455276489258 	 0.7570183277130127 	 0.448167085647583 	 22.66189169883728 	 13.368721723556519 	 0.7577424049377441 	 0.44853806495666504 	 
2025-08-06 15:24:01.647600 test begin: paddle.triu(Tensor([1, 4, 4096, 4096],"float32"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 4, 4096, 4096],"float32"), diagonal=1, ) 	 67108864 	 30544 	 15.122624635696411 	 14.30458402633667 	 0.505795955657959 	 0.4783153533935547 	 15.130069732666016 	 14.30254340171814 	 0.5059459209442139 	 0.4782216548919678 	 
2025-08-06 15:25:02.760799 test begin: paddle.triu(Tensor([1, 97, 1024, 1024],"float16"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 97, 1024, 1024],"float16"), diagonal=1, ) 	 101711872 	 30544 	 22.582165002822876 	 13.038079977035522 	 0.7549467086791992 	 0.4373025894165039 	 22.56218671798706 	 13.032695055007935 	 0.7558650970458984 	 0.43720030784606934 	 
2025-08-06 15:26:17.871941 test begin: paddle.triu(Tensor([25, 1, 2048, 2048],"float16"), )
[Prof] paddle.triu 	 paddle.triu(Tensor([25, 1, 2048, 2048],"float16"), ) 	 104857600 	 30544 	 22.663981914520264 	 13.372455358505249 	 0.759695291519165 	 0.44716382026672363 	 22.647209644317627 	 13.364567518234253 	 0.7573442459106445 	 0.44696497917175293 	 
2025-08-06 15:27:35.886929 test begin: paddle.triu(Tensor([4, 1, 4096, 4096],"float32"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([4, 1, 4096, 4096],"float32"), diagonal=1, ) 	 67108864 	 30544 	 15.379764556884766 	 14.293821573257446 	 0.5055363178253174 	 0.47946667671203613 	 15.128966093063354 	 14.28693675994873 	 0.5058953762054443 	 0.4778163433074951 	 
2025-08-06 15:28:40.772501 test begin: paddle.triu(Tensor([97, 1, 1024, 1024],"float16"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([97, 1, 1024, 1024],"float16"), diagonal=1, ) 	 101711872 	 30544 	 22.60717272758484 	 13.050718784332275 	 0.7557697296142578 	 0.43578243255615234 	 22.594238996505737 	 13.029282808303833 	 0.7557682991027832 	 0.4357726573944092 	 
2025-08-06 15:29:57.108712 test begin: paddle.trunc(Tensor([200, 2540161],"float32"), )
[Prof] paddle.trunc 	 paddle.trunc(Tensor([200, 2540161],"float32"), ) 	 508032200 	 34308 	 117.217289686203 	 100.39763855934143 	 0.0001246929168701172 	 2.990504741668701 	 84.96313333511353 	 45.031198024749756 	 7.653236389160156e-05 	 1.3395624160766602 	 
2025-08-06 15:36:05.345636 test begin: paddle.trunc(Tensor([25401610, 20],"float32"), )
[Prof] paddle.trunc 	 paddle.trunc(Tensor([25401610, 20],"float32"), ) 	 508032200 	 34308 	 117.13325452804565 	 100.38455700874329 	 0.00011515617370605469 	 2.991830348968506 	 85.063884973526 	 45.0314404964447 	 0.00010132789611816406 	 1.3411107063293457 	 
2025-08-06 15:42:12.385929 test begin: paddle.trunc(input=Tensor([1176010, 6, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([1176010, 6, 6, 6],"float64"), ) 	 254018160 	 34308 	 98.81847357749939 	 99.98653984069824 	 0.00010657310485839844 	 2.9795279502868652 	 43.71376585960388 	 45.03979206085205 	 7.43865966796875e-05 	 1.3404748439788818 	 
2025-08-06 15:47:12.381678 test begin: paddle.trunc(input=Tensor([196010, 6, 6, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([196010, 6, 6, 6, 6],"float64"), ) 	 254028960 	 34308 	 98.73285698890686 	 99.97749733924866 	 0.00013303756713867188 	 2.9780969619750977 	 43.728400468826294 	 45.03933310508728 	 7.557868957519531e-05 	 1.3404767513275146 	 
2025-08-06 15:52:12.382750 test begin: paddle.trunc(input=Tensor([30, 39201, 6, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([30, 39201, 6, 6, 6],"float64"), ) 	 254022480 	 34308 	 98.87786769866943 	 99.97221827507019 	 0.00011229515075683594 	 2.9765026569366455 	 43.72080445289612 	 45.03935432434082 	 8.606910705566406e-05 	 1.3421502113342285 	 
2025-08-06 15:57:12.847977 test begin: paddle.trunc(input=Tensor([30, 6, 39201, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([30, 6, 39201, 6, 6],"float64"), ) 	 254022480 	 34308 	 98.88139247894287 	 99.98003911972046 	 0.0001125335693359375 	 2.9766063690185547 	 43.715736627578735 	 45.04461979866028 	 8.559226989746094e-05 	 1.3420209884643555 	 
2025-08-06 16:02:12.890137 test begin: paddle.trunc(input=Tensor([30, 6, 6, 39201, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([30, 6, 6, 39201, 6],"float64"), ) 	 254022480 	 34308 	 98.87844109535217 	 99.97539329528809 	 0.00010180473327636719 	 2.9780113697052 	 43.721097230911255 	 45.04154300689697 	 4.5299530029296875e-05 	 1.340423345565796 	 
2025-08-06 16:07:12.960148 test begin: paddle.trunc(input=Tensor([30, 6, 6, 6, 39201],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([30, 6, 6, 6, 39201],"float64"), ) 	 254022480 	 34308 	 98.87770080566406 	 99.97508764266968 	 0.00010228157043457031 	 2.976525068283081 	 43.719703674316406 	 45.0427463054657 	 4.00543212890625e-05 	 1.3437163829803467 	 
2025-08-06 16:12:14.948582 test begin: paddle.trunc(input=Tensor([60, 117601, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([60, 117601, 6, 6],"float64"), ) 	 254018160 	 34308 	 98.98870348930359 	 100.00614666938782 	 0.00011396408081054688 	 2.9794089794158936 	 43.73709011077881 	 45.05216979980469 	 7.653236389160156e-05 	 1.3412091732025146 	 
2025-08-06 16:17:15.255213 test begin: paddle.trunc(input=Tensor([60, 6, 117601, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([60, 6, 117601, 6],"float64"), ) 	 254018160 	 34308 	 99.00209188461304 	 100.00489568710327 	 0.00011157989501953125 	 2.9807944297790527 	 43.73359680175781 	 45.053218364715576 	 7.843971252441406e-05 	 1.3411779403686523 	 
2025-08-06 16:22:15.632745 test begin: paddle.trunc(input=Tensor([60, 6, 6, 117601],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([60, 6, 6, 117601],"float64"), ) 	 254018160 	 34308 	 99.23830771446228 	 100.01637029647827 	 0.00011777877807617188 	 2.980973482131958 	 43.731714487075806 	 45.054017066955566 	 7.724761962890625e-05 	 1.3404598236083984 	 
2025-08-06 16:27:16.375469 test begin: paddle.unbind(Tensor([20, 3, 1058401, 8],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([20, 3, 1058401, 8],"float32"), axis=0, ) 	 508032480 	 85063 	 2.430881977081299 	 2.580439567565918 	 6.890296936035156e-05 	 0.00023555755615234375 	 308.6766700744629 	 259.1627960205078 	 3.708754301071167 	 3.115265130996704 	 
2025-08-06 16:37:06.713921 test begin: paddle.unbind(Tensor([20, 3, 8, 1058401],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([20, 3, 8, 1058401],"float32"), axis=0, ) 	 508032480 	 85063 	 2.9565916061401367 	 1.9759376049041748 	 0.000110626220703125 	 8.296966552734375e-05 	 308.6906633377075 	 259.1570084095001 	 3.710101366043091 	 3.113985776901245 	 
2025-08-06 16:47:01.037125 test begin: paddle.unbind(Tensor([20, 396901, 8, 8],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([20, 396901, 8, 8],"float32"), axis=0, ) 	 508033280 	 85063 	 2.4343628883361816 	 1.9852826595306396 	 8.96453857421875e-05 	 0.00020503997802734375 	 299.3288972377777 	 254.34191393852234 	 3.5979106426239014 	 3.0560550689697266 	 
2025-08-06 16:56:40.924534 test begin: paddle.unbind(Tensor([30, 3386881, 5],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([30, 3386881, 5],"float32"), axis=0, ) 	 508032150 	 85063 	 3.3835818767547607 	 2.814509630203247 	 0.00011444091796875 	 0.0002269744873046875 	 312.642023563385 	 261.1981179714203 	 3.756181240081787 	 3.137322425842285 	 
2025-08-06 17:06:43.585840 test begin: paddle.unbind(Tensor([30, 9, 1881601],"float32"), axis=0, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc7d015ea40>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1754471805 (unix time) try "date -d @1754471805" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xae1a) received by PID 44570 (TID 0x7fc7c78f8640) from PID 44570 ***]

2025-08-06 17:17:00.777586 test begin: paddle.unbind(Tensor([40, 2116801, 6],"float32"), )
W0806 17:17:10.297557 154069 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f53c5d77010>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception
2025-08-06 17:27:16.494725 test begin: paddle.unbind(Tensor([40, 5, 2540161],"float32"), )
W0806 17:27:24.180945 16675 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fce29553010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 17:37:27.483963 test begin: paddle.unflatten(x=Tensor([4, 793801, 16],"float32"), axis=0, shape=Tensor([2],"int64"), )
W0806 17:37:28.542586 42563 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([4, 793801, 16],"float32"), axis=0, shape=Tensor([2],"int64"), ) 	 50803266 	 101360 	 9.132905006408691 	 0.5414910316467285 	 8.654594421386719e-05 	 9.34600830078125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:37:44.640296 test begin: paddle.unflatten(x=Tensor([40, 6, 2116801],"float32"), axis=0, shape=Tensor([2],"int64"), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([40, 6, 2116801],"float32"), axis=0, shape=Tensor([2],"int64"), ) 	 508032242 	 101360 	 10.131510734558105 	 0.5102381706237793 	 8.511543273925781e-05 	 8.869171142578125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:38:18.168410 test begin: paddle.unflatten(x=Tensor([40, 793801, 16],"float32"), axis=0, shape=Tensor([2],"int64"), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([40, 793801, 16],"float32"), axis=0, shape=Tensor([2],"int64"), ) 	 508032642 	 101360 	 9.10011625289917 	 0.5145354270935059 	 6.222724914550781e-05 	 7.462501525878906e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:38:50.120009 test begin: paddle.unflatten(x=Tensor([5292010, 6, 16],"float32"), axis=0, shape=Tensor([2],"int64"), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([5292010, 6, 16],"float32"), axis=0, shape=Tensor([2],"int64"), ) 	 508032962 	 101360 	 9.220245599746704 	 0.5009171962738037 	 6.246566772460938e-05 	 6.413459777832031e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 17:39:23.601067 test begin: paddle.unfold(Tensor([50, 20321281],"float16"), 0, 5, 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f986f046ce0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 17:49:45.986261 test begin: paddle.unique(Tensor([25401601],"int64"), )
W0806 17:49:46.533951 73324 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.unique 	 paddle.unique(Tensor([25401601],"int64"), ) 	 25401601 	 1489 	 9.981936693191528 	 4.88323187828064 	 8.821487426757812e-05 	 0.0002455711364746094 	 None 	 None 	 None 	 None 	 
2025-08-06 17:50:01.723543 test begin: paddle.unique(Tensor([25401601],"int64"), return_index=True, return_inverse=True, return_counts=True, dtype="int32", )
[Prof] paddle.unique 	 paddle.unique(Tensor([25401601],"int64"), return_index=True, return_inverse=True, return_counts=True, dtype="int32", ) 	 25401601 	 1489 	 15.003170490264893 	 16.60282588005066 	 0.000110626220703125 	 0.00021529197692871094 	 None 	 None 	 None 	 None 	 
2025-08-06 17:50:33.886877 test begin: paddle.unique_consecutive(Tensor([25401601],"float64"), )
[Prof] paddle.unique_consecutive 	 paddle.unique_consecutive(Tensor([25401601],"float64"), ) 	 25401601 	 6999 	 43.26280975341797 	 2.5514280796051025 	 8.249282836914062e-05 	 0.0002162456512451172 	 None 	 None 	 None 	 None 	 
2025-08-06 17:51:20.274107 test begin: paddle.unique_consecutive(Tensor([25401601],"float64"), return_inverse=True, return_counts=True, )
[Prof] paddle.unique_consecutive 	 paddle.unique_consecutive(Tensor([25401601],"float64"), return_inverse=True, return_counts=True, ) 	 25401601 	 6999 	 60.339083433151245 	 7.073036432266235 	 7.724761962890625e-05 	 0.0002079010009765625 	 None 	 None 	 None 	 None 	 
2025-08-06 17:52:28.423809 test begin: paddle.unique_consecutive(Tensor([2540],"float64"), return_inverse=True, return_counts=True, axis=-1, )
[Prof] paddle.unique_consecutive 	 paddle.unique_consecutive(Tensor([2540],"float64"), return_inverse=True, return_counts=True, axis=-1, ) 	 2540 	 6999 	 31.300427436828613 	 1.2293193340301514 	 7.104873657226562e-05 	 0.00019669532775878906 	 None 	 None 	 None 	 None 	 
2025-08-06 17:53:01.413396 test begin: paddle.unsqueeze(Tensor([250, 1024, 1024],"int64"), 1, )
Warning: The core code of paddle.unsqueeze is too complex.
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([250, 1024, 1024],"int64"), 1, ) 	 262144000 	 2405680 	 10.020286083221436 	 8.712979316711426 	 8.153915405273438e-05 	 0.0003008842468261719 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 17:55:12.091853 test begin: paddle.unsqueeze(Tensor([39700, 50, 256],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([39700, 50, 256],"float32"), axis=2, ) 	 508160000 	 2405680 	 10.358556985855103 	 8.645569086074829 	 0.00013899803161621094 	 8.559226989746094e-05 	 101.6904284954071 	 132.03262281417847 	 0.00011086463928222656 	 0.00026297569274902344 	 
2025-08-06 17:59:43.158219 test begin: paddle.unsqueeze(Tensor([40, 1024, 6202],"int64"), 1, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([40, 1024, 6202],"int64"), 1, ) 	 254033920 	 2405680 	 11.121278047561646 	 8.62297010421753 	 0.0001392364501953125 	 8.249282836914062e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 18:01:59.785173 test begin: paddle.unsqueeze(Tensor([40, 6202, 1024],"int64"), 1, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([40, 6202, 1024],"int64"), 1, ) 	 254033920 	 2405680 	 10.026961326599121 	 8.596492767333984 	 0.0001366138458251953 	 0.00026702880859375 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 18:04:08.809770 test begin: paddle.unsqueeze(Tensor([4160, 478, 256],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([4160, 478, 256],"float32"), axis=2, ) 	 509050880 	 2405680 	 13.575999975204468 	 8.690082550048828 	 0.00034880638122558594 	 0.0001246929168701172 	 104.35250186920166 	 138.84275364875793 	 0.00018024444580078125 	 0.0006902217864990234 	 
2025-08-06 18:08:53.670737 test begin: paddle.unsqueeze(Tensor([4160, 50, 2443],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([4160, 50, 2443],"float32"), axis=2, ) 	 508144000 	 2405680 	 10.227740287780762 	 8.697999715805054 	 0.0002930164337158203 	 0.00014090538024902344 	 102.20910549163818 	 147.592524766922 	 0.0001468658447265625 	 0.00027680397033691406 	 
2025-08-06 18:13:41.944252 test begin: paddle.unsqueeze(Tensor([5120, 388, 256],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([5120, 388, 256],"float32"), axis=2, ) 	 508559360 	 2405680 	 10.26257610321045 	 8.762825727462769 	 0.00018548965454101562 	 0.0006101131439208984 	 101.7043924331665 	 133.07385420799255 	 0.00012111663818359375 	 0.00023698806762695312 	 
2025-08-06 18:18:12.932006 test begin: paddle.unsqueeze(Tensor([5120, 50, 1985],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([5120, 50, 1985],"float32"), axis=2, ) 	 508160000 	 2405680 	 10.334311962127686 	 8.715985298156738 	 0.000141143798828125 	 0.00028204917907714844 	 103.71423316001892 	 132.5765872001648 	 0.0001049041748046875 	 0.0002701282501220703 	 
2025-08-06 18:22:46.019753 test begin: paddle.unstack(Tensor([338689, 10, 15],"float32"), axis=-1, )
[Prof] paddle.unstack 	 paddle.unstack(Tensor([338689, 10, 15],"float32"), axis=-1, ) 	 50803350 	 30584 	 10.039508819580078 	 0.5672709941864014 	 0.33505845069885254 	 7.390975952148438e-05 	 14.491537094116211 	 136.25200390815735 	 0.4842982292175293 	 4.548452615737915 	 
2025-08-06 18:25:28.952303 test begin: paddle.unstack(Tensor([338689, 10, 15],"float32"), axis=-2, )
[Prof] paddle.unstack 	 paddle.unstack(Tensor([338689, 10, 15],"float32"), axis=-2, ) 	 50803350 	 30584 	 12.374808549880981 	 0.41177964210510254 	 0.41321444511413574 	 7.271766662597656e-05 	 10.754570245742798 	 38.73267221450806 	 0.3583357334136963 	 1.2950894832611084 	 
2025-08-06 18:26:32.935553 test begin: paddle.unstack(Tensor([5, 10, 1016065],"float32"), axis=-2, )
[Prof] paddle.unstack 	 paddle.unstack(Tensor([5, 10, 1016065],"float32"), axis=-2, ) 	 50803250 	 30584 	 11.918473482131958 	 0.4077107906341553 	 0.39920902252197266 	 8.034706115722656e-05 	 10.132974863052368 	 9.359537839889526 	 0.3383059501647949 	 0.3137633800506592 	 
2025-08-06 18:27:06.383843 test begin: paddle.unstack(Tensor([5, 677377, 15],"float32"), axis=-1, )
[Prof] paddle.unstack 	 paddle.unstack(Tensor([5, 677377, 15],"float32"), axis=-1, ) 	 50803275 	 30584 	 10.042792081832886 	 0.571549654006958 	 0.33512258529663086 	 7.414817810058594e-05 	 14.533006429672241 	 136.23985981941223 	 0.4846014976501465 	 4.55882453918457 	 
2025-08-06 18:29:49.393186 test begin: paddle.unstack(x=Tensor([2, 32, 793801],"float32"), axis=0, )
[Prof] paddle.unstack 	 paddle.unstack(x=Tensor([2, 32, 793801],"float32"), axis=0, ) 	 50803264 	 30584 	 11.882390260696411 	 0.16553378105163574 	 0.39673614501953125 	 9.226799011230469e-05 	 10.695322513580322 	 9.470104932785034 	 0.358234167098999 	 0.3186211585998535 	 
2025-08-06 18:30:23.410788 test begin: paddle.unstack(x=Tensor([2, 49613, 512],"float32"), axis=0, )
[Prof] paddle.unstack 	 paddle.unstack(x=Tensor([2, 49613, 512],"float32"), axis=0, ) 	 50803712 	 30584 	 11.786993980407715 	 0.16312694549560547 	 0.39321255683898926 	 4.38690185546875e-05 	 10.735813617706299 	 9.353127002716064 	 0.35831594467163086 	 0.31208205223083496 	 
2025-08-06 18:30:57.290010 test begin: paddle.unstack(x=Tensor([3101, 32, 512],"float32"), axis=0, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd9232020b0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-08-06 18:41:04.114938 test begin: paddle.var(Tensor([264601, 192, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
W0806 18:41:05.183094 82444 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0806 18:41:05.208660 82444 dygraph_functions.cc:88394] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.var 	 paddle.var(Tensor([264601, 192, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50803392 	 10215 	 13.348664283752441 	 2.807762622833252 	 0.11084842681884766 	 0.2809157371520996 	 16.128294944763184 	 7.948378801345825 	 0.2685737609863281 	 0.19838309288024902 	 
2025-08-06 18:41:46.822412 test begin: paddle.var(Tensor([384, 132301, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([384, 132301, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50803584 	 10215 	 10.551413774490356 	 1.9018476009368896 	 0.07526135444641113 	 0.09490489959716797 	 140.3744342327118 	 7.894872665405273 	 2.0063507556915283 	 0.1577138900756836 	 
2025-08-06 18:44:28.432071 test begin: paddle.var(Tensor([384, 192, 1, 690],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([384, 192, 1, 690],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50872320 	 10215 	 10.42943286895752 	 1.8349857330322266 	 0.07434964179992676 	 0.0914461612701416 	 14.069089889526367 	 7.883761644363403 	 0.20078039169311523 	 0.15753769874572754 	 
2025-08-06 18:45:03.527372 test begin: paddle.var(Tensor([384, 192, 690, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([384, 192, 690, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50872320 	 10215 	 10.435545682907104 	 1.8320140838623047 	 0.07435178756713867 	 0.09143280982971191 	 138.2290894985199 	 7.88658332824707 	 1.9744513034820557 	 0.157606840133667 	 
2025-08-06 18:47:42.777343 test begin: paddle.var(Tensor([384, 96, 1, 1379],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([384, 96, 1, 1379],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50835456 	 10215 	 10.457727670669556 	 1.849229335784912 	 0.07453393936157227 	 0.09229063987731934 	 14.066535234451294 	 7.905465364456177 	 0.2007746696472168 	 0.15793967247009277 	 
2025-08-06 18:48:17.959178 test begin: paddle.var(Tensor([384, 96, 1379, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([384, 96, 1379, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50835456 	 10215 	 10.455048322677612 	 1.8520138263702393 	 0.07451581954956055 	 0.09228014945983887 	 140.9031274318695 	 7.902591705322266 	 2.144906520843506 	 0.15796875953674316 	 
2025-08-06 18:51:00.773779 test begin: paddle.var(Tensor([529201, 96, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([529201, 96, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50803296 	 10215 	 13.129221439361572 	 4.62621808052063 	 0.1091008186340332 	 0.46217870712280273 	 16.260143518447876 	 8.407538175582886 	 0.2735610008239746 	 0.20981931686401367 	 
2025-08-06 18:51:45.328202 test begin: paddle.var(Tensor([58801, 96, 3, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([58801, 96, 3, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50804064 	 10215 	 9.998920679092407 	 1.8097786903381348 	 0.08310532569885254 	 0.1806049346923828 	 13.896077394485474 	 7.8623106479644775 	 0.231459379196167 	 0.19626426696777344 	 
2025-08-06 18:52:19.760506 test begin: paddle.var(Tensor([96, 58801, 3, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([96, 58801, 3, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50804064 	 10215 	 10.132460117340088 	 2.2267684936523438 	 0.07220053672790527 	 0.11261773109436035 	 13.755435705184937 	 8.034296035766602 	 0.19628381729125977 	 0.1605980396270752 	 
2025-08-06 18:52:54.954334 test begin: paddle.var(Tensor([96, 96, 1838, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([96, 96, 1838, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50817024 	 10215 	 10.097816944122314 	 2.1818764209747314 	 0.07195091247558594 	 0.10885858535766602 	 13.756185054779053 	 8.026755809783936 	 0.1962723731994629 	 0.1603989601135254 	 
2025-08-06 18:53:30.089305 test begin: paddle.var(Tensor([96, 96, 3, 1838],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([96, 96, 3, 1838],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50817024 	 10215 	 10.0949068069458 	 2.180079460144043 	 0.0719759464263916 	 0.10885763168334961 	 13.755115032196045 	 8.026675939559937 	 0.19631624221801758 	 0.160369873046875 	 
2025-08-06 18:54:06.556942 test begin: paddle.vecdot(Tensor([12700801, 4],"float32"), Tensor([12700801, 4],"float32"), axis=-1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([12700801, 4],"float32"), Tensor([12700801, 4],"float32"), axis=-1, ) 	 101606408 	 10704 	 12.312562465667725 	 10.007215976715088 	 0.3914029598236084 	 0.4758427143096924 	 17.299274682998657 	 7.326111078262329 	 0.5503146648406982 	 0.3491537570953369 	 combined
2025-08-06 18:54:56.791998 test begin: paddle.vecdot(Tensor([1270081, 4, 5],"float64"), Tensor([1270081, 4, 5],"float64"), axis=1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([1270081, 4, 5],"float64"), Tensor([1270081, 4, 5],"float64"), axis=1, ) 	 50803240 	 10704 	 11.027842998504639 	 6.779389142990112 	 0.3505737781524658 	 0.3231163024902344 	 13.324219465255737 	 7.2089927196502686 	 0.4251418113708496 	 0.34377408027648926 	 combined
2025-08-06 18:55:38.570014 test begin: paddle.vecdot(Tensor([2, 3, 4233601],"float64"), Tensor([2, 3, 4233601],"float64"), axis=-1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([2, 3, 4233601],"float64"), Tensor([2, 3, 4233601],"float64"), axis=-1, ) 	 50803212 	 10704 	 10.659736633300781 	 6.3880791664123535 	 0.25508832931518555 	 0.2027266025543213 	 12.637900352478027 	 6.429326772689819 	 0.40187835693359375 	 0.3080790042877197 	 combined
2025-08-06 18:56:16.614686 test begin: paddle.vecdot(Tensor([2, 3175201, 4],"float64"), Tensor([2, 3175201, 4],"float64"), axis=-1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([2, 3175201, 4],"float64"), Tensor([2, 3175201, 4],"float64"), axis=-1, ) 	 50803216 	 10704 	 10.690442323684692 	 7.663537502288818 	 0.3407480716705322 	 0.364121675491333 	 13.316659927368164 	 7.208060026168823 	 0.423602819442749 	 0.34618353843688965 	 combined
2025-08-06 18:56:57.666449 test begin: paddle.vecdot(Tensor([2116801, 3, 4],"float64"), Tensor([2116801, 3, 4],"float64"), axis=-1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([2116801, 3, 4],"float64"), Tensor([2116801, 3, 4],"float64"), axis=-1, ) 	 50803224 	 10704 	 10.68034029006958 	 7.6657774448394775 	 0.3394637107849121 	 0.36556243896484375 	 13.32047724723816 	 7.194610118865967 	 0.42636895179748535 	 0.3433995246887207 	 combined
2025-08-06 18:57:40.277315 test begin: paddle.vecdot(Tensor([3, 16934401],"float32"), Tensor([3, 16934401],"float32"), axis=-1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([3, 16934401],"float32"), Tensor([3, 16934401],"float32"), axis=-1, ) 	 101606406 	 10704 	 10.373980522155762 	 6.417607069015503 	 0.24701166152954102 	 0.20500779151916504 	 16.549173593521118 	 6.500028371810913 	 0.5274758338928223 	 0.31119203567504883 	 combined
2025-08-06 18:58:22.285239 test begin: paddle.vecdot(Tensor([3, 1693441, 5],"float64"), Tensor([3, 1693441, 5],"float64"), axis=1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([3, 1693441, 5],"float64"), Tensor([3, 1693441, 5],"float64"), axis=1, ) 	 50803230 	 10704 	 75.16774582862854 	 6.421542644500732 	 1.7948765754699707 	 0.20394372940063477 	 12.644204139709473 	 6.4431421756744385 	 0.4019968509674072 	 0.308490514755249 	 combined
2025-08-06 19:00:05.174675 test begin: paddle.vecdot(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), axis=1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), axis=1, ) 	 50803224 	 10704 	 9.977701902389526 	 6.7582173347473145 	 0.3171257972717285 	 0.32231950759887695 	 14.332676887512207 	 9.624744653701782 	 0.45563292503356934 	 0.46021246910095215 	 combined
2025-08-06 19:00:50.123523 test begin: paddle.view(Tensor([100, 10, 10, 50804],"float32"), list[-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 10, 10, 50804],"float32"), list[-1,], ) 	 508040000 	 750201 	 10.213835000991821 	 2.895979881286621 	 0.00011181831359863281 	 0.00012826919555664062 	 32.43580150604248 	 50.47047138214111 	 0.00013113021850585938 	 0.00023984909057617188 	 
2025-08-06 19:02:47.852090 test begin: paddle.view(Tensor([100, 10, 10, 50804],"float32"), list[10,100,-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 10, 10, 50804],"float32"), list[10,100,-1,], ) 	 508040000 	 750201 	 10.381742477416992 	 2.940826416015625 	 0.00018477439880371094 	 0.00011181831359863281 	 31.702372789382935 	 39.83842444419861 	 0.00011610984802246094 	 0.0005052089691162109 	 
2025-08-06 19:04:32.450650 test begin: paddle.view(Tensor([100, 10, 25402, 20],"float32"), list[-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 10, 25402, 20],"float32"), list[-1,], ) 	 508040000 	 750201 	 14.183847188949585 	 2.8774948120117188 	 0.0001385211944580078 	 0.00012111663818359375 	 31.608839511871338 	 38.94741249084473 	 0.00010061264038085938 	 0.00022482872009277344 	 
2025-08-06 19:06:19.907076 test begin: paddle.view(Tensor([100, 10, 25402, 20],"float32"), list[10,100,-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 10, 25402, 20],"float32"), list[10,100,-1,], ) 	 508040000 	 750201 	 10.434223175048828 	 2.8801064491271973 	 0.0001239776611328125 	 8.368492126464844e-05 	 31.646196603775024 	 39.97893023490906 	 0.00010347366333007812 	 0.0002346038818359375 	 
2025-08-06 19:08:02.253842 test begin: paddle.view(Tensor([100, 25402, 10, 20],"float32"), list[-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 25402, 10, 20],"float32"), list[-1,], ) 	 508040000 	 750201 	 13.361814498901367 	 2.933933973312378 	 0.00012135505676269531 	 0.00024700164794921875 	 32.001659631729126 	 39.37592554092407 	 0.00010442733764648438 	 0.00010251998901367188 	 
2025-08-06 19:09:48.168864 test begin: paddle.view(Tensor([100, 25402, 10, 20],"float32"), list[10,100,-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([100, 25402, 10, 20],"float32"), list[10,100,-1,], ) 	 508040000 	 750201 	 10.306463479995728 	 2.9552271366119385 	 0.000118255615234375 	 0.00027680397033691406 	 32.93800735473633 	 39.88201022148132 	 0.00010561943054199219 	 0.000240325927734375 	 
2025-08-06 19:11:31.430936 test begin: paddle.view(Tensor([254020, 10, 10, 20],"float32"), list[-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([254020, 10, 10, 20],"float32"), list[-1,], ) 	 508040000 	 750201 	 10.10326075553894 	 2.945009708404541 	 0.00011110305786132812 	 0.00023245811462402344 	 32.21592140197754 	 39.55978560447693 	 0.00010561943054199219 	 0.0002288818359375 	 
2025-08-06 19:13:13.266646 test begin: paddle.view(Tensor([254020, 10, 10, 20],"float32"), list[10,100,-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([254020, 10, 10, 20],"float32"), list[10,100,-1,], ) 	 508040000 	 750201 	 10.412086248397827 	 2.9820053577423096 	 0.00011038780212402344 	 8.96453857421875e-05 	 33.96200609207153 	 40.478538036346436 	 0.00011324882507324219 	 0.00022721290588378906 	 
2025-08-06 19:14:59.479093 test begin: paddle.view_as(Tensor([10, 10, 10, 50804],"float32"), Tensor([10, 100, 50804],"float32"), )
[Prof] paddle.view_as 	 paddle.view_as(Tensor([10, 10, 10, 50804],"float32"), Tensor([10, 100, 50804],"float32"), ) 	 101608000 	 711815 	 10.093636274337769 	 2.3481435775756836 	 0.00012302398681640625 	 0.00010919570922851562 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-08-06 19:15:44.795090 test begin: paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[-1,1,3,], )
W0806 19:16:13.279592 71862 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[-1,1,3,], ) 	 254016120 	 588277 	 18.005611419677734 	 8.557996273040771 	 0.00011515617370605469 	 0.00028324127197265625 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 19:16:24.665273 test begin: paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[-1,], )
W0806 19:16:48.242785 74843 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[-1,], ) 	 254016120 	 588277 	 16.333673000335693 	 6.899362325668335 	 0.0001251697540283203 	 9.059906005859375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 19:16:57.018165 test begin: paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[2,4,], )
W0806 19:17:17.800280 77189 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([21168010, 4, 3],"int64"), list[2,4,], ) 	 254016120 	 588277 	 13.719866275787354 	 4.653831720352173 	 0.00013685226440429688 	 0.00021719932556152344 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 19:17:24.245443 test begin: paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[-1,1,3,], )
W0806 19:17:55.649327 79350 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[-1,1,3,], ) 	 254016180 	 588277 	 20.686959266662598 	 5.4852941036224365 	 0.00011801719665527344 	 0.0003056526184082031 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 19:18:03.196442 test begin: paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[-1,], )
W0806 19:18:22.031347 82340 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[-1,], ) 	 254016180 	 588277 	 9.890355825424194 	 3.9684107303619385 	 0.00012135505676269531 	 0.0002639293670654297 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 19:18:28.034176 test begin: paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[2,4,], )
W0806 19:18:50.422575 84208 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 1411201, 3],"int64"), list[2,4,], ) 	 254016180 	 588277 	 15.475672483444214 	 4.578192234039307 	 0.00012564659118652344 	 0.00011396408081054688 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 19:18:56.591015 test begin: paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[-1,1,3,], )
W0806 19:19:25.118770 86089 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[-1,1,3,], ) 	 254016240 	 588277 	 17.873881340026855 	 5.361548662185669 	 0.00022077560424804688 	 0.0008692741394042969 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 19:19:32.607286 test begin: paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[-1,], )
W0806 19:19:49.457113 88987 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[-1,], ) 	 254016240 	 588277 	 9.82933259010315 	 3.9082024097442627 	 0.0001442432403564453 	 0.00011873245239257812 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 19:19:54.867433 test begin: paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[2,4,], )
W0806 19:20:15.541170 90643 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

[Prof] paddle.vsplit 	 paddle.vsplit(Tensor([60, 4, 1058401],"int64"), list[2,4,], ) 	 254016240 	 588277 	 13.680443286895752 	 4.723470687866211 	 0.00026607513427734375 	 0.0002570152282714844 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-08-06 19:20:21.789877 test begin: paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], ) 	 76204872 	 32416 	 30.54186701774597 	 29.93843102455139 	 0.16028141975402832 	 0.9472627639770508 	 30.64280414581299 	 2.4809768199920654 	 0.16076898574829102 	 0.00010013580322265625 	 
2025-08-06 19:21:58.784139 test begin: paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], name=None, ) 	 76204872 	 32416 	 30.553752422332764 	 29.952614784240723 	 0.16028547286987305 	 0.9445006847381592 	 30.634297370910645 	 2.3205313682556152 	 0.16080427169799805 	 0.00018930435180664062 	 
2025-08-06 19:23:37.334697 test begin: paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),], ) 	 25401624 	 32416 	 10.202031373977661 	 10.15693998336792 	 0.1605827808380127 	 0.15991663932800293 	 9.990895509719849 	 1.7532119750976562 	 0.15729713439941406 	 7.891654968261719e-05 	 
2025-08-06 19:24:10.574241 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401880 	 32416 	 10.439921379089355 	 10.487121343612671 	 0.08230042457580566 	 0.3304424285888672 	 10.420345783233643 	 2.2446353435516357 	 0.08217167854309082 	 0.0001010894775390625 	 
2025-08-06 19:24:45.388609 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], name=None, ) 	 25401880 	 32416 	 10.441056728363037 	 10.498871088027954 	 0.08229994773864746 	 0.3316488265991211 	 10.413329839706421 	 2.2449452877044678 	 0.0821688175201416 	 7.772445678710938e-05 	 
2025-08-06 19:25:20.133940 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401880 	 32416 	 10.484960794448853 	 10.399105787277222 	 0.0823824405670166 	 0.32726597785949707 	 10.488310813903809 	 2.2668886184692383 	 0.08371305465698242 	 0.0002052783966064453 	 
2025-08-06 19:25:54.950670 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], name=None, ) 	 25401880 	 32416 	 10.478832006454468 	 10.410869598388672 	 0.0823209285736084 	 0.32723546028137207 	 10.488468170166016 	 2.247577428817749 	 0.08236885070800781 	 0.00019741058349609375 	 
2025-08-06 19:26:29.744902 test begin: paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 76204980 	 32416 	 30.828076601028442 	 29.82998490333557 	 0.16176056861877441 	 0.9436407089233398 	 30.82807183265686 	 2.2223665714263916 	 0.16317391395568848 	 9.942054748535156e-05 	 
2025-08-06 19:28:06.876006 test begin: paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], name=None, ) 	 76204980 	 32416 	 30.82727074623108 	 29.82349395751953 	 0.16178584098815918 	 0.9435210227966309 	 30.830471992492676 	 2.2966415882110596 	 0.16173529624938965 	 7.867813110351562e-05 	 
2025-08-06 19:29:44.164398 test begin: paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401660 	 32416 	 10.252640724182129 	 10.1574227809906 	 0.16135716438293457 	 0.15989899635314941 	 10.219847917556763 	 1.7807281017303467 	 0.16085219383239746 	 0.00021719932556152344 	 
2025-08-06 19:30:17.724672 test begin: paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 76204890 	 32416 	 30.58367943763733 	 30.16742706298828 	 0.16210079193115234 	 0.9530432224273682 	 30.576690673828125 	 2.3107900619506836 	 0.16042685508728027 	 0.0001621246337890625 	 
2025-08-06 19:31:54.750714 test begin: paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], name=None, ) 	 76204890 	 32416 	 30.59868335723877 	 30.18267512321472 	 0.16052865982055664 	 0.9514696598052979 	 30.571953535079956 	 2.258380651473999 	 0.1604318618774414 	 7.724761962890625e-05 	 
2025-08-06 19:33:31.842286 test begin: paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401630 	 32416 	 10.199544429779053 	 10.182491302490234 	 0.160597562789917 	 0.1611640453338623 	 9.986561298370361 	 1.8034288883209229 	 0.1572856903076172 	 0.0002040863037109375 	 
2025-08-06 19:34:05.377627 test begin: paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401880 	 32416 	 10.414050102233887 	 10.083370208740234 	 0.08179879188537598 	 0.317669153213501 	 10.372507810592651 	 2.275139331817627 	 0.08143830299377441 	 7.295608520507812e-05 	 
2025-08-06 19:34:39.777214 test begin: paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], name=None, ) 	 25401880 	 32416 	 10.414782524108887 	 10.086632251739502 	 0.08322358131408691 	 0.3176603317260742 	 10.363959789276123 	 2.354745864868164 	 0.08144116401672363 	 7.128715515136719e-05 	 
2025-08-06 19:35:14.234776 test begin: paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 76204920 	 32416 	 30.743210554122925 	 29.950233936309814 	 0.1627333164215088 	 0.9430298805236816 	 30.709482192993164 	 2.335618019104004 	 0.1611328125 	 9.489059448242188e-05 	 
2025-08-06 19:36:51.348861 test begin: paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], name=None, ) 	 76204920 	 32416 	 30.741391897201538 	 29.961135864257812 	 0.16128849983215332 	 0.946000337600708 	 30.712034463882446 	 2.242328405380249 	 0.16111993789672852 	 0.0001971721649169922 	 
2025-08-06 19:38:28.430723 test begin: paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401640 	 32416 	 10.247278928756714 	 10.181145191192627 	 0.16133570671081543 	 0.15990996360778809 	 10.220907211303711 	 1.8193345069885254 	 0.16085028648376465 	 0.00021576881408691406 	 
2025-08-06 19:39:02.098471 test begin: paddle.where(Tensor([1, 400, 127009],"bool"), Tensor([1, 400, 127009],"float32"), Tensor([1, 400, 127009],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([1, 400, 127009],"bool"), Tensor([1, 400, 127009],"float32"), Tensor([1, 400, 127009],"float32"), ) 	 152410800 	 20615 	 10.007415294647217 	 9.99369215965271 	 0.4970822334289551 	 0.4947359561920166 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:39:39.592183 test begin: paddle.where(Tensor([1, 400, 65856],"bool"), Tensor([1, 400, 65856],"float32"), Tensor([2, 400, 65856],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([1, 400, 65856],"bool"), Tensor([1, 400, 65856],"float32"), Tensor([2, 400, 65856],"float32"), ) 	 105369600 	 20615 	 19.29660391807556 	 10.646736860275269 	 0.3186798095703125 	 0.5272140502929688 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:40:29.754427 test begin: paddle.where(Tensor([1, 400, 65856],"bool"), Tensor([2, 400, 65856],"float32"), Tensor([1, 400, 65856],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([1, 400, 65856],"bool"), Tensor([2, 400, 65856],"float32"), Tensor([1, 400, 65856],"float32"), ) 	 105369600 	 20615 	 19.294912576675415 	 10.67341423034668 	 0.3199775218963623 	 0.5276157855987549 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:41:21.490904 test begin: paddle.where(Tensor([1, 772, 65856],"bool"), Tensor([1, 772, 65856],"float32"), Tensor([1, 772, 65856],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([1, 772, 65856],"bool"), Tensor([1, 772, 65856],"float32"), Tensor([1, 772, 65856],"float32"), ) 	 152522496 	 20615 	 10.019660234451294 	 9.965808391571045 	 0.4957160949707031 	 0.493349552154541 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:41:55.494655 test begin: paddle.where(Tensor([2, 400, 65856],"bool"), Tensor([1, 400, 65856],"float32"), Tensor([1, 400, 65856],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([2, 400, 65856],"bool"), Tensor([1, 400, 65856],"float32"), Tensor([1, 400, 65856],"float32"), ) 	 105369600 	 20615 	 23.028743743896484 	 10.65828824043274 	 0.38160228729248047 	 0.527472972869873 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:42:55.854140 test begin: paddle.where(Tensor([2, 400, 65856],"bool"), Tensor([2, 400, 65856],"float32"), Tensor([2, 400, 65856],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([2, 400, 65856],"bool"), Tensor([2, 400, 65856],"float32"), Tensor([2, 400, 65856],"float32"), ) 	 158054400 	 20615 	 10.37302041053772 	 10.328442811965942 	 0.513556957244873 	 0.5124268531799316 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:43:31.220570 test begin: paddle.where(Tensor([4, 125, 320, 320],"bool"), Tensor([4, 125, 320, 320],"float32"), Tensor([4, 125, 320, 320],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 125, 320, 320],"bool"), Tensor([4, 125, 320, 320],"float32"), Tensor([4, 125, 320, 320],"float32"), ) 	 153600000 	 20615 	 10.087669372558594 	 10.033997058868408 	 0.5007011890411377 	 0.49675416946411133 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:44:05.560604 test begin: paddle.where(Tensor([4, 280, 376, 25, 5],"bool"), Tensor([4, 280, 376, 25, 5],"float32"), Tensor([4, 280, 376, 25, 5],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 280, 376, 25, 5],"bool"), Tensor([4, 280, 376, 25, 5],"float32"), Tensor([4, 280, 376, 25, 5],"float32"), ) 	 157920000 	 20615 	 10.370737552642822 	 10.322172164916992 	 0.5163431167602539 	 0.5107834339141846 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:44:40.848775 test begin: paddle.where(Tensor([4, 280, 376, 41, 3],"bool"), Tensor([4, 280, 376, 41, 3],"float32"), Tensor([4, 280, 376, 41, 3],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 280, 376, 41, 3],"bool"), Tensor([4, 280, 376, 41, 3],"float32"), Tensor([4, 280, 376, 41, 3],"float32"), ) 	 155393280 	 20615 	 10.190154790878296 	 10.16905927658081 	 0.5047097206115723 	 0.5049211978912354 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:45:15.522495 test begin: paddle.where(Tensor([4, 280, 605, 25, 3],"bool"), Tensor([4, 280, 605, 25, 3],"float32"), Tensor([4, 280, 605, 25, 3],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 280, 605, 25, 3],"bool"), Tensor([4, 280, 605, 25, 3],"float32"), Tensor([4, 280, 605, 25, 3],"float32"), ) 	 152460000 	 20615 	 10.009371757507324 	 10.013972520828247 	 0.4954254627227783 	 0.4964320659637451 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:45:50.482446 test begin: paddle.where(Tensor([4, 451, 376, 25, 3],"bool"), Tensor([4, 451, 376, 25, 3],"float32"), Tensor([4, 451, 376, 25, 3],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 451, 376, 25, 3],"bool"), Tensor([4, 451, 376, 25, 3],"float32"), Tensor([4, 451, 376, 25, 3],"float32"), ) 	 152618400 	 20615 	 10.014723062515259 	 9.976711988449097 	 0.4959697723388672 	 0.4938933849334717 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:46:24.632566 test begin: paddle.where(Tensor([4, 64, 320, 621],"bool"), Tensor([4, 64, 320, 621],"float32"), Tensor([4, 64, 320, 621],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 64, 320, 621],"bool"), Tensor([4, 64, 320, 621],"float32"), Tensor([4, 64, 320, 621],"float32"), ) 	 152616960 	 20615 	 10.022128820419312 	 9.97071385383606 	 0.49595141410827637 	 0.49364185333251953 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:46:58.881183 test begin: paddle.where(Tensor([4, 64, 621, 320],"bool"), Tensor([4, 64, 621, 320],"float32"), Tensor([4, 64, 621, 320],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 64, 621, 320],"bool"), Tensor([4, 64, 621, 320],"float32"), Tensor([4, 64, 621, 320],"float32"), ) 	 152616960 	 20615 	 10.01891803741455 	 9.980013608932495 	 0.49611735343933105 	 0.493682861328125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:47:33.019051 test begin: paddle.where(Tensor([7, 280, 376, 25, 3],"bool"), Tensor([7, 280, 376, 25, 3],"float32"), Tensor([7, 280, 376, 25, 3],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([7, 280, 376, 25, 3],"bool"), Tensor([7, 280, 376, 25, 3],"float32"), Tensor([7, 280, 376, 25, 3],"float32"), ) 	 165816000 	 20615 	 10.876770734786987 	 10.83194875717163 	 0.5389413833618164 	 0.5363459587097168 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:48:11.371846 test begin: paddle.where(Tensor([8, 64, 320, 320],"bool"), Tensor([8, 64, 320, 320],"float32"), Tensor([8, 64, 320, 320],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([8, 64, 320, 320],"bool"), Tensor([8, 64, 320, 320],"float32"), Tensor([8, 64, 320, 320],"float32"), ) 	 157286400 	 20615 	 10.32788896560669 	 11.652112245559692 	 0.511995792388916 	 0.5100893974304199 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-08-06 19:48:50.091636 test begin: paddle.zeros_like(Tensor([16, 64, 320, 320],"float16"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([16, 64, 320, 320],"float16"), ) 	 104857600 	 74631 	 10.319405317306519 	 10.31617021560669 	 0.14106273651123047 	 0.14118337631225586 	 None 	 None 	 None 	 None 	 
2025-08-06 19:49:12.815369 test begin: paddle.zeros_like(Tensor([4, 1051, 12096],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 1051, 12096],"float32"), ) 	 50851584 	 74631 	 10.025518417358398 	 10.024863004684448 	 0.13697576522827148 	 0.13832592964172363 	 None 	 None 	 None 	 None 	 
2025-08-06 19:49:33.777100 test begin: paddle.zeros_like(Tensor([4, 125, 320, 320],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 125, 320, 320],"float32"), ) 	 51200000 	 74631 	 10.083703994750977 	 10.105105876922607 	 0.1378004550933838 	 0.13823723793029785 	 None 	 None 	 None 	 None 	 
2025-08-06 19:49:54.905669 test begin: paddle.zeros_like(Tensor([4, 249, 320, 320],"float16"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 249, 320, 320],"float16"), ) 	 101990400 	 74631 	 10.040488243103027 	 10.040094137191772 	 0.13723468780517578 	 0.1371605396270752 	 None 	 None 	 None 	 None 	 
2025-08-06 19:50:16.894402 test begin: paddle.zeros_like(Tensor([4, 525, 24193],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 525, 24193],"float32"), ) 	 50805300 	 74631 	 10.012159585952759 	 10.026785850524902 	 0.13798856735229492 	 0.1370089054107666 	 None 	 None 	 None 	 None 	 
2025-08-06 19:50:40.354756 test begin: paddle.zeros_like(Tensor([4, 64, 1241, 320],"float16"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 64, 1241, 320],"float16"), ) 	 101662720 	 74631 	 10.00679063796997 	 10.00858211517334 	 0.13681483268737793 	 0.13675832748413086 	 None 	 None 	 None 	 None 	 
2025-08-06 19:51:02.342056 test begin: paddle.zeros_like(Tensor([4, 64, 320, 1241],"float16"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 64, 320, 1241],"float16"), ) 	 101662720 	 74631 	 10.009411573410034 	 10.009828329086304 	 0.1367964744567871 	 0.13675975799560547 	 None 	 None 	 None 	 None 	 
2025-08-06 19:51:24.404234 test begin: paddle.zeros_like(Tensor([4, 64, 320, 621],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 64, 320, 621],"float32"), ) 	 50872320 	 74631 	 10.022372484207153 	 10.041670083999634 	 0.13704490661621094 	 0.13707685470581055 	 None 	 None 	 None 	 None 	 
2025-08-06 19:51:45.394806 test begin: paddle.zeros_like(Tensor([4, 64, 621, 320],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 64, 621, 320],"float32"), ) 	 50872320 	 74631 	 10.024592399597168 	 10.030356645584106 	 0.1383960247039795 	 0.13709497451782227 	 None 	 None 	 None 	 None 	 
2025-08-06 19:52:06.453540 test begin: paddle.zeros_like(Tensor([8, 64, 320, 320],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([8, 64, 320, 320],"float32"), ) 	 52428800 	 74631 	 10.326119422912598 	 10.329623699188232 	 0.14116549491882324 	 0.14118742942810059 	 None 	 None 	 None 	 None 	 
2025-08-06 19:52:28.196770 test begin: paddle.zeros_like(Tensor([9, 525, 12096],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([9, 525, 12096],"float32"), ) 	 57153600 	 74631 	 11.232893943786621 	 11.254464387893677 	 0.1535320281982422 	 0.1536722183227539 	 None 	 None 	 None 	 None 	 
