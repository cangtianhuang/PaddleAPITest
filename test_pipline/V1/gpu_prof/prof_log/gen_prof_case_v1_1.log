2025-07-24 15:39:41.474069 test begin: paddle.Tensor.__abs__(Tensor([10, 5080321],"float32"), )
W0724 15:39:42.428753 107507 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.__abs__ 	 paddle.Tensor.__abs__(Tensor([10, 5080321],"float32"), ) 	 50803210 	 1000 	 0.29619336128234863 	 0.2992737293243408 	 0.2867922782897949 	 0.28457045555114746 	 0.45051121711730957 	 0.7429518699645996 	 0.3942413330078125 	 0.37953686714172363 	 
2025-07-24 15:39:45.578447 test begin: paddle.Tensor.__abs__(Tensor([49613, 1024],"float32"), )
[Prof] paddle.Tensor.__abs__ 	 paddle.Tensor.__abs__(Tensor([49613, 1024],"float32"), ) 	 50803712 	 1000 	 0.29692721366882324 	 0.30071067810058594 	 0.2871541976928711 	 0.28525614738464355 	 0.4503931999206543 	 0.7428719997406006 	 0.3966054916381836 	 0.3795609474182129 	 
2025-07-24 15:39:49.013656 test begin: paddle.Tensor.__abs__(Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.__abs__ 	 paddle.Tensor.__abs__(Tensor([50803201],"float32"), ) 	 50803201 	 1000 	 0.296130895614624 	 0.2978346347808838 	 0.2868375778198242 	 0.2850673198699951 	 0.450366735458374 	 0.742865800857544 	 0.396742582321167 	 0.3795459270477295 	 
2025-07-24 15:39:52.442883 test begin: paddle.Tensor.__add__(Tensor([1, 32, 388, 4096],"float32"), Tensor([1, 1, 388, 4096],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([1, 32, 388, 4096],"float32"), Tensor([1, 1, 388, 4096],"float32"), ) 	 52445184 	 1000 	 0.33673763275146484 	 0.3259119987487793 	 0.32665491104125977 	 0.31070685386657715 	 0.4766223430633545 	 0.15390634536743164 	 0.2434546947479248 	 0.08449077606201172 	 
2025-07-24 15:39:55.473482 test begin: paddle.Tensor.__add__(Tensor([1, 32, 4096, 388],"float32"), Tensor([1, 1, 4096, 388],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([1, 32, 4096, 388],"float32"), Tensor([1, 1, 4096, 388],"float32"), ) 	 52445184 	 1000 	 0.3358938694000244 	 0.3260197639465332 	 0.3258671760559082 	 0.31136059761047363 	 0.47655344009399414 	 0.15374255180358887 	 0.2434701919555664 	 0.08407449722290039 	 
2025-07-24 15:39:58.440519 test begin: paddle.Tensor.__add__(Tensor([1, 32, 4096, 4096],"float32"), Tensor([1, 1, 4096, 4096],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([1, 32, 4096, 4096],"float32"), Tensor([1, 1, 4096, 4096],"float32"), ) 	 553648128 	 1000 	 4.693381071090698 	 4.625363826751709 	 4.682830095291138 	 4.612291574478149 	 4.965935707092285 	 1.560021162033081 	 1.690131664276123 	 1.4886970520019531 	 
2025-07-24 15:40:32.407435 test begin: paddle.Tensor.__add__(Tensor([1, 32, 4096, 4096],"float32"), Tensor([4, 1, 4096, 4096],"float32"), )
[Error] CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 39.39 GiB of which 160.31 MiB is free. Process 60070 has 39.23 GiB memory in use. Of the allocated memory 18.25 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-07-24 15:41:54.068769 test begin: paddle.Tensor.__add__(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 1, 4096, 4096],"float32"), )
W0724 15:41:55.512619 109766 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 1, 4096, 4096],"float32"), ) 	 83886080 	 1000 	 0.5932102203369141 	 0.5839738845825195 	 0.5824503898620605 	 0.5700118541717529 	 0.6646015644073486 	 0.24418067932128906 	 0.33952784538269043 	 0.14778757095336914 	 
2025-07-24 15:41:59.270125 test begin: paddle.Tensor.__add__(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 4, 4096, 4096],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 4, 4096, 4096],"float32"), ) 	 134217728 	 1000 	 0.5935406684875488 	 0.5883996486663818 	 0.583810567855835 	 0.5769557952880859 	 0.6371467113494873 	 0.05436849594116211 	 0.5770337581634521 	 4.506111145019531e-05 	 
2025-07-24 15:42:04.556097 test begin: paddle.Tensor.__add__(Tensor([2, 256, 336, 336],"float16"), Tensor([2, 256, 336, 336],"float32"), )
W0724 15:42:06.468204 109871 dygraph_functions.cc:87088] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([2, 256, 336, 336],"float16"), Tensor([2, 256, 336, 336],"float32"), ) 	 115605504 	 1000 	 0.7835841178894043 	 0.4688739776611328 	 0.4003620147705078 	 0.4542992115020752 	 0.8034014701843262 	 0.2602577209472656 	 0.41043615341186523 	 0.19060015678405762 	 
2025-07-24 15:42:09.875458 test begin: paddle.Tensor.__add__(Tensor([2, 256, 336, 336],"float32"), Tensor([2, 256, 336, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([2, 256, 336, 336],"float32"), Tensor([2, 256, 336, 336],"float32"), ) 	 115605504 	 1000 	 0.5200114250183105 	 0.5077126026153564 	 0.5021820068359375 	 0.4952402114868164 	 0.5490553379058838 	 0.0551302433013916 	 0.489438533782959 	 7.772445678710938e-05 	 
2025-07-24 15:42:17.894721 test begin: paddle.Tensor.__add__(Tensor([4, 256, 336, 336],"float16"), Tensor([4, 256, 336, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([4, 256, 336, 336],"float16"), Tensor([4, 256, 336, 336],"float32"), ) 	 231211008 	 1000 	 1.5617573261260986 	 0.9267973899841309 	 0.7980546951293945 	 0.9150528907775879 	 1.6002144813537598 	 0.5153014659881592 	 0.8176696300506592 	 0.4467136859893799 	 
2025-07-24 15:42:29.040544 test begin: paddle.Tensor.__add__(Tensor([8, 113, 336, 336],"float16"), Tensor([8, 113, 336, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([8, 113, 336, 336],"float16"), Tensor([8, 113, 336, 336],"float32"), ) 	 204115968 	 1000 	 1.3799121379852295 	 0.8304305076599121 	 0.7051129341125488 	 0.8065965175628662 	 1.414351224899292 	 0.4555842876434326 	 0.7226135730743408 	 0.37909579277038574 	 
2025-07-24 15:42:41.134921 test begin: paddle.Tensor.__add__(Tensor([8, 256, 336, 74],"float32"), Tensor([8, 256, 336, 74],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([8, 256, 336, 74],"float32"), Tensor([8, 256, 336, 74],"float32"), ) 	 101842944 	 1000 	 0.45143604278564453 	 0.44760894775390625 	 0.44164514541625977 	 0.43593263626098633 	 0.48398303985595703 	 0.05431199073791504 	 0.42620396614074707 	 5.269050598144531e-05 	 
2025-07-24 15:42:45.164202 test begin: paddle.Tensor.__add__(Tensor([8, 256, 74, 336],"float32"), Tensor([8, 256, 74, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([8, 256, 74, 336],"float32"), Tensor([8, 256, 74, 336],"float32"), ) 	 101842944 	 1000 	 0.4514760971069336 	 0.44768571853637695 	 0.4418513774871826 	 0.43605875968933105 	 0.48395562171936035 	 0.05369257926940918 	 0.4248077869415283 	 4.220008850097656e-05 	 
2025-07-24 15:42:49.063023 test begin: paddle.Tensor.__add__(Tensor([8, 57, 336, 336],"float16"), Tensor([8, 57, 336, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([8, 57, 336, 336],"float16"), Tensor([8, 57, 336, 336],"float32"), ) 	 102961152 	 1000 	 0.7008645534515381 	 0.4161863327026367 	 0.3571028709411621 	 0.40397191047668457 	 0.7161154747009277 	 0.2322709560394287 	 0.36586427688598633 	 0.16297197341918945 	 
2025-07-24 15:42:53.857002 test begin: paddle.Tensor.__add__(Tensor([8, 57, 336, 336],"float32"), Tensor([8, 57, 336, 336],"float32"), )
[Prof] paddle.Tensor.__add__ 	 paddle.Tensor.__add__(Tensor([8, 57, 336, 336],"float32"), Tensor([8, 57, 336, 336],"float32"), ) 	 102961152 	 1000 	 0.45646095275878906 	 0.4527089595794678 	 0.44671082496643066 	 0.44040632247924805 	 0.48923158645629883 	 0.06450724601745605 	 0.4316704273223877 	 5.340576171875e-05 	 
2025-07-24 15:42:57.971747 test begin: paddle.Tensor.__and__(Tensor([1, 1, 2048, 2048],"bool"), Tensor([1, 13, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 1, 2048, 2048],"bool"), Tensor([1, 13, 2048, 2048],"bool"), ) 	 58720256 	 1000 	 0.16089272499084473 	 0.22648334503173828 	 0.1509566307067871 	 0.21284031867980957 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:59.200252 test begin: paddle.Tensor.__and__(Tensor([1, 1, 2048, 2048],"bool"), Tensor([13, 1, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 1, 2048, 2048],"bool"), Tensor([13, 1, 2048, 2048],"bool"), ) 	 58720256 	 1000 	 0.16082310676574707 	 0.2265148162841797 	 0.15108442306518555 	 0.21404552459716797 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:00.403149 test begin: paddle.Tensor.__and__(Tensor([1, 1, 2048, 24807],"bool"), Tensor([1, 1, 2048, 24807],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 1, 2048, 24807],"bool"), Tensor([1, 1, 2048, 24807],"bool"), ) 	 101609472 	 1000 	 0.11753082275390625 	 0.11547017097473145 	 0.10869050025939941 	 0.10338234901428223 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:02.118920 test begin: paddle.Tensor.__and__(Tensor([1, 1, 24807, 2048],"bool"), Tensor([1, 1, 24807, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 1, 24807, 2048],"bool"), Tensor([1, 1, 24807, 2048],"bool"), ) 	 101609472 	 1000 	 0.11750316619873047 	 0.11543679237365723 	 0.1087179183959961 	 0.10344481468200684 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:03.815379 test begin: paddle.Tensor.__and__(Tensor([1, 13, 2048, 2048],"bool"), Tensor([1, 1, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 13, 2048, 2048],"bool"), Tensor([1, 1, 2048, 2048],"bool"), ) 	 58720256 	 1000 	 0.1802220344543457 	 0.22642254829406738 	 0.17036986351013184 	 0.21378731727600098 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:05.024825 test begin: paddle.Tensor.__and__(Tensor([1, 13, 2048, 2048],"bool"), Tensor([1, 13, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([1, 13, 2048, 2048],"bool"), Tensor([1, 13, 2048, 2048],"bool"), ) 	 109051904 	 1000 	 0.12617707252502441 	 0.12332773208618164 	 0.11731338500976562 	 0.11154770851135254 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:06.777428 test begin: paddle.Tensor.__and__(Tensor([13, 1, 1007, 1007],"bool"), Tensor([13, 4, 1007, 1007],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 1, 1007, 1007],"bool"), Tensor([13, 4, 1007, 1007],"bool"), ) 	 65913185 	 1000 	 0.18441987037658691 	 0.23859238624572754 	 0.17461180686950684 	 0.22553777694702148 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:08.119614 test begin: paddle.Tensor.__and__(Tensor([13, 1, 1007, 3881],"bool"), Tensor([13, 1, 1007, 3881],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 1, 1007, 3881],"bool"), Tensor([13, 1, 1007, 3881],"bool"), ) 	 101612342 	 1000 	 0.11775851249694824 	 0.1158747673034668 	 0.1088259220123291 	 0.10378074645996094 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:09.812832 test begin: paddle.Tensor.__and__(Tensor([13, 1, 2048, 2048],"bool"), Tensor([1, 1, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 1, 2048, 2048],"bool"), Tensor([1, 1, 2048, 2048],"bool"), ) 	 58720256 	 1000 	 0.18021154403686523 	 0.2264385223388672 	 0.17047786712646484 	 0.2137901782989502 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:11.066719 test begin: paddle.Tensor.__and__(Tensor([13, 1, 2048, 2048],"bool"), Tensor([13, 1, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 1, 2048, 2048],"bool"), Tensor([13, 1, 2048, 2048],"bool"), ) 	 109051904 	 1000 	 0.12616968154907227 	 0.1233375072479248 	 0.11727404594421387 	 0.11162781715393066 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:12.826994 test begin: paddle.Tensor.__and__(Tensor([13, 1, 3881, 1007],"bool"), Tensor([13, 1, 3881, 1007],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 1, 3881, 1007],"bool"), Tensor([13, 1, 3881, 1007],"bool"), ) 	 101612342 	 1000 	 0.11774492263793945 	 0.11994218826293945 	 0.10886740684509277 	 0.10356497764587402 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:14.485978 test begin: paddle.Tensor.__and__(Tensor([13, 4, 1007, 1007],"bool"), Tensor([13, 1, 1007, 1007],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 4, 1007, 1007],"bool"), Tensor([13, 1, 1007, 1007],"bool"), ) 	 65913185 	 1000 	 0.19745993614196777 	 0.23853492736816406 	 0.1876053810119629 	 0.22542452812194824 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:15.836123 test begin: paddle.Tensor.__and__(Tensor([13, 4, 1007, 1007],"bool"), Tensor([13, 4, 1007, 1007],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([13, 4, 1007, 1007],"bool"), Tensor([13, 4, 1007, 1007],"bool"), ) 	 105461096 	 1000 	 0.25353193283081055 	 0.12884187698364258 	 0.1138923168182373 	 0.10376453399658203 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:18.795838 test begin: paddle.Tensor.__and__(Tensor([194, 1, 512, 512],"bool"), Tensor([194, 1, 512, 512],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([194, 1, 512, 512],"bool"), Tensor([194, 1, 512, 512],"bool"), ) 	 101711872 	 1000 	 0.1176605224609375 	 0.1181800365447998 	 0.10889792442321777 	 0.10347867012023926 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:21.314240 test begin: paddle.Tensor.__and__(Tensor([51, 1, 1007, 1007],"bool"), Tensor([51, 1, 1007, 1007],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([51, 1, 1007, 1007],"bool"), Tensor([51, 1, 1007, 1007],"bool"), ) 	 103432998 	 1000 	 0.1202688217163086 	 0.11858463287353516 	 0.11131834983825684 	 0.1058797836303711 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:23.011365 test begin: paddle.Tensor.__and__(Tensor([8, 1, 12404, 512],"bool"), Tensor([8, 1, 12404, 512],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([8, 1, 12404, 512],"bool"), Tensor([8, 1, 12404, 512],"bool"), ) 	 101613568 	 1000 	 0.11752104759216309 	 0.11546468734741211 	 0.1086740493774414 	 0.10333943367004395 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:24.633479 test begin: paddle.Tensor.__and__(Tensor([8, 1, 512, 12404],"bool"), Tensor([8, 1, 512, 12404],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([8, 1, 512, 12404],"bool"), Tensor([8, 1, 512, 12404],"bool"), ) 	 101613568 	 1000 	 0.11756157875061035 	 0.11544013023376465 	 0.10872125625610352 	 0.10336923599243164 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:26.314367 test begin: paddle.Tensor.__and__(Tensor([8, 1, 512, 512],"bool"), Tensor([8, 25, 512, 512],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([8, 1, 512, 512],"bool"), Tensor([8, 25, 512, 512],"bool"), ) 	 54525952 	 1000 	 0.18313193321228027 	 0.23529839515686035 	 0.17339324951171875 	 0.22243356704711914 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:27.478841 test begin: paddle.Tensor.__and__(Tensor([8, 25, 512, 512],"bool"), Tensor([8, 1, 512, 512],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([8, 25, 512, 512],"bool"), Tensor([8, 1, 512, 512],"bool"), ) 	 54525952 	 1000 	 0.19311738014221191 	 0.23532581329345703 	 0.18329977989196777 	 0.2226872444152832 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:28.650417 test begin: paddle.Tensor.__and__(Tensor([8, 25, 512, 512],"bool"), Tensor([8, 25, 512, 512],"bool"), )
[Prof] paddle.Tensor.__and__ 	 paddle.Tensor.__and__(Tensor([8, 25, 512, 512],"bool"), Tensor([8, 25, 512, 512],"bool"), ) 	 104857600 	 1000 	 0.12135863304138184 	 0.11886811256408691 	 0.11250758171081543 	 0.1070089340209961 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:30.320648 test begin: paddle.Tensor.__div__(Tensor([8, 16, 396901],"float32"), 2, )
[Prof] paddle.Tensor.__div__ 	 paddle.Tensor.__div__(Tensor([8, 16, 396901],"float32"), 2, ) 	 50803328 	 1000 	 0.2959566116333008 	 0.29784607887268066 	 0.2867581844329834 	 0.2839646339416504 	 0.29620933532714844 	 0.29762887954711914 	 0.24135136604309082 	 0.23500323295593262 	 
2025-07-24 15:43:33.204649 test begin: paddle.Tensor.__div__(Tensor([8, 198451, 32],"float32"), 2, )
[Prof] paddle.Tensor.__div__ 	 paddle.Tensor.__div__(Tensor([8, 198451, 32],"float32"), 2, ) 	 50803456 	 1000 	 0.3116581439971924 	 1.040346384048462 	 0.28685688972473145 	 0.2831413745880127 	 0.2962315082550049 	 0.2976069450378418 	 0.24058866500854492 	 0.23466992378234863 	 
2025-07-24 15:43:39.754002 test begin: paddle.Tensor.__div__(Tensor([99226, 16, 32],"float32"), 2, )
[Prof] paddle.Tensor.__div__ 	 paddle.Tensor.__div__(Tensor([99226, 16, 32],"float32"), 2, ) 	 50803712 	 1000 	 0.29738473892211914 	 0.2978098392486572 	 0.28688812255859375 	 0.2839679718017578 	 0.2962055206298828 	 0.2976562976837158 	 0.2454969882965088 	 0.23558664321899414 	 
2025-07-24 15:43:42.661470 test begin: paddle.Tensor.__eq__(Tensor([138, 369303],"float32"), Tensor([138, 1],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([138, 369303],"float32"), Tensor([138, 1],"float32"), ) 	 50963952 	 1000 	 0.19211769104003906 	 0.24204230308532715 	 0.1821906566619873 	 0.22954344749450684 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:43.961070 test begin: paddle.Tensor.__eq__(Tensor([146, 349866],"float32"), Tensor([146, 1],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([146, 349866],"float32"), Tensor([146, 1],"float32"), ) 	 51080582 	 1000 	 0.19473481178283691 	 0.24261784553527832 	 0.18257522583007812 	 0.2301170825958252 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:45.225631 test begin: paddle.Tensor.__eq__(Tensor([49, 1036801],"float32"), Tensor([49, 1036801],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([49, 1036801],"float32"), Tensor([49, 1036801],"float32"), ) 	 101606498 	 1000 	 0.3272416591644287 	 0.3278357982635498 	 0.31820201873779297 	 0.3162353038787842 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:47.567618 test begin: paddle.Tensor.__eq__(Tensor([49, 1036801],"float32"), Tensor([49, 1],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([49, 1036801],"float32"), Tensor([49, 1],"float32"), ) 	 50803298 	 1000 	 0.19138741493225098 	 0.24120187759399414 	 0.1808772087097168 	 0.2256627082824707 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:48.819353 test begin: paddle.Tensor.__eq__(Tensor([53, 958551],"float32"), Tensor([53, 1],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([53, 958551],"float32"), Tensor([53, 1],"float32"), ) 	 50803256 	 1000 	 0.1906595230102539 	 0.24121809005737305 	 0.18084168434143066 	 0.22786808013916016 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:50.068870 test begin: paddle.Tensor.__eq__(Tensor([53, 958551],"float32"), Tensor([53, 958551],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([53, 958551],"float32"), Tensor([53, 958551],"float32"), ) 	 101606406 	 1000 	 0.32742762565612793 	 0.3277931213378906 	 0.31828999519348145 	 0.31634950637817383 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:52.359499 test begin: paddle.Tensor.__eq__(Tensor([55, 923695],"float32"), Tensor([55, 1],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([55, 923695],"float32"), Tensor([55, 1],"float32"), ) 	 50803280 	 1000 	 0.19129562377929688 	 0.24129462242126465 	 0.18082976341247559 	 0.22851324081420898 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:53.605698 test begin: paddle.Tensor.__eq__(Tensor([55, 923695],"float32"), Tensor([55, 923695],"float32"), )
[Prof] paddle.Tensor.__eq__ 	 paddle.Tensor.__eq__(Tensor([55, 923695],"float32"), Tensor([55, 923695],"float32"), ) 	 101606450 	 1000 	 0.3273792266845703 	 0.3277759552001953 	 0.3182792663574219 	 0.316272497177124 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:55.937623 test begin: paddle.Tensor.__floordiv__(Tensor([10, 10160641],"float32"), Tensor([10, 10160641],"float16"), )
W0724 15:43:59.302189 111195 dygraph_functions.cc:89596] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([10, 10160641],"float32"), Tensor([10, 10160641],"float16"), ) 	 203212820 	 1000 	 1.3732833862304688 	 1.1909501552581787 	 0.7017180919647217 	 1.171367883682251 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:02.122980 test begin: paddle.Tensor.__floordiv__(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), ) 	 50803220 	 1000 	 0.4486117362976074 	 0.4478325843811035 	 0.4395577907562256 	 0.4313924312591553 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:03.842307 test begin: paddle.Tensor.__floordiv__(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float16"), )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float16"), ) 	 101606420 	 1000 	 0.6909000873565674 	 0.5991170406341553 	 0.35285139083862305 	 0.5820636749267578 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:06.915875 test begin: paddle.Tensor.__floordiv__(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), ) 	 50804736 	 1000 	 0.4483778476715088 	 0.447751522064209 	 0.439450740814209 	 0.43131184577941895 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:08.679805 test begin: paddle.Tensor.__floordiv__(Tensor([4, 6350401],"int64"), 4, )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([4, 6350401],"int64"), 4, ) 	 25401604 	 1000 	 0.30599117279052734 	 0.3023357391357422 	 0.1557331085205078 	 0.2849247455596924 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:09.702095 test begin: paddle.Tensor.__floordiv__(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float16"), )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float16"), ) 	 101607424 	 1000 	 0.6907789707183838 	 0.599107027053833 	 0.3530247211456299 	 0.5818488597869873 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:12.762232 test begin: paddle.Tensor.__floordiv__(Tensor([84673, 300],"int64"), 4, )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([84673, 300],"int64"), 4, ) 	 25401900 	 1000 	 0.3053898811340332 	 0.30253124237060547 	 0.15566110610961914 	 0.285259485244751 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:13.838843 test begin: paddle.Tensor.__floordiv__(Tensor([99226, 1024],"float32"), Tensor([99226, 1024],"float16"), )
[Prof] paddle.Tensor.__floordiv__ 	 paddle.Tensor.__floordiv__(Tensor([99226, 1024],"float32"), Tensor([99226, 1024],"float16"), ) 	 203214848 	 1000 	 1.3735806941986084 	 1.1902000904083252 	 0.7019801139831543 	 1.173419713973999 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:19.957084 test begin: paddle.Tensor.__ge__(Tensor([50803201],"int32"), 0, )
[Prof] paddle.Tensor.__ge__ 	 paddle.Tensor.__ge__(Tensor([50803201],"int32"), 0, ) 	 50803201 	 1000 	 0.47009968757629395 	 0.18775653839111328 	 0.2398674488067627 	 0.16717743873596191 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:21.219100 test begin: paddle.Tensor.__getitem__(Tensor([7576, 11, 1280],"bfloat16"), slice(None,-3,None), )
W0724 15:44:25.678946 111335 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 426511360, memory's size is 213340160.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):426511360 > memory_size():213340160.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

2025-07-24 15:44:25.736630 test begin: paddle.Tensor.__getitem__(Tensor([7576, 8, 1677],"bfloat16"), slice(None,-3,None), )
W0724 15:44:28.771155 111347 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 406397472, memory's size is 203279360.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):406397472 > memory_size():203279360.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

2025-07-24 15:44:28.812475 test begin: paddle.Tensor.__getitem__(Tensor([7712, 11, 1280],"bfloat16"), slice(None,-2,None), )
W0724 15:44:32.142678 111366 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 434227200, memory's size is 217169920.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):434227200 > memory_size():217169920.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

2025-07-24 15:44:32.190605 test begin: paddle.Tensor.__getitem__(Tensor([7712, 8, 1647],"bfloat16"), slice(None,-2,None), )
W0724 15:44:36.363688 111406 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 406347840, memory's size is 203226624.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):406347840 > memory_size():203226624.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

2025-07-24 15:44:37.143271 test begin: paddle.Tensor.__getitem__(Tensor([8168, 10, 1280],"bfloat16"), slice(None,-6,None), )
W0724 15:44:40.677914 111441 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 417894400, memory's size is 209100800.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):417894400 > memory_size():209100800.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

2025-07-24 15:44:40.722348 test begin: paddle.Tensor.__getitem__(Tensor([8168, 8, 1555],"bfloat16"), slice(None,-6,None), )
W0724 15:44:43.731042 111451 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 406141120, memory's size is 203219968.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):406141120 > memory_size():203219968.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

2025-07-24 15:44:43.772348 test begin: paddle.Tensor.__getitem__(Tensor([9923, 8, 1280],"bfloat16"), slice(None,-2,None), )
W0724 15:44:46.880060 111464 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 406364160, memory's size is 203223040.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):406364160 > memory_size():203223040.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

2025-07-24 15:44:46.934138 test begin: paddle.Tensor.__getitem__(Tensor([9923, 8, 1280],"bfloat16"), slice(None,-3,None), )
W0724 15:44:49.960670 111473 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 406323200, memory's size is 203223040.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):406323200 > memory_size():203223040.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

2025-07-24 15:44:50.005737 test begin: paddle.Tensor.__getitem__(Tensor([9923, 8, 1280],"bfloat16"), slice(None,-6,None), )
W0724 15:44:53.102203 111486 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 406200320, memory's size is 203223040.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):406200320 > memory_size():203223040.] (at ../paddle/phi/core/dense_tensor_impl.cc:48)

2025-07-24 15:44:53.145950 test begin: paddle.Tensor.__gt__(Tensor([1, 400, 127009],"float32"), 0, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([1, 400, 127009],"float32"), 0, ) 	 50803600 	 1000 	 0.469998836517334 	 0.18608331680297852 	 0.23976850509643555 	 0.17178726196289062 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:54.679475 test begin: paddle.Tensor.__gt__(Tensor([1, 400, 127009],"float32"), 1e-09, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([1, 400, 127009],"float32"), 1e-09, ) 	 50803600 	 1000 	 0.46940064430236816 	 0.18608617782592773 	 0.23987317085266113 	 0.17166423797607422 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:56.153490 test begin: paddle.Tensor.__gt__(Tensor([1, 772, 65856],"float32"), 0, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([1, 772, 65856],"float32"), 0, ) 	 50840832 	 1000 	 0.46969032287597656 	 0.1861405372619629 	 0.2399430274963379 	 0.17213749885559082 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:57.662549 test begin: paddle.Tensor.__gt__(Tensor([1, 772, 65856],"float32"), 1e-09, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([1, 772, 65856],"float32"), 1e-09, ) 	 50840832 	 1000 	 0.46979856491088867 	 0.18616867065429688 	 0.2400355339050293 	 0.1713564395904541 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:59.140472 test begin: paddle.Tensor.__gt__(Tensor([2, 400, 65856],"float32"), 0, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([2, 400, 65856],"float32"), 0, ) 	 52684800 	 1000 	 0.4859919548034668 	 0.19262933731079102 	 0.24829530715942383 	 0.17842507362365723 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:00.692293 test begin: paddle.Tensor.__gt__(Tensor([2, 400, 65856],"float32"), 1e-09, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([2, 400, 65856],"float32"), 1e-09, ) 	 52684800 	 1000 	 0.48604393005371094 	 0.19347691535949707 	 0.2483663558959961 	 0.17840814590454102 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:02.247820 test begin: paddle.Tensor.__gt__(Tensor([324000, 157],"float32"), 0.0, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([324000, 157],"float32"), 0.0, ) 	 50868000 	 1000 	 0.46992993354797363 	 0.1862187385559082 	 0.24010038375854492 	 0.1719052791595459 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:03.788470 test begin: paddle.Tensor.__gt__(Tensor([635041, 80],"float32"), 0.0, )
[Prof] paddle.Tensor.__gt__ 	 paddle.Tensor.__gt__(Tensor([635041, 80],"float32"), 0.0, ) 	 50803280 	 1000 	 0.4693570137023926 	 0.18599867820739746 	 0.23987627029418945 	 0.1718733310699463 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:05.269512 test begin: paddle.Tensor.__le__(Tensor([243360, 209],"float32"), 0.0, )
[Prof] paddle.Tensor.__le__ 	 paddle.Tensor.__le__(Tensor([243360, 209],"float32"), 0.0, ) 	 50862240 	 1000 	 0.4698450565338135 	 0.18620824813842773 	 0.24006056785583496 	 0.17203116416931152 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:06.755880 test begin: paddle.Tensor.__le__(Tensor([282240, 181],"float32"), 0.0, )
[Prof] paddle.Tensor.__le__ 	 paddle.Tensor.__le__(Tensor([282240, 181],"float32"), 0.0, ) 	 51085440 	 1000 	 0.47182345390319824 	 0.18700504302978516 	 0.24112510681152344 	 0.17289519309997559 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:08.268061 test begin: paddle.Tensor.__le__(Tensor([324000, 157],"float32"), 0.0, )
[Prof] paddle.Tensor.__le__ 	 paddle.Tensor.__le__(Tensor([324000, 157],"float32"), 0.0, ) 	 50868000 	 1000 	 0.4699885845184326 	 0.18621277809143066 	 0.24013066291809082 	 0.17198753356933594 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:09.743841 test begin: paddle.Tensor.__le__(Tensor([635041, 80],"float32"), 0.0, )
[Prof] paddle.Tensor.__le__ 	 paddle.Tensor.__le__(Tensor([635041, 80],"float32"), 0.0, ) 	 50803280 	 1000 	 0.4693171977996826 	 0.1875467300415039 	 0.2398087978363037 	 0.17169618606567383 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:11.215372 test begin: paddle.Tensor.__len__(Tensor([100, 1352, 376],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([100, 1352, 376],"float32"), ) 	 50835200 	 1000 	 0.004549264907836914 	 0.0049440860748291016 	 6.198883056640625e-06 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:12.042197 test begin: paddle.Tensor.__len__(Tensor([100, 376, 1352],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([100, 376, 1352],"float32"), ) 	 50835200 	 1000 	 0.0046231746673583984 	 0.004798173904418945 	 5.9604644775390625e-06 	 1.6689300537109375e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:12.885583 test begin: paddle.Tensor.__len__(Tensor([100000, 509],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([100000, 509],"float32"), ) 	 50900000 	 1000 	 0.004530668258666992 	 0.004859209060668945 	 5.9604644775390625e-06 	 1.6927719116210938e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:13.712401 test begin: paddle.Tensor.__len__(Tensor([23, 1501, 1501],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([23, 1501, 1501],"float32"), ) 	 51819023 	 1000 	 0.004650115966796875 	 0.0048558712005615234 	 6.4373016357421875e-06 	 1.6689300537109375e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:14.553286 test begin: paddle.Tensor.__len__(Tensor([360, 376, 376],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([360, 376, 376],"float32"), ) 	 50895360 	 1000 	 0.004635334014892578 	 0.0049173831939697266 	 5.4836273193359375e-06 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:15.380045 test begin: paddle.Tensor.__len__(Tensor([50, 1501, 677],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([50, 1501, 677],"float32"), ) 	 50808850 	 1000 	 0.004613399505615234 	 0.004883289337158203 	 5.9604644775390625e-06 	 1.7642974853515625e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:16.214023 test begin: paddle.Tensor.__len__(Tensor([50, 677, 1501],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([50, 677, 1501],"float32"), ) 	 50808850 	 1000 	 0.0045812129974365234 	 0.004873991012573242 	 5.7220458984375e-06 	 1.71661376953125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:17.038831 test begin: paddle.Tensor.__len__(Tensor([508033, 100],"float32"), )
[Prof] paddle.Tensor.__len__ 	 paddle.Tensor.__len__(Tensor([508033, 100],"float32"), ) 	 50803300 	 1000 	 0.0045702457427978516 	 0.0048062801361083984 	 6.198883056640625e-06 	 1.6927719116210938e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:17.866098 test begin: paddle.Tensor.__lshift__(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), ) 	 101607000 	 1000 	 0.4520559310913086 	 0.4468364715576172 	 0.44082212448120117 	 0.43454766273498535 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:19.955591 test begin: paddle.Tensor.__lshift__(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), ) 	 101606800 	 1000 	 0.4504556655883789 	 0.4466581344604492 	 0.44086599349975586 	 0.4344460964202881 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:22.024018 test begin: paddle.Tensor.__lshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), ) 	 203213200 	 1000 	 0.44970059394836426 	 0.4500694274902344 	 0.43904590606689453 	 0.437758207321167 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:24.945998 test begin: paddle.Tensor.__lshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, ) 	 203213200 	 1000 	 0.4513399600982666 	 0.946760892868042 	 0.43891310691833496 	 0.43755102157592773 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:30.877840 test begin: paddle.Tensor.__lshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), ) 	 203213400 	 1000 	 0.448199987411499 	 0.45009946823120117 	 0.43877625465393066 	 0.437824010848999 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:33.757982 test begin: paddle.Tensor.__lshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, )
[Prof] paddle.Tensor.__lshift__ 	 paddle.Tensor.__lshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, ) 	 203213400 	 1000 	 0.45456361770629883 	 0.6743068695068359 	 0.43869924545288086 	 0.4377467632293701 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:40.056210 test begin: paddle.Tensor.__lt__(Tensor([1034, 3, 64, 128],"float64"), 1, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([1034, 3, 64, 128],"float64"), 1, ) 	 25411584 	 1000 	 0.45511722564697266 	 0.17024445533752441 	 0.23122239112854004 	 0.1532890796661377 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:41.211316 test begin: paddle.Tensor.__lt__(Tensor([256, 13, 64, 128],"float64"), 1, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([256, 13, 64, 128],"float64"), 1, ) 	 27262976 	 1000 	 0.48473405838012695 	 0.18010640144348145 	 0.24737191200256348 	 0.1661233901977539 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:42.445958 test begin: paddle.Tensor.__lt__(Tensor([256, 3, 259, 128],"float64"), 1, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([256, 3, 259, 128],"float64"), 1, ) 	 25460736 	 1000 	 0.455031156539917 	 0.16859722137451172 	 0.2315962314605713 	 0.15037989616394043 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:43.620139 test begin: paddle.Tensor.__lt__(Tensor([256, 3, 64, 517],"float64"), 1, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([256, 3, 64, 517],"float64"), 1, ) 	 25411584 	 1000 	 0.45213913917541504 	 0.168318510055542 	 0.23101019859313965 	 0.15389418601989746 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:44.778786 test begin: paddle.Tensor.__lt__(Tensor([4, 157920, 81],"float32"), 0.1111111111111111, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([4, 157920, 81],"float32"), 0.1111111111111111, ) 	 51166080 	 1000 	 0.4723174571990967 	 0.1873326301574707 	 0.2413802146911621 	 0.1727910041809082 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:46.292253 test begin: paddle.Tensor.__lt__(Tensor([4, 1814401, 7],"float32"), 0.1111111111111111, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([4, 1814401, 7],"float32"), 0.1111111111111111, ) 	 50803228 	 1000 	 0.46927785873413086 	 0.18607330322265625 	 0.23976635932922363 	 0.16915416717529297 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:47.760337 test begin: paddle.Tensor.__lt__(Tensor([46, 157920, 7],"float32"), 0.1111111111111111, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([46, 157920, 7],"float32"), 0.1111111111111111, ) 	 50850240 	 1000 	 0.4696519374847412 	 0.18620967864990234 	 0.24000930786132812 	 0.17110776901245117 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:49.292556 test begin: paddle.Tensor.__lt__(Tensor([50803201],"float32"), 0.7, )
[Prof] paddle.Tensor.__lt__ 	 paddle.Tensor.__lt__(Tensor([50803201],"float32"), 0.7, ) 	 50803201 	 1000 	 0.4710707664489746 	 0.18772625923156738 	 0.23981976509094238 	 0.16793513298034668 	 None 	 None 	 None 	 None 	 
2025-07-24 15:45:50.825830 test begin: paddle.Tensor.__matmul__(Tensor([10, 2304, 2304],"float32"), Tensor([10, 2304, 64],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([10, 2304, 2304],"float32"), Tensor([10, 2304, 64],"float32"), ) 	 54558720 	 1000 	 0.9284586906433105 	 0.9288914203643799 	 0.9159324169158936 	 0.9055287837982178 	 1.3737096786499023 	 1.3737974166870117 	 0.7018911838531494 	 0.7018933296203613 	 
2025-07-24 15:45:56.499754 test begin: paddle.Tensor.__matmul__(Tensor([111, 3, 392, 392],"float32"), Tensor([111, 3, 392, 32],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([111, 3, 392, 392],"float32"), Tensor([111, 3, 392, 32],"float32"), ) 	 55347264 	 1000 	 1.0331661701202393 	 1.0332202911376953 	 1.0206272602081299 	 1.009948492050171 	 1.4488446712493896 	 1.4485235214233398 	 0.7402863502502441 	 0.7400965690612793 	 
2025-07-24 15:46:02.466218 test begin: paddle.Tensor.__matmul__(Tensor([1351, 3, 392, 392],"float32"), Tensor([1351, 3, 392, 32],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([1351, 3, 392, 392],"float32"), Tensor([1351, 3, 392, 32],"float32"), ) 	 673641024 	 1000 	 11.825479745864868 	 11.825567245483398 	 11.81283950805664 	 11.802210330963135 	 16.826679468154907 	 16.768463850021362 	 8.611204624176025 	 8.568681240081787 	 
2025-07-24 15:47:12.050295 test begin: paddle.Tensor.__matmul__(Tensor([176, 2, 392, 392],"float32"), Tensor([176, 2, 392, 32],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([176, 2, 392, 392],"float32"), Tensor([176, 2, 392, 32],"float32"), ) 	 58505216 	 1000 	 1.1031737327575684 	 1.1028449535369873 	 1.0906834602355957 	 1.0789854526519775 	 1.5422027111053467 	 1.54158616065979 	 0.7879717350006104 	 0.7877137660980225 	 
2025-07-24 15:47:18.434545 test begin: paddle.Tensor.__matmul__(Tensor([176, 24, 392, 392],"float32"), Tensor([176, 24, 392, 32],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([176, 24, 392, 392],"float32"), Tensor([176, 24, 392, 32],"float32"), ) 	 702062592 	 1000 	 12.375077486038208 	 12.378307580947876 	 12.36240005493164 	 12.350323677062988 	 17.532021045684814 	 17.531296730041504 	 8.958826065063477 	 8.958329916000366 	 
2025-07-24 15:48:31.213286 test begin: paddle.Tensor.__matmul__(Tensor([176, 3, 246, 392],"float32"), Tensor([176, 3, 392, 32],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([176, 3, 246, 392],"float32"), Tensor([176, 3, 392, 32],"float32"), ) 	 57539328 	 1000 	 0.7987585067749023 	 0.7986884117126465 	 0.786160945892334 	 0.7753064632415771 	 1.3481471538543701 	 1.348268985748291 	 0.6888227462768555 	 0.6888101100921631 	 
2025-07-24 15:48:39.545570 test begin: paddle.Tensor.__matmul__(Tensor([176, 3, 392, 392],"float32"), Tensor([176, 3, 392, 246],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([176, 3, 392, 392],"float32"), Tensor([176, 3, 392, 246],"float32"), ) 	 132050688 	 1000 	 3.1611993312835693 	 3.1632683277130127 	 3.1486432552337646 	 3.1389825344085693 	 7.15819525718689 	 7.159574270248413 	 3.658424139022827 	 3.6584508419036865 	 
2025-07-24 15:49:04.672865 test begin: paddle.Tensor.__matmul__(Tensor([345, 2304, 2304],"float32"), Tensor([345, 2304, 64],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([345, 2304, 2304],"float32"), Tensor([345, 2304, 64],"float32"), ) 	 1882275840 	 1000 	 26.624089002609253 	 26.622100114822388 	 26.601003885269165 	 26.59618902206421 	 41.71444058418274 	 41.71140432357788 	 21.316019773483276 	 21.3141667842865 	 
2025-07-24 15:51:58.441667 test begin: paddle.Tensor.__matmul__(Tensor([49, 1024, 1024],"float32"), Tensor([49, 1024, 64],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([49, 1024, 1024],"float32"), Tensor([49, 1024, 64],"float32"), ) 	 54591488 	 1000 	 0.8283777236938477 	 0.8285868167877197 	 0.8146722316741943 	 0.8033924102783203 	 1.2604255676269531 	 1.2608997821807861 	 0.6439683437347412 	 0.6441819667816162 	 
2025-07-24 15:52:03.780767 test begin: paddle.Tensor.__matmul__(Tensor([60, 2304, 2304],"float32"), Tensor([60, 2304, 368],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([60, 2304, 2304],"float32"), Tensor([60, 2304, 368],"float32"), ) 	 369377280 	 1000 	 13.775289058685303 	 13.777144193649292 	 13.762570142745972 	 13.749010801315308 	 27.19873023033142 	 27.20582342147827 	 13.898410320281982 	 13.901895523071289 	 
2025-07-24 15:53:33.107147 test begin: paddle.Tensor.__matmul__(Tensor([60, 368, 2304],"float32"), Tensor([60, 2304, 64],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([60, 368, 2304],"float32"), Tensor([60, 2304, 64],"float32"), ) 	 59719680 	 1000 	 0.9292070865631104 	 0.92946457862854 	 0.9164745807647705 	 0.9056596755981445 	 1.1969478130340576 	 1.1972148418426514 	 0.6117823123931885 	 0.61128830909729 	 
2025-07-24 15:53:40.811365 test begin: paddle.Tensor.__matmul__(Tensor([776, 1024, 1024],"float32"), Tensor([776, 1024, 64],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([776, 1024, 1024],"float32"), Tensor([776, 1024, 64],"float32"), ) 	 864550912 	 1000 	 11.866706609725952 	 11.866791009902954 	 11.853573083877563 	 11.842856884002686 	 18.527159929275513 	 18.518645524978638 	 9.467378616333008 	 9.462832689285278 	 
2025-07-24 15:54:57.976710 test begin: paddle.Tensor.__matmul__(Tensor([96, 1024, 1024],"float32"), Tensor([96, 1024, 517],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([96, 1024, 1024],"float32"), Tensor([96, 1024, 517],"float32"), ) 	 151486464 	 1000 	 7.371711015701294 	 7.37175726890564 	 7.357749700546265 	 7.3484766483306885 	 13.351900577545166 	 13.344341516494751 	 6.822727918624878 	 6.818826675415039 	 
2025-07-24 15:55:42.999161 test begin: paddle.Tensor.__matmul__(Tensor([96, 517, 1024],"float32"), Tensor([96, 1024, 64],"float32"), )
[Prof] paddle.Tensor.__matmul__ 	 paddle.Tensor.__matmul__(Tensor([96, 517, 1024],"float32"), Tensor([96, 1024, 64],"float32"), ) 	 57114624 	 1000 	 1.0318071842193604 	 1.0321125984191895 	 1.019240379333496 	 1.008047342300415 	 1.370225191116333 	 1.3705863952636719 	 0.7000977993011475 	 0.7002434730529785 	 
2025-07-24 15:55:48.829549 test begin: paddle.Tensor.__mod__(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), ) 	 50803220 	 1000 	 0.44766736030578613 	 0.44776248931884766 	 0.43812060356140137 	 0.436046838760376 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:55:51.729690 test begin: paddle.Tensor.__mod__(Tensor([13, 2, 976985],"int64"), 16, )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([13, 2, 976985],"int64"), 16, ) 	 25401610 	 1000 	 0.5830795764923096 	 0.2992420196533203 	 0.29793858528137207 	 0.2854146957397461 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:55:53.718745 test begin: paddle.Tensor.__mod__(Tensor([13, 30531, 64],"int64"), 16, )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([13, 30531, 64],"int64"), 16, ) 	 25401792 	 1000 	 0.5827760696411133 	 0.29914188385009766 	 0.29777050018310547 	 0.28513288497924805 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:55:55.728636 test begin: paddle.Tensor.__mod__(Tensor([198451, 2, 64],"int64"), 16, )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([198451, 2, 64],"int64"), 16, ) 	 25401728 	 1000 	 0.5829412937164307 	 0.2991774082183838 	 0.29787182807922363 	 0.28507566452026367 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:55:57.744255 test begin: paddle.Tensor.__mod__(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), ) 	 50804736 	 1000 	 0.44779133796691895 	 0.4476935863494873 	 0.43831706047058105 	 0.43544840812683105 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:56:00.629957 test begin: paddle.Tensor.__mod__(Tensor([26, 976985],"int64"), 64, )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([26, 976985],"int64"), 64, ) 	 25401610 	 1000 	 0.5830566883087158 	 0.2991914749145508 	 0.2979140281677246 	 0.2849924564361572 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:56:02.647667 test begin: paddle.Tensor.__mod__(Tensor([396901, 64],"int64"), 64, )
[Prof] paddle.Tensor.__mod__ 	 paddle.Tensor.__mod__(Tensor([396901, 64],"int64"), 64, ) 	 25401664 	 1000 	 0.5828802585601807 	 0.29915666580200195 	 0.29781103134155273 	 0.284747838973999 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:56:04.649315 test begin: paddle.Tensor.__mul__(Tensor([1, 1, 32768, 32768],"float16"), 10000.0, )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([1, 1, 32768, 32768],"float16"), 10000.0, ) 	 1073741824 	 1000 	 3.104703664779663 	 3.091792345046997 	 3.0936241149902344 	 3.076958417892456 	 3.1048195362091064 	 3.0911242961883545 	 3.053218126296997 	 3.0214927196502686 	 
2025-07-24 15:56:58.386427 test begin: paddle.Tensor.__mul__(Tensor([108544, 469],"float32"), Tensor([108544, 469],"float32"), )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([108544, 469],"float32"), Tensor([108544, 469],"float32"), ) 	 101814272 	 1000 	 0.4508993625640869 	 0.4476287364959717 	 0.44136667251586914 	 0.4359297752380371 	 1.1389071941375732 	 0.8948259353637695 	 1.0783426761627197 	 0.4571866989135742 	 
2025-07-24 15:57:03.896844 test begin: paddle.Tensor.__mul__(Tensor([111616, 456],"float32"), Tensor([111616, 456],"float32"), )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([111616, 456],"float32"), Tensor([111616, 456],"float32"), ) 	 101793792 	 1000 	 0.450960636138916 	 0.4475715160369873 	 0.44124650955200195 	 0.4356520175933838 	 1.138685941696167 	 0.894690990447998 	 1.0786285400390625 	 0.4571385383605957 	 
2025-07-24 15:57:09.404772 test begin: paddle.Tensor.__mul__(Tensor([14176, 3584],"float32"), Tensor([14176, 3584],"float32"), )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([14176, 3584],"float32"), Tensor([14176, 3584],"float32"), ) 	 101613568 	 1000 	 0.4499800205230713 	 0.44678592681884766 	 0.4402899742126465 	 0.4354991912841797 	 1.136648178100586 	 0.8930859565734863 	 1.0766429901123047 	 0.45622706413269043 	 
2025-07-24 15:57:14.851432 test begin: paddle.Tensor.__mul__(Tensor([2, 1, 1551, 32768],"float16"), 10000.0, )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([2, 1, 1551, 32768],"float16"), 10000.0, ) 	 101646336 	 1000 	 0.2983253002166748 	 0.29642462730407715 	 0.2887692451477051 	 0.2795145511627197 	 0.2983059883117676 	 0.2962954044342041 	 0.2464439868927002 	 0.2282087802886963 	 
2025-07-24 15:57:19.954273 test begin: paddle.Tensor.__mul__(Tensor([2, 1, 32768, 1551],"float16"), 10000.0, )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([2, 1, 32768, 1551],"float16"), 10000.0, ) 	 101646336 	 1000 	 0.2983386516571045 	 0.2963283061981201 	 0.2892608642578125 	 0.2822763919830322 	 0.29824304580688477 	 0.29634737968444824 	 0.22954916954040527 	 0.2268819808959961 	 
2025-07-24 15:57:24.974754 test begin: paddle.Tensor.__mul__(Tensor([2, 1, 32768, 32768],"float16"), 10000.0, )
[Prof] paddle.Tensor.__mul__ 	 paddle.Tensor.__mul__(Tensor([2, 1, 32768, 32768],"float16"), 10000.0, ) 	 2147483648 	 1000 	 6.206488370895386 	 6.1842591762542725 	 6.195741653442383 	 3.1596665382385254 	 6.20632266998291 	 6.183178901672363 	 6.154563665390015 	 3.159564256668091 	 
2025-07-24 15:59:12.345571 test begin: paddle.Tensor.__ne__(Tensor([144, 392, 901],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([144, 392, 901],"float32"), 0, ) 	 50859648 	 1000 	 0.47141456604003906 	 0.1862494945526123 	 0.24087214469909668 	 0.1696789264678955 	 None 	 None 	 None 	 None 	 
2025-07-24 15:59:13.846607 test begin: paddle.Tensor.__ne__(Tensor([144, 901, 392],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([144, 901, 392],"float32"), 0, ) 	 50859648 	 1000 	 0.47135472297668457 	 0.18622064590454102 	 0.24087095260620117 	 0.17201900482177734 	 None 	 None 	 None 	 None 	 
2025-07-24 15:59:15.371928 test begin: paddle.Tensor.__ne__(Tensor([160, 392, 811],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([160, 392, 811],"float32"), 0, ) 	 50865920 	 1000 	 0.4716014862060547 	 0.186248779296875 	 0.24096393585205078 	 0.17197847366333008 	 None 	 None 	 None 	 None 	 
2025-07-24 15:59:16.887110 test begin: paddle.Tensor.__ne__(Tensor([160, 811, 392],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([160, 811, 392],"float32"), 0, ) 	 50865920 	 1000 	 0.47170329093933105 	 0.18625140190124512 	 0.24100112915039062 	 0.1720445156097412 	 None 	 None 	 None 	 None 	 
2025-07-24 15:59:18.429726 test begin: paddle.Tensor.__ne__(Tensor([176, 392, 737],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([176, 392, 737],"float32"), 0, ) 	 50847104 	 1000 	 0.47149109840393066 	 0.18625593185424805 	 0.2409381866455078 	 0.16826367378234863 	 None 	 None 	 None 	 None 	 
2025-07-24 15:59:19.913874 test begin: paddle.Tensor.__ne__(Tensor([176, 737, 392],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([176, 737, 392],"float32"), 0, ) 	 50847104 	 1000 	 0.4715404510498047 	 0.18623614311218262 	 0.240952730178833 	 0.17200016975402832 	 None 	 None 	 None 	 None 	 
2025-07-24 15:59:21.401452 test begin: paddle.Tensor.__ne__(Tensor([331, 392, 392],"float32"), 0, )
[Prof] paddle.Tensor.__ne__ 	 paddle.Tensor.__ne__(Tensor([331, 392, 392],"float32"), 0, ) 	 50862784 	 1000 	 0.4715754985809326 	 0.18629693984985352 	 0.24096417427062988 	 0.17216277122497559 	 None 	 None 	 None 	 None 	 
2025-07-24 15:59:22.923311 test begin: paddle.Tensor.__neg__(Tensor([128, 396901],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([128, 396901],"float32"), ) 	 50803328 	 1000 	 0.29558420181274414 	 0.2979440689086914 	 0.2863743305206299 	 0.2875699996948242 	 0.29555845260620117 	 0.2977457046508789 	 0.24348950386047363 	 0.23330259323120117 	 
2025-07-24 15:59:25.807239 test begin: paddle.Tensor.__neg__(Tensor([128, 793801],"float16"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([128, 793801],"float16"), ) 	 101606528 	 1000 	 0.29869556427001953 	 0.2962534427642822 	 0.2895057201385498 	 0.28559136390686035 	 0.29856085777282715 	 0.29614686965942383 	 0.2457118034362793 	 0.23180198669433594 	 
2025-07-24 15:59:30.887305 test begin: paddle.Tensor.__neg__(Tensor([22, 81, 94, 311],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([22, 81, 94, 311],"float32"), ) 	 52094988 	 1000 	 0.30330538749694824 	 0.3054018020629883 	 0.29401063919067383 	 0.29447317123413086 	 0.3032102584838867 	 0.30513572692871094 	 0.2527942657470703 	 0.23027729988098145 	 
2025-07-24 15:59:33.816250 test begin: paddle.Tensor.__neg__(Tensor([264, 192612],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([264, 192612],"float32"), ) 	 50849568 	 1000 	 0.29602622985839844 	 0.30570507049560547 	 0.28669095039367676 	 0.28563570976257324 	 0.2959444522857666 	 0.29807162284851074 	 0.24518299102783203 	 0.23305988311767578 	 
2025-07-24 15:59:40.077265 test begin: paddle.Tensor.__neg__(Tensor([4, 435, 94, 311],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([4, 435, 94, 311],"float32"), ) 	 50867160 	 1000 	 0.2960352897644043 	 0.30339741706848145 	 0.2866816520690918 	 0.2877178192138672 	 0.29615259170532227 	 0.298250675201416 	 0.24575066566467285 	 0.2295217514038086 	 
2025-07-24 15:59:43.632557 test begin: paddle.Tensor.__neg__(Tensor([4, 81, 505, 311],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([4, 81, 505, 311],"float32"), ) 	 50885820 	 1000 	 0.29604101181030273 	 0.31861233711242676 	 0.2868022918701172 	 0.28760361671447754 	 0.29625916481018066 	 0.2982909679412842 	 0.24570465087890625 	 0.23177099227905273 	 
2025-07-24 15:59:46.492721 test begin: paddle.Tensor.__neg__(Tensor([4, 81, 94, 1669],"float32"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([4, 81, 94, 1669],"float32"), ) 	 50831064 	 1000 	 0.295926570892334 	 0.2980008125305176 	 0.2866196632385254 	 0.2878432273864746 	 0.29594969749450684 	 0.297971248626709 	 0.2449796199798584 	 0.23068499565124512 	 
2025-07-24 15:59:49.308604 test begin: paddle.Tensor.__neg__(Tensor([528, 192612],"float16"), )
[Prof] paddle.Tensor.__neg__ 	 paddle.Tensor.__neg__(Tensor([528, 192612],"float16"), ) 	 101699136 	 1000 	 0.29889392852783203 	 0.29643702507019043 	 0.28904008865356445 	 0.28621530532836914 	 0.29877448081970215 	 0.29638171195983887 	 0.24806666374206543 	 0.23193645477294922 	 
2025-07-24 15:59:54.315341 test begin: paddle.Tensor.__or__(Tensor([1, 210, 241921],"bool"), Tensor([1, 210, 241921],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 210, 241921],"bool"), Tensor([1, 210, 241921],"bool"), ) 	 101606820 	 1000 	 0.11800622940063477 	 0.1167449951171875 	 0.10947442054748535 	 0.10070276260375977 	 None 	 None 	 None 	 None 	 
2025-07-24 15:59:55.989376 test begin: paddle.Tensor.__or__(Tensor([1, 210, 75600],"bool"), Tensor([4, 210, 75600],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 210, 75600],"bool"), Tensor([4, 210, 75600],"bool"), ) 	 79380000 	 1000 	 0.16184616088867188 	 0.27808356285095215 	 0.1504509449005127 	 0.26540327072143555 	 None 	 None 	 None 	 None 	 
2025-07-24 15:59:57.556293 test begin: paddle.Tensor.__or__(Tensor([1, 218, 233043],"bool"), Tensor([1, 218, 233043],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 218, 233043],"bool"), Tensor([1, 218, 233043],"bool"), ) 	 101606748 	 1000 	 0.11801862716674805 	 0.11656880378723145 	 0.10887384414672852 	 0.10468530654907227 	 None 	 None 	 None 	 None 	 
2025-07-24 15:59:59.226244 test begin: paddle.Tensor.__or__(Tensor([1, 218, 70644],"bool"), Tensor([4, 218, 70644],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 218, 70644],"bool"), Tensor([4, 218, 70644],"bool"), ) 	 77001960 	 1000 	 0.15691208839416504 	 0.26931071281433105 	 0.1472630500793457 	 0.2566046714782715 	 None 	 None 	 None 	 None 	 
2025-07-24 16:00:00.797779 test begin: paddle.Tensor.__or__(Tensor([1, 400, 127009],"bool"), Tensor([1, 400, 127009],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 400, 127009],"bool"), Tensor([1, 400, 127009],"bool"), ) 	 101607200 	 1000 	 0.11718320846557617 	 0.11721968650817871 	 0.10848665237426758 	 0.10317420959472656 	 None 	 None 	 None 	 None 	 
2025-07-24 16:00:02.498849 test begin: paddle.Tensor.__or__(Tensor([1, 400, 65856],"bool"), Tensor([2, 400, 65856],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 400, 65856],"bool"), Tensor([2, 400, 65856],"bool"), ) 	 79027200 	 1000 	 0.13476133346557617 	 0.2317335605621338 	 0.1230003833770752 	 0.21912002563476562 	 None 	 None 	 None 	 None 	 
2025-07-24 16:00:04.016454 test begin: paddle.Tensor.__or__(Tensor([1, 673, 75600],"bool"), Tensor([1, 673, 75600],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 673, 75600],"bool"), Tensor([1, 673, 75600],"bool"), ) 	 101757600 	 1000 	 0.1175072193145752 	 0.11541533470153809 	 0.1089174747467041 	 0.10335135459899902 	 None 	 None 	 None 	 None 	 
2025-07-24 16:00:05.707206 test begin: paddle.Tensor.__or__(Tensor([1, 720, 70644],"bool"), Tensor([1, 720, 70644],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 720, 70644],"bool"), Tensor([1, 720, 70644],"bool"), ) 	 101727360 	 1000 	 0.11798667907714844 	 0.11872744560241699 	 0.10922765731811523 	 0.1035773754119873 	 None 	 None 	 None 	 None 	 
2025-07-24 16:00:07.400369 test begin: paddle.Tensor.__or__(Tensor([1, 772, 65856],"bool"), Tensor([1, 772, 65856],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([1, 772, 65856],"bool"), Tensor([1, 772, 65856],"bool"), ) 	 101681664 	 1000 	 0.11797833442687988 	 0.1151275634765625 	 0.10924720764160156 	 0.1031181812286377 	 None 	 None 	 None 	 None 	 
2025-07-24 16:00:09.063952 test begin: paddle.Tensor.__or__(Tensor([2, 400, 65856],"bool"), Tensor([1, 400, 65856],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([2, 400, 65856],"bool"), Tensor([1, 400, 65856],"bool"), ) 	 79027200 	 1000 	 0.1751546859741211 	 0.23164010047912598 	 0.16559052467346191 	 0.21889281272888184 	 None 	 None 	 None 	 None 	 
2025-07-24 16:00:10.624327 test begin: paddle.Tensor.__or__(Tensor([2, 400, 65856],"bool"), Tensor([2, 400, 65856],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([2, 400, 65856],"bool"), Tensor([2, 400, 65856],"bool"), ) 	 105369600 	 1000 	 0.12107706069946289 	 0.11894917488098145 	 0.11234045028686523 	 0.10687756538391113 	 None 	 None 	 None 	 None 	 
2025-07-24 16:00:12.325964 test begin: paddle.Tensor.__or__(Tensor([4, 210, 75600],"bool"), Tensor([1, 210, 75600],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([4, 210, 75600],"bool"), Tensor([1, 210, 75600],"bool"), ) 	 79380000 	 1000 	 0.21138525009155273 	 0.2780945301055908 	 0.20165753364562988 	 0.2654561996459961 	 None 	 None 	 None 	 None 	 
2025-07-24 16:00:13.927807 test begin: paddle.Tensor.__or__(Tensor([4, 210, 75600],"bool"), Tensor([4, 210, 75600],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([4, 210, 75600],"bool"), Tensor([4, 210, 75600],"bool"), ) 	 127008000 	 1000 	 0.14560866355895996 	 0.14751362800598145 	 0.1369009017944336 	 0.13100934028625488 	 None 	 None 	 None 	 None 	 
2025-07-24 16:00:16.010781 test begin: paddle.Tensor.__or__(Tensor([4, 218, 70644],"bool"), Tensor([1, 218, 70644],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([4, 218, 70644],"bool"), Tensor([1, 218, 70644],"bool"), ) 	 77001960 	 1000 	 0.20457148551940918 	 0.2691781520843506 	 0.19495892524719238 	 0.2565133571624756 	 None 	 None 	 None 	 None 	 
2025-07-24 16:00:17.554125 test begin: paddle.Tensor.__or__(Tensor([4, 218, 70644],"bool"), Tensor([4, 218, 70644],"bool"), )
[Prof] paddle.Tensor.__or__ 	 paddle.Tensor.__or__(Tensor([4, 218, 70644],"bool"), Tensor([4, 218, 70644],"bool"), ) 	 123203136 	 1000 	 0.140486478805542 	 0.14032769203186035 	 0.13176870346069336 	 0.12833380699157715 	 None 	 None 	 None 	 None 	 
2025-07-24 16:00:19.574521 test begin: paddle.Tensor.__pow__(Tensor([23, 17, 256, 256],"float64"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([23, 17, 256, 256],"float64"), 2, ) 	 25624576 	 1000 	 0.5772466659545898 	 0.3010261058807373 	 0.5682187080383301 	 0.28452301025390625 	 0.6093339920043945 	 1.0605792999267578 	 0.554236650466919 	 0.3612394332885742 	 
2025-07-24 16:00:23.271206 test begin: paddle.Tensor.__pow__(Tensor([24, 17, 244, 256],"float64"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([24, 17, 244, 256],"float64"), 2, ) 	 25485312 	 1000 	 0.5740945339202881 	 0.29933953285217285 	 0.5650510787963867 	 0.2825357913970947 	 0.6058509349822998 	 1.0548338890075684 	 0.5482988357543945 	 0.3593459129333496 	 
2025-07-24 16:00:26.962482 test begin: paddle.Tensor.__pow__(Tensor([24, 17, 256, 244],"float64"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([24, 17, 256, 244],"float64"), 2, ) 	 25485312 	 1000 	 0.5740876197814941 	 0.299358606338501 	 0.5650553703308105 	 0.28253936767578125 	 0.6059689521789551 	 1.0548601150512695 	 0.5538580417633057 	 0.3593173027038574 	 
2025-07-24 16:00:30.587949 test begin: paddle.Tensor.__pow__(Tensor([24, 17, 256, 256],"float64"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([24, 17, 256, 256],"float64"), 2, ) 	 26738688 	 1000 	 0.6021716594696045 	 0.31531453132629395 	 0.5926599502563477 	 0.29707980155944824 	 0.6357235908508301 	 1.1062023639678955 	 0.5791971683502197 	 0.37687158584594727 	 
2025-07-24 16:00:34.404000 test begin: paddle.Tensor.__pow__(Tensor([259, 3, 256, 256],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([259, 3, 256, 256],"float32"), 2, ) 	 50921472 	 1000 	 0.3695411682128906 	 0.3020751476287842 	 0.3603816032409668 	 0.2808990478515625 	 0.4532449245452881 	 1.0551745891571045 	 0.40038251876831055 	 0.3593740463256836 	 
2025-07-24 16:00:39.846061 test begin: paddle.Tensor.__pow__(Tensor([28, 32, 241, 241],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([28, 32, 241, 241],"float32"), 2, ) 	 52040576 	 1000 	 0.3774442672729492 	 0.3050227165222168 	 0.3682851791381836 	 0.2879772186279297 	 0.4624965190887451 	 1.0809309482574463 	 0.4087991714477539 	 0.27632856369018555 	 
2025-07-24 16:00:43.968947 test begin: paddle.Tensor.__pow__(Tensor([64, 13, 256, 256],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([64, 13, 256, 256],"float32"), 2, ) 	 54525952 	 1000 	 0.3955047130584717 	 0.9953839778900146 	 0.3863048553466797 	 0.29914188385009766 	 0.4846334457397461 	 1.1288988590240479 	 0.4316072463989258 	 0.38458251953125 	 
2025-07-24 16:00:52.008842 test begin: paddle.Tensor.__pow__(Tensor([64, 3, 1034, 256],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([64, 3, 1034, 256],"float32"), 2, ) 	 50823168 	 1000 	 0.3686065673828125 	 0.3028738498687744 	 0.3594093322753906 	 0.28095173835754395 	 0.4525315761566162 	 1.0526394844055176 	 0.3992443084716797 	 0.3585782051086426 	 
2025-07-24 16:00:55.907905 test begin: paddle.Tensor.__pow__(Tensor([64, 3, 256, 1034],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([64, 3, 256, 1034],"float32"), 2, ) 	 50823168 	 1000 	 0.36859130859375 	 0.2979280948638916 	 0.35939812660217285 	 0.2809123992919922 	 0.45252323150634766 	 1.0527698993682861 	 0.396618127822876 	 0.35871148109436035 	 
2025-07-24 16:00:59.863413 test begin: paddle.Tensor.__pow__(Tensor([8, 110, 241, 241],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([8, 110, 241, 241],"float32"), 2, ) 	 51111280 	 1000 	 0.3711061477661133 	 0.2995877265930176 	 0.36180663108825684 	 0.28252506256103516 	 0.4546949863433838 	 1.0617756843566895 	 0.3991813659667969 	 0.27146244049072266 	 
2025-07-24 16:01:03.833750 test begin: paddle.Tensor.__pow__(Tensor([8, 32, 241, 824],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([8, 32, 241, 824],"float32"), 2, ) 	 50837504 	 1000 	 0.3688521385192871 	 0.5235676765441895 	 0.35959291458129883 	 0.28110337257385254 	 0.45258116722106934 	 1.0544555187225342 	 0.3993978500366211 	 0.3587477207183838 	 
2025-07-24 16:01:10.724802 test begin: paddle.Tensor.__pow__(Tensor([8, 32, 824, 241],"float32"), 2, )
[Prof] paddle.Tensor.__pow__ 	 paddle.Tensor.__pow__(Tensor([8, 32, 824, 241],"float32"), 2, ) 	 50837504 	 1000 	 0.36870837211608887 	 0.2980220317840576 	 0.3594985008239746 	 0.28110289573669434 	 0.45255446434020996 	 1.053138017654419 	 0.3992290496826172 	 0.3587524890899658 	 
2025-07-24 16:01:14.667536 test begin: paddle.Tensor.__radd__(Tensor([192, 104, 32, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 104, 32, 160],"float16"), 0, ) 	 102236160 	 1000 	 0.300250768661499 	 0.29794788360595703 	 0.29114747047424316 	 0.2841184139251709 	 0.3000063896179199 	 0.05462050437927246 	 0.24884557723999023 	 4.9114227294921875e-05 	 
2025-07-24 16:01:19.533829 test begin: paddle.Tensor.__radd__(Tensor([192, 128, 16, 259],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 128, 16, 259],"float16"), 0, ) 	 101842944 	 1000 	 0.29871606826782227 	 0.29687047004699707 	 0.2892460823059082 	 0.28246545791625977 	 0.2989082336425781 	 0.053989410400390625 	 0.2454848289489746 	 5.650520324707031e-05 	 
2025-07-24 16:01:24.369728 test begin: paddle.Tensor.__radd__(Tensor([192, 128, 26, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 128, 26, 160],"float16"), 0, ) 	 102236160 	 1000 	 0.3002297878265381 	 0.2992243766784668 	 0.2911415100097656 	 0.28351545333862305 	 0.3000171184539795 	 0.05535578727722168 	 0.2487490177154541 	 4.7206878662109375e-05 	 
2025-07-24 16:01:29.319983 test begin: paddle.Tensor.__radd__(Tensor([192, 207, 16, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 207, 16, 160],"float16"), 0, ) 	 101744640 	 1000 	 0.29857635498046875 	 0.2965726852416992 	 0.28954219818115234 	 0.2821464538574219 	 0.29859042167663574 	 0.05709719657897949 	 0.23619556427001953 	 5.53131103515625e-05 	 
2025-07-24 16:01:34.159009 test begin: paddle.Tensor.__radd__(Tensor([192, 240, 16, 138],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 240, 16, 138],"float16"), 0, ) 	 101744640 	 1000 	 0.29860448837280273 	 0.30019307136535645 	 0.28941965103149414 	 0.27891039848327637 	 0.2985649108886719 	 0.05422544479370117 	 0.24639678001403809 	 6.604194641113281e-05 	 
2025-07-24 16:01:39.788632 test begin: paddle.Tensor.__radd__(Tensor([192, 240, 28, 80],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 240, 28, 80],"float16"), 0, ) 	 103219200 	 1000 	 0.3026857376098633 	 0.3007829189300537 	 0.29357218742370605 	 0.2862968444824219 	 0.3029439449310303 	 0.05421710014343262 	 0.23312973976135254 	 4.410743713378906e-05 	 
2025-07-24 16:01:44.680222 test begin: paddle.Tensor.__radd__(Tensor([192, 414, 16, 80],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 414, 16, 80],"float16"), 0, ) 	 101744640 	 1000 	 0.29861879348754883 	 0.29654788970947266 	 0.28957366943359375 	 0.28220248222351074 	 0.2986271381378174 	 0.05356264114379883 	 0.24784278869628906 	 5.078315734863281e-05 	 
2025-07-24 16:01:49.424288 test begin: paddle.Tensor.__radd__(Tensor([192, 64, 32, 259],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 64, 32, 259],"float16"), 0, ) 	 101842944 	 1000 	 0.29874467849731445 	 0.752307653427124 	 0.2897169589996338 	 0.28240537643432617 	 0.29894375801086426 	 0.05614614486694336 	 0.24805188179016113 	 6.461143493652344e-05 	 
2025-07-24 16:01:55.697534 test begin: paddle.Tensor.__radd__(Tensor([192, 64, 52, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([192, 64, 52, 160],"float16"), 0, ) 	 102236160 	 1000 	 0.3002443313598633 	 0.2979259490966797 	 0.29116272926330566 	 0.2840099334716797 	 0.3000636100769043 	 0.05371522903442383 	 0.24836969375610352 	 5.054473876953125e-05 	 
2025-07-24 16:02:00.525406 test begin: paddle.Tensor.__radd__(Tensor([311, 128, 16, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([311, 128, 16, 160],"float16"), 0, ) 	 101908480 	 1000 	 0.29886436462402344 	 0.29778099060058594 	 0.2898125648498535 	 0.28277063369750977 	 0.2992589473724365 	 0.053987979888916016 	 0.2442917823791504 	 3.838539123535156e-05 	 
2025-07-24 16:02:05.379278 test begin: paddle.Tensor.__radd__(Tensor([311, 64, 32, 160],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([311, 64, 32, 160],"float16"), 0, ) 	 101908480 	 1000 	 0.29885125160217285 	 0.29709291458129883 	 0.2897505760192871 	 0.2827644348144531 	 0.29918837547302246 	 0.06845307350158691 	 0.24799537658691406 	 5.364418029785156e-05 	 
2025-07-24 16:02:10.225434 test begin: paddle.Tensor.__radd__(Tensor([331, 240, 16, 80],"float16"), 0, )
[Prof] paddle.Tensor.__radd__ 	 paddle.Tensor.__radd__(Tensor([331, 240, 16, 80],"float16"), 0, ) 	 101683200 	 1000 	 0.298445463180542 	 0.29636144638061523 	 0.2893199920654297 	 0.2825322151184082 	 0.29836297035217285 	 0.056998491287231445 	 0.2473304271697998 	 7.987022399902344e-05 	 
2025-07-24 16:02:15.183664 test begin: paddle.Tensor.__rlshift__(Tensor([169345, 300],"int32"), -223, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([169345, 300],"int32"), -223, ) 	 50803500 	 1000 	 0.2987194061279297 	 0.3000454902648926 	 0.1525881290435791 	 0.2790095806121826 	 None 	 None 	 None 	 None 	 
2025-07-24 16:02:16.366482 test begin: paddle.Tensor.__rlshift__(Tensor([200, 254017],"int32"), -223, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([200, 254017],"int32"), -223, ) 	 50803400 	 1000 	 0.29868245124816895 	 0.297832727432251 	 0.15256023406982422 	 0.2784414291381836 	 None 	 None 	 None 	 None 	 
2025-07-24 16:02:17.550191 test begin: paddle.Tensor.__rlshift__(Tensor([200, 508033],"int16"), -212, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([200, 508033],"int16"), -212, ) 	 101606600 	 1000 	 0.336134672164917 	 0.29488563537597656 	 0.0003046989440917969 	 0.27216267585754395 	 None 	 None 	 None 	 None 	 
2025-07-24 16:02:19.188632 test begin: paddle.Tensor.__rlshift__(Tensor([200, 508033],"int16"), 63, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([200, 508033],"int16"), 63, ) 	 101606600 	 1000 	 0.3347172737121582 	 0.29483485221862793 	 0.0003066062927246094 	 0.2701718807220459 	 None 	 None 	 None 	 None 	 
2025-07-24 16:02:20.829257 test begin: paddle.Tensor.__rlshift__(Tensor([338689, 300],"int16"), -212, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([338689, 300],"int16"), -212, ) 	 101606700 	 1000 	 0.3344101905822754 	 0.2948496341705322 	 0.0003094673156738281 	 0.2753121852874756 	 None 	 None 	 None 	 None 	 
2025-07-24 16:02:22.449336 test begin: paddle.Tensor.__rlshift__(Tensor([338689, 300],"int16"), 63, )
[Prof] paddle.Tensor.__rlshift__ 	 paddle.Tensor.__rlshift__(Tensor([338689, 300],"int16"), 63, ) 	 101606700 	 1000 	 0.33472681045532227 	 0.2947967052459717 	 0.0002994537353515625 	 0.2759733200073242 	 None 	 None 	 None 	 None 	 
2025-07-24 16:02:24.077698 test begin: paddle.Tensor.__rmatmul__(Tensor([10160641, 5],"float32"), Tensor([2, 10160641],"float32"), )
[Prof] paddle.Tensor.__rmatmul__ 	 paddle.Tensor.__rmatmul__(Tensor([10160641, 5],"float32"), Tensor([2, 10160641],"float32"), ) 	 71124487 	 1000 	 1.3859837055206299 	 1.382225751876831 	 0.7081961631774902 	 0.7062909603118896 	 2.0917413234710693 	 2.0920372009277344 	 0.10684776306152344 	 0.1069183349609375 	 
2025-07-24 16:02:32.268905 test begin: paddle.Tensor.__rmatmul__(Tensor([25401601, 5],"float32"), Tensor([2, 25401601],"float32"), )
[Prof] paddle.Tensor.__rmatmul__ 	 paddle.Tensor.__rmatmul__(Tensor([25401601, 5],"float32"), Tensor([2, 25401601],"float32"), ) 	 177811207 	 1000 	 3.3989336490631104 	 3.397087335586548 	 1.7368874549865723 	 1.7359774112701416 	 5.23029351234436 	 5.228530406951904 	 0.10688376426696777 	 0.10681462287902832 	 
2025-07-24 16:02:52.606366 test begin: paddle.Tensor.__rmatmul__(Tensor([3, 16934401],"float32"), Tensor([2, 3],"float32"), )
[Prof] paddle.Tensor.__rmatmul__ 	 paddle.Tensor.__rmatmul__(Tensor([3, 16934401],"float32"), Tensor([2, 3],"float32"), ) 	 50803209 	 1000 	 0.7643301486968994 	 0.7750458717346191 	 0.04593014717102051 	 0.0458674430847168 	 4.096147775650024 	 4.093846797943115 	 0.21860861778259277 	 0.220611572265625 	 
2025-07-24 16:03:05.478295 test begin: paddle.Tensor.__rmatmul__(Tensor([3, 5],"float32"), Tensor([16934401, 3],"float32"), )
[Prof] paddle.Tensor.__rmatmul__ 	 paddle.Tensor.__rmatmul__(Tensor([3, 5],"float32"), Tensor([16934401, 3],"float32"), ) 	 50803218 	 1000 	 1.7828755378723145 	 1.7825241088867188 	 0.10711526870727539 	 0.10706949234008789 	 4.185535669326782 	 4.183047533035278 	 0.22554707527160645 	 0.22326302528381348 	 
2025-07-24 16:03:19.742371 test begin: paddle.Tensor.__rmod__(Tensor([2, 3, 8467201],"float32"), Tensor([2, 3, 8467201],"float32"), )
[Prof] paddle.Tensor.__rmod__ 	 paddle.Tensor.__rmod__(Tensor([2, 3, 8467201],"float32"), Tensor([2, 3, 8467201],"float32"), ) 	 101606412 	 1000 	 0.4504270553588867 	 0.4492528438568115 	 0.44056010246276855 	 0.4337284564971924 	 1.136134147644043 	 1.1973040103912354 	 1.076716661453247 	 0.40772485733032227 	 
2025-07-24 16:03:25.507246 test begin: paddle.Tensor.__rmod__(Tensor([2, 6350401, 4],"float32"), Tensor([2, 6350401, 4],"float32"), )
[Prof] paddle.Tensor.__rmod__ 	 paddle.Tensor.__rmod__(Tensor([2, 6350401, 4],"float32"), Tensor([2, 6350401, 4],"float32"), ) 	 101606416 	 1000 	 0.45050692558288574 	 0.44931793212890625 	 0.44083642959594727 	 0.4337902069091797 	 1.136049509048462 	 1.197333574295044 	 1.0753693580627441 	 0.4077906608581543 	 
2025-07-24 16:03:31.246225 test begin: paddle.Tensor.__rmod__(Tensor([4233601, 3, 4],"float32"), Tensor([4233601, 3, 4],"float32"), )
[Prof] paddle.Tensor.__rmod__ 	 paddle.Tensor.__rmod__(Tensor([4233601, 3, 4],"float32"), Tensor([4233601, 3, 4],"float32"), ) 	 101606424 	 1000 	 0.45038819313049316 	 0.4598379135131836 	 0.44069695472717285 	 0.43027734756469727 	 1.3164899349212646 	 1.197535753250122 	 1.256812572479248 	 0.40786290168762207 	 
2025-07-24 16:03:40.156878 test begin: paddle.Tensor.__rmul__(Tensor([176, 392, 737],"float32"), -100.0, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([176, 392, 737],"float32"), -100.0, ) 	 50847104 	 1000 	 0.2957742214202881 	 0.29808974266052246 	 0.2866818904876709 	 0.28433728218078613 	 0.2958087921142578 	 0.2980475425720215 	 0.24530482292175293 	 0.2309587001800537 	 
2025-07-24 16:03:43.024142 test begin: paddle.Tensor.__rmul__(Tensor([176, 737, 392],"float32"), -100.0, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([176, 737, 392],"float32"), -100.0, ) 	 50847104 	 1000 	 0.2956995964050293 	 0.29952073097229004 	 0.28673529624938965 	 0.2842733860015869 	 0.2958188056945801 	 0.2980329990386963 	 0.24505186080932617 	 0.231276273727417 	 
2025-07-24 16:03:45.899159 test begin: paddle.Tensor.__rmul__(Tensor([324000, 157],"float32"), 0.75, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([324000, 157],"float32"), 0.75, ) 	 50868000 	 1000 	 0.29602670669555664 	 0.30121684074401855 	 0.28649282455444336 	 0.28410816192626953 	 0.29616737365722656 	 0.298234224319458 	 0.24528741836547852 	 0.23079276084899902 	 
2025-07-24 16:03:48.735727 test begin: paddle.Tensor.__rmul__(Tensor([324000, 157],"float32"), 1.0, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([324000, 157],"float32"), 1.0, ) 	 50868000 	 1000 	 0.2960963249206543 	 0.3015749454498291 	 0.28696584701538086 	 0.28379130363464355 	 0.2961235046386719 	 0.2982046604156494 	 0.24515843391418457 	 0.2300119400024414 	 
2025-07-24 16:03:51.620203 test begin: paddle.Tensor.__rmul__(Tensor([331, 392, 392],"float32"), -100.0, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([331, 392, 392],"float32"), -100.0, ) 	 50862784 	 1000 	 0.29602646827697754 	 0.2981727123260498 	 0.28310132026672363 	 0.28407859802246094 	 0.29617857933044434 	 0.29816365242004395 	 0.24552106857299805 	 0.23031926155090332 	 
2025-07-24 16:03:54.512651 test begin: paddle.Tensor.__rmul__(Tensor([635041, 80],"float32"), 0.75, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([635041, 80],"float32"), 0.75, ) 	 50803280 	 1000 	 0.29561805725097656 	 0.29784178733825684 	 0.28653860092163086 	 0.28401827812194824 	 0.29561877250671387 	 0.2978212833404541 	 0.24019503593444824 	 0.23175549507141113 	 
2025-07-24 16:03:57.370512 test begin: paddle.Tensor.__rmul__(Tensor([635041, 80],"float32"), 1.0, )
[Prof] paddle.Tensor.__rmul__ 	 paddle.Tensor.__rmul__(Tensor([635041, 80],"float32"), 1.0, ) 	 50803280 	 1000 	 0.2956414222717285 	 0.2979297637939453 	 0.28662109375 	 0.28237128257751465 	 0.29555583000183105 	 0.29776930809020996 	 0.23751568794250488 	 0.23171257972717285 	 
2025-07-24 16:04:00.239176 test begin: paddle.Tensor.__ror__(Tensor([2, 3, 8467201],"int32"), 5, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([2, 3, 8467201],"int32"), 5, ) 	 50803206 	 1000 	 0.29878687858581543 	 0.2978672981262207 	 0.15260577201843262 	 0.2832458019256592 	 None 	 None 	 None 	 None 	 
2025-07-24 16:04:01.420544 test begin: paddle.Tensor.__ror__(Tensor([2, 3, 8467201],"int32"), True, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([2, 3, 8467201],"int32"), True, ) 	 50803206 	 1000 	 0.29874396324157715 	 0.29780125617980957 	 0.15259099006652832 	 0.2833225727081299 	 None 	 None 	 None 	 None 	 
2025-07-24 16:04:02.609047 test begin: paddle.Tensor.__ror__(Tensor([2, 5080321, 5],"int32"), 5, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([2, 5080321, 5],"int32"), 5, ) 	 50803210 	 1000 	 0.29883384704589844 	 0.7478017807006836 	 0.1526339054107666 	 0.2833983898162842 	 None 	 None 	 None 	 None 	 
2025-07-24 16:04:05.124517 test begin: paddle.Tensor.__ror__(Tensor([2, 5080321, 5],"int32"), True, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([2, 5080321, 5],"int32"), True, ) 	 50803210 	 1000 	 0.29877138137817383 	 0.3167905807495117 	 0.1526165008544922 	 0.2834155559539795 	 None 	 None 	 None 	 None 	 
2025-07-24 16:04:07.850661 test begin: paddle.Tensor.__ror__(Tensor([3386881, 3, 5],"int32"), 5, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([3386881, 3, 5],"int32"), 5, ) 	 50803215 	 1000 	 0.29874444007873535 	 0.29787468910217285 	 0.15258455276489258 	 0.28354430198669434 	 None 	 None 	 None 	 None 	 
2025-07-24 16:04:09.042507 test begin: paddle.Tensor.__ror__(Tensor([3386881, 3, 5],"int32"), True, )
[Prof] paddle.Tensor.__ror__ 	 paddle.Tensor.__ror__(Tensor([3386881, 3, 5],"int32"), True, ) 	 50803215 	 1000 	 0.2987082004547119 	 0.2978336811065674 	 0.15257930755615234 	 0.2798452377319336 	 None 	 None 	 None 	 None 	 
2025-07-24 16:04:10.228650 test begin: paddle.Tensor.__rpow__(Tensor([50803201],"float32"), 10000, )
[Prof] paddle.Tensor.__rpow__ 	 paddle.Tensor.__rpow__(Tensor([50803201],"float32"), 10000, ) 	 50803201 	 1000 	 0.5848846435546875 	 0.6305201053619385 	 0.29886794090270996 	 0.3220522403717041 	 0.7027521133422852 	 0.7430062294006348 	 0.6448111534118652 	 0.379638671875 	 
2025-07-24 16:04:14.712266 test begin: paddle.Tensor.__rpow__(Tensor([50803201],"float32"), 10000.0, )
[Prof] paddle.Tensor.__rpow__ 	 paddle.Tensor.__rpow__(Tensor([50803201],"float32"), 10000.0, ) 	 50803201 	 1000 	 0.5848641395568848 	 0.6443805694580078 	 0.2988932132720947 	 0.32918310165405273 	 0.7027115821838379 	 0.7429335117340088 	 0.6490182876586914 	 0.3795769214630127 	 
2025-07-24 16:04:19.142363 test begin: paddle.Tensor.__rrshift__(Tensor([169345, 300],"int32"), 232, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([169345, 300],"int32"), 232, ) 	 50803500 	 1000 	 0.2986478805541992 	 0.29868102073669434 	 0.15253067016601562 	 0.27867722511291504 	 None 	 None 	 None 	 None 	 
2025-07-24 16:04:20.329183 test begin: paddle.Tensor.__rrshift__(Tensor([200, 254017],"int32"), 232, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([200, 254017],"int32"), 232, ) 	 50803400 	 1000 	 0.29869604110717773 	 0.2978076934814453 	 0.15259075164794922 	 0.27860164642333984 	 None 	 None 	 None 	 None 	 
2025-07-24 16:04:21.509228 test begin: paddle.Tensor.__rrshift__(Tensor([200, 508033],"int16"), -255, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([200, 508033],"int16"), -255, ) 	 101606600 	 1000 	 0.33870792388916016 	 0.29613685607910156 	 0.0003056526184082031 	 0.2767307758331299 	 None 	 None 	 None 	 None 	 
2025-07-24 16:04:23.134635 test begin: paddle.Tensor.__rrshift__(Tensor([200, 508033],"int16"), 11, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([200, 508033],"int16"), 11, ) 	 101606600 	 1000 	 0.34078025817871094 	 0.2960035800933838 	 0.00030350685119628906 	 0.2760007381439209 	 None 	 None 	 None 	 None 	 
2025-07-24 16:04:24.795806 test begin: paddle.Tensor.__rrshift__(Tensor([338689, 300],"int16"), -255, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([338689, 300],"int16"), -255, ) 	 101606700 	 1000 	 0.33816957473754883 	 0.2959871292114258 	 0.00030994415283203125 	 0.2756493091583252 	 None 	 None 	 None 	 None 	 
2025-07-24 16:04:26.456194 test begin: paddle.Tensor.__rrshift__(Tensor([338689, 300],"int16"), 11, )
[Prof] paddle.Tensor.__rrshift__ 	 paddle.Tensor.__rrshift__(Tensor([338689, 300],"int16"), 11, ) 	 101606700 	 1000 	 0.3368380069732666 	 0.2960348129272461 	 0.0003104209899902344 	 0.277146577835083 	 None 	 None 	 None 	 None 	 
2025-07-24 16:04:28.090848 test begin: paddle.Tensor.__rshift__(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), ) 	 101607000 	 1000 	 0.44985389709472656 	 0.44679760932922363 	 0.4401686191558838 	 0.43495774269104004 	 None 	 None 	 None 	 None 	 combined
2025-07-24 16:04:30.177996 test begin: paddle.Tensor.__rshift__(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), ) 	 101606800 	 1000 	 0.45005011558532715 	 0.4502091407775879 	 0.4406096935272217 	 0.4346578121185303 	 None 	 None 	 None 	 None 	 combined
2025-07-24 16:04:32.326641 test begin: paddle.Tensor.__rshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), ) 	 203213200 	 1000 	 0.44756293296813965 	 0.450258731842041 	 0.43823957443237305 	 0.4383230209350586 	 None 	 None 	 None 	 None 	 combined
2025-07-24 16:04:37.277720 test begin: paddle.Tensor.__rshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, ) 	 203213200 	 1000 	 0.4483816623687744 	 3.1157336235046387 	 0.4389348030090332 	 0.35349297523498535 	 None 	 None 	 None 	 None 	 combined
2025-07-24 16:04:42.886872 test begin: paddle.Tensor.__rshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), ) 	 203213400 	 1000 	 0.4474067687988281 	 0.4501965045928955 	 0.43807077407836914 	 0.43824148178100586 	 None 	 None 	 None 	 None 	 combined
2025-07-24 16:04:45.838436 test begin: paddle.Tensor.__rshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, )
[Prof] paddle.Tensor.__rshift__ 	 paddle.Tensor.__rshift__(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, ) 	 203213400 	 1000 	 0.4482102394104004 	 3.112398624420166 	 0.43883585929870605 	 0.3535928726196289 	 None 	 None 	 None 	 None 	 combined
2025-07-24 16:04:51.386953 test begin: paddle.Tensor.__rsub__(Tensor([2, 1, 12404, 4096],"float16"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([2, 1, 12404, 4096],"float16"), 1, ) 	 101613568 	 1000 	 0.2984175682067871 	 0.2962915897369385 	 0.2893404960632324 	 0.2775559425354004 	 0.2981760501861572 	 0.2961702346801758 	 0.24740195274353027 	 0.2301797866821289 	 
2025-07-24 16:04:56.470506 test begin: paddle.Tensor.__rsub__(Tensor([2, 1, 4096, 12404],"float16"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([2, 1, 4096, 12404],"float16"), 1, ) 	 101613568 	 1000 	 0.29842472076416016 	 0.29865193367004395 	 0.28894472122192383 	 0.27732300758361816 	 0.2982039451599121 	 0.2962212562561035 	 0.24679780006408691 	 0.22959208488464355 	 
2025-07-24 16:05:01.580720 test begin: paddle.Tensor.__rsub__(Tensor([2, 4, 4096, 4096],"float16"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([2, 4, 4096, 4096],"float16"), 1, ) 	 134217728 	 1000 	 0.3921933174133301 	 0.39006757736206055 	 0.3830606937408447 	 0.3706943988800049 	 0.39264702796936035 	 0.3900468349456787 	 0.341113805770874 	 0.3224177360534668 	 
2025-07-24 16:05:09.729538 test begin: paddle.Tensor.__rsub__(Tensor([2944, 17257],"float32"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([2944, 17257],"float32"), 1, ) 	 50804608 	 1000 	 0.295670747756958 	 0.31311535835266113 	 0.28649163246154785 	 0.27858710289001465 	 0.2954413890838623 	 0.29781055450439453 	 0.2447352409362793 	 0.23044848442077637 	 
2025-07-24 16:05:13.521385 test begin: paddle.Tensor.__rsub__(Tensor([4224, 12028],"float32"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([4224, 12028],"float32"), 1, ) 	 50806272 	 1000 	 0.29582905769348145 	 0.2979249954223633 	 0.28665900230407715 	 0.279296875 	 0.2958106994628906 	 0.29784131050109863 	 0.24260568618774414 	 0.23148870468139648 	 
2025-07-24 16:05:16.390296 test begin: paddle.Tensor.__rsub__(Tensor([7, 1, 4096, 4096],"float16"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([7, 1, 4096, 4096],"float16"), 1, ) 	 117440512 	 1000 	 0.34373974800109863 	 0.3417689800262451 	 0.33454203605651855 	 0.3229243755340576 	 0.343991756439209 	 0.3416450023651123 	 0.2888016700744629 	 0.27223896980285645 	 
2025-07-24 16:05:22.242306 test begin: paddle.Tensor.__rsub__(Tensor([7664, 6629],"float32"), 1, )
[Prof] paddle.Tensor.__rsub__ 	 paddle.Tensor.__rsub__(Tensor([7664, 6629],"float32"), 1, ) 	 50804656 	 1000 	 0.2958338260650635 	 0.2996501922607422 	 0.2864837646484375 	 0.27947139739990234 	 0.295778751373291 	 0.2977726459503174 	 0.24297547340393066 	 0.23236823081970215 	 
2025-07-24 16:05:25.096749 test begin: paddle.Tensor.__rtruediv__(Tensor([15548, 3268],"float32"), 1.0, )
[Prof] paddle.Tensor.__rtruediv__ 	 paddle.Tensor.__rtruediv__(Tensor([15548, 3268],"float32"), 1.0, ) 	 50810864 	 1000 	 0.584038257598877 	 0.6067488193511963 	 0.29841113090515137 	 0.30441713333129883 	 0.5856764316558838 	 1.3381612300872803 	 0.5301263332366943 	 0.34200501441955566 	 
2025-07-24 16:05:29.890229 test begin: paddle.Tensor.__rtruediv__(Tensor([16773, 3029],"float32"), 1.0, )
[Prof] paddle.Tensor.__rtruediv__ 	 paddle.Tensor.__rtruediv__(Tensor([16773, 3029],"float32"), 1.0, ) 	 50805417 	 1000 	 0.5841882228851318 	 0.5958290100097656 	 0.29852747917175293 	 0.3043487071990967 	 0.5854654312133789 	 1.3379688262939453 	 0.5299859046936035 	 0.3418920040130615 	 
2025-07-24 16:05:34.678955 test begin: paddle.Tensor.__rtruediv__(Tensor([26736, 1901],"float32"), 1.0, )
[Prof] paddle.Tensor.__rtruediv__ 	 paddle.Tensor.__rtruediv__(Tensor([26736, 1901],"float32"), 1.0, ) 	 50825136 	 1000 	 0.5843033790588379 	 0.600750207901001 	 0.29853057861328125 	 0.3045077323913574 	 0.5856523513793945 	 1.338632583618164 	 0.5300829410552979 	 0.34206533432006836 	 
2025-07-24 16:05:40.495497 test begin: paddle.Tensor.__rtruediv__(Tensor([37411, 1358],"float32"), 1.0, )
[Prof] paddle.Tensor.__rtruediv__ 	 paddle.Tensor.__rtruediv__(Tensor([37411, 1358],"float32"), 1.0, ) 	 50804138 	 1000 	 0.5841014385223389 	 0.597923755645752 	 0.29843997955322266 	 0.30432653427124023 	 0.5853822231292725 	 1.3380444049835205 	 0.529733419418335 	 0.34192347526550293 	 
2025-07-24 16:05:45.275204 test begin: paddle.Tensor.__rtruediv__(Tensor([6684, 7601],"float32"), 1.0, )
[Prof] paddle.Tensor.__rtruediv__ 	 paddle.Tensor.__rtruediv__(Tensor([6684, 7601],"float32"), 1.0, ) 	 50805084 	 1000 	 0.583834171295166 	 0.5987389087677002 	 0.2982823848724365 	 0.3043515682220459 	 0.5854737758636475 	 1.338090181350708 	 0.5299232006072998 	 0.34194231033325195 	 
2025-07-24 16:05:50.045640 test begin: paddle.Tensor.__rxor__(Tensor([2, 3, 8467201],"int32"), 5, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([2, 3, 8467201],"int32"), 5, ) 	 50803206 	 1000 	 0.29885315895080566 	 0.30170583724975586 	 0.15264534950256348 	 0.28346896171569824 	 None 	 None 	 None 	 None 	 
2025-07-24 16:05:51.235215 test begin: paddle.Tensor.__rxor__(Tensor([2, 3, 8467201],"int32"), True, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([2, 3, 8467201],"int32"), True, ) 	 50803206 	 1000 	 0.29873013496398926 	 0.29793620109558105 	 0.1525883674621582 	 0.2832679748535156 	 None 	 None 	 None 	 None 	 
2025-07-24 16:05:52.421649 test begin: paddle.Tensor.__rxor__(Tensor([2, 5080321, 5],"int32"), 5, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([2, 5080321, 5],"int32"), 5, ) 	 50803210 	 1000 	 0.2987654209136963 	 0.2978847026824951 	 0.1525888442993164 	 0.28360772132873535 	 None 	 None 	 None 	 None 	 
2025-07-24 16:05:53.600232 test begin: paddle.Tensor.__rxor__(Tensor([2, 5080321, 5],"int32"), True, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([2, 5080321, 5],"int32"), True, ) 	 50803210 	 1000 	 0.29871320724487305 	 0.29783034324645996 	 0.1526029109954834 	 0.28348708152770996 	 None 	 None 	 None 	 None 	 
2025-07-24 16:05:54.805776 test begin: paddle.Tensor.__rxor__(Tensor([3386881, 3, 5],"int32"), 5, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([3386881, 3, 5],"int32"), 5, ) 	 50803215 	 1000 	 0.2987632751464844 	 0.2978370189666748 	 0.15260720252990723 	 0.2834353446960449 	 None 	 None 	 None 	 None 	 
2025-07-24 16:05:56.014588 test begin: paddle.Tensor.__rxor__(Tensor([3386881, 3, 5],"int32"), True, )
[Prof] paddle.Tensor.__rxor__ 	 paddle.Tensor.__rxor__(Tensor([3386881, 3, 5],"int32"), True, ) 	 50803215 	 1000 	 0.2987227439880371 	 0.29788780212402344 	 0.15259790420532227 	 0.2836456298828125 	 None 	 None 	 None 	 None 	 
2025-07-24 16:05:57.253050 test begin: paddle.Tensor.__sub__(Tensor([1, 1, 32768, 32768],"float16"), 1, )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([1, 1, 32768, 32768],"float16"), 1, ) 	 1073741824 	 1000 	 3.10465931892395 	 3.0913965702056885 	 3.0953903198242188 	 3.076709270477295 	 3.104773998260498 	 0.05982375144958496 	 3.0505449771881104 	 6.365776062011719e-05 	 
2025-07-24 16:06:47.611975 test begin: paddle.Tensor.__sub__(Tensor([128, 192, 22, 96],"float32"), Tensor([128, 1, 22, 96],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([128, 192, 22, 96],"float32"), Tensor([128, 1, 22, 96],"float32"), ) 	 52174848 	 1000 	 0.3037881851196289 	 0.3242664337158203 	 0.2931807041168213 	 0.29770922660827637 	 0.5357210636138916 	 0.45536255836486816 	 0.2737100124359131 	 0.23263049125671387 	 
2025-07-24 16:06:50.973730 test begin: paddle.Tensor.__sub__(Tensor([128, 192, 96, 22],"float32"), Tensor([128, 1, 96, 22],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([128, 192, 96, 22],"float32"), Tensor([128, 1, 96, 22],"float32"), ) 	 52174848 	 1000 	 0.3038020133972168 	 0.3233308792114258 	 0.2931685447692871 	 0.30350255966186523 	 0.5356018543243408 	 0.45531463623046875 	 0.273634672164917 	 0.23261380195617676 	 
2025-07-24 16:06:54.287959 test begin: paddle.Tensor.__sub__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 1, 96, 96],"float32"), ) 	 53084160 	 1000 	 0.30585527420043945 	 0.31931424140930176 	 0.2935340404510498 	 0.30704832077026367 	 0.4858577251434326 	 0.462160587310791 	 0.24822545051574707 	 0.23613190650939941 	 
2025-07-24 16:06:57.640036 test begin: paddle.Tensor.__sub__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 44, 96, 96],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 44, 96, 96],"float32"), ) 	 103809024 	 1000 	 0.45968055725097656 	 0.4562370777130127 	 0.4491608142852783 	 0.44489121437072754 	 0.4845261573791504 	 0.304091215133667 	 0.42726874351501465 	 0.22945880889892578 	 
2025-07-24 16:07:01.906942 test begin: paddle.Tensor.__sub__(Tensor([2, 1, 1551, 32768],"float16"), 1, )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([2, 1, 1551, 32768],"float16"), 1, ) 	 101646336 	 1000 	 0.2983226776123047 	 0.29630041122436523 	 0.28914475440979004 	 0.2824380397796631 	 0.2982511520385742 	 0.05332374572753906 	 0.24729681015014648 	 2.8371810913085938e-05 	 
2025-07-24 16:07:06.787006 test begin: paddle.Tensor.__sub__(Tensor([2, 1, 32768, 1551],"float16"), 1, )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([2, 1, 32768, 1551],"float16"), 1, ) 	 101646336 	 1000 	 0.2983701229095459 	 0.29628705978393555 	 0.28923940658569336 	 0.2824409008026123 	 0.29824304580688477 	 0.054390907287597656 	 0.2476654052734375 	 6.604194641113281e-05 	 
2025-07-24 16:07:11.666227 test begin: paddle.Tensor.__sub__(Tensor([2, 1, 32768, 32768],"float16"), 1, )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([2, 1, 32768, 32768],"float16"), 1, ) 	 2147483648 	 1000 	 6.20635199546814 	 6.183107137680054 	 6.196619510650635 	 3.1591436862945557 	 6.206392526626587 	 0.05388188362121582 	 6.155275821685791 	 3.9577484130859375e-05 	 
2025-07-24 16:08:53.579568 test begin: paddle.Tensor.__sub__(Tensor([26736, 3029, 1],"float32"), Tensor([26736, 3029, 1],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([26736, 3029, 1],"float32"), Tensor([26736, 3029, 1],"float32"), ) 	 161966688 	 1000 	 0.71402907371521 	 0.7086939811706543 	 0.7042379379272461 	 0.6970171928405762 	 0.7559468746185303 	 0.471468448638916 	 0.69822096824646 	 0.39692163467407227 	 
2025-07-24 16:09:00.285538 test begin: paddle.Tensor.__sub__(Tensor([26736, 3029, 1],"float32"), Tensor([26736, 3029, 2],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([26736, 3029, 1],"float32"), Tensor([26736, 3029, 2],"float32"), ) 	 242950032 	 1000 	 1.1744201183319092 	 1.197767734527588 	 1.163870096206665 	 1.184556245803833 	 2.387899160385132 	 2.466205358505249 	 1.2203059196472168 	 1.2601797580718994 	 
2025-07-24 16:09:14.310124 test begin: paddle.Tensor.__sub__(Tensor([26736, 3029, 2],"float32"), Tensor([26736, 3029, 1],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([26736, 3029, 2],"float32"), Tensor([26736, 3029, 1],"float32"), ) 	 242950032 	 1000 	 1.1734049320220947 	 1.1975932121276855 	 1.162902593612671 	 1.1822097301483154 	 2.225940227508545 	 2.4840986728668213 	 0.7580816745758057 	 1.2782049179077148 	 
2025-07-24 16:09:29.446259 test begin: paddle.Tensor.__sub__(Tensor([26736, 951, 2],"float32"), Tensor([26736, 951, 2],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([26736, 951, 2],"float32"), Tensor([26736, 951, 2],"float32"), ) 	 101703744 	 1000 	 0.4501774311065674 	 0.4471707344055176 	 0.4406118392944336 	 0.4354381561279297 	 0.4751565456390381 	 0.29799699783325195 	 0.41782355308532715 	 0.22359943389892578 	 
2025-07-24 16:09:33.562779 test begin: paddle.Tensor.__sub__(Tensor([29, 192, 96, 96],"float32"), Tensor([29, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([29, 192, 96, 96],"float32"), Tensor([29, 1, 96, 96],"float32"), ) 	 51581952 	 1000 	 0.30035972595214844 	 0.3209109306335449 	 0.2882699966430664 	 0.29978418350219727 	 0.4792053699493408 	 0.4503812789916992 	 0.24479961395263672 	 0.2301011085510254 	 
2025-07-24 16:09:38.616820 test begin: paddle.Tensor.__sub__(Tensor([8387, 3029, 2],"float32"), Tensor([8387, 3029, 2],"float32"), )
[Prof] paddle.Tensor.__sub__ 	 paddle.Tensor.__sub__(Tensor([8387, 3029, 2],"float32"), Tensor([8387, 3029, 2],"float32"), ) 	 101616892 	 1000 	 0.4499661922454834 	 0.44675731658935547 	 0.44028496742248535 	 0.4353811740875244 	 0.47430968284606934 	 0.2977457046508789 	 0.41660618782043457 	 0.22539925575256348 	 
2025-07-24 16:09:42.793852 test begin: paddle.Tensor.__truediv__(Tensor([124, 128, 34, 96],"float32"), Tensor([124, 1, 34, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([124, 128, 34, 96],"float32"), Tensor([124, 1, 34, 96],"float32"), ) 	 52210944 	 1000 	 0.30417299270629883 	 0.3240196704864502 	 0.2931363582611084 	 0.311809778213501 	 0.8114209175109863 	 1.8857462406158447 	 0.4145493507385254 	 0.3210599422454834 	 
2025-07-24 16:09:47.856993 test begin: paddle.Tensor.__truediv__(Tensor([124, 128, 96, 34],"float32"), Tensor([124, 1, 96, 34],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([124, 128, 96, 34],"float32"), Tensor([124, 1, 96, 34],"float32"), ) 	 52210944 	 1000 	 0.3041574954986572 	 0.32397913932800293 	 0.2926900386810303 	 0.311692476272583 	 0.811406135559082 	 1.8857333660125732 	 0.4146101474761963 	 0.3210115432739258 	 
2025-07-24 16:09:52.871209 test begin: paddle.Tensor.__truediv__(Tensor([124, 45, 96, 96],"float32"), Tensor([124, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([124, 45, 96, 96],"float32"), Tensor([124, 1, 96, 96],"float32"), ) 	 52568064 	 1000 	 0.3037300109863281 	 0.32406139373779297 	 0.2924537658691406 	 0.31149792671203613 	 0.771597146987915 	 1.8855814933776855 	 0.39421796798706055 	 0.3209810256958008 	 
2025-07-24 16:09:57.906194 test begin: paddle.Tensor.__truediv__(Tensor([124, 45, 96, 96],"float32"), Tensor([124, 45, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([124, 45, 96, 96],"float32"), Tensor([124, 45, 96, 96],"float32"), ) 	 102850560 	 1000 	 0.45549440383911133 	 0.4574310779571533 	 0.44577479362487793 	 0.4430971145629883 	 1.1478183269500732 	 2.11592960357666 	 1.0849132537841797 	 0.4325287342071533 	 
2025-07-24 16:10:04.592519 test begin: paddle.Tensor.__truediv__(Tensor([128, 128, 33, 96],"float32"), Tensor([128, 1, 33, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 128, 33, 96],"float32"), Tensor([128, 1, 33, 96],"float32"), ) 	 52310016 	 1000 	 0.30426812171936035 	 0.32660818099975586 	 0.29308652877807617 	 0.31258583068847656 	 0.8105854988098145 	 1.8899273872375488 	 0.4141523838043213 	 0.32177281379699707 	 
2025-07-24 16:10:09.670727 test begin: paddle.Tensor.__truediv__(Tensor([128, 128, 96, 33],"float32"), Tensor([128, 1, 96, 33],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 128, 96, 33],"float32"), Tensor([128, 1, 96, 33],"float32"), ) 	 52310016 	 1000 	 0.30423569679260254 	 0.3247334957122803 	 0.2933320999145508 	 0.31241941452026367 	 0.8105156421661377 	 1.8899056911468506 	 0.4140963554382324 	 0.3217353820800781 	 
2025-07-24 16:10:14.705695 test begin: paddle.Tensor.__truediv__(Tensor([128, 192, 22, 96],"float32"), Tensor([128, 1, 22, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 192, 22, 96],"float32"), Tensor([128, 1, 22, 96],"float32"), ) 	 52174848 	 1000 	 0.3043644428253174 	 0.32411694526672363 	 0.2934741973876953 	 0.3105597496032715 	 0.8239264488220215 	 1.8867619037628174 	 0.42101597785949707 	 0.32121706008911133 	 
2025-07-24 16:10:19.799857 test begin: paddle.Tensor.__truediv__(Tensor([128, 192, 96, 22],"float32"), Tensor([128, 1, 96, 22],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 192, 96, 22],"float32"), Tensor([128, 1, 96, 22],"float32"), ) 	 52174848 	 1000 	 0.3043191432952881 	 0.32411932945251465 	 0.29326868057250977 	 0.3117680549621582 	 0.8239431381225586 	 1.886589765548706 	 0.4209871292114258 	 0.32115793228149414 	 
2025-07-24 16:10:24.851090 test begin: paddle.Tensor.__truediv__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 1, 96, 96],"float32"), ) 	 53084160 	 1000 	 0.30628275871276855 	 0.3271324634552002 	 0.29047679901123047 	 0.3147900104522705 	 0.7775938510894775 	 1.9008018970489502 	 0.39731335639953613 	 0.32356834411621094 	 
2025-07-24 16:10:29.924901 test begin: paddle.Tensor.__truediv__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 44, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([128, 44, 96, 96],"float32"), Tensor([128, 44, 96, 96],"float32"), ) 	 103809024 	 1000 	 0.4598269462585449 	 0.47054505348205566 	 0.44997096061706543 	 0.44405698776245117 	 1.1593818664550781 	 2.1353402137756348 	 1.0989415645599365 	 0.4365193843841553 	 
2025-07-24 16:10:39.087369 test begin: paddle.Tensor.__truediv__(Tensor([29, 192, 96, 96],"float32"), Tensor([29, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([29, 192, 96, 96],"float32"), Tensor([29, 1, 96, 96],"float32"), ) 	 51581952 	 1000 	 0.30043935775756836 	 0.32950663566589355 	 0.28883814811706543 	 0.308027982711792 	 0.7655415534973145 	 1.8653209209442139 	 0.39112019538879395 	 0.31757616996765137 	 
2025-07-24 16:10:44.443351 test begin: paddle.Tensor.__truediv__(Tensor([44, 128, 96, 96],"float32"), Tensor([44, 1, 96, 96],"float32"), )
[Prof] paddle.Tensor.__truediv__ 	 paddle.Tensor.__truediv__(Tensor([44, 128, 96, 96],"float32"), Tensor([44, 1, 96, 96],"float32"), ) 	 52310016 	 1000 	 0.3036668300628662 	 0.3246169090270996 	 0.2927525043487549 	 0.31229519844055176 	 0.7755696773529053 	 1.8882701396942139 	 0.39628100395202637 	 0.32148289680480957 	 
2025-07-24 16:10:49.450909 test begin: paddle.Tensor.__xor__(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 1000 	 0.4486546516418457 	 0.4548835754394531 	 0.4377562999725342 	 0.437929630279541 	 None 	 None 	 None 	 None 	 
2025-07-24 16:10:52.418985 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 1000 	 0.4484729766845703 	 0.45143580436706543 	 0.4392848014831543 	 0.43764162063598633 	 None 	 None 	 None 	 None 	 
2025-07-24 16:10:55.371281 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), ) 	 203214240 	 1000 	 0.4486093521118164 	 0.45036888122558594 	 0.4394357204437256 	 0.4383866786956787 	 None 	 None 	 None 	 None 	 
2025-07-24 16:10:58.297480 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), ) 	 203213880 	 1000 	 0.44840073585510254 	 0.45040369033813477 	 0.43970465660095215 	 0.4384450912475586 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:01.245296 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), ) 	 101607264 	 1000 	 0.1171875 	 0.11745524406433105 	 0.10838079452514648 	 0.10518789291381836 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:02.896439 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), ) 	 101607264 	 1000 	 0.4497945308685303 	 0.44676899909973145 	 0.44101715087890625 	 0.434934139251709 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:04.979001 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), ) 	 203213664 	 1000 	 0.44911646842956543 	 0.4503655433654785 	 0.44032812118530273 	 0.43808817863464355 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:09.603132 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 50807520 	 1000 	 0.1682145595550537 	 0.22716569900512695 	 0.15843987464904785 	 0.21375823020935059 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:10.710162 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 101610720 	 1000 	 0.31490349769592285 	 0.48092222213745117 	 0.3046588897705078 	 0.46655869483947754 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:12.515621 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 50807520 	 1000 	 0.29636120796203613 	 0.308058500289917 	 0.28650999069213867 	 0.29397010803222656 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:13.725362 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), ) 	 101608560 	 1000 	 0.11799097061157227 	 0.11624956130981445 	 0.10916972160339355 	 0.1038060188293457 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:15.363996 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), ) 	 101608560 	 1000 	 0.44989824295043945 	 0.44670724868774414 	 0.4402153491973877 	 0.4348151683807373 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:17.457668 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), ) 	 203214960 	 1000 	 0.44840216636657715 	 0.45281267166137695 	 0.43974900245666504 	 0.43766236305236816 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:20.328966 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 50807520 	 1000 	 0.1769418716430664 	 0.2272787094116211 	 0.16715168952941895 	 0.21380186080932617 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:21.455069 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 101610720 	 1000 	 0.11794471740722656 	 0.11643052101135254 	 0.10911011695861816 	 0.10418844223022461 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:23.114551 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50807520 	 1000 	 0.29577112197875977 	 0.30794310569763184 	 0.2859969139099121 	 0.2948944568634033 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:24.324068 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 101610720 	 1000 	 0.45002317428588867 	 0.44669675827026367 	 0.4411613941192627 	 0.43475961685180664 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:26.409100 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101610720 	 1000 	 0.3457052707672119 	 0.47842955589294434 	 0.3360426425933838 	 0.4651341438293457 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:28.215781 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 203217120 	 1000 	 0.4482614994049072 	 0.4504384994506836 	 0.43952322006225586 	 0.43857598304748535 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:31.083700 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), ) 	 101607480 	 1000 	 0.1179666519165039 	 0.11545395851135254 	 0.10902714729309082 	 0.10317206382751465 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:32.742570 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), ) 	 101607480 	 1000 	 0.44985008239746094 	 0.4514005184173584 	 0.4410057067871094 	 0.4343867301940918 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:34.824535 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), ) 	 101607840 	 1000 	 0.11800932884216309 	 0.11916923522949219 	 0.10908651351928711 	 0.10253262519836426 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:39.560070 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), ) 	 101607840 	 1000 	 0.4502079486846924 	 0.4467289447784424 	 0.4409341812133789 	 0.43477678298950195 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:41.630836 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 1000 	 0.11796927452087402 	 0.13512229919433594 	 0.10907220840454102 	 0.10314226150512695 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:45.073327 test begin: paddle.Tensor.__xor__(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 1000 	 0.45006465911865234 	 0.44722485542297363 	 0.4411766529083252 	 0.4347805976867676 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:48.165673 test begin: paddle.Tensor.__xor__(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 1000 	 0.11798477172851562 	 0.11547422409057617 	 0.1090238094329834 	 0.10260915756225586 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:49.821403 test begin: paddle.Tensor.__xor__(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 1000 	 0.4501218795776367 	 0.4467630386352539 	 0.44126057624816895 	 0.4348921775817871 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:51.887354 test begin: paddle.Tensor.__xor__(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101608560 	 1000 	 0.11795711517333984 	 0.11627912521362305 	 0.10911297798156738 	 0.10407423973083496 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:53.520313 test begin: paddle.Tensor.__xor__(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101608560 	 1000 	 0.44982051849365234 	 0.45320868492126465 	 0.4410078525543213 	 0.4343898296356201 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:55.585795 test begin: paddle.Tensor.__xor__(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.Tensor.__xor__ 	 paddle.Tensor.__xor__(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214960 	 1000 	 0.44820570945739746 	 0.45038366317749023 	 0.43933868408203125 	 0.4384896755218506 	 None 	 None 	 None 	 None 	 
2025-07-24 16:11:58.491521 test begin: paddle.Tensor.abs(Tensor([243360, 209],"float32"), )
[Prof] paddle.Tensor.abs 	 paddle.Tensor.abs(Tensor([243360, 209],"float32"), ) 	 50862240 	 1000 	 0.2960782051086426 	 0.29813528060913086 	 0.28675103187561035 	 0.28548574447631836 	 0.4504086971282959 	 0.7438514232635498 	 0.3982853889465332 	 0.3800928592681885 	 
2025-07-24 16:12:01.912583 test begin: paddle.Tensor.abs(Tensor([282240, 181],"float32"), )
[Prof] paddle.Tensor.abs 	 paddle.Tensor.abs(Tensor([282240, 181],"float32"), ) 	 51085440 	 1000 	 0.29724645614624023 	 0.29936885833740234 	 0.28799891471862793 	 0.28683924674987793 	 0.45212268829345703 	 0.7470073699951172 	 0.40026092529296875 	 0.3816647529602051 	 
2025-07-24 16:12:05.359420 test begin: paddle.Tensor.abs(Tensor([324000, 157],"float32"), )
[Prof] paddle.Tensor.abs 	 paddle.Tensor.abs(Tensor([324000, 157],"float32"), ) 	 50868000 	 1000 	 0.29604482650756836 	 0.29814672470092773 	 0.2868795394897461 	 0.2854886054992676 	 0.4503657817840576 	 0.743908166885376 	 0.39862990379333496 	 0.380084753036499 	 
2025-07-24 16:12:08.809936 test begin: paddle.Tensor.abs(Tensor([635041, 80],"float32"), )
[Prof] paddle.Tensor.abs 	 paddle.Tensor.abs(Tensor([635041, 80],"float32"), ) 	 50803280 	 1000 	 0.2956113815307617 	 0.29776430130004883 	 0.28644490242004395 	 0.28550195693969727 	 0.44978952407836914 	 0.742849588394165 	 0.39776015281677246 	 0.3795037269592285 	 
2025-07-24 16:12:12.218415 test begin: paddle.Tensor.add(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.add 	 paddle.Tensor.add(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 1000 	 0.4498295783996582 	 0.4466831684112549 	 0.4398777484893799 	 0.43545007705688477 	 0.48180508613586426 	 0.05942106246948242 	 0.4246234893798828 	 6.628036499023438e-05 	 
2025-07-24 16:12:16.159304 test begin: paddle.Tensor.all(Tensor([1, 1, 2048, 24807],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([1, 1, 2048, 24807],"bool"), ) 	 50804736 	 1000 	 0.05212521553039551 	 0.061203956604003906 	 0.02660202980041504 	 0.031236648559570312 	 None 	 None 	 None 	 None 	 
2025-07-24 16:12:17.003312 test begin: paddle.Tensor.all(Tensor([1, 1, 24807, 2048],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([1, 1, 24807, 2048],"bool"), ) 	 50804736 	 1000 	 0.05221891403198242 	 0.06118512153625488 	 0.026592016220092773 	 0.031261444091796875 	 None 	 None 	 None 	 None 	 
2025-07-24 16:12:17.837444 test begin: paddle.Tensor.all(Tensor([1, 13, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([1, 13, 2048, 2048],"bool"), ) 	 54525952 	 1000 	 0.055294036865234375 	 0.06473088264465332 	 0.02823162078857422 	 0.03305315971374512 	 None 	 None 	 None 	 None 	 
2025-07-24 16:12:18.705420 test begin: paddle.Tensor.all(Tensor([13, 1, 2048, 2048],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([13, 1, 2048, 2048],"bool"), ) 	 54525952 	 1000 	 0.05529284477233887 	 0.06472921371459961 	 0.028238534927368164 	 0.03307628631591797 	 None 	 None 	 None 	 None 	 
2025-07-24 16:12:19.579294 test begin: paddle.Tensor.all(Tensor([159, 10, 32000],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([159, 10, 32000],"bool"), ) 	 50880000 	 1000 	 0.052405595779418945 	 0.06121325492858887 	 0.026764631271362305 	 0.03125929832458496 	 None 	 None 	 None 	 None 	 
2025-07-24 16:12:20.392096 test begin: paddle.Tensor.all(Tensor([2, 10, 2540161],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([2, 10, 2540161],"bool"), ) 	 50803220 	 1000 	 0.05226397514343262 	 0.0611729621887207 	 0.026697397232055664 	 0.031215429306030273 	 None 	 None 	 None 	 None 	 
2025-07-24 16:12:21.203577 test begin: paddle.Tensor.all(Tensor([2, 100, 256000],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([2, 100, 256000],"bool"), ) 	 51200000 	 1000 	 0.05230426788330078 	 0.06171083450317383 	 0.026720523834228516 	 0.03151202201843262 	 None 	 None 	 None 	 None 	 
2025-07-24 16:12:22.029433 test begin: paddle.Tensor.all(Tensor([2, 794, 32000],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([2, 794, 32000],"bool"), ) 	 50816000 	 1000 	 0.05215144157409668 	 0.06116747856140137 	 0.026629924774169922 	 0.031238555908203125 	 None 	 None 	 None 	 None 	 
2025-07-24 16:12:22.846159 test begin: paddle.Tensor.all(Tensor([20, 10, 256000],"bool"), )
[Prof] paddle.Tensor.all 	 paddle.Tensor.all(Tensor([20, 10, 256000],"bool"), ) 	 51200000 	 1000 	 0.05226755142211914 	 0.06202816963195801 	 0.026691198348999023 	 0.03185081481933594 	 None 	 None 	 None 	 None 	 
2025-07-24 16:12:23.672009 test begin: paddle.Tensor.amax(Tensor([1270081, 2, 4, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([1270081, 2, 4, 5],"float32"), axis=-1, keepdim=True, ) 	 50803240 	 1000 	 0.45285868644714355 	 0.471682071685791 	 0.44109153747558594 	 0.4573352336883545 	 1.3160581588745117 	 1.6166141033172607 	 0.336195707321167 	 0.33023619651794434 	 
2025-07-24 16:12:28.596993 test begin: paddle.Tensor.amax(Tensor([1270081, 2, 5, 4],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([1270081, 2, 5, 4],"float32"), axis=2, keepdim=True, ) 	 50803240 	 1000 	 0.5134189128875732 	 0.18380236625671387 	 0.5006833076477051 	 0.16950154304504395 	 1.3867101669311523 	 1.5305168628692627 	 0.354290246963501 	 0.3126564025878906 	 
2025-07-24 16:12:33.216336 test begin: paddle.Tensor.amax(Tensor([1270081, 2, 5, 4],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([1270081, 2, 5, 4],"float32"), axis=None, keepdim=False, ) 	 50803240 	 1000 	 0.15186357498168945 	 0.1529703140258789 	 0.07758951187133789 	 0.07813048362731934 	 1.0446844100952148 	 1.2485346794128418 	 0.21367931365966797 	 0.21256613731384277 	 
2025-07-24 16:12:39.512973 test begin: paddle.Tensor.amax(Tensor([3, 2, 1693441, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 2, 1693441, 5],"float32"), axis=-1, keepdim=True, ) 	 50803230 	 1000 	 0.4533853530883789 	 0.4738047122955322 	 0.4415745735168457 	 0.45641279220581055 	 1.316035509109497 	 1.6166472434997559 	 0.3362240791320801 	 0.33023524284362793 	 
2025-07-24 16:12:44.396271 test begin: paddle.Tensor.amax(Tensor([3, 2, 2116801, 4],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 2, 2116801, 4],"float32"), axis=2, keepdim=True, ) 	 50803224 	 1000 	 6.2934043407440186 	 0.17173075675964355 	 3.2158026695251465 	 0.08772444725036621 	 5.722928285598755 	 1.3419127464294434 	 1.1680688858032227 	 0.22842144966125488 	 
2025-07-24 16:12:58.805080 test begin: paddle.Tensor.amax(Tensor([3, 2, 2116801, 4],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 2, 2116801, 4],"float32"), axis=None, keepdim=False, ) 	 50803224 	 1000 	 0.15189623832702637 	 0.1528923511505127 	 0.07761168479919434 	 0.07811641693115234 	 1.0447404384613037 	 1.248173475265503 	 0.21367478370666504 	 0.21248674392700195 	 
2025-07-24 16:13:02.227957 test begin: paddle.Tensor.amax(Tensor([3, 2, 4, 2116801],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 2, 4, 2116801],"float32"), axis=-1, keepdim=True, ) 	 50803224 	 1000 	 0.16954374313354492 	 0.15473222732543945 	 0.08664870262145996 	 0.07906174659729004 	 1.0663747787475586 	 1.2741999626159668 	 0.2180938720703125 	 0.21687674522399902 	 
2025-07-24 16:13:05.706828 test begin: paddle.Tensor.amax(Tensor([3, 2, 5, 1693441],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 2, 5, 1693441],"float32"), axis=2, keepdim=True, ) 	 50803230 	 1000 	 0.20848727226257324 	 0.21779751777648926 	 0.19680094718933105 	 0.20349884033203125 	 1.261413812637329 	 1.5259394645690918 	 0.3223538398742676 	 0.3116905689239502 	 
2025-07-24 16:13:09.906039 test begin: paddle.Tensor.amax(Tensor([3, 2, 5, 1693441],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 2, 5, 1693441],"float32"), axis=None, keepdim=False, ) 	 50803230 	 1000 	 0.1518540382385254 	 0.15286755561828613 	 0.07757115364074707 	 0.0781254768371582 	 1.0447368621826172 	 1.2483057975769043 	 0.2137155532836914 	 0.21248769760131836 	 
2025-07-24 16:13:13.348760 test begin: paddle.Tensor.amax(Tensor([3, 846721, 4, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 846721, 4, 5],"float32"), axis=-1, keepdim=True, ) 	 50803260 	 1000 	 0.4533979892730713 	 0.4715237617492676 	 0.44158458709716797 	 0.4573047161102295 	 1.315932035446167 	 1.6168365478515625 	 0.3361337184906006 	 0.3304135799407959 	 
2025-07-24 16:13:18.193755 test begin: paddle.Tensor.amax(Tensor([3, 846721, 5, 4],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 846721, 5, 4],"float32"), axis=2, keepdim=True, ) 	 50803260 	 1000 	 0.5134038925170898 	 0.18370628356933594 	 0.5006437301635742 	 0.16947126388549805 	 1.387070894241333 	 1.5310447216033936 	 0.35434532165527344 	 0.31279611587524414 	 
2025-07-24 16:13:22.805193 test begin: paddle.Tensor.amax(Tensor([3, 846721, 5, 4],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amax 	 paddle.Tensor.amax(Tensor([3, 846721, 5, 4],"float32"), axis=None, keepdim=False, ) 	 50803260 	 1000 	 0.15184974670410156 	 0.15291404724121094 	 0.0775902271270752 	 0.07811355590820312 	 1.0446479320526123 	 1.2480254173278809 	 0.21367764472961426 	 0.21241545677185059 	 
2025-07-24 16:13:26.250135 test begin: paddle.Tensor.amin(Tensor([1270081, 2, 4, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([1270081, 2, 4, 5],"float32"), axis=-1, keepdim=True, ) 	 50803240 	 1000 	 0.4533865451812744 	 0.4715843200683594 	 0.44132542610168457 	 0.45750951766967773 	 1.3159425258636475 	 1.6167728900909424 	 0.3361937999725342 	 0.33039236068725586 	 
2025-07-24 16:13:31.103434 test begin: paddle.Tensor.amin(Tensor([1270081, 2, 5, 4],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([1270081, 2, 5, 4],"float32"), axis=2, keepdim=True, ) 	 50803240 	 1000 	 0.5134785175323486 	 0.18753623962402344 	 0.5005097389221191 	 0.16975784301757812 	 1.387113094329834 	 1.530961275100708 	 0.3543388843536377 	 0.3127737045288086 	 
2025-07-24 16:13:37.909486 test begin: paddle.Tensor.amin(Tensor([1270081, 2, 5, 4],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([1270081, 2, 5, 4],"float32"), axis=None, keepdim=False, ) 	 50803240 	 1000 	 0.15187621116638184 	 0.15298938751220703 	 0.07760834693908691 	 0.07813882827758789 	 1.0446546077728271 	 1.2485439777374268 	 0.21364736557006836 	 0.21242642402648926 	 
2025-07-24 16:13:41.468715 test begin: paddle.Tensor.amin(Tensor([3, 2, 1693441, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 2, 1693441, 5],"float32"), axis=-1, keepdim=True, ) 	 50803230 	 1000 	 0.4533717632293701 	 0.4717519283294678 	 0.4414331912994385 	 0.45012378692626953 	 1.3159801959991455 	 1.6171660423278809 	 0.3361985683441162 	 0.33031201362609863 	 
2025-07-24 16:13:46.340828 test begin: paddle.Tensor.amin(Tensor([3, 2, 2116801, 4],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 2, 2116801, 4],"float32"), axis=2, keepdim=True, ) 	 50803224 	 1000 	 6.3274006843566895 	 0.17168164253234863 	 3.2498652935028076 	 0.0876915454864502 	 5.722866773605347 	 1.3419990539550781 	 1.168079137802124 	 0.22842168807983398 	 
2025-07-24 16:14:00.775803 test begin: paddle.Tensor.amin(Tensor([3, 2, 2116801, 4],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 2, 2116801, 4],"float32"), axis=None, keepdim=False, ) 	 50803224 	 1000 	 0.15189552307128906 	 0.15308594703674316 	 0.07760119438171387 	 0.07829928398132324 	 1.0446815490722656 	 1.2487075328826904 	 0.21369385719299316 	 0.21262550354003906 	 
2025-07-24 16:14:04.198886 test begin: paddle.Tensor.amin(Tensor([3, 2, 4, 2116801],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 2, 4, 2116801],"float32"), axis=-1, keepdim=True, ) 	 50803224 	 1000 	 0.16956019401550293 	 0.15471386909484863 	 0.08663129806518555 	 0.07906985282897949 	 1.066319465637207 	 1.274977445602417 	 0.2180805206298828 	 0.21709346771240234 	 
2025-07-24 16:14:07.702615 test begin: paddle.Tensor.amin(Tensor([3, 2, 5, 1693441],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 2, 5, 1693441],"float32"), axis=2, keepdim=True, ) 	 50803230 	 1000 	 0.20849919319152832 	 0.21796083450317383 	 0.19672775268554688 	 0.20394396781921387 	 1.261258602142334 	 1.5263361930847168 	 0.3221738338470459 	 0.31186819076538086 	 
2025-07-24 16:14:11.911274 test begin: paddle.Tensor.amin(Tensor([3, 2, 5, 1693441],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 2, 5, 1693441],"float32"), axis=None, keepdim=False, ) 	 50803230 	 1000 	 0.1518688201904297 	 0.15293359756469727 	 0.07758092880249023 	 0.07812047004699707 	 1.044621467590332 	 1.248666524887085 	 0.21364879608154297 	 0.21257352828979492 	 
2025-07-24 16:14:15.339005 test begin: paddle.Tensor.amin(Tensor([3, 846721, 4, 5],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 846721, 4, 5],"float32"), axis=-1, keepdim=True, ) 	 50803260 	 1000 	 0.4533839225769043 	 0.4714376926422119 	 0.4414501190185547 	 0.45738911628723145 	 1.315971851348877 	 1.6176252365112305 	 0.33620405197143555 	 0.33046817779541016 	 
2025-07-24 16:14:20.210919 test begin: paddle.Tensor.amin(Tensor([3, 846721, 5, 4],"float32"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 846721, 5, 4],"float32"), axis=2, keepdim=True, ) 	 50803260 	 1000 	 0.5133976936340332 	 0.18370890617370605 	 0.5005922317504883 	 0.1696627140045166 	 1.3872265815734863 	 1.5309832096099854 	 0.3544046878814697 	 0.3127708435058594 	 
2025-07-24 16:14:24.801144 test begin: paddle.Tensor.amin(Tensor([3, 846721, 5, 4],"float32"), axis=None, keepdim=False, )
[Prof] paddle.Tensor.amin 	 paddle.Tensor.amin(Tensor([3, 846721, 5, 4],"float32"), axis=None, keepdim=False, ) 	 50803260 	 1000 	 0.15189838409423828 	 0.1528618335723877 	 0.0776066780090332 	 0.07807707786560059 	 1.0446457862854004 	 1.2486820220947266 	 0.21366596221923828 	 0.21257591247558594 	 
2025-07-24 16:14:28.221556 test begin: paddle.Tensor.any(Tensor([1, 1379, 192, 192],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([1, 1379, 192, 192],"bool"), axis=list[2,3,], ) 	 50835456 	 1000 	 0.06468963623046875 	 0.06695771217346191 	 0.033026933670043945 	 0.052565574645996094 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:29.053092 test begin: paddle.Tensor.any(Tensor([1, 1501, 184, 184],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([1, 1501, 184, 184],"bool"), axis=list[2,3,], ) 	 50817856 	 1000 	 0.06405520439147949 	 0.06728577613830566 	 0.0327143669128418 	 0.047183990478515625 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:29.877583 test begin: paddle.Tensor.any(Tensor([1, 300, 184, 921],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([1, 300, 184, 921],"bool"), axis=list[2,3,], ) 	 50839200 	 1000 	 0.0871419906616211 	 0.07817697525024414 	 0.04452180862426758 	 0.03992438316345215 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:30.740750 test begin: paddle.Tensor.any(Tensor([1, 300, 192, 883],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([1, 300, 192, 883],"bool"), axis=list[2,3,], ) 	 50860800 	 1000 	 0.08749890327453613 	 0.07817220687866211 	 0.044679880142211914 	 0.03992652893066406 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:31.608217 test begin: paddle.Tensor.any(Tensor([1, 300, 883, 192],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([1, 300, 883, 192],"bool"), axis=list[2,3,], ) 	 50860800 	 1000 	 0.08747553825378418 	 0.07821273803710938 	 0.04467916488647461 	 0.03995966911315918 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:32.479103 test begin: paddle.Tensor.any(Tensor([1, 300, 921, 184],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([1, 300, 921, 184],"bool"), axis=list[2,3,], ) 	 50839200 	 1000 	 0.0870814323425293 	 0.0781862735748291 	 0.04448533058166504 	 0.039929866790771484 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:33.338586 test begin: paddle.Tensor.any(Tensor([10, 300, 136, 136],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([10, 300, 136, 136],"bool"), axis=list[2,3,], ) 	 55488000 	 1000 	 0.06420445442199707 	 0.0798637866973877 	 0.05237126350402832 	 0.06534981727600098 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:34.252171 test begin: paddle.Tensor.any(Tensor([2, 1374, 136, 136],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([2, 1374, 136, 136],"bool"), axis=list[2,3,], ) 	 50827008 	 1000 	 0.06176161766052246 	 0.07430148124694824 	 0.04999494552612305 	 0.060001373291015625 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:35.103120 test begin: paddle.Tensor.any(Tensor([2, 300, 136, 623],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([2, 300, 136, 623],"bool"), axis=list[2,3,], ) 	 50836800 	 1000 	 0.0856635570526123 	 0.07166337966918945 	 0.043776750564575195 	 0.05720639228820801 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:37.069872 test begin: paddle.Tensor.any(Tensor([2, 300, 623, 136],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([2, 300, 623, 136],"bool"), axis=list[2,3,], ) 	 50836800 	 1000 	 0.0857090950012207 	 0.07167887687683105 	 0.043775320053100586 	 0.05739998817443848 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:39.337055 test begin: paddle.Tensor.any(Tensor([5, 300, 192, 192],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([5, 300, 192, 192],"bool"), axis=list[2,3,], ) 	 55296000 	 1000 	 0.06724238395690918 	 0.07072758674621582 	 0.034334659576416016 	 0.05640530586242676 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:40.248649 test begin: paddle.Tensor.any(Tensor([6, 300, 184, 184],"bool"), axis=list[2,3,], )
[Prof] paddle.Tensor.any 	 paddle.Tensor.any(Tensor([6, 300, 184, 184],"bool"), axis=list[2,3,], ) 	 60940800 	 1000 	 0.08249402046203613 	 0.07952332496643066 	 0.04215073585510254 	 0.06479620933532715 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:41.269848 test begin: paddle.Tensor.argmax(Tensor([13, 498, 8000],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([13, 498, 8000],"float32"), axis=2, ) 	 51792000 	 1000 	 0.27840733528137207 	 0.16912245750427246 	 0.26773738861083984 	 0.15557527542114258 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:42.562513 test begin: paddle.Tensor.argmax(Tensor([14, 457, 8000],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([14, 457, 8000],"float32"), axis=2, ) 	 51184000 	 1000 	 0.27509021759033203 	 0.16808676719665527 	 0.2568786144256592 	 0.15436387062072754 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:43.891509 test begin: paddle.Tensor.argmax(Tensor([14, 477, 8000],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([14, 477, 8000],"float32"), axis=2, ) 	 53424000 	 1000 	 0.286914587020874 	 0.1728959083557129 	 0.2761232852935791 	 0.15920138359069824 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:45.240863 test begin: paddle.Tensor.argmax(Tensor([30, 212, 8000],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([30, 212, 8000],"float32"), axis=2, ) 	 50880000 	 1000 	 0.2735562324523926 	 0.16718697547912598 	 0.26284122467041016 	 0.15352797508239746 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:46.492823 test begin: paddle.Tensor.argmax(Tensor([30, 457, 3706],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([30, 457, 3706],"float32"), axis=2, ) 	 50809260 	 1000 	 0.34845948219299316 	 0.16273093223571777 	 0.33795809745788574 	 0.14903521537780762 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:47.811314 test begin: paddle.Tensor.argmax(Tensor([30, 477, 3551],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([30, 477, 3551],"float32"), axis=2, ) 	 50814810 	 1000 	 0.3586766719818115 	 0.16736412048339844 	 0.34810638427734375 	 0.15331315994262695 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:49.159572 test begin: paddle.Tensor.argmax(Tensor([30, 498, 3401],"float32"), axis=2, )
[Prof] paddle.Tensor.argmax 	 paddle.Tensor.argmax(Tensor([30, 498, 3401],"float32"), axis=2, ) 	 50810940 	 1000 	 0.3702270984649658 	 0.16528034210205078 	 0.3519289493560791 	 0.14445137977600098 	 None 	 None 	 None 	 None 	 
2025-07-24 16:14:50.585599 test begin: paddle.Tensor.astype(Tensor([1, 32, 388, 4096],"float32"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([1, 32, 388, 4096],"float32"), "float32", ) 	 50855936 	 1000 	 0.0031921863555908203 	 0.002412080764770508 	 6.9141387939453125e-06 	 2.0503997802734375e-05 	 0.029158592224121094 	 0.04680895805358887 	 1.9788742065429688e-05 	 3.0517578125e-05 	 
2025-07-24 16:14:52.290159 test begin: paddle.Tensor.astype(Tensor([1, 32, 4096, 388],"float32"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([1, 32, 4096, 388],"float32"), "float32", ) 	 50855936 	 1000 	 0.003137826919555664 	 0.0024344921112060547 	 1.33514404296875e-05 	 1.621246337890625e-05 	 0.029063701629638672 	 0.046973466873168945 	 1.5735626220703125e-05 	 3.3855438232421875e-05 	 
2025-07-24 16:14:54.030298 test begin: paddle.Tensor.astype(Tensor([1, 32, 4096, 4096],"float32"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([1, 32, 4096, 4096],"float32"), "float32", ) 	 536870912 	 1000 	 0.0031707286834716797 	 0.0024356842041015625 	 7.3909759521484375e-06 	 1.811981201171875e-05 	 0.02942657470703125 	 0.046797990798950195 	 1.9788742065429688e-05 	 3.1948089599609375e-05 	 
2025-07-24 16:15:11.206404 test begin: paddle.Tensor.astype(Tensor([1, 4, 4096, 4096],"float32"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([1, 4, 4096, 4096],"float32"), "float32", ) 	 67108864 	 1000 	 0.006900310516357422 	 0.0024416446685791016 	 1.8358230590820312e-05 	 1.4781951904296875e-05 	 0.029433727264404297 	 0.04773664474487305 	 2.384185791015625e-05 	 3.886222839355469e-05 	 
2025-07-24 16:15:13.493474 test begin: paddle.Tensor.astype(Tensor([100352, 1013],"bfloat16"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([100352, 1013],"bfloat16"), "float32", ) 	 101656576 	 1000 	 0.47988462448120117 	 0.5595746040344238 	 0.46609973907470703 	 0.5461678504943848 	 0.45043230056762695 	 0.4539048671722412 	 0.3938136100769043 	 0.386425256729126 	 
2025-07-24 16:15:19.081315 test begin: paddle.Tensor.astype(Tensor([1013, 100352],"bfloat16"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([1013, 100352],"bfloat16"), "float32", ) 	 101656576 	 1000 	 0.47992682456970215 	 0.5595133304595947 	 0.46603918075561523 	 0.5461263656616211 	 0.450359582901001 	 0.45390844345092773 	 0.39392566680908203 	 0.3852212429046631 	 
2025-07-24 16:15:24.512328 test begin: paddle.Tensor.astype(Tensor([12404, 8192],"bfloat16"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([12404, 8192],"bfloat16"), "float32", ) 	 101613568 	 1000 	 0.48000550270080566 	 0.5592989921569824 	 0.466106653213501 	 0.5460033416748047 	 0.45089197158813477 	 0.4536550045013428 	 0.3938567638397217 	 0.3845667839050293 	 
2025-07-24 16:15:29.733227 test begin: paddle.Tensor.astype(Tensor([8192, 12404],"bfloat16"), "float32", )
[Prof] paddle.Tensor.astype 	 paddle.Tensor.astype(Tensor([8192, 12404],"bfloat16"), "float32", ) 	 101613568 	 1000 	 0.4798431396484375 	 0.5616035461425781 	 0.4659144878387451 	 0.5458638668060303 	 0.45103955268859863 	 0.45372724533081055 	 0.39516496658325195 	 0.382796049118042 	 
2025-07-24 16:15:34.968659 test begin: paddle.Tensor.atanh(Tensor([1, 16934401, 3],"float32"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([1, 16934401, 3],"float32"), ) 	 50803203 	 1000 	 0.2972133159637451 	 1.2544991970062256 	 0.28081178665161133 	 0.2786283493041992 	 0.4499831199645996 	 1.622939109802246 	 0.387526273727417 	 0.33203840255737305 	 
2025-07-24 16:15:41.232979 test begin: paddle.Tensor.atanh(Tensor([1, 2, 12700801],"float64"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([1, 2, 12700801],"float64"), ) 	 25401602 	 1000 	 0.444288969039917 	 0.40763020515441895 	 0.4355630874633789 	 0.39736390113830566 	 0.4476499557495117 	 1.6204252243041992 	 0.39592647552490234 	 0.3312861919403076 	 
2025-07-24 16:15:45.213728 test begin: paddle.Tensor.atanh(Tensor([1, 2, 25401601],"float32"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([1, 2, 25401601],"float32"), ) 	 50803202 	 1000 	 0.29714488983154297 	 0.298231840133667 	 0.28816795349121094 	 0.2879903316497803 	 0.4499359130859375 	 1.622694492340088 	 0.39664602279663086 	 0.33173203468322754 	 
2025-07-24 16:15:49.526831 test begin: paddle.Tensor.atanh(Tensor([1, 8467201, 3],"float64"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([1, 8467201, 3],"float64"), ) 	 25401603 	 1000 	 0.44405603408813477 	 0.40715765953063965 	 0.43531131744384766 	 0.39687395095825195 	 0.44797277450561523 	 1.6205482482910156 	 0.396223783493042 	 0.3312959671020508 	 
2025-07-24 16:15:53.509963 test begin: paddle.Tensor.atanh(Tensor([2, 12700801],"float64"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([2, 12700801],"float64"), ) 	 25401602 	 1000 	 0.4439840316772461 	 0.4070131778717041 	 0.4352414608001709 	 0.39688682556152344 	 0.44783496856689453 	 1.6205024719238281 	 0.395521879196167 	 0.33129119873046875 	 
2025-07-24 16:15:57.526827 test begin: paddle.Tensor.atanh(Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([4233601, 2, 3],"float64"), ) 	 25401606 	 1000 	 0.44397997856140137 	 0.40778017044067383 	 0.4352450370788574 	 0.3958888053894043 	 0.44771909713745117 	 1.6206114292144775 	 0.39446568489074707 	 0.3312656879425049 	 
2025-07-24 16:16:01.490432 test begin: paddle.Tensor.atanh(Tensor([6350401, 4],"float64"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([6350401, 4],"float64"), ) 	 25401604 	 1000 	 0.44412827491760254 	 1.3682713508605957 	 0.4353930950164795 	 0.39635467529296875 	 0.4479541778564453 	 1.6211740970611572 	 0.3955204486846924 	 0.3312797546386719 	 
2025-07-24 16:16:07.355497 test begin: paddle.Tensor.atanh(Tensor([8467201, 2, 3],"float32"), )
[Prof] paddle.Tensor.atanh 	 paddle.Tensor.atanh(Tensor([8467201, 2, 3],"float32"), ) 	 50803206 	 1000 	 0.29720067977905273 	 0.2982051372528076 	 0.2882964611053467 	 0.28775572776794434 	 0.44996142387390137 	 1.6226415634155273 	 0.397327184677124 	 0.33173036575317383 	 
2025-07-24 16:16:11.698945 test begin: paddle.Tensor.bmm(Tensor([1, 16934401, 3],"float32"), Tensor([1, 3, 2],"float32"), )
[Prof] paddle.Tensor.bmm 	 paddle.Tensor.bmm(Tensor([1, 16934401, 3],"float32"), Tensor([1, 3, 2],"float32"), ) 	 50803209 	 1000 	 1.7761940956115723 	 1.7767081260681152 	 0.1067054271697998 	 0.106719970703125 	 4.1337361335754395 	 4.135537385940552 	 0.2227482795715332 	 0.22072863578796387 	 
2025-07-24 16:16:24.967501 test begin: paddle.Tensor.bmm(Tensor([1, 170476, 299],"float32"), Tensor([1, 299, 2],"float32"), )
[Prof] paddle.Tensor.bmm 	 paddle.Tensor.bmm(Tensor([1, 170476, 299],"float32"), Tensor([1, 299, 2],"float32"), ) 	 50972922 	 1000 	 0.23900389671325684 	 0.23936009407043457 	 0.2245349884033203 	 0.2210545539855957 	 0.4179966449737549 	 0.4226694107055664 	 0.14228010177612305 	 0.14365863800048828 	 
2025-07-24 16:16:27.119682 test begin: paddle.Tensor.bmm(Tensor([1, 179876, 283],"float32"), Tensor([1, 283, 2],"float32"), )
[Prof] paddle.Tensor.bmm 	 paddle.Tensor.bmm(Tensor([1, 179876, 283],"float32"), Tensor([1, 283, 2],"float32"), ) 	 50905474 	 1000 	 0.240098237991333 	 0.24112653732299805 	 0.22443747520446777 	 0.22123146057128906 	 0.41119384765625 	 0.4171433448791504 	 0.13992595672607422 	 0.14200806617736816 	 
2025-07-24 16:16:29.263042 test begin: paddle.Tensor.bmm(Tensor([1, 191277, 266],"float32"), Tensor([1, 266, 2],"float32"), )
[Prof] paddle.Tensor.bmm 	 paddle.Tensor.bmm(Tensor([1, 191277, 266],"float32"), Tensor([1, 266, 2],"float32"), ) 	 50880214 	 1000 	 0.23345613479614258 	 0.2337796688079834 	 0.21780705451965332 	 0.21401286125183105 	 0.41860532760620117 	 0.4216170310974121 	 0.1425151824951172 	 0.14343762397766113 	 
2025-07-24 16:16:31.396353 test begin: paddle.Tensor.bmm(Tensor([100, 170476, 3],"float32"), Tensor([100, 3, 2],"float32"), )
[Prof] paddle.Tensor.bmm 	 paddle.Tensor.bmm(Tensor([100, 170476, 3],"float32"), Tensor([100, 3, 2],"float32"), ) 	 51143400 	 1000 	 4.845182657241821 	 4.845975160598755 	 4.832851886749268 	 4.829732418060303 	 38.521193742752075 	 38.52326989173889 	 19.68428945541382 	 19.685956239700317 	 
2025-07-24 16:17:59.655170 test begin: paddle.Tensor.bmm(Tensor([89, 191277, 3],"float32"), Tensor([89, 3, 2],"float32"), )
[Prof] paddle.Tensor.bmm 	 paddle.Tensor.bmm(Tensor([89, 191277, 3],"float32"), Tensor([89, 3, 2],"float32"), ) 	 51071493 	 1000 	 4.838592052459717 	 4.839182615280151 	 4.8261871337890625 	 4.823179483413696 	 42.62569785118103 	 42.626479148864746 	 21.781768321990967 	 21.782037496566772 	 
2025-07-24 16:19:37.435871 test begin: paddle.Tensor.bmm(Tensor([95, 179876, 3],"float32"), Tensor([95, 3, 2],"float32"), )
[Prof] paddle.Tensor.bmm 	 paddle.Tensor.bmm(Tensor([95, 179876, 3],"float32"), Tensor([95, 3, 2],"float32"), ) 	 51265230 	 1000 	 4.859821081161499 	 5.085422992706299 	 4.847532272338867 	 4.838613510131836 	 40.39346265792847 	 40.39310884475708 	 20.640952348709106 	 20.64073157310486 	 
2025-07-24 16:21:11.144974 test begin: paddle.Tensor.cast(Tensor([128256, 793],"float16"), Dtype(float16), )
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([128256, 793],"float16"), Dtype(float16), ) 	 101707008 	 1000 	 0.30742979049682617 	 0.002172231674194336 	 0.15707707405090332 	 1.5974044799804688e-05 	 0.30820155143737793 	 0.057175636291503906 	 0.15744781494140625 	 3.218650817871094e-05 	 combined
2025-07-24 16:21:15.563421 test begin: paddle.Tensor.cast(Tensor([152064, 669],"float16"), Dtype(float16), )
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([152064, 669],"float16"), Dtype(float16), ) 	 101730816 	 1000 	 0.31518983840942383 	 0.002064943313598633 	 0.1610705852508545 	 1.52587890625e-05 	 0.3149418830871582 	 0.05727362632751465 	 0.16088294982910156 	 3.0517578125e-05 	 combined
2025-07-24 16:21:20.045114 test begin: paddle.Tensor.cast(Tensor([24807, 4096],"float16"), Dtype(float16), )
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([24807, 4096],"float16"), Dtype(float16), ) 	 101609472 	 1000 	 0.31031227111816406 	 0.002134561538696289 	 0.2995924949645996 	 1.5497207641601562e-05 	 0.31038904190063477 	 0.05709409713745117 	 0.2558023929595947 	 4.291534423828125e-05 	 combined
2025-07-24 16:21:24.525166 test begin: paddle.Tensor.cast(Tensor([28351, 3584],"float16"), Dtype(float16), )
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([28351, 3584],"float16"), Dtype(float16), ) 	 101609984 	 1000 	 0.3148078918457031 	 0.002130746841430664 	 0.16085290908813477 	 1.4543533325195312e-05 	 0.3145279884338379 	 0.04581260681152344 	 0.16062498092651367 	 3.0040740966796875e-05 	 combined
2025-07-24 16:21:30.513316 test begin: paddle.Tensor.cast(Tensor([3584, 28351],"float16"), Dtype(float16), )
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([3584, 28351],"float16"), Dtype(float16), ) 	 101609984 	 1000 	 0.3148190975189209 	 0.0021262168884277344 	 0.16086935997009277 	 1.6450881958007812e-05 	 0.31444716453552246 	 0.057007551193237305 	 0.16061925888061523 	 3.910064697265625e-05 	 combined
2025-07-24 16:21:35.119663 test begin: paddle.Tensor.cast(Tensor([669, 152064],"float16"), Dtype(float16), )
[Prof] paddle.Tensor.cast 	 paddle.Tensor.cast(Tensor([669, 152064],"float16"), Dtype(float16), ) 	 101730816 	 1000 	 0.3152198791503906 	 0.0037164688110351562 	 0.1610698699951172 	 5.936622619628906e-05 	 0.31496596336364746 	 0.050395965576171875 	 0.16087579727172852 	 5.269050598144531e-05 	 combined
2025-07-24 16:21:39.729390 test begin: paddle.Tensor.ceil(Tensor([1, 50803201],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([1, 50803201],"float32"), ) 	 50803201 	 1000 	 0.29566168785095215 	 0.2978804111480713 	 0.2869865894317627 	 0.2875649929046631 	 0.13395214080810547 	 0.13420748710632324 	 0.07469654083251953 	 0.06562662124633789 	 
2025-07-24 16:21:42.280443 test begin: paddle.Tensor.ceil(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 1000 	 0.2957897186279297 	 0.29784703254699707 	 0.28707408905029297 	 0.28754591941833496 	 0.13385653495788574 	 0.13405179977416992 	 0.08308863639831543 	 0.06678295135498047 	 
2025-07-24 16:21:44.855827 test begin: paddle.Tensor.ceil(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 1000 	 0.2956976890563965 	 0.30129241943359375 	 0.2865641117095947 	 0.2874133586883545 	 0.13413023948669434 	 0.13407135009765625 	 0.08346271514892578 	 0.06597542762756348 	 
2025-07-24 16:21:47.379472 test begin: paddle.Tensor.ceil(Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([10, 5080321],"float32"), ) 	 50803210 	 1000 	 0.2957603931427002 	 0.2978324890136719 	 0.28698015213012695 	 0.2875964641571045 	 0.13396215438842773 	 0.13413262367248535 	 0.08368873596191406 	 0.0680842399597168 	 
2025-07-24 16:21:49.881557 test begin: paddle.Tensor.ceil(Tensor([25401601, 2],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([25401601, 2],"float32"), ) 	 50803202 	 1000 	 0.29567646980285645 	 0.2978479862213135 	 0.2869393825531006 	 0.2824852466583252 	 0.13396310806274414 	 0.13405752182006836 	 0.0836329460144043 	 0.06777000427246094 	 
2025-07-24 16:21:52.408563 test begin: paddle.Tensor.ceil(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 1000 	 0.2956728935241699 	 0.29899144172668457 	 0.2869894504547119 	 0.28711795806884766 	 0.1339585781097412 	 0.13416671752929688 	 0.08432388305664062 	 0.06617999076843262 	 
2025-07-24 16:21:54.931155 test begin: paddle.Tensor.ceil(Tensor([2540161, 20],"float32"), )
[Prof] paddle.Tensor.ceil 	 paddle.Tensor.ceil(Tensor([2540161, 20],"float32"), ) 	 50803220 	 1000 	 0.29564738273620605 	 0.299161434173584 	 0.2867472171783447 	 0.2865138053894043 	 0.13399362564086914 	 0.13409638404846191 	 0.06795549392700195 	 0.0678412914276123 	 
2025-07-24 16:21:57.433702 test begin: paddle.Tensor.chunk(Tensor([1034, 32, 64, 48],"float16"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([1034, 32, 64, 48],"float16"), 2, axis=1, ) 	 101646336 	 1000 	 0.4620497226715088 	 0.006960391998291016 	 0.44759488105773926 	 2.0265579223632812e-05 	 0.3103663921356201 	 0.45160460472106934 	 0.25520825386047363 	 0.36478710174560547 	 
2025-07-24 16:22:02.598655 test begin: paddle.Tensor.chunk(Tensor([128, 2068, 192],"float32"), 3, axis=-1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([128, 2068, 192],"float32"), 3, axis=-1, ) 	 50823168 	 1000 	 0.34391283988952637 	 0.008086681365966797 	 0.32874274253845215 	 2.5510787963867188e-05 	 0.30966901779174805 	 0.3086080551147461 	 0.2518584728240967 	 0.217606782913208 	 
2025-07-24 16:22:05.204422 test begin: paddle.Tensor.chunk(Tensor([512, 32, 130, 48],"float16"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([512, 32, 130, 48],"float16"), 2, axis=1, ) 	 102236160 	 1000 	 0.4593503475189209 	 0.007033824920654297 	 0.4452171325683594 	 2.2172927856445312e-05 	 0.3155689239501953 	 0.45380592346191406 	 0.26082396507263184 	 0.3694937229156494 	 
2025-07-24 16:22:10.225442 test begin: paddle.Tensor.chunk(Tensor([512, 32, 64, 49],"float32"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([512, 32, 64, 49],"float32"), 2, axis=1, ) 	 51380224 	 1000 	 0.3529210090637207 	 0.0069887638092041016 	 0.33909177780151367 	 1.8596649169921875e-05 	 0.3172430992126465 	 0.313321590423584 	 0.2628490924835205 	 0.2293226718902588 	 
2025-07-24 16:22:12.866751 test begin: paddle.Tensor.chunk(Tensor([512, 32, 64, 97],"float16"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([512, 32, 64, 97],"float16"), 2, axis=1, ) 	 101711872 	 1000 	 0.45804762840270996 	 0.006908416748046875 	 0.44397521018981934 	 2.5033950805664062e-05 	 0.3129463195800781 	 0.451418399810791 	 0.2584095001220703 	 0.36545801162719727 	 
2025-07-24 16:22:17.895060 test begin: paddle.Tensor.chunk(Tensor([512, 32, 65, 48],"float32"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([512, 32, 65, 48],"float32"), 2, axis=1, ) 	 51118080 	 1000 	 0.35210394859313965 	 0.007029533386230469 	 0.33832216262817383 	 1.811981201171875e-05 	 0.3155379295349121 	 0.3120856285095215 	 0.25941991806030273 	 0.22764849662780762 	 
2025-07-24 16:22:20.554844 test begin: paddle.Tensor.chunk(Tensor([517, 32, 64, 48],"float32"), 2, axis=1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([517, 32, 64, 48],"float32"), 2, axis=1, ) 	 50823168 	 1000 	 0.35024428367614746 	 0.007026195526123047 	 0.336381196975708 	 2.288818359375e-05 	 0.3094801902770996 	 0.3117678165435791 	 0.25475478172302246 	 0.22578954696655273 	 
2025-07-24 16:22:23.156480 test begin: paddle.Tensor.chunk(Tensor([85, 3136, 192],"float32"), 3, axis=-1, )
[Prof] paddle.Tensor.chunk 	 paddle.Tensor.chunk(Tensor([85, 3136, 192],"float32"), 3, axis=-1, ) 	 51179520 	 1000 	 0.34833717346191406 	 0.008015155792236328 	 0.33316493034362793 	 2.002716064453125e-05 	 0.3116755485534668 	 0.31095314025878906 	 0.2536160945892334 	 0.2190861701965332 	 
2025-07-24 16:22:25.797592 test begin: paddle.Tensor.clip(Tensor([1, 386, 65856, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([1, 386, 65856, 2],"float32"), 0, ) 	 50840832 	 1000 	 0.29564690589904785 	 0.2980217933654785 	 0.278627872467041 	 0.2854602336883545 	 0.45029425621032715 	 0.5957958698272705 	 0.3983795642852783 	 0.20313596725463867 	 
2025-07-24 16:22:29.139567 test begin: paddle.Tensor.clip(Tensor([1, 400, 63505, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([1, 400, 63505, 2],"float32"), 0, ) 	 50804000 	 1000 	 0.295412540435791 	 0.2977597713470459 	 0.27879977226257324 	 0.2851724624633789 	 0.4497039318084717 	 0.5952498912811279 	 0.39701294898986816 	 0.20293211936950684 	 
2025-07-24 16:22:32.462887 test begin: paddle.Tensor.clip(Tensor([1, 400, 65856, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([1, 400, 65856, 2],"float32"), 0, ) 	 52684800 	 1000 	 0.30652379989624023 	 0.33076047897338867 	 0.2899625301361084 	 0.2953019142150879 	 0.4661581516265869 	 0.6164543628692627 	 0.41286516189575195 	 0.21010684967041016 	 
2025-07-24 16:22:38.980196 test begin: paddle.Tensor.clip(Tensor([2100, 12096, 3],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([2100, 12096, 3],"float32"), 0, ) 	 76204800 	 1000 	 0.44085192680358887 	 0.68758225440979 	 0.4244873523712158 	 0.43060803413391113 	 0.6725757122039795 	 0.8844752311706543 	 0.6203141212463379 	 0.3015265464782715 	 
2025-07-24 16:22:45.534314 test begin: paddle.Tensor.clip(Tensor([2100, 12097, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([2100, 12097, 2],"float32"), 0, ) 	 50807400 	 1000 	 0.2952287197113037 	 0.2995421886444092 	 0.27884650230407715 	 0.28508448600769043 	 0.4495713710784912 	 0.5954537391662598 	 0.3965940475463867 	 0.2030174732208252 	 
2025-07-24 16:22:48.892866 test begin: paddle.Tensor.clip(Tensor([2101, 12096, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([2101, 12096, 2],"float32"), 0, ) 	 50827392 	 1000 	 0.29534268379211426 	 0.2978997230529785 	 0.27885961532592773 	 0.2853965759277344 	 0.44976186752319336 	 0.5957226753234863 	 0.3977084159851074 	 0.20311355590820312 	 
2025-07-24 16:22:52.195108 test begin: paddle.Tensor.clip(Tensor([4, 525, 12096, 3],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([4, 525, 12096, 3],"float32"), 0, ) 	 76204800 	 1000 	 0.4409003257751465 	 0.4440939426422119 	 0.42418766021728516 	 0.4310932159423828 	 0.6724965572357178 	 0.8846995830535889 	 0.6197640895843506 	 0.30164575576782227 	 
2025-07-24 16:22:57.219111 test begin: paddle.Tensor.clip(Tensor([4, 525, 12097, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([4, 525, 12097, 2],"float32"), 0, ) 	 50807400 	 1000 	 0.2952549457550049 	 0.2978534698486328 	 0.27844834327697754 	 0.28527355194091797 	 0.4495248794555664 	 0.5954737663269043 	 0.3977525234222412 	 0.20299053192138672 	 
2025-07-24 16:23:00.570331 test begin: paddle.Tensor.clip(Tensor([4, 526, 12096, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([4, 526, 12096, 2],"float32"), 0, ) 	 50899968 	 1000 	 0.2957634925842285 	 0.30170655250549316 	 0.2787659168243408 	 0.285459041595459 	 0.45078277587890625 	 0.5966525077819824 	 0.3979794979095459 	 0.20340275764465332 	 
2025-07-24 16:23:03.887396 test begin: paddle.Tensor.clip(Tensor([5, 525, 12096, 2],"float32"), 0, )
[Prof] paddle.Tensor.clip 	 paddle.Tensor.clip(Tensor([5, 525, 12096, 2],"float32"), 0, ) 	 63504000 	 1000 	 0.36778783798217773 	 0.37107396125793457 	 0.3495039939880371 	 0.3577592372894287 	 0.5607411861419678 	 0.7396390438079834 	 0.5075204372406006 	 0.25216150283813477 	 
2025-07-24 16:23:08.018394 test begin: paddle.Tensor.clone(Tensor([3544, 32, 896],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([3544, 32, 896],"bfloat16"), ) 	 101613568 	 1000 	 0.3102731704711914 	 0.31015658378601074 	 0.30132389068603516 	 0.2973012924194336 	 0.6161482334136963 	 0.4535863399505615 	 0.5595543384552002 	 0.3741137981414795 	 
2025-07-24 16:23:13.095824 test begin: paddle.Tensor.clone(Tensor([6017, 19, 896],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6017, 19, 896],"bfloat16"), ) 	 102433408 	 1000 	 0.31700944900512695 	 0.3155789375305176 	 0.16199731826782227 	 0.1611788272857666 	 0.6280050277709961 	 0.4573018550872803 	 0.32085418701171875 	 0.3651096820831299 	 
2025-07-24 16:23:18.117628 test begin: paddle.Tensor.clone(Tensor([6017, 32, 528],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6017, 32, 528],"bfloat16"), ) 	 101663232 	 1000 	 0.31464076042175293 	 0.313021183013916 	 0.16076874732971191 	 0.15988755226135254 	 0.6228513717651367 	 0.45381927490234375 	 0.318234920501709 	 0.3628041744232178 	 
2025-07-24 16:23:23.259353 test begin: paddle.Tensor.clone(Tensor([6036, 19, 896],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6036, 19, 896],"bfloat16"), ) 	 102756864 	 1000 	 0.3180053234100342 	 0.3163323402404785 	 0.16248559951782227 	 0.16158008575439453 	 0.6295342445373535 	 0.4586656093597412 	 0.3216521739959717 	 0.37939929962158203 	 
2025-07-24 16:23:28.307355 test begin: paddle.Tensor.clone(Tensor([6036, 32, 527],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6036, 32, 527],"bfloat16"), ) 	 101791104 	 1000 	 0.315171480178833 	 0.31359219551086426 	 0.1610708236694336 	 0.1601409912109375 	 0.6237967014312744 	 0.45435571670532227 	 0.3187255859375 	 0.3625602722167969 	 
2025-07-24 16:23:33.440540 test begin: paddle.Tensor.clone(Tensor([6078, 19, 896],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6078, 19, 896],"bfloat16"), ) 	 103471872 	 1000 	 0.31356382369995117 	 1.748408317565918 	 0.16023826599121094 	 0.16271424293518066 	 0.6185786724090576 	 0.4617607593536377 	 0.3160369396209717 	 0.3804006576538086 	 
2025-07-24 16:23:41.165482 test begin: paddle.Tensor.clone(Tensor([6078, 32, 523],"bfloat16"), )
[Prof] paddle.Tensor.clone 	 paddle.Tensor.clone(Tensor([6078, 32, 523],"bfloat16"), ) 	 101721408 	 1000 	 0.314255952835083 	 0.3133819103240967 	 0.16058921813964844 	 0.16005945205688477 	 0.6246705055236816 	 0.45399951934814453 	 0.31914782524108887 	 0.3750457763671875 	 
2025-07-24 16:23:47.700614 test begin: paddle.Tensor.conj(Tensor([10, 2540161],"float64"), )
[Prof] paddle.Tensor.conj 	 paddle.Tensor.conj(Tensor([10, 2540161],"float64"), ) 	 25401610 	 1000 	 0.29805469512939453 	 0.0016951560974121094 	 0.28972601890563965 	 1.52587890625e-05 	 0.29758310317993164 	 0.051069021224975586 	 0.2493760585784912 	 6.532669067382812e-05 	 
2025-07-24 16:23:51.103540 test begin: paddle.Tensor.conj(Tensor([1270081, 20],"float64"), )
[Prof] paddle.Tensor.conj 	 paddle.Tensor.conj(Tensor([1270081, 20],"float64"), ) 	 25401620 	 1000 	 0.2980070114135742 	 0.0017137527465820312 	 0.289564847946167 	 1.3828277587890625e-05 	 0.2975316047668457 	 0.046060800552368164 	 0.24927544593811035 	 3.552436828613281e-05 	 
2025-07-24 16:23:52.785986 test begin: paddle.Tensor.cos(Tensor([131072, 388],"float32"), )
[Prof] paddle.Tensor.cos 	 paddle.Tensor.cos(Tensor([131072, 388],"float32"), ) 	 50855936 	 1000 	 0.29561758041381836 	 0.2985410690307617 	 0.28655004501342773 	 0.2882852554321289 	 0.4505331516265869 	 1.041867733001709 	 0.3983793258666992 	 0.354961633682251 	 
2025-07-24 16:23:56.518270 test begin: paddle.Tensor.cos(Tensor([3175201, 16],"float32"), )
[Prof] paddle.Tensor.cos 	 paddle.Tensor.cos(Tensor([3175201, 16],"float32"), ) 	 50803216 	 1000 	 0.29543519020080566 	 0.3005983829498291 	 0.2864711284637451 	 0.28809237480163574 	 0.4499526023864746 	 1.040740728378296 	 0.397266149520874 	 0.35454392433166504 	 
2025-07-24 16:24:00.303187 test begin: paddle.Tensor.cos(Tensor([32768, 1551],"float32"), )
[Prof] paddle.Tensor.cos 	 paddle.Tensor.cos(Tensor([32768, 1551],"float32"), ) 	 50823168 	 1000 	 0.2953472137451172 	 0.2982313632965088 	 0.2863919734954834 	 0.28824758529663086 	 0.4501361846923828 	 1.0412166118621826 	 0.3971264362335205 	 0.35473132133483887 	 
2025-07-24 16:24:04.021599 test begin: paddle.Tensor.cos(Tensor([396901, 128],"float32"), )
[Prof] paddle.Tensor.cos 	 paddle.Tensor.cos(Tensor([396901, 128],"float32"), ) 	 50803328 	 1000 	 0.2953176498413086 	 0.2980945110321045 	 0.28348636627197266 	 0.28799867630004883 	 0.44974470138549805 	 1.0408546924591064 	 0.39754486083984375 	 0.3545703887939453 	 
2025-07-24 16:24:07.802615 test begin: paddle.Tensor.cumprod(Tensor([25401601],"float64"), -1, )
[Prof] paddle.Tensor.cumprod 	 paddle.Tensor.cumprod(Tensor([25401601],"float64"), -1, ) 	 25401601 	 1000 	 0.32699060440063477 	 0.3272976875305176 	 0.1670544147491455 	 0.16718006134033203 	 2.799849033355713 	 2.059252977371216 	 0.00030684471130371094 	 0.001312255859375 	 
2025-07-24 16:24:14.489675 test begin: paddle.Tensor.cumprod(Tensor([50803201],"float32"), -1, )
[Prof] paddle.Tensor.cumprod 	 paddle.Tensor.cumprod(Tensor([50803201],"float32"), -1, ) 	 50803201 	 1000 	 0.3270905017852783 	 0.329620361328125 	 0.16715359687805176 	 0.16837310791015625 	 3.132340908050537 	 2.118882656097412 	 0.0003154277801513672 	 0.0013203620910644531 	 
2025-07-24 16:24:22.053735 test begin: paddle.Tensor.cumsum(Tensor([1, 144, 352801],"float32"), 1, )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([1, 144, 352801],"float32"), 1, ) 	 50803344 	 1000 	 1.2571501731872559 	 0.3538529872894287 	 0.42833471298217773 	 0.3433094024658203 	 6.404057264328003 	 0.9760556221008301 	 1.309250831604004 	 0.3324718475341797 	 
2025-07-24 16:24:32.769615 test begin: paddle.Tensor.cumsum(Tensor([1, 144, 352801],"float32"), 2, )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([1, 144, 352801],"float32"), 2, ) 	 50803344 	 1000 	 0.8916704654693604 	 1.104308843612671 	 0.8820703029632568 	 1.0896532535552979 	 1.6966919898986816 	 1.641617774963379 	 0.5781347751617432 	 0.5593762397766113 	 
2025-07-24 16:24:41.037631 test begin: paddle.Tensor.cumsum(Tensor([1, 254017, 200],"float32"), 1, )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([1, 254017, 200],"float32"), 1, ) 	 50803400 	 1000 	 1.4287426471710205 	 124.2853331565857 	 0.4868760108947754 	 124.27455353736877 	 2.152737617492676 	 122.22513008117676 	 0.43997836112976074 	 41.679020166397095 	 
2025-07-24 16:28:53.109583 test begin: paddle.Tensor.cumsum(Tensor([1, 254017, 200],"float32"), 2, )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([1, 254017, 200],"float32"), 2, ) 	 50803400 	 1000 	 0.40534520149230957 	 2.8184878826141357 	 0.39548158645629883 	 2.8077282905578613 	 4.153340578079224 	 3.440322160720825 	 1.4144325256347656 	 1.1727867126464844 	 
2025-07-24 16:29:05.627408 test begin: paddle.Tensor.cumsum(Tensor([1765, 144, 200],"float32"), 1, )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([1765, 144, 200],"float32"), 1, ) 	 50832000 	 1000 	 1.287520170211792 	 0.37566637992858887 	 0.4387047290802002 	 0.35089731216430664 	 6.431645393371582 	 0.9926791191101074 	 1.3150150775909424 	 0.3381471633911133 	 
2025-07-24 16:29:17.565868 test begin: paddle.Tensor.cumsum(Tensor([1765, 144, 200],"float32"), 2, )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([1765, 144, 200],"float32"), 2, ) 	 50832000 	 1000 	 0.4047889709472656 	 2.8197927474975586 	 0.39481043815612793 	 2.809157609939575 	 4.153604984283447 	 3.444218635559082 	 1.4145097732543945 	 1.1742920875549316 	 
2025-07-24 16:29:30.078269 test begin: paddle.Tensor.cumsum(Tensor([211681, 120],"int64"), )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([211681, 120],"int64"), ) 	 25401720 	 1000 	 0.345822811126709 	 0.3261265754699707 	 2.574920654296875e-05 	 0.16660499572753906 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:29:31.957077 test begin: paddle.Tensor.cumsum(Tensor([300, 84673],"int64"), )
[Prof] paddle.Tensor.cumsum 	 paddle.Tensor.cumsum(Tensor([300, 84673],"int64"), ) 	 25401900 	 1000 	 0.346510648727417 	 0.3259406089782715 	 2.4080276489257812e-05 	 0.16648244857788086 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:29:33.860207 test begin: paddle.Tensor.detach(Tensor([100352, 1013],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([100352, 1013],"bfloat16"), ) 	 101656576 	 1000 	 0.0007252693176269531 	 0.005034923553466797 	 5.9604644775390625e-06 	 0.00011086463928222656 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:29:39.329354 test begin: paddle.Tensor.detach(Tensor([1013, 100352],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([1013, 100352],"bfloat16"), ) 	 101656576 	 1000 	 0.0007073879241943359 	 0.002911806106567383 	 1.0013580322265625e-05 	 1.7642974853515625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:29:42.780738 test begin: paddle.Tensor.detach(Tensor([12404, 8192],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([12404, 8192],"bfloat16"), ) 	 101613568 	 1000 	 0.0007200241088867188 	 0.003525257110595703 	 6.4373016357421875e-06 	 4.76837158203125e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:29:46.042707 test begin: paddle.Tensor.detach(Tensor([1772, 57344],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([1772, 57344],"bfloat16"), ) 	 101613568 	 1000 	 0.0007503032684326172 	 0.002989053726196289 	 1.1682510375976562e-05 	 1.6927719116210938e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:29:49.325729 test begin: paddle.Tensor.detach(Tensor([8192, 12404],"bfloat16"), )
[Prof] paddle.Tensor.detach 	 paddle.Tensor.detach(Tensor([8192, 12404],"bfloat16"), ) 	 101613568 	 1000 	 0.0007789134979248047 	 0.0030422210693359375 	 1.4066696166992188e-05 	 1.6927719116210938e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:29:52.716684 test begin: paddle.Tensor.diag_embed(Tensor([1, 25401601, 2],"float32"), )
[Prof] paddle.Tensor.diag_embed 	 paddle.Tensor.diag_embed(Tensor([1, 25401601, 2],"float32"), ) 	 50803202 	 1000 	 1.7679054737091064 	 1.0142667293548584 	 4.410743713378906e-05 	 0.518254280090332 	 None 	 None 	 None 	 None 	 
2025-07-24 16:29:56.334733 test begin: paddle.Tensor.diag_embed(Tensor([25401601, 1, 2],"float32"), )
[Prof] paddle.Tensor.diag_embed 	 paddle.Tensor.diag_embed(Tensor([25401601, 1, 2],"float32"), ) 	 50803202 	 1000 	 1.7659032344818115 	 1.0148625373840332 	 7.390975952148438e-05 	 0.5181405544281006 	 None 	 None 	 None 	 None 	 
2025-07-24 16:29:59.942104 test begin: paddle.Tensor.diagonal(Tensor([2, 25401601],"float32"), axis1=-2, axis2=-1, )
[Prof] paddle.Tensor.diagonal 	 paddle.Tensor.diagonal(Tensor([2, 25401601],"float32"), axis1=-2, axis2=-1, ) 	 50803202 	 1000 	 0.003863096237182617 	 0.004843950271606445 	 9.775161743164062e-06 	 1.8596649169921875e-05 	 0.14889073371887207 	 0.13778281211853027 	 0.07608675956726074 	 0.06380486488342285 	 
2025-07-24 16:30:01.104366 test begin: paddle.Tensor.diagonal(Tensor([25401601, 2],"float32"), axis1=-2, axis2=-1, )
[Prof] paddle.Tensor.diagonal 	 paddle.Tensor.diagonal(Tensor([25401601, 2],"float32"), axis1=-2, axis2=-1, ) 	 50803202 	 1000 	 0.003895998001098633 	 0.0047757625579833984 	 1.2159347534179688e-05 	 1.6927719116210938e-05 	 0.1486356258392334 	 0.1377699375152588 	 0.07588863372802734 	 0.06232476234436035 	 
2025-07-24 16:30:02.216013 test begin: paddle.Tensor.diagonal(Tensor([3, 8467201],"float64"), axis1=-2, axis2=-1, )
[Prof] paddle.Tensor.diagonal 	 paddle.Tensor.diagonal(Tensor([3, 8467201],"float64"), axis1=-2, axis2=-1, ) 	 25401603 	 1000 	 0.003851652145385742 	 0.004797458648681641 	 7.867813110351562e-06 	 1.71661376953125e-05 	 0.14911723136901855 	 0.138291597366333 	 0.07614326477050781 	 0.061955928802490234 	 
2025-07-24 16:30:03.038636 test begin: paddle.Tensor.diagonal(Tensor([8467201, 3],"float64"), axis1=-2, axis2=-1, )
[Prof] paddle.Tensor.diagonal 	 paddle.Tensor.diagonal(Tensor([8467201, 3],"float64"), axis1=-2, axis2=-1, ) 	 25401603 	 1000 	 0.003881216049194336 	 0.0048656463623046875 	 5.9604644775390625e-06 	 1.8358230590820312e-05 	 0.1493053436279297 	 0.13829851150512695 	 0.07625484466552734 	 0.06410837173461914 	 
2025-07-24 16:30:03.907990 test begin: paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 1000 	 0.9449985027313232 	 0.2615976333618164 	 0.32196569442749023 	 0.24355149269104004 	 None 	 None 	 None 	 None 	 
2025-07-24 16:30:05.681319 test begin: paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=-2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=-2, ) 	 25401664 	 1000 	 0.9448444843292236 	 0.26440930366516113 	 0.3219289779663086 	 0.24106407165527344 	 None 	 None 	 None 	 None 	 
2025-07-24 16:30:07.434398 test begin: paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=2, ) 	 25401664 	 1000 	 0.9448330402374268 	 0.2626354694366455 	 0.3219149112701416 	 0.23314547538757324 	 None 	 None 	 None 	 None 	 
2025-07-24 16:30:09.168575 test begin: paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 1000 	 0.8705580234527588 	 0.26148104667663574 	 0.29658055305480957 	 0.24343228340148926 	 None 	 None 	 None 	 None 	 
2025-07-24 16:30:10.828438 test begin: paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=-2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=-2, ) 	 25401664 	 1000 	 0.8706424236297607 	 0.26147961616516113 	 0.2966022491455078 	 0.24124789237976074 	 None 	 None 	 None 	 None 	 
2025-07-24 16:30:12.501019 test begin: paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=2, ) 	 25401664 	 1000 	 0.8706350326538086 	 0.26293349266052246 	 0.29660534858703613 	 0.24236035346984863 	 None 	 None 	 None 	 None 	 
2025-07-24 16:30:14.208694 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 1000 	 0.8705604076385498 	 0.2614588737487793 	 0.29656410217285156 	 0.24339604377746582 	 None 	 None 	 None 	 None 	 
2025-07-24 16:30:15.869755 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=-2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=-2, ) 	 25401664 	 1000 	 1.070333480834961 	 0.5326085090637207 	 0.36465001106262207 	 0.2801554203033447 	 None 	 None 	 None 	 None 	 
2025-07-24 16:30:19.433928 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=2, ) 	 25401664 	 1000 	 1.0704317092895508 	 0.3067202568054199 	 0.36473560333251953 	 0.2802317142486572 	 None 	 None 	 None 	 None 	 
2025-07-24 16:30:22.353154 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 1000 	 1.0701889991760254 	 0.3021976947784424 	 0.364652156829834 	 0.2813131809234619 	 None 	 None 	 None 	 None 	 
2025-07-24 16:30:24.252154 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=-2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=-2, ) 	 25401664 	 1000 	 0.806997537612915 	 0.26282334327697754 	 0.27489805221557617 	 0.24384140968322754 	 None 	 None 	 None 	 None 	 
2025-07-24 16:30:25.862339 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=2, )
[Prof] paddle.Tensor.diff 	 paddle.Tensor.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=2, ) 	 25401664 	 1000 	 0.8070428371429443 	 0.26280736923217773 	 0.2749762535095215 	 0.24368929862976074 	 None 	 None 	 None 	 None 	 
2025-07-24 16:30:27.474916 test begin: paddle.Tensor.digamma(Tensor([4, 6350401],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([4, 6350401],"float64"), ) 	 25401604 	 1000 	 1.1688740253448486 	 1.829361915588379 	 1.1604914665222168 	 1.1309239864349365 	 8.560898780822754 	 1.084538459777832 	 8.508629322052002 	 0.5541098117828369 	 
2025-07-24 16:30:42.825983 test begin: paddle.Tensor.digamma(Tensor([453601, 7, 8],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([453601, 7, 8],"float64"), ) 	 25401656 	 1000 	 1.168769359588623 	 1.1416900157928467 	 1.160370111465454 	 1.1322402954101562 	 8.559807300567627 	 1.0846154689788818 	 8.509212255477905 	 0.5541684627532959 	 
2025-07-24 16:30:55.928363 test begin: paddle.Tensor.digamma(Tensor([45361, 7, 8, 10],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([45361, 7, 8, 10],"float64"), ) 	 25402160 	 1000 	 1.1689949035644531 	 1.19303560256958 	 1.1567988395690918 	 1.1763088703155518 	 8.559726238250732 	 1.084702968597412 	 8.508803844451904 	 0.5541219711303711 	 
2025-07-24 16:31:10.291442 test begin: paddle.Tensor.digamma(Tensor([5, 635041, 8],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5, 635041, 8],"float64"), ) 	 25401640 	 1000 	 1.1687686443328857 	 1.143878698348999 	 1.1603922843933105 	 1.131075143814087 	 8.55964994430542 	 1.0845153331756592 	 8.50329303741455 	 0.5540964603424072 	 
2025-07-24 16:31:24.923181 test begin: paddle.Tensor.digamma(Tensor([5, 63505, 8, 10],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5, 63505, 8, 10],"float64"), ) 	 25402000 	 1000 	 1.1687700748443604 	 1.3665060997009277 	 1.1602623462677002 	 1.1309823989868164 	 8.559696912765503 	 1.0846006870269775 	 8.507426261901855 	 0.5541102886199951 	 
2025-07-24 16:31:40.554508 test begin: paddle.Tensor.digamma(Tensor([5, 7, 725761],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5, 7, 725761],"float64"), ) 	 25401635 	 1000 	 1.1685960292816162 	 1.1414990425109863 	 1.1601204872131348 	 1.132021188735962 	 8.559363842010498 	 1.084470510482788 	 8.509137392044067 	 0.5541293621063232 	 
2025-07-24 16:31:53.677591 test begin: paddle.Tensor.digamma(Tensor([5, 7, 72577, 10],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5, 7, 72577, 10],"float64"), ) 	 25401950 	 1000 	 1.168663501739502 	 1.1416046619415283 	 1.160179853439331 	 1.131819725036621 	 8.559464693069458 	 1.084407091140747 	 8.508951425552368 	 0.5540585517883301 	 
2025-07-24 16:32:06.758812 test begin: paddle.Tensor.digamma(Tensor([5, 7, 8, 90721],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5, 7, 8, 90721],"float64"), ) 	 25401880 	 1000 	 1.1687548160552979 	 1.1414952278137207 	 1.1602647304534912 	 1.1319005489349365 	 8.561449766159058 	 1.0850789546966553 	 8.508599996566772 	 0.5546829700469971 	 
2025-07-24 16:32:19.848254 test begin: paddle.Tensor.digamma(Tensor([5080321, 5],"float64"), )
[Prof] paddle.Tensor.digamma 	 paddle.Tensor.digamma(Tensor([5080321, 5],"float64"), ) 	 25401605 	 1000 	 1.1686303615570068 	 1.5341618061065674 	 1.160172700881958 	 1.5231878757476807 	 8.566141605377197 	 1.0864899158477783 	 8.515489339828491 	 0.5552372932434082 	 
2025-07-24 16:32:34.238498 test begin: paddle.Tensor.dim(Tensor([111616, 911],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([111616, 911],"bfloat16"), ) 	 101682176 	 1000 	 0.0006899833679199219 	 0.0016222000122070312 	 6.9141387939453125e-06 	 1.71661376953125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 16:32:37.605764 test begin: paddle.Tensor.dim(Tensor([12404, 8192],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([12404, 8192],"bfloat16"), ) 	 101613568 	 1000 	 0.0006864070892333984 	 0.0016207695007324219 	 1.0728836059570312e-05 	 1.811981201171875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 16:32:40.618430 test begin: paddle.Tensor.dim(Tensor([14176, 7168],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([14176, 7168],"bfloat16"), ) 	 101613568 	 1000 	 0.000690460205078125 	 0.0016057491302490234 	 5.9604644775390625e-06 	 1.4543533325195312e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 16:32:42.237835 test begin: paddle.Tensor.dim(Tensor([7168, 14176],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([7168, 14176],"bfloat16"), ) 	 101613568 	 1000 	 0.0006825923919677734 	 0.0015819072723388672 	 5.7220458984375e-06 	 1.4543533325195312e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 16:32:43.870107 test begin: paddle.Tensor.dim(Tensor([911, 111616],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([911, 111616],"bfloat16"), ) 	 101682176 	 1000 	 0.0006954669952392578 	 0.0015866756439208984 	 6.4373016357421875e-06 	 1.52587890625e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 16:32:45.500107 test begin: paddle.Tensor.dim(Tensor([95872, 1060],"bfloat16"), )
[Prof] paddle.Tensor.dim 	 paddle.Tensor.dim(Tensor([95872, 1060],"bfloat16"), ) 	 101624320 	 1000 	 0.00069427490234375 	 0.001569986343383789 	 1.0728836059570312e-05 	 1.5735626220703125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 16:32:47.136346 test begin: paddle.Tensor.dot(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
Warning: The core code of paddle.Tensor.dot is too complex.
[Prof] paddle.Tensor.dot 	 paddle.Tensor.dot(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 1000 	 0.2961616516113281 	 0.2934238910675049 	 0.28713083267211914 	 0.14991021156311035 	 0.7096316814422607 	 0.6037142276763916 	 0.36258745193481445 	 0.3084430694580078 	 
2025-07-24 16:32:50.758025 test begin: paddle.Tensor.equal(Tensor([128, 198451],"int64"), Tensor([128, 198451],"int64"), )
[Prof] paddle.Tensor.equal 	 paddle.Tensor.equal(Tensor([128, 198451],"int64"), Tensor([128, 198451],"int64"), ) 	 50803456 	 1000 	 0.3090085983276367 	 0.3156256675720215 	 0.2981445789337158 	 0.3015720844268799 	 None 	 None 	 None 	 None 	 
2025-07-24 16:32:52.183542 test begin: paddle.Tensor.equal(Tensor([198451, 128],"int64"), Tensor([198451, 128],"int64"), )
[Prof] paddle.Tensor.equal 	 paddle.Tensor.equal(Tensor([198451, 128],"int64"), Tensor([198451, 128],"int64"), ) 	 50803456 	 1000 	 0.3089931011199951 	 0.3131217956542969 	 0.2983710765838623 	 0.30106067657470703 	 None 	 None 	 None 	 None 	 
2025-07-24 16:32:53.608922 test begin: paddle.Tensor.equal(Tensor([2, 12700801],"int64"), 3, )
[Prof] paddle.Tensor.equal 	 paddle.Tensor.equal(Tensor([2, 12700801],"int64"), 3, ) 	 25401602 	 1000 	 0.17803430557250977 	 0.18344759941101074 	 0.0909426212310791 	 0.15394306182861328 	 None 	 None 	 None 	 None 	 
2025-07-24 16:32:54.380885 test begin: paddle.Tensor.equal(Tensor([2540161, 10],"int64"), 3, )
[Prof] paddle.Tensor.equal 	 paddle.Tensor.equal(Tensor([2540161, 10],"int64"), 3, ) 	 25401610 	 1000 	 0.17802691459655762 	 0.1682298183441162 	 0.09094882011413574 	 0.15398526191711426 	 None 	 None 	 None 	 None 	 
2025-07-24 16:32:55.151006 test begin: paddle.Tensor.equal_all(Tensor([25401601],"int64"), Tensor([25401601],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([25401601],"int64"), Tensor([25401601],"int64"), ) 	 50803202 	 1000 	 0.34189701080322266 	 0.38472938537597656 	 0.11628246307373047 	 6.985664367675781e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 16:32:56.748290 test begin: paddle.Tensor.equal_all(Tensor([25401601],"int64"), Tensor([8],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([25401601],"int64"), Tensor([8],"int64"), ) 	 25401609 	 1000 	 0.021592140197753906 	 0.0030655860900878906 	 2.0742416381835938e-05 	 5.626678466796875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 16:32:57.187330 test begin: paddle.Tensor.equal_all(Tensor([8, 3175201],"int64"), Tensor([8, 3175201],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8, 3175201],"int64"), Tensor([8, 3175201],"int64"), ) 	 50803216 	 1000 	 0.34184789657592773 	 0.3795320987701416 	 0.11629629135131836 	 6.747245788574219e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 16:32:58.740492 test begin: paddle.Tensor.equal_all(Tensor([8, 3175201],"int64"), Tensor([8, 3],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8, 3175201],"int64"), Tensor([8, 3],"int64"), ) 	 25401632 	 1000 	 0.017046451568603516 	 0.0029609203338623047 	 8.106231689453125e-06 	 2.0503997802734375e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 16:32:59.168506 test begin: paddle.Tensor.equal_all(Tensor([8, 3],"int64"), Tensor([8, 3175201],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8, 3],"int64"), Tensor([8, 3175201],"int64"), ) 	 25401632 	 1000 	 0.01711130142211914 	 0.002783060073852539 	 1.0013580322265625e-05 	 1.4066696166992188e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 16:32:59.597616 test begin: paddle.Tensor.equal_all(Tensor([8, 3],"int64"), Tensor([8467201, 3],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8, 3],"int64"), Tensor([8467201, 3],"int64"), ) 	 25401627 	 1000 	 0.01697540283203125 	 0.0027761459350585938 	 7.867813110351562e-06 	 1.430511474609375e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 16:33:00.025357 test begin: paddle.Tensor.equal_all(Tensor([8467201, 3],"int64"), Tensor([8, 3],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8467201, 3],"int64"), Tensor([8, 3],"int64"), ) 	 25401627 	 1000 	 0.017045974731445312 	 0.0028090476989746094 	 9.298324584960938e-06 	 1.52587890625e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 16:33:00.453753 test begin: paddle.Tensor.equal_all(Tensor([8467201, 3],"int64"), Tensor([8467201, 3],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8467201, 3],"int64"), Tensor([8467201, 3],"int64"), ) 	 50803206 	 1000 	 0.3418459892272949 	 0.3779327869415283 	 0.11625528335571289 	 7.271766662597656e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 16:33:02.048373 test begin: paddle.Tensor.equal_all(Tensor([8],"int64"), Tensor([25401601],"int64"), )
[Prof] paddle.Tensor.equal_all 	 paddle.Tensor.equal_all(Tensor([8],"int64"), Tensor([25401601],"int64"), ) 	 25401609 	 1000 	 0.017102479934692383 	 0.002778291702270508 	 1.6927719116210938e-05 	 1.430511474609375e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 16:33:02.477674 test begin: paddle.Tensor.erfinv(x=Tensor([211681, 2, 3, 5, 4],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([211681, 2, 3, 5, 4],"float64"), ) 	 25401720 	 1000 	 0.3344600200653076 	 0.31309962272644043 	 0.3258936405181885 	 0.30333614349365234 	 0.44802308082580566 	 1.6421277523040771 	 0.39635348320007324 	 0.3356904983520508 	 
2025-07-24 16:33:06.344848 test begin: paddle.Tensor.erfinv(x=Tensor([4, 105841, 3, 5, 4],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 105841, 3, 5, 4],"float64"), ) 	 25401840 	 1000 	 0.3365516662597656 	 0.31198883056640625 	 0.32808804512023926 	 0.3022043704986572 	 0.44785499572753906 	 1.6422035694122314 	 0.3961162567138672 	 0.33574962615966797 	 
2025-07-24 16:33:10.131346 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 158761, 5, 4],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 158761, 5, 4],"float64"), ) 	 25401760 	 1000 	 0.3375082015991211 	 0.3137388229370117 	 0.3290531635284424 	 0.30283117294311523 	 0.44736313819885254 	 1.6419363021850586 	 0.39562058448791504 	 0.33565258979797363 	 
2025-07-24 16:33:13.937629 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 1058401],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 1058401],"float64"), ) 	 25401624 	 1000 	 0.33499574661254883 	 0.3115267753601074 	 0.32647204399108887 	 0.301835298538208 	 0.44718003273010254 	 1.6419987678527832 	 0.39527273178100586 	 0.335707426071167 	 
2025-07-24 16:33:17.721576 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 264601, 4],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 264601, 4],"float64"), ) 	 25401696 	 1000 	 0.33511829376220703 	 0.3117203712463379 	 0.32667016983032227 	 0.3019850254058838 	 0.4472956657409668 	 1.6420042514801025 	 0.39545440673828125 	 0.33566784858703613 	 
2025-07-24 16:33:21.500617 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 5, 211681],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 3, 5, 211681],"float64"), ) 	 25401720 	 1000 	 0.33467793464660645 	 0.3113679885864258 	 0.32616710662841797 	 0.3017313480377197 	 0.4478886127471924 	 1.6421585083007812 	 0.3959822654724121 	 0.33571362495422363 	 
2025-07-24 16:33:25.302324 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 3175201],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 3175201],"float64"), ) 	 25401608 	 1000 	 0.33490967750549316 	 0.3156886100769043 	 0.3263516426086426 	 0.30228424072265625 	 0.44716405868530273 	 1.6420373916625977 	 0.3956336975097656 	 0.3356921672821045 	 
2025-07-24 16:33:29.089636 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2, 635041, 5],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2, 635041, 5],"float64"), ) 	 25401640 	 1000 	 0.3343019485473633 	 0.31528568267822266 	 0.32580065727233887 	 0.29808497428894043 	 0.4472324848175049 	 1.642122507095337 	 0.395214319229126 	 0.3356814384460449 	 
2025-07-24 16:33:32.882135 test begin: paddle.Tensor.erfinv(x=Tensor([4, 2116801, 3],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 2116801, 3],"float64"), ) 	 25401612 	 1000 	 0.3348212242126465 	 1.4585511684417725 	 0.32623291015625 	 0.29851651191711426 	 0.44721460342407227 	 1.6422266960144043 	 0.392974853515625 	 0.335798978805542 	 
2025-07-24 16:33:40.128879 test begin: paddle.Tensor.erfinv(x=Tensor([4, 423361, 3, 5],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4, 423361, 3, 5],"float64"), ) 	 25401660 	 1000 	 0.3340284824371338 	 0.3131120204925537 	 0.32545995712280273 	 0.2992408275604248 	 0.4472675323486328 	 1.642204999923706 	 0.3923814296722412 	 0.33583927154541016 	 
2025-07-24 16:33:45.980048 test begin: paddle.Tensor.erfinv(x=Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([4233601, 2, 3],"float64"), ) 	 25401606 	 1000 	 0.3324086666107178 	 0.30902576446533203 	 0.3235142230987549 	 0.29937243461608887 	 0.4472341537475586 	 1.6420421600341797 	 0.3954031467437744 	 0.3356971740722656 	 
2025-07-24 16:33:49.771581 test begin: paddle.Tensor.erfinv(x=Tensor([846721, 2, 3, 5],"float64"), )
[Prof] paddle.Tensor.erfinv 	 paddle.Tensor.erfinv(x=Tensor([846721, 2, 3, 5],"float64"), ) 	 25401630 	 1000 	 0.33348584175109863 	 0.3116135597229004 	 0.3249835968017578 	 0.3020195960998535 	 0.4472832679748535 	 1.641998052597046 	 0.39545369148254395 	 0.33568572998046875 	 
2025-07-24 16:33:53.562792 test begin: paddle.Tensor.exp(Tensor([1000000, 26],"float64"), )
[Prof] paddle.Tensor.exp 	 paddle.Tensor.exp(Tensor([1000000, 26],"float64"), ) 	 26000000 	 1000 	 0.30574750900268555 	 0.3074162006378174 	 0.29738759994506836 	 0.2968904972076416 	 0.458998441696167 	 0.4547080993652344 	 0.4057776927947998 	 0.3885223865509033 	 
2025-07-24 16:33:56.232228 test begin: paddle.Tensor.exp(Tensor([2540161, 20],"float32"), )
[Prof] paddle.Tensor.exp 	 paddle.Tensor.exp(Tensor([2540161, 20],"float32"), ) 	 50803220 	 1000 	 0.2955913543701172 	 0.29787182807922363 	 0.2869875431060791 	 0.2875404357910156 	 0.449282169342041 	 0.4465198516845703 	 0.3962395191192627 	 0.37899136543273926 	 
2025-07-24 16:33:59.354366 test begin: paddle.Tensor.exp(Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.exp 	 paddle.Tensor.exp(Tensor([50803201],"float32"), ) 	 50803201 	 1000 	 0.29556894302368164 	 0.2977256774902344 	 0.28684449195861816 	 0.28766679763793945 	 0.4491250514984131 	 0.4465172290802002 	 0.3967568874359131 	 0.3814668655395508 	 
2025-07-24 16:34:02.468899 test begin: paddle.Tensor.exp(Tensor([6350401, 4],"float64"), )
[Prof] paddle.Tensor.exp 	 paddle.Tensor.exp(Tensor([6350401, 4],"float64"), ) 	 25401604 	 1000 	 0.2995178699493408 	 0.30034947395324707 	 0.2910952568054199 	 0.2901468276977539 	 0.4475252628326416 	 0.44442248344421387 	 0.3953409194946289 	 0.3787689208984375 	 
2025-07-24 16:34:05.010526 test begin: paddle.Tensor.exp(Tensor([64, 793801],"float32"), )
[Prof] paddle.Tensor.exp 	 paddle.Tensor.exp(Tensor([64, 793801],"float32"), ) 	 50803264 	 1000 	 0.2955284118652344 	 0.2977457046508789 	 0.28687357902526855 	 0.2876102924346924 	 0.44913172721862793 	 0.44646310806274414 	 0.39672040939331055 	 0.3806765079498291 	 
2025-07-24 16:34:08.112968 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 266, 477, 401],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 266, 477, 401],"float32"), ) 	 50879683 	 1000 	 0.13506841659545898 	 0.004281282424926758 	 0.12371993064880371 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 16:34:10.021395 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 283, 466, 386],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 283, 466, 386],"float32"), ) 	 50904909 	 1000 	 0.13514113426208496 	 0.0043795108795166016 	 0.12369990348815918 	 3.600120544433594e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 16:34:11.948156 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 299, 391, 436],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 299, 391, 436],"float32"), ) 	 50972325 	 1000 	 0.1352081298828125 	 0.0042607784271240234 	 0.12378430366516113 	 1.7881393432617188e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 16:34:13.908750 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 38841, 436],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 38841, 436],"float32"), ) 	 50804029 	 1000 	 0.13479995727539062 	 0.00428318977355957 	 0.12342572212219238 	 1.6689300537109375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 16:34:15.818772 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 391, 43311],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 391, 43311],"float32"), ) 	 50803804 	 1000 	 0.13479351997375488 	 0.004243135452270508 	 0.12335395812988281 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 16:34:17.775474 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 42231, 401],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 42231, 401],"float32"), ) 	 50803894 	 1000 	 0.1347811222076416 	 0.004245758056640625 	 0.12333536148071289 	 1.6450881958007812e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 16:34:19.704632 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 43872, 386],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 43872, 386],"float32"), ) 	 50803777 	 1000 	 0.13478732109069824 	 0.0042667388916015625 	 0.12337589263916016 	 2.0265579223632812e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 16:34:21.619023 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 466, 36340],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 466, 36340],"float32"), ) 	 50803321 	 1000 	 0.13481664657592773 	 0.004179477691650391 	 0.1232607364654541 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 16:34:23.587008 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 477, 35502],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 477, 35502],"float32"), ) 	 50803363 	 1000 	 0.1347644329071045 	 0.004275321960449219 	 0.12337446212768555 	 2.193450927734375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 16:34:25.484915 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([100, 3, 391, 436],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([100, 3, 391, 436],"float32"), ) 	 51142801 	 1000 	 0.13573598861694336 	 0.004161834716796875 	 0.1244053840637207 	 1.7881393432617188e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 16:34:27.420284 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([89, 3, 477, 401],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([89, 3, 477, 401],"float32"), ) 	 51070960 	 1000 	 0.13561248779296875 	 0.004525423049926758 	 0.1239011287689209 	 3.218650817871094e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 16:34:29.352508 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([95, 3, 466, 386],"float32"), )
[Prof] paddle.Tensor.expand_as 	 paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([95, 3, 466, 386],"float32"), ) 	 51264661 	 1000 	 0.13619661331176758 	 0.004228353500366211 	 0.12478804588317871 	 1.7642974853515625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 16:34:31.340991 test begin: paddle.Tensor.fill_(Tensor([50803201],"float32"), 0, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(Tensor([50803201],"float32"), 0, ) 	 50803201 	 1000 	 0.1445925235748291 	 0.13406658172607422 	 0.12971067428588867 	 0.12558746337890625 	 None 	 None 	 None 	 None 	 
2025-07-24 16:34:33.280799 test begin: paddle.Tensor.fill_(Tensor([659782, 77],"float32"), value=-math.inf, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(Tensor([659782, 77],"float32"), value=-math.inf, ) 	 50803214 	 1000 	 0.14469599723815918 	 0.1342620849609375 	 0.1294400691986084 	 0.12578225135803223 	 None 	 None 	 None 	 None 	 
2025-07-24 16:34:37.042997 test begin: paddle.Tensor.fill_(Tensor([77, 659782],"float32"), value=-math.inf, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(Tensor([77, 659782],"float32"), value=-math.inf, ) 	 50803214 	 1000 	 0.1451711654663086 	 0.13431549072265625 	 0.12984919548034668 	 0.12581610679626465 	 None 	 None 	 None 	 None 	 
2025-07-24 16:34:40.108285 test begin: paddle.Tensor.fill_(x=Tensor([10, 158761, 16],"float64"), value=41.2, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(x=Tensor([10, 158761, 16],"float64"), value=41.2, ) 	 25401760 	 1000 	 0.15093135833740234 	 0.13443756103515625 	 0.1358184814453125 	 0.12591123580932617 	 None 	 None 	 None 	 None 	 
2025-07-24 16:34:41.503655 test begin: paddle.Tensor.fill_(x=Tensor([10, 16, 158761],"float64"), value=41.2, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(x=Tensor([10, 16, 158761],"float64"), value=41.2, ) 	 25401760 	 1000 	 0.15075325965881348 	 0.13442683219909668 	 0.13550019264221191 	 0.12592625617980957 	 None 	 None 	 None 	 None 	 
2025-07-24 16:34:42.918741 test begin: paddle.Tensor.fill_(x=Tensor([99226, 16, 16],"float64"), value=41.2, )
[Prof] paddle.Tensor.fill_ 	 paddle.Tensor.fill_(x=Tensor([99226, 16, 16],"float64"), value=41.2, ) 	 25401856 	 1000 	 0.15188932418823242 	 0.13445448875427246 	 0.13676095008850098 	 0.12570977210998535 	 None 	 None 	 None 	 None 	 
2025-07-24 16:34:44.312624 test begin: paddle.Tensor.fill_diagonal_(Tensor([128, 396901],"float32"), 0, wrap=False, )
[Prof] paddle.Tensor.fill_diagonal_ 	 paddle.Tensor.fill_diagonal_(Tensor([128, 396901],"float32"), 0, wrap=False, ) 	 50803328 	 1000 	 0.022645950317382812 	 0.010439157485961914 	 1.9073486328125e-05 	 2.574920654296875e-05 	 0.03196239471435547 	 0.04692697525024414 	 2.6941299438476562e-05 	 3.933906555175781e-05 	 combined
2025-07-24 16:34:47.332667 test begin: paddle.Tensor.fill_diagonal_(Tensor([396901, 128],"float32"), 0, wrap=False, )
[Prof] paddle.Tensor.fill_diagonal_ 	 paddle.Tensor.fill_diagonal_(Tensor([396901, 128],"float32"), 0, wrap=False, ) 	 50803328 	 1000 	 0.02259087562561035 	 0.010532140731811523 	 1.7404556274414062e-05 	 3.266334533691406e-05 	 0.0316166877746582 	 0.05009293556213379 	 2.3365020751953125e-05 	 7.653236389160156e-05 	 combined
2025-07-24 16:34:50.247478 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([12700801, 4, 7],"int32"), Tensor([12700801, 4],"int32"), 0, 1, 2, )
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([12700801, 4, 7],"int32"), Tensor([12700801, 4],"int32"), 0, 1, 2, ) 	 406425632 	 1000 	 87.32287883758545 	 4.356467247009277 	 0.004102230072021484 	 1.4830458164215088 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:37:59.189724 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([1814401, 4, 7],"int32"), Tensor([1814401, 4],"int32"), 0, 1, 2, )
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([1814401, 4, 7],"int32"), Tensor([1814401, 4],"int32"), 0, 1, 2, ) 	 58060832 	 1000 	 4.605233192443848 	 0.6299839019775391 	 0.0015254020690917969 	 0.21447396278381348 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:38:10.448053 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4, 3175201],"int64"), Tensor([2, 4],"int64"), 0, 1, 2, )
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4, 3175201],"int64"), Tensor([2, 4],"int64"), 0, 1, 2, ) 	 25401616 	 1000 	 0.32070016860961914 	 0.3158426284790039 	 0.08177042007446289 	 0.10739684104919434 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:38:12.231909 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4, 6350401],"int32"), Tensor([2, 4],"int32"), 0, 1, 2, )
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4, 6350401],"int32"), Tensor([2, 4],"int32"), 0, 1, 2, ) 	 50803216 	 1000 	 0.32067322731018066 	 0.31718015670776367 	 0.08176732063293457 	 0.10739636421203613 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:38:14.379667 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4233601, 3, 2],"int32"), Tensor([2, 2, 3],"int32"), offset=0, dim1=1, dim2=2, )
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([2, 4233601, 3, 2],"int32"), Tensor([2, 2, 3],"int32"), offset=0, dim1=1, dim2=2, ) 	 50803224 	 1000 	 0.3207411766052246 	 0.3159060478210449 	 0.08179545402526855 	 0.10743117332458496 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:38:16.518144 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([6350401, 4, 7],"int64"), Tensor([6350401, 4],"int64"), 0, 1, 2, )
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([6350401, 4, 7],"int64"), Tensor([6350401, 4],"int64"), 0, 1, 2, ) 	 203212832 	 1000 	 41.252143144607544 	 3.602740526199341 	 0.0023834705352783203 	 1.2266592979431152 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:39:49.403185 test begin: paddle.Tensor.fill_diagonal_tensor(Tensor([907201, 4, 7],"int64"), Tensor([907201, 4],"int64"), 0, 1, 2, )
[Prof] paddle.Tensor.fill_diagonal_tensor 	 paddle.Tensor.fill_diagonal_tensor(Tensor([907201, 4, 7],"int64"), Tensor([907201, 4],"int64"), 0, 1, 2, ) 	 29030432 	 1000 	 2.299915075302124 	 0.5180425643920898 	 0.0008115768432617188 	 0.17607665061950684 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:39:55.476638 test begin: paddle.Tensor.flatten(Tensor([1, 64, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([1, 64, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, ) 	 168448000 	 1000 	 0.005986452102661133 	 0.004715919494628906 	 9.298324584960938e-06 	 1.9073486328125e-05 	 0.041654109954833984 	 0.0578007698059082 	 2.574920654296875e-05 	 5.030632019042969e-05 	 
2025-07-24 16:40:01.074890 test begin: paddle.Tensor.flatten(Tensor([128, 127, 56, 56],"float32"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([128, 127, 56, 56],"float32"), 2, ) 	 50978816 	 1000 	 0.0053539276123046875 	 0.00464630126953125 	 8.821487426757812e-06 	 1.6927719116210938e-05 	 0.04089617729187012 	 0.05703306198120117 	 2.9325485229492188e-05 	 4.8160552978515625e-05 	 
2025-07-24 16:40:02.846656 test begin: paddle.Tensor.flatten(Tensor([128, 254, 56, 56],"float16"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([128, 254, 56, 56],"float16"), 2, ) 	 101957632 	 1000 	 0.0054361820220947266 	 0.0046155452728271484 	 1.3828277587890625e-05 	 1.8358230590820312e-05 	 0.04108119010925293 	 0.05736041069030762 	 1.621246337890625e-05 	 5.245208740234375e-05 	 
2025-07-24 16:40:06.819984 test begin: paddle.Tensor.flatten(Tensor([128, 512, 14, 56],"float32"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([128, 512, 14, 56],"float32"), 2, ) 	 51380224 	 1000 	 0.005373239517211914 	 0.00461888313293457 	 7.62939453125e-06 	 1.7404556274414062e-05 	 0.04103541374206543 	 0.05626511573791504 	 2.9325485229492188e-05 	 3.0279159545898438e-05 	 
2025-07-24 16:40:08.567861 test begin: paddle.Tensor.flatten(Tensor([128, 512, 28, 56],"float16"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([128, 512, 28, 56],"float16"), 2, ) 	 102760448 	 1000 	 0.00540924072265625 	 0.004648447036743164 	 7.152557373046875e-06 	 1.8596649169921875e-05 	 0.04094815254211426 	 0.056615352630615234 	 3.814697265625e-05 	 3.409385681152344e-05 	 
2025-07-24 16:40:13.786068 test begin: paddle.Tensor.flatten(Tensor([128, 512, 56, 14],"float32"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([128, 512, 56, 14],"float32"), 2, ) 	 51380224 	 1000 	 0.005463600158691406 	 0.004656076431274414 	 1.1444091796875e-05 	 1.71661376953125e-05 	 0.052906036376953125 	 0.05649566650390625 	 4.00543212890625e-05 	 3.361701965332031e-05 	 
2025-07-24 16:40:15.586345 test begin: paddle.Tensor.flatten(Tensor([128, 512, 56, 28],"float16"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([128, 512, 56, 28],"float16"), 2, ) 	 102760448 	 1000 	 0.005409717559814453 	 0.0046846866607666016 	 6.67572021484375e-06 	 3.647804260253906e-05 	 0.04346728324890137 	 0.05696249008178711 	 3.147125244140625e-05 	 4.267692565917969e-05 	 
2025-07-24 16:40:19.537110 test begin: paddle.Tensor.flatten(Tensor([32, 512, 56, 56],"float32"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([32, 512, 56, 56],"float32"), 2, ) 	 51380224 	 1000 	 0.005598783493041992 	 0.00466465950012207 	 1.6689300537109375e-05 	 1.7404556274414062e-05 	 0.044016361236572266 	 0.05671238899230957 	 2.6226043701171875e-05 	 4.2438507080078125e-05 	 
2025-07-24 16:40:21.296779 test begin: paddle.Tensor.flatten(Tensor([4, 5, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([4, 5, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, ) 	 52640000 	 1000 	 0.005791664123535156 	 0.0047762393951416016 	 7.152557373046875e-06 	 2.2172927856445312e-05 	 0.04089236259460449 	 0.05744314193725586 	 1.9788742065429688e-05 	 3.838539123535156e-05 	 
2025-07-24 16:40:23.125522 test begin: paddle.Tensor.flatten(Tensor([4, 64, 2, 376, 280],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([4, 64, 2, 376, 280],"float32"), start_axis=1, stop_axis=2, ) 	 53903360 	 1000 	 0.0058498382568359375 	 0.0047454833984375 	 7.62939453125e-06 	 1.8358230590820312e-05 	 0.040720224380493164 	 0.058133602142333984 	 1.7881393432617188e-05 	 4.673004150390625e-05 	 
2025-07-24 16:40:24.972562 test begin: paddle.Tensor.flatten(Tensor([4, 64, 25, 29, 280],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([4, 64, 25, 29, 280],"float32"), start_axis=1, stop_axis=2, ) 	 51968000 	 1000 	 0.005839109420776367 	 0.004825592041015625 	 1.4781951904296875e-05 	 2.7179718017578125e-05 	 0.04084038734436035 	 0.057355403900146484 	 2.6226043701171875e-05 	 3.695487976074219e-05 	 
2025-07-24 16:40:26.777522 test begin: paddle.Tensor.flatten(Tensor([4, 64, 25, 376, 22],"float32"), start_axis=1, stop_axis=2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([4, 64, 25, 376, 22],"float32"), start_axis=1, stop_axis=2, ) 	 52940800 	 1000 	 0.005728721618652344 	 0.004751443862915039 	 8.106231689453125e-06 	 1.71661376953125e-05 	 0.04188275337219238 	 0.056847572326660156 	 3.3855438232421875e-05 	 3.0517578125e-05 	 
2025-07-24 16:40:28.590477 test begin: paddle.Tensor.flatten(Tensor([64, 512, 56, 56],"float16"), 2, )
[Prof] paddle.Tensor.flatten 	 paddle.Tensor.flatten(Tensor([64, 512, 56, 56],"float16"), 2, ) 	 102760448 	 1000 	 0.005423069000244141 	 0.0046842098236083984 	 7.867813110351562e-06 	 1.8358230590820312e-05 	 0.04396963119506836 	 0.05784726142883301 	 2.6226043701171875e-05 	 5.7220458984375e-05 	 
2025-07-24 16:40:32.508904 test begin: paddle.Tensor.flip(Tensor([16, 3, 224, 4726],"float32"), 0, )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([16, 3, 224, 4726],"float32"), 0, ) 	 50813952 	 1000 	 0.9619636535644531 	 0.32330989837646484 	 0.9035763740539551 	 0.29631543159484863 	 0.9616270065307617 	 0.31151556968688965 	 0.9124562740325928 	 0.24319052696228027 	 
2025-07-24 16:40:38.905919 test begin: paddle.Tensor.flip(Tensor([16, 3, 4726, 224],"float32"), 0, )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([16, 3, 4726, 224],"float32"), 0, ) 	 50813952 	 1000 	 0.9620647430419922 	 0.3112950325012207 	 0.9527239799499512 	 0.2971372604370117 	 0.9622571468353271 	 0.3112320899963379 	 0.9123866558074951 	 0.22839117050170898 	 
2025-07-24 16:40:43.151804 test begin: paddle.Tensor.flip(Tensor([16, 64, 224, 224],"float32"), 0, )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([16, 64, 224, 224],"float32"), 0, ) 	 51380224 	 1000 	 0.971846342086792 	 0.31508421897888184 	 0.9628751277923584 	 0.30103302001953125 	 0.9718344211578369 	 0.31484460830688477 	 0.9149107933044434 	 0.24266433715820312 	 
2025-07-24 16:40:47.401360 test begin: paddle.Tensor.flip(Tensor([3, 400, 42337],"float32"), axis=list[-1,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([3, 400, 42337],"float32"), axis=list[-1,], ) 	 50804400 	 1000 	 0.9075090885162354 	 0.31237173080444336 	 0.8982014656066895 	 0.2983427047729492 	 0.907721996307373 	 0.31231093406677246 	 0.8585307598114014 	 0.24414992332458496 	 
2025-07-24 16:40:51.527082 test begin: paddle.Tensor.flip(Tensor([3, 400, 42337],"float32"), axis=list[-2,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([3, 400, 42337],"float32"), axis=list[-2,], ) 	 50804400 	 1000 	 0.9075703620910645 	 0.31975388526916504 	 0.8982417583465576 	 0.3005068302154541 	 0.9078304767608643 	 0.31580448150634766 	 0.8586843013763428 	 0.24699783325195312 	 
2025-07-24 16:40:55.612883 test begin: paddle.Tensor.flip(Tensor([3, 56449, 300],"float32"), axis=list[-1,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([3, 56449, 300],"float32"), axis=list[-1,], ) 	 50804100 	 1000 	 0.9081199169158936 	 0.3125417232513428 	 0.8976590633392334 	 0.29854488372802734 	 0.9086589813232422 	 0.31226134300231934 	 0.8585391044616699 	 0.24011468887329102 	 
2025-07-24 16:40:59.739647 test begin: paddle.Tensor.flip(Tensor([3, 56449, 300],"float32"), axis=list[-2,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([3, 56449, 300],"float32"), axis=list[-2,], ) 	 50804100 	 1000 	 0.9079923629760742 	 0.3154757022857666 	 0.8986485004425049 	 0.30142712593078613 	 0.9085640907287598 	 0.3154761791229248 	 0.8576936721801758 	 0.24557185173034668 	 
2025-07-24 16:41:03.928865 test begin: paddle.Tensor.flip(Tensor([338, 3, 224, 224],"float32"), 0, )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([338, 3, 224, 224],"float32"), 0, ) 	 50878464 	 1000 	 0.9627795219421387 	 0.31341552734375 	 0.9537451267242432 	 0.296893835067749 	 0.9627890586853027 	 0.31215381622314453 	 0.9138319492340088 	 0.24216938018798828 	 
2025-07-24 16:41:09.211485 test begin: paddle.Tensor.flip(Tensor([424, 400, 300],"float32"), axis=list[-1,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([424, 400, 300],"float32"), axis=list[-1,], ) 	 50880000 	 1000 	 0.9102029800415039 	 0.3129243850708008 	 0.9007437229156494 	 0.2973504066467285 	 0.9102497100830078 	 0.3128628730773926 	 0.8610398769378662 	 0.2448585033416748 	 
2025-07-24 16:41:13.336898 test begin: paddle.Tensor.flip(Tensor([424, 400, 300],"float32"), axis=list[-2,], )
[Prof] paddle.Tensor.flip 	 paddle.Tensor.flip(Tensor([424, 400, 300],"float32"), axis=list[-2,], ) 	 50880000 	 1000 	 0.9100608825683594 	 0.32883143424987793 	 0.9006540775299072 	 0.30177879333496094 	 0.9102203845977783 	 0.3164076805114746 	 0.8604075908660889 	 0.24725341796875 	 
2025-07-24 16:41:19.723584 test begin: paddle.Tensor.floor(Tensor([12700801, 4],"float32"), )
[Prof] paddle.Tensor.floor 	 paddle.Tensor.floor(Tensor([12700801, 4],"float32"), ) 	 50803204 	 1000 	 0.2957186698913574 	 0.2977783679962158 	 0.28706955909729004 	 0.28790879249572754 	 0.13395357131958008 	 0.13402867317199707 	 0.0844106674194336 	 0.06668615341186523 	 
2025-07-24 16:41:22.253456 test begin: paddle.Tensor.floor(Tensor([1857, 27358],"float32"), )
[Prof] paddle.Tensor.floor 	 paddle.Tensor.floor(Tensor([1857, 27358],"float32"), ) 	 50803806 	 1000 	 0.29547762870788574 	 0.2978222370147705 	 0.28672099113464355 	 0.2878706455230713 	 0.13393402099609375 	 0.13402962684631348 	 0.08317732810974121 	 0.06718134880065918 	 
2025-07-24 16:41:24.760913 test begin: paddle.Tensor.floor(Tensor([1872, 27139],"float32"), )
[Prof] paddle.Tensor.floor 	 paddle.Tensor.floor(Tensor([1872, 27139],"float32"), ) 	 50804208 	 1000 	 0.29566359519958496 	 0.29785704612731934 	 0.27958178520202637 	 0.2809462547302246 	 0.13401436805725098 	 0.13408231735229492 	 0.07512497901916504 	 0.04541897773742676 	 
2025-07-24 16:41:27.229105 test begin: paddle.Tensor.floor(Tensor([1915, 26530],"float32"), )
[Prof] paddle.Tensor.floor 	 paddle.Tensor.floor(Tensor([1915, 26530],"float32"), ) 	 50804950 	 1000 	 0.29581689834594727 	 0.29779577255249023 	 0.28717565536499023 	 0.28788304328918457 	 0.13405323028564453 	 0.13404226303100586 	 0.08433914184570312 	 0.06761789321899414 	 
2025-07-24 16:41:29.749771 test begin: paddle.Tensor.gather(Tensor([4, 12700801],"float32"), Tensor([4, 1],"int64"), 1, )
[Prof] paddle.Tensor.gather 	 paddle.Tensor.gather(Tensor([4, 12700801],"float32"), Tensor([4, 1],"int64"), 1, ) 	 50803208 	 1000 	 0.009974002838134766 	 0.17466950416564941 	 1.7404556274414062e-05 	 9.393692016601562e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:41:30.911791 test begin: paddle.Tensor.gather(Tensor([4, 4],"float32"), Tensor([25401601, 1],"int64"), 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0423deaaa0>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753347091 (unix time) try "date -d @1753347091" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ac4a) received by PID 109642 (TID 0x7f041f399640) from PID 109642 ***]

2025-07-24 16:52:35.795196 test begin: paddle.Tensor.gather_nd(Tensor([119, 53568, 8],"float32"), Tensor([119, 500, 2],"int64"), )
W0724 16:52:39.743849 139883 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9c59b6ef50>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753347755 (unix time) try "date -d @1753347755" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x221ff) received by PID 139775 (TID 0x7f9c50dfa640) from PID 139775 ***]

2025-07-24 17:02:43.025926 test begin: paddle.Tensor.gather_nd(Tensor([119, 53568, 8],"float32"), Tensor([4, 500, 2],"int64"), )
W0724 17:02:44.052218 144560 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([119, 53568, 8],"float32"), Tensor([4, 500, 2],"int64"), ) 	 51000736 	 1000 	 0.019777536392211914 	 170.60337138175964 	 2.0265579223632812e-05 	 0.00022101402282714844 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 17:05:37.304490 test begin: paddle.Tensor.gather_nd(Tensor([119, 53568, 8],"float32"), Tensor([60, 500, 2],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fef930727a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 17:15:43.378465 test begin: paddle.Tensor.gather_nd(Tensor([16, 3, 156, 80, 85],"float32"), Tensor([516, 4],"int64"), )
W0724 17:15:44.356204 150369 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([16, 3, 156, 80, 85],"float32"), Tensor([516, 4],"int64"), ) 	 50920464 	 1000 	 0.011378765106201172 	 82.77651000022888 	 1.3113021850585938e-05 	 0.00026106834411621094 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 17:17:07.849247 test begin: paddle.Tensor.gather_nd(Tensor([16, 3, 80, 156, 85],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([16, 3, 80, 156, 85],"float32"), Tensor([516, 4],"int64"), ) 	 50920464 	 1000 	 0.01116800308227539 	 85.4341607093811 	 1.6450881958007812e-05 	 0.0002334117889404297 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 17:18:34.382481 test begin: paddle.Tensor.gather_nd(Tensor([16, 3, 80, 80, 166],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([16, 3, 80, 80, 166],"float32"), Tensor([516, 4],"int64"), ) 	 50997264 	 1000 	 0.011123180389404297 	 79.95526576042175 	 1.1682510375976562e-05 	 0.00023412704467773438 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 17:19:57.176639 test begin: paddle.Tensor.gather_nd(Tensor([16, 3, 80, 80, 85],"float32"), Tensor([6350401, 4],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc82253c310>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753349397 (unix time) try "date -d @1753349397" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x24b08) received by PID 150280 (TID 0x7fc8197fb640) from PID 150280 ***]

2025-07-24 17:30:41.962095 test begin: paddle.Tensor.gather_nd(Tensor([16, 6, 80, 80, 85],"float32"), Tensor([516, 4],"int64"), )
W0724 17:30:42.966317 156580 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([16, 6, 80, 80, 85],"float32"), Tensor([516, 4],"int64"), ) 	 52226064 	 1000 	 0.011728525161743164 	 80.73322010040283 	 1.9550323486328125e-05 	 0.0002410411834716797 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 17:32:04.394894 test begin: paddle.Tensor.gather_nd(Tensor([32, 3, 80, 80, 85],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([32, 3, 80, 80, 85],"float32"), Tensor([385, 4],"int64"), ) 	 52225540 	 1000 	 0.010319232940673828 	 57.33048224449158 	 1.1205673217773438e-05 	 0.0002155303955078125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 17:33:02.797737 test begin: paddle.Tensor.gather_nd(Tensor([32, 3, 80, 80, 85],"float32"), Tensor([516, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([32, 3, 80, 80, 85],"float32"), Tensor([516, 4],"int64"), ) 	 52226064 	 1000 	 0.010483741760253906 	 77.03326511383057 	 1.1920928955078125e-05 	 0.00022411346435546875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 17:34:20.929072 test begin: paddle.Tensor.gather_nd(Tensor([4, 1587601, 8],"float32"), Tensor([4, 500, 2],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([4, 1587601, 8],"float32"), Tensor([4, 500, 2],"int64"), ) 	 50807232 	 1000 	 0.010568857192993164 	 155.83143734931946 	 1.1205673217773438e-05 	 9.560585021972656e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 17:36:57.924562 test begin: paddle.Tensor.gather_nd(Tensor([4, 1587601, 8],"float32"), Tensor([4, 793801, 2],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f098271d270>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753350418 (unix time) try "date -d @1753350418" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2632c) received by PID 156460 (TID 0x7f097deef640) from PID 156460 ***]

2025-07-24 17:47:17.273618 test begin: paddle.Tensor.gather_nd(Tensor([4, 53568, 238],"float32"), Tensor([4, 500, 2],"int64"), )
W0724 17:47:18.294178 11129 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([4, 53568, 238],"float32"), Tensor([4, 500, 2],"int64"), ) 	 51000736 	 1000 	 0.021300315856933594 	 165.33940505981445 	 2.6941299438476562e-05 	 0.00023603439331054688 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 17:50:04.670142 test begin: paddle.Tensor.gather_nd(Tensor([4, 53568, 8],"float32"), Tensor([25402, 500, 2],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbe38786aa0>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753351204 (unix time) try "date -d @1753351204" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2ac4) received by PID 10948 (TID 0x7fbe2f9f8640) from PID 10948 ***]

2025-07-24 18:00:32.492735 test begin: paddle.Tensor.gather_nd(Tensor([4, 53568, 8],"float32"), Tensor([4, 3175201, 2],"int64"), )
W0724 18:00:33.236218 30287 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1b726fb0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753351832 (unix time) try "date -d @1753351832" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x7599) received by PID 30105 (TID 0x7f1b6deef640) from PID 30105 ***]

2025-07-24 18:11:03.901856 test begin: paddle.Tensor.gather_nd(Tensor([8, 12, 80, 80, 85],"float32"), Tensor([385, 4],"int64"), )
W0724 18:11:04.935066 43682 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([8, 12, 80, 80, 85],"float32"), Tensor([385, 4],"int64"), ) 	 52225540 	 1000 	 0.018282651901245117 	 62.947250843048096 	 1.3828277587890625e-05 	 0.00022101402282714844 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 18:12:08.595919 test begin: paddle.Tensor.gather_nd(Tensor([8, 3, 312, 80, 85],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([8, 3, 312, 80, 85],"float32"), Tensor([385, 4],"int64"), ) 	 50919940 	 1000 	 0.010544300079345703 	 65.37230372428894 	 1.239776611328125e-05 	 0.0002894401550292969 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 18:13:15.003858 test begin: paddle.Tensor.gather_nd(Tensor([8, 3, 80, 312, 85],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([8, 3, 80, 312, 85],"float32"), Tensor([385, 4],"int64"), ) 	 50919940 	 1000 	 0.010351419448852539 	 63.65090537071228 	 1.2159347534179688e-05 	 0.00027298927307128906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 18:14:19.698739 test begin: paddle.Tensor.gather_nd(Tensor([8, 3, 80, 80, 331],"float32"), Tensor([385, 4],"int64"), )
[Prof] paddle.Tensor.gather_nd 	 paddle.Tensor.gather_nd(Tensor([8, 3, 80, 80, 331],"float32"), Tensor([385, 4],"int64"), ) 	 50843140 	 1000 	 0.019016027450561523 	 62.018091440200806 	 1.1444091796875e-05 	 0.00029587745666503906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 18:15:22.816043 test begin: paddle.Tensor.gather_nd(Tensor([8, 3, 80, 80, 85],"float32"), Tensor([6350401, 4],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3dfe49bf40>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753352722 (unix time) try "date -d @1753352722" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xa9f3) received by PID 43507 (TID 0x7f3df57fb640) from PID 43507 ***]

2025-07-24 18:26:01.850251 test begin: paddle.Tensor.gcd(x=Tensor([1270081, 2, 4, 5],"int32"), y=Tensor([1270081, 2, 4, 5],"int32"), )
W0724 18:26:03.113698 65092 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 18:27:52.305619 65092 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 18:27:52.344354 test begin: paddle.Tensor.gcd(x=Tensor([2, 4, 6350401],"int32"), y=Tensor([2, 4, 6350401],"int32"), )
W0724 18:29:42.391709 67637 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 18:29:42.440296 test begin: paddle.Tensor.gcd(x=Tensor([2, 5080321, 5],"int32"), y=Tensor([2, 5080321, 5],"int32"), )
W0724 18:31:32.568219 69964 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 18:31:32.614735 test begin: paddle.Tensor.gcd(x=Tensor([2540161, 1, 4, 5],"int32"), y=Tensor([2, 1, 5],"int32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_where(_object*, _object*, _object*)
1   where_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::where(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
3   void phi::WhereKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   int* phi::DeviceContext::Alloc<int>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 387.597900MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:31:43.579495 test begin: paddle.Tensor.gcd(x=Tensor([2540161, 4, 5],"int32"), y=Tensor([2540161, 4, 5],"int32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_expand(_object*, _object*, _object*)
1   expand_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>)
2   paddle::experimental::expand(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&)
3   void phi::ExpandKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
4   int* phi::DeviceContext::Alloc<int>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:31:54.887108 test begin: paddle.Tensor.gcd(x=Tensor([6, 1, 1693441, 5],"int32"), y=Tensor([2, 1, 5],"int32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:32:05.612963 test begin: paddle.Tensor.gcd(x=Tensor([6, 1, 4, 2116801],"int32"), y=Tensor([2, 1, 2116801],"int32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_where(_object*, _object*, _object*)
1   where_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::where(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
3   void phi::WhereKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   int* phi::DeviceContext::Alloc<int>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 387.597900MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:32:17.217585 test begin: paddle.Tensor.gcd(x=Tensor([6, 2, 4, 1058401],"int32"), y=Tensor([6, 2, 4, 1058401],"int32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_expand(_object*, _object*, _object*)
1   expand_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>)
2   paddle::experimental::expand(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&)
3   void phi::ExpandKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
4   int* phi::DeviceContext::Alloc<int>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:32:28.509695 test begin: paddle.Tensor.gcd(x=Tensor([6, 2, 846721, 5],"int32"), y=Tensor([6, 2, 846721, 5],"int32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:32:39.766687 test begin: paddle.Tensor.gcd(x=Tensor([6, 423361, 4, 5],"int32"), y=Tensor([6, 423361, 4, 5],"int32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:32:51.045863 test begin: paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([130],"int64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:33:02.082329 test begin: paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([182],"int64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:33:13.078503 test begin: paddle.Tensor.index_select(Tensor([2116801, 24],"float32"), axis=0, index=Tensor([91],"int64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:33:24.122864 test begin: paddle.Tensor.index_select(Tensor([4004, 12689],"float32"), axis=0, index=Tensor([182],"int64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.812500MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:33:35.605192 test begin: paddle.Tensor.index_select(Tensor([4004, 24],"float32"), axis=0, index=Tensor([25401601],"int64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:33:47.158187 test begin: paddle.Tensor.index_select(Tensor([454, 111902],"float32"), axis=0, index=Tensor([130],"int64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.800049MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:33:58.125876 test begin: paddle.Tensor.index_select(Tensor([454, 111902],"float32"), axis=0, index=Tensor([91],"int64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.800049MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:34:09.118601 test begin: paddle.Tensor.index_select(Tensor([454, 24],"float32"), axis=0, index=Tensor([25401601],"int64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_index_select(_object*, _object*, _object*)
1   index_select_ad_func(paddle::Tensor const&, paddle::Tensor const&, int)
2   paddle::experimental::index_select(paddle::Tensor const&, paddle::Tensor const&, int)
3   void phi::IndexSelectKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
4   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 2.271080GB memory on GPU 0, 39.065430GB memory has been allocated and available memory is only 336.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:34:19.858966 test begin: paddle.Tensor.inner(x=Tensor([2, 1058401, 3, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_matmul(_object*, _object*, _object*)
1   matmul_ad_func(paddle::Tensor const&, paddle::Tensor const&, bool, bool)
2   paddle::experimental::matmul(paddle::Tensor const&, paddle::Tensor const&, bool, bool)
3   void phi::MatmulKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, bool, bool, phi::DenseTensor*)
4   void phi::MatMulFunctionImplWithBlas<phi::GPUContext, double>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*, bool, bool, bool, phi::funcs::MatmulPlanner*)
5   double* phi::DeviceContext::Alloc<double>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 1.419426GB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:34:30.737376 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 2, 1058401, 4],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:34:42.501981 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([3, 423361, 5, 4],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:34:53.214869 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float64"), y=Tensor([635041, 2, 5, 4],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:35:03.885235 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 846721],"float64"), y=Tensor([3, 2, 5, 846721],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:35:15.025799 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 635041, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:35:25.703571 test begin: paddle.Tensor.inner(x=Tensor([2116801, 3, 4],"float64"), y=Tensor([2, 5, 4],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:35:37.060608 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 2, 1058401, 4],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:35:49.107645 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([3, 423361, 5, 4],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:35:59.796542 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float64"), y=Tensor([635041, 2, 5, 4],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:36:10.469128 test begin: paddle.Tensor.inner(x=Tensor([3, 8467201],"float64"), y=Tensor([3, 2, 5, 8467201],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:36:25.367673 test begin: paddle.Tensor.inner(x=Tensor([3, 846721],"float64"), y=Tensor([3, 2, 5, 846721],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:36:37.032639 test begin: paddle.Tensor.inner(x=Tensor([423361, 5, 3, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_matmul(_object*, _object*, _object*)
1   matmul_ad_func(paddle::Tensor const&, paddle::Tensor const&, bool, bool)
2   paddle::experimental::matmul(paddle::Tensor const&, paddle::Tensor const&, bool, bool)
3   void phi::MatmulKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, bool, bool, phi::DenseTensor*)
4   void phi::MatMulFunctionImplWithBlas<phi::GPUContext, double>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*, bool, bool, bool, phi::funcs::MatmulPlanner*)
5   double* phi::DeviceContext::Alloc<double>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 1.419429GB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:36:49.279784 test begin: paddle.Tensor.inner(x=Tensor([5, 1270081, 4],"float64"), y=Tensor([2, 5, 4],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:37:01.195395 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 1693441],"float64"), y=Tensor([2, 5, 1693441],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:37:12.156138 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 2540161],"float64"), y=Tensor([2, 5, 2540161],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 290.698486MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:37:23.475321 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([1270081, 5, 4],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:37:34.187227 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 4],"float64"), y=Tensor([2, 3175201, 4],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:37:44.870352 test begin: paddle.Tensor.inner(x=Tensor([6350401, 4],"float64"), y=Tensor([3, 2, 5, 4],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:37:56.082777 test begin: paddle.Tensor.inverse(Tensor([4, 396901, 4, 4],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:38:06.795739 test begin: paddle.Tensor.inverse(Tensor([705601, 6, 6],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:38:17.464744 test begin: paddle.Tensor.inverse(Tensor([793801, 2, 4, 4],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:38:28.139213 test begin: paddle.Tensor.is_complex(Tensor([2, 3, 100, 42337],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.803467MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:38:39.020450 test begin: paddle.Tensor.is_complex(Tensor([2, 3, 105841, 40],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.800781MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:38:49.708482 test begin: paddle.Tensor.is_complex(Tensor([2, 3, 40, 105841],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.800781MB memory on GPU 0, 39.254822GB memory has been allocated and available memory is only 142.375000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:39:00.621266 test begin: paddle.Tensor.is_complex(Tensor([2, 3, 42337, 100],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.803467MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:39:11.294542 test begin: paddle.Tensor.is_complex(Tensor([2, 3176, 100, 40],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.847656MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:39:22.126949 test begin: paddle.Tensor.is_complex(Tensor([2, 3176, 40, 100],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.847656MB memory on GPU 0, 39.254822GB memory has been allocated and available memory is only 142.375000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:39:32.814401 test begin: paddle.Tensor.is_complex(Tensor([2117, 3, 100, 40],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.817139MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:39:43.496327 test begin: paddle.Tensor.is_complex(Tensor([2117, 3, 40, 100],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.817139MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:39:54.180226 test begin: paddle.Tensor.is_complex(Tensor([3, 100, 84673],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.801270MB memory on GPU 0, 39.254822GB memory has been allocated and available memory is only 142.375000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:40:05.085745 test begin: paddle.Tensor.is_complex(Tensor([3, 211681, 40],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799805MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:40:15.760516 test begin: paddle.Tensor.is_complex(Tensor([6351, 100, 40],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.817139MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:40:26.431917 test begin: paddle.Tensor.isclose(x=Tensor([1270081, 4, 5],"float64"), y=Tensor([1270081, 4, 5],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:40:38.983506 test begin: paddle.Tensor.isclose(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:40:50.104763 test begin: paddle.Tensor.isclose(x=Tensor([3, 1693441, 5],"float64"), y=Tensor([3, 1693441, 5],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:41:01.207969 test begin: paddle.Tensor.isclose(x=Tensor([3, 4, 2116801],"float64"), y=Tensor([3, 4, 2116801],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:41:13.559533 test begin: paddle.Tensor.isclose(x=Tensor([50803201],"float32"), y=Tensor([50803201],"float32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:41:25.300390 test begin: paddle.Tensor.isnan(Tensor([25401601],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:41:37.008809 test begin: paddle.Tensor.isnan(Tensor([50803201],"float32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:41:48.843772 test begin: paddle.Tensor.item(Tensor([16934401, 3],"float32"), 0, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:41:59.978356 test begin: paddle.Tensor.item(Tensor([2, 1, 12700801],"int64"), 0, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:42:10.551213 test begin: paddle.Tensor.item(Tensor([2, 1, 25401601],"int32"), 0, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:42:21.277728 test begin: paddle.Tensor.item(Tensor([2, 12700801, 1],"int64"), 0, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:42:31.847161 test begin: paddle.Tensor.item(Tensor([2, 25401601, 1],"int32"), 0, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:42:42.582157 test begin: paddle.Tensor.item(Tensor([25401601, 1, 1],"int64"), 0, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:42:53.164942 test begin: paddle.Tensor.item(Tensor([3, 16934401],"float32"), 0, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:43:04.125412 test begin: paddle.Tensor.item(Tensor([50803201, 1, 1],"int32"), 0, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:43:14.863534 test begin: paddle.Tensor.kthvalue(Tensor([2, 200, 127009],"float32"), k=200, axis=1, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.800537MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:43:25.876258 test begin: paddle.Tensor.kthvalue(Tensor([2, 2540161, 10],"float32"), k=200, axis=1, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254822GB memory has been allocated and available memory is only 142.375000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:43:37.265133 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 317521],"float64"), y=Tensor([4, 5, 4, 317521],"float64"), weight=0.0, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799561MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:43:49.289454 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 317521],"float64"), y=Tensor([4, 5, 4, 317521],"float64"), weight=0.5, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799561MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:44:00.380149 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 317521],"float64"), y=Tensor([4, 5, 4, 317521],"float64"), weight=1.0, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799561MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:44:11.476731 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 423361, 3],"float64"), y=Tensor([4, 5, 423361, 3],"float64"), weight=0.0, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254822GB memory has been allocated and available memory is only 142.375000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:44:23.237710 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 423361, 3],"float64"), y=Tensor([4, 5, 423361, 3],"float64"), weight=0.5, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:44:35.126902 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 423361, 3],"float64"), y=Tensor([4, 5, 423361, 3],"float64"), weight=1.0, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:44:47.070162 test begin: paddle.Tensor.lerp(x=Tensor([4, 529201, 4, 3],"float64"), y=Tensor([4, 529201, 4, 3],"float64"), weight=0.0, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:44:58.253500 test begin: paddle.Tensor.lerp(x=Tensor([4, 529201, 4, 3],"float64"), y=Tensor([4, 529201, 4, 3],"float64"), weight=0.5, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:45:09.338044 test begin: paddle.Tensor.lerp(x=Tensor([4, 529201, 4, 3],"float64"), y=Tensor([4, 529201, 4, 3],"float64"), weight=1.0, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:45:20.410526 test begin: paddle.Tensor.lerp(x=Tensor([423361, 5, 4, 3],"float64"), y=Tensor([423361, 5, 4, 3],"float64"), weight=0.0, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254822GB memory has been allocated and available memory is only 142.375000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:45:31.491708 test begin: paddle.Tensor.lerp(x=Tensor([423361, 5, 4, 3],"float64"), y=Tensor([423361, 5, 4, 3],"float64"), weight=0.5, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:45:42.563092 test begin: paddle.Tensor.lerp(x=Tensor([423361, 5, 4, 3],"float64"), y=Tensor([423361, 5, 4, 3],"float64"), weight=1.0, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799316MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:45:53.703398 test begin: paddle.Tensor.less(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.799072MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:46:05.362960 test begin: paddle.Tensor.less(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.800781MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:46:17.023880 test begin: paddle.Tensor.lgamma(Tensor([100, 100, 2541],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.863037MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:46:27.701281 test begin: paddle.Tensor.lgamma(Tensor([100, 2541, 100],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.863037MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:46:39.392864 test begin: paddle.Tensor.lgamma(Tensor([2541, 100, 100],"float64"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.863037MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:46:50.089025 test begin: paddle.Tensor.lgamma(Tensor([453601, 7, 8],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([453601, 7, 8],"float64"), ) 	 25401656 	 1000 	 0.7123212814331055 	 0.6896979808807373 	 0.7025690078735352 	 0.6792147159576416 	 1.3747608661651611 	 1.586771011352539 	 1.3231663703918457 	 0.8100230693817139 	 
2025-07-24 18:46:56.420011 test begin: paddle.Tensor.lgamma(Tensor([45361, 7, 8, 10],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([45361, 7, 8, 10],"float64"), ) 	 25402160 	 1000 	 0.7132320404052734 	 0.6912190914154053 	 0.7033026218414307 	 0.6814239025115967 	 1.3765921592712402 	 1.5857672691345215 	 1.3256871700286865 	 0.8102529048919678 	 
2025-07-24 18:47:01.840671 test begin: paddle.Tensor.lgamma(Tensor([5, 635041, 8],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([5, 635041, 8],"float64"), ) 	 25401640 	 1000 	 0.7121925354003906 	 0.6897628307342529 	 0.703009843826294 	 0.6794817447662354 	 1.3805859088897705 	 1.585453748703003 	 1.327972412109375 	 0.8101422786712646 	 
2025-07-24 18:47:07.343490 test begin: paddle.Tensor.lgamma(Tensor([5, 63505, 8, 10],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([5, 63505, 8, 10],"float64"), ) 	 25402000 	 1000 	 0.7136497497558594 	 0.6896436214447021 	 0.7031030654907227 	 0.6735045909881592 	 1.3796355724334717 	 1.5852222442626953 	 1.3285925388336182 	 0.8100132942199707 	 
2025-07-24 18:47:12.777712 test begin: paddle.Tensor.lgamma(Tensor([5, 7, 725761],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([5, 7, 725761],"float64"), ) 	 25401635 	 1000 	 0.7134759426116943 	 0.689683198928833 	 0.7044603824615479 	 0.6797065734863281 	 1.3792202472686768 	 1.5857980251312256 	 1.3282289505004883 	 0.8102931976318359 	 
2025-07-24 18:47:18.317487 test begin: paddle.Tensor.lgamma(Tensor([5, 7, 72577, 10],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([5, 7, 72577, 10],"float64"), ) 	 25401950 	 1000 	 0.7125189304351807 	 0.6899080276489258 	 0.7034885883331299 	 0.6735763549804688 	 1.3786280155181885 	 1.5870277881622314 	 1.31781005859375 	 0.8102343082427979 	 
2025-07-24 18:47:23.710669 test begin: paddle.Tensor.lgamma(Tensor([5, 7, 8, 90721],"float64"), )
[Prof] paddle.Tensor.lgamma 	 paddle.Tensor.lgamma(Tensor([5, 7, 8, 90721],"float64"), ) 	 25401880 	 1000 	 0.7128036022186279 	 0.6910848617553711 	 0.7037539482116699 	 0.6810903549194336 	 1.379488468170166 	 1.5857417583465576 	 1.3272392749786377 	 0.8102779388427734 	 
2025-07-24 18:47:29.135831 test begin: paddle.Tensor.log(Tensor([100, 200, 1271],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([100, 200, 1271],"float64"), ) 	 25420000 	 1000 	 0.3051793575286865 	 0.3065762519836426 	 0.2963862419128418 	 0.29602622985839844 	 0.4484059810638428 	 0.4502577781677246 	 0.39183592796325684 	 0.385678768157959 	 
2025-07-24 18:47:31.875163 test begin: paddle.Tensor.log(Tensor([100, 2541, 100],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([100, 2541, 100],"float64"), ) 	 25410000 	 1000 	 0.30471086502075195 	 0.30647993087768555 	 0.2961125373840332 	 0.29604315757751465 	 0.44837141036987305 	 0.44895410537719727 	 0.3972749710083008 	 0.38802385330200195 	 
2025-07-24 18:47:34.444428 test begin: paddle.Tensor.log(Tensor([10000, 5, 509],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([10000, 5, 509],"float64"), ) 	 25450000 	 1000 	 0.3069746494293213 	 0.31609296798706055 	 0.2966930866241455 	 0.29004549980163574 	 0.44857215881347656 	 0.44957923889160156 	 0.38715100288391113 	 0.37994384765625 	 
2025-07-24 18:47:39.760409 test begin: paddle.Tensor.log(Tensor([10000, 847, 3],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([10000, 847, 3],"float64"), ) 	 25410000 	 1000 	 1.0191442966461182 	 1.0143513679504395 	 0.2964963912963867 	 0.2857933044433594 	 0.44890618324279785 	 0.4488716125488281 	 0.3878195285797119 	 0.38496851921081543 	 
2025-07-24 15:39:46.448852 test begin: paddle.Tensor.log(Tensor([1271, 200, 100],"float64"), )
W0724 15:39:47.207787 107678 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([1271, 200, 100],"float64"), ) 	 25420000 	 1000 	 0.30512547492980957 	 0.30860424041748047 	 0.2962183952331543 	 0.29561519622802734 	 0.4480767250061035 	 0.4491696357727051 	 0.3943827152252197 	 0.35371828079223633 	 
2025-07-24 15:39:49.712880 test begin: paddle.Tensor.log(Tensor([1693441, 5, 3],"float64"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([1693441, 5, 3],"float64"), ) 	 25401615 	 1000 	 0.30539989471435547 	 0.3063175678253174 	 0.2967996597290039 	 0.2958519458770752 	 0.44844818115234375 	 0.4488677978515625 	 0.39544010162353516 	 0.3875420093536377 	 
2025-07-24 15:39:52.259324 test begin: paddle.Tensor.log(Tensor([4800, 10585],"float32"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([4800, 10585],"float32"), ) 	 50808000 	 1000 	 0.29856371879577637 	 0.29762935638427734 	 0.2873110771179199 	 0.2871885299682617 	 0.44918298721313477 	 0.44977426528930664 	 0.39587998390197754 	 0.3887960910797119 	 
2025-07-24 15:39:55.416699 test begin: paddle.Tensor.log(Tensor([503002, 101],"float32"), )
[Prof] paddle.Tensor.log 	 paddle.Tensor.log(Tensor([503002, 101],"float32"), ) 	 50803202 	 1000 	 0.29589247703552246 	 0.2975749969482422 	 0.2872743606567383 	 0.2872459888458252 	 0.4505455493927002 	 0.4497039318084717 	 0.3972492218017578 	 0.38835716247558594 	 
2025-07-24 15:39:58.579295 test begin: paddle.Tensor.log10(Tensor([101811, 499],"float32"), )
[Prof] paddle.Tensor.log10 	 paddle.Tensor.log10(Tensor([101811, 499],"float32"), ) 	 50803689 	 1000 	 0.29601216316223145 	 0.30479860305786133 	 0.2874431610107422 	 0.2866554260253906 	 0.4505186080932617 	 0.7458169460296631 	 0.39568376541137695 	 0.3810563087463379 	 
2025-07-24 15:40:04.586142 test begin: paddle.Tensor.log10(Tensor([80, 635041],"float32"), )
[Prof] paddle.Tensor.log10 	 paddle.Tensor.log10(Tensor([80, 635041],"float32"), ) 	 50803280 	 1000 	 0.2960541248321533 	 0.3061094284057617 	 0.2874765396118164 	 0.28705668449401855 	 0.45032548904418945 	 0.7457430362701416 	 0.39687132835388184 	 0.3809945583343506 	 
2025-07-24 15:40:08.092564 test begin: paddle.Tensor.log1p(Tensor([16934401, 3],"float32"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([16934401, 3],"float32"), ) 	 50803203 	 1000 	 0.29753994941711426 	 0.2988548278808594 	 0.287320613861084 	 0.28841471672058105 	 0.4503178596496582 	 0.7457382678985596 	 0.39669346809387207 	 0.38103699684143066 	 
2025-07-24 15:40:11.575531 test begin: paddle.Tensor.log1p(Tensor([2, 25401601],"float32"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([2, 25401601],"float32"), ) 	 50803202 	 1000 	 0.2959115505218506 	 0.30239033699035645 	 0.2873961925506592 	 0.28839945793151855 	 0.4503190517425537 	 0.7457671165466309 	 0.39690494537353516 	 0.3810131549835205 	 
2025-07-24 15:40:15.063401 test begin: paddle.Tensor.log1p(Tensor([2, 3, 4233601],"float64"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([2, 3, 4233601],"float64"), ) 	 25401606 	 1000 	 0.3052239418029785 	 0.33595967292785645 	 0.2938714027404785 	 0.325577974319458 	 0.447953462600708 	 0.7447972297668457 	 0.3952927589416504 	 0.38050413131713867 	 
2025-07-24 15:40:17.945405 test begin: paddle.Tensor.log1p(Tensor([2, 6350401, 2],"float64"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([2, 6350401, 2],"float64"), ) 	 25401604 	 1000 	 0.305187463760376 	 0.3359527587890625 	 0.2966282367706299 	 0.32555556297302246 	 0.44847702980041504 	 0.7448663711547852 	 0.39596128463745117 	 0.3806021213531494 	 
2025-07-24 15:40:20.823992 test begin: paddle.Tensor.log1p(Tensor([25401601],"float64"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([25401601],"float64"), ) 	 25401601 	 1000 	 0.30513644218444824 	 0.3358457088470459 	 0.2965087890625 	 0.3254551887512207 	 0.4481017589569092 	 0.7447266578674316 	 0.3956153392791748 	 0.38050055503845215 	 
2025-07-24 15:40:23.732164 test begin: paddle.Tensor.log1p(Tensor([4233601, 3, 2],"float64"), )
[Prof] paddle.Tensor.log1p 	 paddle.Tensor.log1p(Tensor([4233601, 3, 2],"float64"), ) 	 25401606 	 1000 	 0.3051903247833252 	 0.3360164165496826 	 0.2964956760406494 	 0.32564878463745117 	 0.4484832286834717 	 0.7447288036346436 	 0.39609217643737793 	 0.3804781436920166 	 
2025-07-24 15:40:26.603395 test begin: paddle.Tensor.logical_and(Tensor([50803201],"bool"), Tensor([50803201],"bool"), )
[Prof] paddle.Tensor.logical_and 	 paddle.Tensor.logical_and(Tensor([50803201],"bool"), Tensor([50803201],"bool"), ) 	 101606402 	 1000 	 0.11838459968566895 	 0.11641263961791992 	 0.10910534858703613 	 0.10312509536743164 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:28.330320 test begin: paddle.Tensor.logical_not(Tensor([50803201],"bool"), )
[Prof] paddle.Tensor.logical_not 	 paddle.Tensor.logical_not(Tensor([50803201],"bool"), ) 	 50803201 	 1000 	 0.08195137977600098 	 0.07999801635742188 	 0.07360124588012695 	 0.06752562522888184 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:29.200168 test begin: paddle.Tensor.logical_or(Tensor([50803201],"bool"), Tensor([50803201],"bool"), )
[Prof] paddle.Tensor.logical_or 	 paddle.Tensor.logical_or(Tensor([50803201],"bool"), Tensor([50803201],"bool"), ) 	 101606402 	 1000 	 0.1179356575012207 	 0.11653852462768555 	 0.10101795196533203 	 0.0945746898651123 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:30.965653 test begin: paddle.Tensor.logit(x=Tensor([4, 3, 423361, 5],"float64"), eps=0.2, )
[Prof] paddle.Tensor.logit 	 paddle.Tensor.logit(x=Tensor([4, 3, 423361, 5],"float64"), eps=0.2, ) 	 25401660 	 1000 	 0.32492899894714355 	 0.30272412300109863 	 0.3155491352081299 	 0.29019808769226074 	 0.4436624050140381 	 0.45010924339294434 	 0.391695499420166 	 0.38636136054992676 	 
2025-07-24 15:40:33.569166 test begin: paddle.Tensor.logit(x=Tensor([4, 635041, 2, 5],"float64"), eps=0.2, )
[Prof] paddle.Tensor.logit 	 paddle.Tensor.logit(x=Tensor([4, 635041, 2, 5],"float64"), eps=0.2, ) 	 25401640 	 1000 	 0.3249344825744629 	 0.7472796440124512 	 0.31575512886047363 	 0.29036927223205566 	 0.443652868270874 	 0.44890642166137695 	 0.3914463520050049 	 0.3864471912384033 	 
2025-07-24 15:40:39.213429 test begin: paddle.Tensor.logit(x=Tensor([846721, 3, 2, 5],"float64"), eps=0.2, )
[Prof] paddle.Tensor.logit 	 paddle.Tensor.logit(x=Tensor([846721, 3, 2, 5],"float64"), eps=0.2, ) 	 25401630 	 1000 	 0.3287217617034912 	 0.30611324310302734 	 0.31554102897644043 	 0.289900541305542 	 0.44342923164367676 	 0.4489479064941406 	 0.3914458751678467 	 0.37809205055236816 	 
2025-07-24 15:40:41.915948 test begin: paddle.Tensor.lu(Tensor([16934401, 3],"float32"), )
W0724 15:40:56.330637 108248 backward.cc:466] While running Node (LuGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   LuGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::lu_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, bool, paddle::Tensor*)
4   void phi::LUGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 1.018826PB memory on GPU 0, 1.870117GB memory has been allocated and available memory is only 37.523743GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 15:40:56.345477 test begin: paddle.Tensor.lu(Tensor([2116801, 3, 2, 2],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0efc2d1420>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 15:51:03.955945 test begin: paddle.Tensor.lu(Tensor([2822401, 3, 3],"float64"), )
W0724 15:51:04.625892 114101 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f70719eb070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 16:01:10.763907 test begin: paddle.Tensor.lu(Tensor([3, 16934401],"float32"), )
W0724 16:01:11.743912 118216 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 16:01:21.982371 118216 backward.cc:462] While running Node (LuGradNode) raises an EnforceNotMet exception
[Error] (External) CUBLAS error(15). 
  [Hint: 'CUBLAS_STATUS_NOT_SUPPORTED'.  The functionality requested is not supported ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:212)

2025-07-24 16:01:22.006517 test begin: paddle.Tensor.lu(Tensor([3, 2822401, 3],"float64"), )
W0724 16:01:36.042179 118616 backward.cc:466] While running Node (LuGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   LuGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::lu_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, bool, paddle::Tensor*)
4   void phi::LUGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
5   double* phi::DeviceContext::Alloc<double>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 173.879687TB memory on GPU 0, 1.844727GB memory has been allocated and available memory is only 37.549133GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 16:01:37.262631 test begin: paddle.Tensor.lu(Tensor([3, 3, 2822401],"float64"), )
W0724 16:01:45.072487 118660 backward.cc:462] While running Node (LuGradNode) raises an EnforceNotMet exception
[Error] (External) CUBLAS error(15). 
  [Hint: 'CUBLAS_STATUS_NOT_SUPPORTED'.  The functionality requested is not supported ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:303)

2025-07-24 16:01:45.146979 test begin: paddle.Tensor.lu(Tensor([4, 1587601, 2, 2],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f29dcabb130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 16:11:51.813136 test begin: paddle.Tensor.lu(Tensor([4, 3, 1058401, 2],"float64"), )
W0724 16:11:52.508489 122639 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 16:12:05.392830 122639 backward.cc:466] While running Node (LuGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   LuGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::lu_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, bool, paddle::Tensor*)
4   void phi::LUGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
5   double* phi::DeviceContext::Alloc<double>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 97.807439TB memory on GPU 0, 1.797852GB memory has been allocated and available memory is only 37.596008GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 16:12:05.411733 test begin: paddle.Tensor.lu(Tensor([4, 3, 2, 1058401],"float64"), )
W0724 16:12:13.006846 122694 backward.cc:462] While running Node (LuGradNode) raises an EnforceNotMet exception
[Error] (External) CUBLAS error(15). 
  [Hint: 'CUBLAS_STATUS_NOT_SUPPORTED'.  The functionality requested is not supported ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:303)

2025-07-24 16:12:13.041922 test begin: paddle.Tensor.masked_fill(Tensor([1, 198451, 256],"float32"), Tensor([1, 198451, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 198451, 256],"float32"), Tensor([1, 198451, 1],"bool"), 0.0, ) 	 51001907 	 1000 	 0.14427947998046875 	 0.619598388671875 	 0.0490570068359375 	 0.21065950393676758 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:16.245139 test begin: paddle.Tensor.masked_fill(Tensor([1, 36828, 1380],"float32"), Tensor([1, 36828, 1380],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 36828, 1380],"float32"), Tensor([1, 36828, 1380],"bool"), 0.0, ) 	 101645280 	 1000 	 0.37657737731933594 	 0.6511683464050293 	 0.09623289108276367 	 0.2216489315032959 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:19.812679 test begin: paddle.Tensor.masked_fill(Tensor([1, 36828, 1380],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 36828, 1380],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, ) 	 50859468 	 1000 	 0.1424262523651123 	 0.6212663650512695 	 0.048471927642822266 	 0.21149945259094238 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:22.263707 test begin: paddle.Tensor.masked_fill(Tensor([1, 38367, 1325],"float32"), Tensor([1, 38367, 1325],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 38367, 1325],"float32"), Tensor([1, 38367, 1325],"bool"), 0.0, ) 	 101672550 	 1000 	 0.3773801326751709 	 0.6518635749816895 	 0.096435546875 	 0.22192049026489258 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:25.852674 test begin: paddle.Tensor.masked_fill(Tensor([1, 38367, 1325],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 38367, 1325],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, ) 	 50874642 	 1000 	 0.24709844589233398 	 0.6252360343933105 	 0.0841825008392334 	 0.21147751808166504 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:28.529082 test begin: paddle.Tensor.masked_fill(Tensor([1, 8550, 5942],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 8550, 5942],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, ) 	 50812650 	 1000 	 0.14232540130615234 	 0.6164407730102539 	 0.048418283462524414 	 0.20984840393066406 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:31.027250 test begin: paddle.Tensor.masked_fill(Tensor([1, 8550, 5942],"float32"), Tensor([1, 8550, 5942],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([1, 8550, 5942],"float32"), Tensor([1, 8550, 5942],"bool"), 0.0, ) 	 101608200 	 1000 	 0.3766186237335205 	 0.6872069835662842 	 0.09624242782592773 	 0.22152471542358398 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:34.661084 test begin: paddle.Tensor.masked_fill(Tensor([24, 8550, 256],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([24, 8550, 256],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, ) 	 52539750 	 1000 	 0.46799135208129883 	 0.6423330307006836 	 0.11703085899353027 	 0.3257749080657959 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:40.799185 test begin: paddle.Tensor.masked_fill(Tensor([24, 8550, 256],"float32"), Tensor([24, 8550, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([24, 8550, 256],"float32"), Tensor([24, 8550, 1],"bool"), 0.0, ) 	 52736400 	 1000 	 0.14815306663513184 	 0.636699914932251 	 0.05040121078491211 	 0.3252103328704834 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:43.557774 test begin: paddle.Tensor.masked_fill(Tensor([6, 36828, 256],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([6, 36828, 256],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, ) 	 56604636 	 1000 	 0.4929070472717285 	 0.6883318424224854 	 0.12557291984558105 	 0.3516573905944824 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:47.016619 test begin: paddle.Tensor.masked_fill(Tensor([6, 36828, 256],"float32"), Tensor([6, 36828, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([6, 36828, 256],"float32"), Tensor([6, 36828, 1],"bool"), 0.0, ) 	 56788776 	 1000 	 0.15828919410705566 	 0.6901059150695801 	 0.0538477897644043 	 0.34997129440307617 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:52.612715 test begin: paddle.Tensor.masked_fill(Tensor([6, 38367, 256],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([6, 38367, 256],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, ) 	 58970079 	 1000 	 0.5126974582672119 	 0.7191281318664551 	 0.13082623481750488 	 0.24480462074279785 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:56.178795 test begin: paddle.Tensor.masked_fill(Tensor([6, 38367, 256],"float32"), Tensor([6, 38367, 1],"bool"), 0.0, )
[Prof] paddle.Tensor.masked_fill 	 paddle.Tensor.masked_fill(Tensor([6, 38367, 256],"float32"), Tensor([6, 38367, 1],"bool"), 0.0, ) 	 59161914 	 1000 	 0.16471409797668457 	 0.7155570983886719 	 0.0560455322265625 	 0.24356484413146973 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:58.988013 test begin: paddle.Tensor.masked_select(Tensor([1016065, 50],"float32"), Tensor([1016065, 50],"bool"), )
[Prof] paddle.Tensor.masked_select 	 paddle.Tensor.masked_select(Tensor([1016065, 50],"float32"), Tensor([1016065, 50],"bool"), ) 	 101606500 	 1000 	 2.6187942028045654 	 2.4461312294006348 	 0.001645803451538086 	 0.0023360252380371094 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:13:09.143828 test begin: paddle.Tensor.masked_select(Tensor([15000, 3387],"float32"), Tensor([15000, 3387],"bool"), )
[Prof] paddle.Tensor.masked_select 	 paddle.Tensor.masked_select(Tensor([15000, 3387],"float32"), Tensor([15000, 3387],"bool"), ) 	 101610000 	 1000 	 1.3780827522277832 	 2.4548585414886475 	 0.0008597373962402344 	 0.002346038818359375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:13:16.990687 test begin: paddle.Tensor.masked_select(Tensor([50803201],"float32"), Tensor([50803201],"bool"), )
[Prof] paddle.Tensor.masked_select 	 paddle.Tensor.masked_select(Tensor([50803201],"float32"), Tensor([50803201],"bool"), ) 	 101606402 	 1000 	 4.803328275680542 	 1.1301324367523193 	 0.0029282569885253906 	 0.001033782958984375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:13:30.080071 test begin: paddle.Tensor.masked_select(Tensor([60000, 847],"float32"), Tensor([60000, 847],"bool"), )
[Prof] paddle.Tensor.masked_select 	 paddle.Tensor.masked_select(Tensor([60000, 847],"float32"), Tensor([60000, 847],"bool"), ) 	 101640000 	 1000 	 1.3758842945098877 	 2.438981056213379 	 0.0008647441864013672 	 0.002324342727661133 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:13:37.940191 test begin: paddle.Tensor.matmul(Tensor([110, 12, 197, 197],"float32"), Tensor([110, 12, 197, 64],"float32"), )
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([110, 12, 197, 197],"float32"), Tensor([110, 12, 197, 64],"float32"), ) 	 67870440 	 1000 	 1.0400166511535645 	 1.027550458908081 	 1.0144767761230469 	 0.9999463558197021 	 1.7516357898712158 	 1.7514009475708008 	 0.8950130939483643 	 0.8948659896850586 	 
2025-07-24 16:13:46.028369 test begin: paddle.Tensor.matmul(Tensor([124, 16, 100, 257],"float32"), Tensor([124, 16, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([124, 16, 100, 257],"float32"), Tensor([124, 16, 257, 64],"float32"), ) 	 83621632 	 1000 	 1.0256004333496094 	 1.0254952907562256 	 1.0128889083862305 	 1.0010535717010498 	 2.047497272491455 	 2.2186427116394043 	 1.0461571216583252 	 1.2175624370574951 	 
2025-07-24 16:13:55.284901 test begin: paddle.Tensor.matmul(Tensor([124, 16, 257, 257],"float32"), Tensor([124, 16, 257, 100],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([124, 16, 257, 257],"float32"), Tensor([124, 16, 257, 100],"float32"), ) 	 182030016 	 1000 	 2.9839608669281006 	 2.9814913272857666 	 2.9674019813537598 	 2.9558451175689697 	 6.660620212554932 	 6.679519891738892 	 3.403559446334839 	 3.4132721424102783 	 
2025-07-24 16:14:18.975609 test begin: paddle.Tensor.matmul(Tensor([124, 25, 257, 257],"float32"), Tensor([124, 25, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([124, 25, 257, 257],"float32"), Tensor([124, 25, 257, 64],"float32"), ) 	 255740700 	 1000 	 4.624057769775391 	 5.855747699737549 	 4.611187219619751 	 4.612461090087891 	 8.436004638671875 	 8.437190055847168 	 4.3119964599609375 	 4.311317682266235 	 
2025-07-24 16:14:52.346016 test begin: paddle.Tensor.matmul(Tensor([124, 7, 257, 257],"float32"), Tensor([124, 7, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([124, 7, 257, 257],"float32"), Tensor([124, 7, 257, 64],"float32"), ) 	 71607396 	 1000 	 1.340543508529663 	 1.3433477878570557 	 1.3275551795959473 	 1.319004774093628 	 2.412433385848999 	 2.4131524562835693 	 1.2326645851135254 	 1.2329676151275635 	 
2025-07-24 16:15:02.968945 test begin: paddle.Tensor.matmul(Tensor([128, 11, 197, 197],"float32"), Tensor([128, 11, 197, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 11, 197, 197],"float32"), Tensor([128, 11, 197, 64],"float32"), ) 	 72395136 	 1000 	 1.1042711734771729 	 1.1069762706756592 	 1.083592176437378 	 1.0733158588409424 	 1.8791263103485107 	 1.8786921501159668 	 0.9600656032562256 	 0.9598650932312012 	 
2025-07-24 16:15:10.443073 test begin: paddle.Tensor.matmul(Tensor([128, 12, 168, 197],"float32"), Tensor([128, 12, 197, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 12, 168, 197],"float32"), Tensor([128, 12, 197, 64],"float32"), ) 	 70201344 	 1000 	 1.1889681816101074 	 1.1891839504241943 	 1.1760945320129395 	 1.1651027202606201 	 1.8498170375823975 	 1.850649356842041 	 0.9451611042022705 	 0.9455764293670654 	 
2025-07-24 16:15:17.934421 test begin: paddle.Tensor.matmul(Tensor([128, 12, 197, 197],"float32"), Tensor([128, 12, 197, 168],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 12, 197, 197],"float32"), Tensor([128, 12, 197, 168],"float32"), ) 	 110446080 	 1000 	 2.331378936767578 	 2.3296620845794678 	 2.3185322284698486 	 2.3058199882507324 	 4.302803993225098 	 4.302542686462402 	 2.198655843734741 	 2.198451280593872 	 
2025-07-24 16:15:34.006689 test begin: paddle.Tensor.matmul(Tensor([128, 16, 257, 257],"float32"), Tensor([128, 16, 257, 97],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 16, 257, 257],"float32"), Tensor([128, 16, 257, 97],"float32"), ) 	 186322944 	 1000 	 3.0608129501342773 	 3.061145782470703 	 3.0447463989257812 	 3.0314176082611084 	 6.855276107788086 	 6.855854749679565 	 3.5031492710113525 	 3.5032365322113037 	 
2025-07-24 16:15:57.958082 test begin: paddle.Tensor.matmul(Tensor([128, 16, 97, 257],"float32"), Tensor([128, 16, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 16, 97, 257],"float32"), Tensor([128, 16, 257, 64],"float32"), ) 	 84740096 	 1000 	 1.025998830795288 	 1.0793492794036865 	 1.0131609439849854 	 1.0547406673431396 	 2.103886604309082 	 2.346020221710205 	 1.0749893188476562 	 1.3168959617614746 	 
2025-07-24 16:16:07.353976 test begin: paddle.Tensor.matmul(Tensor([128, 25, 257, 257],"float32"), Tensor([128, 25, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 25, 257, 257],"float32"), Tensor([128, 25, 257, 64],"float32"), ) 	 263990400 	 1000 	 4.7615907192230225 	 4.761984348297119 	 4.748691082000732 	 4.737565994262695 	 8.66825556755066 	 8.667528867721558 	 4.429349899291992 	 4.429031133651733 	 
2025-07-24 16:16:39.393174 test begin: paddle.Tensor.matmul(Tensor([128, 32, 197, 197],"float32"), Tensor([128, 32, 197, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 32, 197, 197],"float32"), Tensor([128, 32, 197, 64],"float32"), ) 	 210604032 	 1000 	 3.1029207706451416 	 3.1019363403320312 	 3.0900940895080566 	 3.078139543533325 	 5.3237950801849365 	 5.323129177093506 	 2.719804525375366 	 2.7200469970703125 	 
2025-07-24 16:17:00.349994 test begin: paddle.Tensor.matmul(Tensor([128, 7, 257, 257],"float32"), Tensor([128, 7, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([128, 7, 257, 257],"float32"), Tensor([128, 7, 257, 64],"float32"), ) 	 73917312 	 1000 	 1.3453621864318848 	 1.3452141284942627 	 1.3327534198760986 	 1.3210840225219727 	 2.4481515884399414 	 2.447880983352661 	 1.2509260177612305 	 1.2507574558258057 	 
2025-07-24 16:17:10.586570 test begin: paddle.Tensor.matmul(Tensor([194, 16, 257, 257],"float32"), Tensor([194, 16, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([194, 16, 257, 257],"float32"), Tensor([194, 16, 257, 64],"float32"), ) 	 256070688 	 1000 	 4.652003049850464 	 4.652180433273315 	 4.6392271518707275 	 4.627546310424805 	 8.446581840515137 	 8.446303367614746 	 4.3161375522613525 	 4.315976619720459 	 
2025-07-24 16:17:41.863465 test begin: paddle.Tensor.matmul(Tensor([336, 12, 197, 197],"float32"), Tensor([336, 12, 197, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([336, 12, 197, 197],"float32"), Tensor([336, 12, 197, 64],"float32"), ) 	 207313344 	 1000 	 3.0628762245178223 	 3.063188076019287 	 3.0480027198791504 	 3.03460693359375 	 5.248486042022705 	 5.247781038284302 	 2.6816422939300537 	 2.681605100631714 	 
2025-07-24 16:18:02.663903 test begin: paddle.Tensor.matmul(Tensor([49, 16, 257, 257],"float32"), Tensor([49, 16, 257, 64],"float32"), )
[Prof] paddle.Tensor.matmul 	 paddle.Tensor.matmul(Tensor([49, 16, 257, 257],"float32"), Tensor([49, 16, 257, 64],"float32"), ) 	 64677648 	 1000 	 1.185046672821045 	 1.1850998401641846 	 1.1723041534423828 	 1.1510515213012695 	 2.1519277095794678 	 2.152521848678589 	 1.0995609760284424 	 1.0998661518096924 	 
2025-07-24 16:18:10.537447 test begin: paddle.Tensor.max(Tensor([1, 400, 127009],"float32"), -2, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([1, 400, 127009],"float32"), -2, ) 	 50803600 	 1000 	 0.2600083351135254 	 0.19581317901611328 	 0.24753451347351074 	 0.1775505542755127 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:18:13.020925 test begin: paddle.Tensor.max(Tensor([1, 400, 127009],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([1, 400, 127009],"float32"), axis=-1, keepdim=True, ) 	 50803600 	 1000 	 0.17902922630310059 	 0.15988397598266602 	 0.09144401550292969 	 0.14445805549621582 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:18:16.033878 test begin: paddle.Tensor.max(Tensor([1, 772, 65856],"float32"), -2, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([1, 772, 65856],"float32"), -2, ) 	 50840832 	 1000 	 0.30852746963500977 	 0.16122794151306152 	 0.15758228302001953 	 0.14319586753845215 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:18:18.650663 test begin: paddle.Tensor.max(Tensor([1, 772, 65856],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([1, 772, 65856],"float32"), axis=-1, keepdim=True, ) 	 50840832 	 1000 	 0.16417860984802246 	 0.15764069557189941 	 0.0838782787322998 	 0.14219450950622559 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:18:20.795061 test begin: paddle.Tensor.max(Tensor([2, 400, 65856],"float32"), -2, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([2, 400, 65856],"float32"), -2, ) 	 52684800 	 1000 	 0.25890254974365234 	 0.16733717918395996 	 0.24726557731628418 	 0.1503291130065918 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:18:23.239574 test begin: paddle.Tensor.max(Tensor([2, 400, 65856],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([2, 400, 65856],"float32"), axis=-1, keepdim=True, ) 	 52684800 	 1000 	 0.16823697090148926 	 0.16231918334960938 	 0.08596324920654297 	 0.14662647247314453 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:18:25.491926 test begin: paddle.Tensor.max(Tensor([324000, 157],"float32"), axis=1, keepdim=True, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([324000, 157],"float32"), axis=1, keepdim=True, ) 	 50868000 	 1000 	 0.5788729190826416 	 0.41142916679382324 	 0.5665345191955566 	 0.39581918716430664 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:18:28.507881 test begin: paddle.Tensor.max(Tensor([635041, 80],"float32"), axis=1, keepdim=True, )
[Prof] paddle.Tensor.max 	 paddle.Tensor.max(Tensor([635041, 80],"float32"), axis=1, keepdim=True, ) 	 50803280 	 1000 	 0.5393314361572266 	 0.5401115417480469 	 0.5267837047576904 	 0.5245001316070557 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:18:31.605740 test begin: paddle.Tensor.mean(Tensor([124, 128, 34, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([124, 128, 34, 96],"float32"), 1, keepdim=True, ) 	 51806208 	 1000 	 0.2053685188293457 	 0.15149283409118652 	 0.1934223175048828 	 0.13718175888061523 	 0.14276528358459473 	 0.19733309745788574 	 0.08734512329101562 	 0.13019514083862305 	 
2025-07-24 16:18:33.100967 test begin: paddle.Tensor.mean(Tensor([124, 128, 96, 34],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([124, 128, 96, 34],"float32"), 1, keepdim=True, ) 	 51806208 	 1000 	 0.20529556274414062 	 0.15140962600708008 	 0.19318151473999023 	 0.13722753524780273 	 0.14273309707641602 	 0.1975867748260498 	 0.08715438842773438 	 0.12958741188049316 	 
2025-07-24 16:18:34.586878 test begin: paddle.Tensor.mean(Tensor([124, 45, 96, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([124, 45, 96, 96],"float32"), 1, keepdim=True, ) 	 51425280 	 1000 	 0.16529512405395508 	 0.1574995517730713 	 0.15329527854919434 	 0.14347100257873535 	 0.14725804328918457 	 0.21265673637390137 	 0.09203457832336426 	 0.14064240455627441 	 
2025-07-24 16:18:38.613501 test begin: paddle.Tensor.mean(Tensor([128, 128, 33, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([128, 128, 33, 96],"float32"), 1, keepdim=True, ) 	 51904512 	 1000 	 0.2057960033416748 	 0.1578226089477539 	 0.19383692741394043 	 0.1379692554473877 	 0.14310097694396973 	 0.1985771656036377 	 0.08591961860656738 	 0.10892772674560547 	 
2025-07-24 16:18:40.145367 test begin: paddle.Tensor.mean(Tensor([128, 128, 96, 33],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([128, 128, 96, 33],"float32"), 1, keepdim=True, ) 	 51904512 	 1000 	 0.2057662010192871 	 0.15236687660217285 	 0.1939225196838379 	 0.13788485527038574 	 0.14307880401611328 	 0.19850659370422363 	 0.08392620086669922 	 0.10685324668884277 	 
2025-07-24 16:18:41.679009 test begin: paddle.Tensor.mean(Tensor([128, 192, 22, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([128, 192, 22, 96],"float32"), 1, keepdim=True, ) 	 51904512 	 1000 	 0.21855711936950684 	 0.1505742073059082 	 0.20627975463867188 	 0.13652849197387695 	 0.1436769962310791 	 0.19698190689086914 	 0.08557295799255371 	 0.1069345474243164 	 
2025-07-24 16:18:43.192902 test begin: paddle.Tensor.mean(Tensor([128, 192, 96, 22],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([128, 192, 96, 22],"float32"), 1, keepdim=True, ) 	 51904512 	 1000 	 0.21850228309631348 	 0.15059256553649902 	 0.20648193359375 	 0.13660740852355957 	 0.14367246627807617 	 0.19698858261108398 	 0.08655881881713867 	 0.10882973670959473 	 
2025-07-24 16:18:44.689280 test begin: paddle.Tensor.mean(Tensor([128, 44, 96, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([128, 44, 96, 96],"float32"), 1, keepdim=True, ) 	 51904512 	 1000 	 0.16766977310180664 	 0.1574859619140625 	 0.15438079833984375 	 0.14324617385864258 	 0.14786314964294434 	 0.21547842025756836 	 0.09196925163269043 	 0.1275324821472168 	 
2025-07-24 16:18:46.186219 test begin: paddle.Tensor.mean(Tensor([29, 192, 96, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([29, 192, 96, 96],"float32"), 1, keepdim=True, ) 	 51314688 	 1000 	 0.16570138931274414 	 0.14812326431274414 	 0.15366744995117188 	 0.1340487003326416 	 0.1407613754272461 	 0.19222593307495117 	 0.0849003791809082 	 0.1256256103515625 	 
2025-07-24 16:18:47.614793 test begin: paddle.Tensor.mean(Tensor([44, 128, 96, 96],"float32"), 1, keepdim=True, )
[Prof] paddle.Tensor.mean 	 paddle.Tensor.mean(Tensor([44, 128, 96, 96],"float32"), 1, keepdim=True, ) 	 51904512 	 1000 	 0.16634011268615723 	 0.15084266662597656 	 0.15449810028076172 	 0.13654685020446777 	 0.14200282096862793 	 0.20213007926940918 	 0.08651304244995117 	 0.13483858108520508 	 
2025-07-24 16:18:49.071130 test begin: paddle.Tensor.min(Tensor([1, 193, 65856, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([1, 193, 65856, 4],"float32"), axis=-1, ) 	 50840832 	 1000 	 0.53867506980896 	 0.868661642074585 	 0.526202917098999 	 0.8505706787109375 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:18:52.870718 test begin: paddle.Tensor.min(Tensor([1, 400, 31753, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([1, 400, 31753, 4],"float32"), axis=-1, ) 	 50804800 	 1000 	 0.5382823944091797 	 0.8677928447723389 	 0.5258944034576416 	 0.8496561050415039 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:18:56.645944 test begin: paddle.Tensor.min(Tensor([1, 400, 65856, 2],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([1, 400, 65856, 2],"float32"), axis=-1, ) 	 52684800 	 1000 	 0.5173146724700928 	 0.824958324432373 	 0.5050516128540039 	 0.8037378787994385 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:19:00.738056 test begin: paddle.Tensor.min(Tensor([1, 400, 65856, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([1, 400, 65856, 4],"float32"), axis=-1, ) 	 105369600 	 1000 	 1.1119048595428467 	 1.7929818630218506 	 1.0994908809661865 	 1.7749505043029785 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:19:08.515528 test begin: paddle.Tensor.min(Tensor([15661, 4, 811],"float32"), axis=1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([15661, 4, 811],"float32"), axis=1, ) 	 50804284 	 1000 	 0.21967792510986328 	 0.3039262294769287 	 0.20775341987609863 	 0.2810523509979248 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:19:11.240844 test begin: paddle.Tensor.min(Tensor([24565, 3, 811],"float32"), axis=1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([24565, 3, 811],"float32"), axis=1, ) 	 59766645 	 1000 	 0.30515527725219727 	 0.4073350429534912 	 0.2931826114654541 	 0.389955997467041 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:19:14.725688 test begin: paddle.Tensor.min(Tensor([24565, 4, 518],"float32"), axis=1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([24565, 4, 518],"float32"), axis=1, ) 	 50898680 	 1000 	 0.2264082431793213 	 0.2677738666534424 	 0.21458721160888672 	 0.2477726936340332 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:19:17.422957 test begin: paddle.Tensor.min(Tensor([3, 525, 12096, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([3, 525, 12096, 4],"float32"), axis=-1, ) 	 76204800 	 1000 	 0.8056626319885254 	 1.3150548934936523 	 0.7934155464172363 	 1.2793238162994385 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:19:25.043895 test begin: paddle.Tensor.min(Tensor([4, 263, 12096, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([4, 263, 12096, 4],"float32"), axis=-1, ) 	 50899968 	 1000 	 0.5392723083496094 	 0.8725306987762451 	 0.5267815589904785 	 0.850409746170044 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:19:28.829036 test begin: paddle.Tensor.min(Tensor([4, 525, 12096, 3],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([4, 525, 12096, 3],"float32"), axis=-1, ) 	 76204800 	 1000 	 0.5268893241882324 	 0.8885903358459473 	 0.5146174430847168 	 0.8708369731903076 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:19:33.761459 test begin: paddle.Tensor.min(Tensor([4, 525, 6049, 4],"float32"), axis=-1, )
[Prof] paddle.Tensor.min 	 paddle.Tensor.min(Tensor([4, 525, 6049, 4],"float32"), axis=-1, ) 	 50811600 	 1000 	 0.538177490234375 	 0.8860244750976562 	 0.5235207080841064 	 0.8490653038024902 	 None 	 None 	 None 	 None 	 
[Error] got 2 tensors and 1 gradients
2025-07-24 16:19:39.702557 test begin: paddle.Tensor.mm(Tensor([10, 10],"float32"), Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.mm 	 paddle.Tensor.mm(Tensor([10, 10],"float32"), Tensor([10, 5080321],"float32"), ) 	 50803310 	 1000 	 0.6533191204071045 	 0.6529500484466553 	 0.13341140747070312 	 0.1334066390991211 	 1.4465885162353516 	 1.4421257972717285 	 0.21112751960754395 	 0.21052908897399902 	 
2025-07-24 16:19:45.460805 test begin: paddle.Tensor.mm(Tensor([5080321, 10],"float32"), Tensor([10, 10],"float32"), )
[Prof] paddle.Tensor.mm 	 paddle.Tensor.mm(Tensor([5080321, 10],"float32"), Tensor([10, 10],"float32"), ) 	 50803310 	 1000 	 0.6515295505523682 	 0.6511223316192627 	 0.13302826881408691 	 0.13303852081298828 	 1.414494276046753 	 1.4119489192962646 	 0.2064836025238037 	 0.20610427856445312 	 
2025-07-24 16:19:51.132493 test begin: paddle.Tensor.mode(Tensor([3, 2, 4233601],"float64"), )
[Prof] paddle.Tensor.mode 	 paddle.Tensor.mode(Tensor([3, 2, 4233601],"float64"), ) 	 25401606 	 1000 	 49.06723237037659 	 9.472237825393677 	 0.00011110305786132812 	 0.0002028942108154297 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:20:52.440527 test begin: paddle.Tensor.mode(Tensor([3, 2, 4233601],"float64"), axis=2, keepdim=True, )
[Prof] paddle.Tensor.mode 	 paddle.Tensor.mode(Tensor([3, 2, 4233601],"float64"), axis=2, keepdim=True, ) 	 25401606 	 1000 	 54.521300315856934 	 9.485665798187256 	 0.000102996826171875 	 0.00023889541625976562 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:21:59.184154 test begin: paddle.Tensor.mode(Tensor([3, 2822401, 3],"float64"), axis=1, keepdim=False, )
[Prof] paddle.Tensor.mode 	 paddle.Tensor.mode(Tensor([3, 2822401, 3],"float64"), axis=1, keepdim=False, ) 	 25401609 	 1000 	 59.352439403533936 	 10.793230295181274 	 0.00010323524475097656 	 0.0002455711364746094 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:23:14.245222 test begin: paddle.Tensor.moveaxis(x=Tensor([120961, 2, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([120961, 2, 3, 5, 7],"float64"), source=0, destination=2, ) 	 25401810 	 1000 	 0.00744175910949707 	 0.004658222198486328 	 1.2874603271484375e-05 	 2.0742416381835938e-05 	 0.04125165939331055 	 0.05318021774291992 	 4.1961669921875e-05 	 5.1975250244140625e-05 	 
2025-07-24 16:23:15.309600 test begin: paddle.Tensor.moveaxis(x=Tensor([120961, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([120961, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 25401810 	 1000 	 0.0074236392974853516 	 0.005732536315917969 	 1.3589859008789062e-05 	 1.9550323486328125e-05 	 0.039565086364746094 	 0.05398964881896973 	 3.504753112792969e-05 	 8.0108642578125e-05 	 
2025-07-24 16:23:16.372066 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 1058401],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 1058401],"float64"), source=0, destination=2, ) 	 25401624 	 1000 	 0.006857395172119141 	 0.004763603210449219 	 7.3909759521484375e-06 	 5.507469177246094e-05 	 0.040634870529174805 	 0.052944183349609375 	 2.09808349609375e-05 	 3.6716461181640625e-05 	 
2025-07-24 16:23:17.426259 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 151201, 7],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 151201, 7],"float64"), source=0, destination=2, ) 	 25401768 	 1000 	 0.006819486618041992 	 0.0046312808990478516 	 8.106231689453125e-06 	 1.9550323486328125e-05 	 0.03971433639526367 	 0.05285763740539551 	 2.8371810913085938e-05 	 4.982948303222656e-05 	 
2025-07-24 16:23:18.493059 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 151201, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 151201, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 25401768 	 1000 	 0.007493019104003906 	 0.005776643753051758 	 1.52587890625e-05 	 1.811981201171875e-05 	 0.03971076011657715 	 0.05301356315612793 	 3.457069396972656e-05 	 5.650520324707031e-05 	 
2025-07-24 16:23:19.548620 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 5, 211681],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 5, 211681],"float64"), source=0, destination=2, ) 	 25401720 	 1000 	 0.006762027740478516 	 0.0046269893646240234 	 1.239776611328125e-05 	 1.811981201171875e-05 	 0.039632558822631836 	 0.052736759185791016 	 3.886222839355469e-05 	 4.076957702636719e-05 	 
2025-07-24 16:23:20.620035 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 5, 211681],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 5, 211681],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 25401720 	 1000 	 0.0073931217193603516 	 0.005887269973754883 	 8.344650268554688e-06 	 1.9788742065429688e-05 	 0.03954505920410156 	 0.055156707763671875 	 2.0742416381835938e-05 	 4.124641418457031e-05 	 
2025-07-24 16:23:21.681765 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 635041, 5],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([4, 2, 635041, 5],"float64"), source=0, destination=2, ) 	 25401640 	 1000 	 0.0068645477294921875 	 0.004517555236816406 	 7.152557373046875e-06 	 2.288818359375e-05 	 0.03944754600524902 	 0.05463004112243652 	 1.7642974853515625e-05 	 9.012222290039062e-05 	 
2025-07-24 16:23:22.739308 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 90721, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([4, 2, 90721, 5, 7],"float64"), source=0, destination=2, ) 	 25401880 	 1000 	 0.006873607635498047 	 0.004670381546020508 	 1.5974044799804688e-05 	 1.8835067749023438e-05 	 0.04034256935119629 	 0.05556321144104004 	 3.719329833984375e-05 	 0.00015854835510253906 	 
2025-07-24 16:23:23.806009 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 90721, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([4, 2, 90721, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 25401880 	 1000 	 0.0074694156646728516 	 0.005759477615356445 	 9.298324584960938e-06 	 2.0503997802734375e-05 	 0.03947019577026367 	 0.052675724029541016 	 2.7418136596679688e-05 	 4.076957702636719e-05 	 
2025-07-24 16:23:24.880393 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 423361, 3, 5],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([4, 423361, 3, 5],"float64"), source=0, destination=2, ) 	 25401660 	 1000 	 0.006917715072631836 	 0.004668235778808594 	 1.4543533325195312e-05 	 1.9311904907226562e-05 	 0.039449214935302734 	 0.05455970764160156 	 2.2172927856445312e-05 	 6.4849853515625e-05 	 
2025-07-24 16:23:25.941700 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 60481, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([4, 60481, 3, 5, 7],"float64"), source=0, destination=2, ) 	 25402020 	 1000 	 0.006815433502197266 	 0.005601406097412109 	 1.0251998901367188e-05 	 8.726119995117188e-05 	 0.03955698013305664 	 0.0700826644897461 	 2.4557113647460938e-05 	 3.719329833984375e-05 	 
2025-07-24 16:23:27.073760 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 60481, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([4, 60481, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 25402020 	 1000 	 0.00747370719909668 	 0.0057713985443115234 	 8.344650268554688e-06 	 2.1696090698242188e-05 	 0.03982734680175781 	 0.05414104461669922 	 2.0503997802734375e-05 	 3.981590270996094e-05 	 
2025-07-24 16:23:28.137640 test begin: paddle.Tensor.moveaxis(x=Tensor([846721, 2, 3, 5],"float64"), source=0, destination=2, )
[Prof] paddle.Tensor.moveaxis 	 paddle.Tensor.moveaxis(x=Tensor([846721, 2, 3, 5],"float64"), source=0, destination=2, ) 	 25401630 	 1000 	 0.006780147552490234 	 0.004522085189819336 	 7.3909759521484375e-06 	 1.9550323486328125e-05 	 0.0395205020904541 	 0.05311465263366699 	 2.4080276489257812e-05 	 7.104873657226562e-05 	 
2025-07-24 16:23:29.199804 test begin: paddle.Tensor.multiply(Tensor([132301, 768],"float16"), Tensor([132301, 1],"float32"), )
W0724 16:23:31.027619 127618 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([132301, 768],"float16"), Tensor([132301, 1],"float32"), ) 	 101739469 	 1000 	 1.0693228244781494 	 0.7344987392425537 	 0.5462965965270996 	 0.7145600318908691 	 1.9088027477264404 	 2.1270699501037598 	 0.6496896743774414 	 0.5430703163146973 	 
2025-07-24 16:23:42.306735 test begin: paddle.Tensor.multiply(Tensor([160, 317521],"float32"), Tensor([160, 1],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([160, 317521],"float32"), Tensor([160, 1],"float32"), ) 	 50803520 	 1000 	 0.29642796516418457 	 0.30344510078430176 	 0.2864036560058594 	 0.29105162620544434 	 0.7706811428070068 	 0.9224593639373779 	 0.26216888427734375 	 0.2353365421295166 	 
2025-07-24 16:23:47.690935 test begin: paddle.Tensor.multiply(Tensor([160, 317521],"float32"), Tensor([160, 317521],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([160, 317521],"float32"), Tensor([160, 317521],"float32"), ) 	 101606720 	 1000 	 0.44979095458984375 	 0.44681262969970703 	 0.4403223991394043 	 0.43501925468444824 	 1.1285078525543213 	 0.8932006359100342 	 1.067676305770874 	 0.4563872814178467 	 
2025-07-24 16:23:53.568172 test begin: paddle.Tensor.multiply(Tensor([160, 635041],"float16"), Tensor([160, 1],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([160, 635041],"float16"), Tensor([160, 1],"float32"), ) 	 101606720 	 1000 	 1.0712347030639648 	 0.6964185237884521 	 0.5472989082336426 	 0.6811387538909912 	 1.9219543933868408 	 2.1360199451446533 	 0.4905712604522705 	 0.43601226806640625 	 
2025-07-24 16:24:02.976578 test begin: paddle.Tensor.multiply(Tensor([16538, 3072],"float32"), Tensor([16538, 1],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([16538, 3072],"float32"), Tensor([16538, 1],"float32"), ) 	 50821274 	 1000 	 0.2953319549560547 	 0.30804967880249023 	 0.28496718406677246 	 0.2955923080444336 	 0.7363412380218506 	 0.9006285667419434 	 0.3762185573577881 	 0.3066544532775879 	 
2025-07-24 16:24:06.759180 test begin: paddle.Tensor.multiply(Tensor([33076, 3072],"float16"), Tensor([33076, 1],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([33076, 3072],"float16"), Tensor([33076, 1],"float32"), ) 	 101642548 	 1000 	 1.0670108795166016 	 0.7230141162872314 	 0.5451178550720215 	 0.7102203369140625 	 1.9034504890441895 	 2.131035566329956 	 0.6478478908538818 	 0.5440824031829834 	 
2025-07-24 16:24:15.989750 test begin: paddle.Tensor.multiply(Tensor([512, 198451],"float16"), Tensor([512, 1],"float32"), )
[Prof] paddle.Tensor.multiply 	 paddle.Tensor.multiply(Tensor([512, 198451],"float16"), Tensor([512, 1],"float32"), ) 	 101607424 	 1000 	 1.0684871673583984 	 0.694800615310669 	 0.5458531379699707 	 0.6822059154510498 	 1.9465720653533936 	 2.155336618423462 	 0.4968247413635254 	 0.5504035949707031 	 
2025-07-24 16:24:25.304611 test begin: paddle.Tensor.nansum(Tensor([105841, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([105841, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401840 	 1000 	 1.0637385845184326 	 0.18950819969177246 	 0.2719705104827881 	 0.17340993881225586 	 0.5297207832336426 	 0.44141650199890137 	 0.27060604095458984 	 0.150299072265625 	 
2025-07-24 16:24:28.176376 test begin: paddle.Tensor.nansum(Tensor([2822401, 3, 3],"float64"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([2822401, 3, 3],"float64"), ) 	 25401609 	 1000 	 0.9355607032775879 	 0.1502082347869873 	 0.19098782539367676 	 0.0767526626586914 	 0.46454882621765137 	 0.4127359390258789 	 0.23734140396118164 	 0.14049458503723145 	 
2025-07-24 16:24:30.622853 test begin: paddle.Tensor.nansum(Tensor([3, 2, 105841, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2, 105841, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401840 	 1000 	 1.0637249946594238 	 0.1894981861114502 	 0.27198314666748047 	 0.17349815368652344 	 0.529717206954956 	 0.44138216972351074 	 0.2706179618835449 	 0.15031147003173828 	 
2025-07-24 16:24:33.459557 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 141121, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2, 3, 141121, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401780 	 1000 	 5.649128198623657 	 0.17336511611938477 	 1.1571886539459229 	 0.08856630325317383 	 0.4655027389526367 	 0.417194128036499 	 0.2377777099609375 	 0.14205074310302734 	 
2025-07-24 16:24:40.723267 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 176401, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 176401, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401744 	 1000 	 0.974083423614502 	 0.19017481803894043 	 0.2490372657775879 	 0.1741645336151123 	 0.5133016109466553 	 0.44329404830932617 	 0.2622394561767578 	 0.15092206001281738 	 
2025-07-24 16:24:43.445303 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 1, 70561],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 1, 70561],"float64"), axis=3, keepdim=True, ) 	 25401960 	 1000 	 0.973858118057251 	 0.200913667678833 	 0.24893975257873535 	 0.17458271980285645 	 0.5134093761444092 	 0.4437253475189209 	 0.26229429244995117 	 0.15108656883239746 	 
2025-07-24 16:24:46.202824 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 35281, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 35281, 2],"float64"), axis=3, keepdim=True, ) 	 25402320 	 1000 	 0.9737679958343506 	 0.1901841163635254 	 0.24892473220825195 	 0.17392683029174805 	 0.5132229328155518 	 0.44505882263183594 	 0.26218748092651367 	 0.1515369415283203 	 
2025-07-24 16:24:48.931668 test begin: paddle.Tensor.nansum(Tensor([3, 2822401, 3],"float64"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 2822401, 3],"float64"), ) 	 25401609 	 1000 	 0.9356098175048828 	 0.15028834342956543 	 0.19105267524719238 	 0.07676935195922852 	 0.4644782543182373 	 0.4128074645996094 	 0.23731040954589844 	 0.1405494213104248 	 
2025-07-24 16:24:52.997570 test begin: paddle.Tensor.nansum(Tensor([3, 3, 2822401],"float64"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 3, 2822401],"float64"), ) 	 25401609 	 1000 	 0.9359607696533203 	 0.1502377986907959 	 0.19126009941101074 	 0.0767362117767334 	 0.46513795852661133 	 0.41283082962036133 	 0.23745226860046387 	 0.14054656028747559 	 
2025-07-24 16:24:56.602412 test begin: paddle.Tensor.nansum(Tensor([3, 3, 5644801],"float32"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 3, 5644801],"float32"), ) 	 50803209 	 1000 	 1.009366512298584 	 0.1527392864227295 	 0.20603370666503906 	 0.07803750038146973 	 0.5565927028656006 	 0.5893607139587402 	 0.2843451499938965 	 0.20072078704833984 	 
2025-07-24 16:24:59.738284 test begin: paddle.Tensor.nansum(Tensor([3, 5644801, 3],"float32"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 5644801, 3],"float32"), ) 	 50803209 	 1000 	 1.0093650817871094 	 0.15276241302490234 	 0.20606136322021484 	 0.07805299758911133 	 0.5565259456634521 	 0.5893716812133789 	 0.28433847427368164 	 0.20072555541992188 	 
2025-07-24 16:25:02.846658 test begin: paddle.Tensor.nansum(Tensor([3, 70561, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([3, 70561, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401960 	 1000 	 1.0639300346374512 	 0.18950128555297852 	 0.27204132080078125 	 0.17360734939575195 	 0.5309414863586426 	 0.44167065620422363 	 0.2712728977203369 	 0.15037918090820312 	 
2025-07-24 16:25:05.697646 test begin: paddle.Tensor.nansum(Tensor([5644801, 3, 3],"float32"), )
[Prof] paddle.Tensor.nansum 	 paddle.Tensor.nansum(Tensor([5644801, 3, 3],"float32"), ) 	 50803209 	 1000 	 1.00931715965271 	 0.15278196334838867 	 0.20606279373168945 	 0.07807064056396484 	 0.5565693378448486 	 0.5893299579620361 	 0.2843592166900635 	 0.2007131576538086 	 
2025-07-24 16:25:08.776733 test begin: paddle.Tensor.neg(Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.neg 	 paddle.Tensor.neg(Tensor([50803201],"float32"), ) 	 50803201 	 1000 	 0.29575347900390625 	 0.3025319576263428 	 0.2864663600921631 	 0.2870364189147949 	 0.2957494258880615 	 0.29775214195251465 	 0.2439720630645752 	 0.2362351417541504 	 
2025-07-24 16:25:11.512269 test begin: paddle.Tensor.nonzero(Tensor([3628801, 14],"bool"), )
[Prof] paddle.Tensor.nonzero 	 paddle.Tensor.nonzero(Tensor([3628801, 14],"bool"), ) 	 50803214 	 1000 	 5.9103474617004395 	 1.4123713970184326 	 0.004035472869873047 	 0.0013208389282226562 	 None 	 None 	 None 	 None 	 
2025-07-24 16:25:19.547075 test begin: paddle.Tensor.nonzero(Tensor([3907939, 13],"bool"), )
[Prof] paddle.Tensor.nonzero 	 paddle.Tensor.nonzero(Tensor([3907939, 13],"bool"), ) 	 50803207 	 1000 	 5.918722629547119 	 1.4129359722137451 	 0.004024505615234375 	 0.0013208389282226562 	 None 	 None 	 None 	 None 	 
2025-07-24 16:25:27.617850 test begin: paddle.Tensor.nonzero(Tensor([4233601, 12],"bool"), )
[Prof] paddle.Tensor.nonzero 	 paddle.Tensor.nonzero(Tensor([4233601, 12],"bool"), ) 	 50803212 	 1000 	 5.907280206680298 	 1.4123814105987549 	 0.004022121429443359 	 0.0013203620910644531 	 None 	 None 	 None 	 None 	 
2025-07-24 16:25:37.295344 test begin: paddle.Tensor.nonzero(Tensor([52640, 966],"bool"), )
[Prof] paddle.Tensor.nonzero 	 paddle.Tensor.nonzero(Tensor([52640, 966],"bool"), ) 	 50850240 	 1000 	 6.5389275550842285 	 1.4130899906158447 	 0.004023313522338867 	 0.0013217926025390625 	 None 	 None 	 None 	 None 	 
2025-07-24 16:25:46.602849 test begin: paddle.Tensor.norm(Tensor([100352, 507],"float32"), )
[Prof] paddle.Tensor.norm 	 paddle.Tensor.norm(Tensor([100352, 507],"float32"), ) 	 50878464 	 1000 	 0.15269160270690918 	 0.1524498462677002 	 0.051880836486816406 	 0.07782411575317383 	 0.996800422668457 	 0.9114551544189453 	 0.9439961910247803 	 0.23298001289367676 	 
2025-07-24 16:25:49.622731 test begin: paddle.Tensor.norm(Tensor([507, 100352],"float32"), )
[Prof] paddle.Tensor.norm 	 paddle.Tensor.norm(Tensor([507, 100352],"float32"), ) 	 50878464 	 1000 	 0.15266942977905273 	 0.1523888111114502 	 0.051882028579711914 	 0.07781696319580078 	 0.9967873096466064 	 0.9114339351654053 	 0.9435694217681885 	 0.23298287391662598 	 
2025-07-24 16:25:52.618129 test begin: paddle.Tensor.norm(Tensor([6202, 8192],"float32"), )
[Prof] paddle.Tensor.norm 	 paddle.Tensor.norm(Tensor([6202, 8192],"float32"), ) 	 50806784 	 1000 	 0.15264344215393066 	 0.15227699279785156 	 0.05187559127807617 	 0.07782530784606934 	 0.9958651065826416 	 0.9101476669311523 	 0.9429945945739746 	 0.23266291618347168 	 
2025-07-24 16:25:55.673270 test begin: paddle.Tensor.norm(Tensor([8192, 6202],"float32"), )
[Prof] paddle.Tensor.norm 	 paddle.Tensor.norm(Tensor([8192, 6202],"float32"), ) 	 50806784 	 1000 	 0.1526637077331543 	 0.152327299118042 	 0.051905155181884766 	 0.07776236534118652 	 0.9960336685180664 	 0.9102375507354736 	 0.9435687065124512 	 0.23270654678344727 	 
2025-07-24 16:26:01.144244 test begin: paddle.Tensor.norm(Tensor([886, 57344],"float32"), )
[Prof] paddle.Tensor.norm 	 paddle.Tensor.norm(Tensor([886, 57344],"float32"), ) 	 50806784 	 1000 	 0.15266156196594238 	 0.15222764015197754 	 0.05188775062561035 	 0.07773113250732422 	 0.9958107471466064 	 0.9102249145507812 	 0.9423580169677734 	 0.2326366901397705 	 
2025-07-24 16:26:04.128391 test begin: paddle.Tensor.not_equal(Tensor([128, 198451],"int64"), Tensor([128, 198451],"int64"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([128, 198451],"int64"), Tensor([128, 198451],"int64"), ) 	 50803456 	 1000 	 0.3101992607116699 	 0.3132009506225586 	 0.3011782169342041 	 0.3014523983001709 	 None 	 None 	 None 	 None 	 
2025-07-24 16:26:05.496281 test begin: paddle.Tensor.not_equal(Tensor([13, 1953970],"int64"), Tensor([1],"int64"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([13, 1953970],"int64"), Tensor([1],"int64"), ) 	 25401611 	 1000 	 0.17598938941955566 	 0.18010687828063965 	 0.16603684425354004 	 0.16782093048095703 	 None 	 None 	 None 	 None 	 
2025-07-24 16:26:06.218684 test begin: paddle.Tensor.not_equal(Tensor([13, 3907939],"bool"), Tensor([1],"bool"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([13, 3907939],"bool"), Tensor([1],"bool"), ) 	 50803208 	 1000 	 0.13741040229797363 	 0.19836783409118652 	 0.12677264213562012 	 0.18613266944885254 	 None 	 None 	 None 	 None 	 
2025-07-24 16:26:07.243476 test begin: paddle.Tensor.not_equal(Tensor([1814401, 14],"int64"), Tensor([1],"int64"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([1814401, 14],"int64"), Tensor([1],"int64"), ) 	 25401615 	 1000 	 0.17601990699768066 	 0.18012189865112305 	 0.16618037223815918 	 0.16810822486877441 	 None 	 None 	 None 	 None 	 
2025-07-24 16:26:07.963960 test begin: paddle.Tensor.not_equal(Tensor([198451, 128],"int64"), Tensor([198451, 128],"int64"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([198451, 128],"int64"), Tensor([198451, 128],"int64"), ) 	 50803456 	 1000 	 0.3101789951324463 	 0.3131837844848633 	 0.30118703842163086 	 0.30183839797973633 	 None 	 None 	 None 	 None 	 
2025-07-24 16:26:09.326398 test begin: paddle.Tensor.not_equal(Tensor([3628801, 14],"bool"), Tensor([1],"bool"), )
[Prof] paddle.Tensor.not_equal 	 paddle.Tensor.not_equal(Tensor([3628801, 14],"bool"), Tensor([1],"bool"), ) 	 50803215 	 1000 	 0.1374058723449707 	 0.19832539558410645 	 0.12771058082580566 	 0.18634605407714844 	 None 	 None 	 None 	 None 	 
2025-07-24 16:26:10.352820 test begin: paddle.Tensor.outer(x=Tensor([12700801, 2],"float64"), y=Tensor([2, 3, 4],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([12700801, 2],"float64"), y=Tensor([2, 3, 4],"float64"), ) 	 25401626 	 1000 	 3.8785462379455566 	 3.8154051303863525 	 0.15853452682495117 	 0.9746863842010498 	 7.481978893280029 	 22.858909368515015 	 2.547283172607422 	 1.1666173934936523 	 
2025-07-24 16:27:03.675267 test begin: paddle.Tensor.outer(x=Tensor([4, 2, 3175201],"float64"), y=Tensor([4, 2, 3],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2, 3175201],"float64"), y=Tensor([4, 2, 3],"float64"), ) 	 25401632 	 1000 	 4.602829694747925 	 3.825699806213379 	 0.15848898887634277 	 0.9772577285766602 	 7.481479167938232 	 22.804904460906982 	 2.546975612640381 	 1.1637046337127686 	 
2025-07-24 16:27:55.344497 test begin: paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4, 2, 3175201],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4, 2, 3175201],"float64"), ) 	 25401632 	 1000 	 4.040893793106079 	 7.110415935516357 	 0.16501808166503906 	 1.8165199756622314 	 7.440601110458374 	 25.534491777420044 	 2.5333287715911865 	 1.304551601409912 	 
2025-07-24 16:28:51.917440 test begin: paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4, 2116801, 3],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4, 2116801, 3],"float64"), ) 	 25401636 	 1000 	 4.034071922302246 	 7.087973594665527 	 0.16488862037658691 	 1.8086826801300049 	 7.454346656799316 	 25.50910711288452 	 2.538336992263794 	 1.3031024932861328 	 
2025-07-24 16:29:50.902044 test begin: paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2, 3],"float64"), y=Tensor([4233601, 2, 3],"float64"), ) 	 25401630 	 1000 	 4.171964168548584 	 7.124369382858276 	 0.1704728603363037 	 1.8201143741607666 	 7.4508216381073 	 25.54535174369812 	 2.5372018814086914 	 1.3048999309539795 	 
2025-07-24 16:30:47.847558 test begin: paddle.Tensor.outer(x=Tensor([4, 2116801, 3],"float64"), y=Tensor([4, 2, 3],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2116801, 3],"float64"), y=Tensor([4, 2, 3],"float64"), ) 	 25401636 	 1000 	 3.8799002170562744 	 3.8330423831939697 	 0.15854740142822266 	 0.978219747543335 	 7.481975078582764 	 22.907334566116333 	 2.5471084117889404 	 1.2617542743682861 	 
2025-07-24 16:31:39.736039 test begin: paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2, 3, 4233601],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2, 3, 4233601],"float64"), ) 	 25401614 	 1000 	 1.5328075885772705 	 2.3752338886260986 	 0.0619659423828125 	 2.351299285888672 	 2.751979351043701 	 8.33608365058899 	 0.9370529651641846 	 1.702888011932373 	 
2025-07-24 16:31:59.135461 test begin: paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2, 3175201, 4],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2, 3175201, 4],"float64"), ) 	 25401616 	 1000 	 1.514496088027954 	 2.3602752685546875 	 0.061884164810180664 	 2.345449447631836 	 2.7474188804626465 	 8.332589864730835 	 0.9354150295257568 	 1.7022836208343506 	 
2025-07-24 16:32:18.380294 test begin: paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2116801, 3, 4],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 2],"float64"), y=Tensor([2116801, 3, 4],"float64"), ) 	 25401620 	 1000 	 1.510019302368164 	 2.352961540222168 	 0.06176495552062988 	 2.338028907775879 	 2.747396469116211 	 8.32857632637024 	 0.9353797435760498 	 1.701442003250122 	 
2025-07-24 16:32:40.695633 test begin: paddle.Tensor.outer(x=Tensor([4, 6350401],"float64"), y=Tensor([2, 3, 4],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4, 6350401],"float64"), y=Tensor([2, 3, 4],"float64"), ) 	 25401628 	 1000 	 3.885387420654297 	 3.8297171592712402 	 0.15856456756591797 	 0.9783093929290771 	 7.481904029846191 	 22.76207184791565 	 2.547375440597534 	 1.1616320610046387 	 
2025-07-24 16:33:31.350260 test begin: paddle.Tensor.outer(x=Tensor([4233601, 2, 3],"float64"), y=Tensor([4, 2, 3],"float64"), )
[Prof] paddle.Tensor.outer 	 paddle.Tensor.outer(x=Tensor([4233601, 2, 3],"float64"), y=Tensor([4, 2, 3],"float64"), ) 	 25401630 	 1000 	 3.8833019733428955 	 3.8201451301574707 	 0.15856313705444336 	 0.975745439529419 	 7.481448173522949 	 22.784000873565674 	 2.5471694469451904 	 1.1628587245941162 	 
2025-07-24 16:34:21.506488 test begin: paddle.Tensor.pow(Tensor([124, 128, 34, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([124, 128, 34, 96],"float32"), 2, ) 	 51806208 	 1000 	 0.3787264823913574 	 0.3039071559906006 	 0.3691070079803467 	 0.29099297523498535 	 0.46039485931396484 	 1.0731816291809082 	 0.40573883056640625 	 0.3656148910522461 	 
2025-07-24 16:34:25.451511 test begin: paddle.Tensor.pow(Tensor([124, 128, 96, 34],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([124, 128, 96, 34],"float32"), 2, ) 	 51806208 	 1000 	 0.37812042236328125 	 0.3051137924194336 	 0.3689687252044678 	 0.2910299301147461 	 0.4604194164276123 	 1.0731801986694336 	 0.4064319133758545 	 0.3655660152435303 	 
2025-07-24 16:34:29.300263 test begin: paddle.Tensor.pow(Tensor([124, 45, 96, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([124, 45, 96, 96],"float32"), 2, ) 	 51425280 	 1000 	 0.3746616840362549 	 0.3015928268432617 	 0.3653538227081299 	 0.28805088996887207 	 0.4568159580230713 	 1.0652587413787842 	 0.40347790718078613 	 0.3629145622253418 	 
2025-07-24 16:34:33.111264 test begin: paddle.Tensor.pow(Tensor([128, 128, 33, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([128, 128, 33, 96],"float32"), 2, ) 	 51904512 	 1000 	 0.37683939933776855 	 0.7724838256835938 	 0.3676011562347412 	 0.2909965515136719 	 0.4615018367767334 	 1.0751869678497314 	 0.40813708305358887 	 0.3662452697753906 	 
2025-07-24 16:34:39.916041 test begin: paddle.Tensor.pow(Tensor([128, 128, 96, 33],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([128, 128, 96, 33],"float32"), 2, ) 	 51904512 	 1000 	 0.3762996196746826 	 0.30434179306030273 	 0.36707592010498047 	 0.2915680408477783 	 0.4617583751678467 	 1.0751452445983887 	 0.40833473205566406 	 0.3662533760070801 	 
2025-07-24 16:34:43.771197 test begin: paddle.Tensor.pow(Tensor([128, 192, 22, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([128, 192, 22, 96],"float32"), 2, ) 	 51904512 	 1000 	 0.3763091564178467 	 0.31917667388916016 	 0.3670318126678467 	 0.2915639877319336 	 0.4617629051208496 	 1.075105905532837 	 0.40888094902038574 	 0.36630749702453613 	 
2025-07-24 16:34:50.121178 test begin: paddle.Tensor.pow(Tensor([128, 192, 96, 22],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([128, 192, 96, 22],"float32"), 2, ) 	 51904512 	 1000 	 0.3763158321380615 	 0.31243395805358887 	 0.3669893741607666 	 0.2914156913757324 	 0.4617593288421631 	 1.0750372409820557 	 0.40854477882385254 	 0.3662374019622803 	 
2025-07-24 16:34:53.980467 test begin: paddle.Tensor.pow(Tensor([128, 44, 96, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([128, 44, 96, 96],"float32"), 2, ) 	 51904512 	 1000 	 0.37631821632385254 	 0.3043665885925293 	 0.36693835258483887 	 0.2886817455291748 	 0.4618213176727295 	 1.0751805305480957 	 0.4085671901702881 	 0.3662872314453125 	 
2025-07-24 16:34:57.889937 test begin: paddle.Tensor.pow(Tensor([29, 192, 96, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([29, 192, 96, 96],"float32"), 2, ) 	 51314688 	 1000 	 0.3720588684082031 	 0.3009324073791504 	 0.3620269298553467 	 0.2878592014312744 	 0.4563581943511963 	 1.0629663467407227 	 0.40282297134399414 	 0.3621370792388916 	 
2025-07-24 16:35:01.757346 test begin: paddle.Tensor.pow(Tensor([44, 128, 96, 96],"float32"), 2, )
[Prof] paddle.Tensor.pow 	 paddle.Tensor.pow(Tensor([44, 128, 96, 96],"float32"), 2, ) 	 51904512 	 1000 	 0.376880407333374 	 0.30437636375427246 	 0.3677024841308594 	 0.29156923294067383 	 0.4616706371307373 	 1.0751655101776123 	 0.4071681499481201 	 0.36629271507263184 	 
2025-07-24 16:35:05.631563 test begin: paddle.Tensor.prod(Tensor([1, 386, 65856, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([1, 386, 65856, 2],"float32"), -1, ) 	 50840832 	 1000 	 0.38846564292907715 	 0.46783018112182617 	 0.37246108055114746 	 0.4530212879180908 	 1.6771667003631592 	 2.0592527389526367 	 1.6221120357513428 	 0.0007195472717285156 	 
2025-07-24 16:35:11.408523 test begin: paddle.Tensor.prod(Tensor([1, 400, 63505, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([1, 400, 63505, 2],"float32"), -1, ) 	 50804000 	 1000 	 0.3894343376159668 	 0.4674065113067627 	 0.3737213611602783 	 0.4533045291900635 	 1.6768038272857666 	 2.0603976249694824 	 1.621967077255249 	 0.0007052421569824219 	 
2025-07-24 16:35:17.229510 test begin: paddle.Tensor.prod(Tensor([1, 400, 65856, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([1, 400, 65856, 2],"float32"), -1, ) 	 52684800 	 1000 	 0.4022858142852783 	 0.48470187187194824 	 0.38680171966552734 	 0.47055935859680176 	 1.7380952835083008 	 2.1298978328704834 	 1.6826393604278564 	 0.0007455348968505859 	 
2025-07-24 16:35:23.205633 test begin: paddle.Tensor.prod(Tensor([2100, 12096, 3],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([2100, 12096, 3],"float32"), -1, ) 	 76204800 	 1000 	 0.41387462615966797 	 0.5120887756347656 	 0.3982241153717041 	 0.49807214736938477 	 1.8902149200439453 	 2.7230379581451416 	 1.8347492218017578 	 0.0010001659393310547 	 
2025-07-24 16:35:30.330906 test begin: paddle.Tensor.prod(Tensor([2100, 12097, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([2100, 12097, 2],"float32"), -1, ) 	 50807400 	 1000 	 0.3878469467163086 	 0.46750903129577637 	 0.37207865715026855 	 0.4534945487976074 	 1.2664005756378174 	 2.05741548538208 	 1.2113125324249268 	 0.0007064342498779297 	 
2025-07-24 16:35:37.138312 test begin: paddle.Tensor.prod(Tensor([2101, 12096, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([2101, 12096, 2],"float32"), -1, ) 	 50827392 	 1000 	 0.38795995712280273 	 0.47991490364074707 	 0.37230634689331055 	 0.45342040061950684 	 1.2666044235229492 	 2.0623631477355957 	 1.2114925384521484 	 0.0007016658782958984 	 
2025-07-24 16:35:42.585320 test begin: paddle.Tensor.prod(Tensor([4, 525, 12096, 3],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([4, 525, 12096, 3],"float32"), -1, ) 	 76204800 	 1000 	 0.41375136375427246 	 0.5120575428009033 	 0.39801573753356934 	 0.4977865219116211 	 2.5058646202087402 	 2.7231054306030273 	 2.4505650997161865 	 0.0010106563568115234 	 
2025-07-24 16:35:50.294984 test begin: paddle.Tensor.prod(Tensor([4, 525, 12097, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([4, 525, 12097, 2],"float32"), -1, ) 	 50807400 	 1000 	 0.38817286491394043 	 0.7009077072143555 	 0.3723006248474121 	 0.45223140716552734 	 1.676762342453003 	 2.0588996410369873 	 1.6216418743133545 	 0.0007114410400390625 	 
2025-07-24 16:35:57.464853 test begin: paddle.Tensor.prod(Tensor([4, 526, 12096, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([4, 526, 12096, 2],"float32"), -1, ) 	 50899968 	 1000 	 0.38884782791137695 	 0.46861720085144043 	 0.37312936782836914 	 0.45478081703186035 	 1.679802417755127 	 2.0607428550720215 	 1.6246943473815918 	 0.0007159709930419922 	 
2025-07-24 16:36:03.293727 test begin: paddle.Tensor.prod(Tensor([5, 525, 12096, 2],"float32"), -1, )
[Prof] paddle.Tensor.prod 	 paddle.Tensor.prod(Tensor([5, 525, 12096, 2],"float32"), -1, ) 	 63504000 	 1000 	 0.4842109680175781 	 0.5829222202301025 	 0.46849894523620605 	 0.5692243576049805 	 2.0932083129882812 	 2.5520811080932617 	 2.0377984046936035 	 0.00089263916015625 	 
2025-07-24 16:36:10.592659 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 58801],"float64"), q=0.75, axis=3, keepdim=True, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3b3dd798a0>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753346770 (unix time) try "date -d @1753346770" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1deb2) received by PID 122546 (TID 0x7f3b34dfa640) from PID 122546 ***]

2025-07-24 16:46:17.845565 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 58801],"float64"), q=0.75, axis=5, )
W0724 16:46:18.507511 137534 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 16:46:18.528978 137534 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 58801],"float64"), q=0.75, axis=5, ) 	 25402032 	 1000 	 11.178117990493774 	 10.991392135620117 	 0.3012411594390869 	 0.3974933624267578 	 5.0608296394348145 	 2.102238178253174 	 0.36945343017578125 	 0.19533562660217285 	 
2025-07-24 16:46:52.146549 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 23521, 5],"float64"), q=0.75, axis=3, keepdim=True, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fadbaa52620>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753347412 (unix time) try "date -d @1753347412" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x218d3) received by PID 137427 (TID 0x7fadb623f640) from PID 137427 ***]

2025-07-24 16:56:59.103306 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 23521, 5],"float64"), q=0.75, axis=5, )
W0724 16:56:59.796759 141718 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 16:56:59.825848 141718 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5ab59031f0>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753348019 (unix time) try "date -d @1753348019" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22929) received by PID 141609 (TID 0x7f5aacdfa640) from PID 141609 ***]

2025-07-24 17:07:05.343928 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 47041, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, )
W0724 17:07:06.046842 146303 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 17:07:06.080446 146303 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.quantile 	 paddle.Tensor.quantile(Tensor([3, 6, 3, 47041, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, ) 	 25402140 	 1000 	 19.61253046989441 	 11.606592893600464 	 0.4897429943084717 	 0.3829202651977539 	 5.974117040634155 	 2.4315319061279297 	 0.36188173294067383 	 0.22594904899597168 	 
2025-07-24 17:07:46.820000 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 47041, 2, 5],"float64"), q=0.75, axis=5, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbb9281abc0>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753348666 (unix time) try "date -d @1753348666" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23b12) received by PID 146194 (TID 0x7fbb8e00c640) from PID 146194 ***]

2025-07-24 17:17:53.603930 test begin: paddle.Tensor.quantile(Tensor([3, 6, 35281, 4, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, )
W0724 17:17:54.399967 151345 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 17:17:54.445525 151345 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f95431aafb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 17:27:58.309689 test begin: paddle.Tensor.quantile(Tensor([3, 6, 35281, 4, 2, 5],"float64"), q=0.75, axis=5, )
W0724 17:27:58.991830 155344 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 17:27:59.029479 155344 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fec0d64b430>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 17:38:02.983673 test begin: paddle.Tensor.quantile(Tensor([3, 70561, 3, 4, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, )
W0724 17:38:03.651484 161346 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 17:38:03.685279 161346 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1c2f012ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 17:48:07.605067 test begin: paddle.Tensor.quantile(Tensor([3, 70561, 3, 4, 2, 5],"float64"), q=0.75, axis=5, )
W0724 17:48:08.303047 12138 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 17:48:08.325896 12138 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb0e16231c0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 17:58:12.324193 test begin: paddle.Tensor.quantile(Tensor([35281, 6, 3, 4, 2, 5],"float64"), q=0.75, axis=3, keepdim=True, )
W0724 17:58:13.023938 26901 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 17:58:13.047132 26901 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2e4f6c6c20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 18:08:17.128045 test begin: paddle.Tensor.quantile(Tensor([35281, 6, 3, 4, 2, 5],"float64"), q=0.75, axis=5, )
W0724 18:08:17.798569 40351 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 18:08:17.837196 40351 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2b7d5271f0>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753352297 (unix time) try "date -d @1753352297" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x9cfb) received by PID 40187 (TID 0x7f2b743f9640) from PID 40187 ***]

2025-07-24 18:18:23.145751 test begin: paddle.Tensor.rad2deg(x=Tensor([1587601, 4, 4],"float64"), )
W0724 18:18:23.830067 54062 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([1587601, 4, 4],"float64"), ) 	 25401616 	 1000 	 0.2980935573577881 	 0.2998831272125244 	 0.2744934558868408 	 0.2763044834136963 	 0.297788143157959 	 0.2984960079193115 	 0.23430275917053223 	 0.21430540084838867 	 
2025-07-24 18:18:26.007104 test begin: paddle.Tensor.rad2deg(x=Tensor([396901, 4, 4, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 1000 	 0.29854512214660645 	 0.29854750633239746 	 0.2744107246398926 	 0.276930570602417 	 0.29805707931518555 	 0.2985513210296631 	 0.23782682418823242 	 0.226487398147583 	 
2025-07-24 18:18:28.260274 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 1587601, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 1587601, 4],"float64"), ) 	 25401616 	 1000 	 0.2984018325805664 	 0.29845142364501953 	 0.27503418922424316 	 0.27706217765808105 	 0.29760003089904785 	 0.29847240447998047 	 0.23714017868041992 	 0.22951960563659668 	 
2025-07-24 18:18:30.509641 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 1000 	 0.29796552658081055 	 0.29847025871276855 	 0.2743525505065918 	 0.27661705017089844 	 0.29762768745422363 	 0.298567533493042 	 0.23722577095031738 	 0.21824216842651367 	 
2025-07-24 18:18:32.751041 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 4, 1587601],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 4, 1587601],"float64"), ) 	 25401616 	 1000 	 0.2983717918395996 	 0.3150320053100586 	 0.2750251293182373 	 0.2764012813568115 	 0.2976553440093994 	 0.2983896732330322 	 0.2259235382080078 	 0.22541427612304688 	 
2025-07-24 18:18:35.016193 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 1000 	 0.7462761402130127 	 0.31543517112731934 	 0.27199840545654297 	 0.28333353996276855 	 0.29767632484436035 	 0.29973435401916504 	 0.24623537063598633 	 0.23265314102172852 	 
2025-07-24 18:18:39.821320 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 1000 	 0.29892969131469727 	 0.29847049713134766 	 0.2744908332824707 	 0.27684521675109863 	 0.2975904941558838 	 0.29851436614990234 	 0.23592019081115723 	 0.22564148902893066 	 
2025-07-24 18:18:42.145098 test begin: paddle.Tensor.rad2deg(x=Tensor([4, 6350401],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([4, 6350401],"float64"), ) 	 25401604 	 1000 	 0.2983839511871338 	 0.29854583740234375 	 0.2750403881072998 	 0.27683377265930176 	 0.2975630760192871 	 0.299652099609375 	 0.23728322982788086 	 0.22805070877075195 	 
2025-07-24 18:18:44.453256 test begin: paddle.Tensor.rad2deg(x=Tensor([6350401, 4],"float64"), )
[Prof] paddle.Tensor.rad2deg 	 paddle.Tensor.rad2deg(x=Tensor([6350401, 4],"float64"), ) 	 25401604 	 1000 	 0.3002967834472656 	 0.30058741569519043 	 0.28327083587646484 	 0.28411412239074707 	 0.2975587844848633 	 0.2984962463378906 	 0.24632549285888672 	 0.2343306541442871 	 
2025-07-24 18:18:46.709783 test begin: paddle.Tensor.rank(Tensor([256, 1536, 3, 44],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([256, 1536, 3, 44],"float32"), ) 	 51904512 	 1000 	 0.04024648666381836 	 0.029559612274169922 	 2.288818359375e-05 	 5.698204040527344e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 18:18:47.680768 test begin: paddle.Tensor.rank(Tensor([256, 1536, 44, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([256, 1536, 44, 3],"float32"), ) 	 51904512 	 1000 	 0.0534515380859375 	 0.0381317138671875 	 4.458427429199219e-05 	 0.0001049041748046875 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 18:18:48.785114 test begin: paddle.Tensor.rank(Tensor([256, 2048, 3, 33],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([256, 2048, 3, 33],"float32"), ) 	 51904512 	 1000 	 0.03943347930908203 	 0.02930450439453125 	 1.4066696166992188e-05 	 4.5299530029296875e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 18:18:49.720238 test begin: paddle.Tensor.rank(Tensor([256, 2048, 33, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([256, 2048, 33, 3],"float32"), ) 	 51904512 	 1000 	 0.053259849548339844 	 0.02954268455505371 	 4.076957702636719e-05 	 7.700920104980469e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 18:18:50.619344 test begin: paddle.Tensor.rank(Tensor([256, 22051, 3, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([256, 22051, 3, 3],"float32"), ) 	 50805504 	 1000 	 0.057393550872802734 	 0.03718280792236328 	 2.3603439331054688e-05 	 5.4836273193359375e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 18:18:51.565044 test begin: paddle.Tensor.rank(Tensor([256, 768, 3, 87],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([256, 768, 3, 87],"float32"), ) 	 51314688 	 1000 	 0.039515018463134766 	 0.02913069725036621 	 2.09808349609375e-05 	 4.172325134277344e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 18:18:52.467504 test begin: paddle.Tensor.rank(Tensor([256, 768, 87, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([256, 768, 87, 3],"float32"), ) 	 51314688 	 1000 	 0.03987264633178711 	 0.029237747192382812 	 1.621246337890625e-05 	 3.814697265625e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 18:18:53.374803 test begin: paddle.Tensor.rank(Tensor([2757, 2048, 3, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([2757, 2048, 3, 3],"float32"), ) 	 50817024 	 1000 	 0.04036831855773926 	 0.02931952476501465 	 1.9550323486328125e-05 	 5.650520324707031e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 18:18:54.274170 test begin: paddle.Tensor.rank(Tensor([3676, 1536, 3, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([3676, 1536, 3, 3],"float32"), ) 	 50817024 	 1000 	 0.0394442081451416 	 0.029311418533325195 	 2.4557113647460938e-05 	 4.482269287109375e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 18:18:55.167086 test begin: paddle.Tensor.rank(Tensor([7351, 768, 3, 3],"float32"), )
[Prof] paddle.Tensor.rank 	 paddle.Tensor.rank(Tensor([7351, 768, 3, 3],"float32"), ) 	 50810112 	 1000 	 0.03947806358337402 	 0.02935934066772461 	 1.2636184692382812e-05 	 4.220008850097656e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 18:18:56.079928 test begin: paddle.Tensor.reciprocal(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 1000 	 0.29676318168640137 	 0.2982609272003174 	 0.28664565086364746 	 0.28794193267822266 	 0.45048999786376953 	 1.0407400131225586 	 0.3970355987548828 	 0.3546111583709717 	 
2025-07-24 18:18:59.916529 test begin: paddle.Tensor.reciprocal(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 1000 	 0.2956840991973877 	 0.29819703102111816 	 0.286912202835083 	 0.2879202365875244 	 0.45025157928466797 	 1.04072904586792 	 0.3976466655731201 	 0.3546011447906494 	 
2025-07-24 18:19:03.690209 test begin: paddle.Tensor.reciprocal(Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([10, 5080321],"float32"), ) 	 50803210 	 1000 	 0.2956972122192383 	 0.30062007904052734 	 0.2869844436645508 	 0.2879159450531006 	 0.450275182723999 	 1.0421295166015625 	 0.3977015018463135 	 0.3545839786529541 	 
2025-07-24 18:19:07.446606 test begin: paddle.Tensor.reciprocal(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 1000 	 0.2957441806793213 	 0.29819703102111816 	 0.28688836097717285 	 0.2879786491394043 	 0.4503312110900879 	 1.0419940948486328 	 0.3963780403137207 	 0.3558797836303711 	 
2025-07-24 18:19:11.254392 test begin: paddle.Tensor.reciprocal(Tensor([2540161, 20],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([2540161, 20],"float32"), ) 	 50803220 	 1000 	 0.29572033882141113 	 0.2981905937194824 	 0.28696393966674805 	 0.2879354953765869 	 0.4503138065338135 	 1.0407233238220215 	 0.3975198268890381 	 0.35460805892944336 	 
2025-07-24 18:19:15.034595 test begin: paddle.Tensor.reciprocal(Tensor([4233601, 12],"float32"), )
[Prof] paddle.Tensor.reciprocal 	 paddle.Tensor.reciprocal(Tensor([4233601, 12],"float32"), ) 	 50803212 	 1000 	 0.29576730728149414 	 0.2982187271118164 	 0.27971386909484863 	 0.2815365791320801 	 0.45035219192504883 	 1.0407848358154297 	 0.3884012699127197 	 0.3546280860900879 	 
2025-07-24 18:19:20.511724 test begin: paddle.Tensor.remainder(Tensor([2, 3, 8467201],"float32"), Tensor([2, 3, 8467201],"float32"), )
[Prof] paddle.Tensor.remainder 	 paddle.Tensor.remainder(Tensor([2, 3, 8467201],"float32"), Tensor([2, 3, 8467201],"float32"), ) 	 101606412 	 1000 	 0.4537208080291748 	 0.4495890140533447 	 0.4355652332305908 	 0.43137693405151367 	 None 	 None 	 None 	 None 	 
2025-07-24 18:19:24.077618 test begin: paddle.Tensor.remainder(Tensor([2, 6350401, 4],"float32"), Tensor([2, 6350401, 4],"float32"), )
[Prof] paddle.Tensor.remainder 	 paddle.Tensor.remainder(Tensor([2, 6350401, 4],"float32"), Tensor([2, 6350401, 4],"float32"), ) 	 101606416 	 1000 	 0.45115137100219727 	 0.4528238773345947 	 0.4383528232574463 	 0.4375431537628174 	 None 	 None 	 None 	 None 	 
2025-07-24 18:19:26.650526 test begin: paddle.Tensor.remainder(Tensor([4233601, 3, 4],"float32"), Tensor([4233601, 3, 4],"float32"), )
[Prof] paddle.Tensor.remainder 	 paddle.Tensor.remainder(Tensor([4233601, 3, 4],"float32"), Tensor([4233601, 3, 4],"float32"), ) 	 101606424 	 1000 	 0.4510536193847656 	 0.44940638542175293 	 0.4410877227783203 	 0.4376997947692871 	 None 	 None 	 None 	 None 	 
2025-07-24 18:19:29.242449 test begin: paddle.Tensor.repeat_interleave(Tensor([1, 1, 198451, 128],"float64"), 3, axis=1, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(Tensor([1, 1, 198451, 128],"float64"), 3, axis=1, ) 	 25401728 	 1000 	 0.8985707759857178 	 0.8891623020172119 	 0.45887255668640137 	 0.8663158416748047 	 1.487703561782837 	 0.5881426334381104 	 0.5066320896148682 	 0.5034637451171875 	 
2025-07-24 18:19:37.147863 test begin: paddle.Tensor.repeat_interleave(Tensor([1, 1, 64, 396901],"float64"), 3, axis=1, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(Tensor([1, 1, 64, 396901],"float64"), 3, axis=1, ) 	 25401664 	 1000 	 1.3554613590240479 	 0.8830726146697998 	 0.45937180519104004 	 0.8499736785888672 	 1.4849779605865479 	 0.5880005359649658 	 0.5062229633331299 	 0.49712228775024414 	 
2025-07-24 18:19:44.390471 test begin: paddle.Tensor.repeat_interleave(Tensor([1, 3101, 64, 128],"float64"), 3, axis=1, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(Tensor([1, 3101, 64, 128],"float64"), 3, axis=1, ) 	 25403392 	 1000 	 0.6981534957885742 	 0.6398932933807373 	 0.0739595890045166 	 0.6150410175323486 	 0.9199750423431396 	 0.5947756767272949 	 0.09461712837219238 	 0.5115234851837158 	 
2025-07-24 18:19:49.358111 test begin: paddle.Tensor.repeat_interleave(Tensor([3101, 1, 64, 128],"float64"), 3, axis=1, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(Tensor([3101, 1, 64, 128],"float64"), 3, axis=1, ) 	 25403392 	 1000 	 0.6183695793151855 	 0.6390411853790283 	 0.31581830978393555 	 0.6071557998657227 	 0.9146990776062012 	 0.5947496891021729 	 0.31168150901794434 	 0.5026540756225586 	 
2025-07-24 18:19:54.256163 test begin: paddle.Tensor.repeat_interleave(x=Tensor([158761, 2, 4, 4, 5],"float64"), repeats=2, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(x=Tensor([158761, 2, 4, 4, 5],"float64"), repeats=2, ) 	 25401760 	 1000 	 219.7887260913849 	 0.46932125091552734 	 8.678436279296875e-05 	 0.444089412689209 	 249.79107403755188 	 0.5451958179473877 	 7.796287536621094e-05 	 0.45317769050598145 	 
2025-07-24 18:27:47.443333 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 158761, 4, 5],"float64"), repeats=2, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 158761, 4, 5],"float64"), repeats=2, ) 	 25401760 	 1000 	 221.8136920928955 	 0.46999502182006836 	 8.916854858398438e-05 	 0.44316554069519043 	 243.59443926811218 	 0.5452382564544678 	 8.7738037109375e-05 	 0.43532323837280273 	 
2025-07-24 18:35:37.098698 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 158761, 5],"float64"), repeats=2, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 158761, 5],"float64"), repeats=2, ) 	 25401760 	 1000 	 199.37042331695557 	 0.4664599895477295 	 0.00021076202392578125 	 0.43369507789611816 	 227.44608187675476 	 0.5453736782073975 	 9.584426879882812e-05 	 0.45239686965942383 	 
2025-07-24 18:42:48.930368 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 198451],"float64"), repeats=2, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 198451],"float64"), repeats=2, ) 	 25401728 	 1000 	 215.67662024497986 	 0.46998143196105957 	 0.00010323524475097656 	 0.4450104236602783 	 246.1489815711975 	 0.5452115535736084 	 0.00010895729064941406 	 0.4557797908782959 	 
2025-07-24 18:50:33.873736 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 79381, 4, 4, 5],"float64"), repeats=2, )
[Prof] paddle.Tensor.repeat_interleave 	 paddle.Tensor.repeat_interleave(x=Tensor([4, 79381, 4, 4, 5],"float64"), repeats=2, ) 	 25401920 	 1000 	 212.9809226989746 	 0.46639466285705566 	 9.298324584960938e-05 	 0.44376564025878906 	 243.99653959274292 	 0.5451862812042236 	 7.653236389160156e-05 	 0.4598872661590576 	 
2025-07-24 18:58:14.544339 test begin: paddle.Tensor.reshape(Tensor([12404, 8192],"bfloat16"), list[-1,8192,], )
[Prof] paddle.Tensor.reshape 	 paddle.Tensor.reshape(Tensor([12404, 8192],"bfloat16"), list[-1,8192,], ) 	 101613568 	 1000 	 0.0055620670318603516 	 0.003947257995605469 	 1.2636184692382812e-05 	 1.9550323486328125e-05 	 0.04552292823791504 	 0.45371580123901367 	 2.6226043701171875e-05 	 0.3777797222137451 	 
2025-07-24 18:58:18.316242 test begin: paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 1000 	 0.519334077835083 	 0.3037388324737549 	 0.48668599128723145 	 0.28803229331970215 	 0.8223371505737305 	 0.30377960205078125 	 0.41943359375 	 0.23689913749694824 	 
2025-07-24 18:58:21.388350 test begin: paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 1000 	 0.8165864944458008 	 0.30350422859191895 	 0.41687536239624023 	 0.2866096496582031 	 0.5113213062286377 	 0.3033726215362549 	 0.4541137218475342 	 0.22884845733642578 	 
2025-07-24 18:58:24.442903 test begin: paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 1000 	 0.8156223297119141 	 0.3034842014312744 	 0.414792537689209 	 0.28653979301452637 	 0.5113592147827148 	 0.3029494285583496 	 0.45453715324401855 	 0.23750543594360352 	 
2025-07-24 18:58:29.174902 test begin: paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 1000 	 1.2210042476654053 	 0.3044016361236572 	 0.4876577854156494 	 0.28893566131591797 	 0.8251583576202393 	 0.3026618957519531 	 0.420154333114624 	 0.23698759078979492 	 
2025-07-24 18:58:33.478243 test begin: paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 1000 	 0.9580092430114746 	 0.32434988021850586 	 0.4833805561065674 	 0.2872276306152344 	 0.5135166645050049 	 0.3043968677520752 	 0.45676684379577637 	 0.23793268203735352 	 
2025-07-24 18:58:39.359063 test begin: paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 1000 	 0.8210210800170898 	 0.3034627437591553 	 0.41605496406555176 	 0.28662800788879395 	 0.511277437210083 	 0.30303430557250977 	 0.45433640480041504 	 0.23214936256408691 	 
2025-07-24 18:58:42.386224 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 1000 	 0.5136392116546631 	 0.3045194149017334 	 0.4887235164642334 	 0.28888821601867676 	 0.8230447769165039 	 0.30519914627075195 	 0.4211602210998535 	 0.22422528266906738 	 
2025-07-24 18:58:45.474999 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 1000 	 0.8279201984405518 	 0.3042933940887451 	 0.42263007164001465 	 0.287384033203125 	 0.5113034248352051 	 0.3033280372619629 	 0.45432233810424805 	 0.23531198501586914 	 
2025-07-24 18:58:48.636096 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 1000 	 0.8154778480529785 	 0.30450010299682617 	 0.4159567356109619 	 0.2794651985168457 	 0.5131027698516846 	 0.30580711364746094 	 0.44305419921875 	 0.23254132270812988 	 
2025-07-24 18:58:51.645639 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 1000 	 0.5139057636260986 	 0.31238269805908203 	 0.4750983715057373 	 0.28269410133361816 	 0.8215701580047607 	 0.3038191795349121 	 0.41968536376953125 	 0.2086176872253418 	 
2025-07-24 18:58:54.700191 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 1000 	 0.823049783706665 	 0.3057236671447754 	 0.42052245140075684 	 0.2886359691619873 	 0.5135719776153564 	 0.3062279224395752 	 0.4566476345062256 	 0.2335679531097412 	 
2025-07-24 18:58:57.691939 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.Tensor.rot90 	 paddle.Tensor.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 1000 	 0.8086071014404297 	 0.3087301254272461 	 0.41385340690612793 	 0.28923487663269043 	 0.5113511085510254 	 0.30298590660095215 	 0.4491612911224365 	 0.22717618942260742 	 
2025-07-24 18:59:00.698984 test begin: paddle.Tensor.round(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.Tensor.round 	 paddle.Tensor.round(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 1000 	 0.29717493057250977 	 0.29793524742126465 	 0.2884397506713867 	 0.2866227626800537 	 0.13406085968017578 	 0.13545966148376465 	 0.08411884307861328 	 0.07408857345581055 	 
2025-07-24 18:59:03.230465 test begin: paddle.Tensor.round(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.Tensor.round 	 paddle.Tensor.round(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 1000 	 0.2959117889404297 	 0.2978553771972656 	 0.28705644607543945 	 0.2865424156188965 	 0.13410043716430664 	 0.13428759574890137 	 0.08453536033630371 	 0.07266569137573242 	 
2025-07-24 18:59:05.928400 test begin: paddle.Tensor.round(Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.round 	 paddle.Tensor.round(Tensor([10, 5080321],"float32"), ) 	 50803210 	 1000 	 0.29587507247924805 	 0.29970693588256836 	 0.28719520568847656 	 0.28647732734680176 	 0.1341085433959961 	 0.13418984413146973 	 0.08449244499206543 	 0.06350827217102051 	 
2025-07-24 18:59:08.406342 test begin: paddle.Tensor.round(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.Tensor.round 	 paddle.Tensor.round(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 1000 	 0.29585814476013184 	 0.297914981842041 	 0.28714871406555176 	 0.28644537925720215 	 0.1340932846069336 	 0.1341845989227295 	 0.0836327075958252 	 0.06282854080200195 	 
2025-07-24 18:59:10.911886 test begin: paddle.Tensor.round(Tensor([2540161, 20],"float32"), )
[Prof] paddle.Tensor.round 	 paddle.Tensor.round(Tensor([2540161, 20],"float32"), ) 	 50803220 	 1000 	 0.29590821266174316 	 0.2998976707458496 	 0.2872171401977539 	 0.28440237045288086 	 0.13412785530090332 	 0.13419365882873535 	 0.08450913429260254 	 0.07366776466369629 	 
2025-07-24 18:59:13.427673 test begin: paddle.Tensor.rsqrt(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.Tensor.rsqrt 	 paddle.Tensor.rsqrt(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 1000 	 0.2968571186065674 	 0.29799628257751465 	 0.2871668338775635 	 0.2874338626861572 	 0.4502880573272705 	 1.0418167114257812 	 0.39748644828796387 	 0.35446810722351074 	 
2025-07-24 18:59:17.281038 test begin: paddle.Tensor.rsqrt(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.Tensor.rsqrt 	 paddle.Tensor.rsqrt(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 1000 	 0.29596567153930664 	 0.2979152202606201 	 0.2855973243713379 	 0.28751134872436523 	 0.45162248611450195 	 1.0417733192443848 	 0.3988196849822998 	 0.3557708263397217 	 
2025-07-24 18:59:21.080739 test begin: paddle.Tensor.rsqrt(Tensor([10, 5080321],"float32"), )
[Prof] paddle.Tensor.rsqrt 	 paddle.Tensor.rsqrt(Tensor([10, 5080321],"float32"), ) 	 50803210 	 1000 	 0.2963752746582031 	 0.31392478942871094 	 0.28676509857177734 	 0.28870344161987305 	 0.45030879974365234 	 1.040475606918335 	 0.3977169990539551 	 0.35445570945739746 	 
2025-07-24 18:59:24.848695 test begin: paddle.Tensor.rsqrt(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.Tensor.rsqrt 	 paddle.Tensor.rsqrt(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 1000 	 0.2959613800048828 	 0.2980921268463135 	 0.2871870994567871 	 0.28466296195983887 	 0.45024895668029785 	 1.0417125225067139 	 0.39754247665405273 	 0.35445427894592285 	 
2025-07-24 18:59:28.620602 test begin: paddle.Tensor.rsqrt(Tensor([2540161, 20],"float32"), )
[Prof] paddle.Tensor.rsqrt 	 paddle.Tensor.rsqrt(Tensor([2540161, 20],"float32"), ) 	 50803220 	 1000 	 0.29714536666870117 	 0.29788899421691895 	 0.2814960479736328 	 0.28129053115844727 	 0.4503037929534912 	 1.040447473526001 	 0.38853907585144043 	 0.3544631004333496 	 
2025-07-24 18:59:33.980216 test begin: paddle.Tensor.scale(Tensor([100352, 1013],"bfloat16"), 0.006378560586546936, )
[Prof] paddle.Tensor.scale 	 paddle.Tensor.scale(Tensor([100352, 1013],"bfloat16"), 0.006378560586546936, ) 	 101656576 	 1000 	 0.3042309284210205 	 0.599254846572876 	 0.2815866470336914 	 0.3027932643890381 	 0.5901679992675781 	 0.7512409687042236 	 0.5175364017486572 	 0.3831486701965332 	 combined
2025-07-24 18:59:41.741553 test begin: paddle.Tensor.scale(Tensor([1013, 100352],"bfloat16"), 0.006378560586546936, )
[Prof] paddle.Tensor.scale 	 paddle.Tensor.scale(Tensor([1013, 100352],"bfloat16"), 0.006378560586546936, ) 	 101656576 	 1000 	 0.299210786819458 	 0.592473030090332 	 0.28948426246643066 	 0.30275487899780273 	 0.589796781539917 	 0.7498941421508789 	 0.5339093208312988 	 0.3831162452697754 	 combined
2025-07-24 18:59:47.340541 test begin: paddle.Tensor.scale(Tensor([12404, 8192],"bfloat16"), 0.006378560586546936, )
[Prof] paddle.Tensor.scale 	 paddle.Tensor.scale(Tensor([12404, 8192],"bfloat16"), 0.006378560586546936, ) 	 101613568 	 1000 	 0.29897427558898926 	 0.5922563076019287 	 0.2891993522644043 	 0.30259084701538086 	 0.5883200168609619 	 0.7496209144592285 	 0.5330827236175537 	 0.3830256462097168 	 combined
2025-07-24 18:59:52.884244 test begin: paddle.Tensor.scale(Tensor([1772, 57344],"bfloat16"), 0.006378560586546936, )
[Prof] paddle.Tensor.scale 	 paddle.Tensor.scale(Tensor([1772, 57344],"bfloat16"), 0.006378560586546936, ) 	 101613568 	 1000 	 0.2990849018096924 	 0.5937113761901855 	 0.28920960426330566 	 0.30401134490966797 	 0.5883722305297852 	 0.7495980262756348 	 0.5030159950256348 	 0.38303446769714355 	 combined
2025-07-24 18:59:58.604650 test begin: paddle.Tensor.scale(Tensor([8192, 12404],"bfloat16"), 0.006378560586546936, )
[Prof] paddle.Tensor.scale 	 paddle.Tensor.scale(Tensor([8192, 12404],"bfloat16"), 0.006378560586546936, ) 	 101613568 	 1000 	 0.29906463623046875 	 0.592257022857666 	 0.28931760787963867 	 0.3026554584503174 	 0.5897152423858643 	 0.7510824203491211 	 0.533954381942749 	 0.3844618797302246 	 combined
2025-07-24 19:00:04.134463 test begin: paddle.Tensor.set_(Tensor([20],"bool"), Tensor([15, 3386881],"bool"), list[20,], list[2,], 0, )
[Prof] paddle.Tensor.set_ 	 paddle.Tensor.set_(Tensor([20],"bool"), Tensor([15, 3386881],"bool"), list[20,], list[2,], 0, ) 	 50803235 	 1000 	 0.10045027732849121 	 0.002246379852294922 	 4.887580871582031e-05 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 19:00:04.981530 test begin: paddle.Tensor.set_(Tensor([20],"bool"), Tensor([16934401, 3],"bool"), list[20,], list[2,], 0, )
[Prof] paddle.Tensor.set_ 	 paddle.Tensor.set_(Tensor([20],"bool"), Tensor([16934401, 3],"bool"), list[20,], list[2,], 0, ) 	 50803223 	 1000 	 0.03565502166748047 	 0.0022411346435546875 	 2.3365020751953125e-05 	 1.5497207641601562e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 19:00:05.747409 test begin: paddle.Tensor.set_(Tensor([50803201],"bool"), Tensor([15, 3],"bool"), list[20,], list[2,], 0, )
[Prof] paddle.Tensor.set_ 	 paddle.Tensor.set_(Tensor([50803201],"bool"), Tensor([15, 3],"bool"), list[20,], list[2,], 0, ) 	 50803246 	 1000 	 0.035767555236816406 	 0.0022411346435546875 	 1.6927719116210938e-05 	 1.5974044799804688e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 19:00:06.510131 test begin: paddle.Tensor.sigmoid(Tensor([1, 1100, 46185],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1, 1100, 46185],"float32"), ) 	 50803500 	 1000 	 0.29634928703308105 	 0.29840993881225586 	 0.28623127937316895 	 0.2877650260925293 	 0.4515812397003174 	 0.4466869831085205 	 0.39214062690734863 	 0.3849775791168213 	 
2025-07-24 19:00:09.668582 test begin: paddle.Tensor.sigmoid(Tensor([1, 12700801, 4],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1, 12700801, 4],"float32"), ) 	 50803204 	 1000 	 0.29514503479003906 	 0.29839468002319336 	 0.27941322326660156 	 0.2816023826599121 	 0.4503617286682129 	 0.4466254711151123 	 0.3878018856048584 	 0.38017964363098145 	 
2025-07-24 19:00:12.809799 test begin: paddle.Tensor.sigmoid(Tensor([1, 6380, 7963],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1, 6380, 7963],"float32"), ) 	 50803940 	 1000 	 0.2952709197998047 	 0.29839372634887695 	 0.28635621070861816 	 0.2879061698913574 	 0.4503509998321533 	 0.4480445384979248 	 0.3972032070159912 	 0.38406920433044434 	 
2025-07-24 19:00:16.080900 test begin: paddle.Tensor.sigmoid(Tensor([1, 8550, 5942],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1, 8550, 5942],"float32"), ) 	 50804100 	 1000 	 0.2953166961669922 	 0.2983431816101074 	 0.28345251083374023 	 0.2877812385559082 	 0.45026469230651855 	 0.4466731548309326 	 0.3969120979309082 	 0.3829987049102783 	 
2025-07-24 19:00:19.237679 test begin: paddle.Tensor.sigmoid(Tensor([11547, 1100, 4],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([11547, 1100, 4],"float32"), ) 	 50806800 	 1000 	 0.2988126277923584 	 0.29842662811279297 	 0.2877311706542969 	 0.2879307270050049 	 0.45027637481689453 	 0.4480609893798828 	 0.39573097229003906 	 0.3856852054595947 	 
2025-07-24 19:00:22.425823 test begin: paddle.Tensor.sigmoid(Tensor([1486, 8550, 4],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1486, 8550, 4],"float32"), ) 	 50821200 	 1000 	 0.29506921768188477 	 0.2984917163848877 	 0.2861001491546631 	 0.288013219833374 	 0.45188045501708984 	 0.44680190086364746 	 0.39896678924560547 	 0.38434886932373047 	 
2025-07-24 19:00:25.564492 test begin: paddle.Tensor.sigmoid(Tensor([1991, 6380, 4],"float32"), )
[Prof] paddle.Tensor.sigmoid 	 paddle.Tensor.sigmoid(Tensor([1991, 6380, 4],"float32"), ) 	 50810320 	 1000 	 0.29669857025146484 	 0.2984585762023926 	 0.28076887130737305 	 0.2816965579986572 	 0.4503302574157715 	 0.44670581817626953 	 0.3872854709625244 	 0.3773324489593506 	 
2025-07-24 19:00:28.826278 test begin: paddle.Tensor.sign(Tensor([1016065, 5, 5],"float64"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([1016065, 5, 5],"float64"), ) 	 25401625 	 1000 	 0.3088531494140625 	 0.2983980178833008 	 0.3004720211029053 	 0.2880220413208008 	 0.29755568504333496 	 0.13471674919128418 	 0.24538516998291016 	 0.0736837387084961 	 
2025-07-24 19:00:30.934596 test begin: paddle.Tensor.sign(Tensor([1124, 45199],"float32"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([1124, 45199],"float32"), ) 	 50803676 	 1000 	 0.3436579704284668 	 0.29793787002563477 	 0.3284485340118408 	 0.2801833152770996 	 0.29616761207580566 	 0.1354672908782959 	 0.2367992401123047 	 0.06790614128112793 	 
2025-07-24 19:00:33.704972 test begin: paddle.Tensor.sign(Tensor([12700801, 2],"float64"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([12700801, 2],"float64"), ) 	 25401602 	 1000 	 0.3093140125274658 	 1.234508752822876 	 0.3005259037017822 	 0.28780674934387207 	 0.2977173328399658 	 0.13515043258666992 	 0.24564623832702637 	 0.07129955291748047 	 
2025-07-24 19:00:39.064488 test begin: paddle.Tensor.sign(Tensor([1587601, 32],"float32"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([1587601, 32],"float32"), ) 	 50803232 	 1000 	 0.8114004135131836 	 0.3196375370025635 	 0.3349118232727051 	 0.2872586250305176 	 0.2961745262145996 	 0.13424205780029297 	 0.24532032012939453 	 0.07277774810791016 	 
2025-07-24 19:00:42.672445 test begin: paddle.Tensor.sign(Tensor([50000, 102, 5],"float64"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([50000, 102, 5],"float64"), ) 	 25500000 	 1000 	 0.3195686340332031 	 0.29951024055480957 	 0.3019692897796631 	 0.2887742519378662 	 0.299039363861084 	 0.13529229164123535 	 0.24848103523254395 	 0.07057785987854004 	 
2025-07-24 19:00:46.278549 test begin: paddle.Tensor.sign(Tensor([50000, 5, 102],"float64"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([50000, 5, 102],"float64"), ) 	 25500000 	 1000 	 0.31260037422180176 	 0.30045342445373535 	 0.2965104579925537 	 0.28874921798706055 	 0.2990243434906006 	 0.13530206680297852 	 0.23987865447998047 	 0.06154656410217285 	 
2025-07-24 19:00:48.394216 test begin: paddle.Tensor.sign(Tensor([50000, 509],"float64"), )
[Prof] paddle.Tensor.sign 	 paddle.Tensor.sign(Tensor([50000, 509],"float64"), ) 	 25450000 	 1000 	 0.3102893829345703 	 0.301302433013916 	 0.30094242095947266 	 0.2879598140716553 	 0.2984468936920166 	 0.13491463661193848 	 0.24768900871276855 	 0.06693553924560547 	 
2025-07-24 19:00:50.518182 test begin: paddle.Tensor.signbit(Tensor([12, 1058401, 2],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc3139aea40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 19:10:55.558852 test begin: paddle.Tensor.signbit(Tensor([12, 20, 105841],"float64"), )
W0724 19:10:56.261281 148285 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd9e9fcf010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 19:21:00.761143 test begin: paddle.Tensor.signbit(Tensor([12, 20, 211681],"float32"), )
W0724 19:21:01.791594  6801 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7eff2f8ab010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 19:31:07.081535 test begin: paddle.Tensor.signbit(Tensor([12, 20, 423361],"int16"), )
W0724 19:31:09.669005 30347 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0e5a95ae30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 19:41:14.024597 test begin: paddle.Tensor.signbit(Tensor([12, 2116801, 2],"float32"), )
W0724 19:41:15.060132 52766 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f77898ff070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 19:51:21.073150 test begin: paddle.Tensor.signbit(Tensor([12, 4233601, 2],"int16"), )
W0724 19:51:22.243474 76048 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f15a6d73010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 20:01:29.424263 test begin: paddle.Tensor.signbit(Tensor([1270081, 20, 2],"float32"), )
W0724 20:01:30.378224 100650 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fab110f3010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 20:11:37.133620 test begin: paddle.Tensor.signbit(Tensor([2540161, 20, 2],"int16"), )
W0724 20:11:39.825292 123494 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6d71447070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 20:21:43.053032 test begin: paddle.Tensor.signbit(Tensor([635041, 20, 2],"float64"), )
W0724 20:21:43.752195 146351 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f818fa0b010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 20:31:49.972305 test begin: paddle.Tensor.sin(Tensor([131072, 388],"float32"), )
W0724 20:31:51.310783  7123 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.sin 	 paddle.Tensor.sin(Tensor([131072, 388],"float32"), ) 	 50855936 	 1000 	 0.3086421489715576 	 0.3018524646759033 	 0.28711843490600586 	 0.28775954246520996 	 0.4508330821990967 	 0.7442684173583984 	 0.3958725929260254 	 0.3802027702331543 	 
2025-07-24 20:31:54.427394 test begin: paddle.Tensor.sin(Tensor([3175201, 16],"float32"), )
[Prof] paddle.Tensor.sin 	 paddle.Tensor.sin(Tensor([3175201, 16],"float32"), ) 	 50803216 	 1000 	 0.29872608184814453 	 0.29820871353149414 	 0.2881643772125244 	 0.28795957565307617 	 0.4505147933959961 	 0.7434508800506592 	 0.39516425132751465 	 0.37981200218200684 	 
2025-07-24 20:31:57.916442 test begin: paddle.Tensor.sin(Tensor([32768, 1551],"float32"), )
[Prof] paddle.Tensor.sin 	 paddle.Tensor.sin(Tensor([32768, 1551],"float32"), ) 	 50823168 	 1000 	 0.2956838607788086 	 0.2982935905456543 	 0.2868921756744385 	 0.2881917953491211 	 0.45080041885375977 	 0.7436702251434326 	 0.39556431770324707 	 0.3799862861633301 	 
2025-07-24 20:32:01.402152 test begin: paddle.Tensor.sin(Tensor([396901, 128],"float32"), )
[Prof] paddle.Tensor.sin 	 paddle.Tensor.sin(Tensor([396901, 128],"float32"), ) 	 50803328 	 1000 	 0.2956087589263916 	 0.29810285568237305 	 0.2868680953979492 	 0.2878091335296631 	 0.45046496391296387 	 0.7459549903869629 	 0.39519667625427246 	 0.38110780715942383 	 
2025-07-24 20:32:04.852624 test begin: paddle.Tensor.slice(Tensor([12700801, 4],"float32"), list[1,], list[0,], list[1,], )
[Prof] paddle.Tensor.slice 	 paddle.Tensor.slice(Tensor([12700801, 4],"float32"), list[1,], list[0,], list[1,], ) 	 50803204 	 1000 	 0.007772207260131836 	 0.013240814208984375 	 3.075599670410156e-05 	 2.47955322265625e-05 	 0.4962940216064453 	 0.4793722629547119 	 0.25343942642211914 	 0.24491024017333984 	 combined
2025-07-24 20:32:06.940616 test begin: paddle.Tensor.slice(Tensor([4, 12700801],"float32"), list[1,], list[0,], list[1,], )
[Prof] paddle.Tensor.slice 	 paddle.Tensor.slice(Tensor([4, 12700801],"float32"), list[1,], list[0,], list[1,], ) 	 50803204 	 1000 	 0.007372856140136719 	 0.013211488723754883 	 1.0251998901367188e-05 	 2.2172927856445312e-05 	 0.14911389350891113 	 0.13786721229553223 	 0.07619190216064453 	 0.06823611259460449 	 combined
2025-07-24 20:32:08.075051 test begin: paddle.Tensor.slice_scatter(Tensor([4233601, 6],"float64"), Tensor([4233601, 3],"float64"), list[1,], list[0,], list[6,], list[2,], )
[Prof] paddle.Tensor.slice_scatter 	 paddle.Tensor.slice_scatter(Tensor([4233601, 6],"float64"), Tensor([4233601, 3],"float64"), list[1,], list[0,], list[6,], list[2,], ) 	 38102409 	 1000 	 0.3763408660888672 	 0.6881189346313477 	 0.3619353771209717 	 0.23433828353881836 	 1.0689246654510498 	 0.7655251026153564 	 0.1820065975189209 	 0.19559049606323242 	 
2025-07-24 20:32:12.296189 test begin: paddle.Tensor.slice_scatter(Tensor([8, 3175201],"float64"), Tensor([8, 3],"float64"), list[1,], list[0,], list[6,], list[2,], )
[Prof] paddle.Tensor.slice_scatter 	 paddle.Tensor.slice_scatter(Tensor([8, 3175201],"float64"), Tensor([8, 3],"float64"), list[1,], list[0,], list[6,], list[2,], ) 	 25401632 	 1000 	 0.014191389083862305 	 0.3161036968231201 	 1.0013580322265625e-05 	 0.10744190216064453 	 0.3236367702484131 	 0.31816840171813965 	 0.054979801177978516 	 0.08111238479614258 	 
2025-07-24 20:32:14.362103 test begin: paddle.Tensor.slice_scatter(Tensor([8467201, 6],"float64"), Tensor([8467201, 3],"float64"), list[1,], list[0,], list[6,], list[2,], )
[Prof] paddle.Tensor.slice_scatter 	 paddle.Tensor.slice_scatter(Tensor([8467201, 6],"float64"), Tensor([8467201, 3],"float64"), list[1,], list[0,], list[6,], list[2,], ) 	 76204809 	 1000 	 0.7479748725891113 	 1.3629708290100098 	 0.7337179183959961 	 0.4640829563140869 	 2.100562810897827 	 1.5079805850982666 	 0.35708093643188477 	 0.38578104972839355 	 
2025-07-24 20:32:22.831585 test begin: paddle.Tensor.sqrt(Tensor([276, 80, 48, 48],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([276, 80, 48, 48],"float32"), ) 	 50872320 	 1000 	 0.2962307929992676 	 0.2994539737701416 	 0.2867302894592285 	 0.28870582580566406 	 0.4515116214752197 	 0.7483360767364502 	 0.39640212059020996 	 0.3823399543762207 	 
2025-07-24 20:32:26.359676 test begin: paddle.Tensor.sqrt(Tensor([329, 80, 44, 44],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([329, 80, 44, 44],"float32"), ) 	 50955520 	 1000 	 0.29624438285827637 	 0.299818754196167 	 0.28720664978027344 	 0.289395809173584 	 0.452101469039917 	 0.7493748664855957 	 0.3982515335083008 	 0.38285279273986816 	 
2025-07-24 20:32:29.823343 test begin: paddle.Tensor.sqrt(Tensor([397, 80, 40, 40],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([397, 80, 40, 40],"float32"), ) 	 50816000 	 1000 	 0.2949373722076416 	 0.2990438938140869 	 0.28629446029663086 	 0.2889432907104492 	 0.4510800838470459 	 0.7472686767578125 	 0.39714479446411133 	 0.38181328773498535 	 
2025-07-24 20:32:33.324321 test begin: paddle.Tensor.sqrt(Tensor([64, 345, 48, 48],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 345, 48, 48],"float32"), ) 	 50872320 	 1000 	 0.2965404987335205 	 0.5306396484375 	 0.28644824028015137 	 0.288362979888916 	 0.45132899284362793 	 0.7483506202697754 	 0.39700865745544434 	 0.3823080062866211 	 
2025-07-24 20:32:39.314835 test begin: paddle.Tensor.sqrt(Tensor([64, 411, 44, 44],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 411, 44, 44],"float32"), ) 	 50924544 	 1000 	 0.30065417289733887 	 0.2996675968170166 	 0.2879953384399414 	 0.2893180847167969 	 0.453049898147583 	 0.7489206790924072 	 0.3982374668121338 	 0.3826267719268799 	 
2025-07-24 20:32:42.778138 test begin: paddle.Tensor.sqrt(Tensor([64, 497, 40, 40],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 497, 40, 40],"float32"), ) 	 50892800 	 1000 	 0.29755663871765137 	 0.299497127532959 	 0.2861511707305908 	 0.288494348526001 	 0.4528379440307617 	 0.7485301494598389 	 0.39813923835754395 	 0.3824188709259033 	 
2025-07-24 20:32:46.296483 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 207, 48],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 207, 48],"float32"), ) 	 50872320 	 1000 	 0.29702305793762207 	 0.29938507080078125 	 0.28650808334350586 	 0.28907322883605957 	 0.45137476921081543 	 0.7509596347808838 	 0.39722418785095215 	 0.38362836837768555 	 
2025-07-24 20:32:49.771347 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 226, 44],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 226, 44],"float32"), ) 	 50913280 	 1000 	 0.7459619045257568 	 0.31249165534973145 	 0.28656697273254395 	 0.28914666175842285 	 0.4518401622772217 	 0.7487561702728271 	 0.3978002071380615 	 0.38253188133239746 	 
2025-07-24 20:32:55.361605 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 249, 40],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 249, 40],"float32"), ) 	 50995200 	 1000 	 0.2958555221557617 	 0.3000984191894531 	 0.28708481788635254 	 0.2895991802215576 	 0.45240068435668945 	 0.7499978542327881 	 0.3985762596130371 	 0.3831949234008789 	 
2025-07-24 20:32:58.869860 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 40, 249],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 40, 249],"float32"), ) 	 50995200 	 1000 	 0.29584383964538574 	 0.30004215240478516 	 0.2871887683868408 	 0.2896707057952881 	 0.45243120193481445 	 0.7499885559082031 	 0.39647912979125977 	 0.3832056522369385 	 
2025-07-24 20:33:02.386855 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 44, 226],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 44, 226],"float32"), ) 	 50913280 	 1000 	 0.2953455448150635 	 0.2996490001678467 	 0.28669190406799316 	 0.28920435905456543 	 0.4532783031463623 	 0.7487754821777344 	 0.37897443771362305 	 0.3825547695159912 	 
2025-07-24 20:33:06.207562 test begin: paddle.Tensor.sqrt(Tensor([64, 80, 48, 207],"float32"), )
[Prof] paddle.Tensor.sqrt 	 paddle.Tensor.sqrt(Tensor([64, 80, 48, 207],"float32"), ) 	 50872320 	 1000 	 0.29509711265563965 	 0.30067920684814453 	 0.2864649295806885 	 0.29035472869873047 	 0.45138120651245117 	 0.7495019435882568 	 0.397860050201416 	 0.38356590270996094 	 
2025-07-24 20:33:09.662245 test begin: paddle.Tensor.square(Tensor([2, 25401601],"float32"), )
[Prof] paddle.Tensor.square 	 paddle.Tensor.square(Tensor([2, 25401601],"float32"), ) 	 50803202 	 1000 	 0.2960975170135498 	 0.2980167865753174 	 0.2871837615966797 	 0.2868528366088867 	 0.4502754211425781 	 1.0567364692687988 	 0.3968315124511719 	 0.27120518684387207 	 
2025-07-24 20:33:13.473535 test begin: paddle.Tensor.square(Tensor([396901, 128],"float32"), )
[Prof] paddle.Tensor.square 	 paddle.Tensor.square(Tensor([396901, 128],"float32"), ) 	 50803328 	 1000 	 0.2981300354003906 	 0.297868013381958 	 0.2873201370239258 	 0.2869269847869873 	 0.45018863677978516 	 1.0553464889526367 	 0.3966503143310547 	 0.26984453201293945 	 
2025-07-24 20:33:17.281294 test begin: paddle.Tensor.square(Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.square 	 paddle.Tensor.square(Tensor([50803201],"float32"), ) 	 50803201 	 1000 	 0.29599499702453613 	 0.29789304733276367 	 0.2874565124511719 	 0.2870786190032959 	 0.4502575397491455 	 1.0553154945373535 	 0.3932912349700928 	 0.26982665061950684 	 
2025-07-24 20:33:21.002877 test begin: paddle.Tensor.square(Tensor([8, 6350401],"float32"), )
[Prof] paddle.Tensor.square 	 paddle.Tensor.square(Tensor([8, 6350401],"float32"), ) 	 50803208 	 1000 	 0.2982945442199707 	 0.2978792190551758 	 0.2871367931365967 	 0.2870972156524658 	 0.45174646377563477 	 1.055370569229126 	 0.3981904983520508 	 0.26985883712768555 	 
2025-07-24 20:33:24.769488 test begin: paddle.Tensor.squeeze(Tensor([1, 2, 3840, 10240],"float32"), 0, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([1, 2, 3840, 10240],"float32"), 0, ) 	 78643200 	 1000 	 0.0047609806060791016 	 0.0041484832763671875 	 3.170967102050781e-05 	 1.8596649169921875e-05 	 0.04375600814819336 	 0.052309274673461914 	 6.008148193359375e-05 	 5.936622619628906e-05 	 
2025-07-24 20:33:27.436595 test begin: paddle.Tensor.squeeze(Tensor([1, 3, 1654, 10240],"float32"), 0, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([1, 3, 1654, 10240],"float32"), 0, ) 	 50810880 	 1000 	 0.004531383514404297 	 0.004144906997680664 	 9.775161743164062e-06 	 1.7642974853515625e-05 	 0.0428469181060791 	 0.05291271209716797 	 2.8848648071289062e-05 	 7.081031799316406e-05 	 
2025-07-24 20:33:29.198082 test begin: paddle.Tensor.squeeze(Tensor([1, 3, 3840, 10240],"float32"), 0, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([1, 3, 3840, 10240],"float32"), 0, ) 	 117964800 	 1000 	 0.004433155059814453 	 0.0041773319244384766 	 1.4066696166992188e-05 	 1.9311904907226562e-05 	 0.042916059494018555 	 0.05315041542053223 	 5.221366882324219e-05 	 5.054473876953125e-05 	 
2025-07-24 20:33:33.060717 test begin: paddle.Tensor.squeeze(Tensor([1, 3, 3840, 4411],"float32"), 0, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([1, 3, 3840, 4411],"float32"), 0, ) 	 50814720 	 1000 	 0.004372596740722656 	 0.004767179489135742 	 7.62939453125e-06 	 5.340576171875e-05 	 0.04353904724121094 	 0.05340075492858887 	 3.3855438232421875e-05 	 8.654594421386719e-05 	 
2025-07-24 20:33:34.843309 test begin: paddle.Tensor.squeeze(Tensor([16, 1, 125, 25500],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([16, 1, 125, 25500],"float32"), 1, ) 	 51000000 	 1000 	 0.004469871520996094 	 0.012372970581054688 	 6.4373016357421875e-06 	 0.00011467933654785156 	 0.0431370735168457 	 0.07447481155395508 	 5.1021575927734375e-05 	 6.747245788574219e-05 	 
2025-07-24 20:33:39.398676 test begin: paddle.Tensor.squeeze(Tensor([16, 1, 80, 39691],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([16, 1, 80, 39691],"float32"), 1, ) 	 50804480 	 1000 	 0.0046138763427734375 	 0.00412297248840332 	 2.5510787963867188e-05 	 1.6450881958007812e-05 	 0.0430448055267334 	 0.05286359786987305 	 2.86102294921875e-05 	 7.152557373046875e-05 	 
2025-07-24 20:33:41.201156 test begin: paddle.Tensor.squeeze(Tensor([16, 2, 80, 25500],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([16, 2, 80, 25500],"float32"), 1, ) 	 65280000 	 1000 	 0.004573345184326172 	 0.003935575485229492 	 1.5020370483398438e-05 	 1.8358230590820312e-05 	 0.045229196548461914 	 0.05053257942199707 	 2.9802322387695312e-05 	 7.152557373046875e-05 	 
2025-07-24 20:33:43.425791 test begin: paddle.Tensor.squeeze(Tensor([200, 1, 127009, 2],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([200, 1, 127009, 2],"float32"), 1, ) 	 50803600 	 1000 	 0.0047910213470458984 	 0.004085540771484375 	 2.2649765014648438e-05 	 1.71661376953125e-05 	 0.043087005615234375 	 0.05429887771606445 	 3.0994415283203125e-05 	 6.270408630371094e-05 	 
2025-07-24 20:33:45.163797 test begin: paddle.Tensor.squeeze(Tensor([200, 1, 37632, 7],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([200, 1, 37632, 7],"float32"), 1, ) 	 52684800 	 1000 	 0.004418373107910156 	 0.004716634750366211 	 1.049041748046875e-05 	 5.817413330078125e-05 	 0.042906761169433594 	 0.05320334434509277 	 3.647804260253906e-05 	 5.793571472167969e-05 	 
2025-07-24 20:33:47.009546 test begin: paddle.Tensor.squeeze(Tensor([200, 4, 37632, 2],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([200, 4, 37632, 2],"float32"), 1, ) 	 60211200 	 1000 	 0.004429817199707031 	 0.0039064884185791016 	 6.9141387939453125e-06 	 1.7642974853515625e-05 	 0.043183088302612305 	 0.04969668388366699 	 4.1961669921875e-05 	 0.00016069412231445312 	 
2025-07-24 20:33:49.264995 test begin: paddle.Tensor.squeeze(Tensor([25, 1, 80, 25500],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([25, 1, 80, 25500],"float32"), 1, ) 	 51000000 	 1000 	 0.004438877105712891 	 0.004900693893432617 	 1.0013580322265625e-05 	 5.841255187988281e-05 	 0.04313254356384277 	 0.06366753578186035 	 3.0279159545898438e-05 	 5.7697296142578125e-05 	 
2025-07-24 20:33:51.038818 test begin: paddle.Tensor.squeeze(Tensor([676, 1, 37632, 2],"float32"), 1, )
[Prof] paddle.Tensor.squeeze 	 paddle.Tensor.squeeze(Tensor([676, 1, 37632, 2],"float32"), 1, ) 	 50878464 	 1000 	 0.004426717758178711 	 0.004067182540893555 	 6.67572021484375e-06 	 1.7881393432617188e-05 	 0.0652616024017334 	 0.05238938331604004 	 3.552436828613281e-05 	 5.555152893066406e-05 	 
2025-07-24 20:33:52.789111 test begin: paddle.Tensor.std(Tensor([1024, 1024, 25],"float64"), )
W0724 20:33:53.271919 12085 dygraph_functions.cc:88394] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([1024, 1024, 25],"float64"), ) 	 26214400 	 1000 	 1.3415491580963135 	 0.18439149856567383 	 2.384185791015625e-05 	 0.09412002563476562 	 1.5294427871704102 	 0.7914628982543945 	 0.19563698768615723 	 0.09018492698669434 	 
2025-07-24 20:34:00.031385 test begin: paddle.Tensor.std(Tensor([1024, 1024, 49],"float32"), )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([1024, 1024, 49],"float32"), ) 	 51380224 	 1000 	 1.1216702461242676 	 0.16826987266540527 	 4.0531158447265625e-05 	 0.08596086502075195 	 1.3532156944274902 	 0.784611701965332 	 0.17314672470092773 	 0.08939194679260254 	 
2025-07-24 20:34:04.337864 test begin: paddle.Tensor.std(Tensor([1024, 3101, 8],"float64"), )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([1024, 3101, 8],"float64"), ) 	 25403392 	 1000 	 1.3122107982635498 	 0.17888355255126953 	 2.0265579223632812e-05 	 0.09134197235107422 	 1.4834907054901123 	 0.7693049907684326 	 0.18983674049377441 	 0.08763313293457031 	 
2025-07-24 20:34:08.612250 test begin: paddle.Tensor.std(Tensor([1024, 6202, 8],"float32"), )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([1024, 6202, 8],"float32"), ) 	 50806784 	 1000 	 1.0986671447753906 	 0.16665434837341309 	 3.409385681152344e-05 	 0.08512377738952637 	 1.3388736248016357 	 0.7778439521789551 	 0.17134928703308105 	 0.08860325813293457 	 
2025-07-24 20:34:12.848564 test begin: paddle.Tensor.std(Tensor([1444, 35183],"float32"), axis=1, )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([1444, 35183],"float32"), axis=1, ) 	 50804252 	 1000 	 1.1336076259613037 	 0.1768028736114502 	 2.5987625122070312e-05 	 0.16044402122497559 	 1.3609397411346436 	 0.7819077968597412 	 0.17418670654296875 	 0.10004925727844238 	 
2025-07-24 20:34:17.120038 test begin: paddle.Tensor.std(Tensor([3101, 1024, 8],"float64"), )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([3101, 1024, 8],"float64"), ) 	 25403392 	 1000 	 1.3082592487335205 	 0.17909884452819824 	 3.0517578125e-05 	 0.09147453308105469 	 1.4828624725341797 	 0.7694442272186279 	 0.18974590301513672 	 0.08766627311706543 	 
2025-07-24 20:34:21.455921 test begin: paddle.Tensor.std(Tensor([49613, 1024],"float32"), axis=1, )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([49613, 1024],"float32"), axis=1, ) 	 50803712 	 1000 	 1.0968072414398193 	 0.16779065132141113 	 1.8596649169921875e-05 	 0.15161728858947754 	 1.3485875129699707 	 0.7812657356262207 	 0.19684338569641113 	 0.09997057914733887 	 
2025-07-24 20:34:25.673022 test begin: paddle.Tensor.std(Tensor([6202, 1024, 8],"float32"), )
[Prof] paddle.Tensor.std 	 paddle.Tensor.std(Tensor([6202, 1024, 8],"float32"), ) 	 50806784 	 1000 	 1.1062989234924316 	 0.16669869422912598 	 5.340576171875e-05 	 0.08516454696655273 	 1.3401029109954834 	 0.7777116298675537 	 0.17131280899047852 	 0.08860659599304199 	 
2025-07-24 20:34:29.938486 test begin: paddle.Tensor.subtract(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.Tensor.subtract 	 paddle.Tensor.subtract(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 1000 	 0.45174360275268555 	 0.44677257537841797 	 0.4422440528869629 	 0.4354848861694336 	 0.4736440181732178 	 0.29773664474487305 	 0.4161520004272461 	 0.23433709144592285 	 
2025-07-24 20:34:34.085346 test begin: paddle.Tensor.sum(Tensor([106496, 478],"float32"), axis=-1, )
[Prof] paddle.Tensor.sum 	 paddle.Tensor.sum(Tensor([106496, 478],"float32"), axis=-1, ) 	 50905088 	 1000 	 0.1499183177947998 	 0.15534663200378418 	 0.13800835609436035 	 0.14003324508666992 	 0.13842272758483887 	 0.051207542419433594 	 0.08276748657226562 	 3.719329833984375e-05 	 
2025-07-24 20:34:37.021458 test begin: paddle.Tensor.sum(Tensor([108544, 469],"float32"), axis=-1, )
[Prof] paddle.Tensor.sum 	 paddle.Tensor.sum(Tensor([108544, 469],"float32"), axis=-1, ) 	 50907136 	 1000 	 0.1500072479248047 	 0.1550736427307129 	 0.1380631923675537 	 0.14024949073791504 	 0.13822484016418457 	 0.05171942710876465 	 0.08254790306091309 	 3.8623809814453125e-05 	 
2025-07-24 20:34:39.216197 test begin: paddle.Tensor.sum(Tensor([111616, 456],"float32"), axis=-1, )
[Prof] paddle.Tensor.sum 	 paddle.Tensor.sum(Tensor([111616, 456],"float32"), axis=-1, ) 	 50896896 	 1000 	 0.15030121803283691 	 0.15273785591125488 	 0.13834190368652344 	 0.13778901100158691 	 0.13864922523498535 	 0.051598548889160156 	 0.08286380767822266 	 6.127357482910156e-05 	 
2025-07-24 20:34:40.530419 test begin: paddle.Tensor.sum(Tensor([14176, 3584],"float32"), axis=-1, )
[Prof] paddle.Tensor.sum 	 paddle.Tensor.sum(Tensor([14176, 3584],"float32"), axis=-1, ) 	 50806784 	 1000 	 0.1461772918701172 	 0.1558516025543213 	 0.13433361053466797 	 0.14075589179992676 	 0.137526273727417 	 0.05142927169799805 	 0.08175110816955566 	 4.887580871582031e-05 	 
2025-07-24 20:34:41.851621 test begin: paddle.Tensor.take_along_axis(Tensor([128, 1000],"float32"), indices=Tensor([128, 396901],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([128, 1000],"float32"), indices=Tensor([128, 396901],"int32"), axis=-1, ) 	 50931328 	 1000 	 0.7741401195526123 	 0.47666406631469727 	 0.2636079788208008 	 0.454700231552124 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:34:54.429594 test begin: paddle.Tensor.take_along_axis(Tensor([128, 396901],"float32"), indices=Tensor([128, 1],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([128, 396901],"float32"), indices=Tensor([128, 1],"int32"), axis=-1, ) 	 50803456 	 1000 	 0.30400586128234863 	 0.01740407943725586 	 0.10340380668640137 	 5.364418029785156e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:34:56.066914 test begin: paddle.Tensor.take_along_axis(Tensor([128, 396901],"float32"), indices=Tensor([128, 396901],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([128, 396901],"float32"), indices=Tensor([128, 396901],"int32"), axis=-1, ) 	 101606656 	 1000 	 1.8644938468933105 	 0.7369565963745117 	 0.4773848056793213 	 0.7193801403045654 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:35:06.990109 test begin: paddle.Tensor.take_along_axis(Tensor([50804, 1000],"float32"), indices=Tensor([50804, 1],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([50804, 1000],"float32"), indices=Tensor([50804, 1],"int32"), axis=-1, ) 	 50854804 	 1000 	 0.3093910217285156 	 0.017092466354370117 	 0.10517692565917969 	 4.553794860839844e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:35:08.649704 test begin: paddle.Tensor.take_along_axis(Tensor([80, 1000],"float32"), indices=Tensor([80, 635041],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([80, 1000],"float32"), indices=Tensor([80, 635041],"int32"), axis=-1, ) 	 50883280 	 1000 	 0.7629485130310059 	 0.47927045822143555 	 0.25980448722839355 	 0.45244383811950684 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:35:21.645651 test begin: paddle.Tensor.take_along_axis(Tensor([80, 635041],"float32"), indices=Tensor([80, 1],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([80, 635041],"float32"), indices=Tensor([80, 1],"int32"), axis=-1, ) 	 50803360 	 1000 	 0.30394983291625977 	 0.017652034759521484 	 0.10340547561645508 	 4.744529724121094e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:35:23.253297 test begin: paddle.Tensor.take_along_axis(Tensor([80, 635041],"float32"), indices=Tensor([80, 635041],"int32"), axis=-1, )
[Prof] paddle.Tensor.take_along_axis 	 paddle.Tensor.take_along_axis(Tensor([80, 635041],"float32"), indices=Tensor([80, 635041],"int32"), axis=-1, ) 	 101606560 	 1000 	 1.4016621112823486 	 0.747582197189331 	 0.47710323333740234 	 0.7281107902526855 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:35:32.805782 test begin: paddle.Tensor.tanh(Tensor([1, 16934401, 3],"float32"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([1, 16934401, 3],"float32"), ) 	 50803203 	 1000 	 0.29590892791748047 	 1.4918830394744873 	 0.28721189498901367 	 0.2875711917877197 	 0.45023465156555176 	 0.44672107696533203 	 0.3929760456085205 	 0.3851203918457031 	 
2025-07-24 20:35:39.450254 test begin: paddle.Tensor.tanh(Tensor([1, 2, 12700801],"float64"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([1, 2, 12700801],"float64"), ) 	 25401602 	 1000 	 0.3155403137207031 	 0.30349159240722656 	 0.2908506393432617 	 0.2891407012939453 	 0.44942188262939453 	 0.44458436965942383 	 0.39571046829223633 	 0.3829624652862549 	 
2025-07-24 20:35:42.023886 test begin: paddle.Tensor.tanh(Tensor([1, 2, 25401601],"float32"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([1, 2, 25401601],"float32"), ) 	 50803202 	 1000 	 0.29694151878356934 	 0.2980942726135254 	 0.28691959381103516 	 0.28784942626953125 	 0.45031046867370605 	 0.44794535636901855 	 0.38904356956481934 	 0.3854842185974121 	 
2025-07-24 20:35:45.212741 test begin: paddle.Tensor.tanh(Tensor([1, 8467201, 3],"float64"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([1, 8467201, 3],"float64"), ) 	 25401603 	 1000 	 0.29935526847839355 	 0.3003387451171875 	 0.29088830947875977 	 0.29009079933166504 	 0.44834136962890625 	 0.4446580410003662 	 0.3946857452392578 	 0.3827095031738281 	 
2025-07-24 20:35:47.762049 test begin: paddle.Tensor.tanh(Tensor([2, 12700801],"float64"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([2, 12700801],"float64"), ) 	 25401602 	 1000 	 0.2995631694793701 	 0.300368070602417 	 0.29054737091064453 	 0.2902255058288574 	 0.4481959342956543 	 0.44587111473083496 	 0.3950493335723877 	 0.38352012634277344 	 
2025-07-24 20:35:50.339284 test begin: paddle.Tensor.tanh(Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([4233601, 2, 3],"float64"), ) 	 25401606 	 1000 	 0.2995579242706299 	 0.30035924911499023 	 0.2907881736755371 	 0.2902717590332031 	 0.4480419158935547 	 0.4445784091949463 	 0.39504384994506836 	 0.3826107978820801 	 
2025-07-24 20:35:52.873771 test begin: paddle.Tensor.tanh(Tensor([6350401, 4],"float64"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([6350401, 4],"float64"), ) 	 25401604 	 1000 	 0.2993130683898926 	 0.3059074878692627 	 0.29081010818481445 	 0.2913339138031006 	 0.44922471046447754 	 0.4445371627807617 	 0.3891942501068115 	 0.38274121284484863 	 
2025-07-24 20:35:55.494411 test begin: paddle.Tensor.tanh(Tensor([8467201, 2, 3],"float32"), )
[Prof] paddle.Tensor.tanh 	 paddle.Tensor.tanh(Tensor([8467201, 2, 3],"float32"), ) 	 50803206 	 1000 	 0.2961559295654297 	 0.29810166358947754 	 0.2872309684753418 	 0.2879030704498291 	 0.4501767158508301 	 0.4466414451599121 	 0.3963050842285156 	 0.3843872547149658 	 
2025-07-24 20:35:58.708335 test begin: paddle.Tensor.tile(Tensor([198451, 1, 256],"float32"), tuple(1,1,1,), )
[Prof] paddle.Tensor.tile 	 paddle.Tensor.tile(Tensor([198451, 1, 256],"float32"), tuple(1,1,1,), ) 	 50803456 	 1000 	 0.29851555824279785 	 0.31483936309814453 	 0.2851696014404297 	 0.15992307662963867 	 0.31290221214294434 	 0.048505544662475586 	 0.15981125831604004 	 4.506111145019531e-05 	 
2025-07-24 20:36:01.326368 test begin: paddle.Tensor.tile(Tensor([36858, 1, 1379],"float32"), tuple(1,1,1,), )
[Prof] paddle.Tensor.tile 	 paddle.Tensor.tile(Tensor([36858, 1, 1379],"float32"), tuple(1,1,1,), ) 	 50827182 	 1000 	 0.2970266342163086 	 0.32910799980163574 	 0.28552865982055664 	 0.16000795364379883 	 0.3153536319732666 	 0.04762434959411621 	 0.16038775444030762 	 4.482269287109375e-05 	 
2025-07-24 20:36:04.012888 test begin: paddle.Tensor.tile(Tensor([36858, 6, 256],"float32"), tuple(1,1,1,), )
[Prof] paddle.Tensor.tile 	 paddle.Tensor.tile(Tensor([36858, 6, 256],"float32"), tuple(1,1,1,), ) 	 56613888 	 1000 	 0.33129167556762695 	 0.35965633392333984 	 0.31871557235717773 	 0.3191843032836914 	 0.34495067596435547 	 0.0533137321472168 	 0.2879753112792969 	 6.365776062011719e-05 	 
2025-07-24 20:36:07.754816 test begin: paddle.Tensor.tile(Tensor([38402, 1, 1323],"float32"), tuple(1,1,1,), )
[Prof] paddle.Tensor.tile 	 paddle.Tensor.tile(Tensor([38402, 1, 1323],"float32"), tuple(1,1,1,), ) 	 50805846 	 1000 	 1.2209525108337402 	 0.3130073547363281 	 0.2850675582885742 	 0.15983295440673828 	 0.31275320053100586 	 0.048963308334350586 	 0.15975499153137207 	 6.628036499023438e-05 	 
2025-07-24 20:36:11.305573 test begin: paddle.Tensor.tile(Tensor([38402, 6, 256],"float32"), tuple(1,1,1,), )
[Prof] paddle.Tensor.tile 	 paddle.Tensor.tile(Tensor([38402, 6, 256],"float32"), tuple(1,1,1,), ) 	 58985472 	 1000 	 0.3434619903564453 	 0.3626890182495117 	 0.332319974899292 	 0.33895158767700195 	 0.35904622077941895 	 0.04931235313415527 	 0.3024940490722656 	 6.771087646484375e-05 	 
2025-07-24 20:36:14.351224 test begin: paddle.Tensor.tolist(Tensor([11, 1679, 32, 43],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7d4200ee90>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 20:46:19.208516 test begin: paddle.Tensor.tolist(Tensor([11, 25, 2149, 43],"int64"), )
W0724 20:46:19.827881 47127 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5f3d64b010>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753361780 (unix time) try "date -d @1753361780" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xb666) received by PID 46694 (TID 0x7f5f343f9640) from PID 46694 ***]

2025-07-24 20:56:28.053751 test begin: paddle.Tensor.tolist(Tensor([11, 25, 32, 2887],"int64"), )
W0724 20:56:28.616467 75861 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5163eff010>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753362388 (unix time) try "date -d @1753362388" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x126f9) received by PID 75513 (TID 0x7f515f499640) from PID 75513 ***]

2025-07-24 21:06:37.081697 test begin: paddle.Tensor.tolist(Tensor([739, 25, 32, 43],"int64"), )
W0724 21:06:39.973105 101128 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4ff3967010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 21:16:43.849510 test begin: paddle.Tensor.topk(Tensor([1, 50803201],"float32"), 5, 1, True, True, )
W0724 21:16:49.013759 126717 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.Tensor.topk 	 paddle.Tensor.topk(Tensor([1, 50803201],"float32"), 5, 1, True, True, ) 	 50803201 	 1000 	 177.53284454345703 	 5.988820552825928 	 177.52274441719055 	 0.3409767150878906 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:20:18.859079 test begin: paddle.Tensor.topk(Tensor([1024, 1034, 48],"float32"), 2, axis=-1, )
[Prof] paddle.Tensor.topk 	 paddle.Tensor.topk(Tensor([1024, 1034, 48],"float32"), 2, axis=-1, ) 	 50823168 	 1000 	 2.6659955978393555 	 11.165116548538208 	 2.6543374061584473 	 5.704503774642944 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:20:40.344357 test begin: paddle.Tensor.topk(Tensor([1024, 8, 6202],"float32"), 2, axis=-1, )
[Prof] paddle.Tensor.topk 	 paddle.Tensor.topk(Tensor([1024, 8, 6202],"float32"), 2, axis=-1, ) 	 50806784 	 1000 	 0.38220715522766113 	 1.5405173301696777 	 0.37204933166503906 	 0.08758807182312012 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:20:47.605642 test begin: paddle.Tensor.topk(Tensor([128, 396901],"float32"), 5, 1, True, True, )
[Prof] paddle.Tensor.topk 	 paddle.Tensor.topk(Tensor([128, 396901],"float32"), 5, 1, True, True, ) 	 50803328 	 1000 	 1.4776968955993652 	 1.4882936477661133 	 1.4675712585449219 	 0.08446526527404785 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:20:55.950053 test begin: paddle.Tensor.topk(Tensor([132301, 8, 48],"float32"), 2, axis=-1, )
[Prof] paddle.Tensor.topk 	 paddle.Tensor.topk(Tensor([132301, 8, 48],"float32"), 2, axis=-1, ) 	 50803584 	 1000 	 2.6669654846191406 	 11.163669109344482 	 2.6566569805145264 	 5.7069971561431885 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:21:15.403666 test begin: paddle.Tensor.topk(Tensor([50804, 1000],"float32"), 5, 1, True, True, )
[Prof] paddle.Tensor.topk 	 paddle.Tensor.topk(Tensor([50804, 1000],"float32"), 5, 1, True, True, ) 	 50804000 	 1000 	 0.6512656211853027 	 2.4146645069122314 	 0.6412289142608643 	 0.13730072975158691 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:21:23.755077 test begin: paddle.Tensor.transpose(Tensor([106496, 955],"bfloat16"), list[1,0,], )
[Prof] paddle.Tensor.transpose 	 paddle.Tensor.transpose(Tensor([106496, 955],"bfloat16"), list[1,0,], ) 	 101703680 	 1000 	 0.003395557403564453 	 0.004548788070678711 	 8.344650268554688e-06 	 2.3126602172851562e-05 	 0.04339933395385742 	 0.45413970947265625 	 3.528594970703125e-05 	 0.3703792095184326 	 
2025-07-24 21:21:27.704943 test begin: paddle.Tensor.transpose(Tensor([108544, 937],"bfloat16"), list[1,0,], )
[Prof] paddle.Tensor.transpose 	 paddle.Tensor.transpose(Tensor([108544, 937],"bfloat16"), list[1,0,], ) 	 101705728 	 1000 	 0.0033698081970214844 	 0.0052378177642822266 	 1.430511474609375e-05 	 5.6743621826171875e-05 	 0.06821799278259277 	 0.45408201217651367 	 5.7697296142578125e-05 	 0.37549567222595215 	 
2025-07-24 21:21:31.543561 test begin: paddle.Tensor.transpose(Tensor([111616, 911],"bfloat16"), list[1,0,], )
[Prof] paddle.Tensor.transpose 	 paddle.Tensor.transpose(Tensor([111616, 911],"bfloat16"), list[1,0,], ) 	 101682176 	 1000 	 0.0033524036407470703 	 0.0046155452728271484 	 1.71661376953125e-05 	 2.0265579223632812e-05 	 0.0441131591796875 	 0.45413708686828613 	 4.482269287109375e-05 	 0.37543678283691406 	 
2025-07-24 21:21:37.324147 test begin: paddle.Tensor.transpose(Tensor([14176, 7168],"bfloat16"), list[1,0,], )
[Prof] paddle.Tensor.transpose 	 paddle.Tensor.transpose(Tensor([14176, 7168],"bfloat16"), list[1,0,], ) 	 101613568 	 1000 	 0.00742340087890625 	 0.008652687072753906 	 1.1444091796875e-05 	 2.2172927856445312e-05 	 0.05189967155456543 	 0.453779935836792 	 4.8160552978515625e-05 	 0.3700602054595947 	 
2025-07-24 21:21:41.711910 test begin: paddle.Tensor.tril(Tensor([1, 2, 25401601],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([1, 2, 25401601],"float32"), -1, ) 	 50803202 	 1000 	 0.3027830123901367 	 0.27484989166259766 	 0.2856876850128174 	 0.2552661895751953 	 0.2998697757720947 	 0.2679450511932373 	 0.2425546646118164 	 0.20619988441467285 	 
2025-07-24 21:21:45.922389 test begin: paddle.Tensor.tril(Tensor([1, 25401601, 2],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([1, 25401601, 2],"float32"), -1, ) 	 50803202 	 1000 	 0.8692748546600342 	 0.3213925361633301 	 0.41220855712890625 	 0.30983829498291016 	 0.41633033752441406 	 0.32166552543640137 	 0.3674931526184082 	 0.26012301445007324 	 
2025-07-24 21:21:50.038024 test begin: paddle.Tensor.tril(Tensor([12700801, 2, 2],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([12700801, 2, 2],"float32"), -1, ) 	 50803204 	 1000 	 0.4210941791534424 	 0.3239405155181885 	 0.4128742218017578 	 0.3125014305114746 	 0.4171102046966553 	 0.32558512687683105 	 0.3684673309326172 	 0.2635951042175293 	 
2025-07-24 21:21:53.277538 test begin: paddle.Tensor.tril(Tensor([2, 12700801, 2],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([2, 12700801, 2],"float32"), -1, ) 	 50803204 	 1000 	 0.4231874942779541 	 0.3214132785797119 	 0.41242122650146484 	 0.30982398986816406 	 0.4162726402282715 	 0.3217496871948242 	 0.3640468120574951 	 0.2577250003814697 	 
2025-07-24 21:21:56.470151 test begin: paddle.Tensor.tril(Tensor([2, 2, 12700801],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([2, 2, 12700801],"float32"), -1, ) 	 50803204 	 1000 	 0.30042386054992676 	 0.266939640045166 	 0.2923603057861328 	 0.2553131580352783 	 0.29987168312072754 	 0.26682233810424805 	 0.2508068084716797 	 0.19477558135986328 	 
2025-07-24 21:21:59.302498 test begin: paddle.Tensor.tril(Tensor([2, 25401601],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([2, 25401601],"float32"), -1, ) 	 50803202 	 1000 	 0.30041050910949707 	 0.18277764320373535 	 0.29234886169433594 	 0.17137956619262695 	 0.2998366355895996 	 0.18267488479614258 	 0.2494182586669922 	 0.1233055591583252 	 
2025-07-24 21:22:02.060759 test begin: paddle.Tensor.tril(Tensor([25401601, 2],"float32"), -1, )
[Prof] paddle.Tensor.tril 	 paddle.Tensor.tril(Tensor([25401601, 2],"float32"), -1, ) 	 50803202 	 1000 	 0.4203190803527832 	 0.3064138889312744 	 0.4122316837310791 	 0.2949395179748535 	 0.4176316261291504 	 0.30628514289855957 	 0.36885857582092285 	 0.24515271186828613 	 
2025-07-24 21:22:05.190074 test begin: paddle.Tensor.trunc(Tensor([1814401, 28],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([1814401, 28],"float32"), ) 	 50803228 	 1000 	 0.0076673030853271484 	 0.2980344295501709 	 2.0503997802734375e-05 	 0.28725123405456543 	 0.04907584190368652 	 0.1341700553894043 	 3.2901763916015625e-05 	 0.06977272033691406 	 
2025-07-24 21:22:07.584200 test begin: paddle.Tensor.trunc(Tensor([2, 3175201, 8],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([2, 3175201, 8],"float32"), ) 	 50803216 	 1000 	 0.007772207260131836 	 0.29790759086608887 	 2.8371810913085938e-05 	 0.28730034828186035 	 0.04890012741088867 	 0.13421988487243652 	 4.7206878662109375e-05 	 0.0664665699005127 	 
2025-07-24 21:22:10.031803 test begin: paddle.Tensor.trunc(Tensor([2, 8, 3175201],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([2, 8, 3175201],"float32"), ) 	 50803216 	 1000 	 0.007633686065673828 	 0.2986643314361572 	 1.4543533325195312e-05 	 0.2841646671295166 	 0.04958319664001465 	 0.13415193557739258 	 3.5762786865234375e-05 	 0.06546568870544434 	 
2025-07-24 21:22:12.396813 test begin: paddle.Tensor.trunc(Tensor([28, 1814401],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([28, 1814401],"float32"), ) 	 50803228 	 1000 	 0.008000850677490234 	 0.30106377601623535 	 3.1948089599609375e-05 	 0.2871713638305664 	 0.049147844314575195 	 0.13412261009216309 	 3.4809112548828125e-05 	 0.07288002967834473 	 
2025-07-24 21:22:14.741217 test begin: paddle.Tensor.trunc(Tensor([6350401, 8],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([6350401, 8],"float32"), ) 	 50803208 	 1000 	 0.011773347854614258 	 0.2979271411895752 	 1.52587890625e-05 	 0.28110527992248535 	 0.0492396354675293 	 0.13413548469543457 	 4.410743713378906e-05 	 0.06619882583618164 	 
2025-07-24 21:22:17.160730 test begin: paddle.Tensor.trunc(Tensor([793801, 8, 8],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([793801, 8, 8],"float32"), ) 	 50803264 	 1000 	 0.007572650909423828 	 0.29789113998413086 	 1.5497207641601562e-05 	 0.28719282150268555 	 0.048841238021850586 	 0.1341402530670166 	 3.647804260253906e-05 	 0.0728597640991211 	 
2025-07-24 21:22:19.569058 test begin: paddle.Tensor.trunc(Tensor([8, 6350401],"float32"), )
[Prof] paddle.Tensor.trunc 	 paddle.Tensor.trunc(Tensor([8, 6350401],"float32"), ) 	 50803208 	 1000 	 0.007648944854736328 	 0.29793238639831543 	 2.193450927734375e-05 	 0.28676271438598633 	 0.05397486686706543 	 0.13420844078063965 	 3.361701965332031e-05 	 0.07400155067443848 	 
2025-07-24 21:22:21.956328 test begin: paddle.Tensor.unbind(Tensor([3, 115, 2304, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([3, 115, 2304, 64],"float32"), 0, ) 	 50872320 	 1000 	 0.0081024169921875 	 0.006343841552734375 	 9.298324584960938e-06 	 2.0742416381835938e-05 	 0.34929418563842773 	 0.3043553829193115 	 0.2806267738342285 	 0.21659445762634277 	 
2025-07-24 21:22:24.355580 test begin: paddle.Tensor.unbind(Tensor([3, 1351, 196, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([3, 1351, 196, 64],"float32"), 0, ) 	 50840832 	 1000 	 0.008560419082641602 	 0.006354808807373047 	 2.7179718017578125e-05 	 2.2411346435546875e-05 	 0.3498091697692871 	 0.3055150508880615 	 0.2801387310028076 	 0.21897077560424805 	 
2025-07-24 21:22:26.747928 test begin: paddle.Tensor.unbind(Tensor([3, 60, 2304, 123],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([3, 60, 2304, 123],"float32"), 0, ) 	 51010560 	 1000 	 0.008194446563720703 	 0.006330013275146484 	 8.58306884765625e-06 	 2.002716064453125e-05 	 0.3502495288848877 	 0.3050374984741211 	 0.2869377136230469 	 0.21855568885803223 	 
2025-07-24 21:22:29.145051 test begin: paddle.Tensor.unbind(Tensor([3, 60, 4411, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([3, 60, 4411, 64],"float32"), 0, ) 	 50814720 	 1000 	 0.01391744613647461 	 0.006375551223754883 	 3.814697265625e-05 	 2.288818359375e-05 	 0.34872913360595703 	 0.30403685569763184 	 0.29232358932495117 	 0.21740961074829102 	 
2025-07-24 21:22:31.575788 test begin: paddle.Tensor.unbind(Tensor([3, 864, 196, 101],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([3, 864, 196, 101],"float32"), 0, ) 	 51311232 	 1000 	 0.008342504501342773 	 0.00637364387512207 	 4.57763671875e-05 	 3.0279159545898438e-05 	 0.35358285903930664 	 0.30701541900634766 	 0.29231762886047363 	 0.21958017349243164 	 
2025-07-24 21:22:34.009941 test begin: paddle.Tensor.unbind(Tensor([3, 864, 307, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([3, 864, 307, 64],"float32"), 0, ) 	 50927616 	 1000 	 0.008198261260986328 	 0.006256103515625 	 1.239776611328125e-05 	 2.1219253540039062e-05 	 0.3517284393310547 	 0.3046553134918213 	 0.29557347297668457 	 0.19536137580871582 	 
2025-07-24 21:22:40.007828 test begin: paddle.Tensor.unbind(Tensor([3, 960, 196, 91],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([3, 960, 196, 91],"float32"), 0, ) 	 51367680 	 1000 	 0.008309602737426758 	 0.006281614303588867 	 1.2159347534179688e-05 	 2.002716064453125e-05 	 0.35315942764282227 	 0.3074302673339844 	 0.2974381446838379 	 0.1966536045074463 	 
2025-07-24 21:22:42.477054 test begin: paddle.Tensor.unbind(Tensor([3, 960, 276, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([3, 960, 276, 64],"float32"), 0, ) 	 50872320 	 1000 	 0.025355815887451172 	 0.010950803756713867 	 5.7697296142578125e-05 	 2.09808349609375e-05 	 0.3513824939727783 	 0.3044750690460205 	 0.22504305839538574 	 0.18958425521850586 	 
2025-07-24 21:22:44.962485 test begin: paddle.Tensor.unbind(Tensor([5, 864, 196, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([5, 864, 196, 64],"float32"), 0, ) 	 54190080 	 1000 	 0.01581859588623047 	 0.013006210327148438 	 1.239776611328125e-05 	 2.5272369384765625e-05 	 0.37177467346191406 	 0.32215285301208496 	 0.29668140411376953 	 0.1820821762084961 	 
2025-07-24 21:22:47.542860 test begin: paddle.Tensor.unbind(Tensor([5, 960, 196, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([5, 960, 196, 64],"float32"), 0, ) 	 60211200 	 1000 	 0.010298967361450195 	 0.00798487663269043 	 1.52587890625e-05 	 1.9788742065429688e-05 	 0.4152393341064453 	 0.3574717044830322 	 0.3555154800415039 	 0.24470949172973633 	 
2025-07-24 21:22:54.082368 test begin: paddle.Tensor.unbind(Tensor([6, 60, 2304, 64],"float32"), 0, )
[Prof] paddle.Tensor.unbind 	 paddle.Tensor.unbind(Tensor([6, 60, 2304, 64],"float32"), 0, ) 	 53084160 	 1000 	 0.011436939239501953 	 0.009418010711669922 	 1.2159347534179688e-05 	 5.173683166503906e-05 	 0.3661208152770996 	 0.31539225578308105 	 0.2922704219818115 	 0.2002255916595459 	 
2025-07-24 21:22:56.627860 test begin: paddle.Tensor.unique(Tensor([25401601],"int64"), )
[Prof] paddle.Tensor.unique 	 paddle.Tensor.unique(Tensor([25401601],"int64"), ) 	 25401601 	 1000 	 6.76351523399353 	 3.287583589553833 	 0.00010347366333007812 	 0.00013709068298339844 	 None 	 None 	 None 	 None 	 
2025-07-24 21:23:07.214523 test begin: paddle.Tensor.unsqueeze(Tensor([172, 544, 544],"float32"), 0, )
Warning: The core code of paddle.Tensor.unsqueeze is too complex.
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([172, 544, 544],"float32"), 0, ) 	 50900992 	 1000 	 0.008478164672851562 	 0.007047414779663086 	 1.1205673217773438e-05 	 2.0265579223632812e-05 	 0.04961395263671875 	 0.059621572494506836 	 4.4345855712890625e-05 	 3.504753112792969e-05 	 
2025-07-24 21:23:09.086063 test begin: paddle.Tensor.unsqueeze(Tensor([172, 544, 544],"float32"), 1, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([172, 544, 544],"float32"), 1, ) 	 50900992 	 1000 	 0.008508682250976562 	 0.007482290267944336 	 1.8358230590820312e-05 	 5.650520324707031e-05 	 0.052825927734375 	 0.05975747108459473 	 5.459785461425781e-05 	 6.461143493652344e-05 	 
2025-07-24 21:23:10.898773 test begin: paddle.Tensor.unsqueeze(Tensor([2, 3840, 10240],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([2, 3840, 10240],"float32"), 0, ) 	 78643200 	 1000 	 0.0041179656982421875 	 0.003580808639526367 	 8.821487426757812e-06 	 1.8596649169921875e-05 	 0.04636883735656738 	 0.05390429496765137 	 3.719329833984375e-05 	 4.38690185546875e-05 	 
2025-07-24 21:23:13.806653 test begin: paddle.Tensor.unsqueeze(Tensor([200, 467, 544],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([200, 467, 544],"float32"), 0, ) 	 50809600 	 1000 	 0.0041713714599609375 	 0.0036427974700927734 	 1.4066696166992188e-05 	 1.8358230590820312e-05 	 0.0452733039855957 	 0.05270123481750488 	 3.4332275390625e-05 	 6.175041198730469e-05 	 
2025-07-24 21:23:15.611418 test begin: paddle.Tensor.unsqueeze(Tensor([200, 467, 544],"float32"), 1, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([200, 467, 544],"float32"), 1, ) 	 50809600 	 1000 	 0.0066034793853759766 	 0.0036902427673339844 	 4.792213439941406e-05 	 2.8848648071289062e-05 	 0.0451202392578125 	 0.055379629135131836 	 3.2901763916015625e-05 	 3.981590270996094e-05 	 
2025-07-24 21:23:17.500128 test begin: paddle.Tensor.unsqueeze(Tensor([200, 544, 467],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([200, 544, 467],"float32"), 0, ) 	 50809600 	 1000 	 0.004094362258911133 	 0.0037987232208251953 	 2.0742416381835938e-05 	 2.2172927856445312e-05 	 0.04485726356506348 	 0.05302572250366211 	 4.1961669921875e-05 	 5.53131103515625e-05 	 
2025-07-24 21:23:19.278007 test begin: paddle.Tensor.unsqueeze(Tensor([200, 544, 467],"float32"), 1, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([200, 544, 467],"float32"), 1, ) 	 50809600 	 1000 	 0.004111528396606445 	 0.0037658214569091797 	 6.4373016357421875e-06 	 3.933906555175781e-05 	 0.04268789291381836 	 0.05265927314758301 	 4.1484832763671875e-05 	 4.982948303222656e-05 	 
2025-07-24 21:23:21.035236 test begin: paddle.Tensor.unsqueeze(Tensor([3, 1654, 10240],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([3, 1654, 10240],"float32"), 0, ) 	 50810880 	 1000 	 0.00408172607421875 	 0.0037834644317626953 	 7.3909759521484375e-06 	 3.4332275390625e-05 	 0.04221463203430176 	 0.052999019622802734 	 2.0742416381835938e-05 	 7.390975952148438e-05 	 
2025-07-24 21:23:22.804385 test begin: paddle.Tensor.unsqueeze(Tensor([3, 3840, 4411],"float32"), 0, )
[Prof] paddle.Tensor.unsqueeze 	 paddle.Tensor.unsqueeze(Tensor([3, 3840, 4411],"float32"), 0, ) 	 50814720 	 1000 	 0.004105091094970703 	 0.0036003589630126953 	 7.62939453125e-06 	 2.288818359375e-05 	 0.04200148582458496 	 0.05225801467895508 	 3.600120544433594e-05 	 3.5762786865234375e-05 	 
2025-07-24 21:23:24.632149 test begin: paddle.Tensor.var(Tensor([1000, 50804],"float32"), axis=0, )
W0724 21:23:25.417567 144283 dygraph_functions.cc:88394] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.Tensor.var 	 paddle.Tensor.var(Tensor([1000, 50804],"float32"), axis=0, ) 	 50804000 	 1000 	 1.2789251804351807 	 0.17345547676086426 	 2.3126602172851562e-05 	 0.15435361862182617 	 1.4338982105255127 	 0.7675590515136719 	 0.2092130184173584 	 0.1959819793701172 	 
2025-07-24 21:23:29.331478 test begin: paddle.Tensor.var(Tensor([100000, 255],"float64"), axis=0, )
[Prof] paddle.Tensor.var 	 paddle.Tensor.var(Tensor([100000, 255],"float64"), axis=0, ) 	 25500000 	 1000 	 1.756016492843628 	 0.1944880485534668 	 4.124641418457031e-05 	 0.09935545921325684 	 1.7080795764923096 	 0.7627739906311035 	 0.24938368797302246 	 0.15592694282531738 	 
2025-07-24 21:23:34.371594 test begin: paddle.Tensor.var(Tensor([1000000, 26],"float64"), axis=0, )
[Prof] paddle.Tensor.var 	 paddle.Tensor.var(Tensor([1000000, 26],"float64"), axis=0, ) 	 26000000 	 1000 	 6.164212703704834 	 0.1897754669189453 	 3.314018249511719e-05 	 0.09695076942443848 	 3.9257655143737793 	 0.7867913246154785 	 0.5743858814239502 	 0.16055583953857422 	 
2025-07-24 21:23:46.057775 test begin: paddle.Tensor.var(Tensor([6350401, 4],"float64"), axis=0, )
[Prof] paddle.Tensor.var 	 paddle.Tensor.var(Tensor([6350401, 4],"float64"), axis=0, ) 	 25401604 	 1000 	 11.534983158111572 	 0.25013017654418945 	 4.6253204345703125e-05 	 0.12775731086730957 	 6.5837695598602295 	 0.7662045955657959 	 0.960723876953125 	 0.1562807559967041 	 
2025-07-24 21:24:05.812158 test begin: paddle.Tensor.var(Tensor([64801, 784],"float32"), axis=0, )
[Prof] paddle.Tensor.var 	 paddle.Tensor.var(Tensor([64801, 784],"float32"), axis=0, ) 	 50803984 	 1000 	 1.4099531173706055 	 0.26118969917297363 	 2.0265579223632812e-05 	 0.13273906707763672 	 1.5008931159973145 	 0.8014233112335205 	 0.21910953521728516 	 0.16376852989196777 	 
2025-07-24 21:24:10.656978 test begin: paddle.Tensor.zero_(Tensor([100352, 507],"float32"), )
[Prof] paddle.Tensor.zero_ 	 paddle.Tensor.zero_(Tensor([100352, 507],"float32"), ) 	 50878464 	 1000 	 0.14573383331298828 	 0.13430023193359375 	 0.1319589614868164 	 0.12680530548095703 	 None 	 None 	 None 	 None 	 
2025-07-24 21:24:12.599057 test begin: paddle.Tensor.zero_(Tensor([507, 100352],"float32"), )
[Prof] paddle.Tensor.zero_ 	 paddle.Tensor.zero_(Tensor([507, 100352],"float32"), ) 	 50878464 	 1000 	 0.14399266242980957 	 0.13431119918823242 	 0.1265721321105957 	 0.12724661827087402 	 None 	 None 	 None 	 None 	 
2025-07-24 21:24:14.568940 test begin: paddle.Tensor.zero_(Tensor([6202, 8192],"float32"), )
[Prof] paddle.Tensor.zero_ 	 paddle.Tensor.zero_(Tensor([6202, 8192],"float32"), ) 	 50806784 	 1000 	 0.14522910118103027 	 0.13414430618286133 	 0.13166332244873047 	 0.12709355354309082 	 None 	 None 	 None 	 None 	 
2025-07-24 21:24:16.554813 test begin: paddle.Tensor.zero_(Tensor([8192, 6202],"float32"), )
[Prof] paddle.Tensor.zero_ 	 paddle.Tensor.zero_(Tensor([8192, 6202],"float32"), ) 	 50806784 	 1000 	 0.14751052856445312 	 0.1341094970703125 	 0.13397502899169922 	 0.12707829475402832 	 None 	 None 	 None 	 None 	 
2025-07-24 21:24:18.587524 test begin: paddle.Tensor.zero_(Tensor([886, 57344],"float32"), )
[Prof] paddle.Tensor.zero_ 	 paddle.Tensor.zero_(Tensor([886, 57344],"float32"), ) 	 50806784 	 1000 	 0.14530324935913086 	 0.13409161567687988 	 0.13177251815795898 	 0.12705326080322266 	 None 	 None 	 None 	 None 	 
2025-07-24 21:24:20.575656 test begin: paddle.abs(Tensor([13, 64, 256, 256],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([13, 64, 256, 256],"float32"), ) 	 54525952 	 1000 	 0.3187549114227295 	 0.32134509086608887 	 0.3083806037902832 	 0.3060140609741211 	 0.48325586318969727 	 0.7982988357543945 	 0.4306173324584961 	 0.4071955680847168 	 
2025-07-24 21:24:24.337648 test begin: paddle.abs(Tensor([16, 128, 128, 194],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 128, 128, 194],"float32"), ) 	 50855936 	 1000 	 0.29660701751708984 	 0.29991984367370605 	 0.2874588966369629 	 0.2848691940307617 	 0.4510042667388916 	 0.7436704635620117 	 0.39849853515625 	 0.37995028495788574 	 
2025-07-24 21:24:27.803168 test begin: paddle.abs(Tensor([16, 128, 194, 128],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 128, 194, 128],"float32"), ) 	 50855936 	 1000 	 0.29660749435424805 	 0.2981433868408203 	 0.2875025272369385 	 0.2850167751312256 	 0.45090246200561523 	 0.7437074184417725 	 0.39716577529907227 	 0.3800017833709717 	 
2025-07-24 21:24:31.295425 test begin: paddle.abs(Tensor([16, 194, 128, 128],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 194, 128, 128],"float32"), ) 	 50855936 	 1000 	 0.29660844802856445 	 0.30190229415893555 	 0.28719329833984375 	 0.2862861156463623 	 0.45102930068969727 	 0.7436273097991943 	 0.39371609687805176 	 0.37994384765625 	 
2025-07-24 21:24:34.783271 test begin: paddle.abs(Tensor([16, 256, 194, 64],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 256, 194, 64],"float32"), ) 	 50855936 	 1000 	 1.138495683670044 	 0.31572937965393066 	 0.2874281406402588 	 0.2785029411315918 	 0.45114636421203613 	 0.7437124252319336 	 0.38700389862060547 	 0.3800203800201416 	 
2025-07-24 21:24:40.909987 test begin: paddle.abs(Tensor([16, 256, 64, 194],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 256, 64, 194],"float32"), ) 	 50855936 	 1000 	 0.2992556095123291 	 0.29815196990966797 	 0.2803974151611328 	 0.2751338481903076 	 0.4509849548339844 	 0.7436990737915039 	 0.3894832134246826 	 0.3799409866333008 	 
2025-07-24 21:24:44.420003 test begin: paddle.abs(Tensor([16, 49, 256, 256],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 49, 256, 256],"float32"), ) 	 51380224 	 1000 	 0.3145613670349121 	 0.3011791706085205 	 0.2907743453979492 	 0.2879490852355957 	 0.4553987979888916 	 0.7513175010681152 	 0.40056753158569336 	 0.38388657569885254 	 
2025-07-24 21:24:47.901559 test begin: paddle.abs(Tensor([16, 64, 194, 256],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 64, 194, 256],"float32"), ) 	 50855936 	 1000 	 0.29660487174987793 	 0.29813575744628906 	 0.28743600845336914 	 0.2850775718688965 	 0.4525158405303955 	 0.7436745166778564 	 0.39797163009643555 	 0.37999582290649414 	 
2025-07-24 21:24:51.373286 test begin: paddle.abs(Tensor([16, 64, 256, 194],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 64, 256, 194],"float32"), ) 	 50855936 	 1000 	 0.2965872287750244 	 0.30026912689208984 	 0.28740453720092773 	 0.28499722480773926 	 0.4508359432220459 	 0.7450408935546875 	 0.39836597442626953 	 0.38135623931884766 	 
2025-07-24 21:24:55.099517 test begin: paddle.abs(Tensor([16, 776, 64, 64],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([16, 776, 64, 64],"float32"), ) 	 50855936 	 1000 	 0.2982189655303955 	 0.2981257438659668 	 0.2804703712463379 	 0.27859067916870117 	 0.4509258270263672 	 0.7437083721160889 	 0.3797597885131836 	 0.3800017833709717 	 
2025-07-24 21:24:58.661652 test begin: paddle.abs(Tensor([25, 128, 128, 128],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([25, 128, 128, 128],"float32"), ) 	 52428800 	 1000 	 0.9039394855499268 	 0.32616496086120605 	 0.28794002532958984 	 0.29206204414367676 	 0.4648261070251465 	 0.7666916847229004 	 0.4034700393676758 	 0.39171624183654785 	 
2025-07-24 21:25:05.573893 test begin: paddle.abs(Tensor([49, 256, 64, 64],"float32"), )
[Prof] paddle.abs 	 paddle.abs(Tensor([49, 256, 64, 64],"float32"), ) 	 51380224 	 1000 	 0.30220770835876465 	 0.3011901378631592 	 0.291715145111084 	 0.2879946231842041 	 0.45555996894836426 	 0.7525417804718018 	 0.40311098098754883 	 0.38509607315063477 	 
2025-07-24 21:25:09.095050 test begin: paddle.acos(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.acos 	 paddle.acos(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 1000 	 0.29881763458251953 	 0.29797935485839844 	 0.2845783233642578 	 0.28690671920776367 	 0.4521782398223877 	 2.080960750579834 	 0.39823126792907715 	 0.3544480800628662 	 
2025-07-24 21:25:14.214591 test begin: paddle.acos(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.acos 	 paddle.acos(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 1000 	 0.29628539085388184 	 0.297741174697876 	 0.28773951530456543 	 0.2867591381072998 	 0.45067572593688965 	 2.0820817947387695 	 0.3979926109313965 	 0.3556993007659912 	 
2025-07-24 21:25:19.010442 test begin: paddle.acos(Tensor([10, 5080321],"float32"), )
[Prof] paddle.acos 	 paddle.acos(Tensor([10, 5080321],"float32"), ) 	 50803210 	 1000 	 0.29624056816101074 	 0.2977938652038574 	 0.28789639472961426 	 0.2868359088897705 	 0.4504432678222656 	 2.0820729732513428 	 0.3976316452026367 	 0.35442566871643066 	 
2025-07-24 21:25:23.859855 test begin: paddle.acos(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.acos 	 paddle.acos(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 1000 	 0.29625725746154785 	 0.2977926731109619 	 0.28775691986083984 	 0.28684210777282715 	 0.45046448707580566 	 2.0821034908294678 	 0.39751482009887695 	 0.35450029373168945 	 
2025-07-24 21:25:28.729018 test begin: paddle.acos(Tensor([5080321, 10],"float32"), )
[Prof] paddle.acos 	 paddle.acos(Tensor([5080321, 10],"float32"), ) 	 50803210 	 1000 	 0.29851222038269043 	 0.2977890968322754 	 0.2891225814819336 	 0.2868235111236572 	 0.4506218433380127 	 2.082320213317871 	 0.3977165222167969 	 0.35448360443115234 	 
2025-07-24 21:25:33.611294 test begin: paddle.acos(x=Tensor([3, 3, 5644801],"float32"), )
[Prof] paddle.acos 	 paddle.acos(x=Tensor([3, 3, 5644801],"float32"), ) 	 50803209 	 1000 	 0.316342830657959 	 0.3026549816131592 	 0.2888956069946289 	 0.28679966926574707 	 0.4505765438079834 	 2.0833170413970947 	 0.39737606048583984 	 0.35568690299987793 	 
2025-07-24 21:25:41.718019 test begin: paddle.acos(x=Tensor([3, 5644801, 3],"float32"), )
[Prof] paddle.acos 	 paddle.acos(x=Tensor([3, 5644801, 3],"float32"), ) 	 50803209 	 1000 	 0.2962303161621094 	 0.2978818416595459 	 0.28763651847839355 	 0.28573060035705566 	 0.4507150650024414 	 2.0833420753479004 	 0.39722776412963867 	 0.35450053215026855 	 
2025-07-24 21:25:46.546952 test begin: paddle.acos(x=Tensor([5644801, 3, 3],"float32"), )
[Prof] paddle.acos 	 paddle.acos(x=Tensor([5644801, 3, 3],"float32"), ) 	 50803209 	 1000 	 0.296248197555542 	 0.3036916255950928 	 0.2800638675689697 	 0.27770161628723145 	 0.4504680633544922 	 2.083555221557617 	 0.3883068561553955 	 0.3544642925262451 	 
2025-07-24 21:25:51.460288 test begin: paddle.acosh(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.acosh 	 paddle.acosh(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 1000 	 0.295870304107666 	 0.2991352081298828 	 0.28024792671203613 	 0.2818787097930908 	 0.45219969749450684 	 1.3407936096191406 	 0.38997697830200195 	 0.34198689460754395 	 
2025-07-24 21:25:55.580900 test begin: paddle.acosh(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.acosh 	 paddle.acosh(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 1000 	 0.2958638668060303 	 0.29906201362609863 	 0.28026247024536133 	 0.28156375885009766 	 0.45223212242126465 	 1.33955717086792 	 0.3902316093444824 	 0.34195470809936523 	 
2025-07-24 21:25:59.797899 test begin: paddle.acosh(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.acosh 	 paddle.acosh(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 1000 	 0.29590559005737305 	 0.2990577220916748 	 0.2875533103942871 	 0.28795289993286133 	 0.4521462917327881 	 1.3392255306243896 	 0.3971219062805176 	 0.3431122303009033 	 
2025-07-24 21:26:03.858827 test begin: paddle.add(x=Tensor([2, 256, 320, 352],"float32"), y=Tensor([2, 256, 320, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([2, 256, 320, 352],"float32"), y=Tensor([2, 256, 320, 352],"float32"), ) 	 115343360 	 1000 	 0.5120975971221924 	 0.5065169334411621 	 0.5011646747589111 	 0.49434518814086914 	 0.5491459369659424 	 0.0540461540222168 	 0.49071645736694336 	 5.078315734863281e-05 	 
2025-07-24 21:26:10.625779 test begin: paddle.add(x=Tensor([2, 256, 336, 336],"float32"), y=Tensor([2, 256, 336, 336],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([2, 256, 336, 336],"float32"), y=Tensor([2, 256, 336, 336],"float32"), ) 	 115605504 	 1000 	 0.5120811462402344 	 0.5098476409912109 	 0.5025224685668945 	 0.49510669708251953 	 0.5501887798309326 	 0.05375838279724121 	 0.4919455051422119 	 4.57763671875e-05 	 
2025-07-24 21:26:15.135344 test begin: paddle.add(x=Tensor([2, 256, 352, 352],"float32"), y=Tensor([2, 256, 352, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([2, 256, 352, 352],"float32"), y=Tensor([2, 256, 352, 352],"float32"), ) 	 126877696 	 1000 	 0.5615622997283936 	 0.5582118034362793 	 0.5520486831665039 	 0.5461075305938721 	 0.6037716865539551 	 0.058838844299316406 	 0.5453948974609375 	 6.604194641113281e-05 	 
2025-07-24 21:26:20.102153 test begin: paddle.add(x=Tensor([8, 256, 320, 78],"float32"), y=Tensor([8, 256, 320, 78],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 320, 78],"float32"), y=Tensor([8, 256, 320, 78],"float32"), ) 	 102236160 	 1000 	 0.4533884525299072 	 0.4495582580566406 	 0.44384193420410156 	 0.4364662170410156 	 0.48710012435913086 	 0.05519580841064453 	 0.42902135848999023 	 4.792213439941406e-05 	 
2025-07-24 21:26:24.109425 test begin: paddle.add(x=Tensor([8, 256, 336, 74],"float32"), y=Tensor([8, 256, 336, 74],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 336, 74],"float32"), y=Tensor([8, 256, 336, 74],"float32"), ) 	 101842944 	 1000 	 0.4515960216522217 	 0.4497871398925781 	 0.43462395668029785 	 0.42372679710388184 	 0.48392772674560547 	 0.06023406982421875 	 0.4169144630432129 	 4.172325134277344e-05 	 
2025-07-24 21:26:28.150366 test begin: paddle.add(x=Tensor([8, 256, 352, 71],"float32"), y=Tensor([8, 256, 352, 71],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 352, 71],"float32"), y=Tensor([8, 256, 352, 71],"float32"), ) 	 102367232 	 1000 	 0.4539473056793213 	 0.45009827613830566 	 0.4369990825653076 	 0.4314296245574951 	 0.48641252517700195 	 0.06043601036071777 	 0.4150557518005371 	 4.9591064453125e-05 	 
2025-07-24 21:26:32.197201 test begin: paddle.add(x=Tensor([8, 256, 71, 352],"float32"), y=Tensor([8, 256, 71, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 71, 352],"float32"), y=Tensor([8, 256, 71, 352],"float32"), ) 	 102367232 	 1000 	 0.45536279678344727 	 0.45682835578918457 	 0.44451117515563965 	 0.4370245933532715 	 0.48629307746887207 	 0.05367875099182129 	 0.4280087947845459 	 5.936622619628906e-05 	 
2025-07-24 21:26:39.104710 test begin: paddle.add(x=Tensor([8, 256, 74, 336],"float32"), y=Tensor([8, 256, 74, 336],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 256, 74, 336],"float32"), y=Tensor([8, 256, 74, 336],"float32"), ) 	 101842944 	 1000 	 0.4530189037322998 	 0.44777989387512207 	 0.4417531490325928 	 0.4349954128265381 	 0.48390674591064453 	 0.07506752014160156 	 0.4254171848297119 	 0.00010704994201660156 	 
2025-07-24 21:26:42.969863 test begin: paddle.add(x=Tensor([8, 52, 352, 352],"float32"), y=Tensor([8, 52, 352, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 52, 352, 352],"float32"), y=Tensor([8, 52, 352, 352],"float32"), ) 	 103088128 	 1000 	 0.458204984664917 	 0.4532740116119385 	 0.44756650924682617 	 0.4404940605163574 	 0.4900810718536377 	 0.07180094718933105 	 0.4314558506011963 	 6.4849853515625e-05 	 
2025-07-24 21:26:46.917097 test begin: paddle.add(x=Tensor([8, 57, 320, 352],"float32"), y=Tensor([8, 57, 320, 352],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 57, 320, 352],"float32"), y=Tensor([8, 57, 320, 352],"float32"), ) 	 102727680 	 1000 	 0.4554924964904785 	 0.4609394073486328 	 0.44600915908813477 	 0.4392964839935303 	 0.48830175399780273 	 0.05500149726867676 	 0.4296000003814697 	 7.009506225585938e-05 	 
2025-07-24 21:26:50.977685 test begin: paddle.add(x=Tensor([8, 57, 336, 336],"float32"), y=Tensor([8, 57, 336, 336],"float32"), )
[Prof] paddle.add 	 paddle.add(x=Tensor([8, 57, 336, 336],"float32"), y=Tensor([8, 57, 336, 336],"float32"), ) 	 102961152 	 1000 	 0.45647692680358887 	 0.45276331901550293 	 0.4468502998352051 	 0.4374535083770752 	 0.4907550811767578 	 0.05420041084289551 	 0.4148674011230469 	 3.719329833984375e-05 	 
2025-07-24 21:26:55.015438 test begin: paddle.add_n(list[Tensor([194, 128, 64, 64],"float16"),Tensor([194, 128, 64, 64],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([194, 128, 64, 64],"float16"),Tensor([194, 128, 64, 64],"float16"),], ) 	 203423744 	 1000 	 0.5735902786254883 	 1.5167999267578125 	 0.5619838237762451 	 0.7749195098876953 	 None 	 None 	 None 	 None 	 combined
2025-07-24 21:27:01.105572 test begin: paddle.add_n(list[Tensor([388, 256, 32, 32],"float16"),Tensor([388, 256, 32, 32],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([388, 256, 32, 32],"float16"),Tensor([388, 256, 32, 32],"float16"),], ) 	 203423744 	 1000 	 0.5723817348480225 	 1.5175256729125977 	 0.5613045692443848 	 0.7753305435180664 	 None 	 None 	 None 	 None 	 combined
2025-07-24 21:27:07.051905 test begin: paddle.add_n(list[Tensor([64, 128, 194, 64],"float16"),Tensor([64, 128, 194, 64],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 128, 194, 64],"float16"),Tensor([64, 128, 194, 64],"float16"),], ) 	 203423744 	 1000 	 0.5747003555297852 	 1.522594690322876 	 0.5629947185516357 	 0.7763259410858154 	 None 	 None 	 None 	 None 	 combined
2025-07-24 21:27:16.359271 test begin: paddle.add_n(list[Tensor([64, 128, 64, 194],"float16"),Tensor([64, 128, 64, 194],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 128, 64, 194],"float16"),Tensor([64, 128, 64, 194],"float16"),], ) 	 203423744 	 1000 	 0.5748739242553711 	 1.5161933898925781 	 0.5649640560150146 	 0.7745816707611084 	 None 	 None 	 None 	 None 	 combined
2025-07-24 21:27:22.246897 test begin: paddle.add_n(list[Tensor([64, 128, 64, 97],"float32"),Tensor([64, 128, 64, 97],"float32"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 128, 64, 97],"float32"),Tensor([64, 128, 64, 97],"float32"),], ) 	 101711872 	 1000 	 0.47336316108703613 	 1.0610260963439941 	 0.46335482597351074 	 0.5434978008270264 	 None 	 None 	 None 	 None 	 combined
2025-07-24 21:27:25.525926 test begin: paddle.add_n(list[Tensor([64, 128, 97, 64],"float32"),Tensor([64, 128, 97, 64],"float32"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 128, 97, 64],"float32"),Tensor([64, 128, 97, 64],"float32"),], ) 	 101711872 	 1000 	 0.4735097885131836 	 1.0583853721618652 	 0.4635646343231201 	 0.540827751159668 	 None 	 None 	 None 	 None 	 combined
2025-07-24 21:27:28.706640 test begin: paddle.add_n(list[Tensor([64, 1551, 32, 32],"float16"),Tensor([64, 1551, 32, 32],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 1551, 32, 32],"float16"),Tensor([64, 1551, 32, 32],"float16"),], ) 	 203292672 	 1000 	 0.573411226272583 	 1.518782138824463 	 0.5595142841339111 	 0.7766861915588379 	 None 	 None 	 None 	 None 	 combined
2025-07-24 21:27:34.715012 test begin: paddle.add_n(list[Tensor([64, 194, 64, 64],"float32"),Tensor([64, 194, 64, 64],"float32"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 194, 64, 64],"float32"),Tensor([64, 194, 64, 64],"float32"),], ) 	 101711872 	 1000 	 1.2169077396392822 	 1.065352439880371 	 0.46105098724365234 	 0.540881872177124 	 None 	 None 	 None 	 None 	 combined
2025-07-24 21:27:40.644556 test begin: paddle.add_n(list[Tensor([64, 256, 194, 32],"float16"),Tensor([64, 256, 194, 32],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 256, 194, 32],"float16"),Tensor([64, 256, 194, 32],"float16"),], ) 	 203423744 	 1000 	 0.572354793548584 	 1.5203919410705566 	 0.5622615814208984 	 0.7767417430877686 	 None 	 None 	 None 	 None 	 combined
2025-07-24 21:27:46.592228 test begin: paddle.add_n(list[Tensor([64, 256, 32, 194],"float16"),Tensor([64, 256, 32, 194],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 256, 32, 194],"float16"),Tensor([64, 256, 32, 194],"float16"),], ) 	 203423744 	 1000 	 0.5756800174713135 	 1.516334056854248 	 0.5644843578338623 	 0.7746684551239014 	 None 	 None 	 None 	 None 	 combined
2025-07-24 21:27:52.531224 test begin: paddle.add_n(list[Tensor([64, 388, 64, 64],"float16"),Tensor([64, 388, 64, 64],"float16"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([64, 388, 64, 64],"float16"),Tensor([64, 388, 64, 64],"float16"),], ) 	 203423744 	 1000 	 0.5723352432250977 	 1.5176441669464111 	 0.5605330467224121 	 0.775141716003418 	 None 	 None 	 None 	 None 	 combined
2025-07-24 21:27:58.424313 test begin: paddle.add_n(list[Tensor([97, 128, 64, 64],"float32"),Tensor([97, 128, 64, 64],"float32"),], )
[Prof] paddle.add_n 	 paddle.add_n(list[Tensor([97, 128, 64, 64],"float32"),Tensor([97, 128, 64, 64],"float32"),], ) 	 101711872 	 1000 	 0.47341346740722656 	 1.0585112571716309 	 0.46343088150024414 	 0.540930986404419 	 None 	 None 	 None 	 None 	 combined
2025-07-24 21:28:01.637933 test begin: paddle.addmm(Tensor([1016065, 50],"float32"), Tensor([1016065, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([1016065, 50],"float32"), Tensor([1016065, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, ) 	 132092450 	 1000 	 1.0229952335357666 	 0.9908308982849121 	 0.5226016044616699 	 0.33647656440734863 	 2.675553321838379 	 1.8732242584228516 	 0.3905823230743408 	 0.47795915603637695 	 
2025-07-24 21:28:11.333428 test begin: paddle.addmm(Tensor([30, 1693441],"float32"), Tensor([30, 80],"float32"), Tensor([80, 1693441],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([30, 1693441],"float32"), Tensor([30, 80],"float32"), Tensor([80, 1693441],"float32"), alpha=1.0, beta=2.0, ) 	 186280910 	 1000 	 1.1678309440612793 	 1.17006516456604 	 0.39687585830688477 	 0.29441142082214355 	 3.2175374031066895 	 2.093686103820801 	 0.41112208366394043 	 0.42772603034973145 	 
2025-07-24 21:28:24.577112 test begin: paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 1016065],"float32"), Tensor([1016065, 50],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 1016065],"float32"), Tensor([1016065, 50],"float32"), alpha=1.0, beta=2.0, ) 	 81286700 	 1000 	 0.3164551258087158 	 0.31220245361328125 	 0.10736417770385742 	 0.10643601417541504 	 1.0953047275543213 	 0.6215786933898926 	 0.18673324584960938 	 0.21204447746276855 	 
2025-07-24 21:28:28.242361 test begin: paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 1693441],"float32"), Tensor([1693441, 50],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 1693441],"float32"), Tensor([1693441, 50],"float32"), alpha=1.0, beta=2.0, ) 	 135476780 	 1000 	 0.5045108795166016 	 0.49684810638427734 	 0.17166519165039062 	 0.16939854621887207 	 1.8270368576049805 	 1.0500884056091309 	 0.2338113784790039 	 0.2134709358215332 	 
2025-07-24 21:28:34.337160 test begin: paddle.addmm(Tensor([30, 635041],"float32"), Tensor([30, 80],"float32"), Tensor([80, 635041],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([30, 635041],"float32"), Tensor([30, 80],"float32"), Tensor([80, 635041],"float32"), alpha=1.0, beta=2.0, ) 	 69856910 	 1000 	 0.8209397792816162 	 0.422196626663208 	 0.216078519821167 	 0.14237189292907715 	 1.2194912433624268 	 0.7931909561157227 	 0.17795586585998535 	 0.20230412483215332 	 
2025-07-24 21:28:41.916308 test begin: paddle.addmm(Tensor([635041, 50],"float32"), Tensor([635041, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, )
[Prof] paddle.addmm 	 paddle.addmm(Tensor([635041, 50],"float32"), Tensor([635041, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, ) 	 82559330 	 1000 	 0.6479201316833496 	 0.6301007270812988 	 0.331010103225708 	 0.2146008014678955 	 1.6930603981018066 	 1.1818323135375977 	 0.2467179298400879 	 0.3015143871307373 	 
2025-07-24 21:28:47.938186 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 5080321],"float64"), y=Tensor([5080321, 5],"float64"), )
[Prof] paddle.addmm 	 paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 5080321],"float64"), y=Tensor([5080321, 5],"float64"), ) 	 50803235 	 1000 	 0.6187000274658203 	 0.6214823722839355 	 0.21086621284484863 	 0.21189379692077637 	 3.0052764415740967 	 2.5340518951416016 	 0.2191624641418457 	 0.2586972713470459 	 
2025-07-24 21:28:55.906555 test begin: paddle.all(Tensor([423361, 6, 10],"float64"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([423361, 6, 10],"float64"), None, False, None, ) 	 25401660 	 1000 	 0.20074701309204102 	 0.15056419372558594 	 0.06834053993225098 	 0.07687973976135254 	 None 	 None 	 None 	 None 	 
2025-07-24 21:28:56.849396 test begin: paddle.all(Tensor([5, 1016065, 10],"bool"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([5, 1016065, 10],"bool"), None, False, None, ) 	 50803250 	 1000 	 0.05220532417297363 	 0.06130862236022949 	 0.026657819747924805 	 0.03131699562072754 	 None 	 None 	 None 	 None 	 
2025-07-24 21:28:57.720972 test begin: paddle.all(Tensor([5, 508033, 10],"float64"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([5, 508033, 10],"float64"), None, False, None, ) 	 25401650 	 1000 	 0.20067286491394043 	 0.15028929710388184 	 0.06824684143066406 	 0.07680845260620117 	 None 	 None 	 None 	 None 	 
2025-07-24 21:28:58.662851 test begin: paddle.all(Tensor([5, 6, 1693441],"bool"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([5, 6, 1693441],"bool"), None, False, None, ) 	 50803230 	 1000 	 0.05346202850341797 	 0.06130552291870117 	 0.02788853645324707 	 0.03131461143493652 	 None 	 None 	 None 	 None 	 
2025-07-24 21:28:59.489802 test begin: paddle.all(Tensor([5, 6, 846721],"float64"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([5, 6, 846721],"float64"), None, False, None, ) 	 25401630 	 1000 	 0.20216751098632812 	 0.1502702236175537 	 0.06876087188720703 	 0.0767674446105957 	 None 	 None 	 None 	 None 	 
2025-07-24 21:29:00.365144 test begin: paddle.all(Tensor([50803201],"bool"), )
[Prof] paddle.all 	 paddle.all(Tensor([50803201],"bool"), ) 	 50803201 	 1000 	 0.05221223831176758 	 0.06131625175476074 	 0.025919437408447266 	 0.03130698204040527 	 None 	 None 	 None 	 None 	 
2025-07-24 21:29:01.173363 test begin: paddle.all(Tensor([846721, 6, 10],"bool"), None, False, None, )
[Prof] paddle.all 	 paddle.all(Tensor([846721, 6, 10],"bool"), None, False, None, ) 	 50803260 	 1000 	 0.05222129821777344 	 0.06131339073181152 	 0.02666449546813965 	 0.03132367134094238 	 None 	 None 	 None 	 None 	 
2025-07-24 21:29:02.010637 test begin: paddle.allclose(Tensor([1124, 45199],"float32"), Tensor([1124, 45199],"float32"), )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([1124, 45199],"float32"), Tensor([1124, 45199],"float32"), ) 	 101607352 	 1000 	 1.0613431930541992 	 3.4180209636688232 	 1.0504274368286133 	 9.369850158691406e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:29:08.541642 test begin: paddle.allclose(Tensor([13, 32, 122124],"float32"), Tensor([13, 32, 122124],"float32"), rtol=0.0001, atol=0.0001, )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([13, 32, 122124],"float32"), Tensor([13, 32, 122124],"float32"), rtol=0.0001, atol=0.0001, ) 	 101607168 	 1000 	 1.0582878589630127 	 3.4157955646514893 	 1.046895980834961 	 0.00011563301086425781 	 None 	 None 	 None 	 None 	 
2025-07-24 21:29:14.703694 test begin: paddle.allclose(Tensor([13, 61062, 64],"float32"), Tensor([13, 61062, 64],"float32"), rtol=0.0001, atol=0.0001, )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([13, 61062, 64],"float32"), Tensor([13, 61062, 64],"float32"), rtol=0.0001, atol=0.0001, ) 	 101607168 	 1000 	 1.0902881622314453 	 3.4160690307617188 	 1.0789573192596436 	 0.00022840499877929688 	 None 	 None 	 None 	 None 	 
2025-07-24 21:29:22.889057 test begin: paddle.allclose(Tensor([1587601, 32],"float32"), Tensor([1587601, 32],"float32"), )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([1587601, 32],"float32"), Tensor([1587601, 32],"float32"), ) 	 101606464 	 1000 	 1.0547993183135986 	 3.4117848873138428 	 1.0434935092926025 	 0.0002448558807373047 	 None 	 None 	 None 	 None 	 
2025-07-24 21:29:29.965585 test begin: paddle.allclose(Tensor([24807, 32, 64],"float32"), Tensor([24807, 32, 64],"float32"), rtol=0.0001, atol=0.0001, )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([24807, 32, 64],"float32"), Tensor([24807, 32, 64],"float32"), rtol=0.0001, atol=0.0001, ) 	 101609472 	 1000 	 1.0812265872955322 	 3.4511799812316895 	 1.0698513984680176 	 0.00011563301086425781 	 None 	 None 	 None 	 None 	 
2025-07-24 21:29:37.972801 test begin: paddle.allclose(Tensor([30522, 1665],"float32"), Tensor([30522, 1665],"float32"), )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([30522, 1665],"float32"), Tensor([30522, 1665],"float32"), ) 	 101638260 	 1000 	 1.0595753192901611 	 3.415442943572998 	 1.0451018810272217 	 0.00015091896057128906 	 None 	 None 	 None 	 None 	 
2025-07-24 21:29:44.683330 test begin: paddle.allclose(Tensor([6350401, 8],"float32"), Tensor([6350401, 8],"float32"), )
[Prof] paddle.allclose 	 paddle.allclose(Tensor([6350401, 8],"float32"), Tensor([6350401, 8],"float32"), ) 	 101606416 	 1000 	 1.083580493927002 	 3.4113528728485107 	 1.0688536167144775 	 8.797645568847656e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:29:50.914564 test begin: paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[-1,-2,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 1000 	 0.22251629829406738 	 0.17829394340515137 	 0.11350870132446289 	 0.09108471870422363 	 1.1022601127624512 	 1.3126411437988281 	 0.22517800331115723 	 0.22355318069458008 	 
2025-07-24 21:29:54.595074 test begin: paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 1000 	 0.23637056350708008 	 0.20154833793640137 	 0.12079071998596191 	 0.10364222526550293 	 1.1146230697631836 	 1.3564484119415283 	 0.2278759479522705 	 0.2309126853942871 	 
2025-07-24 21:29:58.368694 test begin: paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[0,1,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 10, 508033],"float32"), axis=list[0,1,], keepdim=False, ) 	 50803300 	 1000 	 0.19734454154968262 	 0.15974974632263184 	 0.17784905433654785 	 0.13720273971557617 	 1.0933747291564941 	 1.2915103435516357 	 0.27897095680236816 	 0.26349496841430664 	 
2025-07-24 21:30:01.979801 test begin: paddle.amax(Tensor([10, 508033, 10],"float32"), axis=list[-1,-2,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 508033, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 1000 	 0.22214484214782715 	 0.1797013282775879 	 0.11350345611572266 	 0.09257173538208008 	 1.1010775566101074 	 1.3124616146087646 	 0.22517156600952148 	 0.22346234321594238 	 
2025-07-24 21:30:05.644022 test begin: paddle.amax(Tensor([10, 508033, 10],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 508033, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 1000 	 0.5711655616760254 	 0.45489501953125 	 0.5502200126647949 	 0.4309518337249756 	 1.3108789920806885 	 1.461857795715332 	 0.3348541259765625 	 0.2982621192932129 	 
2025-07-24 21:30:10.284831 test begin: paddle.amax(Tensor([10, 508033, 10],"float32"), axis=list[0,1,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([10, 508033, 10],"float32"), axis=list[0,1,], keepdim=False, ) 	 50803300 	 1000 	 5.931381464004517 	 0.16629910469055176 	 3.0308947563171387 	 0.08498978614807129 	 5.878088474273682 	 1.2941410541534424 	 1.2001726627349854 	 0.22008013725280762 	 
2025-07-24 21:30:24.451217 test begin: paddle.amax(Tensor([508033, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([508033, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 1000 	 0.4400901794433594 	 0.3004636764526367 	 0.4230811595916748 	 0.2736823558807373 	 1.1776618957519531 	 1.3122475147247314 	 0.30082225799560547 	 0.26799535751342773 	 
2025-07-24 21:30:30.949725 test begin: paddle.amax(Tensor([508033, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([508033, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 1000 	 0.25415921211242676 	 0.21604228019714355 	 0.12991690635681152 	 0.11038994789123535 	 1.1327760219573975 	 1.4257335662841797 	 0.23137521743774414 	 0.24278879165649414 	 
2025-07-24 21:30:35.077053 test begin: paddle.amax(Tensor([508033, 10, 10],"float32"), axis=list[0,1,], keepdim=False, )
[Prof] paddle.amax 	 paddle.amax(Tensor([508033, 10, 10],"float32"), axis=list[0,1,], keepdim=False, ) 	 50803300 	 1000 	 5.9316112995147705 	 0.16624164581298828 	 3.0323476791381836 	 0.08492422103881836 	 5.876930475234985 	 1.2926852703094482 	 1.1990103721618652 	 0.22007250785827637 	 
2025-07-24 21:30:49.263939 test begin: paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[-1,-2,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 1000 	 0.22208762168884277 	 0.1782236099243164 	 0.11345648765563965 	 0.09100627899169922 	 1.1022307872772217 	 1.3124566078186035 	 0.22517180442810059 	 0.22338390350341797 	 
2025-07-24 21:30:52.948796 test begin: paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 1000 	 0.2363126277923584 	 0.20007586479187012 	 0.12074637413024902 	 0.10224246978759766 	 1.117295265197754 	 1.3561174869537354 	 0.22917461395263672 	 0.2309103012084961 	 
2025-07-24 21:30:56.723680 test begin: paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[0,1,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 10, 508033],"float32"), axis=list[0,1,], keepdim=False, ) 	 50803300 	 1000 	 0.19739484786987305 	 0.16225075721740723 	 0.18573331832885742 	 0.13962841033935547 	 1.0917565822601318 	 1.2915267944335938 	 0.2789576053619385 	 0.26351428031921387 	 
2025-07-24 21:31:00.319985 test begin: paddle.amin(Tensor([10, 508033, 10],"float32"), axis=list[-1,-2,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 508033, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 1000 	 0.2221083641052246 	 0.17820191383361816 	 0.11348915100097656 	 0.09108138084411621 	 1.1010994911193848 	 1.3139433860778809 	 0.22516536712646484 	 0.22347426414489746 	 
2025-07-24 21:31:04.125418 test begin: paddle.amin(Tensor([10, 508033, 10],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 508033, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 1000 	 0.5723683834075928 	 0.8951239585876465 	 0.5579164028167725 	 0.4374427795410156 	 1.311218500137329 	 1.4634361267089844 	 0.33495092391967773 	 0.29836201667785645 	 
2025-07-24 21:31:11.353902 test begin: paddle.amin(Tensor([10, 508033, 10],"float32"), axis=list[0,1,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([10, 508033, 10],"float32"), axis=list[0,1,], keepdim=False, ) 	 50803300 	 1000 	 5.931507587432861 	 0.1674959659576416 	 3.0309391021728516 	 0.08613896369934082 	 5.878152132034302 	 1.292781114578247 	 1.2001638412475586 	 0.22010183334350586 	 
2025-07-24 21:31:25.504354 test begin: paddle.amin(Tensor([508033, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([508033, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 	 50803300 	 1000 	 0.4401412010192871 	 0.292527437210083 	 0.42769622802734375 	 0.27472877502441406 	 1.1775627136230469 	 1.313647985458374 	 0.3008384704589844 	 0.26802706718444824 	 
2025-07-24 21:31:29.617079 test begin: paddle.amin(Tensor([508033, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([508033, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 	 50803300 	 1000 	 0.2542145252227783 	 0.2162933349609375 	 0.12987542152404785 	 0.11055684089660645 	 1.1315388679504395 	 1.426079511642456 	 0.23140287399291992 	 0.2427382469177246 	 
2025-07-24 21:31:36.463264 test begin: paddle.amin(Tensor([508033, 10, 10],"float32"), axis=list[0,1,], keepdim=False, )
[Prof] paddle.amin 	 paddle.amin(Tensor([508033, 10, 10],"float32"), axis=list[0,1,], keepdim=False, ) 	 50803300 	 1000 	 5.9313271045684814 	 0.16629433631896973 	 3.032076835632324 	 0.08493852615356445 	 5.8770530223846436 	 1.2942805290222168 	 1.1989622116088867 	 0.22006750106811523 	 
2025-07-24 21:31:50.603383 test begin: paddle.any(Tensor([1, 12404, 4096],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([1, 12404, 4096],"bool"), ) 	 50806784 	 1000 	 0.05246305465698242 	 0.06467556953430176 	 0.02678704261779785 	 0.03303813934326172 	 None 	 None 	 None 	 None 	 
2025-07-24 21:31:51.414003 test begin: paddle.any(Tensor([1, 300, 169345],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([1, 300, 169345],"bool"), ) 	 50803500 	 1000 	 0.052724361419677734 	 0.0647127628326416 	 0.02691817283630371 	 0.0330502986907959 	 None 	 None 	 None 	 None 	 
2025-07-24 21:31:52.243138 test begin: paddle.any(Tensor([1124, 45199],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([1124, 45199],"bool"), ) 	 50803676 	 1000 	 0.052725791931152344 	 0.06468558311462402 	 0.026919126510620117 	 0.033045291900634766 	 None 	 None 	 None 	 None 	 
2025-07-24 21:31:53.163100 test begin: paddle.any(Tensor([1587601, 32],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([1587601, 32],"bool"), ) 	 50803232 	 1000 	 0.05278325080871582 	 0.06468868255615234 	 0.026956796646118164 	 0.03305339813232422 	 None 	 None 	 None 	 None 	 
2025-07-24 21:31:53.995451 test begin: paddle.any(Tensor([42, 300, 4096],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([42, 300, 4096],"bool"), ) 	 51609600 	 1000 	 0.05325722694396973 	 0.06552863121032715 	 0.027193307876586914 	 0.03346443176269531 	 None 	 None 	 None 	 None 	 
2025-07-24 21:31:54.827715 test begin: paddle.any(Tensor([512, 99226],"bool"), )
[Prof] paddle.any 	 paddle.any(Tensor([512, 99226],"bool"), ) 	 50803712 	 1000 	 0.05271768569946289 	 0.06469082832336426 	 0.026923179626464844 	 0.03304028511047363 	 None 	 None 	 None 	 None 	 
2025-07-24 21:31:55.640355 test begin: paddle.argmax(Tensor([15877, 100, 32],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([15877, 100, 32],"float32"), axis=1, ) 	 50806400 	 1000 	 0.7495179176330566 	 0.18319129943847656 	 0.7316360473632812 	 0.1624748706817627 	 None 	 None 	 None 	 None 	 
2025-07-24 21:31:57.530428 test begin: paddle.argmax(Tensor([29151, 100, 18],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([29151, 100, 18],"float32"), axis=1, ) 	 52471800 	 1000 	 0.6422231197357178 	 0.1753549575805664 	 0.6313636302947998 	 0.15723299980163574 	 None 	 None 	 None 	 None 	 
2025-07-24 21:31:59.311660 test begin: paddle.argmax(Tensor([29151, 28, 64],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([29151, 28, 64],"float32"), axis=1, ) 	 52238592 	 1000 	 1.5036509037017822 	 0.17847728729248047 	 1.4931445121765137 	 0.16137242317199707 	 None 	 None 	 None 	 None 	 
2025-07-24 21:32:01.825126 test begin: paddle.argmax(Tensor([29151, 55, 32],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([29151, 55, 32],"float32"), axis=1, ) 	 51305760 	 1000 	 0.7577207088470459 	 0.16886091232299805 	 0.7472872734069824 	 0.154465913772583 	 None 	 None 	 None 	 None 	 
2025-07-24 21:32:03.567649 test begin: paddle.argmax(Tensor([39691, 20, 64],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([39691, 20, 64],"float32"), axis=1, ) 	 50804480 	 1000 	 2.045407772064209 	 0.1805126667022705 	 2.0348806381225586 	 0.15516400337219238 	 None 	 None 	 None 	 None 	 
2025-07-24 21:32:06.658375 test begin: paddle.argmax(Tensor([7939, 100, 64],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([7939, 100, 64],"float32"), axis=1, ) 	 50809600 	 1000 	 0.7653567790985107 	 0.1810441017150879 	 0.7519166469573975 	 0.16709566116333008 	 None 	 None 	 None 	 None 	 
2025-07-24 21:32:08.418218 test begin: paddle.argmax(Tensor([80239, 10, 64],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([80239, 10, 64],"float32"), axis=1, ) 	 51352960 	 1000 	 4.132685899734497 	 0.19624638557434082 	 4.122111797332764 	 0.17521357536315918 	 None 	 None 	 None 	 None 	 
2025-07-24 21:32:13.563787 test begin: paddle.argmax(Tensor([80239, 20, 32],"float32"), axis=1, )
[Prof] paddle.argmax 	 paddle.argmax(Tensor([80239, 20, 32],"float32"), axis=1, ) 	 51352960 	 1000 	 2.06864857673645 	 0.17760729789733887 	 2.056853771209717 	 0.1634368896484375 	 None 	 None 	 None 	 None 	 
2025-07-24 21:32:16.656491 test begin: paddle.argmin(Tensor([104534, 3, 3, 3, 3, 3],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([104534, 3, 3, 3, 3, 3],"float64"), axis=0, ) 	 25401762 	 1000 	 0.47887301445007324 	 0.1608438491821289 	 0.46715831756591797 	 0.08214688301086426 	 None 	 None 	 None 	 None 	 
2025-07-24 21:32:17.817189 test begin: paddle.argmin(Tensor([203213, 5, 5, 5],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([203213, 5, 5, 5],"float64"), axis=0, ) 	 25401625 	 1000 	 0.48691844940185547 	 0.16269373893737793 	 0.4762299060821533 	 0.08312010765075684 	 None 	 None 	 None 	 None 	 
2025-07-24 21:32:18.986853 test begin: paddle.argmin(Tensor([3, 104534, 3, 3, 3, 3],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([3, 104534, 3, 3, 3, 3],"float64"), axis=0, ) 	 25401762 	 1000 	 6.811806678771973 	 0.20344758033752441 	 6.795820474624634 	 0.1882314682006836 	 None 	 None 	 None 	 None 	 
2025-07-24 21:32:26.577427 test begin: paddle.argmin(Tensor([3, 3, 104534, 3, 3, 3],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([3, 3, 104534, 3, 3, 3],"float64"), axis=0, ) 	 25401762 	 1000 	 6.80932879447937 	 0.20516681671142578 	 6.791370153427124 	 0.18090367317199707 	 None 	 None 	 None 	 None 	 
2025-07-24 21:32:34.139794 test begin: paddle.argmin(Tensor([3, 3, 3, 104534, 3, 3],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([3, 3, 3, 104534, 3, 3],"float64"), axis=0, ) 	 25401762 	 1000 	 6.8099915981292725 	 0.22254633903503418 	 6.791183948516846 	 0.1882002353668213 	 None 	 None 	 None 	 None 	 
2025-07-24 21:32:42.689475 test begin: paddle.argmin(Tensor([3, 3, 3, 3, 104534, 3],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([3, 3, 3, 3, 104534, 3],"float64"), axis=0, ) 	 25401762 	 1000 	 6.824746608734131 	 0.20360398292541504 	 6.79802393913269 	 0.18821978569030762 	 None 	 None 	 None 	 None 	 
2025-07-24 21:32:50.278912 test begin: paddle.argmin(Tensor([3, 3, 3, 3, 3, 104534],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([3, 3, 3, 3, 3, 104534],"float64"), axis=0, ) 	 25401762 	 1000 	 6.829688787460327 	 0.20346617698669434 	 6.799379825592041 	 0.18816447257995605 	 None 	 None 	 None 	 None 	 
2025-07-24 21:32:58.040127 test begin: paddle.argmin(Tensor([4, 4, 4, 4, 99226],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([4, 4, 4, 4, 99226],"float64"), axis=0, ) 	 25401856 	 1000 	 5.108428716659546 	 0.19151949882507324 	 5.097668647766113 	 0.16978979110717773 	 None 	 None 	 None 	 None 	 
2025-07-24 21:33:03.929441 test begin: paddle.argmin(Tensor([4, 4, 4, 99226, 4],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([4, 4, 4, 99226, 4],"float64"), axis=0, ) 	 25401856 	 1000 	 5.108130216598511 	 0.19132065773010254 	 5.097440719604492 	 0.17651104927062988 	 None 	 None 	 None 	 None 	 
2025-07-24 21:33:09.779997 test begin: paddle.argmin(Tensor([4, 4, 99226, 4, 4],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([4, 4, 99226, 4, 4],"float64"), axis=0, ) 	 25401856 	 1000 	 5.117347955703735 	 0.19133758544921875 	 5.097374200820923 	 0.1766831874847412 	 None 	 None 	 None 	 None 	 
2025-07-24 21:33:15.667863 test begin: paddle.argmin(Tensor([4, 99226, 4, 4, 4],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([4, 99226, 4, 4, 4],"float64"), axis=0, ) 	 25401856 	 1000 	 5.108391284942627 	 0.19133663177490234 	 5.093817710876465 	 0.1763613224029541 	 None 	 None 	 None 	 None 	 
2025-07-24 21:33:21.536684 test begin: paddle.argmin(Tensor([5, 203213, 5, 5],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([5, 203213, 5, 5],"float64"), axis=0, ) 	 25401625 	 1000 	 4.086438179016113 	 0.1960287094116211 	 4.075754165649414 	 0.18126463890075684 	 None 	 None 	 None 	 None 	 
2025-07-24 21:33:26.537142 test begin: paddle.argmin(Tensor([5, 5, 203213, 5],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([5, 5, 203213, 5],"float64"), axis=0, ) 	 25401625 	 1000 	 4.087821960449219 	 0.19603204727172852 	 4.077175617218018 	 0.18134546279907227 	 None 	 None 	 None 	 None 	 
2025-07-24 21:33:31.427516 test begin: paddle.argmin(Tensor([5, 5, 5, 203213],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([5, 5, 5, 203213],"float64"), axis=0, ) 	 25401625 	 1000 	 4.08870267868042 	 0.4590632915496826 	 4.075109004974365 	 0.18137168884277344 	 None 	 None 	 None 	 None 	 
2025-07-24 21:33:39.418782 test begin: paddle.argmin(Tensor([99226, 4, 4, 4, 4],"float64"), axis=0, )
[Prof] paddle.argmin 	 paddle.argmin(Tensor([99226, 4, 4, 4, 4],"float64"), axis=0, ) 	 25401856 	 1000 	 0.44136738777160645 	 0.3240342140197754 	 0.4306938648223877 	 0.16561532020568848 	 None 	 None 	 None 	 None 	 
2025-07-24 21:33:40.708682 test begin: paddle.argsort(Tensor([25401601],"float64"), stable=True, )
[Prof] paddle.argsort 	 paddle.argsort(Tensor([25401601],"float64"), stable=True, ) 	 25401601 	 1000 	 15.181365013122559 	 7.493643760681152 	 9.131431579589844e-05 	 0.3338038921356201 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:34:08.944443 test begin: paddle.argsort(Tensor([50803201],"float32"), stable=True, )
[Prof] paddle.argsort 	 paddle.argsort(Tensor([50803201],"float32"), stable=True, ) 	 50803201 	 1000 	 16.259145736694336 	 7.87869668006897 	 8.606910705566406e-05 	 0.5370795726776123 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:34:45.290835 test begin: paddle.argsort(Tensor([50803201],"int32"), stable=True, )
[Prof] paddle.argsort 	 paddle.argsort(Tensor([50803201],"int32"), stable=True, ) 	 50803201 	 1000 	 19.24541664123535 	 7.233880996704102 	 9.465217590332031e-05 	 0.4927244186401367 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:35:21.863503 test begin: paddle.as_complex(Tensor([32, 15, 207, 8, 32, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([32, 15, 207, 8, 32, 2],"float32"), ) 	 50872320 	 1000 	 0.0030519962310791016 	 0.0044481754302978516 	 1.0967254638671875e-05 	 1.9073486328125e-05 	 0.04222559928894043 	 0.060075998306274414 	 2.2172927856445312e-05 	 0.00014400482177734375 	 
2025-07-24 21:35:23.384948 test begin: paddle.as_complex(Tensor([32, 15, 8, 207, 32, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([32, 15, 8, 207, 32, 2],"float32"), ) 	 50872320 	 1000 	 0.003027200698852539 	 0.004486560821533203 	 8.58306884765625e-06 	 1.7881393432617188e-05 	 0.0387425422668457 	 0.06227588653564453 	 3.62396240234375e-05 	 8.893013000488281e-05 	 
2025-07-24 21:35:24.905571 test begin: paddle.as_complex(Tensor([32, 15, 8, 8, 827, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([32, 15, 8, 8, 827, 2],"float32"), ) 	 50810880 	 1000 	 0.003061532974243164 	 0.0044651031494140625 	 1.2874603271484375e-05 	 1.811981201171875e-05 	 0.03844928741455078 	 0.058679819107055664 	 2.47955322265625e-05 	 5.650520324707031e-05 	 
2025-07-24 21:35:26.378616 test begin: paddle.as_complex(Tensor([32, 388, 8, 8, 32, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([32, 388, 8, 8, 32, 2],"float32"), ) 	 50855936 	 1000 	 0.003075838088989258 	 0.0044400691986083984 	 7.867813110351562e-06 	 1.6689300537109375e-05 	 0.038605451583862305 	 0.05787253379821777 	 2.4557113647460938e-05 	 3.337860107421875e-05 	 
2025-07-24 21:35:27.816904 test begin: paddle.as_complex(Tensor([827, 15, 8, 8, 32, 2],"float32"), )
[Prof] paddle.as_complex 	 paddle.as_complex(Tensor([827, 15, 8, 8, 32, 2],"float32"), ) 	 50810880 	 1000 	 0.0030634403228759766 	 0.004413604736328125 	 6.4373016357421875e-06 	 1.71661376953125e-05 	 0.03862333297729492 	 0.0637047290802002 	 3.147125244140625e-05 	 3.7670135498046875e-05 	 
2025-07-24 21:35:29.276543 test begin: paddle.as_strided(Tensor([1587601, 32],"float32"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([1587601, 32],"float32"), shape=tuple(3,), stride=tuple(1,), ) 	 50803232 	 1000 	 0.017263412475585938 	 0.004774332046508789 	 1.0728836059570312e-05 	 4.315376281738281e-05 	 0.1486966609954834 	 0.13844990730285645 	 0.07589554786682129 	 0.056389808654785156 	 
2025-07-24 21:35:30.475863 test begin: paddle.as_strided(Tensor([3175201, 32],"float16"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([3175201, 32],"float16"), shape=tuple(3,), stride=tuple(1,), ) 	 101606432 	 1000 	 0.01711750030517578 	 0.004396200180053711 	 1.0013580322265625e-05 	 1.8835067749023438e-05 	 0.14619922637939453 	 0.13694381713867188 	 0.07454037666320801 	 0.05546307563781738 	 
2025-07-24 21:35:32.685168 test begin: paddle.as_strided(Tensor([3175201, 32],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([3175201, 32],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), ) 	 101606432 	 1000 	 0.017366647720336914 	 0.0045812129974365234 	 1.1920928955078125e-05 	 1.9311904907226562e-05 	 0.147705078125 	 0.13798880577087402 	 0.07607722282409668 	 0.0509340763092041 	 
2025-07-24 21:35:35.070868 test begin: paddle.as_strided(Tensor([32, 1587601],"float32"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([32, 1587601],"float32"), shape=tuple(3,), stride=tuple(1,), ) 	 50803232 	 1000 	 0.01737213134765625 	 0.004789113998413086 	 1.3589859008789062e-05 	 0.000133514404296875 	 0.14878177642822266 	 0.1372833251953125 	 0.07594776153564453 	 0.029325246810913086 	 
2025-07-24 21:35:39.801288 test begin: paddle.as_strided(Tensor([32, 3175201],"float16"), shape=tuple(3,), stride=tuple(1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([32, 3175201],"float16"), shape=tuple(3,), stride=tuple(1,), ) 	 101606432 	 1000 	 0.01737070083618164 	 0.0044286251068115234 	 1.0251998901367188e-05 	 1.8358230590820312e-05 	 0.14626717567443848 	 0.13693594932556152 	 0.07469844818115234 	 0.05647587776184082 	 
2025-07-24 21:35:42.307192 test begin: paddle.as_strided(Tensor([32, 3175201],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), )
[Prof] paddle.as_strided 	 paddle.as_strided(Tensor([32, 3175201],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), ) 	 101606432 	 1000 	 0.01741790771484375 	 0.0045850276947021484 	 1.0013580322265625e-05 	 2.4080276489257812e-05 	 0.14650535583496094 	 0.13801312446594238 	 0.07475638389587402 	 0.05681133270263672 	 
2025-07-24 21:35:44.863477 test begin: paddle.asin(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.asin 	 paddle.asin(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 1000 	 0.29563045501708984 	 0.297640323638916 	 0.2871088981628418 	 0.2866084575653076 	 0.45027756690979004 	 1.783470630645752 	 0.3974745273590088 	 0.3645341396331787 	 
2025-07-24 21:35:49.670356 test begin: paddle.asin(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.asin 	 paddle.asin(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 1000 	 0.29569053649902344 	 0.2976040840148926 	 0.2872185707092285 	 0.2864668369293213 	 0.4503207206726074 	 1.7832441329956055 	 0.39718008041381836 	 0.36451220512390137 	 
2025-07-24 21:35:54.459326 test begin: paddle.asin(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.asin 	 paddle.asin(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 1000 	 0.2957139015197754 	 1.7277004718780518 	 0.28713011741638184 	 0.28035974502563477 	 0.4503612518310547 	 1.7834851741790771 	 0.3968648910522461 	 0.3645317554473877 	 
2025-07-24 21:36:02.355158 test begin: paddle.asinh(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.asinh 	 paddle.asinh(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 1000 	 0.3012056350708008 	 0.30050206184387207 	 0.2856874465942383 	 0.2831995487213135 	 0.4503920078277588 	 1.3381783962249756 	 0.3871471881866455 	 0.34188032150268555 	 
2025-07-24 21:36:06.750911 test begin: paddle.asinh(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.asinh 	 paddle.asinh(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 1000 	 0.3041403293609619 	 0.30045413970947266 	 0.287106990814209 	 0.2827568054199219 	 0.45041966438293457 	 1.3381164073944092 	 0.3844115734100342 	 0.3419685363769531 	 
2025-07-24 21:36:11.078352 test begin: paddle.asinh(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.asinh 	 paddle.asinh(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 1000 	 0.30286669731140137 	 0.30034899711608887 	 0.28726935386657715 	 0.28306126594543457 	 0.4503037929534912 	 1.3381743431091309 	 0.38771867752075195 	 0.34192419052124023 	 
2025-07-24 21:36:15.435362 test begin: paddle.atan(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.atan 	 paddle.atan(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 1000 	 0.2992267608642578 	 0.29764652252197266 	 0.2905728816986084 	 0.2867100238800049 	 0.4502272605895996 	 1.043351173400879 	 0.3968641757965088 	 0.35544824600219727 	 
2025-07-24 21:36:19.543166 test begin: paddle.atan(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.atan 	 paddle.atan(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 1000 	 0.2977910041809082 	 0.29965782165527344 	 0.282167911529541 	 0.28047657012939453 	 0.4503934383392334 	 1.0434472560882568 	 0.38770365715026855 	 0.35549259185791016 	 
2025-07-24 21:36:23.335365 test begin: paddle.atan(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.atan 	 paddle.atan(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 1000 	 0.2979254722595215 	 0.3146474361419678 	 0.28220653533935547 	 0.281872034072876 	 0.4518778324127197 	 1.0434691905975342 	 0.3840031623840332 	 0.3554873466491699 	 
2025-07-24 21:36:27.147147 test begin: paddle.atan2(Tensor([100],"float64"), Tensor([254017, 100],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(Tensor([100],"float64"), Tensor([254017, 100],"float64"), ) 	 25401800 	 1000 	 0.8813681602478027 	 0.3143441677093506 	 0.3003115653991699 	 0.3019266128540039 	 1.6836276054382324 	 2.7453811168670654 	 0.34443235397338867 	 0.2550010681152344 	 
2025-07-24 21:36:33.909649 test begin: paddle.atan2(Tensor([111, 222, 1031],"float64"), Tensor([222, 1031],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(Tensor([111, 222, 1031],"float64"), Tensor([222, 1031],"float64"), ) 	 25634784 	 1000 	 0.8974301815032959 	 0.326859712600708 	 0.3003530502319336 	 0.2990837097167969 	 1.2295150756835938 	 2.990701913833618 	 0.3136463165283203 	 0.3053882122039795 	 
2025-07-24 21:36:43.139565 test begin: paddle.atan2(Tensor([111, 688, 333],"float64"), Tensor([688, 333],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(Tensor([111, 688, 333],"float64"), Tensor([688, 333],"float64"), ) 	 25659648 	 1000 	 0.8827943801879883 	 0.32284116744995117 	 0.3006770610809326 	 0.29910993576049805 	 1.2320585250854492 	 2.996957302093506 	 0.3144097328186035 	 0.3061189651489258 	 
2025-07-24 21:36:49.748584 test begin: paddle.atan2(Tensor([344, 222, 333],"float64"), Tensor([222, 333],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(Tensor([344, 222, 333],"float64"), Tensor([222, 333],"float64"), ) 	 25504470 	 1000 	 0.8817958831787109 	 0.3166351318359375 	 0.3011765480041504 	 0.2975747585296631 	 1.312654733657837 	 2.9876046180725098 	 0.3345458507537842 	 0.3052029609680176 	 
2025-07-24 21:36:56.353675 test begin: paddle.atan2(x=Tensor([19601, 6, 6, 6, 6],"float64"), y=Tensor([19601, 6, 6, 6, 6],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(x=Tensor([19601, 6, 6, 6, 6],"float64"), y=Tensor([19601, 6, 6, 6, 6],"float64"), ) 	 50805792 	 1000 	 0.4460577964782715 	 0.4537465572357178 	 0.42834925651550293 	 0.4418318271636963 	 0.732072114944458 	 3.4122281074523926 	 0.6587386131286621 	 0.3873894214630127 	 
2025-07-24 21:37:04.979578 test begin: paddle.atan2(x=Tensor([3, 39201, 6, 6, 6],"float64"), y=Tensor([3, 39201, 6, 6, 6],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(x=Tensor([3, 39201, 6, 6, 6],"float64"), y=Tensor([3, 39201, 6, 6, 6],"float64"), ) 	 50804496 	 1000 	 0.44826841354370117 	 0.4538536071777344 	 0.42938232421875 	 0.441051721572876 	 0.7314012050628662 	 3.4120287895202637 	 0.6696786880493164 	 0.3872830867767334 	 
2025-07-24 21:37:13.164153 test begin: paddle.atan2(x=Tensor([3, 6, 39201, 6, 6],"float64"), y=Tensor([3, 6, 39201, 6, 6],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(x=Tensor([3, 6, 39201, 6, 6],"float64"), y=Tensor([3, 6, 39201, 6, 6],"float64"), ) 	 50804496 	 1000 	 0.4493558406829834 	 0.45644330978393555 	 0.4338512420654297 	 0.44153904914855957 	 0.7314221858978271 	 3.4119081497192383 	 0.6722853183746338 	 0.3873410224914551 	 
2025-07-24 21:37:19.991595 test begin: paddle.atan2(x=Tensor([3, 6, 6, 39201, 6],"float64"), y=Tensor([3, 6, 6, 39201, 6],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(x=Tensor([3, 6, 6, 39201, 6],"float64"), y=Tensor([3, 6, 6, 39201, 6],"float64"), ) 	 50804496 	 1000 	 0.4455091953277588 	 0.45372962951660156 	 0.43533992767333984 	 0.44171977043151855 	 0.7328357696533203 	 3.4120028018951416 	 0.6746914386749268 	 0.3885374069213867 	 
2025-07-24 21:37:26.742841 test begin: paddle.atan2(x=Tensor([3, 6, 6, 6, 39201],"float64"), y=Tensor([3, 6, 6, 6, 39201],"float64"), )
[Prof] paddle.atan2 	 paddle.atan2(x=Tensor([3, 6, 6, 6, 39201],"float64"), y=Tensor([3, 6, 6, 6, 39201],"float64"), ) 	 50804496 	 1000 	 0.4459266662597656 	 0.45371174812316895 	 0.43587398529052734 	 0.4417989253997803 	 0.731013298034668 	 3.4137120246887207 	 0.6698765754699707 	 0.3873324394226074 	 
2025-07-24 21:37:33.386150 test begin: paddle.atanh(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.atanh 	 paddle.atanh(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 1000 	 0.29902029037475586 	 1.1250617504119873 	 0.28885817527770996 	 0.2870919704437256 	 0.4518003463745117 	 1.6248068809509277 	 0.3975536823272705 	 0.3319065570831299 	 
2025-07-24 21:37:41.113443 test begin: paddle.atanh(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.atanh 	 paddle.atanh(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 1000 	 0.29855823516845703 	 0.2982337474822998 	 0.2889113426208496 	 0.28717923164367676 	 0.450390100479126 	 1.6244235038757324 	 0.39093613624572754 	 0.3318312168121338 	 
2025-07-24 21:37:45.512528 test begin: paddle.atanh(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.atanh 	 paddle.atanh(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 1000 	 0.2976095676422119 	 0.2982673645019531 	 0.2818903923034668 	 0.28092169761657715 	 0.45038747787475586 	 1.624410629272461 	 0.38265085220336914 	 0.33182859420776367 	 
2025-07-24 21:37:49.858206 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2, 1058401],"float64"), ) 	 25401624 	 1000 	 0.0018849372863769531 	 0.008414268493652344 	 1.0728836059570312e-05 	 1.9311904907226562e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:37:50.398801 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 1058401],"float64"), ) 	 76204872 	 1000 	 0.002875089645385742 	 0.007184028625488281 	 1.2159347534179688e-05 	 1.811981201171875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:37:51.981001 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401864 	 1000 	 0.0028831958770751953 	 0.0072400569915771484 	 7.152557373046875e-06 	 1.811981201171875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:37:52.543351 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401864 	 1000 	 0.0029191970825195312 	 0.0075817108154296875 	 6.67572021484375e-06 	 3.910064697265625e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:37:53.087391 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 1058401],"float64"), ) 	 25401864 	 1000 	 0.002836465835571289 	 0.00720977783203125 	 7.152557373046875e-06 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:37:53.627519 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), ) 	 25401900 	 1000 	 0.0028569698333740234 	 0.007548332214355469 	 7.152557373046875e-06 	 2.1696090698242188e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:37:54.259409 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), ) 	 25401870 	 1000 	 0.002983570098876953 	 0.010148286819458008 	 9.5367431640625e-06 	 1.9788742065429688e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:37:54.836114 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), ) 	 25401880 	 1000 	 0.0030808448791503906 	 0.012529373168945312 	 8.821487426757812e-06 	 5.173683166503906e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:37:55.510557 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401900 	 1000 	 0.002838611602783203 	 0.00733494758605957 	 1.0013580322265625e-05 	 1.7881393432617188e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:37:56.062007 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401870 	 1000 	 0.0028836727142333984 	 0.00725865364074707 	 5.7220458984375e-06 	 2.0742416381835938e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:37:56.612669 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401880 	 1000 	 0.0028297901153564453 	 0.007478237152099609 	 1.0967254638671875e-05 	 3.170967102050781e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:37:57.153459 test begin: paddle.atleast_1d(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), ) 	 76204836 	 1000 	 0.002865314483642578 	 0.007310152053833008 	 5.4836273193359375e-06 	 1.7642974853515625e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:37:58.776422 test begin: paddle.atleast_1d(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401660 	 1000 	 0.0027604103088378906 	 0.007319211959838867 	 5.7220458984375e-06 	 1.9073486328125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:37:59.320129 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401660 	 1000 	 0.0028090476989746094 	 0.007314205169677734 	 6.198883056640625e-06 	 1.8358230590820312e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:37:59.867625 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2116801],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2116801],"float64"), ) 	 25401660 	 1000 	 0.002816915512084961 	 0.007377147674560547 	 5.7220458984375e-06 	 1.9073486328125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:00.405763 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4233601, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4233601, 2],"float64"), ) 	 25401654 	 1000 	 0.0027780532836914062 	 0.007242918014526367 	 5.9604644775390625e-06 	 1.7881393432617188e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:00.944545 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), ) 	 25401656 	 1000 	 0.002802133560180664 	 0.0073375701904296875 	 7.3909759521484375e-06 	 1.71661376953125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:01.492245 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4233601, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4233601, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401654 	 1000 	 0.005399942398071289 	 0.00719451904296875 	 1.621246337890625e-05 	 1.8358230590820312e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:02.051934 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401656 	 1000 	 0.0027964115142822266 	 0.007287740707397461 	 5.4836273193359375e-06 	 2.0503997802734375e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:02.598889 test begin: paddle.atleast_1d(Tensor([3, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 423361, 5],"float64"), ) 	 25401660 	 1000 	 0.0016150474548339844 	 0.006062030792236328 	 7.867813110351562e-06 	 1.7881393432617188e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:03.267544 test begin: paddle.atleast_1d(Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401900 	 1000 	 0.002808094024658203 	 0.0074117183685302734 	 5.9604644775390625e-06 	 1.7881393432617188e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:03.822402 test begin: paddle.atleast_1d(Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), ) 	 76204980 	 1000 	 0.0029134750366210938 	 0.00732111930847168 	 1.621246337890625e-05 	 1.9311904907226562e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:05.438242 test begin: paddle.atleast_1d(Tensor([3, 4233601, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4233601, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401654 	 1000 	 0.002840757369995117 	 0.007352113723754883 	 1.1205673217773438e-05 	 1.8835067749023438e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:05.979073 test begin: paddle.atleast_1d(Tensor([3, 4233601, 2],"float64"), Tensor([3, 4233601, 2],"float64"), Tensor([3, 4233601, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 4233601, 2],"float64"), Tensor([3, 4233601, 2],"float64"), Tensor([3, 4233601, 2],"float64"), ) 	 76204818 	 1000 	 0.0028333663940429688 	 0.007271766662597656 	 1.2159347534179688e-05 	 1.811981201171875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:09.515777 test begin: paddle.atleast_1d(Tensor([3, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 846721, 2, 5],"float64"), ) 	 25401630 	 1000 	 0.0016207695007324219 	 0.006113767623901367 	 1.2159347534179688e-05 	 1.6927719116210938e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:10.083194 test begin: paddle.atleast_1d(Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401870 	 1000 	 0.002839803695678711 	 0.008002042770385742 	 5.4836273193359375e-06 	 4.863739013671875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:11.977931 test begin: paddle.atleast_1d(Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), ) 	 76204890 	 1000 	 0.002911090850830078 	 0.007409572601318359 	 1.7404556274414062e-05 	 3.266334533691406e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:13.637881 test begin: paddle.atleast_1d(Tensor([3175201, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3175201, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401656 	 1000 	 0.0028235912322998047 	 0.007420778274536133 	 1.1920928955078125e-05 	 1.9788742065429688e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:14.186580 test begin: paddle.atleast_1d(Tensor([3175201, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([3175201, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), ) 	 76204824 	 1000 	 0.0028455257415771484 	 0.007364988327026367 	 7.867813110351562e-06 	 3.457069396972656e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:15.808670 test begin: paddle.atleast_1d(Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([635041, 4, 2, 5],"float64"), ) 	 25401640 	 1000 	 0.0016551017761230469 	 0.009435415267944336 	 7.62939453125e-06 	 4.363059997558594e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:16.371281 test begin: paddle.atleast_1d(Tensor([635041, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([635041, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401880 	 1000 	 0.0028841495513916016 	 0.0075855255126953125 	 5.7220458984375e-06 	 2.4318695068359375e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:16.983678 test begin: paddle.atleast_1d(Tensor([635041, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_1d 	 paddle.atleast_1d(Tensor([635041, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), ) 	 76204920 	 1000 	 0.0028645992279052734 	 0.00725245475769043 	 6.198883056640625e-06 	 1.6927719116210938e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:18.632806 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 1058401],"float64"), ) 	 25401624 	 1000 	 0.001990795135498047 	 0.006570100784301758 	 5.9604644775390625e-06 	 3.409385681152344e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:19.174066 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 1058401],"float64"), ) 	 76204872 	 1000 	 0.003815174102783203 	 0.007492542266845703 	 6.198883056640625e-06 	 2.5272369384765625e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:20.828537 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401864 	 1000 	 0.0037994384765625 	 0.007393598556518555 	 7.867813110351562e-06 	 2.0742416381835938e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:21.504236 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401864 	 1000 	 0.0038208961486816406 	 0.0072100162506103516 	 6.198883056640625e-06 	 1.71661376953125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:22.070869 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 1058401],"float64"), ) 	 25401864 	 1000 	 0.003777742385864258 	 0.0077550411224365234 	 1.7881393432617188e-05 	 3.4809112548828125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:22.636226 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), ) 	 25401900 	 1000 	 0.004140615463256836 	 0.007801055908203125 	 2.3603439331054688e-05 	 4.267692565917969e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:23.181226 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), ) 	 25401870 	 1000 	 0.004088640213012695 	 0.007269144058227539 	 1.8835067749023438e-05 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:23.729043 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), ) 	 25401880 	 1000 	 0.003725767135620117 	 0.007203578948974609 	 1.4066696166992188e-05 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:24.268018 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401900 	 1000 	 0.007359504699707031 	 0.007395267486572266 	 9.298324584960938e-06 	 2.7179718017578125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:24.827013 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401870 	 1000 	 0.0038635730743408203 	 0.007289409637451172 	 7.62939453125e-06 	 1.8835067749023438e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:25.374910 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401880 	 1000 	 0.0037374496459960938 	 0.0072269439697265625 	 5.9604644775390625e-06 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:26.016176 test begin: paddle.atleast_2d(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), ) 	 76204836 	 1000 	 0.003794431686401367 	 0.007244586944580078 	 5.4836273193359375e-06 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:27.768047 test begin: paddle.atleast_2d(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401660 	 1000 	 0.003782987594604492 	 0.007544040679931641 	 7.3909759521484375e-06 	 2.2172927856445312e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:28.315581 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401660 	 1000 	 0.003735065460205078 	 0.007266521453857422 	 7.3909759521484375e-06 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:28.855876 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2116801],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2116801],"float64"), ) 	 25401660 	 1000 	 0.003686189651489258 	 0.0071756839752197266 	 6.198883056640625e-06 	 1.7642974853515625e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:29.422001 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4233601, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4233601, 2],"float64"), ) 	 25401654 	 1000 	 0.0039026737213134766 	 0.01030874252319336 	 0.00014090538024902344 	 3.933906555175781e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:29.967864 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), ) 	 25401656 	 1000 	 0.003764629364013672 	 0.007239103317260742 	 6.198883056640625e-06 	 1.7642974853515625e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:30.506104 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4233601, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4233601, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401654 	 1000 	 0.0037238597869873047 	 0.00720524787902832 	 6.4373016357421875e-06 	 1.811981201171875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:31.051662 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401656 	 1000 	 0.003754138946533203 	 0.007208108901977539 	 7.152557373046875e-06 	 2.4318695068359375e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:31.568810 test begin: paddle.atleast_2d(Tensor([3, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 423361, 5],"float64"), ) 	 25401660 	 1000 	 0.001924753189086914 	 0.006164073944091797 	 6.198883056640625e-06 	 1.6927719116210938e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:32.101193 test begin: paddle.atleast_2d(Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401900 	 1000 	 0.003813505172729492 	 0.007481575012207031 	 7.3909759521484375e-06 	 1.8835067749023438e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:32.640143 test begin: paddle.atleast_2d(Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), ) 	 76204980 	 1000 	 0.003782510757446289 	 0.0074005126953125 	 7.3909759521484375e-06 	 1.9073486328125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:34.279249 test begin: paddle.atleast_2d(Tensor([3, 4233601, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4233601, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401654 	 1000 	 0.0037097930908203125 	 0.0072476863861083984 	 5.7220458984375e-06 	 1.9788742065429688e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:34.837853 test begin: paddle.atleast_2d(Tensor([3, 4233601, 2],"float64"), Tensor([3, 4233601, 2],"float64"), Tensor([3, 4233601, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 4233601, 2],"float64"), Tensor([3, 4233601, 2],"float64"), Tensor([3, 4233601, 2],"float64"), ) 	 76204818 	 1000 	 0.003989219665527344 	 0.010208368301391602 	 1.5497207641601562e-05 	 2.09808349609375e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:39.283484 test begin: paddle.atleast_2d(Tensor([3, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 846721, 2, 5],"float64"), ) 	 25401630 	 1000 	 0.0019664764404296875 	 0.006061077117919922 	 1.8835067749023438e-05 	 1.7642974853515625e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:39.929498 test begin: paddle.atleast_2d(Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401870 	 1000 	 0.003744840621948242 	 0.007407188415527344 	 5.4836273193359375e-06 	 1.8835067749023438e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:40.609594 test begin: paddle.atleast_2d(Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), ) 	 76204890 	 1000 	 0.005342721939086914 	 0.007340908050537109 	 2.574920654296875e-05 	 3.409385681152344e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:42.293152 test begin: paddle.atleast_2d(Tensor([3175201, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3175201, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401656 	 1000 	 0.0038299560546875 	 0.007319927215576172 	 1.1444091796875e-05 	 3.0994415283203125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:42.834534 test begin: paddle.atleast_2d(Tensor([3175201, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([3175201, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), ) 	 76204824 	 1000 	 0.0037660598754882812 	 0.00727081298828125 	 1.4543533325195312e-05 	 1.7881393432617188e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:44.579731 test begin: paddle.atleast_2d(Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([635041, 4, 2, 5],"float64"), ) 	 25401640 	 1000 	 0.001988649368286133 	 0.006349325180053711 	 6.198883056640625e-06 	 3.337860107421875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:45.238043 test begin: paddle.atleast_2d(Tensor([635041, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([635041, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401880 	 1000 	 0.0037848949432373047 	 0.007481813430786133 	 7.62939453125e-06 	 2.0265579223632812e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:45.776362 test begin: paddle.atleast_2d(Tensor([635041, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_2d 	 paddle.atleast_2d(Tensor([635041, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), ) 	 76204920 	 1000 	 0.003902435302734375 	 0.007232666015625 	 1.8596649169921875e-05 	 2.2649765014648438e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:47.421444 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 1058401],"float64"), ) 	 25401624 	 1000 	 0.0023109912872314453 	 0.006205558776855469 	 7.62939453125e-06 	 1.811981201171875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:48.015551 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 1058401],"float64"), ) 	 76204872 	 1000 	 0.004755258560180664 	 0.007218360900878906 	 6.198883056640625e-06 	 1.7642974853515625e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:49.788933 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401864 	 1000 	 0.004719257354736328 	 0.007090330123901367 	 5.9604644775390625e-06 	 1.811981201171875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:50.350237 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 1058401],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401864 	 1000 	 0.004723787307739258 	 0.0072133541107177734 	 7.152557373046875e-06 	 1.9311904907226562e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:50.902854 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 1058401],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 1058401],"float64"), ) 	 25401864 	 1000 	 0.005468130111694336 	 0.007906675338745117 	 2.0742416381835938e-05 	 4.482269287109375e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:51.452826 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), ) 	 25401900 	 1000 	 0.004698038101196289 	 0.0073089599609375 	 1.6927719116210938e-05 	 1.8358230590820312e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:52.003504 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), ) 	 25401870 	 1000 	 0.0047113895416259766 	 0.00728154182434082 	 5.9604644775390625e-06 	 1.811981201171875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:52.555225 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), ) 	 25401880 	 1000 	 0.007191658020019531 	 0.007869243621826172 	 2.574920654296875e-05 	 4.267692565917969e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:53.183625 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401900 	 1000 	 0.0047647953033447266 	 0.007233619689941406 	 2.0503997802734375e-05 	 1.9311904907226562e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:53.737952 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401870 	 1000 	 0.006133556365966797 	 0.008308649063110352 	 2.193450927734375e-05 	 5.6743621826171875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:54.315471 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401880 	 1000 	 0.004825592041015625 	 0.007313251495361328 	 1.430511474609375e-05 	 1.7881393432617188e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:54.870183 test begin: paddle.atleast_3d(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), ) 	 76204836 	 1000 	 0.004958629608154297 	 0.007293224334716797 	 2.09808349609375e-05 	 2.1696090698242188e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:56.497762 test begin: paddle.atleast_3d(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401660 	 1000 	 0.0046465396881103516 	 0.007193088531494141 	 5.7220458984375e-06 	 1.8358230590820312e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:57.046731 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401660 	 1000 	 0.0062351226806640625 	 0.007310390472412109 	 3.1948089599609375e-05 	 1.811981201171875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:57.616248 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2116801],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2116801],"float64"), ) 	 25401660 	 1000 	 0.004851579666137695 	 0.00718998908996582 	 9.059906005859375e-06 	 2.0742416381835938e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:58.161550 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4233601, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4233601, 2],"float64"), ) 	 25401654 	 1000 	 0.004867076873779297 	 0.010237693786621094 	 7.867813110351562e-06 	 2.1696090698242188e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:58.724358 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), ) 	 25401656 	 1000 	 0.004667520523071289 	 0.007444143295288086 	 5.9604644775390625e-06 	 3.170967102050781e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:59.269425 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4233601, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4233601, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401654 	 1000 	 0.004688739776611328 	 0.007256269454956055 	 6.198883056640625e-06 	 1.9788742065429688e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:38:59.811094 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401656 	 1000 	 0.004695892333984375 	 0.007219552993774414 	 6.4373016357421875e-06 	 1.8835067749023438e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:00.355713 test begin: paddle.atleast_3d(Tensor([3, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 423361, 5],"float64"), ) 	 25401660 	 1000 	 0.0030655860900878906 	 0.010557174682617188 	 2.5987625122070312e-05 	 6.580352783203125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:00.901267 test begin: paddle.atleast_3d(Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401900 	 1000 	 0.004728078842163086 	 0.007531404495239258 	 7.3909759521484375e-06 	 2.002716064453125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:01.438583 test begin: paddle.atleast_3d(Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), Tensor([3, 4, 423361, 5],"float64"), ) 	 76204980 	 1000 	 0.004776716232299805 	 0.007250785827636719 	 8.58306884765625e-06 	 1.9073486328125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:03.202015 test begin: paddle.atleast_3d(Tensor([3, 4233601, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4233601, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401654 	 1000 	 0.00479888916015625 	 0.01036524772644043 	 8.344650268554688e-06 	 1.9788742065429688e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:03.789933 test begin: paddle.atleast_3d(Tensor([3, 4233601, 2],"float64"), Tensor([3, 4233601, 2],"float64"), Tensor([3, 4233601, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 4233601, 2],"float64"), Tensor([3, 4233601, 2],"float64"), Tensor([3, 4233601, 2],"float64"), ) 	 76204818 	 1000 	 0.0054683685302734375 	 0.007205009460449219 	 2.0265579223632812e-05 	 1.7881393432617188e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:05.415116 test begin: paddle.atleast_3d(Tensor([3, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 846721, 2, 5],"float64"), ) 	 25401630 	 1000 	 0.002272367477416992 	 0.006191730499267578 	 6.4373016357421875e-06 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:05.974473 test begin: paddle.atleast_3d(Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401870 	 1000 	 0.004755258560180664 	 0.007413148880004883 	 7.3909759521484375e-06 	 1.811981201171875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:06.520418 test begin: paddle.atleast_3d(Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), Tensor([3, 846721, 2, 5],"float64"), ) 	 76204890 	 1000 	 0.004747152328491211 	 0.007921934127807617 	 1.1920928955078125e-05 	 4.1484832763671875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:08.123755 test begin: paddle.atleast_3d(Tensor([3175201, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3175201, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), ) 	 25401656 	 1000 	 0.004642009735107422 	 0.007486104965209961 	 6.9141387939453125e-06 	 3.24249267578125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:08.688414 test begin: paddle.atleast_3d(Tensor([3175201, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([3175201, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), Tensor([3175201, 4, 2],"float64"), ) 	 76204824 	 1000 	 0.0046961307525634766 	 0.007394075393676758 	 1.7642974853515625e-05 	 1.811981201171875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:10.660117 test begin: paddle.atleast_3d(Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([635041, 4, 2, 5],"float64"), ) 	 25401640 	 1000 	 0.0022330284118652344 	 0.006185293197631836 	 1.0013580322265625e-05 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:11.249666 test begin: paddle.atleast_3d(Tensor([635041, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([635041, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), ) 	 25401880 	 1000 	 0.0047490596771240234 	 0.007345438003540039 	 8.821487426757812e-06 	 1.8358230590820312e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:11.802246 test begin: paddle.atleast_3d(Tensor([635041, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), )
[Prof] paddle.atleast_3d 	 paddle.atleast_3d(Tensor([635041, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), Tensor([635041, 4, 2, 5],"float64"), ) 	 76204920 	 1000 	 0.004752635955810547 	 0.007372379302978516 	 2.002716064453125e-05 	 1.9311904907226562e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:14.859183 test begin: paddle.bincount(Tensor([25401601],"int64"), minlength=Tensor([1],"int32"), )
[Prof] paddle.bincount 	 paddle.bincount(Tensor([25401601],"int64"), minlength=Tensor([1],"int32"), ) 	 25401602 	 1000 	 0.9743611812591553 	 0.8348245620727539 	 0.0004858970642089844 	 0.00047898292541503906 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:18.090377 test begin: paddle.bincount(Tensor([50803201],"int32"), weights=Tensor([50803201],"float32"), )
[Prof] paddle.bincount 	 paddle.bincount(Tensor([50803201],"int32"), weights=Tensor([50803201],"float32"), ) 	 101606402 	 1000 	 1.8169066905975342 	 1.4376146793365479 	 0.0010700225830078125 	 0.0010726451873779297 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:22.810321 test begin: paddle.bincount(x=Tensor([50803201],"int32"), weights=Tensor([50803201],"int32"), )
[Prof] paddle.bincount 	 paddle.bincount(x=Tensor([50803201],"int32"), weights=Tensor([50803201],"int32"), ) 	 101606402 	 1000 	 1.7083170413970947 	 1.8457045555114746 	 0.0009665489196777344 	 0.0009694099426269531 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:27.734305 test begin: paddle.bitwise_and(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 1000 	 0.4498598575592041 	 0.45036792755126953 	 0.4410889148712158 	 0.43815159797668457 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:30.700978 test begin: paddle.bitwise_and(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 1000 	 0.44969868659973145 	 0.450408935546875 	 0.43445611000061035 	 0.4320528507232666 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:33.591493 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), ) 	 203214240 	 1000 	 0.6790802478790283 	 0.4649062156677246 	 0.4335446357727051 	 0.43170881271362305 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:40.032991 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), ) 	 203213880 	 1000 	 0.4490833282470703 	 0.45034170150756836 	 0.4404449462890625 	 0.4383533000946045 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:42.950694 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), ) 	 101607264 	 1000 	 0.1187598705291748 	 0.11841106414794922 	 0.11012911796569824 	 0.10621285438537598 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:44.663579 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), ) 	 101607264 	 1000 	 0.45051026344299316 	 0.44683265686035156 	 0.44190168380737305 	 0.4346737861633301 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:46.745418 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), ) 	 203213664 	 1000 	 0.449786901473999 	 0.45037841796875 	 0.4346120357513428 	 0.4291999340057373 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:49.762173 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 50807520 	 1000 	 0.17797589302062988 	 0.2273118495941162 	 0.16820335388183594 	 0.21358776092529297 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:50.891288 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 101610720 	 1000 	 0.3146090507507324 	 0.4794788360595703 	 0.3051273822784424 	 0.46609950065612793 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:52.689532 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 50807520 	 1000 	 0.3113081455230713 	 0.31134796142578125 	 0.28821563720703125 	 0.2942652702331543 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:54.000820 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), ) 	 101608560 	 1000 	 0.11874723434448242 	 0.1161186695098877 	 0.10990309715270996 	 0.10383868217468262 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:55.679461 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), ) 	 101608560 	 1000 	 0.45052218437194824 	 0.4468996524810791 	 0.44190335273742676 	 0.4346787929534912 	 None 	 None 	 None 	 None 	 
2025-07-24 21:39:57.964186 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), ) 	 203214960 	 1000 	 0.4497339725494385 	 0.45035839080810547 	 0.44011878967285156 	 0.4382805824279785 	 None 	 None 	 None 	 None 	 
2025-07-24 21:40:00.879863 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 50807520 	 1000 	 0.18470501899719238 	 0.2271254062652588 	 0.17501258850097656 	 0.21340560913085938 	 None 	 None 	 None 	 None 	 
2025-07-24 21:40:02.059298 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 101610720 	 1000 	 0.11867880821228027 	 0.11626720428466797 	 0.10981512069702148 	 0.10400843620300293 	 None 	 None 	 None 	 None 	 
2025-07-24 21:40:03.710347 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50807520 	 1000 	 0.2969093322753906 	 0.3078896999359131 	 0.2866511344909668 	 0.2943284511566162 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:27.254161 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
W0724 15:40:28.480830 107959 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 101610720 	 1000 	 0.4505035877227783 	 0.44787096977233887 	 0.4347724914550781 	 0.42778587341308594 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:29.683230 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101610720 	 1000 	 0.34585070610046387 	 0.4788384437561035 	 0.3358802795410156 	 0.46586155891418457 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:31.512816 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 203217120 	 1000 	 0.4492337703704834 	 0.45095229148864746 	 0.44057798385620117 	 0.4385707378387451 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:34.528087 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), ) 	 101607480 	 1000 	 0.1183164119720459 	 0.12343215942382812 	 0.10979747772216797 	 0.10318922996520996 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:37.083419 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), ) 	 101607480 	 1000 	 0.6748151779174805 	 0.4499359130859375 	 0.44170331954956055 	 0.42952513694763184 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:40.310523 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), ) 	 101607840 	 1000 	 0.1185145378112793 	 0.1166219711303711 	 0.1094825267791748 	 0.10338091850280762 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:42.057435 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), ) 	 101607840 	 1000 	 0.45498013496398926 	 0.4467945098876953 	 0.4419088363647461 	 0.4351801872253418 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:44.195074 test begin: paddle.bitwise_and(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 1000 	 0.11850214004516602 	 0.11538910865783691 	 0.1100473403930664 	 0.10354256629943848 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:45.929942 test begin: paddle.bitwise_and(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 1000 	 0.45175862312316895 	 0.4466848373413086 	 0.4418752193450928 	 0.43489813804626465 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:48.049412 test begin: paddle.bitwise_and(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 1000 	 0.11846184730529785 	 0.11538219451904297 	 0.11008954048156738 	 0.10344958305358887 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:49.704929 test begin: paddle.bitwise_and(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 1000 	 0.45056676864624023 	 0.4467189311981201 	 0.4418637752532959 	 0.4351317882537842 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:51.938552 test begin: paddle.bitwise_and(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101608560 	 1000 	 0.11879801750183105 	 0.11613225936889648 	 0.10315418243408203 	 0.0979301929473877 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:53.723391 test begin: paddle.bitwise_and(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101608560 	 1000 	 0.4506850242614746 	 0.44687509536743164 	 0.4350559711456299 	 0.42423009872436523 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:55.897704 test begin: paddle.bitwise_and(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_and 	 paddle.bitwise_and(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214960 	 1000 	 0.449021577835083 	 0.4502906799316406 	 0.43356943130493164 	 0.4322085380554199 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:58.882482 test begin: paddle.bitwise_invert(Tensor([12700801, 4, 1],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([12700801, 4, 1],"int32"), ) 	 50803204 	 1000 	 0.29663658142089844 	 0.2980358600616455 	 0.2880260944366455 	 0.2867586612701416 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:00.117943 test begin: paddle.bitwise_invert(Tensor([2, 1270081, 4, 5],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([2, 1270081, 4, 5],"int32"), ) 	 50803240 	 1000 	 0.29621434211730957 	 0.29791688919067383 	 0.28800535202026367 	 0.2862081527709961 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:01.340315 test begin: paddle.bitwise_invert(Tensor([2, 3, 1693441, 5],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([2, 3, 1693441, 5],"int32"), ) 	 50803230 	 1000 	 0.2980525493621826 	 0.29791784286499023 	 0.284259557723999 	 0.2858304977416992 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:02.548588 test begin: paddle.bitwise_invert(Tensor([2, 3, 4, 2116801],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([2, 3, 4, 2116801],"int32"), ) 	 50803224 	 1000 	 0.2962367534637451 	 0.2979872226715088 	 0.28777527809143066 	 0.28664636611938477 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:03.729206 test begin: paddle.bitwise_invert(Tensor([3, 16934401, 1],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([3, 16934401, 1],"int32"), ) 	 50803203 	 1000 	 0.29624485969543457 	 0.2979264259338379 	 0.288177490234375 	 0.2867257595062256 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:06.283073 test begin: paddle.bitwise_invert(Tensor([3, 4, 4233601],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([3, 4, 4233601],"int32"), ) 	 50803212 	 1000 	 0.29737329483032227 	 0.7442038059234619 	 0.2881641387939453 	 0.286517858505249 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:09.070466 test begin: paddle.bitwise_invert(Tensor([846721, 3, 4, 5],"int32"), )
[Prof] paddle.bitwise_invert 	 paddle.bitwise_invert(Tensor([846721, 3, 4, 5],"int32"), ) 	 50803260 	 1000 	 0.9609253406524658 	 1.1829044818878174 	 0.2808549404144287 	 0.2781527042388916 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:13.641004 test begin: paddle.bitwise_left_shift(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), ) 	 101607000 	 1000 	 0.4507167339324951 	 0.44680356979370117 	 0.4414083957672119 	 0.4351308345794678 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:15.697987 test begin: paddle.bitwise_left_shift(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), ) 	 101606800 	 1000 	 0.4505190849304199 	 0.4467658996582031 	 0.43467259407043457 	 0.4288465976715088 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:17.836309 test begin: paddle.bitwise_left_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), ) 	 203213200 	 1000 	 0.4492788314819336 	 0.4502429962158203 	 0.43868517875671387 	 0.4322352409362793 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:21.210191 test begin: paddle.bitwise_left_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), False, ) 	 203213200 	 1000 	 0.4484426975250244 	 0.4501008987426758 	 0.4392261505126953 	 0.437335729598999 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:24.147748 test begin: paddle.bitwise_left_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), ) 	 203213400 	 1000 	 0.448807954788208 	 0.45221900939941406 	 0.439591646194458 	 0.43845415115356445 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:27.082673 test begin: paddle.bitwise_left_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, )
[Prof] paddle.bitwise_left_shift 	 paddle.bitwise_left_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), False, ) 	 203213400 	 1000 	 0.44846487045288086 	 0.45171427726745605 	 0.43935608863830566 	 0.4393622875213623 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:30.059161 test begin: paddle.bitwise_not(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101607120 	 1000 	 0.2987210750579834 	 0.29624104499816895 	 0.2905855178833008 	 0.2847473621368408 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:31.706640 test begin: paddle.bitwise_not(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), ) 	 101607120 	 1000 	 0.2980318069458008 	 0.2991909980773926 	 0.2903873920440674 	 0.2847254276275635 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:33.339933 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), ) 	 101607120 	 1000 	 0.3015913963317871 	 0.2989339828491211 	 0.28995323181152344 	 0.2844059467315674 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:34.981837 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), ) 	 101606940 	 1000 	 0.7473902702331543 	 1.1893417835235596 	 0.2903859615325928 	 0.284346342086792 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:40.107486 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), ) 	 50803632 	 1000 	 0.0895378589630127 	 0.0853574275970459 	 0.07471084594726562 	 0.06176614761352539 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:41.031233 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), ) 	 50803632 	 1000 	 0.2977287769317627 	 0.30267953872680664 	 0.28845787048339844 	 0.28619909286499023 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:42.249854 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), ) 	 101606832 	 1000 	 0.29852771759033203 	 0.2962629795074463 	 0.2891867160797119 	 0.2847912311553955 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:43.849120 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), ) 	 50804280 	 1000 	 0.08855986595153809 	 0.0798947811126709 	 0.08059501647949219 	 0.06818914413452148 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:44.717891 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), ) 	 50804280 	 1000 	 0.2968580722808838 	 0.2986421585083008 	 0.2885003089904785 	 0.28620123863220215 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:45.916559 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), ) 	 101607480 	 1000 	 0.298403263092041 	 0.2962977886199951 	 0.2901008129119873 	 0.28484630584716797 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:47.533368 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 50805360 	 1000 	 0.08981919288635254 	 0.07993173599243164 	 0.08200693130493164 	 0.06839537620544434 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:48.407971 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 50805360 	 1000 	 0.2996091842651367 	 0.2986891269683838 	 0.2884707450866699 	 0.28621339797973633 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:49.607577 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 101608560 	 1000 	 0.29848623275756836 	 0.2991006374359131 	 0.2906620502471924 	 0.28481388092041016 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:51.266190 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), ) 	 50803740 	 1000 	 0.08968806266784668 	 0.07923173904418945 	 0.08195376396179199 	 0.06771707534790039 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:52.143199 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), ) 	 50803740 	 1000 	 0.29613423347473145 	 0.2979550361633301 	 0.28806018829345703 	 0.2864720821380615 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:53.329437 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), ) 	 50803920 	 1000 	 0.08968186378479004 	 0.0796806812286377 	 0.08172297477722168 	 0.06723666191101074 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:54.235228 test begin: paddle.bitwise_not(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), ) 	 50803920 	 1000 	 0.29592275619506836 	 0.2996702194213867 	 0.28798842430114746 	 0.2863433361053467 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:55.421243 test begin: paddle.bitwise_not(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), ) 	 50803920 	 1000 	 0.08969426155090332 	 0.08738183975219727 	 0.08182191848754883 	 0.0675511360168457 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:56.331572 test begin: paddle.bitwise_not(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), ) 	 50803920 	 1000 	 0.29592204093933105 	 0.2978665828704834 	 0.28804850578308105 	 0.28623104095458984 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:57.518215 test begin: paddle.bitwise_not(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), ) 	 50803920 	 1000 	 0.08952474594116211 	 0.07926011085510254 	 0.0816802978515625 	 0.0675051212310791 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:58.389742 test begin: paddle.bitwise_not(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50803920 	 1000 	 0.297039270401001 	 0.2979722023010254 	 0.287930965423584 	 0.28649115562438965 	 None 	 None 	 None 	 None 	 
2025-07-24 15:41:59.569829 test begin: paddle.bitwise_not(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 50804280 	 1000 	 0.08961701393127441 	 0.08039593696594238 	 0.08167409896850586 	 0.06649541854858398 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:00.449039 test begin: paddle.bitwise_not(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50804280 	 1000 	 0.2960066795349121 	 0.29788875579833984 	 0.28806066513061523 	 0.2864353656768799 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:01.632698 test begin: paddle.bitwise_not(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_not 	 paddle.bitwise_not(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101607480 	 1000 	 0.2984657287597656 	 0.2974720001220703 	 0.29065752029418945 	 0.2846865653991699 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:03.234132 test begin: paddle.bitwise_or(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 1000 	 0.44921016693115234 	 0.4503448009490967 	 0.44049549102783203 	 0.43856000900268555 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:06.168279 test begin: paddle.bitwise_or(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 1000 	 0.4489419460296631 	 0.4530060291290283 	 0.44004106521606445 	 0.43846678733825684 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:09.058100 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), ) 	 203214240 	 1000 	 0.449876070022583 	 0.6802215576171875 	 0.44017553329467773 	 0.4319164752960205 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:13.876553 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), ) 	 203213880 	 1000 	 0.45320844650268555 	 0.4502525329589844 	 0.43997716903686523 	 0.4384286403656006 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:17.512944 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), ) 	 101607264 	 1000 	 0.11970853805541992 	 0.11693930625915527 	 0.10952281951904297 	 0.10513687133789062 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:19.158292 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), ) 	 101607264 	 1000 	 0.45068955421447754 	 0.44681334495544434 	 0.44185614585876465 	 0.43528079986572266 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:21.244774 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), ) 	 203213664 	 1000 	 0.44880175590515137 	 0.45040297508239746 	 0.44012975692749023 	 0.4383871555328369 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:24.211644 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 50807520 	 1000 	 0.15580487251281738 	 0.22740888595581055 	 0.1463327407836914 	 0.21442770957946777 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:25.331320 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 101610720 	 1000 	 0.31502437591552734 	 0.48272705078125 	 0.29833102226257324 	 0.4529552459716797 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:27.142693 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 50807520 	 1000 	 0.2966475486755371 	 0.3082122802734375 	 0.2869093418121338 	 0.2955334186553955 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:28.337252 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), ) 	 101608560 	 1000 	 0.11840081214904785 	 0.11804699897766113 	 0.10945606231689453 	 0.1039578914642334 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:29.975935 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), ) 	 101608560 	 1000 	 0.4505934715270996 	 0.4467437267303467 	 0.4419538974761963 	 0.4352564811706543 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:32.069761 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), ) 	 203214960 	 1000 	 0.4523468017578125 	 0.45277953147888184 	 0.44037652015686035 	 0.43825626373291016 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:35.055947 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 50807520 	 1000 	 0.17367935180664062 	 0.23173284530639648 	 0.16416525840759277 	 0.21360182762145996 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:39.518809 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 101610720 	 1000 	 0.11887645721435547 	 0.11596107482910156 	 0.10933041572570801 	 0.10414481163024902 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:41.184792 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50807520 	 1000 	 0.2962000370025635 	 0.3077380657196045 	 0.28655266761779785 	 0.29513978958129883 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:42.404325 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 101610720 	 1000 	 0.45059657096862793 	 0.4469015598297119 	 0.4393022060394287 	 0.4352107048034668 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:44.458042 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101610720 	 1000 	 0.3459634780883789 	 0.47876691818237305 	 0.3313436508178711 	 0.4659745693206787 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:46.321324 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 203217120 	 1000 	 0.44894862174987793 	 0.4513556957244873 	 0.44027113914489746 	 0.4384801387786865 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:49.226992 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), ) 	 101607480 	 1000 	 0.11774420738220215 	 0.11752533912658691 	 0.10921454429626465 	 0.1030740737915039 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:50.888755 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), ) 	 101607480 	 1000 	 0.45087623596191406 	 0.44672107696533203 	 0.44168543815612793 	 0.4351835250854492 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:53.026637 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), ) 	 101607840 	 1000 	 0.13138079643249512 	 0.11514425277709961 	 0.10915517807006836 	 0.10139727592468262 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:54.730459 test begin: paddle.bitwise_or(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), ) 	 101607840 	 1000 	 0.4505760669708252 	 0.44670581817626953 	 0.4418783187866211 	 0.4351940155029297 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:56.807023 test begin: paddle.bitwise_or(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 1000 	 0.11767888069152832 	 0.11512160301208496 	 0.10731148719787598 	 0.10279083251953125 	 None 	 None 	 None 	 None 	 
2025-07-24 15:42:58.436656 test begin: paddle.bitwise_or(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 1000 	 0.4505932331085205 	 0.4466841220855713 	 0.4418463706970215 	 0.4351968765258789 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:00.498123 test begin: paddle.bitwise_or(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 1000 	 0.11767005920410156 	 0.11508011817932129 	 0.10921049118041992 	 0.10328316688537598 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:02.141823 test begin: paddle.bitwise_or(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 1000 	 0.45055365562438965 	 0.4466862678527832 	 0.4417891502380371 	 0.43517374992370605 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:04.221827 test begin: paddle.bitwise_or(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101608560 	 1000 	 0.1180734634399414 	 0.11580467224121094 	 0.10936808586120605 	 0.10396909713745117 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:05.879677 test begin: paddle.bitwise_or(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101608560 	 1000 	 0.4522576332092285 	 0.4466991424560547 	 0.44182729721069336 	 0.4351074695587158 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:07.966267 test begin: paddle.bitwise_or(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_or 	 paddle.bitwise_or(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214960 	 1000 	 0.44968509674072266 	 0.4503178596496582 	 0.44071054458618164 	 0.4385969638824463 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:10.905965 test begin: paddle.bitwise_right_shift(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([169345, 300],"int32"), Tensor([169345, 300],"int32"), ) 	 101607000 	 1000 	 0.450608491897583 	 0.44666194915771484 	 0.44174885749816895 	 0.43469882011413574 	 None 	 None 	 None 	 None 	 combined
2025-07-24 15:43:13.041683 test begin: paddle.bitwise_right_shift(Tensor([200, 127009],"int64"), Tensor([200, 127009],"int64"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([200, 127009],"int64"), Tensor([200, 127009],"int64"), ) 	 50803600 	 1000 	 0.45261263847351074 	 0.446042537689209 	 0.4335763454437256 	 0.42726659774780273 	 None 	 None 	 None 	 None 	 combined
2025-07-24 15:43:14.781743 test begin: paddle.bitwise_right_shift(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([200, 254017],"int32"), Tensor([200, 254017],"int32"), ) 	 101606800 	 1000 	 0.45052576065063477 	 0.4467606544494629 	 0.4417836666107178 	 0.43497705459594727 	 None 	 None 	 None 	 None 	 combined
2025-07-24 15:43:18.647349 test begin: paddle.bitwise_right_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([200, 508033],"int16"), Tensor([200, 508033],"int16"), ) 	 203213200 	 1000 	 0.4486379623413086 	 0.4503030776977539 	 0.4363985061645508 	 0.437777042388916 	 None 	 None 	 None 	 None 	 combined
2025-07-24 15:43:22.288020 test begin: paddle.bitwise_right_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([338689, 300],"int16"), Tensor([338689, 300],"int16"), ) 	 203213400 	 1000 	 0.44750308990478516 	 0.4500603675842285 	 0.43870091438293457 	 0.4375267028808594 	 None 	 None 	 None 	 None 	 combined
2025-07-24 15:43:25.210594 test begin: paddle.bitwise_right_shift(Tensor([84673, 300],"int64"), Tensor([84673, 300],"int64"), )
[Prof] paddle.bitwise_right_shift 	 paddle.bitwise_right_shift(Tensor([84673, 300],"int64"), Tensor([84673, 300],"int64"), ) 	 50803800 	 1000 	 0.45030903816223145 	 0.44589662551879883 	 0.43993425369262695 	 0.4340178966522217 	 None 	 None 	 None 	 None 	 combined
2025-07-24 15:43:26.952000 test begin: paddle.bitwise_xor(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 141121, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 1000 	 0.449509859085083 	 0.45032310485839844 	 0.4407072067260742 	 0.43863987922668457 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:29.882847 test begin: paddle.bitwise_xor(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 141121, 3, 4, 1, 5, 2],"int16"), ) 	 203214240 	 1000 	 0.4486079216003418 	 0.450336217880249 	 0.44004321098327637 	 0.4381742477416992 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:32.776199 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 141121, 4, 1, 5, 2],"int16"), ) 	 203214240 	 1000 	 0.44942331314086914 	 0.4650552272796631 	 0.44086718559265137 	 0.43811964988708496 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:39.743068 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 188161, 1, 5, 2],"int16"), ) 	 203213880 	 1000 	 0.448880672454834 	 0.45028018951416016 	 0.44009947776794434 	 0.4385228157043457 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:42.665779 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"bool"), ) 	 101607264 	 1000 	 0.11831140518188477 	 0.11716437339782715 	 0.10953879356384277 	 0.1054387092590332 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:44.365082 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 117601, 2],"int32"), ) 	 101607264 	 1000 	 0.4506857395172119 	 0.4488029479980469 	 0.44210171699523926 	 0.4348890781402588 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:46.476240 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 235201, 2],"int16"), ) 	 203213664 	 1000 	 0.44889044761657715 	 0.45038557052612305 	 0.4404022693634033 	 0.438579797744751 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:49.429937 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 50807520 	 1000 	 0.1686878204345703 	 0.2272803783416748 	 0.158735990524292 	 0.21430349349975586 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:50.550707 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 101610720 	 1000 	 0.31497788429260254 	 0.4818539619445801 	 0.3054318428039551 	 0.4667201042175293 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:52.356985 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 50807520 	 1000 	 0.29857921600341797 	 0.3082084655761719 	 0.28693652153015137 	 0.2951209545135498 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:53.574491 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"bool"), ) 	 101608560 	 1000 	 0.11819672584533691 	 0.11609125137329102 	 0.10944199562072754 	 0.10427188873291016 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:55.245385 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 47041],"int32"), ) 	 101608560 	 1000 	 0.45063304901123047 	 0.4475436210632324 	 0.4420051574707031 	 0.43376803398132324 	 None 	 None 	 None 	 None 	 
2025-07-24 15:43:57.354573 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 94081],"int16"), ) 	 203214960 	 1000 	 0.4490041732788086 	 0.45335936546325684 	 0.44030141830444336 	 0.43785548210144043 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:00.324723 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 50807520 	 1000 	 0.17716026306152344 	 0.2270522117614746 	 0.16751885414123535 	 0.21416831016540527 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:01.436448 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"bool"), ) 	 101610720 	 1000 	 0.11828398704528809 	 0.11624908447265625 	 0.10942506790161133 	 0.10442447662353516 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:03.073178 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 50807520 	 1000 	 0.2968907356262207 	 0.3078117370605469 	 0.2867593765258789 	 0.2945976257324219 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:04.281605 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), Tensor([2, 3, 3, 3, 4, 23521, 5, 2],"int32"), ) 	 101610720 	 1000 	 0.45061802864074707 	 0.4467628002166748 	 0.4420337677001953 	 0.43511223793029785 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:06.403185 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 101610720 	 1000 	 0.34605956077575684 	 0.47867441177368164 	 0.3361508846282959 	 0.4660298824310303 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:08.256641 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 47041, 5, 2],"int16"), ) 	 203217120 	 1000 	 0.44904208183288574 	 0.4503040313720703 	 0.440418004989624 	 0.4387533664703369 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:11.147775 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"bool"), ) 	 101607480 	 1000 	 0.11773824691772461 	 0.11531472206115723 	 0.1090242862701416 	 0.10340428352355957 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:12.788155 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), Tensor([2, 3, 3, 3, 94081, 1, 5, 2],"int32"), ) 	 101607480 	 1000 	 0.4508500099182129 	 0.4494330883026123 	 0.44197845458984375 	 0.43474626541137695 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:14.859758 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"bool"), ) 	 101607840 	 1000 	 0.1180124282836914 	 0.11537051200866699 	 0.10920929908752441 	 0.10358047485351562 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:16.524979 test begin: paddle.bitwise_xor(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), Tensor([2, 3, 3, 70561, 4, 1, 5, 2],"int32"), ) 	 101607840 	 1000 	 0.4506242275238037 	 0.44669079780578613 	 0.44198131561279297 	 0.43500447273254395 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:18.594066 test begin: paddle.bitwise_xor(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 1000 	 0.11768031120300293 	 0.11549210548400879 	 0.10929369926452637 	 0.10367560386657715 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:20.230630 test begin: paddle.bitwise_xor(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), Tensor([2, 3, 70561, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 1000 	 0.4505584239959717 	 0.45057249069213867 	 0.44202232360839844 	 0.4348294734954834 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:23.750148 test begin: paddle.bitwise_xor(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101607840 	 1000 	 0.11767148971557617 	 0.11553382873535156 	 0.10907721519470215 	 0.1036367416381836 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:25.891875 test begin: paddle.bitwise_xor(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), Tensor([2, 70561, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101607840 	 1000 	 0.45055460929870605 	 0.44669532775878906 	 0.4349191188812256 	 0.42854952812194824 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:27.983477 test begin: paddle.bitwise_xor(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 	 101608560 	 1000 	 0.11807012557983398 	 0.11612367630004883 	 0.10235834121704102 	 0.09785795211791992 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:29.649225 test begin: paddle.bitwise_xor(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), Tensor([47041, 3, 3, 3, 4, 1, 5, 2],"int32"), ) 	 101608560 	 1000 	 0.45064377784729004 	 0.446779727935791 	 0.4351480007171631 	 0.4287376403808594 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:31.814597 test begin: paddle.bitwise_xor(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), )
[Prof] paddle.bitwise_xor 	 paddle.bitwise_xor(Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([94081, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 	 203214960 	 1000 	 0.4508066177368164 	 0.45032787322998047 	 0.4406399726867676 	 0.4384777545928955 	 None 	 None 	 None 	 None 	 
2025-07-24 15:44:34.747103 test begin: paddle.bmm(Tensor([112, 1043, 435],"float32"), Tensor([112, 435, 64],"float32"), )
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.bmm 	 paddle.bmm(Tensor([112, 1043, 435],"float32"), Tensor([112, 435, 64],"float32"), ) 	 53933040 	 1000 	 1.559560775756836 	 0.8989565372467041 	 0.87931227684021 	 0.8799347877502441 	 1.6111102104187012 	 1.6127970218658447 	 0.8224070072174072 	 0.8247518539428711 	 
2025-07-24 15:44:43.697777 test begin: paddle.bmm(Tensor([112, 435, 435],"float32"), Tensor([112, 435, 1043],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([112, 435, 435],"float32"), Tensor([112, 435, 1043],"float32"), ) 	 72008160 	 1000 	 3.3687806129455566 	 3.3688888549804688 	 3.3565547466278076 	 3.3530030250549316 	 6.9481871128082275 	 6.9483864307403564 	 3.5502357482910156 	 3.5501739978790283 	 
2025-07-24 15:45:06.440132 test begin: paddle.bmm(Tensor([14, 81, 7332],"float32"), Tensor([14, 7332, 512],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([14, 81, 7332],"float32"), Tensor([14, 7332, 512],"float32"), ) 	 60870264 	 1000 	 1.477949619293213 	 1.4780542850494385 	 1.4656541347503662 	 1.4620857238769531 	 1.4223206043243408 	 1.4235954284667969 	 0.7265088558197021 	 0.7277431488037109 	 
2025-07-24 15:45:13.286805 test begin: paddle.bmm(Tensor([1825, 435, 435],"float32"), Tensor([1825, 435, 64],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([1825, 435, 435],"float32"), Tensor([1825, 435, 64],"float32"), ) 	 396143625 	 1000 	 6.039783477783203 	 6.264779567718506 	 6.027530670166016 	 6.019916772842407 	 9.99491024017334 	 9.993348836898804 	 5.107100486755371 	 5.106561899185181 	 
2025-07-24 15:45:55.791496 test begin: paddle.bmm(Tensor([26, 1024, 1024],"float32"), Tensor([26, 1024, 1909],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([26, 1024, 1024],"float32"), Tensor([26, 1024, 1909],"float32"), ) 	 78088192 	 1000 	 5.950019359588623 	 5.9467504024505615 	 5.926267385482788 	 5.923617601394653 	 12.070857286453247 	 12.069886207580566 	 6.1678626537323 	 6.167526483535767 	 
2025-07-24 15:46:35.526127 test begin: paddle.bmm(Tensor([26, 1909, 1024],"float32"), Tensor([26, 1024, 12],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([26, 1909, 1024],"float32"), Tensor([26, 1024, 12],"float32"), ) 	 51144704 	 1000 	 1.425154685974121 	 0.8355662822723389 	 0.820411205291748 	 0.8168041706085205 	 0.9420766830444336 	 0.941925048828125 	 0.4811873435974121 	 0.48115992546081543 	 
2025-07-24 15:46:42.198972 test begin: paddle.bmm(Tensor([269, 435, 435],"float32"), Tensor([269, 435, 64],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([269, 435, 435],"float32"), Tensor([269, 435, 64],"float32"), ) 	 58390485 	 1000 	 0.9012176990509033 	 0.9048588275909424 	 0.8889739513397217 	 0.8844544887542725 	 1.495039463043213 	 1.4951744079589844 	 0.7637419700622559 	 0.7639181613922119 	 
2025-07-24 15:46:48.121249 test begin: paddle.bmm(Tensor([4, 1733, 7332],"float32"), Tensor([4, 7332, 512],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([4, 1733, 7332],"float32"), Tensor([4, 7332, 512],"float32"), ) 	 65841360 	 1000 	 4.387699604034424 	 4.3867528438568115 	 4.373718023300171 	 4.370827913284302 	 6.331470012664795 	 6.331310033798218 	 3.235293388366699 	 3.235212564468384 	 
2025-07-24 15:47:10.758863 test begin: paddle.bmm(Tensor([4, 81, 156801],"float32"), Tensor([4, 156801, 512],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([4, 81, 156801],"float32"), Tensor([4, 156801, 512],"float32"), ) 	 371931972 	 1000 	 31.410764694213867 	 31.422982215881348 	 31.398463487625122 	 31.40024757385254 	 8.223381757736206 	 8.230218410491943 	 4.204640626907349 	 4.219656467437744 	 
2025-07-24 15:48:37.325752 test begin: paddle.bmm(Tensor([4, 81, 24807],"float32"), Tensor([4, 24807, 512],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([4, 81, 24807],"float32"), Tensor([4, 24807, 512],"float32"), ) 	 58842204 	 1000 	 4.980248212814331 	 4.9802565574646 	 4.965387344360352 	 4.9647510051727295 	 1.3908824920654297 	 1.3911235332489014 	 0.7105956077575684 	 0.710578441619873 	 
2025-07-24 15:48:53.717544 test begin: paddle.bmm(Tensor([4, 81, 7332],"float32"), Tensor([4, 7332, 1733],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([4, 81, 7332],"float32"), Tensor([4, 7332, 1733],"float32"), ) 	 53200992 	 1000 	 1.4801537990570068 	 1.4790174961090088 	 1.4680118560791016 	 1.463775396347046 	 1.6435787677764893 	 1.6414270401000977 	 0.8408486843109131 	 0.8380346298217773 	 
2025-07-24 15:49:00.879075 test begin: paddle.bmm(Tensor([49, 1024, 1024],"float32"), Tensor([49, 1024, 12],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([49, 1024, 1024],"float32"), Tensor([49, 1024, 12],"float32"), ) 	 51982336 	 1000 	 0.8347241878509521 	 0.8332235813140869 	 0.8213036060333252 	 0.8178098201751709 	 0.9945757389068604 	 0.9942855834960938 	 0.5082192420959473 	 0.5080521106719971 	 
2025-07-24 15:49:05.399706 test begin: paddle.bmm(Tensor([86, 81, 7332],"float32"), Tensor([86, 7332, 512],"float32"), )
[Prof] paddle.bmm 	 paddle.bmm(Tensor([86, 81, 7332],"float32"), Tensor([86, 7332, 512],"float32"), ) 	 373917336 	 1000 	 5.85834527015686 	 5.85724139213562 	 5.845740795135498 	 5.839897871017456 	 8.36170482635498 	 8.363822221755981 	 4.272556304931641 	 4.273494720458984 	 
2025-07-24 15:49:40.347953 test begin: paddle.broadcast_tensors(list[Tensor([127009, 200],"float64"),Tensor([127009, 200],"float64"),], )
[Prof] paddle.broadcast_tensors 	 paddle.broadcast_tensors(list[Tensor([127009, 200],"float64"),Tensor([127009, 200],"float64"),], ) 	 50803600 	 1000 	 0.614469051361084 	 0.00743865966796875 	 0.31383299827575684 	 2.002716064453125e-05 	 0.625952959060669 	 0.06158614158630371 	 0.15989971160888672 	 2.956390380859375e-05 	 
2025-07-24 15:49:43.926826 test begin: paddle.broadcast_tensors(list[Tensor([200, 127009],"float64"),Tensor([200, 127009],"float64"),], )
[Prof] paddle.broadcast_tensors 	 paddle.broadcast_tensors(list[Tensor([200, 127009],"float64"),Tensor([200, 127009],"float64"),], ) 	 50803600 	 1000 	 0.6146798133850098 	 0.007575511932373047 	 0.3140103816986084 	 2.6702880859375e-05 	 0.6259496212005615 	 0.06256246566772461 	 0.1599268913269043 	 4.8160552978515625e-05 	 
2025-07-24 15:49:47.361959 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([12700801],"float64"), )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([12700801],"float64"), ) 	 38102403 	 1000 	 1.3422908782958984 	 1.0408270359039307 	 1.331465721130371 	 1.0298659801483154 	 None 	 None 	 None 	 None 	 
2025-07-24 15:49:52.444458 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([12700801],"float64"), out_int32=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([12700801],"float64"), out_int32=True, ) 	 38102403 	 1000 	 1.7873115539550781 	 1.0335454940795898 	 1.3250980377197266 	 1.022402286529541 	 None 	 None 	 None 	 None 	 
2025-07-24 15:49:57.816568 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([12700801],"float64"), right=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([12700801],"float64"), right=True, ) 	 38102403 	 1000 	 1.3440513610839844 	 1.0423760414123535 	 1.3331871032714844 	 1.029026746749878 	 None 	 None 	 None 	 None 	 
2025-07-24 15:50:00.996922 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([4],"float64"), )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([4],"float64"), ) 	 25401606 	 1000 	 0.31212711334228516 	 0.31222105026245117 	 0.3009319305419922 	 0.3014087677001953 	 None 	 None 	 None 	 None 	 
2025-07-24 15:50:02.148268 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([4],"float64"), out_int32=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([4],"float64"), out_int32=True, ) 	 25401606 	 1000 	 0.28520846366882324 	 0.24550652503967285 	 0.2741532325744629 	 0.23396897315979004 	 None 	 None 	 None 	 None 	 
2025-07-24 15:50:03.206710 test begin: paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([4],"float64"), right=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 12700801],"float64"), Tensor([4],"float64"), right=True, ) 	 25401606 	 1000 	 0.31934356689453125 	 0.31481051445007324 	 0.30663323402404785 	 0.30391860008239746 	 None 	 None 	 None 	 None 	 
2025-07-24 15:50:04.373856 test begin: paddle.bucketize(Tensor([2, 25401601],"float64"), Tensor([25401601],"float64"), )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 25401601],"float64"), Tensor([25401601],"float64"), ) 	 76204803 	 1000 	 2.774003028869629 	 2.1431572437286377 	 2.763169288635254 	 2.132120370864868 	 None 	 None 	 None 	 None 	 
2025-07-24 15:50:10.940152 test begin: paddle.bucketize(Tensor([2, 25401601],"float64"), Tensor([25401601],"float64"), out_int32=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 25401601],"float64"), Tensor([25401601],"float64"), out_int32=True, ) 	 76204803 	 1000 	 2.762528657913208 	 2.136756181716919 	 2.751490592956543 	 2.125809669494629 	 None 	 None 	 None 	 None 	 
2025-07-24 15:50:17.400626 test begin: paddle.bucketize(Tensor([2, 25401601],"float64"), Tensor([25401601],"float64"), right=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 25401601],"float64"), Tensor([25401601],"float64"), right=True, ) 	 76204803 	 1000 	 2.7770917415618896 	 2.15265154838562 	 2.7647321224212646 	 2.1416537761688232 	 None 	 None 	 None 	 None 	 
2025-07-24 15:50:23.908094 test begin: paddle.bucketize(Tensor([2, 4],"float64"), Tensor([25401601],"float64"), )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 4],"float64"), Tensor([25401601],"float64"), ) 	 25401609 	 1000 	 0.010305404663085938 	 0.010706186294555664 	 1.5497207641601562e-05 	 2.9087066650390625e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 15:50:24.459014 test begin: paddle.bucketize(Tensor([2, 4],"float64"), Tensor([25401601],"float64"), out_int32=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 4],"float64"), Tensor([25401601],"float64"), out_int32=True, ) 	 25401609 	 1000 	 0.010925054550170898 	 0.010651350021362305 	 1.33514404296875e-05 	 2.47955322265625e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 15:50:25.006513 test begin: paddle.bucketize(Tensor([2, 4],"float64"), Tensor([25401601],"float64"), right=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([2, 4],"float64"), Tensor([25401601],"float64"), right=True, ) 	 25401609 	 1000 	 0.01080465316772461 	 0.010696649551391602 	 1.33514404296875e-05 	 2.288818359375e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 15:50:25.557649 test begin: paddle.bucketize(Tensor([6350401, 4],"float64"), Tensor([4],"float64"), )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([6350401, 4],"float64"), Tensor([4],"float64"), ) 	 25401608 	 1000 	 0.3188011646270752 	 0.31527113914489746 	 0.30576467514038086 	 0.3043696880340576 	 None 	 None 	 None 	 None 	 
2025-07-24 15:50:26.719262 test begin: paddle.bucketize(Tensor([6350401, 4],"float64"), Tensor([4],"float64"), out_int32=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([6350401, 4],"float64"), Tensor([4],"float64"), out_int32=True, ) 	 25401608 	 1000 	 0.2835509777069092 	 0.24466848373413086 	 0.27263975143432617 	 0.23356986045837402 	 None 	 None 	 None 	 None 	 
2025-07-24 15:50:27.774309 test begin: paddle.bucketize(Tensor([6350401, 4],"float64"), Tensor([4],"float64"), right=True, )
[Prof] paddle.bucketize 	 paddle.bucketize(Tensor([6350401, 4],"float64"), Tensor([4],"float64"), right=True, ) 	 25401608 	 1000 	 0.3164834976196289 	 0.31481122970581055 	 0.30544614791870117 	 0.3038804531097412 	 None 	 None 	 None 	 None 	 
2025-07-24 15:50:28.957076 test begin: paddle.cartesian_prod(list[Tensor([4],"int32"),Tensor([4],"int32"),Tensor([50803201],"int32"),], )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd1e8d6aec0>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753344033 (unix time) try "date -d @1753344033" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a546) received by PID 107846 (TID 0x7fd1e02f9640) from PID 107846 ***]

2025-07-24 16:00:55.215183 test begin: paddle.cast(Tensor([1, 1, 32768, 32768],"float16"), dtype=Dtype(float16), )
W0724 16:01:18.818965 118089 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cast 	 paddle.cast(Tensor([1, 1, 32768, 32768],"float16"), dtype=Dtype(float16), ) 	 1073741824 	 1000 	 3.2361297607421875 	 0.0034799575805664062 	 1.6533753871917725 	 2.5510787963867188e-05 	 3.2334070205688477 	 0.06690478324890137 	 1.6521453857421875 	 4.506111145019531e-05 	 combined
2025-07-24 16:01:49.068132 test begin: paddle.cast(Tensor([128256, 793],"bfloat16"), Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([128256, 793],"bfloat16"), Dtype(float16), ) 	 101707008 	 1000 	 0.29860424995422363 	 0.5137507915496826 	 0.2892181873321533 	 0.497330904006958 	 0.2982330322265625 	 0.5071911811828613 	 0.2405080795288086 	 0.4228487014770508 	 combined
2025-07-24 16:01:55.273531 test begin: paddle.cast(Tensor([2, 1, 1551, 32768],"float16"), dtype=Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([2, 1, 1551, 32768],"float16"), dtype=Dtype(float16), ) 	 101646336 	 1000 	 0.3102748394012451 	 0.0030188560485839844 	 0.2988736629486084 	 1.9550323486328125e-05 	 0.3101921081542969 	 0.05425572395324707 	 0.24602079391479492 	 7.2479248046875e-05 	 combined
2025-07-24 16:01:59.777220 test begin: paddle.cast(Tensor([2, 1, 32768, 1551],"float16"), dtype=Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([2, 1, 32768, 1551],"float16"), dtype=Dtype(float16), ) 	 101646336 	 1000 	 0.3101978302001953 	 0.0030410289764404297 	 0.2987492084503174 	 2.0503997802734375e-05 	 0.3101990222930908 	 0.052124977111816406 	 0.24629616737365723 	 5.4836273193359375e-05 	 combined
2025-07-24 16:02:04.310080 test begin: paddle.cast(Tensor([2, 1, 32768, 32768],"float16"), dtype=Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([2, 1, 32768, 32768],"float16"), dtype=Dtype(float16), ) 	 2147483648 	 1000 	 6.471786975860596 	 0.00202178955078125 	 3.3056113719940186 	 2.3126602172851562e-05 	 6.4729249477386475 	 0.06182980537414551 	 3.3075451850891113 	 5.7697296142578125e-05 	 combined
2025-07-24 16:03:52.235347 test begin: paddle.cast(Tensor([2, 1024, 50304],"float16"), dtype="float32", )
[Prof] paddle.cast 	 paddle.cast(Tensor([2, 1024, 50304],"float16"), dtype="float32", ) 	 103022592 	 1000 	 0.4884040355682373 	 0.5568606853485107 	 0.47663140296936035 	 0.5433197021484375 	 0.45738935470581055 	 0.46001100540161133 	 0.4042508602142334 	 0.37467527389526367 	 combined
2025-07-24 16:03:57.711662 test begin: paddle.cast(Tensor([33076, 3072],"bfloat16"), Dtype(float16), )
[Prof] paddle.cast 	 paddle.cast(Tensor([33076, 3072],"bfloat16"), Dtype(float16), ) 	 101609472 	 1000 	 0.29801082611083984 	 0.5107100009918213 	 0.2885723114013672 	 0.4970991611480713 	 0.2980225086212158 	 0.5065710544586182 	 0.2415318489074707 	 0.4173922538757324 	 combined
2025-07-24 16:04:02.787935 test begin: paddle.cast(Tensor([8, 1024, 12404],"float16"), dtype="float32", )
[Prof] paddle.cast 	 paddle.cast(Tensor([8, 1024, 12404],"float16"), dtype="float32", ) 	 101613568 	 1000 	 0.4818587303161621 	 0.5586330890655518 	 0.4703383445739746 	 0.535790205001831 	 0.45100879669189453 	 0.4536137580871582 	 0.39784717559814453 	 0.35954785346984863 	 combined
2025-07-24 16:04:08.661185 test begin: paddle.cast(Tensor([8, 253, 50304],"float16"), dtype="float32", )
[Prof] paddle.cast 	 paddle.cast(Tensor([8, 253, 50304],"float16"), dtype="float32", ) 	 101815296 	 1000 	 0.48252391815185547 	 0.5550918579101562 	 0.47087669372558594 	 0.53525710105896 	 0.451709508895874 	 0.45455408096313477 	 0.3984031677246094 	 0.3681762218475342 	 combined
2025-07-24 16:04:14.082628 test begin: paddle.cdist(Tensor([12700801, 4],"float32"), Tensor([1, 4],"float32"), p=1, )
[Prof] paddle.cdist 	 paddle.cdist(Tensor([12700801, 4],"float32"), Tensor([1, 4],"float32"), p=1, ) 	 50803208 	 1000 	 0.6937599182128906 	 26.78866934776306 	 0.35442352294921875 	 26.76572299003601 	 8.372646570205688 	 14.18963623046875 	 2.852855920791626 	 2.894977331161499 	 
2025-07-24 16:05:05.313484 test begin: paddle.cdist(Tensor([6380, 7963],"float32"), Tensor([1, 7963],"float32"), p=1, )
[Prof] paddle.cdist 	 paddle.cdist(Tensor([6380, 7963],"float32"), Tensor([1, 7963],"float32"), p=1, ) 	 50811903 	 1000 	 0.44986701011657715 	 0.19427061080932617 	 0.22985148429870605 	 0.17201900482177734 	 2.183887243270874 	 1.17793607711792 	 0.7424986362457275 	 0.2404317855834961 	 
2025-07-24 16:05:12.838822 test begin: paddle.cdist(Tensor([8550, 5942],"float32"), Tensor([1, 5942],"float32"), p=1, )
[Prof] paddle.cdist 	 paddle.cdist(Tensor([8550, 5942],"float32"), Tensor([1, 5942],"float32"), p=1, ) 	 50810042 	 1000 	 0.4506094455718994 	 0.19385027885437012 	 0.230224609375 	 0.1714937686920166 	 2.189476728439331 	 1.1604490280151367 	 0.7444732189178467 	 0.23602008819580078 	 
2025-07-24 16:05:17.666764 test begin: paddle.cdist(Tensor([900, 56449],"float32"), Tensor([1, 56449],"float32"), p=1, )
[Prof] paddle.cdist 	 paddle.cdist(Tensor([900, 56449],"float32"), Tensor([1, 56449],"float32"), p=1, ) 	 50860549 	 1000 	 0.4815552234649658 	 0.29027891159057617 	 0.16388154029846191 	 0.26802492141723633 	 2.155963897705078 	 1.2229962348937988 	 0.7331352233886719 	 0.3123908042907715 	 
2025-07-24 16:05:22.755343 test begin: paddle.ceil(Tensor([12404, 32, 128],"float32"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([12404, 32, 128],"float32"), ) 	 50806784 	 1000 	 0.29611825942993164 	 0.2980690002441406 	 0.28707098960876465 	 0.28659844398498535 	 0.13428831100463867 	 0.13420414924621582 	 0.08345675468444824 	 0.07349443435668945 	 
2025-07-24 16:05:25.199279 test begin: paddle.ceil(Tensor([141121, 6, 3, 1, 2, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([141121, 6, 3, 1, 2, 5],"float64"), ) 	 25401780 	 1000 	 0.29799652099609375 	 0.298539400100708 	 0.2890794277191162 	 0.28697872161865234 	 0.13408827781677246 	 0.13466262817382812 	 0.08368778228759766 	 0.07013916969299316 	 
2025-07-24 16:05:27.040105 test begin: paddle.ceil(Tensor([3, 141121, 3, 4, 1, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 141121, 3, 4, 1, 5],"float64"), ) 	 25401780 	 1000 	 0.29798221588134766 	 0.2985973358154297 	 0.2892284393310547 	 0.28717589378356934 	 0.13408803939819336 	 0.13480544090270996 	 0.0838782787322998 	 0.07096076011657715 	 
2025-07-24 16:05:28.968010 test begin: paddle.ceil(Tensor([3, 282241, 3, 1, 2, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 282241, 3, 1, 2, 5],"float64"), ) 	 25401690 	 1000 	 0.2980782985687256 	 0.2985200881958008 	 0.28928661346435547 	 0.2871062755584717 	 0.13408923149108887 	 0.13460040092468262 	 0.08360528945922852 	 0.07053041458129883 	 
2025-07-24 16:05:30.857446 test begin: paddle.ceil(Tensor([3, 6, 141121, 1, 2, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 141121, 1, 2, 5],"float64"), ) 	 25401780 	 1000 	 0.2980036735534668 	 0.29847097396850586 	 0.28952479362487793 	 0.2870328426361084 	 0.1340470314025879 	 0.1345813274383545 	 0.08385062217712402 	 0.07043194770812988 	 
2025-07-24 16:05:32.763774 test begin: paddle.ceil(Tensor([3, 6, 3, 1, 2, 235201],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 1, 2, 235201],"float64"), ) 	 25401708 	 1000 	 0.2976822853088379 	 0.3047647476196289 	 0.2888977527618408 	 0.2869548797607422 	 0.13466262817382812 	 0.13462018966674805 	 0.08450484275817871 	 0.07043242454528809 	 
2025-07-24 16:05:34.589498 test begin: paddle.ceil(Tensor([3, 6, 3, 1, 94081, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 1, 94081, 5],"float64"), ) 	 25401870 	 1000 	 0.2981393337249756 	 0.3102846145629883 	 0.2892911434173584 	 0.2870190143585205 	 0.13407230377197266 	 0.13461780548095703 	 0.08395147323608398 	 0.06253290176391602 	 
2025-07-24 16:05:40.122281 test begin: paddle.ceil(Tensor([3, 6, 3, 4, 1, 117601],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 4, 1, 117601],"float64"), ) 	 25401816 	 1000 	 0.2981390953063965 	 0.2984752655029297 	 0.28929734230041504 	 0.28697943687438965 	 0.13409924507141113 	 0.13471770286560059 	 0.08404898643493652 	 0.07086038589477539 	 
2025-07-24 16:05:41.961957 test begin: paddle.ceil(Tensor([3, 6, 3, 4, 23521, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 4, 23521, 5],"float64"), ) 	 25402680 	 1000 	 0.2979297637939453 	 0.30434703826904297 	 0.2890510559082031 	 0.28681421279907227 	 0.13422107696533203 	 0.1346147060394287 	 0.08372092247009277 	 0.06954216957092285 	 
2025-07-24 16:05:43.811668 test begin: paddle.ceil(Tensor([3, 6, 3, 47041, 2, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 47041, 2, 5],"float64"), ) 	 25402140 	 1000 	 0.2980842590332031 	 0.2985243797302246 	 0.28927040100097656 	 0.2870149612426758 	 0.1340649127960205 	 0.1347966194152832 	 0.08415627479553223 	 0.06042122840881348 	 
2025-07-24 16:05:45.703742 test begin: paddle.ceil(Tensor([3, 6, 3, 94081, 1, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 3, 94081, 1, 5],"float64"), ) 	 25401870 	 1000 	 0.2981109619140625 	 0.2985234260559082 	 0.2892148494720459 	 0.2870359420776367 	 0.13410425186157227 	 0.13457369804382324 	 0.0840153694152832 	 0.06017279624938965 	 
2025-07-24 16:05:47.580859 test begin: paddle.ceil(Tensor([3, 6, 70561, 4, 1, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([3, 6, 70561, 4, 1, 5],"float64"), ) 	 25401960 	 1000 	 0.29775166511535645 	 0.3077657222747803 	 0.28233861923217773 	 0.2813563346862793 	 0.13409042358398438 	 0.13456273078918457 	 0.07247805595397949 	 0.04155158996582031 	 
2025-07-24 16:05:49.612967 test begin: paddle.ceil(Tensor([32, 12404, 128],"float32"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([32, 12404, 128],"float32"), ) 	 50806784 	 1000 	 0.29601454734802246 	 0.2979257106781006 	 0.28707146644592285 	 0.28645825386047363 	 0.13426733016967773 	 0.13424062728881836 	 0.08373379707336426 	 0.07094216346740723 	 
2025-07-24 16:05:52.013490 test begin: paddle.ceil(Tensor([32, 32, 49613],"float32"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([32, 32, 49613],"float32"), ) 	 50803712 	 1000 	 0.2957780361175537 	 0.30156850814819336 	 0.2867734432220459 	 0.2864689826965332 	 0.1348414421081543 	 0.13416194915771484 	 0.08337712287902832 	 0.07218527793884277 	 
2025-07-24 16:05:54.445179 test begin: paddle.ceil(Tensor([70561, 6, 3, 4, 1, 5],"float64"), )
[Prof] paddle.ceil 	 paddle.ceil(Tensor([70561, 6, 3, 4, 1, 5],"float64"), ) 	 25401960 	 1000 	 0.2977173328399658 	 0.29846739768981934 	 0.28891658782958984 	 0.28696608543395996 	 0.13400959968566895 	 0.13457798957824707 	 0.08366584777832031 	 0.06652498245239258 	 
2025-07-24 16:05:56.261882 test begin: paddle.chunk(Tensor([115, 216, 64, 64],"float16"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([115, 216, 64, 64],"float16"), 3, axis=1, ) 	 101744640 	 1000 	 0.432586669921875 	 0.007550954818725586 	 0.41684961318969727 	 2.5510787963867188e-05 	 0.30837178230285645 	 0.5052483081817627 	 0.2495582103729248 	 0.417935848236084 	 
2025-07-24 16:06:01.420875 test begin: paddle.chunk(Tensor([16, 128, 24807],"float32"), 2, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([16, 128, 24807],"float32"), 2, axis=1, ) 	 50804736 	 1000 	 0.337296724319458 	 0.006448984146118164 	 0.32303905487060547 	 2.9325485229492188e-05 	 0.3128471374511719 	 0.30792975425720215 	 0.25698184967041016 	 0.21606898307800293 	 
2025-07-24 16:06:03.927037 test begin: paddle.chunk(Tensor([16, 128, 25500],"float32"), 2, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([16, 128, 25500],"float32"), 2, axis=1, ) 	 52224000 	 1000 	 0.3536257743835449 	 0.00641942024230957 	 0.3392951488494873 	 2.0742416381835938e-05 	 0.32118940353393555 	 0.31542253494262695 	 0.2654387950897217 	 0.2360825538635254 	 
2025-07-24 16:06:06.599274 test begin: paddle.chunk(Tensor([4, 216, 1838, 64],"float16"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([4, 216, 1838, 64],"float16"), 3, axis=1, ) 	 101634048 	 1000 	 0.42972636222839355 	 0.007606029510498047 	 0.41426730155944824 	 2.9802322387695312e-05 	 0.30812835693359375 	 0.5028040409088135 	 0.24569416046142578 	 0.40885043144226074 	 
2025-07-24 16:06:11.597640 test begin: paddle.chunk(Tensor([4, 216, 64, 1838],"float16"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([4, 216, 64, 1838],"float16"), 3, axis=1, ) 	 101634048 	 1000 	 0.4296729564666748 	 0.009641170501708984 	 0.41430163383483887 	 6.175041198730469e-05 	 0.30820369720458984 	 0.5026040077209473 	 0.2480297088623047 	 0.41344666481018066 	 
2025-07-24 16:06:18.132924 test begin: paddle.chunk(Tensor([4, 216, 64, 919],"float32"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([4, 216, 64, 919],"float32"), 3, axis=1, ) 	 50817024 	 1000 	 0.33710312843322754 	 0.012449979782104492 	 0.3126029968261719 	 2.6226043701171875e-05 	 0.308117151260376 	 0.3052537441253662 	 0.23951435089111328 	 0.19434285163879395 	 
2025-07-24 16:06:20.867507 test begin: paddle.chunk(Tensor([4, 216, 919, 64],"float32"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([4, 216, 919, 64],"float32"), 3, axis=1, ) 	 50817024 	 1000 	 0.3371450901031494 	 0.01282644271850586 	 0.313464879989624 	 8.58306884765625e-05 	 0.30829572677612305 	 0.30528783798217773 	 0.24024605751037598 	 0.1849358081817627 	 
2025-07-24 16:06:23.500713 test begin: paddle.chunk(Tensor([58, 216, 64, 64],"float32"), 3, axis=1, )
[Prof] paddle.chunk 	 paddle.chunk(Tensor([58, 216, 64, 64],"float32"), 3, axis=1, ) 	 51314688 	 1000 	 0.34479618072509766 	 0.007483720779418945 	 0.32024621963500977 	 2.193450927734375e-05 	 0.31108593940734863 	 0.30826592445373535 	 0.25221991539001465 	 0.22332000732421875 	 
2025-07-24 16:06:26.091566 test begin: paddle.clip(Tensor([1408, 36082],"float32"), min=-2, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([1408, 36082],"float32"), min=-2, max=2, ) 	 50803456 	 1000 	 0.29558730125427246 	 0.29806089401245117 	 0.2774982452392578 	 0.2845883369445801 	 0.4505190849304199 	 0.734816312789917 	 0.3977079391479492 	 0.15019989013671875 	 
2025-07-24 16:06:29.560772 test begin: paddle.clip(Tensor([2, 3840, 10240],"float32"), 0, 255, )
[Prof] paddle.clip 	 paddle.clip(Tensor([2, 3840, 10240],"float32"), 0, 255, ) 	 78643200 	 1000 	 0.45589637756347656 	 0.45829129219055176 	 0.4387218952178955 	 0.4453423023223877 	 0.694279670715332 	 1.1196417808532715 	 0.641442060470581 	 0.22892498970031738 	 
2025-07-24 16:06:34.906014 test begin: paddle.clip(Tensor([23, 17, 256, 256],"float64"), min=0, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([23, 17, 256, 256],"float64"), min=0, max=2, ) 	 25624576 	 1000 	 0.3007376194000244 	 0.3095896244049072 	 0.28311753273010254 	 0.27521729469299316 	 0.4528670310974121 	 0.7258353233337402 	 0.40004634857177734 	 0.14834046363830566 	 
2025-07-24 16:06:39.413128 test begin: paddle.clip(Tensor([24, 17, 244, 256],"float64"), min=0, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([24, 17, 244, 256],"float64"), min=0, max=2, ) 	 25485312 	 1000 	 0.29901576042175293 	 0.2995152473449707 	 0.2813453674316406 	 0.2861013412475586 	 0.45040082931518555 	 0.7218637466430664 	 0.3979227542877197 	 0.1475811004638672 	 
2025-07-24 16:06:42.161446 test begin: paddle.clip(Tensor([24, 17, 256, 244],"float64"), min=0, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([24, 17, 256, 244],"float64"), min=0, max=2, ) 	 25485312 	 1000 	 0.29902052879333496 	 0.3022022247314453 	 0.2812528610229492 	 0.28580307960510254 	 0.4497106075286865 	 0.7218396663665771 	 0.39684486389160156 	 0.1476142406463623 	 
2025-07-24 16:06:44.991821 test begin: paddle.clip(Tensor([24, 17, 256, 256],"float64"), min=0, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([24, 17, 256, 256],"float64"), min=0, max=2, ) 	 26738688 	 1000 	 0.3137936592102051 	 0.3139679431915283 	 0.29610347747802734 	 0.3009774684906006 	 0.4709036350250244 	 0.7559671401977539 	 0.41828465461730957 	 0.15454721450805664 	 
2025-07-24 16:06:47.881175 test begin: paddle.clip(Tensor([3, 1654, 10240],"float32"), 0, 255, )
[Prof] paddle.clip 	 paddle.clip(Tensor([3, 1654, 10240],"float32"), 0, 255, ) 	 50810880 	 1000 	 0.2959022521972656 	 0.29819154739379883 	 0.2770562171936035 	 0.2813408374786377 	 0.4503037929534912 	 0.7336211204528809 	 0.3974616527557373 	 0.14998316764831543 	 
2025-07-24 16:06:51.304909 test begin: paddle.clip(Tensor([3, 3840, 4411],"float32"), 0, 255, )
[Prof] paddle.clip 	 paddle.clip(Tensor([3, 3840, 4411],"float32"), 0, 255, ) 	 50814720 	 1000 	 0.29582834243774414 	 0.29803013801574707 	 0.27840137481689453 	 0.28482556343078613 	 0.4506494998931885 	 0.735131025314331 	 0.3974416255950928 	 0.15029668807983398 	 
2025-07-24 16:06:54.678009 test begin: paddle.clip(Tensor([8269, 6144],"float32"), min=-2, max=2, )
[Prof] paddle.clip 	 paddle.clip(Tensor([8269, 6144],"float32"), min=-2, max=2, ) 	 50804736 	 1000 	 0.29582810401916504 	 0.2979755401611328 	 0.27803492546081543 	 0.28470611572265625 	 0.45029401779174805 	 0.7335231304168701 	 0.3973362445831299 	 0.14993977546691895 	 
2025-07-24 16:06:58.089531 test begin: paddle.clone(Tensor([145, 12, 112, 261],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([145, 12, 112, 261],"float32"), ) 	 50863680 	 1000 	 0.31549859046936035 	 0.3138241767883301 	 0.16122770309448242 	 0.16024327278137207 	 0.31641507148742676 	 0.048296213150024414 	 0.16163158416748047 	 3.600120544433594e-05 	 
2025-07-24 16:07:00.610936 test begin: paddle.clone(Tensor([145, 12, 261, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([145, 12, 261, 112],"float32"), ) 	 50863680 	 1000 	 0.31548404693603516 	 0.31382060050964355 	 0.16120243072509766 	 0.16025781631469727 	 0.3164229393005371 	 0.04984593391418457 	 0.16163182258605957 	 3.886222839355469e-05 	 
2025-07-24 16:07:03.190816 test begin: paddle.clone(Tensor([145, 28, 112, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([145, 28, 112, 112],"float32"), ) 	 50928640 	 1000 	 0.31101083755493164 	 0.3112454414367676 	 0.29542064666748047 	 0.2914738655090332 	 0.310955286026001 	 0.07305598258972168 	 0.24663591384887695 	 7.772445678710938e-05 	 
2025-07-24 16:07:05.941294 test begin: paddle.clone(Tensor([22, 185, 112, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([22, 185, 112, 112],"float32"), ) 	 51054080 	 1000 	 0.3146655559539795 	 0.314990758895874 	 0.16080999374389648 	 0.16089797019958496 	 0.3149263858795166 	 0.048438072204589844 	 0.16088581085205078 	 6.079673767089844e-05 	 
2025-07-24 16:07:08.557663 test begin: paddle.clone(Tensor([22, 64, 112, 323],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([22, 64, 112, 323],"float32"), ) 	 50935808 	 1000 	 0.31104350090026855 	 0.31136560440063477 	 0.3020622730255127 	 0.29769444465637207 	 0.3109254837036133 	 0.0491180419921875 	 0.25807833671569824 	 5.984306335449219e-05 	 
2025-07-24 16:07:11.231654 test begin: paddle.clone(Tensor([22, 64, 323, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([22, 64, 323, 112],"float32"), ) 	 50935808 	 1000 	 0.3110039234161377 	 0.31124091148376465 	 0.3016481399536133 	 0.29758358001708984 	 0.310896635055542 	 0.049347639083862305 	 0.258056640625 	 4.673004150390625e-05 	 
2025-07-24 16:07:13.788833 test begin: paddle.clone(Tensor([338, 12, 112, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([338, 12, 112, 112],"float32"), ) 	 50878464 	 1000 	 0.31063055992126465 	 0.3123588562011719 	 0.3012807369232178 	 0.2957427501678467 	 0.31063175201416016 	 0.048586368560791016 	 0.2577507495880127 	 3.0994415283203125e-05 	 
2025-07-24 16:07:16.346115 test begin: paddle.clone(Tensor([43, 256, 56, 83],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([43, 256, 56, 83],"float32"), ) 	 51165184 	 1000 	 0.3122260570526123 	 0.5381455421447754 	 0.3026907444000244 	 0.29918575286865234 	 0.3123331069946289 	 0.048952579498291016 	 0.25951504707336426 	 5.6743621826171875e-05 	 
2025-07-24 16:07:20.291059 test begin: paddle.clone(Tensor([43, 256, 83, 56],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([43, 256, 83, 56],"float32"), ) 	 51165184 	 1000 	 0.31220006942749023 	 0.3171579837799072 	 0.3028712272644043 	 0.2986276149749756 	 0.3122842311859131 	 0.04844522476196289 	 0.25930023193359375 	 2.8371810913085938e-05 	 
2025-07-24 16:07:22.824346 test begin: paddle.clone(Tensor([43, 377, 56, 56],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([43, 377, 56, 56],"float32"), ) 	 50837696 	 1000 	 0.31566357612609863 	 0.31342196464538574 	 0.16131114959716797 	 0.16003870964050293 	 0.31525087356567383 	 0.048447370529174805 	 0.16105151176452637 	 3.24249267578125e-05 	 
2025-07-24 16:07:25.500446 test begin: paddle.clone(Tensor([64, 256, 56, 56],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([64, 256, 56, 56],"float32"), ) 	 51380224 	 1000 	 0.3134918212890625 	 0.3138706684112549 	 0.3041970729827881 	 0.3004176616668701 	 0.31365156173706055 	 0.048606157302856445 	 0.26070427894592285 	 3.0279159545898438e-05 	 
2025-07-24 16:07:28.046464 test begin: paddle.clone(Tensor([64, 64, 112, 112],"float32"), )
[Prof] paddle.clone 	 paddle.clone(Tensor([64, 64, 112, 112],"float32"), ) 	 51380224 	 1000 	 0.3134939670562744 	 0.3139219284057617 	 0.3041269779205322 	 0.3005070686340332 	 0.3137025833129883 	 0.04996323585510254 	 0.2602546215057373 	 6.461143493652344e-05 	 
2025-07-24 16:07:30.669013 test begin: paddle.column_stack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], ) 	 76204872 	 1000 	 0.9300055503845215 	 0.9396524429321289 	 0.9140937328338623 	 0.9075052738189697 	 0.9287707805633545 	 0.0693662166595459 	 0.8559544086456299 	 5.340576171875e-05 	 
2025-07-24 16:07:39.434893 test begin: paddle.column_stack(list[Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2, 1058401],"float64"),], ) 	 25401624 	 1000 	 0.316098690032959 	 0.3164198398590088 	 0.2949941158294678 	 0.15997648239135742 	 0.31070685386657715 	 0.0804283618927002 	 0.24585390090942383 	 6.961822509765625e-05 	 
2025-07-24 16:07:41.591684 test begin: paddle.column_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401870 	 1000 	 0.31616997718811035 	 0.3215484619140625 	 0.300570011138916 	 0.30698204040527344 	 0.31261634826660156 	 0.09309077262878418 	 0.24391913414001465 	 0.00010228157043457031 	 
2025-07-24 16:07:43.684212 test begin: paddle.column_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401870 	 1000 	 0.31533360481262207 	 0.320681095123291 	 0.2996091842651367 	 0.3063774108886719 	 0.31311750411987305 	 0.07005929946899414 	 0.24533700942993164 	 4.4345855712890625e-05 	 
2025-07-24 16:07:45.723232 test begin: paddle.column_stack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], ) 	 76204836 	 1000 	 0.9275925159454346 	 0.921971321105957 	 0.9114904403686523 	 0.9072844982147217 	 0.9310214519500732 	 0.07156181335449219 	 0.86238694190979 	 3.600120544433594e-05 	 
2025-07-24 16:07:52.012381 test begin: paddle.column_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 25401654 	 1000 	 0.3169422149658203 	 0.31842613220214844 	 0.300980806350708 	 0.30440473556518555 	 0.31264638900756836 	 0.06841588020324707 	 0.24454307556152344 	 4.458427429199219e-05 	 
2025-07-24 16:07:54.083189 test begin: paddle.column_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401654 	 1000 	 0.3161332607269287 	 0.318744421005249 	 0.3002347946166992 	 0.3045320510864258 	 0.3125014305114746 	 0.06799077987670898 	 0.24439358711242676 	 4.363059997558594e-05 	 
2025-07-24 16:07:56.051545 test begin: paddle.column_stack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 76204980 	 1000 	 0.9273009300231934 	 0.9282147884368896 	 0.911442756652832 	 0.9134664535522461 	 0.9309053421020508 	 0.06893539428710938 	 0.8632891178131104 	 3.62396240234375e-05 	 
2025-07-24 16:08:01.839893 test begin: paddle.column_stack(list[Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401660 	 1000 	 0.3139219284057617 	 0.3133115768432617 	 0.30071091651916504 	 0.1599724292755127 	 0.3138570785522461 	 0.05507469177246094 	 0.2583496570587158 	 0.00012683868408203125 	 
2025-07-24 16:08:03.903979 test begin: paddle.column_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401654 	 1000 	 0.31632184982299805 	 0.3117866516113281 	 0.30045413970947266 	 0.2976956367492676 	 0.3123905658721924 	 0.0695946216583252 	 0.24446845054626465 	 3.981590270996094e-05 	 
2025-07-24 16:08:05.911684 test begin: paddle.column_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 76204818 	 1000 	 0.9332985877990723 	 0.9272983074188232 	 0.917449951171875 	 0.912865400314331 	 0.9459209442138672 	 0.0720205307006836 	 0.8774960041046143 	 4.839897155761719e-05 	 
2025-07-24 16:08:12.115243 test begin: paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401870 	 1000 	 0.3154106140136719 	 0.3152651786804199 	 0.2993810176849365 	 0.3008742332458496 	 0.3123435974121094 	 0.0687868595123291 	 0.24453496932983398 	 2.8371810913085938e-05 	 
2025-07-24 16:08:14.164726 test begin: paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 76204890 	 1000 	 0.9339618682861328 	 0.9369785785675049 	 0.9179267883300781 	 0.9222214221954346 	 0.942894458770752 	 0.0700688362121582 	 0.8744723796844482 	 5.125999450683594e-05 	 
2025-07-24 16:08:19.977724 test begin: paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401630 	 1000 	 0.3157539367675781 	 0.3132662773132324 	 0.3026285171508789 	 0.1599287986755371 	 0.31216955184936523 	 0.058373451232910156 	 0.2565033435821533 	 3.814697265625e-05 	 
2025-07-24 16:08:22.006570 test begin: paddle.column_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], ) 	 76204824 	 1000 	 0.9769246578216553 	 1.3984956741333008 	 0.9611208438873291 	 1.3798458576202393 	 0.9601268768310547 	 0.0745542049407959 	 0.8915908336639404 	 5.7697296142578125e-05 	 
2025-07-24 16:08:28.488501 test begin: paddle.column_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 76204920 	 1000 	 0.9257667064666748 	 1.0311391353607178 	 0.9097187519073486 	 1.0162558555603027 	 0.9367361068725586 	 0.06920385360717773 	 0.8660867214202881 	 5.1975250244140625e-05 	 
2025-07-24 16:08:34.426334 test begin: paddle.column_stack(list[Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.column_stack 	 paddle.column_stack(list[Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401640 	 1000 	 0.3085463047027588 	 0.5382533073425293 	 0.295529842376709 	 0.1599740982055664 	 0.32633328437805176 	 0.0544896125793457 	 0.27047228813171387 	 6.29425048828125e-05 	 
2025-07-24 16:08:39.296619 test begin: paddle.combinations(Tensor([25401601],"int64"), 0, True, )
[Prof] paddle.combinations 	 paddle.combinations(Tensor([25401601],"int64"), 0, True, ) 	 25401601 	 1000 	 0.012552976608276367 	 0.00411224365234375 	 9.298324584960938e-06 	 1.7881393432617188e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:08:39.790262 test begin: paddle.combinations(Tensor([50803201],"int32"), 1, True, )
[Prof] paddle.combinations 	 paddle.combinations(Tensor([50803201],"int32"), 1, True, ) 	 50803201 	 1000 	 5.496527433395386 	 2.120455503463745 	 0.0032334327697753906 	 0.0013132095336914062 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:08:53.945959 test begin: paddle.complex(Tensor([1, 793801, 64],"float32"), Tensor([1, 793801, 64],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([1, 793801, 64],"float32"), Tensor([1, 793801, 64],"float32"), ) 	 101606528 	 1000 	 0.5927176475524902 	 0.5880570411682129 	 0.5832598209381104 	 0.5746991634368896 	 0.5914921760559082 	 0.07153844833374023 	 0.533571720123291 	 4.124641418457031e-05 	 
2025-07-24 16:08:58.564997 test begin: paddle.complex(Tensor([1, 8192, 6202],"float32"), Tensor([1, 8192, 6202],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([1, 8192, 6202],"float32"), Tensor([1, 8192, 6202],"float32"), ) 	 101613568 	 1000 	 0.5916385650634766 	 0.5879571437835693 	 0.5822279453277588 	 0.574512243270874 	 0.5897564888000488 	 0.0684666633605957 	 0.5322649478912354 	 3.457069396972656e-05 	 
2025-07-24 16:09:02.959837 test begin: paddle.complex(Tensor([1, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([1, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), ) 	 51380224 	 1000 	 0.4907667636871338 	 0.47293925285339355 	 0.4808034896850586 	 0.45892977714538574 	 0.519012451171875 	 0.2874290943145752 	 0.4587385654449463 	 0.20112872123718262 	 
2025-07-24 16:09:06.486406 test begin: paddle.complex(Tensor([20, 2417, 1051],"float32"), Tensor([20, 2417, 1051],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([20, 2417, 1051],"float32"), Tensor([20, 2417, 1051],"float32"), ) 	 101610680 	 1000 	 0.5920140743255615 	 0.5879433155059814 	 0.5827107429504395 	 0.5744986534118652 	 0.5911273956298828 	 0.06780624389648438 	 0.532160758972168 	 3.361701965332031e-05 	 
2025-07-24 16:09:10.794786 test begin: paddle.complex(Tensor([20, 2538, 1001],"float32"), Tensor([20, 2538, 1001],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([20, 2538, 1001],"float32"), Tensor([20, 2538, 1001],"float32"), ) 	 101621520 	 1000 	 0.593151330947876 	 0.5880241394042969 	 0.5838634967803955 	 0.5745751857757568 	 0.5906999111175537 	 0.0678861141204834 	 0.5314092636108398 	 4.673004150390625e-05 	 
2025-07-24 16:09:15.480536 test begin: paddle.complex(Tensor([20, 64, 39691],"float32"), Tensor([20, 64, 39691],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([20, 64, 39691],"float32"), Tensor([20, 64, 39691],"float32"), ) 	 101608960 	 1000 	 0.591637134552002 	 0.5880296230316162 	 0.5822410583496094 	 0.5681338310241699 	 0.5909435749053955 	 0.10776877403259277 	 0.5218842029571533 	 0.0004718303680419922 	 
2025-07-24 16:09:20.179260 test begin: paddle.complex(Tensor([756, 64, 1051],"float32"), Tensor([756, 64, 1051],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([756, 64, 1051],"float32"), Tensor([756, 64, 1051],"float32"), ) 	 101703168 	 1000 	 0.5922832489013672 	 0.5898563861846924 	 0.5829112529754639 	 0.5747885704040527 	 0.5911879539489746 	 0.06789827346801758 	 0.533635139465332 	 4.076957702636719e-05 	 
2025-07-24 16:09:24.676190 test begin: paddle.complex(Tensor([794, 64, 1001],"float32"), Tensor([794, 64, 1001],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([794, 64, 1001],"float32"), Tensor([794, 64, 1001],"float32"), ) 	 101733632 	 1000 	 0.592980146408081 	 0.5967390537261963 	 0.5836343765258789 	 0.5710833072662354 	 0.5909290313720703 	 0.11306977272033691 	 0.531773567199707 	 8.392333984375e-05 	 
2025-07-24 16:09:32.505071 test begin: paddle.complex(Tensor([97, 8192, 64],"float32"), Tensor([1, 8192, 64],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([97, 8192, 64],"float32"), Tensor([1, 8192, 64],"float32"), ) 	 51380224 	 1000 	 0.48293495178222656 	 1.1417205333709717 	 0.47296690940856934 	 0.4542272090911865 	 0.5159206390380859 	 0.2871885299682617 	 0.4557352066040039 	 0.20375847816467285 	 
2025-07-24 16:09:39.146369 test begin: paddle.complex(Tensor([97, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), )
[Prof] paddle.complex 	 paddle.complex(Tensor([97, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), ) 	 101711872 	 1000 	 0.5922837257385254 	 0.5885019302368164 	 0.5829639434814453 	 0.575045108795166 	 0.5904209613800049 	 0.06930732727050781 	 0.5329139232635498 	 5.054473876953125e-05 	 
2025-07-24 16:09:43.564017 test begin: paddle.concat(list[Tensor([101606401],"bfloat16"),], )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([101606401],"bfloat16"),], ) 	 101606401 	 1000 	 0.3085145950317383 	 0.3133211135864258 	 0.15763640403747559 	 0.1599745750427246 	 0.6243271827697754 	 0.45365071296691895 	 0.3189389705657959 	 0.3770420551300049 	 
2025-07-24 16:09:48.408717 test begin: paddle.concat(list[Tensor([254, 32, 112, 112],"float16"),Tensor([254, 32, 112, 112],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([254, 32, 112, 112],"float16"),Tensor([254, 32, 112, 112],"float16"),], axis=1, ) 	 203915264 	 1000 	 0.610095739364624 	 0.9073622226715088 	 0.59694504737854 	 0.891892671585083 	 0.9360153675079346 	 0.061453819274902344 	 0.8749964237213135 	 3.2901763916015625e-05 	 
2025-07-24 16:09:58.341455 test begin: paddle.concat(list[Tensor([512, 16, 112, 112],"float16"),Tensor([512, 16, 112, 112],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([512, 16, 112, 112],"float16"),Tensor([512, 16, 112, 112],"float16"),], axis=1, ) 	 205520896 	 1000 	 0.6150293350219727 	 0.9163539409637451 	 0.6021203994750977 	 0.8998956680297852 	 0.9356095790863037 	 0.06382298469543457 	 0.8689303398132324 	 6.866455078125e-05 	 
2025-07-24 16:10:08.759182 test begin: paddle.concat(list[Tensor([512, 16, 112, 112],"float16"),Tensor([512, 32, 112, 112],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([512, 16, 112, 112],"float16"),Tensor([512, 32, 112, 112],"float16"),], axis=1, ) 	 308281344 	 1000 	 0.9244861602783203 	 1.609821081161499 	 0.9112694263458252 	 1.5956940650939941 	 1.42122220993042 	 0.06165671348571777 	 1.3598079681396484 	 3.910064697265625e-05 	 
2025-07-24 16:10:24.128846 test begin: paddle.concat(list[Tensor([512, 32, 112, 112],"float16"),Tensor([512, 16, 112, 112],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([512, 32, 112, 112],"float16"),Tensor([512, 16, 112, 112],"float16"),], axis=1, ) 	 308281344 	 1000 	 0.9222779273986816 	 1.9900059700012207 	 0.9092457294464111 	 1.5249722003936768 	 1.42364501953125 	 0.08771729469299316 	 1.362781286239624 	 0.00010776519775390625 	 
2025-07-24 16:10:42.124480 test begin: paddle.concat(list[Tensor([512, 32, 112, 56],"float16"),Tensor([512, 32, 112, 56],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([512, 32, 112, 56],"float16"),Tensor([512, 32, 112, 56],"float16"),], axis=1, ) 	 205520896 	 1000 	 0.6148321628570557 	 0.9147934913635254 	 0.6016006469726562 	 0.9008846282958984 	 0.9356639385223389 	 0.062467098236083984 	 0.8746762275695801 	 4.4345855712890625e-05 	 
2025-07-24 16:10:53.325879 test begin: paddle.concat(list[Tensor([512, 32, 56, 112],"float16"),Tensor([512, 32, 56, 112],"float16"),], axis=1, )
[Prof] paddle.concat 	 paddle.concat(list[Tensor([512, 32, 56, 112],"float16"),Tensor([512, 32, 56, 112],"float16"),], axis=1, ) 	 205520896 	 1000 	 0.614971399307251 	 0.9144961833953857 	 0.601874589920044 	 0.9005646705627441 	 0.9356181621551514 	 0.062479496002197266 	 0.8705668449401855 	 4.124641418457031e-05 	 
2025-07-24 16:11:03.895462 test begin: paddle.conj(Tensor([2, 20, 2, 635041],"float32"), )
[Prof] paddle.conj 	 paddle.conj(Tensor([2, 20, 2, 635041],"float32"), ) 	 50803280 	 1000 	 0.3074381351470947 	 0.0023059844970703125 	 0.29909324645996094 	 1.9073486328125e-05 	 0.307849645614624 	 0.06440281867980957 	 0.25014829635620117 	 0.00019979476928710938 	 
2025-07-24 16:11:07.452134 test begin: paddle.conj(Tensor([2, 20, 423361, 3],"float32"), )
[Prof] paddle.conj 	 paddle.conj(Tensor([2, 20, 423361, 3],"float32"), ) 	 50803320 	 1000 	 0.3074951171875 	 0.0017800331115722656 	 0.29925966262817383 	 1.621246337890625e-05 	 0.3078041076660156 	 0.04490327835083008 	 0.2586100101470947 	 4.4345855712890625e-05 	 
2025-07-24 16:11:09.912326 test begin: paddle.conj(Tensor([2, 4233601, 2, 3],"float32"), )
[Prof] paddle.conj 	 paddle.conj(Tensor([2, 4233601, 2, 3],"float32"), ) 	 50803212 	 1000 	 0.3093249797821045 	 0.0018205642700195312 	 0.3011350631713867 	 1.6927719116210938e-05 	 0.30910563468933105 	 0.059047698974609375 	 0.2600696086883545 	 6.794929504394531e-05 	 
2025-07-24 16:11:12.169945 test begin: paddle.conj(Tensor([423361, 20, 2, 3],"float32"), )
[Prof] paddle.conj 	 paddle.conj(Tensor([423361, 20, 2, 3],"float32"), ) 	 50803320 	 1000 	 0.307448148727417 	 0.0017883777618408203 	 0.299243688583374 	 1.6450881958007812e-05 	 0.3077991008758545 	 0.0444788932800293 	 0.2586698532104492 	 3.695487976074219e-05 	 
2025-07-24 16:11:14.391245 test begin: paddle.copysign(Tensor([12, 1058401, 2],"float64"), Tensor([12, 1058401, 2],"float64"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([12, 1058401, 2],"float64"), Tensor([12, 1058401, 2],"float64"), ) 	 50803248 	 1000 	 0.44902896881103516 	 0.44325947761535645 	 0.43620896339416504 	 0.43181848526000977 	 0.7407891750335693 	 1.516991376876831 	 0.681354284286499 	 0.3112485408782959 	 
2025-07-24 16:11:19.563068 test begin: paddle.copysign(Tensor([12, 20, 105841],"float64"), Tensor([12, 20, 105841],"float64"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([12, 20, 105841],"float64"), Tensor([12, 20, 105841],"float64"), ) 	 50803680 	 1000 	 0.4488675594329834 	 0.443103551864624 	 0.43622779846191406 	 0.4315066337585449 	 0.7406497001647949 	 1.5154738426208496 	 0.6800038814544678 	 0.30989837646484375 	 
2025-07-24 16:11:24.194426 test begin: paddle.copysign(Tensor([12, 20, 211681],"float32"), Tensor([12, 20, 211681],"float32"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([12, 20, 211681],"float32"), Tensor([12, 20, 211681],"float32"), ) 	 101606880 	 1000 	 0.45067405700683594 	 0.446835994720459 	 0.43784165382385254 	 0.4352076053619385 	 1.193103551864624 	 1.5526957511901855 	 1.133671522140503 	 0.31758928298950195 	 
2025-07-24 16:11:30.285571 test begin: paddle.copysign(Tensor([12, 2116801, 2],"float32"), Tensor([12, 2116801, 2],"float32"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([12, 2116801, 2],"float32"), Tensor([12, 2116801, 2],"float32"), ) 	 101606448 	 1000 	 0.45039796829223633 	 0.44687342643737793 	 0.4376375675201416 	 0.43519043922424316 	 1.1963810920715332 	 1.552952766418457 	 1.1359977722167969 	 0.3176422119140625 	 
2025-07-24 16:11:37.637261 test begin: paddle.copysign(Tensor([1270081, 20, 2],"float32"), Tensor([1270081, 20, 2],"float32"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([1270081, 20, 2],"float32"), Tensor([1270081, 20, 2],"float32"), ) 	 101606480 	 1000 	 0.45037174224853516 	 0.4468262195587158 	 0.429394006729126 	 0.42940735816955566 	 1.1961917877197266 	 1.553039312362671 	 1.127835988998413 	 0.31768083572387695 	 
2025-07-24 16:11:44.962277 test begin: paddle.copysign(Tensor([635041, 20, 2],"float64"), Tensor([635041, 20, 2],"float64"), )
[Prof] paddle.copysign 	 paddle.copysign(Tensor([635041, 20, 2],"float64"), Tensor([635041, 20, 2],"float64"), ) 	 50803280 	 1000 	 0.44891357421875 	 0.4582500457763672 	 0.42822861671447754 	 0.42051053047180176 	 0.7405228614807129 	 1.5170507431030273 	 0.6719512939453125 	 0.3098471164703369 	 
2025-07-24 16:11:49.859508 test begin: paddle.cos(Tensor([1587601, 32],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([1587601, 32],"float32"), ) 	 50803232 	 1000 	 0.2955207824707031 	 0.29836463928222656 	 0.2732667922973633 	 0.27964329719543457 	 0.450624942779541 	 1.0410518646240234 	 0.38805246353149414 	 0.35464000701904297 	 
2025-07-24 16:11:53.592308 test begin: paddle.cos(Tensor([198451, 256],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([198451, 256],"float32"), ) 	 50803456 	 1000 	 0.29543471336364746 	 0.29834723472595215 	 0.2792649269104004 	 0.27668094635009766 	 0.4506556987762451 	 1.0410635471343994 	 0.3840353488922119 	 0.35465502738952637 	 
2025-07-24 16:11:57.271181 test begin: paddle.cos(Tensor([32768, 1551],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([32768, 1551],"float32"), ) 	 50823168 	 1000 	 0.2955331802368164 	 0.2983741760253906 	 0.2794044017791748 	 0.281003475189209 	 0.4506227970123291 	 1.0414552688598633 	 0.3864285945892334 	 0.35483837127685547 	 
2025-07-24 16:12:01.018131 test begin: paddle.cos(Tensor([396901, 128],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([396901, 128],"float32"), ) 	 50803328 	 1000 	 0.29532718658447266 	 0.2982513904571533 	 0.2790822982788086 	 0.2807009220123291 	 0.4503345489501953 	 1.041092872619629 	 0.3869192600250244 	 0.35469722747802734 	 
2025-07-24 16:12:04.758315 test begin: paddle.cos(Tensor([5000, 10161],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([5000, 10161],"float32"), ) 	 50805000 	 1000 	 0.2954404354095459 	 0.29827404022216797 	 0.2793092727661133 	 0.2806828022003174 	 0.45052552223205566 	 1.0410096645355225 	 0.38210153579711914 	 0.35467028617858887 	 
2025-07-24 16:12:08.496244 test begin: paddle.cos(Tensor([8192, 6202],"float32"), )
[Prof] paddle.cos 	 paddle.cos(Tensor([8192, 6202],"float32"), ) 	 50806784 	 1000 	 0.2953472137451172 	 0.29825735092163086 	 0.2791016101837158 	 0.28035473823547363 	 0.45069336891174316 	 1.0411632061004639 	 0.38446927070617676 	 0.3546769618988037 	 
2025-07-24 16:12:12.228473 test begin: paddle.cosh(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 1000 	 0.29489588737487793 	 0.2988557815551758 	 0.2787818908691406 	 0.28154563903808594 	 0.4506032466888428 	 0.7434172630310059 	 0.38848114013671875 	 0.3798341751098633 	 
2025-07-24 16:12:15.654439 test begin: paddle.cosh(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 1000 	 0.294846773147583 	 0.2988014221191406 	 0.2785196304321289 	 0.28766393661499023 	 0.4504990577697754 	 0.7434625625610352 	 0.38833022117614746 	 0.37986278533935547 	 
2025-07-24 16:12:19.050784 test begin: paddle.cosh(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 1000 	 0.2948191165924072 	 0.2987189292907715 	 0.278613805770874 	 0.28134989738464355 	 0.4504096508026123 	 0.7433910369873047 	 0.3854641914367676 	 0.3798258304595947 	 
2025-07-24 16:12:22.495520 test begin: paddle.cosh(Tensor([28, 32, 241, 241],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([28, 32, 241, 241],"float32"), ) 	 52040576 	 1000 	 0.3015453815460205 	 0.3060119152069092 	 0.2853360176086426 	 0.28748345375061035 	 0.4609966278076172 	 0.7613892555236816 	 0.3984673023223877 	 0.38901782035827637 	 
2025-07-24 16:12:25.994514 test begin: paddle.cosh(Tensor([8, 110, 241, 241],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([8, 110, 241, 241],"float32"), ) 	 51111280 	 1000 	 0.2964057922363281 	 0.30053162574768066 	 0.28023290634155273 	 0.2828199863433838 	 0.452923059463501 	 0.747809886932373 	 0.3884315490722656 	 0.38208794593811035 	 
2025-07-24 16:12:29.544533 test begin: paddle.cosh(Tensor([8, 32, 241, 824],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([8, 32, 241, 824],"float32"), ) 	 50837504 	 1000 	 0.2953529357910156 	 0.2989342212677002 	 0.2861487865447998 	 0.28758692741394043 	 0.4507296085357666 	 0.7438905239105225 	 0.39411139488220215 	 0.38007211685180664 	 
2025-07-24 16:12:32.922448 test begin: paddle.cosh(Tensor([8, 32, 824, 241],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(Tensor([8, 32, 824, 241],"float32"), ) 	 50837504 	 1000 	 0.29534220695495605 	 1.4773828983306885 	 0.2864079475402832 	 0.2805619239807129 	 0.45064592361450195 	 0.7439343929290771 	 0.3969414234161377 	 0.38007307052612305 	 
2025-07-24 16:12:39.545286 test begin: paddle.cosh(x=Tensor([3, 3, 5644801],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(x=Tensor([3, 3, 5644801],"float32"), ) 	 50803209 	 1000 	 0.29483771324157715 	 0.2988302707672119 	 0.2783620357513428 	 0.28139448165893555 	 0.45036983489990234 	 0.7434408664703369 	 0.3841893672943115 	 0.3798494338989258 	 
2025-07-24 16:12:42.945112 test begin: paddle.cosh(x=Tensor([3, 5644801, 3],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(x=Tensor([3, 5644801, 3],"float32"), ) 	 50803209 	 1000 	 0.2948424816131592 	 0.2987504005432129 	 0.27829790115356445 	 0.2811756134033203 	 0.4505038261413574 	 0.7434148788452148 	 0.3825249671936035 	 0.37986254692077637 	 
2025-07-24 16:12:46.387532 test begin: paddle.cosh(x=Tensor([5644801, 3, 3],"float32"), )
[Prof] paddle.cosh 	 paddle.cosh(x=Tensor([5644801, 3, 3],"float32"), ) 	 50803209 	 1000 	 0.29480671882629395 	 0.7531969547271729 	 0.2779409885406494 	 0.28056836128234863 	 0.4504990577697754 	 0.7434048652648926 	 0.38857102394104004 	 0.37981224060058594 	 
2025-07-24 16:12:52.607933 test begin: paddle.count_nonzero(Tensor([1, 14, 129601, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 14, 129601, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, ) 	 25401796 	 1000 	 0.6063463687896729 	 0.5260641574859619 	 0.20660972595214844 	 0.17927074432373047 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:12:54.806463 test begin: paddle.count_nonzero(Tensor([1, 14, 129601, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 14, 129601, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, ) 	 25401796 	 1000 	 0.6063201427459717 	 0.5258965492248535 	 0.20660901069641113 	 0.17917585372924805 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 1, 129601, 1]) and output[0] has a shape of torch.Size([1, 129601]).
2025-07-24 16:12:56.969258 test begin: paddle.count_nonzero(Tensor([1, 14, 5, 362881],"float64"), axis=list[1,3,], keepdim=False, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 14, 5, 362881],"float64"), axis=list[1,3,], keepdim=False, name=None, ) 	 25401670 	 1000 	 0.6131227016448975 	 0.5436317920684814 	 0.15668725967407227 	 0.13873553276062012 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:12:59.109494 test begin: paddle.count_nonzero(Tensor([1, 14, 5, 362881],"float64"), axis=list[1,3,], keepdim=True, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 14, 5, 362881],"float64"), axis=list[1,3,], keepdim=True, name=None, ) 	 25401670 	 1000 	 0.6131963729858398 	 0.5435271263122559 	 0.15667080879211426 	 0.13883209228515625 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 1, 5, 1]) and output[0] has a shape of torch.Size([1, 5]).
2025-07-24 16:13:01.290313 test begin: paddle.count_nonzero(Tensor([1, 362881, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 362881, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, ) 	 25401670 	 1000 	 0.6120150089263916 	 0.5524313449859619 	 0.1563582420349121 	 0.14109373092651367 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:13:03.491876 test begin: paddle.count_nonzero(Tensor([1, 362881, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([1, 362881, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, ) 	 25401670 	 1000 	 0.6119353771209717 	 0.5522849559783936 	 0.15632128715515137 	 0.14104366302490234 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 1, 5, 1]) and output[0] has a shape of torch.Size([1, 5]).
2025-07-24 16:13:05.651751 test begin: paddle.count_nonzero(Tensor([2, 1270081, 4, 5],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([2, 1270081, 4, 5],"float32"), axis=-1, keepdim=False, ) 	 50803240 	 1000 	 0.97503662109375 	 1.062150239944458 	 0.3323702812194824 	 0.362001895904541 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:13:09.585278 test begin: paddle.count_nonzero(Tensor([2, 3, 1693441, 5],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([2, 3, 1693441, 5],"float32"), axis=-1, keepdim=False, ) 	 50803230 	 1000 	 0.9748895168304443 	 1.062178373336792 	 0.33219289779663086 	 0.36202120780944824 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:13:13.535171 test begin: paddle.count_nonzero(Tensor([2, 3, 4, 2116801],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([2, 3, 4, 2116801],"float32"), axis=-1, keepdim=False, ) 	 50803224 	 1000 	 0.8761000633239746 	 0.8671455383300781 	 0.2237563133239746 	 0.22150897979736328 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:13:16.908715 test begin: paddle.count_nonzero(Tensor([25921, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([25921, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, ) 	 25402580 	 1000 	 0.5940499305725098 	 0.5274810791015625 	 0.20237112045288086 	 0.17970705032348633 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:13:19.038671 test begin: paddle.count_nonzero(Tensor([25921, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([25921, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, ) 	 25402580 	 1000 	 0.5941126346588135 	 0.5275082588195801 	 0.2023909091949463 	 0.17971563339233398 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([25921, 1, 5, 1]) and output[0] has a shape of torch.Size([25921, 5]).
2025-07-24 16:13:21.177527 test begin: paddle.count_nonzero(Tensor([846721, 3, 4, 5],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.count_nonzero 	 paddle.count_nonzero(Tensor([846721, 3, 4, 5],"float32"), axis=-1, keepdim=False, ) 	 50803260 	 1000 	 0.9751002788543701 	 1.0621232986450195 	 0.3323192596435547 	 0.36198997497558594 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:13:25.084565 test begin: paddle.crop(x=Tensor([16934401, 3],"float32"), shape=list[2,2,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([16934401, 3],"float32"), shape=list[2,2,], ) 	 50803203 	 1000 	 0.03206467628479004 	 0.013425827026367188 	 2.765655517578125e-05 	 2.2172927856445312e-05 	 0.1536564826965332 	 0.14302492141723633 	 0.08852410316467285 	 0.03650617599487305 	 combined
2025-07-24 16:13:26.213793 test begin: paddle.crop(x=Tensor([2, 1411201, 3, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([2, 1411201, 3, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 	 25401618 	 1000 	 0.02020430564880371 	 0.018709421157836914 	 1.6927719116210938e-05 	 5.5789947509765625e-05 	 0.16410017013549805 	 0.15420317649841309 	 0.11207938194274902 	 0.00011134147644042969 	 combined
2025-07-24 16:13:27.066816 test begin: paddle.crop(x=Tensor([2, 3, 1411201, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([2, 3, 1411201, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 	 25401618 	 1000 	 0.09199905395507812 	 0.02623605728149414 	 0.06453537940979004 	 2.3126602172851562e-05 	 0.22661685943603516 	 0.5869288444519043 	 0.1658000946044922 	 0.09998154640197754 	 combined
2025-07-24 16:13:28.663169 test begin: paddle.crop(x=Tensor([2, 3, 3, 1411201],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([2, 3, 3, 1411201],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 	 25401618 	 1000 	 0.0272519588470459 	 0.026996612548828125 	 1.8835067749023438e-05 	 5.340576171875e-05 	 0.19983816146850586 	 0.3983614444732666 	 0.12755298614501953 	 0.06793999671936035 	 combined
2025-07-24 16:13:29.814243 test begin: paddle.crop(x=Tensor([3, 16934401],"float32"), shape=list[2,2,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([3, 16934401],"float32"), shape=list[2,2,], ) 	 50803203 	 1000 	 0.028638124465942383 	 0.020598411560058594 	 2.0742416381835938e-05 	 6.222724914550781e-05 	 0.1847062110900879 	 0.4396822452545166 	 0.12255501747131348 	 0.08985543251037598 	 combined
2025-07-24 16:13:31.282061 test begin: paddle.crop(x=Tensor([3, 8467201],"float64"), shape=list[2,2,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([3, 8467201],"float64"), shape=list[2,2,], ) 	 25401603 	 1000 	 0.01909184455871582 	 0.01320648193359375 	 1.1920928955078125e-05 	 2.288818359375e-05 	 0.1482696533203125 	 0.4403715133666992 	 0.09659552574157715 	 0.09000444412231445 	 combined
2025-07-24 16:13:32.399283 test begin: paddle.crop(x=Tensor([8467201, 3],"float64"), shape=list[2,2,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([8467201, 3],"float64"), shape=list[2,2,], ) 	 25401603 	 1000 	 0.028171539306640625 	 0.020266056060791016 	 1.5020370483398438e-05 	 2.4080276489257812e-05 	 0.1515812873840332 	 0.14352726936340332 	 0.09058761596679688 	 0.036675214767456055 	 combined
2025-07-24 16:13:33.309780 test begin: paddle.crop(x=Tensor([940801, 3, 3, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )
[Prof] paddle.crop 	 paddle.crop(x=Tensor([940801, 3, 3, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 	 25401627 	 1000 	 0.018534421920776367 	 0.019357681274414062 	 1.1682510375976562e-05 	 2.4557113647460938e-05 	 0.15190720558166504 	 0.15482831001281738 	 0.10009503364562988 	 0.011836051940917969 	 combined
2025-07-24 16:13:34.176402 test begin: paddle.cross(x=Tensor([2822401, 3, 3],"float64"), y=Tensor([2822401, 3, 3],"float64"), axis=1, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([2822401, 3, 3],"float64"), y=Tensor([2822401, 3, 3],"float64"), axis=1, ) 	 50803218 	 1000 	 0.4504420757293701 	 0.4514491558074951 	 0.4392850399017334 	 0.43530750274658203 	 0.7493488788604736 	 0.8992979526519775 	 0.6823158264160156 	 0.45946383476257324 	 
2025-07-24 16:13:40.005300 test begin: paddle.cross(x=Tensor([2822401, 3, 3],"float64"), y=Tensor([2822401, 3, 3],"float64"), axis=2, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([2822401, 3, 3],"float64"), y=Tensor([2822401, 3, 3],"float64"), axis=2, ) 	 50803218 	 1000 	 0.45069074630737305 	 0.4510502815246582 	 0.44011569023132324 	 0.436765193939209 	 0.7563567161560059 	 0.9016082286834717 	 0.6677913665771484 	 0.4606664180755615 	 
2025-07-24 16:13:44.126956 test begin: paddle.cross(x=Tensor([3, 2822401, 3],"float64"), y=Tensor([3, 2822401, 3],"float64"), axis=0, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([3, 2822401, 3],"float64"), y=Tensor([3, 2822401, 3],"float64"), axis=0, ) 	 50803218 	 1000 	 0.4473261833190918 	 0.4488985538482666 	 0.42902207374572754 	 0.4275808334350586 	 0.7393479347229004 	 0.8971211910247803 	 0.6703953742980957 	 0.45842838287353516 	 
2025-07-24 16:13:48.194014 test begin: paddle.cross(x=Tensor([3, 2822401, 3],"float64"), y=Tensor([3, 2822401, 3],"float64"), axis=2, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([3, 2822401, 3],"float64"), y=Tensor([3, 2822401, 3],"float64"), axis=2, ) 	 50803218 	 1000 	 0.4506542682647705 	 0.4522891044616699 	 0.44001054763793945 	 0.4359617233276367 	 0.7565245628356934 	 0.9017486572265625 	 0.6965041160583496 	 0.4607117176055908 	 
2025-07-24 16:13:52.327444 test begin: paddle.cross(x=Tensor([3, 3, 2822401],"float64"), y=Tensor([3, 3, 2822401],"float64"), axis=0, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([3, 3, 2822401],"float64"), y=Tensor([3, 3, 2822401],"float64"), axis=0, ) 	 50803218 	 1000 	 0.4472932815551758 	 0.4579007625579834 	 0.4365561008453369 	 0.4333686828613281 	 0.7393879890441895 	 0.8971178531646729 	 0.6801929473876953 	 0.45841097831726074 	 
2025-07-24 16:13:58.396258 test begin: paddle.cross(x=Tensor([3, 3, 2822401],"float64"), y=Tensor([3, 3, 2822401],"float64"), axis=1, )
[Prof] paddle.cross 	 paddle.cross(x=Tensor([3, 3, 2822401],"float64"), y=Tensor([3, 3, 2822401],"float64"), axis=1, ) 	 50803218 	 1000 	 0.4477553367614746 	 0.4481239318847656 	 0.4285392761230469 	 0.4264028072357178 	 0.7406084537506104 	 0.8962550163269043 	 0.6713461875915527 	 0.4578735828399658 	 
2025-07-24 16:14:02.563341 test begin: paddle.cummax(Tensor([100, 508033],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcffd7ea8c0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 17:14:08.691581 test begin: paddle.cummax(Tensor([100, 508033],"float32"), axis=-1, )
W0724 17:14:09.677325 149588 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cummax 	 paddle.cummax(Tensor([100, 508033],"float32"), axis=-1, ) 	 50803300 	 1000 	 59.20209765434265 	 2.549232244491577 	 59.18798208236694 	 2.5333175659179688 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 17:16:16.159207 test begin: paddle.cummax(Tensor([100, 508033],"float32"), axis=-2, )
[Prof] paddle.cummax 	 paddle.cummax(Tensor([100, 508033],"float32"), axis=-2, ) 	 50803300 	 1000 	 0.7939138412475586 	 0.7906744480133057 	 0.7786300182342529 	 0.7756531238555908 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 17:16:26.366011 test begin: paddle.cummax(Tensor([508033, 100],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcad7467100>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 18:16:32.524729 test begin: paddle.cummax(Tensor([508033, 100],"float32"), axis=-1, )
W0724 18:16:33.545403 51265 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cummax 	 paddle.cummax(Tensor([508033, 100],"float32"), axis=-1, ) 	 50803300 	 1000 	 1.3347392082214355 	 8.571202993392944 	 1.313924789428711 	 8.515887260437012 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 18:16:48.643601 test begin: paddle.cummax(Tensor([508033, 100],"float32"), axis=-2, )
[Prof] paddle.cummax 	 paddle.cummax(Tensor([508033, 100],"float32"), axis=-2, ) 	 50803300 	 1000 	 251.04576516151428 	 250.73236918449402 	 251.03256630897522 	 250.70059156417847 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 18:25:18.531457 test begin: paddle.cummin(Tensor([100, 508033],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f996bef3100>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 19:25:25.542209 test begin: paddle.cummin(Tensor([100, 508033],"float32"), axis=-1, )
W0724 19:25:26.565740 16869 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cummin 	 paddle.cummin(Tensor([100, 508033],"float32"), axis=-1, ) 	 50803300 	 1000 	 59.13357329368591 	 2.551856756210327 	 59.12043213844299 	 2.536142349243164 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 19:27:32.741422 test begin: paddle.cummin(Tensor([100, 508033],"float32"), axis=-2, )
[Prof] paddle.cummin 	 paddle.cummin(Tensor([100, 508033],"float32"), axis=-2, ) 	 50803300 	 1000 	 0.7920279502868652 	 0.7908570766448975 	 0.778843879699707 	 0.7652201652526855 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 19:27:43.002639 test begin: paddle.cummin(Tensor([508033, 100],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f69edcb7100>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 20:27:50.960706 test begin: paddle.cummin(Tensor([508033, 100],"float32"), axis=-1, )
W0724 20:27:51.958885 160982 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cummin 	 paddle.cummin(Tensor([508033, 100],"float32"), axis=-1, ) 	 50803300 	 1000 	 1.3379042148590088 	 8.554044008255005 	 1.3133783340454102 	 8.530053853988647 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 20:28:05.848090 test begin: paddle.cummin(Tensor([508033, 100],"float32"), axis=-2, )
[Prof] paddle.cummin 	 paddle.cummin(Tensor([508033, 100],"float32"), axis=-2, ) 	 50803300 	 1000 	 251.09490418434143 	 250.8316593170166 	 251.07994079589844 	 250.81659483909607 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 20:36:33.984902 test begin: paddle.cumprod(Tensor([2, 127009, 10, 10],"float64"), 1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 127009, 10, 10],"float64"), 1, ) 	 25401800 	 1000 	 66.12796473503113 	 64.37107276916504 	 66.11240100860596 	 64.35633850097656 	 258.2581593990326 	 66.1702721118927 	 0.06594133377075195 	 0.06529760360717773 	 
2025-07-24 20:44:10.650575 test begin: paddle.cumprod(Tensor([2, 3, 10, 423361],"float64"), 1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 10, 423361],"float64"), 1, ) 	 25401660 	 1000 	 0.30251455307006836 	 0.3028099536895752 	 0.2940385341644287 	 0.2919454574584961 	 2.8542709350585938 	 2.041694164276123 	 0.0003502368927001953 	 0.0013020038604736328 	 
2025-07-24 20:44:17.293148 test begin: paddle.cumprod(Tensor([2, 3, 3, 4, 705601],"float32"), dim=0, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 3, 4, 705601],"float32"), dim=0, ) 	 50803272 	 1000 	 0.3296070098876953 	 0.3117055892944336 	 0.3208756446838379 	 0.2940809726715088 	 3.409273147583008 	 2.1144726276397705 	 0.0004870891571044922 	 0.0013189315795898438 	 
2025-07-24 20:44:25.154948 test begin: paddle.cumprod(Tensor([2, 3, 3, 4, 705601],"float32"), dim=1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 3, 4, 705601],"float32"), dim=1, ) 	 50803272 	 1000 	 0.3226451873779297 	 0.31797194480895996 	 0.3138437271118164 	 0.30709195137023926 	 3.271131992340088 	 2.1335654258728027 	 0.00040531158447265625 	 0.0013468265533447266 	 
2025-07-24 20:44:32.981169 test begin: paddle.cumprod(Tensor([2, 3, 3, 564481, 5],"float32"), dim=0, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 3, 564481, 5],"float32"), dim=0, ) 	 50803290 	 1000 	 0.343181848526001 	 0.31906604766845703 	 0.32108616828918457 	 0.30069661140441895 	 3.4111387729644775 	 2.1329877376556396 	 0.0004756450653076172 	 0.0013005733489990234 	 
2025-07-24 20:44:42.067067 test begin: paddle.cumprod(Tensor([2, 3, 3, 564481, 5],"float32"), dim=1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 3, 564481, 5],"float32"), dim=1, ) 	 50803290 	 1000 	 0.3236725330352783 	 0.5571718215942383 	 0.30715274810791016 	 0.3075833320617676 	 3.2721095085144043 	 2.1402108669281006 	 0.0004184246063232422 	 0.0013239383697509766 	 
2025-07-24 20:44:51.411536 test begin: paddle.cumprod(Tensor([2, 3, 423361, 10],"float64"), 1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 423361, 10],"float64"), 1, ) 	 25401660 	 1000 	 0.3164651393890381 	 0.3027536869049072 	 0.29569053649902344 	 0.29177188873291016 	 2.849050283432007 	 2.0513646602630615 	 0.0003485679626464844 	 0.0011982917785644531 	 
2025-07-24 20:44:58.041851 test begin: paddle.cumprod(Tensor([2, 3, 423361, 4, 5],"float32"), dim=0, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 423361, 4, 5],"float32"), dim=0, ) 	 50803320 	 1000 	 0.3296678066253662 	 0.3117990493774414 	 0.32102012634277344 	 0.30083322525024414 	 3.4164621829986572 	 2.1306326389312744 	 0.00047469139099121094 	 0.001317739486694336 	 
2025-07-24 20:45:06.040911 test begin: paddle.cumprod(Tensor([2, 3, 423361, 4, 5],"float32"), dim=1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 3, 423361, 4, 5],"float32"), dim=1, ) 	 50803320 	 1000 	 0.32224464416503906 	 0.31824350357055664 	 0.3067741394042969 	 0.30065107345581055 	 3.2821106910705566 	 2.135707378387451 	 0.0004143714904785156 	 0.0013294219970703125 	 
2025-07-24 20:45:13.919416 test begin: paddle.cumprod(Tensor([2, 423361, 3, 4, 5],"float32"), dim=0, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([2, 423361, 3, 4, 5],"float32"), dim=0, ) 	 50803320 	 1000 	 0.33100008964538574 	 0.32558679580688477 	 0.3222825527191162 	 0.30063343048095703 	 3.415987968444824 	 2.1196048259735107 	 0.00047397613525390625 	 0.0013124942779541016 	 
2025-07-24 20:45:21.832712 test begin: paddle.cumprod(Tensor([2, 423361, 3, 4, 5],"float32"), dim=1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4f2b11a980>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753361722 (unix time) try "date -d @1753361722" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2738f) received by PID 160655 (TID 0x7f4f266b7640) from PID 160655 ***]

2025-07-24 20:55:29.210270 test begin: paddle.cumprod(Tensor([282241, 3, 3, 4, 5],"float32"), dim=0, )
W0724 20:55:30.372896 72702 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f3abe01ae00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 21:05:34.078203 test begin: paddle.cumprod(Tensor([282241, 3, 3, 4, 5],"float32"), dim=1, )
W0724 21:05:35.177933 98217 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([282241, 3, 3, 4, 5],"float32"), dim=1, ) 	 50803380 	 1000 	 0.33069491386413574 	 0.3229689598083496 	 0.3182961940765381 	 0.30510425567626953 	 3.2684133052825928 	 2.1437742710113525 	 0.0004169940948486328 	 0.0013358592987060547 	 
2025-07-24 21:05:43.689635 test begin: paddle.cumprod(Tensor([84673, 3, 10, 10],"float64"), 1, )
[Prof] paddle.cumprod 	 paddle.cumprod(Tensor([84673, 3, 10, 10],"float64"), 1, ) 	 25401900 	 1000 	 0.30435776710510254 	 0.3042640686035156 	 0.29401135444641113 	 0.286008358001709 	 2.85779070854187 	 2.04421067237854 	 0.0003459453582763672 	 0.0012340545654296875 	 
2025-07-24 21:05:50.481582 test begin: paddle.cumsum(Tensor([50803201],"float32"), axis=0, )
[Prof] paddle.cumsum 	 paddle.cumsum(Tensor([50803201],"float32"), axis=0, ) 	 50803201 	 1000 	 0.3550107479095459 	 0.3289525508880615 	 4.124641418457031e-05 	 0.16803431510925293 	 0.4008207321166992 	 0.9459936618804932 	 4.1961669921875e-05 	 0.2416858673095703 	 
2025-07-24 21:05:54.308542 test begin: paddle.deg2rad(Tensor([25401601],"int64"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([25401601],"int64"), ) 	 25401601 	 1000 	 0.37919116020202637 	 0.233229398727417 	 0.19372057914733887 	 0.21791362762451172 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:05:56.154366 test begin: paddle.deg2rad(Tensor([50803201],"float32"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([50803201],"float32"), ) 	 50803201 	 1000 	 0.29613232612609863 	 0.2998507022857666 	 0.28168606758117676 	 0.28497934341430664 	 0.2961752414703369 	 0.2988770008087158 	 0.23916244506835938 	 0.23682427406311035 	 
2025-07-24 21:05:59.184708 test begin: paddle.deg2rad(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 1000 	 0.29627060890197754 	 0.2978510856628418 	 0.28172850608825684 	 0.2818634510040283 	 0.2961254119873047 	 0.2976951599121094 	 0.2442770004272461 	 0.23500585556030273 	 
2025-07-24 21:06:02.129703 test begin: paddle.deg2rad(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 1000 	 0.29601144790649414 	 0.2978785037994385 	 0.2816181182861328 	 0.28338050842285156 	 0.29610419273376465 	 0.29775404930114746 	 0.24438929557800293 	 0.23491501808166504 	 
2025-07-24 21:06:05.146423 test begin: paddle.deg2rad(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.deg2rad 	 paddle.deg2rad(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 1000 	 0.29858946800231934 	 0.2978944778442383 	 0.2816140651702881 	 0.2834737300872803 	 0.29608154296875 	 0.2977333068847656 	 0.24404120445251465 	 0.2346501350402832 	 
2025-07-24 21:06:08.024744 test begin: paddle.diag(Tensor([2000, 25402],"float32"), )
[Prof] paddle.diag 	 paddle.diag(Tensor([2000, 25402],"float32"), ) 	 50804000 	 1000 	 0.02046799659729004 	 0.02476668357849121 	 3.170967102050781e-05 	 4.553794860839844e-05 	 0.15033888816833496 	 0.1406106948852539 	 0.06277990341186523 	 0.0697176456451416 	 
2025-07-24 21:06:09.314576 test begin: paddle.diag(Tensor([2000, 25402],"float32"), offset=-1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([2000, 25402],"float32"), offset=-1, ) 	 50804000 	 1000 	 0.008691787719726562 	 0.017222166061401367 	 1.7881393432617188e-05 	 5.53131103515625e-05 	 0.15283703804016113 	 0.1406707763671875 	 0.0780494213104248 	 0.07183122634887695 	 
2025-07-24 21:06:10.459316 test begin: paddle.diag(Tensor([2000, 25402],"float32"), offset=1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([2000, 25402],"float32"), offset=1, ) 	 50804000 	 1000 	 0.008723974227905273 	 0.017179250717163086 	 1.8596649169921875e-05 	 3.62396240234375e-05 	 0.15262961387634277 	 0.14055514335632324 	 0.07785153388977051 	 0.07177734375 	 
2025-07-24 21:06:11.626167 test begin: paddle.diag(Tensor([25402, 2000],"float32"), )
[Prof] paddle.diag 	 paddle.diag(Tensor([25402, 2000],"float32"), ) 	 50804000 	 1000 	 0.011978626251220703 	 0.01882171630859375 	 4.00543212890625e-05 	 6.604194641113281e-05 	 0.15308570861816406 	 0.1391136646270752 	 0.0781090259552002 	 0.07044696807861328 	 
2025-07-24 21:06:12.782783 test begin: paddle.diag(Tensor([25402, 2000],"float32"), offset=-1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([25402, 2000],"float32"), offset=-1, ) 	 50804000 	 1000 	 0.00864720344543457 	 0.01710057258605957 	 1.0251998901367188e-05 	 3.981590270996094e-05 	 0.15311026573181152 	 0.139862060546875 	 0.07819199562072754 	 0.07158637046813965 	 
2025-07-24 21:06:13.963135 test begin: paddle.diag(Tensor([25402, 2000],"float32"), offset=1, )
[Prof] paddle.diag 	 paddle.diag(Tensor([25402, 2000],"float32"), offset=1, ) 	 50804000 	 1000 	 0.008562326431274414 	 0.017020463943481445 	 1.1205673217773438e-05 	 3.647804260253906e-05 	 0.15321803092956543 	 0.1392049789428711 	 0.07818841934204102 	 0.07024407386779785 	 
2025-07-24 21:06:15.118431 test begin: paddle.diag_embed(Tensor([1058401, 3, 8],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([1058401, 3, 8],"float64"), ) 	 25401624 	 1000 	 2.996797561645508 	 2.6128990650177 	 7.939338684082031e-05 	 1.3350107669830322 	 None 	 None 	 None 	 None 	 
2025-07-24 21:06:21.321038 test begin: paddle.diag_embed(Tensor([1411201, 3, 6],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([1411201, 3, 6],"float64"), ) 	 25401618 	 1000 	 4.550605773925781 	 2.201911687850952 	 0.00014853477478027344 	 1.121994972229004 	 None 	 None 	 None 	 None 	 
2025-07-24 21:06:29.039232 test begin: paddle.diag_embed(Tensor([2, 1058401, 12],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([2, 1058401, 12],"float64"), ) 	 25401624 	 1000 	 4.578846454620361 	 3.84385347366333 	 9.250640869140625e-05 	 0.9807374477386475 	 None 	 None 	 None 	 None 	 
2025-07-24 21:06:39.251633 test begin: paddle.diag_embed(Tensor([2, 1587601, 8],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([2, 1587601, 8],"float64"), ) 	 25401616 	 1000 	 3.667276620864868 	 2.6155242919921875 	 0.0001163482666015625 	 1.3356475830078125 	 None 	 None 	 None 	 None 	 
2025-07-24 21:06:46.150213 test begin: paddle.diag_embed(Tensor([2, 2116801, 6],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([2, 2116801, 6],"float64"), ) 	 25401612 	 1000 	 3.3401951789855957 	 2.1949267387390137 	 9.775161743164062e-05 	 1.1214401721954346 	 None 	 None 	 None 	 None 	 
2025-07-24 21:06:52.294543 test begin: paddle.diag_embed(Tensor([705601, 3, 12],"float64"), )
[Prof] paddle.diag_embed 	 paddle.diag_embed(Tensor([705601, 3, 12],"float64"), ) 	 25401636 	 1000 	 4.595995664596558 	 3.8338897228240967 	 0.0001354217529296875 	 0.979191780090332 	 None 	 None 	 None 	 None 	 
2025-07-24 21:07:01.381876 test begin: paddle.diagonal(x=Tensor([117601, 6, 6, 6],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([117601, 6, 6, 6],"float64"), ) 	 25401816 	 1000 	 0.005534648895263672 	 0.008872747421264648 	 1.1682510375976562e-05 	 6.437301635742188e-05 	 0.14981961250305176 	 0.13879132270812988 	 0.07642960548400879 	 0.06850886344909668 	 
2025-07-24 21:07:02.320874 test begin: paddle.diagonal(x=Tensor([176401, 6, 6, 2, 2],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([176401, 6, 6, 2, 2],"float64"), ) 	 25401744 	 1000 	 0.0037491321563720703 	 0.004445791244506836 	 5.9604644775390625e-06 	 1.811981201171875e-05 	 0.14768743515014648 	 0.13974928855895996 	 0.07542014122009277 	 0.06937456130981445 	 
2025-07-24 21:07:03.174874 test begin: paddle.diagonal(x=Tensor([6, 117601, 6, 6],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([6, 117601, 6, 6],"float64"), ) 	 25401816 	 1000 	 0.0038068294525146484 	 0.004407405853271484 	 1.1920928955078125e-05 	 1.8596649169921875e-05 	 0.14968490600585938 	 0.1389617919921875 	 0.07641792297363281 	 0.06349921226501465 	 
2025-07-24 21:07:04.021179 test begin: paddle.diagonal(x=Tensor([6, 176401, 6, 2, 2],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([6, 176401, 6, 2, 2],"float64"), ) 	 25401744 	 1000 	 0.003732442855834961 	 0.004509925842285156 	 6.9141387939453125e-06 	 1.7881393432617188e-05 	 0.1475827693939209 	 0.13850641250610352 	 0.07528805732727051 	 0.0664055347442627 	 
2025-07-24 21:07:04.864535 test begin: paddle.diagonal(x=Tensor([6, 176401, 6, 2, 2],"float64"), axis1=-1, axis2=2, )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([6, 176401, 6, 2, 2],"float64"), axis1=-1, axis2=2, ) 	 25401744 	 1000 	 0.0038995742797851562 	 0.0046727657318115234 	 6.67572021484375e-06 	 1.8358230590820312e-05 	 0.30571556091308594 	 0.2993175983428955 	 0.15615320205688477 	 0.1529402732849121 	 
2025-07-24 21:07:06.115752 test begin: paddle.diagonal(x=Tensor([6, 6, 117601, 6],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([6, 6, 117601, 6],"float64"), ) 	 25401816 	 1000 	 0.0038242340087890625 	 0.0044155120849609375 	 9.775161743164062e-06 	 1.9073486328125e-05 	 0.1965043544769287 	 0.29601407051086426 	 0.10034322738647461 	 0.15055179595947266 	 
2025-07-24 21:07:07.261471 test begin: paddle.diagonal(x=Tensor([6, 6, 176401, 2, 2],"float64"), axis1=-1, axis2=2, )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([6, 6, 176401, 2, 2],"float64"), axis1=-1, axis2=2, ) 	 25401744 	 1000 	 0.00385284423828125 	 0.004632234573364258 	 9.298324584960938e-06 	 1.7642974853515625e-05 	 0.14729595184326172 	 0.13904619216918945 	 0.0751945972442627 	 0.06907963752746582 	 
2025-07-24 21:07:08.102398 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 117601],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([6, 6, 6, 117601],"float64"), ) 	 25401816 	 1000 	 0.0055921077728271484 	 0.00454401969909668 	 7.867813110351562e-06 	 2.1219253540039062e-05 	 0.19640183448791504 	 0.29474878311157227 	 0.10032129287719727 	 0.15051579475402832 	 
2025-07-24 21:07:09.258956 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 2, 58801],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([6, 6, 6, 2, 58801],"float64"), ) 	 25402032 	 1000 	 0.008011341094970703 	 0.00887155532836914 	 1.0251998901367188e-05 	 6.341934204101562e-05 	 0.20013713836669922 	 0.2929229736328125 	 0.1021728515625 	 0.14960074424743652 	 
2025-07-24 21:07:10.552381 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 2, 58801],"float64"), axis1=-1, axis2=2, )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([6, 6, 6, 2, 58801],"float64"), axis1=-1, axis2=2, ) 	 25402032 	 1000 	 0.003964900970458984 	 0.004710197448730469 	 1.33514404296875e-05 	 1.9311904907226562e-05 	 0.15225887298583984 	 0.14010858535766602 	 0.07779741287231445 	 0.06937694549560547 	 
2025-07-24 21:07:11.400337 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 58801, 2],"float64"), )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([6, 6, 6, 58801, 2],"float64"), ) 	 25402032 	 1000 	 0.008014678955078125 	 0.007554054260253906 	 1.430511474609375e-05 	 2.1219253540039062e-05 	 0.20024514198303223 	 0.29299402236938477 	 0.10225963592529297 	 0.14966034889221191 	 
2025-07-24 21:07:12.577272 test begin: paddle.diagonal(x=Tensor([6, 6, 6, 58801, 2],"float64"), axis1=-1, axis2=2, )
[Prof] paddle.diagonal 	 paddle.diagonal(x=Tensor([6, 6, 6, 58801, 2],"float64"), axis1=-1, axis2=2, ) 	 25402032 	 1000 	 0.003867626190185547 	 0.004717111587524414 	 1.811981201171875e-05 	 1.7642974853515625e-05 	 0.2801527976989746 	 0.2731287479400635 	 0.14313340187072754 	 0.13950061798095703 	 
2025-07-24 21:07:13.762739 test begin: paddle.diagonal_scatter(Tensor([10, 10160641],"int16"), Tensor([10],"int16"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([10, 10160641],"int16"), Tensor([10],"int16"), offset=0, axis1=0, axis2=1, ) 	 101606420 	 1000 	 0.3216674327850342 	 0.31914401054382324 	 0.08194518089294434 	 0.10744309425354004 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:07:16.834290 test begin: paddle.diagonal_scatter(Tensor([10, 5080321],"bool"), Tensor([10],"bool"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([10, 5080321],"bool"), Tensor([10],"bool"), offset=0, axis1=0, axis2=1, ) 	 50803220 	 1000 	 0.09047126770019531 	 0.08712124824523926 	 0.02307868003845215 	 0.02952861785888672 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:07:18.563342 test begin: paddle.diagonal_scatter(Tensor([10, 5080321],"int32"), Tensor([10],"int32"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([10, 5080321],"int32"), Tensor([10],"int32"), offset=0, axis1=0, axis2=1, ) 	 50803220 	 1000 	 0.32163119316101074 	 0.31757545471191406 	 0.0819406509399414 	 0.10885024070739746 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:07:20.733953 test begin: paddle.diagonal_scatter(Tensor([10160641, 10],"int16"), Tensor([10],"int16"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([10160641, 10],"int16"), Tensor([10],"int16"), offset=0, axis1=0, axis2=1, ) 	 101606420 	 1000 	 0.32144904136657715 	 0.3160250186920166 	 0.08190035820007324 	 0.10740876197814941 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:07:23.742414 test begin: paddle.diagonal_scatter(Tensor([5080321, 10],"bool"), Tensor([10],"bool"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([5080321, 10],"bool"), Tensor([10],"bool"), offset=0, axis1=0, axis2=1, ) 	 50803220 	 1000 	 0.09053230285644531 	 0.10150003433227539 	 0.023080825805664062 	 0.029584407806396484 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:07:25.453737 test begin: paddle.diagonal_scatter(Tensor([5080321, 10],"int32"), Tensor([10],"int32"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.diagonal_scatter 	 paddle.diagonal_scatter(Tensor([5080321, 10],"int32"), Tensor([10],"int32"), offset=0, axis1=0, axis2=1, ) 	 50803220 	 1000 	 0.32143306732177734 	 0.3160421848297119 	 0.08190798759460449 	 0.10741424560546875 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:07:27.611100 test begin: paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 1000 	 0.9455115795135498 	 0.2629203796386719 	 0.32205891609191895 	 0.24310660362243652 	 None 	 None 	 None 	 None 	 
2025-07-24 21:07:31.323715 test begin: paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=-2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=-2, ) 	 25401664 	 1000 	 0.9453556537628174 	 0.501983642578125 	 0.32201313972473145 	 0.23228073120117188 	 None 	 None 	 None 	 None 	 
2025-07-24 21:07:34.196553 test begin: paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([396901, 4, 4, 4],"float64"), axis=2, ) 	 25401664 	 1000 	 0.9452834129333496 	 0.9351372718811035 	 0.32202672958374023 	 0.2351670265197754 	 None 	 None 	 None 	 None 	 
2025-07-24 21:07:38.797563 test begin: paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 1000 	 0.8752455711364746 	 0.2639765739440918 	 0.29686880111694336 	 0.24356293678283691 	 None 	 None 	 None 	 None 	 
2025-07-24 21:07:40.497875 test begin: paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=-2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=-2, ) 	 25401664 	 1000 	 0.8714990615844727 	 0.2667832374572754 	 0.29692912101745605 	 0.24321866035461426 	 None 	 None 	 None 	 None 	 
2025-07-24 21:07:42.250980 test begin: paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 396901, 4, 4],"float64"), axis=2, ) 	 25401664 	 1000 	 0.8716366291046143 	 0.26158642768859863 	 0.2969174385070801 	 0.24329304695129395 	 None 	 None 	 None 	 None 	 
2025-07-24 21:07:43.928851 test begin: paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 1000 	 0.8745231628417969 	 0.26160287857055664 	 0.2968466281890869 	 0.24358057975769043 	 None 	 None 	 None 	 None 	 
2025-07-24 21:07:45.626221 test begin: paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=-2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=-2, ) 	 25401664 	 1000 	 1.07108473777771 	 0.29965877532958984 	 0.36495232582092285 	 0.28121185302734375 	 None 	 None 	 None 	 None 	 
2025-07-24 21:07:47.589539 test begin: paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 396901, 4],"float64"), axis=2, ) 	 25401664 	 1000 	 1.0723934173583984 	 0.3031904697418213 	 0.36628293991088867 	 0.2826652526855469 	 None 	 None 	 None 	 None 	 
2025-07-24 21:07:49.625354 test begin: paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 1000 	 1.0709092617034912 	 0.29962730407714844 	 0.3648834228515625 	 0.281571626663208 	 None 	 None 	 None 	 None 	 
2025-07-24 21:07:51.536514 test begin: paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=-2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=-2, ) 	 25401664 	 1000 	 0.8075060844421387 	 0.2628903388977051 	 0.2751150131225586 	 0.24451613426208496 	 None 	 None 	 None 	 None 	 
2025-07-24 21:07:53.150339 test begin: paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=2, )
[Prof] paddle.diff 	 paddle.diff(x=Tensor([4, 4, 4, 396901],"float64"), axis=2, ) 	 25401664 	 1000 	 0.8108022212982178 	 0.2654757499694824 	 0.2750890254974365 	 0.24457502365112305 	 None 	 None 	 None 	 None 	 
2025-07-24 21:07:54.786417 test begin: paddle.digamma(Tensor([16538, 3, 32, 32],"float32"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([16538, 3, 32, 32],"float32"), ) 	 50804736 	 1000 	 0.9615707397460938 	 1.0674149990081787 	 0.953052282333374 	 1.0566716194152832 	 4.494480609893799 	 1.0749547481536865 	 4.424527406692505 	 0.5492472648620605 	 
2025-07-24 21:08:04.218669 test begin: paddle.digamma(Tensor([8, 3, 32, 33076],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3, 32, 33076],"float64"), ) 	 25402368 	 1000 	 1.1696784496307373 	 1.148181438446045 	 1.1604671478271484 	 1.136462926864624 	 8.556382179260254 	 1.0888853073120117 	 8.50448489189148 	 0.5556941032409668 	 
2025-07-24 21:08:17.363855 test begin: paddle.digamma(Tensor([8, 3, 32, 66151],"float32"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3, 32, 66151],"float32"), ) 	 50803968 	 1000 	 0.9645397663116455 	 1.0672860145568848 	 0.9563024044036865 	 1.0561769008636475 	 4.499988079071045 	 1.0748119354248047 	 4.448033809661865 	 0.5491313934326172 	 
2025-07-24 21:08:26.780237 test begin: paddle.digamma(Tensor([8, 3, 33076, 32],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3, 33076, 32],"float64"), ) 	 25402368 	 1000 	 1.174041509628296 	 1.1671655178070068 	 1.1658425331115723 	 1.1298587322235107 	 8.558577060699463 	 1.0889954566955566 	 8.506415128707886 	 0.555699348449707 	 
2025-07-24 21:08:42.500963 test begin: paddle.digamma(Tensor([8, 3, 66151, 32],"float32"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3, 66151, 32],"float32"), ) 	 50803968 	 1000 	 0.9652643203735352 	 1.0668909549713135 	 0.9491796493530273 	 1.0555133819580078 	 4.5121190547943115 	 1.07476806640625 	 4.450467109680176 	 0.549124002456665 	 
2025-07-24 21:08:51.883318 test begin: paddle.digamma(Tensor([8, 3101, 32, 32],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 3101, 32, 32],"float64"), ) 	 25403392 	 1000 	 1.1750023365020752 	 1.148271083831787 	 1.159550666809082 	 1.1381072998046875 	 8.56107783317566 	 1.087360143661499 	 8.500570058822632 	 0.5555627346038818 	 
2025-07-24 21:09:05.048528 test begin: paddle.digamma(Tensor([8, 6202, 32, 32],"float32"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8, 6202, 32, 32],"float32"), ) 	 50806784 	 1000 	 0.9635848999023438 	 1.0673527717590332 	 0.9555168151855469 	 1.0573549270629883 	 4.507375001907349 	 1.0747747421264648 	 4.455332279205322 	 0.5491623878479004 	 
2025-07-24 21:09:14.733910 test begin: paddle.digamma(Tensor([8269, 3, 32, 32],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(Tensor([8269, 3, 32, 32],"float64"), ) 	 25402368 	 1000 	 1.174163579940796 	 1.1483654975891113 	 1.165978193283081 	 1.138352632522583 	 8.562153816223145 	 1.0887668132781982 	 8.510356187820435 	 0.5570049285888672 	 
2025-07-24 21:09:27.870240 test begin: paddle.digamma(x=Tensor([19601, 6, 6, 6, 6],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(x=Tensor([19601, 6, 6, 6, 6],"float64"), ) 	 25402896 	 1000 	 1.1740846633911133 	 1.1514463424682617 	 1.1657123565673828 	 1.1371030807495117 	 8.574646234512329 	 1.088470220565796 	 8.52179503440857 	 0.5554795265197754 	 
2025-07-24 21:09:41.440940 test begin: paddle.digamma(x=Tensor([3, 39201, 6, 6, 6],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(x=Tensor([3, 39201, 6, 6, 6],"float64"), ) 	 25402248 	 1000 	 1.175642490386963 	 1.1461961269378662 	 1.1642405986785889 	 1.1298468112945557 	 8.590637922286987 	 1.088916301727295 	 8.536167621612549 	 0.5556426048278809 	 
2025-07-24 21:09:55.200498 test begin: paddle.digamma(x=Tensor([3, 6, 39201, 6, 6],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(x=Tensor([3, 6, 39201, 6, 6],"float64"), ) 	 25402248 	 1000 	 1.173478364944458 	 1.152367353439331 	 1.1651113033294678 	 1.136491060256958 	 8.587217330932617 	 1.0871648788452148 	 8.534972429275513 	 0.5554637908935547 	 
2025-07-24 21:10:08.513229 test begin: paddle.digamma(x=Tensor([3, 6, 6, 39201, 6],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(x=Tensor([3, 6, 6, 39201, 6],"float64"), ) 	 25402248 	 1000 	 1.1737258434295654 	 1.147043228149414 	 1.1654458045959473 	 1.1368436813354492 	 8.580610513687134 	 1.0870442390441895 	 8.528393507003784 	 0.5554149150848389 	 
2025-07-24 21:10:21.640542 test begin: paddle.digamma(x=Tensor([3, 6, 6, 6, 39201],"float64"), )
[Prof] paddle.digamma 	 paddle.digamma(x=Tensor([3, 6, 6, 6, 39201],"float64"), ) 	 25402248 	 1000 	 1.175933837890625 	 1.1474850177764893 	 1.1663434505462646 	 1.1373095512390137 	 8.58402156829834 	 1.087022304534912 	 8.531650066375732 	 0.5553874969482422 	 
2025-07-24 21:10:34.845076 test begin: paddle.dist(x=Tensor([10],"float64"), y=Tensor([2540161, 10],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([10],"float64"), y=Tensor([2540161, 10],"float64"), ) 	 25401620 	 1000 	 0.45430541038513184 	 0.4557206630706787 	 0.11583781242370605 	 0.15526580810546875 	 6.737946510314941 	 2.874685049057007 	 1.3806524276733398 	 0.22579431533813477 	 
2025-07-24 21:10:49.754615 test begin: paddle.dist(x=Tensor([113401, 1, 1, 4, 4],"float64"), y=Tensor([113401, 8, 7, 1, 4],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([113401, 1, 1, 4, 4],"float64"), y=Tensor([113401, 8, 7, 1, 4],"float64"), ) 	 27216240 	 1000 	 1.3794848918914795 	 1.4055674076080322 	 0.3517317771911621 	 0.479295015335083 	 8.53633189201355 	 11.315595865249634 	 1.7438738346099854 	 0.890472412109375 	 
2025-07-24 21:11:13.923751 test begin: paddle.dist(x=Tensor([1587601, 1, 4, 4],"float64"), y=Tensor([7, 1, 4],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([1587601, 1, 4, 4],"float64"), y=Tensor([7, 1, 4],"float64"), ) 	 25401644 	 1000 	 2.216029167175293 	 2.2982208728790283 	 0.5646889209747314 	 0.7819423675537109 	 15.281675100326538 	 19.25690770149231 	 2.5995147228240967 	 1.4048776626586914 	 
2025-07-24 21:11:55.011294 test begin: paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 453601, 7, 1, 4],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 453601, 7, 1, 4],"float64"), ) 	 25401688 	 1000 	 1.3715944290161133 	 1.3946187496185303 	 0.3497631549835205 	 0.47484803199768066 	 9.959326028823853 	 11.322205066680908 	 1.695802927017212 	 0.825922966003418 	 
2025-07-24 21:12:20.555668 test begin: paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 8, 396901, 1, 4],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([2, 1, 1, 4, 4],"float64"), y=Tensor([2, 8, 396901, 1, 4],"float64"), ) 	 25401696 	 1000 	 1.3726036548614502 	 1.3938007354736328 	 0.34969639778137207 	 0.47441554069519043 	 9.956460237503052 	 11.319613933563232 	 1.6943233013153076 	 0.825824499130249 	 
2025-07-24 21:12:45.236675 test begin: paddle.dist(x=Tensor([2, 1, 3175201, 4],"float64"), y=Tensor([7, 1, 4],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([2, 1, 3175201, 4],"float64"), y=Tensor([7, 1, 4],"float64"), ) 	 25401636 	 1000 	 3.0457892417907715 	 3.0684726238250732 	 0.7763135433197021 	 1.0463249683380127 	 16.245957851409912 	 20.841246366500854 	 2.7656753063201904 	 1.520556926727295 	 
2025-07-24 21:13:29.055894 test begin: paddle.dist(x=Tensor([2, 1, 4, 4],"float64"), y=Tensor([6350401, 1, 4],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([2, 1, 4, 4],"float64"), y=Tensor([6350401, 1, 4],"float64"), ) 	 25401636 	 1000 	 2.7130982875823975 	 2.7563059329986572 	 0.6910150051116943 	 0.937255859375 	 17.64067316055298 	 22.36877179145813 	 3.000882387161255 	 1.6320469379425049 	 
2025-07-24 21:14:15.263766 test begin: paddle.dist(x=Tensor([2, 1, 793801, 4, 4],"float64"), y=Tensor([2, 8, 793801, 1, 4],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([2, 1, 793801, 4, 4],"float64"), y=Tensor([2, 8, 793801, 1, 4],"float64"), ) 	 76204896 	 1000 	 3.7550253868103027 	 3.7520034313201904 	 0.956756591796875 	 1.2759592533111572 	 18.16801929473877 	 24.65793251991272 	 3.708967447280884 	 1.9384310245513916 	 
2025-07-24 21:15:08.310771 test begin: paddle.dist(x=Tensor([2, 793801, 1, 4, 4],"float64"), y=Tensor([2, 793801, 7, 1, 4],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([2, 793801, 1, 4, 4],"float64"), y=Tensor([2, 793801, 7, 1, 4],"float64"), ) 	 69854488 	 1000 	 2.526503324508667 	 2.557098388671875 	 0.6443054676055908 	 0.8713321685791016 	 15.286545753479004 	 20.0934579372406 	 3.122650623321533 	 1.5802581310272217 	 
2025-07-24 21:15:51.893634 test begin: paddle.dist(x=Tensor([25401601],"float64"), y=Tensor([4, 25401601],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([25401601],"float64"), y=Tensor([4, 25401601],"float64"), ) 	 127008005 	 1000 	 2.348095417022705 	 2.3571248054504395 	 0.598074197769165 	 0.8028655052185059 	 8.494672775268555 	 12.606806993484497 	 2.1689441204071045 	 1.0742590427398682 	 
2025-07-24 21:16:21.120096 test begin: paddle.dist(x=Tensor([6350401],"float64"), y=Tensor([4, 6350401],"float64"), )
[Prof] paddle.dist 	 paddle.dist(x=Tensor([6350401],"float64"), y=Tensor([4, 6350401],"float64"), ) 	 31752005 	 1000 	 0.6016101837158203 	 0.6010282039642334 	 0.15340900421142578 	 0.2054615020751953 	 2.1464438438415527 	 3.1912643909454346 	 0.5476489067077637 	 0.271625280380249 	 
2025-07-24 21:16:29.191871 test begin: paddle.divide(Tensor([128, 396901],"float32"), Tensor([1, 396901],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([128, 396901],"float32"), Tensor([1, 396901],"float32"), ) 	 51200229 	 1000 	 0.3015472888946533 	 0.3115396499633789 	 0.2877991199493408 	 0.2929568290710449 	 0.8014876842498779 	 1.8381900787353516 	 0.4093616008758545 	 0.31301093101501465 	 
2025-07-24 21:16:34.249000 test begin: paddle.divide(Tensor([51059, 995],"float32"), Tensor([1, 995],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([51059, 995],"float32"), Tensor([1, 995],"float32"), ) 	 50804700 	 1000 	 0.29793596267700195 	 0.32117295265197754 	 0.27986669540405273 	 0.2988266944885254 	 0.9025158882141113 	 1.8646156787872314 	 0.30710697174072266 	 0.2729949951171875 	 
2025-07-24 21:16:41.552580 test begin: paddle.divide(Tensor([51059, 995],"float32"), Tensor([51059, 995],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([51059, 995],"float32"), Tensor([51059, 995],"float32"), ) 	 101607410 	 1000 	 0.4508552551269531 	 0.4497835636138916 	 0.4332695007324219 	 0.43232297897338867 	 1.1538283824920654 	 2.092270851135254 	 1.083272933959961 	 0.42767763137817383 	 
2025-07-24 21:16:48.274941 test begin: paddle.divide(Tensor([512, 99226],"float32"), Tensor([1, 99226],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([512, 99226],"float32"), Tensor([1, 99226],"float32"), ) 	 50902938 	 1000 	 0.29634904861450195 	 0.3104586601257324 	 0.2783238887786865 	 0.2921149730682373 	 0.8425877094268799 	 1.835519552230835 	 0.28660082817077637 	 0.31351280212402344 	 
2025-07-24 21:16:53.236178 test begin: paddle.divide(Tensor([544, 93431],"float32"), Tensor([1, 93431],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([544, 93431],"float32"), Tensor([1, 93431],"float32"), ) 	 50919895 	 1000 	 0.29909229278564453 	 0.31055307388305664 	 0.2785468101501465 	 0.29862093925476074 	 0.8941123485565186 	 1.8403160572052002 	 0.3043055534362793 	 0.31307435035705566 	 
2025-07-24 21:16:58.270022 test begin: paddle.divide(Tensor([544, 93431],"float32"), Tensor([544, 93431],"float32"), )
[Prof] paddle.divide 	 paddle.divide(Tensor([544, 93431],"float32"), Tensor([544, 93431],"float32"), ) 	 101652928 	 1000 	 0.45105743408203125 	 0.4499962329864502 	 0.43375515937805176 	 0.4383988380432129 	 1.154203176498413 	 2.0943634510040283 	 1.0922482013702393 	 0.4278733730316162 	 
2025-07-24 21:17:04.793494 test begin: paddle.divide(x=Tensor([16934401, 3],"float32"), y=Tensor([3],"float32"), )
[Prof] paddle.divide 	 paddle.divide(x=Tensor([16934401, 3],"float32"), y=Tensor([3],"float32"), ) 	 50803206 	 1000 	 0.2974731922149658 	 0.32564759254455566 	 0.2789304256439209 	 0.2926025390625 	 5.660071611404419 	 1.855355978012085 	 1.9299321174621582 	 0.2709228992462158 	 
2025-07-24 21:17:14.785470 test begin: paddle.divide(x=Tensor([187679, 271],"float32"), y=Tensor([271],"float32"), )
[Prof] paddle.divide 	 paddle.divide(x=Tensor([187679, 271],"float32"), y=Tensor([271],"float32"), ) 	 50861280 	 1000 	 0.2987070083618164 	 0.3101658821105957 	 0.28008008003234863 	 0.29163336753845215 	 1.006093978881836 	 1.8393588066101074 	 0.3419971466064453 	 0.26827168464660645 	 
2025-07-24 21:17:19.913566 test begin: paddle.dot(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), )
Warning: The core code of paddle.dot is too complex.
[Prof] paddle.dot 	 paddle.dot(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), ) 	 50803202 	 1000 	 6.738507986068726 	 0.29480886459350586 	 6.729200601577759 	 0.14996957778930664 	 0.6269619464874268 	 0.6002392768859863 	 0.3203153610229492 	 0.3066556453704834 	 
2025-07-24 21:17:30.196604 test begin: paddle.dot(x=Tensor([50803201],"float32"), y=Tensor([50803201],"float32"), )
[Prof] paddle.dot 	 paddle.dot(x=Tensor([50803201],"float32"), y=Tensor([50803201],"float32"), ) 	 101606402 	 1000 	 0.29680347442626953 	 0.29276084899902344 	 0.28781771659851074 	 0.14956402778625488 	 0.71018385887146 	 0.6039807796478271 	 0.3627817630767822 	 0.30860352516174316 	 
2025-07-24 21:17:33.789350 test begin: paddle.dot(x=Tensor([50803201],"int32"), y=Tensor([50803201],"int32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7efa3d06ed10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 21:57:42.243800 test begin: paddle.dsplit(Tensor([1411201, 3, 6],"int64"), list[-1,1,3,], )
W0724 21:57:42.796442 76550 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 21:57:44.964313 76550 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:57:45.128934 test begin: paddle.dsplit(Tensor([1411201, 3, 6],"int64"), list[-1,], )
W0724 21:57:47.478210 76698 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:57:47.504614 test begin: paddle.dsplit(Tensor([1411201, 3, 6],"int64"), list[2,4,], )
W0724 21:57:48.257480 76797 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:57:48.450015 test begin: paddle.dsplit(Tensor([4, 1058401, 6],"int64"), list[-1,1,3,], )
W0724 21:57:49.411756 76802 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:57:49.433335 test begin: paddle.dsplit(Tensor([4, 1058401, 6],"int64"), list[-1,], )
W0724 21:57:50.155575 76816 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:57:50.182020 test begin: paddle.dsplit(Tensor([4, 1058401, 6],"int64"), list[2,4,], )
W0724 21:57:50.926533 76822 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:57:50.950006 test begin: paddle.dsplit(Tensor([4, 3, 2116801],"int64"), list[-1,1,3,], )
W0724 21:57:52.082770 76825 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:57:52.115324 test begin: paddle.dsplit(Tensor([4, 3, 2116801],"int64"), list[-1,], )
W0724 21:57:52.831050 77038 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:57:52.850018 test begin: paddle.dsplit(Tensor([4, 3, 2116801],"int64"), list[2,4,], )
W0724 21:57:53.569684 77061 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:57:53.587292 test begin: paddle.dstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], ) 	 76204872 	 1000 	 0.9408566951751709 	 0.9397103786468506 	 0.9036803245544434 	 0.9235508441925049 	 0.956946611404419 	 0.09213733673095703 	 0.8743345737457275 	 0.0001704692840576172 	 
2025-07-24 21:58:00.592523 test begin: paddle.dstack(list[Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2, 1058401],"float64"),], ) 	 25401624 	 1000 	 0.31998372077941895 	 0.317185640335083 	 0.3040635585784912 	 0.15998411178588867 	 0.3205716609954834 	 0.056342124938964844 	 0.25687479972839355 	 6.842613220214844e-05 	 
2025-07-24 21:58:02.704478 test begin: paddle.dstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401900 	 1000 	 0.32213878631591797 	 0.3228473663330078 	 0.30156469345092773 	 0.30840420722961426 	 0.32090306282043457 	 0.0703277587890625 	 0.24678730964660645 	 5.7220458984375e-05 	 
2025-07-24 21:58:04.821585 test begin: paddle.dstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401900 	 1000 	 0.32248950004577637 	 0.3311426639556885 	 0.30290794372558594 	 0.3164844512939453 	 0.3231980800628662 	 0.11128854751586914 	 0.24852633476257324 	 0.0001010894775390625 	 
2025-07-24 21:58:07.050910 test begin: paddle.dstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], ) 	 76204836 	 1000 	 0.9731149673461914 	 0.9406547546386719 	 0.9537787437438965 	 0.9252901077270508 	 0.9702131748199463 	 0.08666205406188965 	 0.8958282470703125 	 0.00012350082397460938 	 
2025-07-24 21:58:13.205079 test begin: paddle.dstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401660 	 1000 	 0.3265652656555176 	 0.3189883232116699 	 0.3035862445831299 	 0.3020303249359131 	 0.32280969619750977 	 0.10008382797241211 	 0.24933838844299316 	 0.00012803077697753906 	 
2025-07-24 21:58:15.497856 test begin: paddle.dstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401660 	 1000 	 0.32636523246765137 	 0.32598423957824707 	 0.3065307140350342 	 0.31129932403564453 	 0.3223683834075928 	 0.08092236518859863 	 0.24315142631530762 	 5.936622619628906e-05 	 
2025-07-24 21:58:17.615198 test begin: paddle.dstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2116801],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2116801],"float64"),], ) 	 25401660 	 1000 	 0.32384705543518066 	 0.3213460445404053 	 0.3043861389160156 	 0.3060030937194824 	 0.3225119113922119 	 0.10249710083007812 	 0.2462904453277588 	 7.62939453125e-05 	 
2025-07-24 21:58:19.774163 test begin: paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401900 	 1000 	 0.32285332679748535 	 0.3240535259246826 	 0.3029794692993164 	 0.30565977096557617 	 0.32361936569213867 	 0.09890365600585938 	 0.2484908103942871 	 0.0001068115234375 	 
2025-07-24 21:58:21.875774 test begin: paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 76204980 	 1000 	 0.9677379131317139 	 0.9461052417755127 	 0.9480745792388916 	 0.9312689304351807 	 0.969165563583374 	 0.09412765502929688 	 0.8956656455993652 	 0.00010585784912109375 	 
2025-07-24 21:58:27.984312 test begin: paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401660 	 1000 	 0.3244149684906006 	 0.3167703151702881 	 0.3086719512939453 	 0.15996861457824707 	 0.32238101959228516 	 0.07461285591125488 	 0.2602229118347168 	 0.00019669532775878906 	 
2025-07-24 21:58:30.176993 test begin: paddle.dstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 76204818 	 1000 	 1.5547702312469482 	 2.6512563228607178 	 1.5352239608764648 	 2.390779733657837 	 2.599702835083008 	 0.09115099906921387 	 2.524797201156616 	 8.225440979003906e-05 	 
2025-07-24 21:58:41.856657 test begin: paddle.dstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 76204890 	 1000 	 0.964536190032959 	 2.522892713546753 	 0.9450135231018066 	 2.508223533630371 	 1.067000389099121 	 0.0907139778137207 	 0.9935765266418457 	 5.650520324707031e-05 	 
2025-07-24 21:58:50.022686 test begin: paddle.dstack(list[Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401630 	 1000 	 0.3933236598968506 	 0.3268296718597412 	 0.3767819404602051 	 0.15991997718811035 	 0.39595675468444824 	 0.07131505012512207 	 0.33552980422973633 	 5.2928924560546875e-05 	 
2025-07-24 21:58:53.158235 test begin: paddle.dstack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], ) 	 76204824 	 1000 	 1.558835506439209 	 2.405895233154297 	 1.538440465927124 	 2.3912360668182373 	 2.602949380874634 	 0.06886458396911621 	 2.5291647911071777 	 5.507469177246094e-05 	 
2025-07-24 21:59:03.094496 test begin: paddle.dstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 76204920 	 1000 	 0.9632461071014404 	 2.5243728160858154 	 0.9436554908752441 	 2.509552001953125 	 1.0661275386810303 	 0.10801815986633301 	 0.9923315048217773 	 0.0001049041748046875 	 
2025-07-24 21:59:11.101213 test begin: paddle.dstack(list[Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.dstack 	 paddle.dstack(list[Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401640 	 1000 	 0.3906128406524658 	 0.31519389152526855 	 0.3747444152832031 	 0.15996670722961426 	 0.3962867259979248 	 0.07373189926147461 	 0.3365015983581543 	 6.556510925292969e-05 	 
2025-07-24 21:59:13.446146 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([200, 8, 498, 498],"float32"), Tensor([200, 8, 498, 64],"float32"), )
Warning: The core code of paddle.einsum is too complex.
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([200, 8, 498, 498],"float32"), Tensor([200, 8, 498, 64],"float32"), ) 	 447801600 	 1000 	 6.091890811920166 	 6.0932371616363525 	 6.029330253601074 	 6.025932788848877 	 12.361016750335693 	 9.525166273117065 	 2.526750087738037 	 4.866119384765625 	 
2025-07-24 21:59:57.601726 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([209, 8, 477, 477],"float32"), Tensor([209, 8, 477, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([209, 8, 477, 477],"float32"), Tensor([209, 8, 477, 64],"float32"), ) 	 431471304 	 1000 	 6.032697916030884 	 6.027700185775757 	 5.973215818405151 	 5.973210096359253 	 12.366576910018921 	 9.62083888053894 	 2.528867244720459 	 4.914451837539673 	 
2025-07-24 22:00:40.510083 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([218, 8, 457, 457],"float32"), Tensor([218, 8, 457, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([218, 8, 457, 457],"float32"), Tensor([218, 8, 457, 64],"float32"), ) 	 415241168 	 1000 	 6.087902307510376 	 6.0856475830078125 	 6.0291078090667725 	 6.03199315071106 	 12.448668718338013 	 9.846920728683472 	 2.547226905822754 	 5.0312511920928955 	 
2025-07-24 22:01:22.949998 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([26, 8, 498, 498],"float32"), Tensor([26, 8, 498, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([26, 8, 498, 498],"float32"), Tensor([26, 8, 498, 64],"float32"), ) 	 58214208 	 1000 	 0.8252689838409424 	 0.8244004249572754 	 0.7655103206634521 	 0.7715070247650146 	 1.6594023704528809 	 1.2822601795196533 	 0.34029507637023926 	 0.6543569564819336 	 
2025-07-24 22:01:28.700826 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([28, 8, 477, 477],"float32"), Tensor([28, 8, 477, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([28, 8, 477, 477],"float32"), Tensor([28, 8, 477, 64],"float32"), ) 	 57804768 	 1000 	 0.8792705535888672 	 0.880810022354126 	 0.7964494228363037 	 0.827021598815918 	 1.752793788909912 	 1.3713858127593994 	 0.3591947555541992 	 0.7005796432495117 	 
2025-07-24 22:01:34.674994 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 54, 498, 498],"float32"), Tensor([30, 54, 498, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 54, 498, 498],"float32"), Tensor([30, 54, 498, 64],"float32"), ) 	 453399120 	 1000 	 6.114514589309692 	 6.11345100402832 	 6.0553202629089355 	 6.055800676345825 	 12.458670854568481 	 9.585712909698486 	 2.5482094287872314 	 4.897942543029785 	 
2025-07-24 22:02:17.532079 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 56, 477, 477],"float32"), Tensor([30, 56, 477, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 56, 477, 477],"float32"), Tensor([30, 56, 477, 64],"float32"), ) 	 433535760 	 1000 	 6.089688301086426 	 6.086826324462891 	 6.027862787246704 	 6.028198003768921 	 12.448843955993652 	 9.693928956985474 	 2.5468108654022217 	 4.953381061553955 	 
2025-07-24 22:03:01.910088 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 58, 457, 457],"float32"), Tensor([30, 58, 457, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 58, 457, 457],"float32"), Tensor([30, 58, 457, 64],"float32"), ) 	 414288780 	 1000 	 6.072582483291626 	 6.078000068664551 	 6.010819435119629 	 6.024505615234375 	 12.420117378234863 	 9.82366156578064 	 2.539438009262085 	 5.021905422210693 	 
2025-07-24 22:03:44.159193 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 7, 498, 498],"float32"), Tensor([30, 7, 498, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 7, 498, 498],"float32"), Tensor([30, 7, 498, 64],"float32"), ) 	 58773960 	 1000 	 0.8237512111663818 	 0.8254067897796631 	 0.7599658966064453 	 0.7726545333862305 	 1.6710224151611328 	 1.2893831729888916 	 0.3416774272918701 	 0.6594352722167969 	 
2025-07-24 22:03:49.927012 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 426, 498],"float32"), Tensor([30, 8, 498, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 426, 498],"float32"), Tensor([30, 8, 498, 64],"float32"), ) 	 58564800 	 1000 	 0.9248335361480713 	 0.9300901889801025 	 0.8655991554260254 	 0.8731696605682373 	 1.6982495784759521 	 1.3241500854492188 	 0.3469557762145996 	 0.6764895915985107 	 
2025-07-24 22:03:55.958965 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 444, 477],"float32"), Tensor([30, 8, 477, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 444, 477],"float32"), Tensor([30, 8, 477, 64],"float32"), ) 	 58155840 	 1000 	 0.883455753326416 	 0.8824496269226074 	 0.820183277130127 	 0.8288788795471191 	 1.7285759449005127 	 1.354011058807373 	 0.3528897762298584 	 0.692357063293457 	 
2025-07-24 22:04:01.941264 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 457, 457],"float32"), Tensor([30, 8, 457, 464],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 457, 457],"float32"), Tensor([30, 8, 457, 464],"float32"), ) 	 101015280 	 1000 	 3.3764946460723877 	 3.8711254596710205 	 3.3175127506256104 	 3.3207828998565674 	 7.64158034324646 	 6.764660358428955 	 1.5613791942596436 	 3.4554708003997803 	 
2025-07-24 22:04:26.978804 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 464, 457],"float32"), Tensor([30, 8, 457, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 464, 457],"float32"), Tensor([30, 8, 457, 64],"float32"), ) 	 57911040 	 1000 	 0.8541421890258789 	 0.8552384376525879 	 0.7948188781738281 	 0.8020734786987305 	 1.750208854675293 	 1.383659839630127 	 0.3578662872314453 	 0.7068133354187012 	 
2025-07-24 22:04:32.962430 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 477, 477],"float32"), Tensor([30, 8, 477, 444],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 477, 477],"float32"), Tensor([30, 8, 477, 444],"float32"), ) 	 105436080 	 1000 	 3.490361452102661 	 3.4934089183807373 	 3.4314510822296143 	 3.440586566925049 	 7.679791450500488 	 6.7659735679626465 	 1.5683202743530273 	 3.4582817554473877 	 
2025-07-24 22:04:57.464410 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 477, 477],"float32"), Tensor([30, 8, 477, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 477, 477],"float32"), Tensor([30, 8, 477, 64],"float32"), ) 	 61933680 	 1000 	 0.8824031352996826 	 0.883737325668335 	 0.8219931125640869 	 0.8247115612030029 	 1.8102061748504639 	 1.4100067615509033 	 0.3701670169830322 	 0.7211000919342041 	 
2025-07-24 22:05:03.662140 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 498, 498],"float32"), Tensor([30, 8, 498, 426],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 8, 498, 498],"float32"), Tensor([30, 8, 498, 426],"float32"), ) 	 110436480 	 1000 	 3.6667845249176025 	 4.863965749740601 	 3.6083455085754395 	 3.6011221408843994 	 7.76692533493042 	 6.825646162033081 	 1.5894038677215576 	 3.4890613555908203 	 
2025-07-24 22:05:29.965053 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 9, 457, 457],"float32"), Tensor([30, 9, 457, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([30, 9, 457, 457],"float32"), Tensor([30, 9, 457, 64],"float32"), ) 	 64286190 	 1000 	 0.9497215747833252 	 0.9526100158691406 	 0.8902547359466553 	 0.8920261859893799 	 1.9510126113891602 	 1.5388431549072266 	 0.39858341217041016 	 0.7861800193786621 	 
2025-07-24 22:05:37.161139 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([31, 8, 457, 457],"float32"), Tensor([31, 8, 457, 64],"float32"), )
[Prof] paddle.einsum 	 paddle.einsum("b h i j, b h j d -> b h i d", Tensor([31, 8, 457, 457],"float32"), Tensor([31, 8, 457, 64],"float32"), ) 	 59048056 	 1000 	 0.9427347183227539 	 0.9474411010742188 	 0.8841972351074219 	 0.8883402347564697 	 1.8696682453155518 	 1.4883344173431396 	 0.3815734386444092 	 0.7604265213012695 	 
2025-07-24 22:05:43.600116 test begin: paddle.empty_like(Tensor([101606401],"uint8"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([101606401],"uint8"), ) 	 101606401 	 1000 	 0.01163172721862793 	 0.009372949600219727 	 8.58306884765625e-06 	 3.457069396972656e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:44.769238 test begin: paddle.empty_like(Tensor([4096, 12404],"bool"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([4096, 12404],"bool"), ) 	 50806784 	 1000 	 0.011443376541137695 	 0.005625724792480469 	 8.821487426757812e-06 	 3.0994415283203125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:45.501791 test begin: paddle.empty_like(Tensor([4096, 12404],"float32"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([4096, 12404],"float32"), ) 	 50806784 	 1000 	 0.011652708053588867 	 0.005343437194824219 	 7.3909759521484375e-06 	 3.695487976074219e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:46.343598 test begin: paddle.empty_like(Tensor([793801, 64],"bool"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([793801, 64],"bool"), ) 	 50803264 	 1000 	 0.011659622192382812 	 0.010554313659667969 	 1.0013580322265625e-05 	 4.5299530029296875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:47.097681 test begin: paddle.empty_like(Tensor([793801, 64],"float32"), )
[Prof] paddle.empty_like 	 paddle.empty_like(Tensor([793801, 64],"float32"), ) 	 50803264 	 1000 	 0.011651277542114258 	 0.005349397659301758 	 9.059906005859375e-06 	 3.790855407714844e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:47.944281 test begin: paddle.equal(Tensor([4148, 6124],"int64"), Tensor([4148, 6124],"int64"), )
[Prof] paddle.equal 	 paddle.equal(Tensor([4148, 6124],"int64"), Tensor([4148, 6124],"int64"), ) 	 50804704 	 1000 	 0.3113830089569092 	 0.3145294189453125 	 0.3004026412963867 	 0.30173420906066895 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:49.485133 test begin: paddle.equal(Tensor([416, 61062],"int64"), 0, )
[Prof] paddle.equal 	 paddle.equal(Tensor([416, 61062],"int64"), 0, ) 	 25401792 	 1000 	 0.17775201797485352 	 0.16836285591125488 	 0.09079623222351074 	 0.15462756156921387 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:50.254309 test begin: paddle.equal(Tensor([512, 49613],"int64"), 0, )
[Prof] paddle.equal 	 paddle.equal(Tensor([512, 49613],"int64"), 0, ) 	 25401856 	 1000 	 0.17857098579406738 	 0.16837382316589355 	 0.09120535850524902 	 0.1546339988708496 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:51.027441 test begin: paddle.equal(Tensor([846721, 30],"int64"), 0, )
[Prof] paddle.equal 	 paddle.equal(Tensor([846721, 30],"int64"), 0, ) 	 25401630 	 1000 	 0.1785259246826172 	 0.1683790683746338 	 0.09120821952819824 	 0.15252375602722168 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:51.833390 test begin: paddle.equal(Tensor([846721, 30],"int64"), Tensor([846721, 30],"int64"), )
[Prof] paddle.equal 	 paddle.equal(Tensor([846721, 30],"int64"), Tensor([846721, 30],"int64"), ) 	 50803260 	 1000 	 0.3112823963165283 	 0.31729817390441895 	 0.3004786968231201 	 0.30332303047180176 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:53.351934 test begin: paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([1, 2, 10, 2540161],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([1, 2, 10, 2540161],"bool"), ) 	 50803540 	 1000 	 0.016882896423339844 	 0.00255584716796875 	 1.4543533325195312e-05 	 2.5033950805664062e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:54.307379 test begin: paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([1, 2, 1587601, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([1, 2, 1587601, 16],"bool"), ) 	 50803552 	 1000 	 0.016859769821166992 	 0.002502918243408203 	 1.0967254638671875e-05 	 1.9311904907226562e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:55.032965 test begin: paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([1, 317521, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([1, 317521, 10, 16],"bool"), ) 	 50803680 	 1000 	 0.016912221908569336 	 0.0025212764739990234 	 1.52587890625e-05 	 1.621246337890625e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:55.762793 test begin: paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([158761, 2, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([158761, 2, 10, 16],"bool"), ) 	 50803840 	 1000 	 0.01697397232055664 	 0.0024802684783935547 	 2.193450927734375e-05 	 1.52587890625e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:56.510578 test begin: paddle.equal_all(Tensor([1, 2, 10, 2540161],"bool"), Tensor([1, 2, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 2, 10, 2540161],"bool"), Tensor([1, 2, 10, 16],"bool"), ) 	 50803540 	 1000 	 0.016891956329345703 	 0.002493619918823242 	 9.775161743164062e-06 	 1.6927719116210938e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:57.230552 test begin: paddle.equal_all(Tensor([1, 2, 10, 2540161],"bool"), Tensor([1, 2, 10, 2540161],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 2, 10, 2540161],"bool"), Tensor([1, 2, 10, 2540161],"bool"), ) 	 101606440 	 1000 	 0.16982460021972656 	 0.20798873901367188 	 0.0577855110168457 	 6.937980651855469e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:59.159106 test begin: paddle.equal_all(Tensor([1, 2, 1587601, 16],"bool"), Tensor([1, 2, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 2, 1587601, 16],"bool"), Tensor([1, 2, 10, 16],"bool"), ) 	 50803552 	 1000 	 0.01692342758178711 	 0.002476930618286133 	 8.821487426757812e-06 	 1.8358230590820312e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:05:59.903193 test begin: paddle.equal_all(Tensor([1, 2, 1587601, 16],"bool"), Tensor([1, 2, 1587601, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 2, 1587601, 16],"bool"), Tensor([1, 2, 1587601, 16],"bool"), ) 	 101606464 	 1000 	 0.16980314254760742 	 0.20516610145568848 	 0.057787179946899414 	 7.081031799316406e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:06:01.685423 test begin: paddle.equal_all(Tensor([1, 317521, 10, 16],"bool"), Tensor([1, 2, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 317521, 10, 16],"bool"), Tensor([1, 2, 10, 16],"bool"), ) 	 50803680 	 1000 	 0.016991615295410156 	 0.0025370121002197266 	 9.059906005859375e-06 	 1.7881393432617188e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:06:02.411319 test begin: paddle.equal_all(Tensor([1, 317521, 10, 16],"bool"), Tensor([1, 317521, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([1, 317521, 10, 16],"bool"), Tensor([1, 317521, 10, 16],"bool"), ) 	 101606720 	 1000 	 0.16982436180114746 	 0.2054915428161621 	 0.05778932571411133 	 7.009506225585938e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:06:04.240410 test begin: paddle.equal_all(Tensor([128],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([128],"float32"), Tensor([50803201],"float32"), ) 	 50803329 	 1000 	 0.017160892486572266 	 0.0024628639221191406 	 1.1920928955078125e-05 	 1.5974044799804688e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:06:05.078965 test begin: paddle.equal_all(Tensor([158761, 2, 10, 16],"bool"), Tensor([1, 2, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([158761, 2, 10, 16],"bool"), Tensor([1, 2, 10, 16],"bool"), ) 	 50803840 	 1000 	 0.01700735092163086 	 0.002519845962524414 	 9.298324584960938e-06 	 1.6689300537109375e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:06:05.809736 test begin: paddle.equal_all(Tensor([158761, 2, 10, 16],"bool"), Tensor([158761, 2, 10, 16],"bool"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([158761, 2, 10, 16],"bool"), Tensor([158761, 2, 10, 16],"bool"), ) 	 101607040 	 1000 	 0.17016053199768066 	 0.20656561851501465 	 0.0579068660736084 	 7.271766662597656e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:06:07.604497 test begin: paddle.equal_all(Tensor([16, 16],"float32"), Tensor([16, 3175201],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([16, 16],"float32"), Tensor([16, 3175201],"float32"), ) 	 50803472 	 1000 	 0.017059803009033203 	 0.002507925033569336 	 8.344650268554688e-06 	 1.6450881958007812e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:06:08.543753 test begin: paddle.equal_all(Tensor([16, 16],"float32"), Tensor([3175201, 16],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([16, 16],"float32"), Tensor([3175201, 16],"float32"), ) 	 50803472 	 1000 	 0.017234325408935547 	 0.0024938583374023438 	 1.1682510375976562e-05 	 2.2649765014648438e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:06:09.395814 test begin: paddle.equal_all(Tensor([16, 3175201],"float32"), Tensor([16, 16],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([16, 3175201],"float32"), Tensor([16, 16],"float32"), ) 	 50803472 	 1000 	 0.030550241470336914 	 0.0025434494018554688 	 2.002716064453125e-05 	 1.71661376953125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:06:10.334495 test begin: paddle.equal_all(Tensor([16, 3175201],"float32"), Tensor([16, 3175201],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([16, 3175201],"float32"), Tensor([16, 3175201],"float32"), ) 	 101606432 	 1000 	 0.38013529777526855 	 0.41707372665405273 	 0.12928533554077148 	 7.748603820800781e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:06:12.824137 test begin: paddle.equal_all(Tensor([3175201, 16],"float32"), Tensor([16, 16],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([3175201, 16],"float32"), Tensor([16, 16],"float32"), ) 	 50803472 	 1000 	 0.024059534072875977 	 0.004243373870849609 	 2.002716064453125e-05 	 1.8596649169921875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:06:13.717670 test begin: paddle.equal_all(Tensor([3175201, 16],"float32"), Tensor([3175201, 16],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([3175201, 16],"float32"), Tensor([3175201, 16],"float32"), ) 	 101606432 	 1000 	 0.3801605701446533 	 0.417896032333374 	 0.12932062149047852 	 7.796287536621094e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:06:16.299928 test begin: paddle.equal_all(Tensor([50803201],"float32"), Tensor([128],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([50803201],"float32"), Tensor([128],"float32"), ) 	 50803329 	 1000 	 0.017232894897460938 	 0.0025136470794677734 	 1.4781951904296875e-05 	 1.6689300537109375e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:06:17.204768 test begin: paddle.equal_all(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.equal_all 	 paddle.equal_all(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 1000 	 0.3801748752593994 	 0.4285852909088135 	 0.12930583953857422 	 9.751319885253906e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:06:19.727110 test begin: paddle.erf(Tensor([11, 2309237],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([11, 2309237],"float64"), ) 	 25401607 	 1000 	 0.33905911445617676 	 0.30387043952941895 	 0.3308379650115967 	 0.2929496765136719 	 0.4477560520172119 	 1.6393296718597412 	 0.3916938304901123 	 0.33484911918640137 	 
2025-07-24 22:06:25.190098 test begin: paddle.erf(Tensor([1494212, 17],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([1494212, 17],"float64"), ) 	 25401604 	 1000 	 0.3376443386077881 	 0.30775022506713867 	 0.3295285701751709 	 0.29215288162231445 	 0.44774675369262695 	 1.643303632736206 	 0.39121317863464355 	 0.33614611625671387 	 
2025-07-24 22:06:29.662251 test begin: paddle.erf(Tensor([211681, 2, 3, 5, 4],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([211681, 2, 3, 5, 4],"float64"), ) 	 25401720 	 1000 	 0.3371767997741699 	 0.3038160800933838 	 0.32906532287597656 	 0.2928755283355713 	 0.4473433494567871 	 1.6418871879577637 	 0.39141201972961426 	 0.3361189365386963 	 
2025-07-24 22:06:33.501197 test begin: paddle.erf(Tensor([4, 105841, 3, 5, 4],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 105841, 3, 5, 4],"float64"), ) 	 25401840 	 1000 	 0.3373394012451172 	 0.7577207088470459 	 0.32907867431640625 	 0.29283785820007324 	 0.4473552703857422 	 1.6409542560577393 	 0.3895430564880371 	 0.3349037170410156 	 
2025-07-24 22:06:40.926927 test begin: paddle.erf(Tensor([4, 2, 158761, 5, 4],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 158761, 5, 4],"float64"), ) 	 25401760 	 1000 	 0.3375234603881836 	 0.3038756847381592 	 0.32922983169555664 	 0.2927558422088623 	 0.4477410316467285 	 1.6393375396728516 	 0.39156103134155273 	 0.3361837863922119 	 
2025-07-24 22:06:44.754856 test begin: paddle.erf(Tensor([4, 2, 3, 1058401],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 3, 1058401],"float64"), ) 	 25401624 	 1000 	 0.33898162841796875 	 0.30384159088134766 	 0.33083128929138184 	 0.2929558753967285 	 0.4477214813232422 	 1.6382107734680176 	 0.391735315322876 	 0.3349032402038574 	 
2025-07-24 22:06:48.592966 test begin: paddle.erf(Tensor([4, 2, 3, 264601, 4],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 3, 264601, 4],"float64"), ) 	 25401696 	 1000 	 0.3374457359313965 	 0.30385923385620117 	 0.3292701244354248 	 0.2928898334503174 	 0.44777560234069824 	 1.638068437576294 	 0.3914353847503662 	 0.3348655700683594 	 
2025-07-24 22:06:52.405917 test begin: paddle.erf(Tensor([4, 2, 3, 5, 211681],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 3, 5, 211681],"float64"), ) 	 25401720 	 1000 	 0.33716750144958496 	 0.3089609146118164 	 0.3287818431854248 	 0.29407382011413574 	 0.4486260414123535 	 1.637976884841919 	 0.39246201515197754 	 0.33489561080932617 	 
2025-07-24 22:06:56.292777 test begin: paddle.erf(Tensor([4, 2, 635041, 5],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 2, 635041, 5],"float64"), ) 	 25401640 	 1000 	 0.33786892890930176 	 0.3038184642791748 	 0.32967066764831543 	 0.292574405670166 	 0.44975924491882324 	 1.639275312423706 	 0.393932580947876 	 0.334883451461792 	 
2025-07-24 22:07:00.226018 test begin: paddle.erf(Tensor([4, 423361, 3, 5],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([4, 423361, 3, 5],"float64"), ) 	 25401660 	 1000 	 0.33782219886779785 	 0.30377817153930664 	 0.3294970989227295 	 0.2930786609649658 	 0.4481375217437744 	 1.641904592514038 	 0.39238429069519043 	 0.33611583709716797 	 
2025-07-24 22:07:04.033424 test begin: paddle.erf(Tensor([846721, 2, 3, 5],"float64"), )
[Prof] paddle.erf 	 paddle.erf(Tensor([846721, 2, 3, 5],"float64"), ) 	 25401630 	 1000 	 0.3376021385192871 	 0.30383896827697754 	 0.3294851779937744 	 0.29289889335632324 	 0.44779467582702637 	 1.6393458843231201 	 0.3919186592102051 	 0.3348534107208252 	 
2025-07-24 22:07:07.894825 test begin: paddle.erfinv(x=Tensor([211681, 2, 3, 5, 4],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([211681, 2, 3, 5, 4],"float64"), ) 	 25401720 	 1000 	 0.3321669101715088 	 0.30934762954711914 	 0.3234386444091797 	 0.29859209060668945 	 0.44702744483947754 	 1.6445000171661377 	 0.39136576652526855 	 0.33585524559020996 	 
2025-07-24 22:07:11.765948 test begin: paddle.erfinv(x=Tensor([4, 105841, 3, 5, 4],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 105841, 3, 5, 4],"float64"), ) 	 25401840 	 1000 	 0.3338432312011719 	 0.3092529773712158 	 0.32500457763671875 	 0.2990694046020508 	 0.44710564613342285 	 1.6440682411193848 	 0.3908860683441162 	 0.337083101272583 	 
2025-07-24 22:07:15.635917 test begin: paddle.erfinv(x=Tensor([4, 2, 158761, 5, 4],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 158761, 5, 4],"float64"), ) 	 25401760 	 1000 	 0.3335909843444824 	 0.30922722816467285 	 0.3211238384246826 	 0.2990107536315918 	 0.44904422760009766 	 1.6427862644195557 	 0.393280029296875 	 0.3358299732208252 	 
2025-07-24 22:07:19.429253 test begin: paddle.erfinv(x=Tensor([4, 2, 3, 1058401],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 3, 1058401],"float64"), ) 	 25401624 	 1000 	 0.33309507369995117 	 0.31121253967285156 	 0.3241310119628906 	 0.30096936225891113 	 0.44750285148620605 	 1.6440885066986084 	 0.38697123527526855 	 0.3358469009399414 	 
2025-07-24 22:07:23.244585 test begin: paddle.erfinv(x=Tensor([4, 2, 3, 264601, 4],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 3, 264601, 4],"float64"), ) 	 25401696 	 1000 	 0.3321876525878906 	 0.30953526496887207 	 0.3234071731567383 	 0.29929375648498535 	 0.4491450786590576 	 1.6454906463623047 	 0.39404892921447754 	 0.3358802795410156 	 
2025-07-24 22:07:27.038892 test begin: paddle.erfinv(x=Tensor([4, 2, 3, 5, 211681],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 3, 5, 211681],"float64"), ) 	 25401720 	 1000 	 0.33297276496887207 	 0.3120858669281006 	 0.324099063873291 	 0.2989208698272705 	 0.4470236301422119 	 1.6441435813903809 	 0.3854522705078125 	 0.335890531539917 	 
2025-07-24 22:07:33.088964 test begin: paddle.erfinv(x=Tensor([4, 2, 3175201],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 3175201],"float64"), ) 	 25401608 	 1000 	 0.3347024917602539 	 0.30920958518981934 	 0.3257143497467041 	 0.2990260124206543 	 0.44879984855651855 	 1.6430647373199463 	 0.39338040351867676 	 0.33585309982299805 	 
2025-07-24 22:07:40.145321 test begin: paddle.erfinv(x=Tensor([4, 2, 635041, 5],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2, 635041, 5],"float64"), ) 	 25401640 	 1000 	 0.331958532333374 	 0.31181883811950684 	 0.32305431365966797 	 0.30034804344177246 	 0.4485514163970947 	 1.6430096626281738 	 0.39313602447509766 	 0.33591485023498535 	 
2025-07-24 22:07:43.939098 test begin: paddle.erfinv(x=Tensor([4, 2116801, 3],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 2116801, 3],"float64"), ) 	 25401612 	 1000 	 0.33324766159057617 	 0.31510496139526367 	 0.32442522048950195 	 0.3002147674560547 	 0.447573184967041 	 1.6441879272460938 	 0.39208149909973145 	 0.3358428478240967 	 
2025-07-24 22:07:47.749060 test begin: paddle.erfinv(x=Tensor([4, 423361, 3, 5],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4, 423361, 3, 5],"float64"), ) 	 25401660 	 1000 	 0.33201169967651367 	 0.3101496696472168 	 0.3231351375579834 	 0.2962207794189453 	 0.4474043846130371 	 1.6455671787261963 	 0.3910863399505615 	 0.33589720726013184 	 
2025-07-24 22:07:51.540710 test begin: paddle.erfinv(x=Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([4233601, 2, 3],"float64"), ) 	 25401606 	 1000 	 0.33251070976257324 	 0.31069278717041016 	 0.3235909938812256 	 0.30052876472473145 	 0.44873476028442383 	 1.6440093517303467 	 0.3932168483734131 	 0.33709096908569336 	 
2025-07-24 22:07:55.398693 test begin: paddle.erfinv(x=Tensor([846721, 2, 3, 5],"float64"), )
[Prof] paddle.erfinv 	 paddle.erfinv(x=Tensor([846721, 2, 3, 5],"float64"), ) 	 25401630 	 1000 	 0.33297228813171387 	 0.31197690963745117 	 0.32413697242736816 	 0.30121803283691406 	 0.44756507873535156 	 1.6440885066986084 	 0.39169907569885254 	 0.3358309268951416 	 
2025-07-24 22:07:59.249043 test begin: paddle.exp(Tensor([125, 1, 640, 640],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([125, 1, 640, 640],"float32"), ) 	 51200000 	 1000 	 0.2991807460784912 	 0.3000662326812744 	 0.29089903831481934 	 0.2894432544708252 	 0.45295143127441406 	 0.4515564441680908 	 0.3958871364593506 	 0.34952569007873535 	 
2025-07-24 22:08:02.456195 test begin: paddle.exp(Tensor([13, 243, 1007, 16],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([13, 243, 1007, 16],"float32"), ) 	 50897808 	 1000 	 0.2961008548736572 	 0.30119872093200684 	 0.2873237133026123 	 0.2886695861816406 	 0.4516007900238037 	 0.4475853443145752 	 0.3946816921234131 	 0.37627720832824707 	 
2025-07-24 22:08:05.626744 test begin: paddle.exp(Tensor([13, 64, 1007, 61],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([13, 64, 1007, 61],"float32"), ) 	 51107264 	 1000 	 0.29737043380737305 	 0.29961729049682617 	 0.2886989116668701 	 0.2798795700073242 	 0.452420711517334 	 0.4496481418609619 	 0.39583849906921387 	 0.34708380699157715 	 
2025-07-24 22:08:08.957990 test begin: paddle.exp(Tensor([13, 64, 3817, 16],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([13, 64, 3817, 16],"float32"), ) 	 50811904 	 1000 	 0.2954976558685303 	 0.30111026763916016 	 0.287233829498291 	 0.28780245780944824 	 0.44955968856811523 	 0.4467604160308838 	 0.39312744140625 	 0.3726212978363037 	 
2025-07-24 22:08:12.102892 test begin: paddle.exp(Tensor([16, 1, 4962, 640],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([16, 1, 4962, 640],"float32"), ) 	 50810880 	 1000 	 0.2956376075744629 	 0.29785704612731934 	 0.2873237133026123 	 0.28726744651794434 	 0.44948625564575195 	 0.44671130180358887 	 0.39330291748046875 	 0.37726616859436035 	 
2025-07-24 22:08:15.329037 test begin: paddle.exp(Tensor([16, 1, 640, 4962],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([16, 1, 640, 4962],"float32"), ) 	 50810880 	 1000 	 0.29561519622802734 	 0.3057694435119629 	 0.28729939460754395 	 0.2883310317993164 	 0.44941210746765137 	 0.44814467430114746 	 0.39264702796936035 	 0.377521276473999 	 
2025-07-24 22:08:18.500738 test begin: paddle.exp(Tensor([16, 8, 640, 640],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([16, 8, 640, 640],"float32"), ) 	 52428800 	 1000 	 0.30499863624572754 	 0.30727601051330566 	 0.29674577713012695 	 0.2964329719543457 	 0.46392107009887695 	 0.4607264995574951 	 0.407520055770874 	 0.39159107208251953 	 
2025-07-24 22:08:21.806441 test begin: paddle.exp(Tensor([50, 64, 1007, 16],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([50, 64, 1007, 16],"float32"), ) 	 51558400 	 1000 	 0.29996252059936523 	 0.3021812438964844 	 0.2915077209472656 	 0.2910807132720947 	 0.4560978412628174 	 0.45598387718200684 	 0.3993535041809082 	 0.38635802268981934 	 
2025-07-24 22:08:25.124934 test begin: paddle.exp(Tensor([56, 1, 960, 960],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([56, 1, 960, 960],"float32"), ) 	 51609600 	 1000 	 0.3001885414123535 	 0.30248093605041504 	 0.2918660640716553 	 0.29152536392211914 	 0.4579281806945801 	 0.45363759994506836 	 0.40097522735595703 	 0.38358545303344727 	 
2025-07-24 22:08:28.364610 test begin: paddle.exp(Tensor([8, 1, 6616, 960],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([8, 1, 6616, 960],"float32"), ) 	 50810880 	 1000 	 0.2955763339996338 	 0.29790377616882324 	 0.2872154712677002 	 0.2872297763824463 	 0.4495055675506592 	 0.4480926990509033 	 0.3930826187133789 	 0.3787212371826172 	 
2025-07-24 22:08:31.492551 test begin: paddle.exp(Tensor([8, 1, 960, 6616],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([8, 1, 960, 6616],"float32"), ) 	 50810880 	 1000 	 0.29556751251220703 	 0.9802701473236084 	 0.2871522903442383 	 0.2871577739715576 	 0.4494900703430176 	 0.4468574523925781 	 0.3928489685058594 	 0.36644792556762695 	 
2025-07-24 22:08:36.427321 test begin: paddle.exp(Tensor([8, 7, 960, 960],"float32"), )
[Prof] paddle.exp 	 paddle.exp(Tensor([8, 7, 960, 960],"float32"), ) 	 51609600 	 1000 	 0.30020809173583984 	 0.3178131580352783 	 0.29194045066833496 	 0.29103708267211914 	 0.45656776428222656 	 0.45374321937561035 	 0.40030717849731445 	 0.3708045482635498 	 
2025-07-24 22:08:41.647060 test begin: paddle.expand_as(Tensor([1621, 80, 1, 1],"float32"), Tensor([1621, 80, 28, 28],"float16"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([1621, 80, 1, 1],"float32"), Tensor([1621, 80, 28, 28],"float16"), ) 	 101798800 	 1000 	 0.2701113224029541 	 0.003615856170654297 	 0.25898122787475586 	 2.0265579223632812e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 22:08:45.721318 test begin: paddle.expand_as(Tensor([511, 127, 1, 1],"float32"), Tensor([511, 127, 28, 28],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([511, 127, 1, 1],"float32"), Tensor([511, 127, 28, 28],"float32"), ) 	 50944145 	 1000 	 0.13747882843017578 	 0.0037038326263427734 	 0.12224698066711426 	 2.2172927856445312e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 22:08:47.717418 test begin: paddle.expand_as(Tensor([511, 80, 1, 1243],"float32"), Tensor([511, 80, 28, 1243],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([511, 80, 1, 1243],"float32"), Tensor([511, 80, 28, 1243],"float32"), ) 	 1473601360 	 1000 	 4.060884475708008 	 0.003704071044921875 	 4.049672365188599 	 2.2411346435546875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 22:09:45.145825 test begin: paddle.expand_as(Tensor([511, 80, 1, 1],"float32"), Tensor([511, 80, 28, 45],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([511, 80, 1, 1],"float32"), Tensor([511, 80, 28, 45],"float32"), ) 	 51549680 	 1000 	 0.14034032821655273 	 0.003634929656982422 	 0.1294572353363037 	 1.8358230590820312e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 22:09:48.046492 test begin: paddle.expand_as(Tensor([511, 80, 1, 1],"float32"), Tensor([511, 80, 45, 28],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([511, 80, 1, 1],"float32"), Tensor([511, 80, 45, 28],"float32"), ) 	 51549680 	 1000 	 0.13915657997131348 	 0.003640890121459961 	 0.12829184532165527 	 1.9311904907226562e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 22:09:50.069860 test begin: paddle.expand_as(Tensor([511, 80, 1243, 1],"float32"), Tensor([511, 80, 1243, 28],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([511, 80, 1243, 1],"float32"), Tensor([511, 80, 1243, 28],"float32"), ) 	 1473601360 	 1000 	 4.32255220413208 	 0.003669261932373047 	 4.30939507484436 	 2.0265579223632812e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 22:10:57.048252 test begin: paddle.expand_as(Tensor([512, 127, 1, 1],"float32"), Tensor([512, 127, 28, 28],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 127, 1, 1],"float32"), Tensor([512, 127, 28, 28],"float32"), ) 	 51043840 	 1000 	 0.13899016380310059 	 0.0036170482635498047 	 0.1281290054321289 	 1.9073486328125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 22:10:59.073814 test begin: paddle.expand_as(Tensor([512, 254, 1, 1],"float32"), Tensor([512, 254, 28, 28],"float16"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 254, 1, 1],"float32"), Tensor([512, 254, 28, 28],"float16"), ) 	 102087680 	 1000 	 0.2705070972442627 	 0.003705739974975586 	 0.25952839851379395 	 2.0503997802734375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 22:11:03.209690 test begin: paddle.expand_as(Tensor([512, 80, 1, 1241],"float32"), Tensor([512, 80, 28, 1241],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 80, 1, 1241],"float32"), Tensor([512, 80, 28, 1241],"float32"), ) 	 1474109440 	 1000 	 4.061880111694336 	 0.0036597251892089844 	 4.050696849822998 	 2.1219253540039062e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 22:12:02.312138 test begin: paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 28, 45],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 28, 45],"float32"), ) 	 51650560 	 1000 	 0.1406874656677246 	 0.0036497116088867188 	 0.12976884841918945 	 1.9550323486328125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 22:12:04.520716 test begin: paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 28, 89],"float16"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 28, 89],"float16"), ) 	 102113280 	 1000 	 0.27199268341064453 	 0.0036499500274658203 	 0.26117563247680664 	 2.0265579223632812e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 22:12:08.710795 test begin: paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 45, 28],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 45, 28],"float32"), ) 	 51650560 	 1000 	 0.1394186019897461 	 0.007049083709716797 	 0.12038064002990723 	 2.1219253540039062e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 22:12:10.698461 test begin: paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 89, 28],"float16"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 80, 1, 1],"float32"), Tensor([512, 80, 89, 28],"float16"), ) 	 102113280 	 1000 	 0.2708323001861572 	 0.003652811050415039 	 0.25960659980773926 	 1.8835067749023438e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 22:12:14.944582 test begin: paddle.expand_as(Tensor([512, 80, 1241, 1],"float32"), Tensor([512, 80, 1241, 28],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([512, 80, 1241, 1],"float32"), Tensor([512, 80, 1241, 28],"float32"), ) 	 1474109440 	 1000 	 4.559291839599609 	 0.0037119388580322266 	 4.312138319015503 	 2.2172927856445312e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 22:13:15.322592 test begin: paddle.expand_as(Tensor([811, 80, 1, 1],"float32"), Tensor([811, 80, 28, 28],"float32"), )
[Prof] paddle.expand_as 	 paddle.expand_as(Tensor([811, 80, 1, 1],"float32"), Tensor([811, 80, 28, 28],"float32"), ) 	 50930800 	 1000 	 0.13869404792785645 	 0.0036852359771728516 	 0.127852201461792 	 1.8596649169921875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 22:13:17.374843 test begin: paddle.expm1(Tensor([198451, 16, 32],"float16"), )
[Prof] paddle.expm1 	 paddle.expm1(Tensor([198451, 16, 32],"float16"), ) 	 101606912 	 1000 	 0.33560633659362793 	 0.3059194087982178 	 0.32732224464416504 	 0.29332828521728516 	 0.44735026359558105 	 0.7469890117645264 	 0.39090657234191895 	 0.3823707103729248 	 
2025-07-24 22:13:23.221543 test begin: paddle.expm1(Tensor([8, 16, 793801],"float16"), )
[Prof] paddle.expm1 	 paddle.expm1(Tensor([8, 16, 793801],"float16"), ) 	 101606528 	 1000 	 0.3373591899871826 	 0.31755781173706055 	 0.32919740676879883 	 0.2934129238128662 	 0.44890403747558594 	 0.7455079555511475 	 0.3924992084503174 	 0.38088297843933105 	 
2025-07-24 22:13:28.880159 test begin: paddle.expm1(Tensor([8, 396901, 32],"float16"), )
[Prof] paddle.expm1 	 paddle.expm1(Tensor([8, 396901, 32],"float16"), ) 	 101606656 	 1000 	 0.3357880115509033 	 0.30415916442871094 	 0.3275125026702881 	 0.2935774326324463 	 0.44775819778442383 	 0.7467098236083984 	 0.39080047607421875 	 0.3821084499359131 	 
2025-07-24 22:13:34.607914 test begin: paddle.fft.fftn(Tensor([226801, 7, 32],"float32"), )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([226801, 7, 32],"float32"), ) 	 50803424 	 1000 	 11.848287582397461 	 16.024694681167603 	 5.269050598144531e-05 	 1.8184969425201416 	 19.627935647964478 	 15.545232772827148 	 1.5440878868103027 	 1.987546443939209 	 
2025-07-24 22:14:42.564093 test begin: paddle.fft.fftn(Tensor([39, 40708, 32],"float32"), )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([39, 40708, 32],"float32"), ) 	 50803584 	 1000 	 9.413960456848145 	 10.591337203979492 	 4.315376281738281e-05 	 1.0853023529052734 	 11.895265817642212 	 10.13899564743042 	 1.0110626220703125 	 1.1511321067810059 	 
2025-07-24 22:15:26.637734 test begin: paddle.fft.fftn(Tensor([39, 7, 186093],"float32"), )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([39, 7, 186093],"float32"), ) 	 50803389 	 1000 	 6.988870620727539 	 5.999019384384155 	 3.075599670410156e-05 	 0.7667534351348877 	 7.557720422744751 	 5.5215137004852295 	 0.7726643085479736 	 0.8061506748199463 	 
2025-07-24 22:15:56.309034 test begin: paddle.fft.fftn(Tensor([7, 32, 481, 481],"float32"), axes=list[2,3,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([7, 32, 481, 481],"float32"), axes=list[2,3,], ) 	 51824864 	 1000 	 5.995145559310913 	 4.337397336959839 	 2.288818359375e-05 	 0.8844037055969238 	 5.527525186538696 	 3.874908447265625 	 0.8064024448394775 	 0.9912924766540527 	 
2025-07-24 22:16:18.600241 test begin: paddle.fft.fftn(Tensor([8, 28, 481, 481],"float32"), axes=list[2,3,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([8, 28, 481, 481],"float32"), axes=list[2,3,], ) 	 51824864 	 1000 	 5.993123769760132 	 4.331884860992432 	 1.9073486328125e-05 	 0.8845908641815186 	 5.527235269546509 	 3.866745948791504 	 0.8063621520996094 	 0.9900894165039062 	 
2025-07-24 22:16:40.312853 test begin: paddle.fft.fftn(Tensor([8, 32, 413, 481],"float32"), axes=list[2,3,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([8, 32, 413, 481],"float32"), axes=list[2,3,], ) 	 50855168 	 1000 	 5.9767746925354 	 4.235492706298828 	 1.9788742065429688e-05 	 0.8650410175323486 	 5.6094465255737305 	 3.7532448768615723 	 0.8183953762054443 	 0.9578921794891357 	 
2025-07-24 22:17:01.963018 test begin: paddle.fft.fftn(Tensor([8, 32, 481, 413],"float32"), axes=list[2,3,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(Tensor([8, 32, 481, 413],"float32"), axes=list[2,3,], ) 	 50855168 	 1000 	 5.956332683563232 	 4.2304768562316895 	 2.1457672119140625e-05 	 0.8640425205230713 	 5.626366853713989 	 3.747235059738159 	 0.8208470344543457 	 0.9572820663452148 	 
2025-07-24 22:17:23.946289 test begin: paddle.fft.fftn(x=Tensor([50, 133, 39, 14, 14],"float32"), axes=list[-3,-2,-1,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(x=Tensor([50, 133, 39, 14, 14],"float32"), axes=list[-3,-2,-1,], ) 	 50832600 	 1000 	 6.023988485336304 	 2.9738614559173584 	 2.7179718017578125e-05 	 0.6079258918762207 	 4.383137941360474 	 2.4983177185058594 	 0.6393682956695557 	 0.6372072696685791 	 
2025-07-24 22:17:43.016153 test begin: paddle.fft.fftn(x=Tensor([50, 8, 39, 14, 233],"float32"), axes=list[-3,-2,-1,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(x=Tensor([50, 8, 39, 14, 233],"float32"), axes=list[-3,-2,-1,], ) 	 50887200 	 1000 	 6.807507753372192 	 3.6986372470855713 	 1.9788742065429688e-05 	 0.7555522918701172 	 5.966328144073486 	 3.214036464691162 	 0.8704004287719727 	 0.8211092948913574 	 
2025-07-24 22:18:04.882536 test begin: paddle.fft.fftn(x=Tensor([50, 8, 39, 233, 14],"float32"), axes=list[-3,-2,-1,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(x=Tensor([50, 8, 39, 233, 14],"float32"), axes=list[-3,-2,-1,], ) 	 50887200 	 1000 	 7.235428810119629 	 4.14124059677124 	 2.2649765014648438e-05 	 0.846412181854248 	 6.538707971572876 	 3.6561408042907715 	 0.9537746906280518 	 0.934298038482666 	 
2025-07-24 22:18:28.806999 test begin: paddle.fft.fftn(x=Tensor([50, 8, 649, 14, 14],"float32"), axes=list[-3,-2,-1,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(x=Tensor([50, 8, 649, 14, 14],"float32"), axes=list[-3,-2,-1,], ) 	 50881600 	 1000 	 6.378958225250244 	 3.5591330528259277 	 1.9550323486328125e-05 	 0.7281036376953125 	 4.9662816524505615 	 3.074124336242676 	 0.7244255542755127 	 0.7850387096405029 	 
2025-07-24 22:18:49.223456 test begin: paddle.fft.fftn(x=Tensor([831, 8, 39, 14, 14],"float32"), axes=list[-3,-2,-1,], )
[Prof] paddle.fft.fftn 	 paddle.fft.fftn(x=Tensor([831, 8, 39, 14, 14],"float32"), axes=list[-3,-2,-1,], ) 	 50817312 	 1000 	 6.0301597118377686 	 2.9758262634277344 	 1.9788742065429688e-05 	 0.6091794967651367 	 4.381076335906982 	 2.493330478668213 	 0.6389782428741455 	 0.6369421482086182 	 
2025-07-24 22:19:07.094711 test begin: paddle.fft.ifftn(x=Tensor([4, 4, 6, 264601],"float64"), s=list[2,4,], axes=tuple(0,1,), )
[Prof] paddle.fft.ifftn 	 paddle.fft.ifftn(x=Tensor([4, 4, 6, 264601],"float64"), s=list[2,4,], axes=tuple(0,1,), ) 	 25401696 	 1000 	 3.000368595123291 	 1.3907711505889893 	 1.8835067749023438e-05 	 0.35524559020996094 	 2.8672454357147217 	 1.9031288623809814 	 0.36720919609069824 	 0.39035630226135254 	 
2025-07-24 22:19:17.181075 test begin: paddle.fft.ifftn(x=Tensor([4, 4, 793801, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), )
[Prof] paddle.fft.ifftn 	 paddle.fft.ifftn(x=Tensor([4, 4, 793801, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), ) 	 25401632 	 1000 	 2.8399627208709717 	 1.3905844688415527 	 4.506111145019531e-05 	 0.3552439212799072 	 2.602792501449585 	 1.9041149616241455 	 0.33188843727111816 	 0.3902146816253662 	 
2025-07-24 22:19:26.878249 test begin: paddle.fft.ifftn(x=Tensor([4, 529201, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), )
[Prof] paddle.fft.ifftn 	 paddle.fft.ifftn(x=Tensor([4, 529201, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), ) 	 25401648 	 1000 	 0.2559692859649658 	 0.49329352378845215 	 1.6689300537109375e-05 	 0.12573981285095215 	 0.1668848991394043 	 0.7273344993591309 	 0.02128124237060547 	 0.09302425384521484 	 
2025-07-24 22:19:31.232022 test begin: paddle.fft.ifftn(x=Tensor([529201, 4, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), )
[Prof] paddle.fft.ifftn 	 paddle.fft.ifftn(x=Tensor([529201, 4, 6, 2],"float64"), s=list[2,4,], axes=tuple(0,1,), ) 	 25401648 	 1000 	 0.23523640632629395 	 0.49324703216552734 	 1.7404556274414062e-05 	 0.12576651573181152 	 0.1667327880859375 	 0.28015875816345215 	 0.021260976791381836 	 0.05728793144226074 	 
2025-07-24 22:19:34.868584 test begin: paddle.fft.ihfft(x=Tensor([2, 1411201, 3, 3],"float64"), )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([2, 1411201, 3, 3],"float64"), ) 	 25401618 	 1000 	 1.8628144264221191 	 0.7924275398254395 	 0.3808095455169678 	 0.38799548149108887 	 3.4395248889923096 	 2.628016948699951 	 0.5857717990875244 	 0.537104606628418 	 
2025-07-24 22:19:44.921732 test begin: paddle.fft.ihfft(x=Tensor([2, 1411201, 3, 3],"float64"), n=2, )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([2, 1411201, 3, 3],"float64"), n=2, ) 	 25401618 	 1000 	 1.9769022464752197 	 0.7682211399078369 	 0.3374178409576416 	 0.3902738094329834 	 2.872394561767578 	 1.831622838973999 	 0.36785197257995605 	 0.3739442825317383 	 
2025-07-24 22:19:53.444163 test begin: paddle.fft.ihfft(x=Tensor([2, 1411201, 3, 3],"float64"), n=2, axis=1, )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([2, 1411201, 3, 3],"float64"), n=2, axis=1, ) 	 25401618 	 1000 	 0.07243776321411133 	 0.048152923583984375 	 1.71661376953125e-05 	 7.748603820800781e-05 	 0.1677088737487793 	 0.1872859001159668 	 0.021440744400024414 	 0.00011277198791503906 	 
2025-07-24 22:19:54.491280 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 1058401, 3],"float64"), )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([2, 4, 1058401, 3],"float64"), ) 	 25401624 	 1000 	 1.8665030002593994 	 0.7606377601623535 	 0.3819584846496582 	 0.3892078399658203 	 3.4428701400756836 	 2.627080202102661 	 0.5882389545440674 	 0.5371899604797363 	 
2025-07-24 22:20:04.306948 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 1058401, 3],"float64"), n=2, )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([2, 4, 1058401, 3],"float64"), n=2, ) 	 25401624 	 1000 	 1.9756042957305908 	 0.7654621601104736 	 0.33614516258239746 	 0.3902256488800049 	 2.8738739490509033 	 1.829967737197876 	 0.36807990074157715 	 0.3739504814147949 	 
2025-07-24 22:20:12.814222 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 1058401, 3],"float64"), n=2, axis=1, )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([2, 4, 1058401, 3],"float64"), n=2, axis=1, ) 	 25401624 	 1000 	 1.7453243732452393 	 0.6952800750732422 	 0.29818010330200195 	 0.23720717430114746 	 2.4681167602539062 	 1.7043201923370361 	 0.31505560874938965 	 0.2899441719055176 	 
2025-07-24 22:20:20.401801 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 3, 1058401],"float64"), )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([2, 4, 3, 1058401],"float64"), ) 	 25401624 	 1000 	 6.9764564037323 	 6.114844799041748 	 0.5088608264923096 	 0.5678308010101318 	 13.097453832626343 	 11.81479263305664 	 1.0773656368255615 	 1.0065422058105469 	 
2025-07-24 22:21:01.795784 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 3, 1058401],"float64"), n=2, )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([2, 4, 3, 1058401],"float64"), n=2, ) 	 25401624 	 1000 	 0.07301712036132812 	 0.03662872314453125 	 2.3126602172851562e-05 	 4.744529724121094e-05 	 0.16710591316223145 	 0.1525437831878662 	 0.021373510360717773 	 0.00011348724365234375 	 
2025-07-24 22:21:02.765116 test begin: paddle.fft.ihfft(x=Tensor([2, 4, 3, 1058401],"float64"), n=2, axis=1, )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([2, 4, 3, 1058401],"float64"), n=2, axis=1, ) 	 25401624 	 1000 	 1.8580322265625 	 0.7036676406860352 	 0.3168485164642334 	 0.23730778694152832 	 2.4558653831481934 	 1.7020809650421143 	 0.31477808952331543 	 0.2896866798400879 	 
2025-07-24 22:21:11.628476 test begin: paddle.fft.ihfft(x=Tensor([705601, 4, 3, 3],"float64"), )
[Prof] paddle.fft.ihfft 	 paddle.fft.ihfft(x=Tensor([705601, 4, 3, 3],"float64"), ) 	 25401636 	 1000 	 1.863767147064209 	 0.7635304927825928 	 0.3817260265350342 	 0.38932180404663086 	 3.436335325241089 	 2.6272504329681396 	 0.5861167907714844 	 0.5367786884307861 	 
2025-07-24 22:21:21.376807 test begin: paddle.fft.ihfft2(x=Tensor([1270081, 4, 5],"float64"), )
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([1270081, 4, 5],"float64"), ) 	 25401620 	 1000 	 2.100416898727417 	 2.1315557956695557 	 0.35739946365356445 	 0.3629283905029297 	 3.9989895820617676 	 3.518805980682373 	 0.5831797122955322 	 0.4494030475616455 	 
2025-07-24 22:21:34.212637 test begin: paddle.fft.ihfft2(x=Tensor([2822401, 3, 3],"float64"), s=tuple(1,2,), )
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([2822401, 3, 3],"float64"), s=tuple(1,2,), ) 	 25401609 	 1000 	 0.7736992835998535 	 0.6327979564666748 	 0.1318042278289795 	 0.16132593154907227 	 1.149075984954834 	 1.163989543914795 	 0.1468503475189209 	 0.1485595703125 	 
2025-07-24 22:21:40.155808 test begin: paddle.fft.ihfft2(x=Tensor([3, 1693441, 5],"float64"), )
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([3, 1693441, 5],"float64"), ) 	 25401615 	 1000 	 8.851902961730957 	 7.387439489364624 	 0.6971664428710938 	 0.6874022483825684 	 15.557216882705688 	 8.785609006881714 	 1.1346473693847656 	 0.6904919147491455 	 
2025-07-24 22:22:21.899221 test begin: paddle.fft.ihfft2(x=Tensor([3, 4, 2116801],"float64"), )
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([3, 4, 2116801],"float64"), ) 	 25401612 	 1000 	 9.116385459899902 	 7.318528413772583 	 0.621366024017334 	 0.49782800674438477 	 13.65082836151123 	 13.148953914642334 	 0.996124267578125 	 0.8950738906860352 	 
2025-07-24 22:23:06.384805 test begin: paddle.fft.ihfft2(x=Tensor([4, 2116801, 3],"float64"), s=tuple(1,2,), )
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([4, 2116801, 3],"float64"), s=tuple(1,2,), ) 	 25401612 	 1000 	 0.0982964038848877 	 0.060420989990234375 	 2.0742416381835938e-05 	 7.724761962890625e-05 	 0.16867470741271973 	 0.19773340225219727 	 0.021528244018554688 	 0.00011229515075683594 	 
2025-07-24 22:23:07.543767 test begin: paddle.fft.ihfft2(x=Tensor([4, 3, 2116801],"float64"), s=tuple(1,2,), )
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([4, 3, 2116801],"float64"), s=tuple(1,2,), ) 	 25401612 	 1000 	 0.09736037254333496 	 0.061162471771240234 	 1.7642974853515625e-05 	 6.628036499023438e-05 	 0.16885018348693848 	 0.3034200668334961 	 0.02155137062072754 	 0.03859972953796387 	 
2025-07-24 22:23:08.766891 test begin: paddle.fft.ihfft2(x=Tensor([4, 3, 3, 705601],"float64"), )
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([4, 3, 3, 705601],"float64"), ) 	 25401636 	 1000 	 7.292667865753174 	 7.315449953079224 	 0.4960916042327881 	 0.4977853298187256 	 13.630647659301758 	 12.766148567199707 	 0.9943070411682129 	 0.8690636157989502 	 
2025-07-24 22:23:52.356438 test begin: paddle.fft.ihfft2(x=Tensor([4, 3, 705601, 3],"float64"), )
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([4, 3, 705601, 3],"float64"), ) 	 25401636 	 1000 	 9.470293045043945 	 9.000355958938599 	 0.7444131374359131 	 0.70914626121521 	 15.130207061767578 	 10.496326923370361 	 1.1060473918914795 	 0.7158026695251465 	 
2025-07-24 22:24:39.960830 test begin: paddle.fft.ihfft2(x=Tensor([4, 705601, 3, 3],"float64"), )
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([4, 705601, 3, 3],"float64"), ) 	 25401636 	 1000 	 2.2703654766082764 	 2.3806560039520264 	 0.3862466812133789 	 0.40528321266174316 	 4.042365550994873 	 3.858539581298828 	 0.5897610187530518 	 0.49269628524780273 	 
2025-07-24 22:24:53.526266 test begin: paddle.fft.ihfft2(x=Tensor([940801, 3, 3, 3],"float64"), )
[Prof] paddle.fft.ihfft2 	 paddle.fft.ihfft2(x=Tensor([940801, 3, 3, 3],"float64"), ) 	 25401627 	 1000 	 2.2702577114105225 	 2.3935182094573975 	 0.38622522354125977 	 0.40534114837646484 	 4.049254655838013 	 3.858067274093628 	 0.5907871723175049 	 0.4927797317504883 	 
2025-07-24 22:25:08.620120 test begin: paddle.fft.ihfftn(Tensor([1270081, 4, 5],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([1270081, 4, 5],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401620 	 1000 	 2.0994462966918945 	 2.134052038192749 	 0.35866308212280273 	 0.3628387451171875 	 3.999081611633301 	 3.5180389881134033 	 0.5837371349334717 	 0.44931817054748535 	 
2025-07-24 22:25:21.391125 test begin: paddle.fft.ihfftn(Tensor([3, 1693441, 5],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([3, 1693441, 5],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401615 	 1000 	 8.855096817016602 	 7.388751029968262 	 0.6957066059112549 	 0.6857891082763672 	 15.55074691772461 	 8.78373670578003 	 1.135587215423584 	 0.6889111995697021 	 
2025-07-24 22:26:03.099977 test begin: paddle.fft.ihfftn(Tensor([3, 4, 2116801],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([3, 4, 2116801],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401612 	 1000 	 9.113310098648071 	 7.317084074020386 	 0.6200149059295654 	 0.4988522529602051 	 13.653995752334595 	 13.14405083656311 	 0.996016263961792 	 0.8950977325439453 	 
2025-07-24 22:26:48.056650 test begin: paddle.fft.ihfftn(Tensor([4, 3, 3, 705601],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([4, 3, 3, 705601],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401636 	 1000 	 7.296853542327881 	 7.311607599258423 	 0.49748826026916504 	 0.4975893497467041 	 13.631728649139404 	 12.763151407241821 	 0.9956986904144287 	 0.8692185878753662 	 
2025-07-24 22:27:30.101296 test begin: paddle.fft.ihfftn(Tensor([4, 3, 705601, 3],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([4, 3, 705601, 3],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401636 	 1000 	 9.46876859664917 	 9.000669002532959 	 0.7444911003112793 	 0.7092404365539551 	 15.126308917999268 	 10.499888896942139 	 1.103447437286377 	 0.7146844863891602 	 
2025-07-24 22:28:15.368389 test begin: paddle.fft.ihfftn(Tensor([4, 705601, 3, 3],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([4, 705601, 3, 3],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401636 	 1000 	 2.270082712173462 	 2.386720895767212 	 0.38753390312194824 	 0.40546321868896484 	 4.0408711433410645 	 3.8592283725738525 	 0.5897777080535889 	 0.4925048351287842 	 
2025-07-24 22:28:30.011267 test begin: paddle.fft.ihfftn(Tensor([940801, 3, 3, 3],"float64"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(Tensor([940801, 3, 3, 3],"float64"), None, tuple(-2,-1,), "backward", None, ) 	 25401627 	 1000 	 2.2705745697021484 	 2.831300735473633 	 0.3862276077270508 	 0.40535426139831543 	 4.0461649894714355 	 3.8589885234832764 	 0.5906996726989746 	 0.4925386905670166 	 
2025-07-24 22:28:44.329136 test begin: paddle.fft.ihfftn(x=Tensor([4, 3, 1058401, 2],"float64"), )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(x=Tensor([4, 3, 1058401, 2],"float64"), ) 	 25401624 	 1000 	 15.233215808868408 	 14.126906156539917 	 0.9178979396820068 	 1.0307984352111816 	 18.4233877658844 	 14.27579927444458 	 1.047189712524414 	 1.041351079940796 	 
2025-07-24 22:29:47.817854 test begin: paddle.fft.ihfftn(x=Tensor([4, 3, 5, 423361],"float64"), )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(x=Tensor([4, 3, 5, 423361],"float64"), ) 	 25401660 	 1000 	 7.296338796615601 	 6.415631532669067 	 0.43930864334106445 	 0.46797919273376465 	 15.390176057815552 	 10.897495031356812 	 0.9830605983734131 	 0.7947134971618652 	 
2025-07-24 22:30:28.918774 test begin: paddle.fft.ihfftn(x=Tensor([4, 635041, 5, 2],"float64"), )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(x=Tensor([4, 635041, 5, 2],"float64"), ) 	 25401640 	 1000 	 16.861624479293823 	 15.66196584701538 	 1.012387990951538 	 1.1436479091644287 	 19.99099826812744 	 15.796404600143433 	 1.1343700885772705 	 1.1524324417114258 	 
2025-07-24 22:31:41.316944 test begin: paddle.fft.ihfftn(x=Tensor([846721, 3, 5, 2],"float64"), )
[Prof] paddle.fft.ihfftn 	 paddle.fft.ihfftn(x=Tensor([846721, 3, 5, 2],"float64"), ) 	 25401630 	 1000 	 18.94514536857605 	 15.582878112792969 	 1.139495611190796 	 1.3246994018554688 	 17.172597408294678 	 15.716692447662354 	 0.9780800342559814 	 1.3380706310272217 	 
2025-07-24 22:32:51.817849 test begin: paddle.fft.rfft(Tensor([20, 1210, 2101],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([20, 1210, 2101],"float32"), ) 	 50844200 	 1000 	 2.6068532466888428 	 2.0179617404937744 	 0.5321903228759766 	 0.6871225833892822 	 4.907825469970703 	 3.3866021633148193 	 1.0015974044799805 	 1.1545324325561523 	 
2025-07-24 22:33:06.406929 test begin: paddle.fft.rfft(Tensor([20, 1270, 2001],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([20, 1270, 2001],"float32"), ) 	 50825400 	 1000 	 1.986424207687378 	 1.329636573791504 	 0.40569496154785156 	 0.45258569717407227 	 3.684368371963501 	 2.020231246948242 	 0.7539827823638916 	 0.6881701946258545 	 
2025-07-24 22:33:16.891115 test begin: paddle.fft.rfft(Tensor([20, 64, 39691],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([20, 64, 39691],"float32"), ) 	 50804480 	 1000 	 4.73757004737854 	 4.113908290863037 	 0.48358917236328125 	 0.5254387855529785 	 9.105918407440186 	 7.514086008071899 	 0.9289712905883789 	 0.9606964588165283 	 
2025-07-24 22:33:45.037196 test begin: paddle.fft.rfft(Tensor([378, 64, 2101],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([378, 64, 2101],"float32"), ) 	 50827392 	 1000 	 2.6058590412139893 	 2.0131418704986572 	 0.5319228172302246 	 0.6847569942474365 	 4.887800693511963 	 3.3770997524261475 	 0.9972176551818848 	 1.1510217189788818 	 
2025-07-24 22:34:00.854195 test begin: paddle.fft.rfft(Tensor([397, 64, 2001],"float32"), )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([397, 64, 2001],"float32"), ) 	 50841408 	 1000 	 1.9834935665130615 	 1.3297529220581055 	 0.4054601192474365 	 0.4530925750732422 	 3.658034563064575 	 2.023409366607666 	 0.7461342811584473 	 0.6910905838012695 	 
2025-07-24 22:34:11.238511 test begin: paddle.fft.rfft(Tensor([4, 32, 32, 12404],"float32"), axis=-1, norm="forward", )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([4, 32, 32, 12404],"float32"), axis=-1, norm="forward", ) 	 50806784 	 1000 	 4.898017644882202 	 4.34593939781189 	 0.4998283386230469 	 0.5545682907104492 	 10.133789777755737 	 8.768836975097656 	 0.941133975982666 	 0.9975242614746094 	 
2025-07-24 22:34:40.867473 test begin: paddle.fft.rfft(Tensor([4, 32, 6202, 64],"float32"), axis=-1, norm="forward", )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([4, 32, 6202, 64],"float32"), axis=-1, norm="forward", ) 	 50806784 	 1000 	 1.2719807624816895 	 0.6260993480682373 	 0.32497239112854004 	 0.3198585510253906 	 3.5146491527557373 	 1.8386306762695312 	 0.5978834629058838 	 0.46988797187805176 	 
2025-07-24 22:34:49.518483 test begin: paddle.fft.rfft(Tensor([4, 6202, 32, 64],"float32"), axis=-1, norm="forward", )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([4, 6202, 32, 64],"float32"), axis=-1, norm="forward", ) 	 50806784 	 1000 	 1.2747490406036377 	 0.6261281967163086 	 0.32495570182800293 	 0.31988000869750977 	 3.513671398162842 	 1.8385858535766602 	 0.6005542278289795 	 0.470123291015625 	 
2025-07-24 22:34:58.175126 test begin: paddle.fft.rfft(Tensor([776, 32, 32, 64],"float32"), axis=-1, norm="forward", )
[Prof] paddle.fft.rfft 	 paddle.fft.rfft(Tensor([776, 32, 32, 64],"float32"), axis=-1, norm="forward", ) 	 50855936 	 1000 	 1.277404546737671 	 0.6405034065246582 	 0.32537364959716797 	 0.3201875686645508 	 3.5116870403289795 	 1.8426387310028076 	 0.5977067947387695 	 0.4703822135925293 	 
2025-07-24 22:35:08.486220 test begin: paddle.fft.rfft2(Tensor([26, 32, 250, 250],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([26, 32, 250, 250],"float32"), ) 	 52000000 	 1000 	 1.478320598602295 	 0.8262269496917725 	 0.37755584716796875 	 0.422579288482666 	 3.5966572761535645 	 1.924816608428955 	 0.6120600700378418 	 0.49173426628112793 	 
2025-07-24 22:35:17.810324 test begin: paddle.fft.rfft2(Tensor([32, 26, 250, 250],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([32, 26, 250, 250],"float32"), ) 	 52000000 	 1000 	 1.478076696395874 	 0.8272988796234131 	 0.37755274772644043 	 0.42340517044067383 	 3.5953688621520996 	 1.9249458312988281 	 0.6120269298553467 	 0.4917769432067871 	 
2025-07-24 22:35:27.075442 test begin: paddle.fft.rfft2(Tensor([32, 32, 199, 250],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([32, 32, 199, 250],"float32"), ) 	 50944000 	 1000 	 2.6967384815216064 	 1.4932494163513184 	 0.6884596347808838 	 0.7610633373260498 	 6.125044822692871 	 3.2622108459472656 	 1.1794626712799072 	 0.8334131240844727 	 
2025-07-24 22:35:42.761740 test begin: paddle.fft.rfft2(Tensor([32, 32, 250, 199],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([32, 32, 250, 199],"float32"), ) 	 50944000 	 1000 	 2.739459753036499 	 1.643268346786499 	 0.46558642387390137 	 0.420640230178833 	 5.239642381668091 	 2.7109086513519287 	 0.8911271095275879 	 0.6926321983337402 	 
2025-07-24 22:35:56.552940 test begin: paddle.fft.rfft2(Tensor([8, 102, 250, 250],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([8, 102, 250, 250],"float32"), ) 	 51000000 	 1000 	 1.4532535076141357 	 0.8303036689758301 	 0.3705143928527832 	 0.4151020050048828 	 3.5284740924835205 	 1.885993480682373 	 0.6009218692779541 	 0.48212480545043945 	 
2025-07-24 22:36:05.712448 test begin: paddle.fft.rfft2(Tensor([8, 32, 250, 794],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([8, 32, 250, 794],"float32"), ) 	 50816000 	 1000 	 2.1062610149383545 	 1.4693152904510498 	 0.4312167167663574 	 0.49922728538513184 	 4.391627073287964 	 2.736600637435913 	 0.7469167709350586 	 0.6995630264282227 	 
2025-07-24 22:36:19.409498 test begin: paddle.fft.rfft2(Tensor([8, 32, 794, 250],"float32"), )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(Tensor([8, 32, 794, 250],"float32"), ) 	 50816000 	 1000 	 2.3261935710906982 	 1.6316413879394531 	 0.5953192710876465 	 0.8350484371185303 	 5.2463767528533936 	 3.5154337882995605 	 0.8921892642974854 	 0.8989295959472656 	 
2025-07-24 22:36:33.515659 test begin: paddle.fft.rfft2(x=Tensor([32, 15, 15, 7057],"float32"), axes=tuple(1,2,), norm="ortho", )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(x=Tensor([32, 15, 15, 7057],"float32"), axes=tuple(1,2,), norm="ortho", ) 	 50810400 	 1000 	 1.6015236377716064 	 1.5903651714324951 	 0.3271596431732178 	 0.40509486198425293 	 3.9318695068359375 	 3.0510146617889404 	 0.6702733039855957 	 0.6248705387115479 	 
2025-07-24 22:36:46.207947 test begin: paddle.fft.rfft2(x=Tensor([32, 15, 414, 256],"float32"), axes=tuple(1,2,), norm="ortho", )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(x=Tensor([32, 15, 414, 256],"float32"), axes=tuple(1,2,), norm="ortho", ) 	 50872320 	 1000 	 2.3217482566833496 	 1.8688623905181885 	 0.3389856815338135 	 0.3767695426940918 	 4.2689900398254395 	 3.1281301975250244 	 0.622978925704956 	 0.534365177154541 	 
2025-07-24 22:36:59.304855 test begin: paddle.fft.rfft2(x=Tensor([32, 414, 15, 256],"float32"), axes=tuple(1,2,), norm="ortho", )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(x=Tensor([32, 414, 15, 256],"float32"), axes=tuple(1,2,), norm="ortho", ) 	 50872320 	 1000 	 1.6711299419403076 	 1.6710679531097412 	 0.3414909839630127 	 0.4277472496032715 	 4.3012754917144775 	 3.4335577487945557 	 0.6273622512817383 	 0.5848042964935303 	 
2025-07-24 22:37:13.284760 test begin: paddle.fft.rfft2(x=Tensor([883, 15, 15, 256],"float32"), axes=tuple(1,2,), norm="ortho", )
[Prof] paddle.fft.rfft2 	 paddle.fft.rfft2(x=Tensor([883, 15, 15, 256],"float32"), axes=tuple(1,2,), norm="ortho", ) 	 50860800 	 1000 	 1.5669198036193848 	 1.5657823085784912 	 0.3202643394470215 	 0.40088963508605957 	 3.898402690887451 	 3.0276846885681152 	 0.6630837917327881 	 0.6187624931335449 	 
2025-07-24 22:37:25.042769 test begin: paddle.fft.rfftn(Tensor([26, 32, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([26, 32, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 52000000 	 1000 	 1.4827184677124023 	 0.824232816696167 	 0.37758517265319824 	 0.42073559761047363 	 3.5969653129577637 	 1.9238147735595703 	 0.6150002479553223 	 0.49193644523620605 	 
2025-07-24 22:37:34.355886 test begin: paddle.fft.rfftn(Tensor([32, 15, 15, 7057],"float32"), None, tuple(1,2,), "ortho", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 15, 15, 7057],"float32"), None, tuple(1,2,), "ortho", None, ) 	 50810400 	 1000 	 1.6003568172454834 	 1.5868315696716309 	 0.3270547389984131 	 0.40519165992736816 	 3.9332799911499023 	 3.0523128509521484 	 0.6716322898864746 	 0.6260731220245361 	 
2025-07-24 22:37:46.334846 test begin: paddle.fft.rfftn(Tensor([32, 15, 414, 256],"float32"), None, tuple(1,2,), "ortho", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 15, 414, 256],"float32"), None, tuple(1,2,), "ortho", None, ) 	 50872320 	 1000 	 2.3217599391937256 	 1.8473749160766602 	 0.3389246463775635 	 0.3766968250274658 	 4.270223379135132 	 3.127948045730591 	 0.6241881847381592 	 0.5327141284942627 	 
2025-07-24 22:37:59.335442 test begin: paddle.fft.rfftn(Tensor([32, 26, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 26, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 52000000 	 1000 	 1.4785118103027344 	 0.8250064849853516 	 0.3775966167449951 	 0.4225926399230957 	 3.5984067916870117 	 1.926516056060791 	 0.612065315246582 	 0.4931609630584717 	 
2025-07-24 22:38:08.781252 test begin: paddle.fft.rfftn(Tensor([32, 32, 199, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 32, 199, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 50944000 	 1000 	 2.6933510303497314 	 1.4984807968139648 	 0.6886098384857178 	 0.7613320350646973 	 5.957470655441284 	 3.262232542037964 	 1.013505220413208 	 0.8337206840515137 	 
2025-07-24 22:38:24.780308 test begin: paddle.fft.rfftn(Tensor([32, 32, 250, 199],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 32, 250, 199],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 50944000 	 1000 	 2.739427089691162 	 1.6447997093200684 	 0.4670133590698242 	 0.41925811767578125 	 5.240200757980347 	 2.70913028717041 	 0.8912341594696045 	 0.6927883625030518 	 
2025-07-24 22:38:39.788578 test begin: paddle.fft.rfftn(Tensor([32, 414, 15, 256],"float32"), None, tuple(1,2,), "ortho", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([32, 414, 15, 256],"float32"), None, tuple(1,2,), "ortho", None, ) 	 50872320 	 1000 	 1.6722214221954346 	 1.6773159503936768 	 0.34147071838378906 	 0.4277520179748535 	 4.301680564880371 	 3.434433937072754 	 0.6274240016937256 	 0.5847022533416748 	 
2025-07-24 22:38:52.370720 test begin: paddle.fft.rfftn(Tensor([8, 102, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([8, 102, 250, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 51000000 	 1000 	 1.4522004127502441 	 0.8115777969360352 	 0.37052249908447266 	 0.41564273834228516 	 3.5313117504119873 	 1.8856782913208008 	 0.602325439453125 	 0.48215413093566895 	 
2025-07-24 22:39:01.435756 test begin: paddle.fft.rfftn(Tensor([8, 32, 250, 794],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([8, 32, 250, 794],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 50816000 	 1000 	 2.107788562774658 	 1.4667344093322754 	 0.43001270294189453 	 0.4992249011993408 	 4.392222881317139 	 2.7380571365356445 	 0.7469906806945801 	 0.6996293067932129 	 
2025-07-24 22:39:13.564459 test begin: paddle.fft.rfftn(Tensor([8, 32, 794, 250],"float32"), None, tuple(-2,-1,), "backward", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([8, 32, 794, 250],"float32"), None, tuple(-2,-1,), "backward", None, ) 	 50816000 	 1000 	 2.3262457847595215 	 1.858168125152588 	 0.5939698219299316 	 0.8322892189025879 	 5.246191740036011 	 3.518385648727417 	 0.8922262191772461 	 0.8989012241363525 	 
2025-07-24 22:39:30.015748 test begin: paddle.fft.rfftn(Tensor([883, 15, 15, 256],"float32"), None, tuple(1,2,), "ortho", None, )
[Prof] paddle.fft.rfftn 	 paddle.fft.rfftn(Tensor([883, 15, 15, 256],"float32"), None, tuple(1,2,), "ortho", None, ) 	 50860800 	 1000 	 1.5707695484161377 	 1.576852798461914 	 0.3217129707336426 	 0.3995704650878906 	 3.8939173221588135 	 3.0293173789978027 	 0.6630079746246338 	 0.6186621189117432 	 
2025-07-24 22:39:42.791706 test begin: paddle.flatten(Tensor([4051, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([4051, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, ) 	 50815744 	 1000 	 0.005700826644897461 	 0.004301548004150391 	 9.059906005859375e-06 	 4.3392181396484375e-05 	 0.06917762756347656 	 0.0581212043762207 	 2.7179718017578125e-05 	 6.508827209472656e-05 	 
2025-07-24 22:39:44.589625 test begin: paddle.flatten(Tensor([4096, 254, 7, 7],"float32"), start_axis=1, stop_axis=-1, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([4096, 254, 7, 7],"float32"), start_axis=1, stop_axis=-1, ) 	 50978816 	 1000 	 0.005689144134521484 	 0.004185676574707031 	 1.1444091796875e-05 	 1.9311904907226562e-05 	 0.04478573799133301 	 0.056702613830566406 	 2.2649765014648438e-05 	 4.220008850097656e-05 	 
2025-07-24 22:39:46.328547 test begin: paddle.flatten(Tensor([4096, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([4096, 256, 7, 7],"float32"), start_axis=1, stop_axis=-1, ) 	 51380224 	 1000 	 0.005669116973876953 	 0.004134416580200195 	 7.3909759521484375e-06 	 1.9788742065429688e-05 	 0.0444331169128418 	 0.05780839920043945 	 2.5510787963867188e-05 	 5.459785461425781e-05 	 
2025-07-24 22:39:48.085806 test begin: paddle.flatten(Tensor([416, 50, 10, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([416, 50, 10, 256],"float32"), start_axis=2, ) 	 53248000 	 1000 	 0.005601406097412109 	 0.004039764404296875 	 1.430511474609375e-05 	 1.7881393432617188e-05 	 0.04457283020019531 	 0.05875730514526367 	 2.09808349609375e-05 	 7.700920104980469e-05 	 
2025-07-24 22:39:49.984225 test begin: paddle.flatten(Tensor([416, 50, 7, 349],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([416, 50, 7, 349],"float32"), start_axis=2, ) 	 50814400 	 1000 	 0.005669832229614258 	 0.004046916961669922 	 8.344650268554688e-06 	 1.9073486328125e-05 	 0.04487109184265137 	 0.059287071228027344 	 1.8596649169921875e-05 	 6.842613220214844e-05 	 
2025-07-24 22:39:51.727368 test begin: paddle.flatten(Tensor([416, 69, 7, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([416, 69, 7, 256],"float32"), start_axis=2, ) 	 51437568 	 1000 	 0.005559444427490234 	 0.004024505615234375 	 7.3909759521484375e-06 	 2.002716064453125e-05 	 0.0643460750579834 	 0.05807995796203613 	 2.7894973754882812e-05 	 5.91278076171875e-05 	 
2025-07-24 22:39:53.577824 test begin: paddle.flatten(Tensor([512, 50, 7, 284],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([512, 50, 7, 284],"float32"), start_axis=2, ) 	 50892800 	 1000 	 0.005651235580444336 	 0.004002571105957031 	 2.0265579223632812e-05 	 1.9311904907226562e-05 	 0.04456305503845215 	 0.059067726135253906 	 2.7418136596679688e-05 	 6.079673767089844e-05 	 
2025-07-24 22:39:55.330228 test begin: paddle.flatten(Tensor([512, 50, 8, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([512, 50, 8, 256],"float32"), start_axis=2, ) 	 52428800 	 1000 	 0.00550532341003418 	 0.00406193733215332 	 7.867813110351562e-06 	 1.9073486328125e-05 	 0.04461526870727539 	 0.05750679969787598 	 2.5033950805664062e-05 	 4.9114227294921875e-05 	 
2025-07-24 22:39:57.122891 test begin: paddle.flatten(Tensor([512, 56, 7, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([512, 56, 7, 256],"float32"), start_axis=2, ) 	 51380224 	 1000 	 0.0055239200592041016 	 0.004068851470947266 	 8.106231689453125e-06 	 2.4557113647460938e-05 	 0.04474449157714844 	 0.057584524154663086 	 3.0040740966796875e-05 	 7.176399230957031e-05 	 
2025-07-24 22:39:59.047663 test begin: paddle.flatten(Tensor([568, 50, 7, 256],"float32"), start_axis=2, )
[Prof] paddle.flatten 	 paddle.flatten(Tensor([568, 50, 7, 256],"float32"), start_axis=2, ) 	 50892800 	 1000 	 0.005608797073364258 	 0.004044055938720703 	 1.430511474609375e-05 	 1.9550323486328125e-05 	 0.04486370086669922 	 0.058364152908325195 	 2.6941299438476562e-05 	 4.9114227294921875e-05 	 
2025-07-24 22:40:00.808666 test begin: paddle.flip(Tensor([127, 8, 224, 224],"float32"), axis=list[3,], )
[Prof] paddle.flip 	 paddle.flip(Tensor([127, 8, 224, 224],"float32"), axis=list[3,], ) 	 50978816 	 1000 	 0.9689469337463379 	 0.3133409023284912 	 0.9601337909698486 	 0.29862022399902344 	 0.9730732440948486 	 0.3130381107330322 	 0.9185972213745117 	 0.2429218292236328 	 
2025-07-24 22:40:05.092406 test begin: paddle.flip(Tensor([1351, 3, 112, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([1351, 3, 112, 112],"float32"), axis=-1, ) 	 50840832 	 1000 	 0.969052791595459 	 0.312680721282959 	 0.9602491855621338 	 0.29836249351501465 	 0.9718608856201172 	 0.31275224685668945 	 0.9177794456481934 	 0.24292325973510742 	 
2025-07-24 22:40:09.352793 test begin: paddle.flip(Tensor([3, 338, 224, 224],"float32"), axis=list[3,], )
[Prof] paddle.flip 	 paddle.flip(Tensor([3, 338, 224, 224],"float32"), axis=list[3,], ) 	 50878464 	 1000 	 0.967087984085083 	 0.3222343921661377 	 0.9579207897186279 	 0.2989046573638916 	 0.9683041572570801 	 0.3137552738189697 	 0.914259672164917 	 0.24267864227294922 	 
2025-07-24 22:40:13.657984 test begin: paddle.flip(Tensor([3, 8, 224, 9451],"float32"), axis=list[3,], )
[Prof] paddle.flip 	 paddle.flip(Tensor([3, 8, 224, 9451],"float32"), axis=list[3,], ) 	 50808576 	 1000 	 0.9667325019836426 	 0.31800174713134766 	 0.957970380783081 	 0.30110645294189453 	 0.9668385982513428 	 0.3124222755432129 	 0.9018328189849854 	 0.24343538284301758 	 
2025-07-24 22:40:17.897364 test begin: paddle.flip(Tensor([3, 8, 9451, 224],"float32"), axis=list[3,], )
[Prof] paddle.flip 	 paddle.flip(Tensor([3, 8, 9451, 224],"float32"), axis=list[3,], ) 	 50808576 	 1000 	 0.9659523963928223 	 0.3123605251312256 	 0.9572463035583496 	 0.2981889247894287 	 0.96604323387146 	 0.31369543075561523 	 0.9125142097473145 	 0.24445390701293945 	 
2025-07-24 22:40:22.197816 test begin: paddle.flip(Tensor([52, 3, 112, 2908],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([52, 3, 112, 2908],"float32"), axis=-1, ) 	 50808576 	 1000 	 0.9671816825866699 	 0.3126401901245117 	 0.9583981037139893 	 0.2983837127685547 	 0.967299222946167 	 0.3126811981201172 	 0.8976798057556152 	 0.24381017684936523 	 
2025-07-24 22:40:26.621447 test begin: paddle.flip(Tensor([52, 3, 2908, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([52, 3, 2908, 112],"float32"), axis=-1, ) 	 50808576 	 1000 	 0.9687180519104004 	 0.7666475772857666 	 0.960015058517456 	 0.2981395721435547 	 0.9702250957489014 	 0.31253623962402344 	 0.916449785232544 	 0.2431199550628662 	 
2025-07-24 22:40:32.210428 test begin: paddle.flip(Tensor([52, 78, 112, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([52, 78, 112, 112],"float32"), axis=-1, ) 	 50878464 	 1000 	 0.9712023735046387 	 0.3253779411315918 	 0.9625389575958252 	 0.2981119155883789 	 0.9697871208190918 	 0.3125114440917969 	 0.9162185192108154 	 0.24205613136291504 	 
2025-07-24 22:40:39.286500 test begin: paddle.flip(Tensor([64, 3, 112, 2363],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([64, 3, 112, 2363],"float32"), axis=-1, ) 	 50813952 	 1000 	 0.9687650203704834 	 0.3125591278076172 	 0.9599673748016357 	 0.2984623908996582 	 0.967679500579834 	 0.31258583068847656 	 0.9137430191040039 	 0.22648310661315918 	 
2025-07-24 22:40:43.546211 test begin: paddle.flip(Tensor([64, 3, 2363, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([64, 3, 2363, 112],"float32"), axis=-1, ) 	 50813952 	 1000 	 0.9729442596435547 	 0.3125295639038086 	 0.9642951488494873 	 0.2982053756713867 	 0.9691922664642334 	 0.31255602836608887 	 0.9154174327850342 	 0.22581791877746582 	 
2025-07-24 22:40:47.794037 test begin: paddle.flip(Tensor([64, 64, 112, 112],"float32"), axis=-1, )
[Prof] paddle.flip 	 paddle.flip(Tensor([64, 64, 112, 112],"float32"), axis=-1, ) 	 51380224 	 1000 	 0.9837069511413574 	 0.315706729888916 	 0.9750902652740479 	 0.30172085762023926 	 0.9792375564575195 	 0.3155090808868408 	 0.9243454933166504 	 0.2466583251953125 	 
2025-07-24 22:40:52.128562 test begin: paddle.floor(Tensor([100000, 170, 3],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([100000, 170, 3],"float32"), ) 	 51000000 	 1000 	 0.29686617851257324 	 0.30574560165405273 	 0.28816962242126465 	 0.28803324699401855 	 0.13525176048278809 	 0.134660005569458 	 0.08128762245178223 	 0.06737351417541504 	 
2025-07-24 22:40:54.729925 test begin: paddle.floor(Tensor([100000, 2, 255],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([100000, 2, 255],"float32"), ) 	 51000000 	 1000 	 0.29688239097595215 	 0.2991006374359131 	 0.2882375717163086 	 0.28385066986083984 	 0.13467001914978027 	 0.13463497161865234 	 0.08051681518554688 	 0.06418442726135254 	 
2025-07-24 22:40:57.298219 test begin: paddle.floor(Tensor([322, 157920],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([322, 157920],"float32"), ) 	 50850240 	 1000 	 0.29741382598876953 	 0.29821085929870605 	 0.2886378765106201 	 0.28592848777770996 	 0.13416099548339844 	 0.13431167602539062 	 0.07892942428588867 	 0.06613874435424805 	 
2025-07-24 22:40:59.807422 test begin: paddle.floor(Tensor([4, 12700801],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([4, 12700801],"float32"), ) 	 50803204 	 1000 	 0.29576778411865234 	 0.29796600341796875 	 0.2870051860809326 	 0.2873709201812744 	 0.13417458534240723 	 0.1342313289642334 	 0.08003997802734375 	 0.06743717193603516 	 
2025-07-24 22:41:02.408723 test begin: paddle.floor(Tensor([8467201, 2, 3],"float32"), )
[Prof] paddle.floor 	 paddle.floor(Tensor([8467201, 2, 3],"float32"), ) 	 50803206 	 1000 	 0.2957594394683838 	 0.2979736328125 	 0.2870926856994629 	 0.2872962951660156 	 0.13415074348449707 	 0.13437676429748535 	 0.07689332962036133 	 0.03600931167602539 	 
2025-07-24 22:41:04.975515 test begin: paddle.floor(x=Tensor([100, 352, 38, 38],"float32"), )
[Prof] paddle.floor 	 paddle.floor(x=Tensor([100, 352, 38, 38],"float32"), ) 	 50828800 	 1000 	 0.29576969146728516 	 0.3054654598236084 	 0.28699302673339844 	 0.2872927188873291 	 0.13412785530090332 	 0.13447856903076172 	 0.08009076118469238 	 0.05849885940551758 	 
2025-07-24 22:41:07.994247 test begin: paddle.floor(x=Tensor([100, 4, 3343, 38],"float32"), )
[Prof] paddle.floor 	 paddle.floor(x=Tensor([100, 4, 3343, 38],"float32"), ) 	 50813600 	 1000 	 0.2958052158355713 	 0.3019144535064697 	 0.287050724029541 	 0.2869455814361572 	 0.1341862678527832 	 0.13432621955871582 	 0.08017420768737793 	 0.0658726692199707 	 
2025-07-24 22:41:10.504864 test begin: paddle.floor(x=Tensor([100, 4, 38, 3343],"float32"), )
[Prof] paddle.floor 	 paddle.floor(x=Tensor([100, 4, 38, 3343],"float32"), ) 	 50813600 	 1000 	 0.295867919921875 	 0.2980022430419922 	 0.2863614559173584 	 0.28749537467956543 	 0.1341540813446045 	 0.13433551788330078 	 0.08043956756591797 	 0.03727316856384277 	 
2025-07-24 22:41:13.004106 test begin: paddle.floor(x=Tensor([8796, 4, 38, 38],"float32"), )
[Prof] paddle.floor 	 paddle.floor(x=Tensor([8796, 4, 38, 38],"float32"), ) 	 50805696 	 1000 	 0.2956211566925049 	 0.298022985458374 	 0.28677821159362793 	 0.28722238540649414 	 0.13427376747131348 	 0.1342935562133789 	 0.07876873016357422 	 0.06394195556640625 	 
2025-07-24 22:41:15.564366 test begin: paddle.floor_divide(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 50803600 	 1000 	 0.42714834213256836 	 0.4947957992553711 	 0.41721343994140625 	 0.47936367988586426 	 None 	 None 	 None 	 None 	 
2025-07-24 22:41:17.357716 test begin: paddle.floor_divide(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), ) 	 50803600 	 1000 	 0.4230635166168213 	 0.49063920974731445 	 0.4129972457885742 	 0.4747805595397949 	 None 	 None 	 None 	 None 	 
2025-07-24 22:41:19.098180 test begin: paddle.floor_divide(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 101606800 	 1000 	 0.4519798755645752 	 0.4553215503692627 	 0.44309139251708984 	 0.4428229331970215 	 None 	 None 	 None 	 None 	 
2025-07-24 22:41:21.776056 test begin: paddle.floor_divide(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), ) 	 50803220 	 1000 	 0.4476332664489746 	 0.44920921325683594 	 0.43890905380249023 	 0.4339320659637451 	 None 	 None 	 None 	 None 	 
2025-07-24 22:41:23.508711 test begin: paddle.floor_divide(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), ) 	 101606420 	 1000 	 0.4534945487976074 	 0.4552466869354248 	 0.44469738006591797 	 0.4426615238189697 	 None 	 None 	 None 	 None 	 
2025-07-24 22:41:26.062807 test begin: paddle.floor_divide(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), ) 	 50804736 	 1000 	 0.4472377300262451 	 0.44780755043029785 	 0.43868160247802734 	 0.43529796600341797 	 None 	 None 	 None 	 None 	 
2025-07-24 22:41:27.775943 test begin: paddle.floor_divide(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.floor_divide 	 paddle.floor_divide(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), ) 	 101606440 	 1000 	 0.4533381462097168 	 0.456165075302124 	 0.4444735050201416 	 0.4426534175872803 	 None 	 None 	 None 	 None 	 
2025-07-24 22:41:30.351458 test begin: paddle.fmax(Tensor([10, 50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([10, 50803201],"float32"), Tensor([50803201],"float32"), ) 	 558835211 	 1000 	 4.447415351867676 	 4.8417510986328125 	 4.431355237960815 	 4.394371509552002 	 45.19587016105652 	 27.211283206939697 	 45.132853746414185 	 1.9887769222259521 	 
2025-07-24 22:43:15.163826 test begin: paddle.fmax(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), ) 	 101606420 	 1000 	 0.4507274627685547 	 0.44797754287719727 	 0.44151949882507324 	 0.43697452545166016 	 0.7338061332702637 	 2.6462759971618652 	 0.6723117828369141 	 0.20787906646728516 	 
2025-07-24 22:43:21.967685 test begin: paddle.fmax(Tensor([10, 5080321],"float32"), Tensor([5080321],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([10, 5080321],"float32"), Tensor([5080321],"float32"), ) 	 55883531 	 1000 	 0.455242395401001 	 0.4592766761779785 	 0.44176411628723145 	 0.4340176582336426 	 4.541280031204224 	 2.7479095458984375 	 4.479034662246704 	 0.2005922794342041 	 
2025-07-24 22:43:32.018317 test begin: paddle.fmax(Tensor([30, 200, 8468],"float32"), Tensor([30, 200, 8468],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([30, 200, 8468],"float32"), Tensor([30, 200, 8468],"float32"), ) 	 101616000 	 1000 	 0.4506196975708008 	 0.45125818252563477 	 0.44146132469177246 	 0.43543124198913574 	 0.7337234020233154 	 2.644257068634033 	 0.6693661212921143 	 0.2076258659362793 	 
2025-07-24 22:43:41.726154 test begin: paddle.fmax(Tensor([30, 42337, 40],"float32"), Tensor([30, 42337, 40],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([30, 42337, 40],"float32"), Tensor([30, 42337, 40],"float32"), ) 	 101608800 	 1000 	 0.4508039951324463 	 0.4501912593841553 	 0.4416046142578125 	 0.4367237091064453 	 0.7352571487426758 	 2.6452999114990234 	 0.6725461483001709 	 0.20801925659179688 	 
2025-07-24 22:43:48.657297 test begin: paddle.fmax(Tensor([3386881, 15],"float32"), Tensor([15],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([3386881, 15],"float32"), Tensor([15],"float32"), ) 	 50803230 	 1000 	 0.29917263984680176 	 0.30574512481689453 	 0.2861475944519043 	 0.2925715446472168 	 8.35862922668457 	 2.4761390686035156 	 8.294026851654053 	 0.1683356761932373 	 
2025-07-24 22:44:03.661267 test begin: paddle.fmax(Tensor([3386881, 15],"float32"), Tensor([3386881, 15],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([3386881, 15],"float32"), Tensor([3386881, 15],"float32"), ) 	 101606430 	 1000 	 0.45055699348449707 	 0.4466979503631592 	 0.44115662574768066 	 0.43564534187316895 	 0.7343039512634277 	 2.6445209980010986 	 0.6720576286315918 	 0.20787477493286133 	 
2025-07-24 22:44:10.725581 test begin: paddle.fmax(Tensor([6351, 200, 40],"float32"), Tensor([6351, 200, 40],"float32"), )
[Prof] paddle.fmax 	 paddle.fmax(Tensor([6351, 200, 40],"float32"), Tensor([6351, 200, 40],"float32"), ) 	 101616000 	 1000 	 0.4506571292877197 	 0.4486691951751709 	 0.44141483306884766 	 0.435528039932251 	 0.7338635921478271 	 2.644484519958496 	 0.6713552474975586 	 0.20765447616577148 	 
2025-07-24 22:44:17.524796 test begin: paddle.fmin(Tensor([10, 50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([10, 50803201],"float32"), Tensor([50803201],"float32"), ) 	 558835211 	 1000 	 4.448854923248291 	 4.412930011749268 	 4.43638277053833 	 4.400456428527832 	 45.17414045333862 	 27.21445631980896 	 45.111401081085205 	 1.9851698875427246 	 
2025-07-24 22:45:58.985758 test begin: paddle.fmin(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), ) 	 101606420 	 1000 	 0.45366382598876953 	 0.44680333137512207 	 0.4395110607147217 	 0.43545055389404297 	 0.7331480979919434 	 2.6445846557617188 	 0.671586275100708 	 0.20908045768737793 	 
2025-07-24 22:46:06.719582 test begin: paddle.fmin(Tensor([10, 5080321],"float32"), Tensor([5080321],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([10, 5080321],"float32"), Tensor([5080321],"float32"), ) 	 55883531 	 1000 	 0.4507572650909424 	 0.4460785388946533 	 0.4402949810028076 	 0.43430209159851074 	 4.54267430305481 	 2.7524712085723877 	 4.480395793914795 	 0.20062875747680664 	 
2025-07-24 22:46:16.662788 test begin: paddle.fmin(Tensor([30, 200, 8468],"float32"), Tensor([30, 200, 8468],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([30, 200, 8468],"float32"), Tensor([30, 200, 8468],"float32"), ) 	 101616000 	 1000 	 0.4509289264678955 	 0.4469268321990967 	 0.4364585876464844 	 0.43520593643188477 	 0.736457347869873 	 2.640397310256958 	 0.6738419532775879 	 0.20758056640625 	 
2025-07-24 22:46:23.840022 test begin: paddle.fmin(Tensor([30, 42337, 40],"float32"), Tensor([30, 42337, 40],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([30, 42337, 40],"float32"), Tensor([30, 42337, 40],"float32"), ) 	 101608800 	 1000 	 0.45080113410949707 	 0.44676661491394043 	 0.44121766090393066 	 0.43540167808532715 	 0.734076738357544 	 2.6490702629089355 	 0.6719741821289062 	 0.20803308486938477 	 
2025-07-24 22:46:30.551616 test begin: paddle.fmin(Tensor([3386881, 15],"float32"), Tensor([15],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([3386881, 15],"float32"), Tensor([15],"float32"), ) 	 50803230 	 1000 	 0.29726576805114746 	 0.3241257667541504 	 0.28612756729125977 	 0.2931180000305176 	 8.363317966461182 	 2.47871994972229 	 8.300858497619629 	 0.16872668266296387 	 
2025-07-24 22:46:43.786528 test begin: paddle.fmin(Tensor([3386881, 15],"float32"), Tensor([3386881, 15],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([3386881, 15],"float32"), Tensor([3386881, 15],"float32"), ) 	 101606430 	 1000 	 0.450641393661499 	 0.44675540924072266 	 0.4389474391937256 	 0.4354879856109619 	 0.7342453002929688 	 2.6470181941986084 	 0.6712863445281982 	 0.20784235000610352 	 
2025-07-24 22:46:50.652083 test begin: paddle.fmin(Tensor([6351, 200, 40],"float32"), Tensor([6351, 200, 40],"float32"), )
[Prof] paddle.fmin 	 paddle.fmin(Tensor([6351, 200, 40],"float32"), Tensor([6351, 200, 40],"float32"), ) 	 101616000 	 1000 	 0.4507272243499756 	 0.4518132209777832 	 0.4410994052886963 	 0.43650007247924805 	 0.7350056171417236 	 2.642141819000244 	 0.6734910011291504 	 0.20772290229797363 	 
2025-07-24 22:46:57.356719 test begin: paddle.frac(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 1000 	 0.8089361190795898 	 0.2979240417480469 	 0.7859249114990234 	 0.28723645210266113 	 1.1843147277832031 	 0.05733180046081543 	 0.6050496101379395 	 6.103515625e-05 	 
2025-07-24 22:47:01.480278 test begin: paddle.frac(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 1000 	 0.8142735958099365 	 0.2979748249053955 	 0.7713954448699951 	 0.2871997356414795 	 1.1883184909820557 	 0.054054975509643555 	 0.6077020168304443 	 3.552436828613281e-05 	 
2025-07-24 22:47:07.738629 test begin: paddle.frac(Tensor([16934401, 3],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([16934401, 3],"float32"), ) 	 50803203 	 1000 	 0.806227445602417 	 0.3001236915588379 	 0.7842481136322021 	 0.2886056900024414 	 1.1854362487792969 	 0.05572366714477539 	 0.6062915325164795 	 6.198883056640625e-05 	 
2025-07-24 22:47:11.702114 test begin: paddle.frac(Tensor([2, 12700801],"float64"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([2, 12700801],"float64"), ) 	 25401602 	 1000 	 0.7498981952667236 	 0.300459623336792 	 0.7274243831634521 	 0.2874491214752197 	 1.0652077198028564 	 0.055225372314453125 	 0.5442521572113037 	 7.557868957519531e-05 	 
2025-07-24 22:47:14.923793 test begin: paddle.frac(Tensor([2, 25401601],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([2, 25401601],"float32"), ) 	 50803202 	 1000 	 0.8179898262023926 	 0.29790711402893066 	 0.7842347621917725 	 0.2872133255004883 	 1.185487985610962 	 0.05355668067932129 	 0.6050131320953369 	 6.079673767089844e-05 	 
2025-07-24 22:47:18.932391 test begin: paddle.frac(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 1000 	 0.8089706897735596 	 0.2979910373687744 	 0.7863695621490479 	 0.28687262535095215 	 1.1857151985168457 	 0.06998658180236816 	 0.605039119720459 	 9.584426879882812e-05 	 
2025-07-24 22:47:22.949499 test begin: paddle.frac(Tensor([8467201, 3],"float64"), )
[Prof] paddle.frac 	 paddle.frac(Tensor([8467201, 3],"float64"), ) 	 25401603 	 1000 	 0.7484428882598877 	 0.29828834533691406 	 0.7247581481933594 	 0.28710317611694336 	 1.0663871765136719 	 0.06861305236816406 	 0.5441486835479736 	 9.322166442871094e-05 	 
2025-07-24 22:47:26.272699 test begin: paddle.full_like(Tensor([1, 1, 2048, 24807],"bool"), -65504.0, dtype=Dtype(float16), )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([1, 1, 2048, 24807],"bool"), -65504.0, dtype=Dtype(float16), ) 	 50804736 	 1000 	 0.06963253021240234 	 0.07156085968017578 	 0.05807185173034668 	 0.05536603927612305 	 None 	 None 	 None 	 None 	 
2025-07-24 22:47:27.106390 test begin: paddle.full_like(Tensor([1, 1, 24807, 2048],"bool"), -65504.0, dtype=Dtype(float16), )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([1, 1, 24807, 2048],"bool"), -65504.0, dtype=Dtype(float16), ) 	 50804736 	 1000 	 0.06897830963134766 	 0.06892132759094238 	 0.05000925064086914 	 0.04892849922180176 	 None 	 None 	 None 	 None 	 
2025-07-24 22:47:27.949294 test begin: paddle.full_like(Tensor([1, 12404, 4096],"float32"), 1, )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([1, 12404, 4096],"float32"), 1, ) 	 50806784 	 1000 	 0.1341075897216797 	 0.13814306259155273 	 0.12361264228820801 	 0.1212916374206543 	 None 	 None 	 None 	 None 	 
Error: Can not import paddle core while this file exists: /usr/local/lib/python3.10/dist-packages/paddle/base/libpaddle.so
KeyboardInterrupt

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 9, in <module>
    from tester import (APIConfig, APITestAccuracy, APITestCINNVSDygraph,
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/__init__.py", line 74, in __getattr__
    from .api_config import APIConfig
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/__init__.py", line 22, in __getattr__
    from .config_analyzer import APIConfig
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py", line 8, in <module>
    import paddle
  File "/usr/local/lib/python3.10/dist-packages/paddle/__init__.py", line 38, in <module>
    from .base import core  # noqa: F401
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/__init__.py", line 38, in <module>
    from . import (  # noqa: F401
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/backward.py", line 28, in <module>
    from . import core, framework, log_helper, unique_name
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/core.py", line 388, in <module>
    raise e
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/core.py", line 267, in <module>
    from . import libpaddle
ImportError: initialization failed
2025-07-24 15:40:35.516912 test begin: paddle.full_like(Tensor([1, 13, 2048, 2048],"bool"), -65504.0, dtype=Dtype(float16), )
W0724 15:40:39.517371 108115 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.full_like 	 paddle.full_like(Tensor([1, 13, 2048, 2048],"bool"), -65504.0, dtype=Dtype(float16), ) 	 54525952 	 1000 	 0.08247876167297363 	 0.0847787857055664 	 0.06349062919616699 	 0.06022047996520996 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:40.205629 test begin: paddle.full_like(Tensor([1, 300, 169345],"float32"), 1, )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([1, 300, 169345],"float32"), 1, ) 	 50803500 	 1000 	 0.13414621353149414 	 0.13428378105163574 	 0.1239171028137207 	 0.12210893630981445 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:41.330089 test begin: paddle.full_like(Tensor([13, 1, 2048, 2048],"bool"), -65504.0, dtype=Dtype(float16), )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([13, 1, 2048, 2048],"bool"), -65504.0, dtype=Dtype(float16), ) 	 54525952 	 1000 	 0.07636022567749023 	 0.07346320152282715 	 0.06312775611877441 	 0.060977935791015625 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:42.245695 test begin: paddle.full_like(Tensor([199, 256000],"float32"), 0.0, )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([199, 256000],"float32"), 0.0, ) 	 50944000 	 1000 	 0.13627862930297852 	 0.14389872550964355 	 0.12419629096984863 	 0.12215471267700195 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:43.373562 test begin: paddle.full_like(Tensor([42, 300, 4096],"float32"), 1, )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([42, 300, 4096],"float32"), 1, ) 	 51609600 	 1000 	 0.1369929313659668 	 0.14243173599243164 	 0.12604737281799316 	 0.12438464164733887 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:44.494035 test begin: paddle.full_like(Tensor([6, 8467201],"float32"), 0.0, )
[Prof] paddle.full_like 	 paddle.full_like(Tensor([6, 8467201],"float32"), 0.0, ) 	 50803206 	 1000 	 0.137160062789917 	 0.13440656661987305 	 0.12518858909606934 	 0.12211036682128906 	 None 	 None 	 None 	 None 	 
2025-07-24 15:40:45.609193 test begin: paddle.gammainc(Tensor([1270081, 40],"float32"), y=Tensor([1270081, 40],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([1270081, 40],"float32"), y=Tensor([1270081, 40],"float32"), ) 	 101606480 	 1000 	 4.212329626083374 	 2.279956579208374 	 0.0030438899993896484 	 2.254931926727295 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-07-24 15:40:56.538262 test begin: paddle.gammainc(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), ) 	 101606480 	 1000 	 4.209826469421387 	 2.266871452331543 	 0.003041982650756836 	 2.255443811416626 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-07-24 15:41:09.070129 test begin: paddle.gammainc(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), ) 	 101606460 	 1000 	 4.218136548995972 	 2.2668449878692627 	 0.0030364990234375 	 2.2554166316986084 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-07-24 15:41:20.129505 test begin: paddle.gammainc(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), ) 	 101606448 	 1000 	 4.212046384811401 	 2.26822829246521 	 0.003042936325073242 	 2.2569215297698975 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-07-24 15:41:30.681862 test begin: paddle.gammainc(Tensor([3, 16934401],"float32"), y=Tensor([3, 16934401],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([3, 16934401],"float32"), y=Tensor([3, 16934401],"float32"), ) 	 101606406 	 1000 	 4.209423780441284 	 2.2817723751068115 	 0.0030410289764404297 	 2.253002882003784 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-07-24 15:41:42.281497 test begin: paddle.gammainc(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), )
[Prof] paddle.gammainc 	 paddle.gammainc(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), ) 	 101606520 	 1000 	 4.213444232940674 	 2.26759934425354 	 0.0030417442321777344 	 2.2546706199645996 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igamma: input' is not implemented.
2025-07-24 15:41:52.848727 test begin: paddle.gammaincc(Tensor([1270081, 40],"float32"), Tensor([1270081, 40],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([1270081, 40],"float32"), Tensor([1270081, 40],"float32"), ) 	 101606480 	 1000 	 3.921570301055908 	 7.096731662750244 	 0.002767801284790039 	 7.085773229598999 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-07-24 15:42:07.696324 test begin: paddle.gammaincc(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), ) 	 101606480 	 1000 	 3.916768789291382 	 7.113148212432861 	 0.002766847610473633 	 7.089234113693237 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-07-24 15:42:23.789922 test begin: paddle.gammaincc(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), ) 	 101606460 	 1000 	 3.9139187335968018 	 7.096638441085815 	 0.0027666091918945312 	 7.085509777069092 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-07-24 15:42:39.605795 test begin: paddle.gammaincc(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), ) 	 101606448 	 1000 	 3.915369987487793 	 7.101141452789307 	 0.002768993377685547 	 7.078354835510254 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-07-24 15:42:54.414826 test begin: paddle.gammaincc(Tensor([3, 16934401],"float32"), Tensor([3, 16934401],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([3, 16934401],"float32"), Tensor([3, 16934401],"float32"), ) 	 101606406 	 1000 	 3.9149370193481445 	 7.096374273300171 	 0.002766132354736328 	 7.085216760635376 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-07-24 15:43:09.213875 test begin: paddle.gammaincc(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), )
[Prof] paddle.gammaincc 	 paddle.gammaincc(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), ) 	 101606520 	 1000 	 3.922149419784546 	 7.827034950256348 	 0.002768278121948242 	 7.077728509902954 	 None 	 None 	 None 	 None 	 
[Error] the derivative for 'igammac: input' is not implemented.
2025-07-24 15:43:25.356822 test begin: paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([482, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([482, 1],"int64"), ) 	 58720738 	 1000 	 0.08812570571899414 	 15.810697793960571 	 0.07802462577819824 	 0.00021195411682128906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:43:42.903090 test begin: paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([496, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([496, 1],"int64"), ) 	 58720752 	 1000 	 0.08933496475219727 	 16.40808367729187 	 0.07980918884277344 	 0.0002110004425048828 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:44:01.019427 test begin: paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([512, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 2, 7],"float32"), Tensor([512, 1],"int64"), ) 	 58720768 	 1000 	 0.09299373626708984 	 23.04659104347229 	 0.08268165588378906 	 0.000102996826171875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:44:25.821563 test begin: paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([482, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([482, 1],"int64"), ) 	 58720738 	 1000 	 0.08823227882385254 	 15.718647241592407 	 0.07892537117004395 	 0.00021266937255859375 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:44:43.315478 test begin: paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([496, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([496, 1],"int64"), ) 	 58720752 	 1000 	 0.09046053886413574 	 16.35286784172058 	 0.08105778694152832 	 0.00021195411682128906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:45:01.380932 test begin: paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([512, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 2048, 7, 2],"float32"), Tensor([512, 1],"int64"), ) 	 58720768 	 1000 	 0.09212970733642578 	 22.526517868041992 	 0.08250737190246582 	 0.00021791458129882812 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:45:25.625880 test begin: paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([482, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([482, 1],"int64"), ) 	 50878946 	 1000 	 0.12741732597351074 	 19.958282232284546 	 0.10880875587463379 	 0.00025081634521484375 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:45:50.411136 test begin: paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([496, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([496, 1],"int64"), ) 	 50878960 	 1000 	 0.12042784690856934 	 16.385782957077026 	 0.11110091209411621 	 0.00020956993103027344 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:46:08.317127 test begin: paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([512, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([2048, 507, 7, 7],"float32"), Tensor([512, 1],"int64"), ) 	 50878976 	 1000 	 0.12444233894348145 	 16.817355394363403 	 0.11503410339355469 	 0.00021147727966308594 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:46:26.669290 test begin: paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([482, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([482, 1],"int64"), ) 	 50878946 	 1000 	 0.28220248222351074 	 15.800135612487793 	 0.27282071113586426 	 0.0002377033233642578 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:46:45.290417 test begin: paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([496, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([496, 1],"int64"), ) 	 50878960 	 1000 	 0.2885761260986328 	 16.109444856643677 	 0.2791764736175537 	 0.00023698806762695312 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:47:04.301369 test begin: paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([512, 1],"int64"), )
[Prof] paddle.gather 	 paddle.gather(Tensor([507, 2048, 7, 7],"float32"), Tensor([512, 1],"int64"), ) 	 50878976 	 1000 	 0.3002641201019287 	 23.36109161376953 	 0.2906467914581299 	 0.0002579689025878906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:47:30.628585 test begin: paddle.gather_nd(Tensor([1, 14176, 7168],"bfloat16"), Tensor([7780, 2],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8c4de3cf70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 15:57:35.683356 test begin: paddle.gather_nd(Tensor([1, 14176, 7168],"bfloat16"), Tensor([8162, 2],"int64"), )
W0724 15:57:40.011922 116995 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4e55ff2ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 16:07:42.517286 test begin: paddle.gather_nd(Tensor([1, 8192, 12404],"bfloat16"), Tensor([7780, 2],"int64"), )
W0724 16:07:44.273334 120946 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2caea1af50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 16:17:47.132084 test begin: paddle.gather_nd(Tensor([1, 8192, 12404],"bfloat16"), Tensor([8162, 2],"int64"), )
W0724 16:17:48.862080 125218 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8a2d7d6ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 16:27:51.991385 test begin: paddle.gather_nd(Tensor([10, 41344, 128],"float32"), index=Tensor([10, 500, 2],"int64"), )
W0724 16:27:53.002700 129608 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([10, 41344, 128],"float32"), index=Tensor([10, 500, 2],"int64"), ) 	 52930320 	 1000 	 0.023882150650024414 	 422.41491651535034 	 3.218650817871094e-05 	 0.00022101402282714844 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:34:56.883052 test begin: paddle.gather_nd(Tensor([10, 41344, 128],"float32"), index=Tensor([20, 500, 2],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4b84c9ec20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 16:45:01.679254 test begin: paddle.gather_nd(Tensor([10, 41344, 128],"float32"), index=Tensor([5, 500, 2],"int64"), )
W0724 16:45:02.716917 136822 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.gather_nd 	 paddle.gather_nd(Tensor([10, 41344, 128],"float32"), index=Tensor([5, 500, 2],"int64"), ) 	 52925320 	 1000 	 0.01586151123046875 	 218.77711415290833 	 2.2411346435546875e-05 	 0.00023603439331054688 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:48:42.393644 test begin: paddle.gather_nd(Tensor([2, 8192, 7168],"bfloat16"), Tensor([7780, 2],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbc42a32e00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 16:58:47.064818 test begin: paddle.gather_nd(Tensor([2, 8192, 7168],"bfloat16"), Tensor([8162, 2],"int64"), )
W0724 16:58:49.247052 142571 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f924b622ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 17:08:51.606770 test begin: paddle.gather_nd(Tensor([20, 19846, 128],"float32"), index=Tensor([20, 500, 2],"int64"), )
W0724 17:08:52.596287 147157 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1653c06f50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 17:18:56.424156 test begin: paddle.gather_nd(Tensor([20, 19846, 128],"float32"), index=Tensor([20, 9923, 2],"int64"), )
W0724 17:18:57.404021 151626 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8cbc35ed70>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753349336 (unix time) try "date -d @1753349336" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x24fef) received by PID 151535 (TID 0x7f8cb39f8640) from PID 151535 ***]

2025-07-24 17:29:03.633292 test begin: paddle.gather_nd(Tensor([20, 41344, 128],"float32"), index=Tensor([20, 635041, 2],"int64"), )
W0724 17:29:06.291844 155695 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f70d857afb0>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753349943 (unix time) try "date -d @1753349943" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x25fbc) received by PID 155580 (TID 0x7f70cf9f8640) from PID 155580 ***]

2025-07-24 17:39:42.144145 test begin: paddle.gather_nd(Tensor([20, 41344, 128],"float32"), index=Tensor([25402, 500, 2],"int64"), )
W0724 17:39:44.889264 163727 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe0311a2fb0>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753350582 (unix time) try "date -d @1753350582" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27efb) received by PID 163579 (TID 0x7fe0283f9640) from PID 163579 ***]

2025-07-24 17:50:16.723863 test begin: paddle.gather_nd(Tensor([20, 41344, 62],"float32"), index=Tensor([20, 500, 2],"int64"), )
W0724 17:50:17.816466 15003 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f63d1c3afb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 18:00:21.876801 test begin: paddle.gcd(Tensor([10, 5080321],"int32"), Tensor([10, 5080321],"int32"), )
W0724 18:00:23.156354 29979 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 18:02:12.289695 29979 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 18:02:12.335161 test begin: paddle.gcd(Tensor([2540161, 20],"int32"), Tensor([2540161, 20],"int32"), )
W0724 18:04:02.390473 32557 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 18:04:04.226264 test begin: paddle.gcd(x=Tensor([1270081, 2, 4, 5],"int32"), y=Tensor([1270081, 2, 4, 5],"int32"), )
W0724 18:05:54.274346 34941 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 18:05:54.316581 test begin: paddle.gcd(x=Tensor([2540161, 1, 4, 5],"int32"), y=Tensor([2, 1, 5],"int32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_where(_object*, _object*, _object*)
1   where_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::where(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
3   void phi::WhereKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   int* phi::DeviceContext::Alloc<int>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 387.597900MB memory on GPU 0, 39.254883GB memory has been allocated and available memory is only 142.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:06:05.273536 test begin: paddle.gcd(x=Tensor([6, 1, 1693441, 5],"int32"), y=Tensor([2, 1, 5],"int32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_where(_object*, _object*, _object*)
1   where_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::where(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
3   void phi::WhereKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   int* phi::DeviceContext::Alloc<int>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 387.597900MB memory on GPU 0, 39.348633GB memory has been allocated and available memory is only 46.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:06:16.267434 test begin: paddle.gcd(x=Tensor([6, 1, 4, 2116801],"int32"), y=Tensor([2, 1, 2116801],"int32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_expand(_object*, _object*, _object*)
1   expand_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>)
2   paddle::experimental::expand(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&)
3   void phi::ExpandKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
4   int* phi::DeviceContext::Alloc<int>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 387.597900MB memory on GPU 0, 39.348633GB memory has been allocated and available memory is only 46.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-24 18:06:27.045667 test begin: paddle.gcd(x=Tensor([6, 2, 4, 1058401],"int32"), y=Tensor([6, 2, 4, 1058401],"int32"), )
W0724 18:08:17.404827 37953 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 18:08:17.449093 test begin: paddle.gcd(x=Tensor([6, 2, 846721, 5],"int32"), y=Tensor([6, 2, 846721, 5],"int32"), )
W0724 18:10:07.524693 40360 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 18:10:07.663248 test begin: paddle.gcd(x=Tensor([6, 423361, 4, 5],"int32"), y=Tensor([6, 423361, 4, 5],"int32"), )
W0724 18:11:57.736960 42660 backward.cc:462] While running Node (RemainderGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 18:11:57.782786 test begin: paddle.geometric.segment_max(Tensor([1270081, 20],"float64"), Tensor([1270081],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f28246c34c0>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   c10::TensorImpl::~TensorImpl()
1   c10::TensorImpl::~TensorImpl()
2   torch::autograd::deleteNode(torch::autograd::Node*)
3   torch::autograd::CopySlices::release_variables()
4   torch::autograd::deleteNode(torch::autograd::Node*)
5   c10::TensorImpl::~TensorImpl()
6   c10::TensorImpl::~TensorImpl()
7   c10::impl::PyObjectSlot::~PyObjectSlot()
8   c10::impl::PyObjectSlot::maybe_destroy_pyobj()

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753352517 (unix time) try "date -d @1753352517" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x7476) received by PID 29814 (TID 0x7f281b9f8640) from PID 29814 ***]

2025-07-24 18:22:16.618958 test begin: paddle.geometric.segment_max(Tensor([2540161, 20],"float32"), Tensor([2540161],"int64"), )
W0724 18:22:17.683547 59280 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8caed4ac20>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::autograd::deleteNode(torch::autograd::Node*)
1   c10::TensorImpl::~TensorImpl()
2   c10::TensorImpl::~TensorImpl()
3   c10::impl::PyObjectSlot::~PyObjectSlot()
4   c10::impl::PyObjectSlot::maybe_destroy_pyobj()

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753353136 (unix time) try "date -d @1753353136" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xe6e1) received by PID 59105 (TID 0x7f8caa53c640) from PID 59105 ***]

2025-07-24 18:32:37.002680 test begin: paddle.geometric.segment_max(Tensor([40, 1270081],"float32"), Tensor([40],"int64"), )
W0724 18:32:39.503551 73480 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.segment_max 	 paddle.geometric.segment_max(Tensor([40, 1270081],"float32"), Tensor([40],"int64"), ) 	 50803280 	 1000 	 0.8498377799987793 	 10.762920141220093 	 0.0008170604705810547 	 0.00036525726318359375 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 18:32:53.769022 test begin: paddle.geometric.segment_max(Tensor([40, 2540161],"float16"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_max 	 paddle.geometric.segment_max(Tensor([40, 2540161],"float16"), Tensor([40],"int64"), ) 	 101606480 	 1000 	 1.7231175899505615 	 13.246902227401733 	 0.0016751289367675781 	 0.00036334991455078125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 18:33:13.212489 test begin: paddle.geometric.segment_max(Tensor([40, 635041],"float64"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_max 	 paddle.geometric.segment_max(Tensor([40, 635041],"float64"), Tensor([40],"int64"), ) 	 25401680 	 1000 	 0.5830984115600586 	 12.776252031326294 	 0.0005521774291992188 	 0.0003063678741455078 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 18:33:28.276710 test begin: paddle.geometric.segment_max(Tensor([5080321, 20],"float16"), Tensor([5080321],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbbad1baa40>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::autograd::deleteNode(torch::autograd::Node*)
1   c10::TensorImpl::~TensorImpl()
2   c10::TensorImpl::~TensorImpl()
3   c10::impl::PyObjectSlot::~PyObjectSlot()
4   c10::impl::PyObjectSlot::maybe_destroy_pyobj()

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753353808 (unix time) try "date -d @1753353808" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x11e98) received by PID 73368 (TID 0x7fbba43f9640) from PID 73368 ***]

2025-07-24 18:43:50.776004 test begin: paddle.geometric.segment_mean(Tensor([1270081, 20],"float64"), Tensor([1270081],"int64"), )
W0724 18:43:51.497048 89032 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([1270081, 20],"float64"), Tensor([1270081],"int64"), ) 	 26671701 	 1000 	 0.4671809673309326 	 0.8238368034362793 	 0.0004286766052246094 	 0.00012254714965820312 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 18:43:55.805760 test begin: paddle.geometric.segment_mean(Tensor([25401601, 20],"float16"), Tensor([25401601],"int64"), )
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([25401601, 20],"float16"), Tensor([25401601],"int64"), ) 	 533433621 	 1000 	 8.7131986618042 	 10.972833156585693 	 0.008671998977661133 	 0.0013921260833740234 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 18:44:55.992233 test begin: paddle.geometric.segment_mean(Tensor([25401601, 20],"float32"), Tensor([25401601],"int64"), )
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([25401601, 20],"float32"), Tensor([25401601],"int64"), ) 	 533433621 	 1000 	 6.312088489532471 	 14.478714942932129 	 0.006260871887207031 	 0.002511262893676758 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 18:46:26.380469 test begin: paddle.geometric.segment_mean(Tensor([25401601, 20],"float64"), Tensor([25401601],"int64"), )
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([25401601, 20],"float64"), Tensor([25401601],"int64"), ) 	 533433621 	 1000 	 10.731258153915405 	 23.798243284225464 	 0.010684490203857422 	 0.00469970703125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 18:48:31.982316 test begin: paddle.geometric.segment_mean(Tensor([2540161, 20],"float32"), Tensor([2540161],"int64"), )
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([2540161, 20],"float32"), Tensor([2540161],"int64"), ) 	 53343381 	 1000 	 0.6472535133361816 	 1.4401240348815918 	 0.0006077289581298828 	 0.00022983551025390625 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 18:48:40.780877 test begin: paddle.geometric.segment_mean(Tensor([40, 1270081],"float32"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([40, 1270081],"float32"), Tensor([40],"int64"), ) 	 50803280 	 1000 	 0.49777865409851074 	 1.1623084545135498 	 0.00046563148498535156 	 0.00021505355834960938 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 18:52:56.389232 test begin: paddle.geometric.segment_mean(Tensor([40, 2540161],"float16"), Tensor([40],"int64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe45984ee60>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753354976 (unix time) try "date -d @1753354976" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x15b0c) received by PID 88844 (TID 0x7fe450dfa640) from PID 88844 ***]

2025-07-24 19:03:03.496279 test begin: paddle.geometric.segment_mean(Tensor([40, 635041],"float64"), Tensor([40],"int64"), )
W0724 19:03:04.190809 130675 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([40, 635041],"float64"), Tensor([40],"int64"), ) 	 25401680 	 1000 	 0.46857643127441406 	 1.1348066329956055 	 0.00044155120849609375 	 0.0002243518829345703 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:05:45.929059 test begin: paddle.geometric.segment_mean(Tensor([5080321, 20],"float16"), Tensor([5080321],"int64"), )
[Prof] paddle.geometric.segment_mean 	 paddle.geometric.segment_mean(Tensor([5080321, 20],"float16"), Tensor([5080321],"int64"), ) 	 106686741 	 1000 	 2.0114471912384033 	 3.5515782833099365 	 0.001978158950805664 	 0.0006268024444580078 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:06:07.765515 test begin: paddle.geometric.segment_min(Tensor([1270081, 20],"float64"), Tensor([1270081],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([1270081, 20],"float64"), Tensor([1270081],"int64"), ) 	 26671701 	 1000 	 0.5205140113830566 	 1.1860551834106445 	 0.0004932880401611328 	 7.486343383789062e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:06:10.714301 test begin: paddle.geometric.segment_min(Tensor([25401601, 20],"float16"), Tensor([25401601],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([25401601, 20],"float16"), Tensor([25401601],"int64"), ) 	 533433621 	 1000 	 11.13961410522461 	 24.096993684768677 	 0.011071443557739258 	 0.0025610923767089844 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:07:10.516281 test begin: paddle.geometric.segment_min(Tensor([25401601, 20],"float32"), Tensor([25401601],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([25401601, 20],"float32"), Tensor([25401601],"int64"), ) 	 533433621 	 1000 	 10.35700535774231 	 16.141722917556763 	 0.010307788848876953 	 0.0031218528747558594 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:08:04.208259 test begin: paddle.geometric.segment_min(Tensor([25401601, 20],"float64"), Tensor([25401601],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([25401601, 20],"float64"), Tensor([25401601],"int64"), ) 	 533433621 	 1000 	 10.952965259552002 	 19.935460805892944 	 0.01085972785949707 	 0.0019538402557373047 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:09:04.367084 test begin: paddle.geometric.segment_min(Tensor([2540161, 20],"float32"), Tensor([2540161],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([2540161, 20],"float32"), Tensor([2540161],"int64"), ) 	 53343381 	 1000 	 1.0006113052368164 	 1.5947990417480469 	 0.0009577274322509766 	 0.00019621849060058594 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:09:09.334902 test begin: paddle.geometric.segment_min(Tensor([40, 1270081],"float32"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([40, 1270081],"float32"), Tensor([40],"int64"), ) 	 50803280 	 1000 	 0.8002698421478271 	 0.9379124641418457 	 0.0007753372192382812 	 0.00017118453979492188 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:09:13.182731 test begin: paddle.geometric.segment_min(Tensor([40, 2540161],"float16"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([40, 2540161],"float16"), Tensor([40],"int64"), ) 	 101606480 	 1000 	 1.6155130863189697 	 2.0044713020324707 	 0.0015900135040283203 	 0.00023937225341796875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:09:22.124372 test begin: paddle.geometric.segment_min(Tensor([40, 635041],"float64"), Tensor([40],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([40, 635041],"float64"), Tensor([40],"int64"), ) 	 25401680 	 1000 	 0.4924612045288086 	 0.71278977394104 	 0.00047326087951660156 	 0.00012230873107910156 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:09:24.742975 test begin: paddle.geometric.segment_min(Tensor([5080321, 20],"float16"), Tensor([5080321],"int64"), )
[Prof] paddle.geometric.segment_min 	 paddle.geometric.segment_min(Tensor([5080321, 20],"float16"), Tensor([5080321],"int64"), ) 	 106686741 	 1000 	 2.119286060333252 	 5.674700021743774 	 0.002082347869873047 	 0.0002300739288330078 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:09:37.219481 test begin: paddle.geometric.segment_sum(Tensor([25401601, 15],"float16"), Tensor([25401601],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([25401601, 15],"float16"), Tensor([25401601],"int64"), ) 	 406425616 	 1000 	 5.6754536628723145 	 4.409409284591675 	 0.00563812255859375 	 2.2509994506835938 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:10:01.365833 test begin: paddle.geometric.segment_sum(Tensor([25401601, 15],"float32"), Tensor([25401601],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([25401601, 15],"float32"), Tensor([25401601],"int64"), ) 	 406425616 	 1000 	 4.235562324523926 	 3.425936698913574 	 0.004197120666503906 	 1.7496685981750488 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:10:24.430785 test begin: paddle.geometric.segment_sum(Tensor([2540161, 20],"float32"), Tensor([2540161],"int32"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([2540161, 20],"float32"), Tensor([2540161],"int32"), ) 	 53343381 	 1000 	 0.6618688106536865 	 0.5197546482086182 	 0.0006337165832519531 	 0.2655158042907715 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:10:29.104877 test begin: paddle.geometric.segment_sum(Tensor([30, 1693441],"float32"), Tensor([30],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([30, 1693441],"float32"), Tensor([30],"int64"), ) 	 50803260 	 1000 	 0.5787506103515625 	 0.4078507423400879 	 0.0005526542663574219 	 0.20636391639709473 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:10:32.074416 test begin: paddle.geometric.segment_sum(Tensor([30, 3386881],"float16"), Tensor([30],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([30, 3386881],"float16"), Tensor([30],"int64"), ) 	 101606460 	 1000 	 1.4679396152496338 	 1.1920583248138428 	 0.0014352798461914062 	 0.6029536724090576 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:10:40.441055 test begin: paddle.geometric.segment_sum(Tensor([3386881, 15],"float32"), Tensor([3386881],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([3386881, 15],"float32"), Tensor([3386881],"int64"), ) 	 54190096 	 1000 	 0.5125212669372559 	 0.41195154190063477 	 0.0004858970642089844 	 0.21039581298828125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:10:43.147533 test begin: paddle.geometric.segment_sum(Tensor([40, 1270081],"float32"), Tensor([40],"int32"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([40, 1270081],"float32"), Tensor([40],"int32"), ) 	 50803280 	 1000 	 0.6024808883666992 	 0.4738893508911133 	 0.0005774497985839844 	 0.2420816421508789 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:10:46.360692 test begin: paddle.geometric.segment_sum(Tensor([50803201, 20],"float32"), Tensor([50803201],"int32"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([50803201, 20],"float32"), Tensor([50803201],"int32"), ) 	 1066867221 	 1000 	 12.785514831542969 	 10.226045846939087 	 0.01267385482788086 	 2.6148521900177 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:11:51.029612 test begin: paddle.geometric.segment_sum(Tensor([6773761, 15],"float16"), Tensor([6773761],"int64"), )
[Prof] paddle.geometric.segment_sum 	 paddle.geometric.segment_sum(Tensor([6773761, 15],"float16"), Tensor([6773761],"int64"), ) 	 108380176 	 1000 	 2.8139216899871826 	 1.3979418277740479 	 0.0027675628662109375 	 0.71421217918396 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:11:58.895830 test begin: paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "max", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "max", None, None, ) 	 25401640 	 1000 	 1.0291461944580078 	 3.7423453330993652 	 0.3507814407348633 	 0.00044274330139160156 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:12:06.543178 test begin: paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mean", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mean", None, None, ) 	 25401640 	 1000 	 1.0829761028289795 	 4.42622184753418 	 0.2195873260498047 	 0.00023055076599121094 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:12:14.119399 test begin: paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "min", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([10, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "min", None, None, ) 	 25401640 	 1000 	 1.0257604122161865 	 3.741764783859253 	 0.34961509704589844 	 0.0004642009735107422 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:12:21.207490 test begin: paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "max", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "max", None, None, ) 	 25401650 	 1000 	 0.4494915008544922 	 2.262631416320801 	 0.15309929847717285 	 0.0004558563232421875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:12:25.156371 test begin: paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mean", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mean", None, None, ) 	 25401650 	 1000 	 0.3604154586791992 	 2.605976104736328 	 0.07366275787353516 	 0.00018358230590820312 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:12:29.335010 test begin: paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "min", None, None, )
[Prof] paddle.geometric.send_u_recv 	 paddle.geometric.send_u_recv(Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "min", None, None, ) 	 25401650 	 1000 	 0.4509732723236084 	 2.253408432006836 	 0.15314173698425293 	 0.00046443939208984375 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:12:34.078824 test begin: paddle.geometric.send_ue_recv(Tensor([10, 1693441, 5],"float64"), Tensor([15, 1693441, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff5806fd750>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753356154 (unix time) try "date -d @1753356154" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1fd25) received by PID 130341 (TID 0x7ff5779f8640) from PID 130341 ***]

2025-07-24 19:22:43.833562 test begin: paddle.geometric.send_ue_recv(Tensor([10, 1693441, 5],"float64"), Tensor([15, 1693441, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, )
W0724 19:22:46.066076 10726 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f48d7efac50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 19:32:48.441622 test begin: paddle.geometric.send_ue_recv(Tensor([10, 1693441],"float64"), Tensor([15, 1693441],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, )
W0724 19:32:49.350948 34205 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([10, 1693441],"float64"), Tensor([15, 1693441],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, ) 	 42336055 	 1000 	 0.871337890625 	 2.858700752258301 	 0.17737865447998047 	 0.00018215179443359375 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:32:55.155864 test begin: paddle.geometric.send_ue_recv(Tensor([10, 20],"float64"), Tensor([1270081, 20],"float64"), Tensor([1270081],"int64"), Tensor([1270081],"int64"), "add", "mean", None, None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4e9f47f100>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 19:43:03.900677 test begin: paddle.geometric.send_ue_recv(Tensor([10, 2540161],"float64"), Tensor([15, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, )
W0724 19:43:05.281591 56982 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([10, 2540161],"float64"), Tensor([15, 2540161],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, ) 	 63504055 	 1000 	 1.3085105419158936 	 3.527714252471924 	 0.2665574550628662 	 0.00023126602172851562 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:43:13.610297 test begin: paddle.geometric.send_ue_recv(Tensor([10, 508033, 5],"float64"), Tensor([15, 508033, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, )
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([10, 508033, 5],"float64"), Tensor([15, 508033, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, ) 	 33022175 	 1000 	 80.69583892822266 	 3.470883369445801 	 0.00012040138244628906 	 0.0005080699920654297 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:46:07.982403 test begin: paddle.geometric.send_ue_recv(Tensor([10, 508033, 5],"float64"), Tensor([15, 508033, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, )
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([10, 508033, 5],"float64"), Tensor([15, 508033, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, ) 	 33022175 	 1000 	 85.3653154373169 	 2.9808273315429688 	 0.00011610984802246094 	 0.00023293495178222656 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:50:48.538802 test begin: paddle.geometric.send_ue_recv(Tensor([10, 8, 317521],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, )
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([10, 8, 317521],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, ) 	 25401830 	 1000 	 92.70902371406555 	 3.6002848148345947 	 0.0001456737518310547 	 0.00048422813415527344 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:55:48.845692 test begin: paddle.geometric.send_ue_recv(Tensor([10, 8, 317521],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, )
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([10, 8, 317521],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, ) 	 25401830 	 1000 	 95.87303590774536 	 2.4304652214050293 	 0.0002262592315673828 	 0.0002715587615966797 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:02:20.764968 test begin: paddle.geometric.send_ue_recv(Tensor([10, 8, 5],"float64"), Tensor([3175201, 8, 1],"float64"), Tensor([3175201],"int64"), Tensor([3175201],"int64"), "mul", "max", None, None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2e8b96a980>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 20:12:28.179152 test begin: paddle.geometric.send_ue_recv(Tensor([10, 8, 5],"float64"), Tensor([3175201, 8, 1],"float64"), Tensor([3175201],"int64"), Tensor([3175201],"int64"), "mul", "sum", None, None, )
W0724 20:12:29.009914 125691 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fba664f6f20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 20:23:00.319581 test begin: paddle.geometric.send_ue_recv(Tensor([1270081, 20],"float64"), Tensor([15, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, )
W0724 20:23:01.158525 149598 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([1270081, 20],"float64"), Tensor([15, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", "mean", None, None, ) 	 25401950 	 1000 	 0.3599560260772705 	 1.6806862354278564 	 0.07361483573913574 	 0.00033473968505859375 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:23:05.171590 test begin: paddle.geometric.send_ue_recv(Tensor([635041, 8, 5],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, )
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([635041, 8, 5],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "max", None, None, ) 	 25401790 	 1000 	 0.7603707313537598 	 1.7606596946716309 	 5.2928924560546875e-05 	 0.0004565715789794922 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:23:11.100341 test begin: paddle.geometric.send_ue_recv(Tensor([635041, 8, 5],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, )
[Prof] paddle.geometric.send_ue_recv 	 paddle.geometric.send_ue_recv(Tensor([635041, 8, 5],"float64"), Tensor([15, 8, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", "sum", None, None, ) 	 25401790 	 1000 	 0.2372143268585205 	 1.4013266563415527 	 4.9591064453125e-05 	 0.00012969970703125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:23:14.293871 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 20],"float64"), Tensor([25401601],"int64"), Tensor([25401601],"int64"), "add", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 20],"float64"), Tensor([25401601],"int64"), Tensor([25401601],"int64"), "add", ) 	 50805302 	 1000 	 7.948040246963501 	 6.063882827758789 	 0.0001266002655029297 	 3.0981671810150146 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:24:07.310035 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 20],"float64"), Tensor([25401601],"int64"), Tensor([25401601],"int64"), "mul", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 20],"float64"), Tensor([25401601],"int64"), Tensor([25401601],"int64"), "mul", ) 	 50805302 	 1000 	 7.931901216506958 	 6.0653321743011475 	 0.00010728836059570312 	 3.099740743637085 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:25:01.830214 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 254017],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 254017],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", ) 	 25401830 	 1000 	 6.078630208969116 	 0.05033469200134277 	 8.296966552734375e-05 	 0.0386655330657959 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:25:10.271205 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 254017],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([100, 254017],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", ) 	 25401830 	 1000 	 7.830964088439941 	 0.050383806228637695 	 9.083747863769531e-05 	 0.038605690002441406 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:25:22.162125 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", ) 	 25401750 	 1000 	 0.08887863159179688 	 0.011207342147827148 	 1.811981201171875e-05 	 3.409385681152344e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:25:23.058052 test begin: paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([100, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", ) 	 25401750 	 1000 	 0.08908987045288086 	 0.011458873748779297 	 2.2172927856445312e-05 	 2.9325485229492188e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:25:23.997228 test begin: paddle.geometric.send_uv(Tensor([100, 20],"float64"), Tensor([100, 1],"float64"), Tensor([25401601],"int64"), Tensor([25401601],"int64"), "add", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([100, 20],"float64"), Tensor([100, 1],"float64"), Tensor([25401601],"int64"), Tensor([25401601],"int64"), "add", ) 	 50805302 	 1000 	 9.371220588684082 	 6.064994812011719 	 8.249282836914062e-05 	 3.098135232925415 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:26:19.247504 test begin: paddle.geometric.send_uv(Tensor([100, 20],"float64"), Tensor([25401601, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([100, 20],"float64"), Tensor([25401601, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", ) 	 25403631 	 1000 	 0.7892880439758301 	 0.011298894882202148 	 6.318092346191406e-05 	 3.457069396972656e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:26:30.011697 test begin: paddle.geometric.send_uv(Tensor([100, 254017],"float64"), Tensor([100, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([100, 254017],"float64"), Tensor([100, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", ) 	 25401830 	 1000 	 4.031849145889282 	 0.06621599197387695 	 6.437301635742188e-05 	 0.03862643241882324 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:26:37.124135 test begin: paddle.geometric.send_uv(Tensor([1270081, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([1270081, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", ) 	 26671731 	 1000 	 1.5246219635009766 	 0.011214733123779297 	 6.127357482910156e-05 	 2.9802322387695312e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:26:41.254745 test begin: paddle.geometric.send_uv(Tensor([1270081, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([1270081, 1],"float64"), Tensor([1270081, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", ) 	 26671731 	 1000 	 0.49378108978271484 	 0.011768341064453125 	 8.535385131835938e-05 	 4.935264587402344e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:26:43.438122 test begin: paddle.geometric.send_uv(Tensor([1270081, 20],"float64"), Tensor([1270081, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([1270081, 20],"float64"), Tensor([1270081, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", ) 	 26671731 	 1000 	 0.08914518356323242 	 0.01233363151550293 	 1.6927719116210938e-05 	 5.2928924560546875e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:26:44.818935 test begin: paddle.geometric.send_uv(Tensor([25401601, 1],"float64"), Tensor([25401601, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([25401601, 1],"float64"), Tensor([25401601, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", ) 	 533433651 	 1000 	 0.08969426155090332 	 0.024045705795288086 	 2.2649765014648438e-05 	 4.124641418457031e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:27:06.509247 test begin: paddle.geometric.send_uv(Tensor([25401601, 1],"float64"), Tensor([25401601, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([25401601, 1],"float64"), Tensor([25401601, 20],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "mul", ) 	 533433651 	 1000 	 0.09511828422546387 	 0.011405467987060547 	 2.2172927856445312e-05 	 3.24249267578125e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:27:29.082548 test begin: paddle.geometric.send_uv(Tensor([25401601, 20],"float64"), Tensor([25401601, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", )
[Prof] paddle.geometric.send_uv 	 paddle.geometric.send_uv(Tensor([25401601, 20],"float64"), Tensor([25401601, 1],"float64"), Tensor([15],"int64"), Tensor([15],"int64"), "add", ) 	 533433651 	 1000 	 0.09079194068908691 	 0.011288642883300781 	 3.1948089599609375e-05 	 3.2901763916015625e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 20:27:51.572040 test begin: paddle.greater_equal(Tensor([13, 15266, 16, 4, 1],"int64"), Tensor([13, 15266, 16, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 15266, 16, 4, 1],"int64"), Tensor([13, 15266, 16, 1, 8],"int64"), ) 	 38103936 	 1000 	 0.5590980052947998 	 0.5078115463256836 	 0.5495414733886719 	 0.4908714294433594 	 None 	 None 	 None 	 None 	 
2025-07-24 20:27:53.440433 test begin: paddle.greater_equal(Tensor([13, 2, 122124, 4, 1],"int64"), Tensor([13, 2, 122124, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 122124, 4, 1],"int64"), Tensor([13, 2, 122124, 1, 8],"int64"), ) 	 38102688 	 1000 	 0.5565764904022217 	 0.5022892951965332 	 0.5466077327728271 	 0.4900996685028076 	 None 	 None 	 None 	 None 	 
2025-07-24 20:27:55.182105 test begin: paddle.greater_equal(Tensor([13, 2, 16, 4, 15266],"int64"), Tensor([13, 2, 16, 1, 15266],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 16, 4, 15266],"int64"), Tensor([13, 2, 16, 1, 15266],"int64"), ) 	 31753280 	 1000 	 0.2126762866973877 	 0.2207798957824707 	 0.20302200317382812 	 0.20695757865905762 	 None 	 None 	 None 	 None 	 
2025-07-24 20:27:56.141739 test begin: paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 61062],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 61062],"int64"), ) 	 25403456 	 1000 	 0.5653622150421143 	 0.44452738761901855 	 0.5561048984527588 	 0.43231916427612305 	 None 	 None 	 None 	 None 	 
2025-07-24 20:27:57.689361 test begin: paddle.greater_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), ) 	 25405120 	 1000 	 1.0474541187286377 	 0.9098262786865234 	 1.037978172302246 	 0.8979489803314209 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:00.067575 test begin: paddle.greater_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 61062, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 61062, 8],"int64"), ) 	 228616128 	 1000 	 1.7901091575622559 	 1.570662260055542 	 1.7793316841125488 	 1.5586917400360107 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:07.090657 test begin: paddle.greater_equal(Tensor([13, 2, 244247, 4, 1],"int64"), Tensor([13, 2, 244247, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 2, 244247, 4, 1],"int64"), Tensor([13, 2, 244247, 1, 8],"int64"), ) 	 76205064 	 1000 	 1.1117541790008545 	 0.9981043338775635 	 1.102463960647583 	 0.9862592220306396 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:10.556355 test begin: paddle.greater_equal(Tensor([13, 30531, 16, 4, 1],"int64"), Tensor([13, 30531, 16, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([13, 30531, 16, 4, 1],"int64"), Tensor([13, 30531, 16, 1, 8],"int64"), ) 	 76205376 	 1000 	 1.1043107509613037 	 0.9981749057769775 	 1.0945124626159668 	 0.9854352474212646 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:13.921277 test begin: paddle.greater_equal(Tensor([16935, 10, 15, 20],"float32"), Tensor([16935, 10, 15, 20],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([16935, 10, 15, 20],"float32"), Tensor([16935, 10, 15, 20],"float32"), ) 	 101610000 	 1000 	 0.3282186985015869 	 0.32793617248535156 	 0.31925153732299805 	 0.31671953201293945 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:16.321945 test begin: paddle.greater_equal(Tensor([198451, 2, 16, 4, 1],"int64"), Tensor([198451, 2, 16, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([198451, 2, 16, 4, 1],"int64"), Tensor([198451, 2, 16, 1, 8],"int64"), ) 	 76205184 	 1000 	 1.1112995147705078 	 0.9980173110961914 	 1.0943961143493652 	 0.9859275817871094 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:19.737936 test begin: paddle.greater_equal(Tensor([49613, 1024, 1, 1],"float32"), Tensor([1],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([49613, 1024, 1, 1],"float32"), Tensor([1],"float32"), ) 	 50803713 	 1000 	 0.18890976905822754 	 0.2312936782836914 	 0.1784067153930664 	 0.21915793418884277 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:21.041612 test begin: paddle.greater_equal(Tensor([5, 10, 15, 67738],"float32"), Tensor([5, 10, 15, 67738],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([5, 10, 15, 67738],"float32"), Tensor([5, 10, 15, 67738],"float32"), ) 	 101607000 	 1000 	 0.32735371589660645 	 0.32810068130493164 	 0.3110668659210205 	 0.309520959854126 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:23.312429 test begin: paddle.greater_equal(Tensor([5, 10, 50804, 20],"float32"), Tensor([5, 10, 50804, 20],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([5, 10, 50804, 20],"float32"), Tensor([5, 10, 50804, 20],"float32"), ) 	 101608000 	 1000 	 0.329345703125 	 0.3279397487640381 	 0.3183009624481201 	 0.3165731430053711 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:25.676433 test begin: paddle.greater_equal(Tensor([5, 33869, 15, 20],"float32"), Tensor([5, 33869, 15, 20],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([5, 33869, 15, 20],"float32"), Tensor([5, 33869, 15, 20],"float32"), ) 	 101607000 	 1000 	 0.33960747718811035 	 0.32857608795166016 	 0.31827449798583984 	 0.3165726661682129 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:30.973530 test begin: paddle.greater_equal(Tensor([8, 1024, 1, 6202],"float32"), Tensor([1],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([8, 1024, 1, 6202],"float32"), Tensor([1],"float32"), ) 	 50806785 	 1000 	 0.18832850456237793 	 0.2404634952545166 	 0.17791366577148438 	 0.22076678276062012 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:32.298024 test begin: paddle.greater_equal(Tensor([8, 1024, 6202, 1],"float32"), Tensor([1],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([8, 1024, 6202, 1],"float32"), Tensor([1],"float32"), ) 	 50806785 	 1000 	 0.18727660179138184 	 0.2313075065612793 	 0.1775953769683838 	 0.21885299682617188 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:33.541109 test begin: paddle.greater_equal(Tensor([8, 6350401, 1, 1],"float32"), Tensor([1],"float32"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([8, 6350401, 1, 1],"float32"), Tensor([1],"float32"), ) 	 50803209 	 1000 	 0.18801403045654297 	 0.2313249111175537 	 0.1782398223876953 	 0.21938776969909668 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:34.772258 test begin: paddle.greater_equal(Tensor([99226, 2, 16, 4, 1],"int64"), Tensor([99226, 2, 16, 1, 8],"int64"), )
[Prof] paddle.greater_equal 	 paddle.greater_equal(Tensor([99226, 2, 16, 4, 1],"int64"), Tensor([99226, 2, 16, 1, 8],"int64"), ) 	 38102784 	 1000 	 1.2311596870422363 	 0.5104131698608398 	 0.5451858043670654 	 0.48931026458740234 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:38.913091 test begin: paddle.greater_than(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 50803600 	 1000 	 0.19582891464233398 	 0.2572948932647705 	 0.18205595016479492 	 0.23694801330566406 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:40.231615 test begin: paddle.greater_than(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), ) 	 50803600 	 1000 	 0.18807029724121094 	 0.24950599670410156 	 0.17848587036132812 	 0.23573613166809082 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:41.493299 test begin: paddle.greater_than(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 101606800 	 1000 	 0.32729339599609375 	 0.3279082775115967 	 0.3183257579803467 	 0.31671857833862305 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:43.805726 test begin: paddle.greater_than(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), ) 	 101606420 	 1000 	 0.32730793952941895 	 0.3278660774230957 	 0.31809163093566895 	 0.31682300567626953 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:46.154848 test begin: paddle.greater_than(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float32"), )
W0724 20:28:49.549626 163390 dygraph_functions.cc:90428] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float32"), ) 	 203212812 	 1000 	 1.1308960914611816 	 0.718228816986084 	 0.5778412818908691 	 0.7068088054656982 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:51.614757 test begin: paddle.greater_than(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float64"), ) 	 203212812 	 1000 	 2.0301358699798584 	 0.9821319580078125 	 1.0379414558410645 	 0.9705913066864014 	 None 	 None 	 None 	 None 	 
2025-07-24 20:28:58.649923 test begin: paddle.greater_than(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), ) 	 101606440 	 1000 	 0.3274102210998535 	 0.32790064811706543 	 0.3183009624481201 	 0.31670331954956055 	 None 	 None 	 None 	 None 	 
2025-07-24 20:29:01.037642 test begin: paddle.greater_than(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float32"), ) 	 203212816 	 1000 	 1.130810022354126 	 0.7201502323150635 	 0.5777859687805176 	 0.7075562477111816 	 None 	 None 	 None 	 None 	 
2025-07-24 20:29:06.636569 test begin: paddle.greater_than(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float64"), ) 	 203212816 	 1000 	 2.0319759845733643 	 0.9794068336486816 	 1.0383546352386475 	 0.9678480625152588 	 None 	 None 	 None 	 None 	 
2025-07-24 20:29:13.761420 test begin: paddle.greater_than(Tensor([4, 3, 2116801],"float16"), Tensor([4, 3, 2116801],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3, 2116801],"float16"), Tensor([4, 3, 2116801],"float64"), ) 	 50803224 	 1000 	 0.5182080268859863 	 0.2510833740234375 	 0.26549363136291504 	 0.2395157814025879 	 None 	 None 	 None 	 None 	 
2025-07-24 20:29:15.677650 test begin: paddle.greater_than(Tensor([4, 3, 4233601],"float16"), Tensor([4, 3, 4233601],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3, 4233601],"float16"), Tensor([4, 3, 4233601],"float32"), ) 	 101606424 	 1000 	 0.5699944496154785 	 0.36356639862060547 	 0.29126620292663574 	 0.35178112983703613 	 None 	 None 	 None 	 None 	 
2025-07-24 20:29:18.415749 test begin: paddle.greater_than(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float32"), ) 	 203212824 	 1000 	 1.1305525302886963 	 0.7184197902679443 	 0.5777099132537842 	 0.706895112991333 	 None 	 None 	 None 	 None 	 
2025-07-24 20:29:23.821804 test begin: paddle.greater_than(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float64"), ) 	 203212824 	 1000 	 2.0316851139068604 	 0.9792170524597168 	 1.038160800933838 	 0.9677042961120605 	 None 	 None 	 None 	 None 	 
2025-07-24 20:29:30.982845 test begin: paddle.greater_than(Tensor([4, 3175201, 2],"float16"), Tensor([4, 3175201, 2],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 3175201, 2],"float16"), Tensor([4, 3175201, 2],"float64"), ) 	 50803216 	 1000 	 0.5167930126190186 	 1.0037693977355957 	 0.2640812397003174 	 0.2392258644104004 	 None 	 None 	 None 	 None 	 
2025-07-24 20:29:35.628035 test begin: paddle.greater_than(Tensor([4, 6350401, 2],"float16"), Tensor([4, 6350401, 2],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4, 6350401, 2],"float16"), Tensor([4, 6350401, 2],"float32"), ) 	 101606416 	 1000 	 0.5754311084747314 	 0.36539268493652344 	 0.29114270210266113 	 0.35159802436828613 	 None 	 None 	 None 	 None 	 
2025-07-24 20:29:41.600065 test begin: paddle.greater_than(Tensor([4233601, 3, 2],"float16"), Tensor([4233601, 3, 2],"float64"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([4233601, 3, 2],"float16"), Tensor([4233601, 3, 2],"float64"), ) 	 50803212 	 1000 	 0.5172498226165771 	 0.2513279914855957 	 0.264324426651001 	 0.23113751411437988 	 None 	 None 	 None 	 None 	 
2025-07-24 20:29:43.417122 test begin: paddle.greater_than(Tensor([8467201, 3, 2],"float16"), Tensor([8467201, 3, 2],"float32"), )
[Prof] paddle.greater_than 	 paddle.greater_than(Tensor([8467201, 3, 2],"float16"), Tensor([8467201, 3, 2],"float32"), ) 	 101606412 	 1000 	 0.5697784423828125 	 0.3634960651397705 	 0.2911100387573242 	 0.35212230682373047 	 None 	 None 	 None 	 None 	 
2025-07-24 20:29:46.148853 test begin: paddle.heaviside(Tensor([24807, 2048],"float32"), Tensor([1],"float32"), )
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([24807, 2048],"float32"), Tensor([1],"float32"), ) 	 50804737 	 1000 	 0.29738926887512207 	 0.3032968044281006 	 0.28676366806030273 	 0.2913174629211426 	 None 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-07-24 20:30:23.124073 test begin: paddle.heaviside(Tensor([24807, 2048],"float32"), Tensor([2048],"float32"), )
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([24807, 2048],"float32"), Tensor([2048],"float32"), ) 	 50806784 	 1000 	 0.2979319095611572 	 0.30594301223754883 	 0.2877049446105957 	 0.29402589797973633 	 None 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-07-24 20:30:26.775869 test begin: paddle.heaviside(Tensor([24807, 2048],"float32"), Tensor([24807, 2048],"float32"), )
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([24807, 2048],"float32"), Tensor([24807, 2048],"float32"), ) 	 101609472 	 1000 	 0.4508183002471924 	 0.4467785358428955 	 0.4414045810699463 	 0.43524980545043945 	 None 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-07-24 20:30:30.849330 test begin: paddle.heaviside(Tensor([300, 169345],"float32"), Tensor([169345],"float32"), )
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([300, 169345],"float32"), Tensor([169345],"float32"), ) 	 50972845 	 1000 	 0.296306848526001 	 0.30750203132629395 	 0.2834174633026123 	 0.2955436706542969 	 None 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-07-24 20:30:33.813762 test begin: paddle.heaviside(Tensor([300, 169345],"float32"), Tensor([1],"float32"), )
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([300, 169345],"float32"), Tensor([1],"float32"), ) 	 50803501 	 1000 	 0.29671239852905273 	 0.307420015335083 	 0.28652167320251465 	 0.2924957275390625 	 None 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-07-24 20:31:11.116619 test begin: paddle.heaviside(Tensor([300, 169345],"float32"), Tensor([300, 169345],"float32"), )
[Prof] paddle.heaviside 	 paddle.heaviside(Tensor([300, 169345],"float32"), Tensor([300, 169345],"float32"), ) 	 101607000 	 1000 	 0.45079994201660156 	 0.44679689407348633 	 0.441530704498291 	 0.43257665634155273 	 None 	 None 	 None 	 None 	 
[Error] derivative for aten::heaviside is not implemented
2025-07-24 20:31:15.168666 test begin: paddle.histogram(input=Tensor([4, 6350401],"int64"), )
[Prof] paddle.histogram 	 paddle.histogram(input=Tensor([4, 6350401],"int64"), ) 	 25401604 	 1000 	 6.433353424072266 	 0.7669203281402588 	 0.0003712177276611328 	 0.0004146099090576172 	 None 	 None 	 None 	 None 	 
2025-07-24 20:31:22.860573 test begin: paddle.histogram(input=Tensor([6350401, 4],"int64"), )
[Prof] paddle.histogram 	 paddle.histogram(input=Tensor([6350401, 4],"int64"), ) 	 25401604 	 1000 	 6.431525945663452 	 0.7678332328796387 	 0.0003693103790283203 	 0.0004143714904785156 	 None 	 None 	 None 	 None 	 
2025-07-24 20:31:30.514688 test begin: paddle.histogram_bin_edges(Tensor([2540161, 20],"float32"), bins=10, min=0, max=1, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([2540161, 20],"float32"), bins=10, min=0, max=1, ) 	 50803220 	 1000 	 0.0992591381072998 	 0.01615309715270996 	 1.5974044799804688e-05 	 4.172325134277344e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 20:31:31.464186 test begin: paddle.histogram_bin_edges(Tensor([2540161, 20],"float32"), bins=10, min=0.2, max=0.9, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([2540161, 20],"float32"), bins=10, min=0.2, max=0.9, ) 	 50803220 	 1000 	 0.09772038459777832 	 0.06660294532775879 	 1.239776611328125e-05 	 3.933906555175781e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 20:31:32.458460 test begin: paddle.histogram_bin_edges(Tensor([2540161, 20],"float32"), bins=10, min=1, max=1, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([2540161, 20],"float32"), bins=10, min=1, max=1, ) 	 50803220 	 1000 	 0.09839534759521484 	 0.018752574920654297 	 2.0265579223632812e-05 	 6.031990051269531e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 20:31:33.489703 test begin: paddle.histogram_bin_edges(Tensor([5, 10160641],"float32"), bins=10, min=0, max=1, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([5, 10160641],"float32"), bins=10, min=0, max=1, ) 	 50803205 	 1000 	 0.09779620170593262 	 0.01583242416381836 	 2.2411346435546875e-05 	 3.4332275390625e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 20:31:34.415476 test begin: paddle.histogram_bin_edges(Tensor([5, 10160641],"float32"), bins=10, min=0.2, max=0.9, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([5, 10160641],"float32"), bins=10, min=0.2, max=0.9, ) 	 50803205 	 1000 	 0.1216738224029541 	 0.01595926284790039 	 2.5987625122070312e-05 	 2.7894973754882812e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 20:31:36.562425 test begin: paddle.histogram_bin_edges(Tensor([5, 10160641],"float32"), bins=10, min=1, max=1, )
[Prof] paddle.histogram_bin_edges 	 paddle.histogram_bin_edges(Tensor([5, 10160641],"float32"), bins=10, min=1, max=1, ) 	 50803205 	 1000 	 0.09831357002258301 	 0.016177892684936523 	 1.8358230590820312e-05 	 3.123283386230469e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 20:31:38.347317 test begin: paddle.histogramdd(Tensor([12700801, 2, 2],"float64"), bins=5, weights=Tensor([12700801, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )
/usr/local/lib/python3.10/dist-packages/paddle/tensor/linalg.py:5741: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  edge = paddle.to_tensor(edge)
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd8471bf370>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 20:41:43.716801 test begin: paddle.histogramdd(Tensor([4, 1587601, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 1587601],"float64"), ranges=None, density=False, )
W0724 20:41:44.493444 33137 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/paddle/tensor/linalg.py:5741: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  edge = paddle.to_tensor(edge)
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([4, 1587601, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 1587601],"float64"), ranges=None, density=False, ) 	 31752020 	 1000 	 362.58111667633057 	 147.33515787124634 	 0.002796173095703125 	 0.0003688335418701172 	 None 	 None 	 None 	 None 	 
2025-07-24 20:50:16.536522 test begin: paddle.histogramdd(Tensor([4, 1587601, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 1587601],"float64"), ranges=None, density=True, )
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([4, 1587601, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 1587601],"float64"), ranges=None, density=True, ) 	 31752020 	 1000 	 368.0238971710205 	 148.4702877998352 	 0.0026099681854248047 	 0.0003764629364013672 	 None 	 None 	 None 	 None 	 
2025-07-24 20:58:56.280433 test begin: paddle.histogramdd(Tensor([4, 6350401, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 6350401],"float64"), ranges=None, density=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0ddfa07640>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 21:09:01.315400 test begin: paddle.histogramdd(Tensor([4, 6350401, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 6350401],"float64"), ranges=None, density=True, )
W0724 21:09:03.823994 107781 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/paddle/tensor/linalg.py:5741: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  edge = paddle.to_tensor(edge)
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6e9dc03040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 21:19:06.185889 test begin: paddle.histogramdd(Tensor([6350401, 2, 2],"float64"), bins=5, weights=Tensor([6350401, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )
W0724 21:19:07.469800 133042 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/paddle/tensor/linalg.py:5741: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  edge = paddle.to_tensor(edge)
[Prof] paddle.histogramdd 	 paddle.histogramdd(Tensor([6350401, 2, 2],"float64"), bins=5, weights=Tensor([6350401, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 	 38102406 	 1000 	 360.72091841697693 	 6.43962550163269 	 0.030516862869262695 	 0.0005323886871337891 	 None 	 None 	 None 	 None 	 
2025-07-24 21:25:16.020000 test begin: paddle.hsplit(Tensor([1411201, 6, 3],"int64"), list[-1,1,3,], )
W0724 21:25:17.013603 148882 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:25:17.040141 test begin: paddle.hsplit(Tensor([1411201, 6, 3],"int64"), list[-1,], )
W0724 21:25:17.762584 148885 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:25:17.779072 test begin: paddle.hsplit(Tensor([1411201, 6, 3],"int64"), list[2,4,], )
W0724 21:25:18.522890 148955 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:25:18.552486 test begin: paddle.hsplit(Tensor([4, 2116801, 3],"int64"), list[-1,1,3,], )
W0724 21:25:19.686513 148962 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:25:19.710718 test begin: paddle.hsplit(Tensor([4, 2116801, 3],"int64"), list[-1,], )
W0724 21:25:20.454536 149039 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:25:20.539797 test begin: paddle.hsplit(Tensor([4, 2116801, 3],"int64"), list[2,4,], )
W0724 21:25:21.270625 149046 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:25:21.286984 test begin: paddle.hsplit(Tensor([4, 6, 1058401],"int64"), list[-1,1,3,], )
W0724 21:25:22.280161 149048 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:25:22.300975 test begin: paddle.hsplit(Tensor([4, 6, 1058401],"int64"), list[-1,], )
W0724 21:25:23.056358 149119 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:25:23.093747 test begin: paddle.hsplit(Tensor([4, 6, 1058401],"int64"), list[2,4,], )
W0724 21:25:23.760183 149130 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:25:23.774445 test begin: paddle.hstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], ) 	 76204872 	 1000 	 0.927983283996582 	 0.9235594272613525 	 0.9093592166900635 	 0.902534008026123 	 0.9294860363006592 	 0.07318544387817383 	 0.861037015914917 	 0.00011110305786132812 	 
2025-07-24 21:25:29.859595 test begin: paddle.hstack(list[Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2, 1058401],"float64"),], ) 	 25401624 	 1000 	 0.31641411781311035 	 0.31320691108703613 	 0.2928473949432373 	 0.15989041328430176 	 0.3114149570465088 	 0.08360934257507324 	 0.24485111236572266 	 0.0001304149627685547 	 
2025-07-24 21:25:31.970068 test begin: paddle.hstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401870 	 1000 	 0.32028651237487793 	 0.3273444175720215 	 0.299579381942749 	 0.30527520179748535 	 0.3130049705505371 	 0.09276342391967773 	 0.24304866790771484 	 0.00012183189392089844 	 
2025-07-24 21:25:34.173873 test begin: paddle.hstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401870 	 1000 	 0.3185293674468994 	 0.3249983787536621 	 0.2902219295501709 	 0.3062551021575928 	 0.3135945796966553 	 0.07010173797607422 	 0.24364137649536133 	 4.8160552978515625e-05 	 
2025-07-24 21:25:39.820563 test begin: paddle.hstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], ) 	 76204836 	 1000 	 0.9331660270690918 	 0.9231436252593994 	 0.9110860824584961 	 0.9087457656860352 	 0.9307453632354736 	 0.08954739570617676 	 0.8612971305847168 	 0.00010275840759277344 	 
2025-07-24 21:25:45.684442 test begin: paddle.hstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 25401654 	 1000 	 0.31873631477355957 	 0.3178863525390625 	 0.2924013137817383 	 0.2966735363006592 	 0.3132295608520508 	 0.07741713523864746 	 0.2335493564605713 	 9.393692016601562e-05 	 
2025-07-24 21:25:47.913405 test begin: paddle.hstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401654 	 1000 	 0.3176240921020508 	 0.3186452388763428 	 0.292161226272583 	 0.30453944206237793 	 0.31308794021606445 	 0.09154295921325684 	 0.2424328327178955 	 0.00011587142944335938 	 
2025-07-24 21:25:49.965275 test begin: paddle.hstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 76204980 	 1000 	 0.9309024810791016 	 0.9415500164031982 	 0.9110474586486816 	 0.9145934581756592 	 0.932417631149292 	 0.06902074813842773 	 0.8604812622070312 	 4.00543212890625e-05 	 
2025-07-24 21:25:56.109415 test begin: paddle.hstack(list[Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401660 	 1000 	 0.3150312900543213 	 0.3131585121154785 	 0.2994873523712158 	 0.15989160537719727 	 0.3113265037536621 	 0.07068252563476562 	 0.25364112854003906 	 0.00010418891906738281 	 
2025-07-24 21:25:58.155665 test begin: paddle.hstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401654 	 1000 	 0.31703805923461914 	 0.31151747703552246 	 0.29918575286865234 	 0.29036521911621094 	 0.31456685066223145 	 0.08305644989013672 	 0.2363874912261963 	 0.0001087188720703125 	 
2025-07-24 21:26:00.267746 test begin: paddle.hstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 76204818 	 1000 	 0.933666467666626 	 0.9291863441467285 	 0.91593337059021 	 0.9119794368743896 	 0.9468474388122559 	 0.0692284107208252 	 0.8773548603057861 	 6.318092346191406e-05 	 
2025-07-24 21:26:09.330533 test begin: paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401870 	 1000 	 0.3181607723236084 	 0.3151514530181885 	 0.30004382133483887 	 0.300978422164917 	 0.31319689750671387 	 0.07027363777160645 	 0.2372438907623291 	 7.033348083496094e-05 	 
2025-07-24 21:26:11.417137 test begin: paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 76204890 	 1000 	 0.9329514503479004 	 0.937119722366333 	 0.915278434753418 	 0.9220175743103027 	 0.9440064430236816 	 0.0696113109588623 	 0.8740329742431641 	 7.462501525878906e-05 	 
2025-07-24 21:26:17.505582 test begin: paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401630 	 1000 	 0.31759023666381836 	 0.3132474422454834 	 0.3022325038909912 	 0.15994524955749512 	 0.3144557476043701 	 0.05537247657775879 	 0.2576138973236084 	 6.341934204101562e-05 	 
2025-07-24 21:26:19.576639 test begin: paddle.hstack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], ) 	 76204824 	 1000 	 0.9731407165527344 	 1.3939299583435059 	 0.9554486274719238 	 1.3794996738433838 	 0.9607975482940674 	 0.06918978691101074 	 0.8905339241027832 	 7.462501525878906e-05 	 
2025-07-24 21:26:26.167997 test begin: paddle.hstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 76204920 	 1000 	 0.9305675029754639 	 1.0329053401947021 	 0.9101419448852539 	 1.0114810466766357 	 0.9380440711975098 	 0.0944526195526123 	 0.8560266494750977 	 0.00018858909606933594 	 
2025-07-24 21:26:32.897663 test begin: paddle.hstack(list[Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.hstack 	 paddle.hstack(list[Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401640 	 1000 	 0.3099825382232666 	 0.3133676052093506 	 0.2947361469268799 	 0.16001558303833008 	 0.3273794651031494 	 0.06126093864440918 	 0.2712066173553467 	 0.00011801719665527344 	 
2025-07-24 21:26:34.953613 test begin: paddle.hypot(Tensor([10, 5080321],"float32"), Tensor([10, 1],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([10, 5080321],"float32"), Tensor([10, 1],"float32"), ) 	 50803220 	 1000 	 0.9666097164154053 	 0.3313407897949219 	 0.24630093574523926 	 0.30239224433898926 	 1.114790678024292 	 1.8319518566131592 	 0.22876596450805664 	 0.31159210205078125 	 
2025-07-24 21:26:42.643306 test begin: paddle.hypot(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), ) 	 101606420 	 1000 	 1.4842441082000732 	 0.4483480453491211 	 0.37798571586608887 	 0.43735337257385254 	 1.6659824848175049 	 1.7940542697906494 	 0.34073615074157715 	 0.45770883560180664 	 
2025-07-24 21:26:50.486875 test begin: paddle.hypot(Tensor([2540161, 20],"float32"), Tensor([2540161, 20],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([2540161, 20],"float32"), Tensor([2540161, 20],"float32"), ) 	 101606440 	 1000 	 1.480884075164795 	 0.4483342170715332 	 0.37928152084350586 	 0.4372999668121338 	 1.6656415462493896 	 1.7928683757781982 	 0.3406858444213867 	 0.4576985836029053 	 
2025-07-24 21:26:58.389679 test begin: paddle.hypot(Tensor([50803201],"float32"), Tensor([1],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([50803201],"float32"), Tensor([1],"float32"), ) 	 50803202 	 1000 	 0.9614863395690918 	 0.3145473003387451 	 0.2457277774810791 	 0.3029661178588867 	 1.060572624206543 	 1.806642770767212 	 0.21677136421203613 	 0.3067605495452881 	 
2025-07-24 21:27:04.147564 test begin: paddle.hypot(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 1000 	 1.4788248538970947 	 0.45429325103759766 	 0.3778996467590332 	 0.4372408390045166 	 1.6669340133666992 	 1.791717529296875 	 0.3406822681427002 	 0.4577212333679199 	 
2025-07-24 21:27:14.843191 test begin: paddle.hypot(Tensor([5080321, 10],"float32"), Tensor([5080321, 1],"float32"), )
[Prof] paddle.hypot 	 paddle.hypot(Tensor([5080321, 10],"float32"), Tensor([5080321, 1],"float32"), ) 	 55883531 	 1000 	 1.0157418251037598 	 0.3352828025817871 	 0.2591409683227539 	 0.32347679138183594 	 1.2986397743225098 	 2.0990893840789795 	 0.3316028118133545 	 0.42853617668151855 	 
2025-07-24 21:27:21.496631 test begin: paddle.i0(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 1000 	 0.4653348922729492 	 0.40746259689331055 	 0.456348180770874 	 0.3969614505767822 	 0.4477510452270508 	 0.8573398590087891 	 0.39411139488220215 	 0.43839550018310547 	 
2025-07-24 21:27:25.524176 test begin: paddle.i0(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 1000 	 0.4676082134246826 	 0.4074711799621582 	 0.4588184356689453 	 0.3974802494049072 	 0.44513821601867676 	 0.854407548904419 	 0.3930084705352783 	 0.4365346431732178 	 
2025-07-24 21:27:29.342454 test begin: paddle.i0(Tensor([25401601],"float64"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([25401601],"float64"), ) 	 25401601 	 1000 	 0.496518611907959 	 0.4646592140197754 	 0.48668551445007324 	 0.44866037368774414 	 0.5097177028656006 	 0.9159953594207764 	 0.4485514163970947 	 0.4680311679840088 	 
2025-07-24 21:27:32.805129 test begin: paddle.i0(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 1000 	 0.4680914878845215 	 1.0745034217834473 	 0.4520890712738037 	 0.3974034786224365 	 0.4462707042694092 	 0.8550572395324707 	 0.3836944103240967 	 0.43677687644958496 	 
2025-07-24 21:27:39.617541 test begin: paddle.i0(Tensor([50803201],"float32"), )
[Prof] paddle.i0 	 paddle.i0(Tensor([50803201],"float32"), ) 	 50803201 	 1000 	 0.46856117248535156 	 0.4073636531829834 	 0.451798677444458 	 0.39140963554382324 	 0.4447059631347656 	 0.854304313659668 	 0.37960004806518555 	 0.4364924430847168 	 
2025-07-24 21:27:43.508025 test begin: paddle.i0e(Tensor([25401601],"float64"), )
[Prof] paddle.i0e 	 paddle.i0e(Tensor([25401601],"float64"), ) 	 25401601 	 1000 	 0.3941633701324463 	 0.36896252632141113 	 0.3785419464111328 	 0.34933018684387207 	 0.5883681774139404 	 2.0044779777526855 	 0.5264508724212646 	 0.4097158908843994 	 
2025-07-24 21:27:48.035040 test begin: paddle.i0e(Tensor([50803201],"float32"), )
[Prof] paddle.i0e 	 paddle.i0e(Tensor([50803201],"float32"), ) 	 50803201 	 1000 	 0.43024468421936035 	 0.36945629119873047 	 0.4142162799835205 	 0.3511519432067871 	 0.5894968509674072 	 1.9306766986846924 	 0.5256819725036621 	 0.39475297927856445 	 
2025-07-24 21:27:53.051312 test begin: paddle.i1(Tensor([25401601],"float64"), )
[Prof] paddle.i1 	 paddle.i1(Tensor([25401601],"float64"), ) 	 25401601 	 1000 	 0.5030465126037598 	 0.4718163013458252 	 0.4841599464416504 	 0.45580625534057617 	 0.601813793182373 	 3.210029125213623 	 0.5404658317565918 	 0.2982048988342285 	 
2025-07-24 21:27:59.084405 test begin: paddle.i1(Tensor([50803201],"float32"), )
[Prof] paddle.i1 	 paddle.i1(Tensor([50803201],"float32"), ) 	 50803201 	 1000 	 0.3423349857330322 	 0.4101693630218506 	 0.33173060417175293 	 0.3940591812133789 	 0.5893363952636719 	 3.3442389965057373 	 0.5272550582885742 	 0.31048154830932617 	 
2025-07-24 21:28:05.499490 test begin: paddle.i1e(Tensor([25401601],"float64"), )
[Prof] paddle.i1e 	 paddle.i1e(Tensor([25401601],"float64"), ) 	 25401601 	 1000 	 0.3950350284576416 	 0.37276196479797363 	 0.37742161750793457 	 0.36295413970947266 	 0.5954351425170898 	 3.849625825881958 	 0.5271337032318115 	 0.30283117294311523 	 
2025-07-24 21:28:11.846122 test begin: paddle.i1e(Tensor([50803201],"float32"), )
[Prof] paddle.i1e 	 paddle.i1e(Tensor([50803201],"float32"), ) 	 50803201 	 1000 	 0.3195955753326416 	 0.2974052429199219 	 0.3038904666900635 	 0.28155016899108887 	 0.5872445106506348 	 4.044471025466919 	 0.5252254009246826 	 0.31830739974975586 	 
2025-07-24 21:28:20.558944 test begin: paddle.incubate.nn.functional.fused_dropout_add(Tensor([7576, 13412],"bfloat16"), Tensor([7576, 13412],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, )
/usr/local/lib/python3.10/dist-packages/paddle/incubate/nn/functional/fused_dropout_add.py:100: UserWarning: Currently, fused_dropout_add maybe has precision problem, so it falls back to dropout + add. 
  warnings.warn(
[Prof] paddle.incubate.nn.functional.fused_dropout_add 	 paddle.incubate.nn.functional.fused_dropout_add(Tensor([7576, 13412],"bfloat16"), Tensor([7576, 13412],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, ) 	 203218624 	 1000 	 0.4489729404449463 	 0.4504060745239258 	 0.4300673007965088 	 0.42034077644348145 	 0.9639079570770264 	 0.4535858631134033 	 0.8875834941864014 	 0.36582112312316895 	 combined
2025-07-24 21:28:27.868013 test begin: paddle.incubate.nn.functional.fused_dropout_add(Tensor([7712, 13176],"bfloat16"), Tensor([7712, 13176],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, )
[Prof] paddle.incubate.nn.functional.fused_dropout_add 	 paddle.incubate.nn.functional.fused_dropout_add(Tensor([7712, 13176],"bfloat16"), Tensor([7712, 13176],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, ) 	 203226624 	 1000 	 0.4501917362213135 	 0.45040202140808105 	 0.42939066886901855 	 0.4203910827636719 	 0.9626178741455078 	 0.4550797939300537 	 0.8877215385437012 	 0.36870694160461426 	 combined
2025-07-24 21:28:35.291212 test begin: paddle.incubate.nn.functional.fused_dropout_add(Tensor([79381, 1280],"bfloat16"), Tensor([79381, 1280],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, )
[Prof] paddle.incubate.nn.functional.fused_dropout_add 	 paddle.incubate.nn.functional.fused_dropout_add(Tensor([79381, 1280],"bfloat16"), Tensor([79381, 1280],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, ) 	 203215360 	 1000 	 0.4552912712097168 	 0.45041751861572266 	 0.4300415515899658 	 0.4202585220336914 	 0.9623825550079346 	 0.45351362228393555 	 0.8851072788238525 	 0.35805249214172363 	 combined
2025-07-24 21:28:44.098238 test begin: paddle.incubate.nn.functional.fused_dropout_add(Tensor([8168, 12440],"bfloat16"), Tensor([8168, 12440],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, )
[Prof] paddle.incubate.nn.functional.fused_dropout_add 	 paddle.incubate.nn.functional.fused_dropout_add(Tensor([8168, 12440],"bfloat16"), Tensor([8168, 12440],"bfloat16"), p=0.0, training=True, mode="upscale_in_train", name=None, ) 	 203219840 	 1000 	 0.4487740993499756 	 0.45039844512939453 	 0.437335729598999 	 0.42884349822998047 	 0.9653754234313965 	 0.45358848571777344 	 0.8948910236358643 	 0.3689236640930176 	 combined
2025-07-24 21:28:51.483092 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 14176, 7168],"bfloat16"), Tensor([7168, 12800],"bfloat16"), Tensor([12800],"bfloat16"), )
W0724 21:29:07.950054 156681 backward.cc:462] While running Node (FusedGemmEpilogueGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-24 21:29:08.002058 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 12404],"bfloat16"), Tensor([12404, 8192],"bfloat16"), None, False, None, )
W0724 21:29:15.971967 157320 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-24 21:29:16.003749 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 16384],"bfloat16"), Tensor([16384, 6202],"bfloat16"), None, False, None, )
W0724 21:29:25.329377 157504 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-24 21:29:25.398981 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 24807],"bfloat16"), Tensor([24807, 8192],"bfloat16"), None, False, None, )
W0724 21:29:49.255741 157819 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-24 21:29:49.311758 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 4096, 8192],"bfloat16"), Tensor([8192, 12404],"bfloat16"), None, transpose_weight=False, )
W0724 21:29:57.722821 158573 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-24 21:29:57.752002 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 6202, 16384],"bfloat16"), Tensor([16384, 8192],"bfloat16"), None, False, None, )
W0724 21:30:09.217769 158826 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-24 21:30:09.260708 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 8192, 12404],"bfloat16"), Tensor([12404, 12800],"bfloat16"), Tensor([12800],"bfloat16"), )
W0724 21:30:30.938843 159300 backward.cc:462] While running Node (FusedGemmEpilogueGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-24 21:30:31.009510 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 8192, 7168],"bfloat16"), Tensor([7168, 14176],"bfloat16"), Tensor([14176],"bfloat16"), )
W0724 21:30:42.842720 160330 backward.cc:462] While running Node (FusedGemmEpilogueGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-24 21:30:42.907546 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([2, 4096, 16384],"bfloat16"), Tensor([16384, 8192],"bfloat16"), None, False, None, )
W0724 21:30:56.782567 160592 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-24 21:30:56.875438 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([2, 8192, 7168],"bfloat16"), Tensor([7168, 12800],"bfloat16"), Tensor([12800],"bfloat16"), )
W0724 21:31:15.562345 161096 backward.cc:462] While running Node (FusedGemmEpilogueGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-24 21:31:15.622691 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 4096, 8192],"bfloat16"), Tensor([8192, 100352],"bfloat16"), None, transpose_weight=False, )
W0724 21:33:44.730890 161735 backward.cc:462] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-24 21:33:45.074085 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1016065, 50],"float32"), Tensor([40, 50],"float32"), None, False, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1016065, 50],"float32"), Tensor([40, 50],"float32"), None, False, True, ) 	 50805250 	 1000 	 0.4731428623199463 	 0.4791569709777832 	 0.4514181613922119 	 0.43830370903015137 	 0.9037840366363525 	 0.899590253829956 	 0.3074967861175537 	 0.3060898780822754 	 
2025-07-24 21:33:51.652679 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1016065, 50],"float32"), Tensor([50, 40],"float32"), None, False, False, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1016065, 50],"float32"), Tensor([50, 40],"float32"), None, False, False, ) 	 50805250 	 1000 	 0.47182369232177734 	 0.4661252498626709 	 0.4550027847290039 	 0.4459800720214844 	 0.9097244739532471 	 0.9070143699645996 	 0.3095684051513672 	 0.3086719512939453 	 
2025-07-24 21:33:55.925716 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1270081, 30],"float32"), Tensor([40, 1270081],"float32"), None, True, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1270081, 30],"float32"), Tensor([40, 1270081],"float32"), None, True, True, ) 	 88905670 	 1000 	 0.39793896675109863 	 0.39366817474365234 	 0.20241332054138184 	 0.20110368728637695 	 0.6903033256530762 	 0.693777322769165 	 0.17627692222595215 	 0.17716217041015625 	 
2025-07-24 21:33:59.538509 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1693441, 30],"float32"), Tensor([40, 1693441],"float32"), None, True, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([1693441, 30],"float32"), Tensor([40, 1693441],"float32"), None, True, True, ) 	 118540870 	 1000 	 0.5216100215911865 	 0.5176706314086914 	 0.26563167572021484 	 0.26430749893188477 	 0.9172859191894531 	 0.9135799407958984 	 0.23395991325378418 	 0.2332932949066162 	 
2025-07-24 21:34:04.439954 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1270081],"float32"), Tensor([1270081, 40],"float32"), None, False, False, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1270081],"float32"), Tensor([1270081, 40],"float32"), None, False, False, ) 	 88905670 	 1000 	 0.41942405700683594 	 0.4147765636444092 	 0.21416020393371582 	 0.21193432807922363 	 0.7338578701019287 	 0.7296178340911865 	 0.18700122833251953 	 0.1863417625427246 	 
2025-07-24 21:34:08.337709 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1270081],"float32"), Tensor([40, 1270081],"float32"), None, False, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1270081],"float32"), Tensor([40, 1270081],"float32"), None, False, True, ) 	 88905670 	 1000 	 0.40462589263916016 	 0.40332865715026855 	 0.206573486328125 	 0.2066805362701416 	 0.6922497749328613 	 0.6875920295715332 	 0.1764514446258545 	 0.17559051513671875 	 
2025-07-24 21:34:12.024692 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1693441],"float32"), Tensor([1693441, 40],"float32"), None, False, False, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1693441],"float32"), Tensor([1693441, 40],"float32"), None, False, False, ) 	 118540870 	 1000 	 0.5424127578735352 	 0.5394115447998047 	 0.276932954788208 	 0.2750227451324463 	 0.9547481536865234 	 0.9520962238311768 	 0.24349188804626465 	 0.24315810203552246 	 
2025-07-24 21:34:17.109076 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1693441],"float32"), Tensor([40, 1693441],"float32"), None, False, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 1693441],"float32"), Tensor([40, 1693441],"float32"), None, False, True, ) 	 118540870 	 1000 	 0.5308210849761963 	 0.528601884841919 	 0.2710287570953369 	 0.26983213424682617 	 0.9078378677368164 	 0.9065415859222412 	 0.23249483108520508 	 0.23156142234802246 	 
2025-07-24 21:34:21.984509 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([1016065, 50],"float32"), None, False, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([1016065, 50],"float32"), None, False, True, ) 	 50804750 	 1000 	 0.31801843643188477 	 0.29934167861938477 	 0.2859954833984375 	 0.2708919048309326 	 0.6307880878448486 	 0.6286211013793945 	 0.21458911895751953 	 0.21342039108276367 	 
2025-07-24 21:34:25.187362 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([50, 1016065],"float32"), None, False, False, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([50, 1016065],"float32"), None, False, False, ) 	 50804750 	 1000 	 0.30391454696655273 	 0.2951774597167969 	 0.28137636184692383 	 0.27304959297180176 	 0.6510038375854492 	 0.6463277339935303 	 0.22145819664001465 	 0.2198495864868164 	 
2025-07-24 21:34:28.378484 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 1016065],"float32"), Tensor([40, 50],"float32"), None, True, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 1016065],"float32"), Tensor([40, 50],"float32"), None, True, True, ) 	 50805250 	 1000 	 0.46157240867614746 	 0.45946764945983887 	 0.44618940353393555 	 0.42642736434936523 	 0.8871679306030273 	 0.8821766376495361 	 0.30201196670532227 	 0.30022549629211426 	 
2025-07-24 21:34:32.600419 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([1016065, 50],"float32"), None, True, True, )
[Prof] paddle.incubate.nn.functional.fused_matmul_bias 	 paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([1016065, 50],"float32"), None, True, True, ) 	 50804750 	 1000 	 0.3140597343444824 	 0.3089261054992676 	 0.28679680824279785 	 0.271395206451416 	 0.6544675827026367 	 0.6536509990692139 	 0.22255444526672363 	 0.22241711616516113 	 
2025-07-24 21:34:37.274045 test begin: paddle.incubate.nn.functional.swiglu(Tensor([14176, 7168],"bfloat16"), )
W0724 21:34:40.408092  5427 backward.cc:462] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (float32) does not match the type of data (bfloat16) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():16 != phi::CppTypeToDataType<T>::Type():10.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-24 21:34:40.434610 test begin: paddle.incubate.segment_max(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:129: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_max" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_max" instead.
    Reason: paddle.incubate.segment_max will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:147: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_max" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_max" instead.
    Reason: paddle.incubate.segment_max will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.incubate.segment_max 	 paddle.incubate.segment_max(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), ) 	 50803206 	 1000 	 0.6784913539886475 	 0.8552746772766113 	 0.0006477832794189453 	 0.00011444091796875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:34:43.963169 test begin: paddle.incubate.segment_mean(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:129: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_mean" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_mean" instead.
    Reason: paddle.incubate.segment_mean will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:147: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_mean" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_mean" instead.
    Reason: paddle.incubate.segment_mean will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.incubate.segment_mean 	 paddle.incubate.segment_mean(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), ) 	 50803206 	 1000 	 0.37136340141296387 	 0.9224293231964111 	 0.0003266334533691406 	 0.00011706352233886719 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:36:57.071586 test begin: paddle.incubate.segment_min(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:129: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_min" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_min" instead.
    Reason: paddle.incubate.segment_min will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:147: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_min" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_min" instead.
    Reason: paddle.incubate.segment_min will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.incubate.segment_min 	 paddle.incubate.segment_min(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), ) 	 50803206 	 1000 	 0.6747512817382812 	 0.8505716323852539 	 0.0006518363952636719 	 0.00011396408081054688 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:37:00.372931 test begin: paddle.incubate.segment_sum(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:129: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_sum" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_sum" instead.
    Reason: paddle.incubate.segment_sum will be removed in future [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:147: VisibleDeprecationWarning: [93m
Warning:
API "paddle.incubate.tensor.math.segment_sum" is deprecated since 2.4.0, and will be removed in future versions. Please use "paddle.geometric.segment_sum" instead.
    Reason: paddle.incubate.segment_sum will be removed in future [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.incubate.segment_sum 	 paddle.incubate.segment_sum(Tensor([3, 16934401],"float32"), Tensor([3],"int32"), ) 	 50803206 	 1000 	 0.6860446929931641 	 0.6031432151794434 	 0.0006577968597412109 	 0.3054049015045166 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:37:07.322770 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1013, 1, 224, 224],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1013, 1, 224, 224],"float32"), ) 	 50828288 	 1000 	 0.26070165634155273 	 0.6093358993530273 	 0.25140833854675293 	 0.15592026710510254 	 0.32734060287475586 	 0.8938310146331787 	 0.27379512786865234 	 0.4566800594329834 	 combined
2025-07-24 21:37:11.128667 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([145, 7, 224, 224],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([145, 7, 224, 224],"float32"), ) 	 50928640 	 1000 	 0.26029348373413086 	 0.6104583740234375 	 0.2511751651763916 	 0.15622162818908691 	 0.3292653560638428 	 0.895535945892334 	 0.2776048183441162 	 0.4575226306915283 	 combined
2025-07-24 21:37:14.835765 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([3, 338, 224, 224],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([3, 338, 224, 224],"float32"), ) 	 50878464 	 1000 	 0.25954389572143555 	 0.6190047264099121 	 0.2510244846343994 	 0.1560359001159668 	 0.32901692390441895 	 0.8948147296905518 	 0.27729272842407227 	 0.4571380615234375 	 combined
2025-07-24 21:37:18.557610 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([4511, 11, 32, 32],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([4511, 11, 32, 32],"float32"), ) 	 50811904 	 1000 	 0.7470159530639648 	 0.6785218715667725 	 0.7384846210479736 	 0.17434287071228027 	 0.4492475986480713 	 0.89404296875 	 0.3978762626647949 	 0.4567749500274658 	 combined
2025-07-24 21:37:22.973010 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([5, 203, 224, 224],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([5, 203, 224, 224],"float32"), ) 	 50928640 	 1000 	 0.25966572761535645 	 0.6103885173797607 	 0.25113844871520996 	 0.15621614456176758 	 0.32920074462890625 	 0.8982260227203369 	 0.2639908790588379 	 0.4589087963104248 	 combined
2025-07-24 21:37:26.671952 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([7, 7088, 32, 32],"float32"), )
[Prof] paddle.incubate.softmax_mask_fuse_upper_triangle 	 paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([7, 7088, 32, 32],"float32"), ) 	 50806784 	 1000 	 0.7464783191680908 	 0.6756627559661865 	 0.7379188537597656 	 0.17288637161254883 	 0.44920945167541504 	 0.8983004093170166 	 0.3887474536895752 	 0.45819544792175293 	 combined
2025-07-24 21:37:31.119227 test begin: paddle.index_add(Tensor([100, 100, 25402],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 25402],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 100, 25402],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 25402],"float32"), ) 	 304824020 	 1000 	 1.993187427520752 	 3.852097272872925 	 0.6744251251220703 	 0.0002243518829345703 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:37:50.179936 test begin: paddle.index_add(Tensor([100, 100, 25],"float32"), Tensor([5081],"int32"), 2, Tensor([100, 100, 5081],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 100, 25],"float32"), Tensor([5081],"int32"), 2, Tensor([100, 100, 5081],"float32"), ) 	 51065081 	 1000 	 1.7599527835845947 	 367.2781431674957 	 0.899407148361206 	 0.00021719932556152344 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:44:01.106127 test begin: paddle.index_add(Tensor([100, 100, 5081],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 5081],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 100, 5081],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 5081],"float32"), ) 	 60972020 	 1000 	 0.4109175205230713 	 1.764763593673706 	 0.1397111415863037 	 6.604194641113281e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:44:05.597245 test begin: paddle.index_add(Tensor([100, 100, 5081],"float32"), Tensor([20],"int32"), 2, Tensor([100, 100, 20],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 100, 5081],"float32"), Tensor([20],"int32"), 2, Tensor([100, 100, 20],"float32"), ) 	 51010020 	 1000 	 0.34171152114868164 	 1.7576570510864258 	 0.11614751815795898 	 9.059906005859375e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:44:09.743073 test begin: paddle.index_add(Tensor([100, 100, 5],"float32"), Tensor([101607],"int32"), 1, Tensor([100, 101607, 5],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff4236fa800>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753365249 (unix time) try "date -d @1753365249" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x206ca) received by PID 132810 (TID 0x7ff41ecc4640) from PID 132810 ***]

2025-07-24 21:54:17.980589 test begin: paddle.index_add(Tensor([100, 101607, 5],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 5],"float32"), )
W0724 21:54:18.981055 65639 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 101607, 5],"float32"), Tensor([20],"int32"), 1, Tensor([100, 20, 5],"float32"), ) 	 50813520 	 1000 	 0.31674909591674805 	 1.8271141052246094 	 0.10762166976928711 	 0.0001354217529296875 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:54:22.699445 test begin: paddle.index_add(Tensor([100, 2540161],"float32"), Tensor([20],"int32"), 0, Tensor([20, 2540161],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 2540161],"float32"), Tensor([20],"int32"), 0, Tensor([20, 2540161],"float32"), ) 	 304819340 	 1000 	 2.001858949661255 	 3.33567214012146 	 0.6791303157806396 	 0.00022339820861816406 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:54:43.250765 test begin: paddle.index_add(Tensor([100, 508033],"float32"), Tensor([20],"int32"), 0, Tensor([20, 508033],"float32"), )
[Prof] paddle.index_add 	 paddle.index_add(Tensor([100, 508033],"float32"), Tensor([20],"int32"), 0, Tensor([20, 508033],"float32"), ) 	 60963980 	 1000 	 0.40384817123413086 	 1.77449631690979 	 0.13737845420837402 	 9.72747802734375e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:54:47.767462 test begin: paddle.index_add(Tensor([100, 5],"float32"), Tensor([10160641],"int32"), 0, Tensor([10160641, 5],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2c02279240>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 22:05:25.501679 test begin: paddle.index_add(Tensor([10160641, 5],"float32"), Tensor([20],"int32"), 0, Tensor([20, 5],"float32"), )
W0724 22:05:26.508680 102158 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.index_add 	 paddle.index_add(Tensor([10160641, 5],"float32"), Tensor([20],"int32"), 0, Tensor([20, 5],"float32"), ) 	 50803325 	 1000 	 0.3166658878326416 	 1.7819490432739258 	 0.10761213302612305 	 0.00010752677917480469 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:05:30.326857 test begin: paddle.index_fill(Tensor([10, 1016065, 10],"float16"), Tensor([5],"int64"), 1, 0.5, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 1016065, 10],"float16"), Tensor([5],"int64"), 1, 0.5, ) 	 101606505 	 1000 	 0.8206021785736084 	 0.3278670310974121 	 0.0006928443908691406 	 0.10786938667297363 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:05:39.204368 test begin: paddle.index_fill(Tensor([10, 15, 169345],"int64"), Tensor([5],"int32"), 1, -1, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 15, 169345],"int64"), Tensor([5],"int32"), 1, -1, ) 	 25401755 	 1000 	 1.0031225681304932 	 0.36821937561035156 	 0.063751220703125 	 0.1251814365386963 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:05:43.135823 test begin: paddle.index_fill(Tensor([10, 15, 338689],"bool"), Tensor([5],"int32"), 1, True, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 15, 338689],"bool"), Tensor([5],"int32"), 1, True, ) 	 50803355 	 1000 	 0.9160358905792236 	 0.17052364349365234 	 0.0007421970367431641 	 0.057996273040771484 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:05:46.501437 test begin: paddle.index_fill(Tensor([10, 15, 677377],"float16"), Tensor([5],"int64"), 1, 0.5, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 15, 677377],"float16"), Tensor([5],"int64"), 1, 0.5, ) 	 101606555 	 1000 	 2.0423953533172607 	 0.5004875659942627 	 0.0019028186798095703 	 0.16584062576293945 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:05:55.184452 test begin: paddle.index_fill(Tensor([10, 254017, 10],"int64"), Tensor([5],"int32"), 1, -1, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 254017, 10],"int64"), Tensor([5],"int32"), 1, -1, ) 	 25401705 	 1000 	 0.660496711730957 	 0.31794095039367676 	 0.04489731788635254 	 0.1080467700958252 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:05:57.669769 test begin: paddle.index_fill(Tensor([10, 508033, 10],"bool"), Tensor([5],"int32"), 1, True, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([10, 508033, 10],"bool"), Tensor([5],"int32"), 1, True, ) 	 50803305 	 1000 	 0.35351037979125977 	 0.08819770812988281 	 0.00020051002502441406 	 0.029895782470703125 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:05:59.818130 test begin: paddle.index_fill(Tensor([169345, 15, 10],"int64"), Tensor([5],"int32"), 1, -1, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([169345, 15, 10],"int64"), Tensor([5],"int32"), 1, -1, ) 	 25401755 	 1000 	 1.0978643894195557 	 0.4561476707458496 	 0.06987476348876953 	 0.1552276611328125 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:06:03.317293 test begin: paddle.index_fill(Tensor([338689, 15, 10],"bool"), Tensor([5],"int32"), 1, True, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([338689, 15, 10],"bool"), Tensor([5],"int32"), 1, True, ) 	 50803355 	 1000 	 1.0892257690429688 	 0.1698758602142334 	 0.0009601116180419922 	 0.0560612678527832 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:06:07.218453 test begin: paddle.index_fill(Tensor([677377, 15, 10],"float16"), Tensor([5],"int64"), 1, 0.5, )
[Prof] paddle.index_fill 	 paddle.index_fill(Tensor([677377, 15, 10],"float16"), Tensor([5],"int64"), 1, 0.5, ) 	 101606555 	 1000 	 2.455108880996704 	 0.5294520854949951 	 0.0023157596588134766 	 0.1789076328277588 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:06:16.488306 test begin: paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, ) 	 25628144 	 1000 	 0.3652486801147461 	 0.4576432704925537 	 0.021881103515625 	 0.012386083602905273 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:06:19.231593 test begin: paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), False, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), False, ) 	 25628144 	 1000 	 0.3565704822540283 	 0.3291890621185303 	 0.025997161865234375 	 0.08388662338256836 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:06:21.370094 test begin: paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 42, 99, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, ) 	 25628144 	 1000 	 0.3565065860748291 	 0.44956493377685547 	 0.02599930763244629 	 0.013272762298583984 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:06:25.619727 test begin: paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, ) 	 25541904 	 1000 	 0.3653874397277832 	 0.45876574516296387 	 0.021854877471923828 	 0.012331962585449219 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:06:29.330141 test begin: paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), False, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), False, ) 	 25541904 	 1000 	 0.3561549186706543 	 0.32807326316833496 	 0.025968074798583984 	 0.08364057540893555 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:06:31.473119 test begin: paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([110, 74, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, ) 	 25541904 	 1000 	 0.3574483394622803 	 0.44839978218078613 	 0.025983810424804688 	 0.01322627067565918 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:06:33.701231 test begin: paddle.index_put(Tensor([193, 42, 56, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([193, 42, 56, 56],"float64"), tuple(Tensor([16, 16],"int32"),Tensor([16, 16],"int32"),Tensor([1, 16],"int32"),), Tensor([16, 16, 56],"float64"), True, ) 	 25435280 	 1000 	 0.3634030818939209 	 0.47037744522094727 	 0.021744966506958008 	 0.0123138427734375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:06:39.127133 test begin: paddle.index_put(Tensor([193, 42, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, )
[Prof] paddle.index_put 	 paddle.index_put(Tensor([193, 42, 56, 56],"float64"), tuple(Tensor([16, 16],"int64"),Tensor([16, 16],"int64"),Tensor([1, 16],"int64"),), Tensor([16, 16, 56],"float64"), True, ) 	 25435280 	 1000 	 0.3619856834411621 	 0.459244966506958 	 0.02591395378112793 	 0.01318812370300293 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:06:41.435942 test begin: paddle.index_sample(Tensor([1865664, 100],"float32"), Tensor([1865664, 14],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([1865664, 100],"float32"), Tensor([1865664, 14],"int64"), ) 	 212685696 	 1000 	 0.7516462802886963 	 1.1394155025482178 	 0.7419531345367432 	 0.38798975944519043 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:06:49.529826 test begin: paddle.index_sample(Tensor([1865664, 28],"float32"), Tensor([1865664, 14],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([1865664, 28],"float32"), Tensor([1865664, 14],"int64"), ) 	 78357888 	 1000 	 0.525226354598999 	 0.8043355941772461 	 0.5170888900756836 	 0.2741994857788086 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:06:53.108319 test begin: paddle.index_sample(Tensor([1865664, 28],"float32"), Tensor([1865664, 1],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([1865664, 28],"float32"), Tensor([1865664, 1],"int64"), ) 	 54104256 	 1000 	 0.4230775833129883 	 0.15806317329406738 	 0.4070312976837158 	 0.07862591743469238 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:06:55.120608 test begin: paddle.index_sample(Tensor([25401601, 100],"float32"), Tensor([25401601, 1],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([25401601, 100],"float32"), Tensor([25401601, 1],"int64"), ) 	 2565561701 	 1000 	 4.541287660598755 	 2.0984649658203125 	 4.526015520095825 	 1.0717785358428955 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:07:58.015660 test begin: paddle.index_sample(Tensor([25401601, 20],"float32"), Tensor([25401601, 1],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([25401601, 20],"float32"), Tensor([25401601, 1],"int64"), ) 	 533433621 	 1000 	 4.332056045532227 	 1.7986247539520264 	 4.323756217956543 	 0.9202280044555664 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:08:18.570687 test begin: paddle.index_sample(Tensor([2540161, 20],"float32"), Tensor([2540161, 1],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([2540161, 20],"float32"), Tensor([2540161, 1],"int64"), ) 	 53343381 	 1000 	 0.5393226146697998 	 0.19848942756652832 	 0.5310981273651123 	 0.10127544403076172 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:08:20.839829 test begin: paddle.index_sample(Tensor([508033, 100],"float32"), Tensor([508033, 1],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([508033, 100],"float32"), Tensor([508033, 1],"int64"), ) 	 51311333 	 1000 	 0.11950349807739258 	 0.06523323059082031 	 0.111236572265625 	 0.007903814315795898 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:08:22.188865 test begin: paddle.index_sample(Tensor([5135296, 10],"float32"), Tensor([5135296, 1],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([5135296, 10],"float32"), Tensor([5135296, 1],"int64"), ) 	 56488256 	 1000 	 0.9573962688446045 	 0.31704282760620117 	 0.9491753578186035 	 0.16184568405151367 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:08:25.398372 test begin: paddle.index_sample(Tensor([5135296, 10],"float32"), Tensor([5135296, 5],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([5135296, 10],"float32"), Tensor([5135296, 5],"int64"), ) 	 77029440 	 1000 	 1.0735487937927246 	 0.8682115077972412 	 1.0615012645721436 	 0.29387450218200684 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:08:30.093052 test begin: paddle.index_sample(Tensor([5135296, 20],"float32"), Tensor([5135296, 5],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([5135296, 20],"float32"), Tensor([5135296, 5],"int64"), ) 	 128382400 	 1000 	 1.111877679824829 	 1.006777048110962 	 1.103579044342041 	 0.33890819549560547 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:08:39.089261 test begin: paddle.index_sample(Tensor([932832, 100],"float32"), Tensor([932832, 28],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([932832, 100],"float32"), Tensor([932832, 28],"int64"), ) 	 119402496 	 1000 	 0.506361722946167 	 0.881436824798584 	 0.4982743263244629 	 0.29942774772644043 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:08:44.274148 test begin: paddle.index_sample(Tensor([932832, 55],"float32"), Tensor([932832, 1],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([932832, 55],"float32"), Tensor([932832, 1],"int64"), ) 	 52238592 	 1000 	 0.21313691139221191 	 0.08618569374084473 	 0.20500588417053223 	 0.040271759033203125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:08:45.764497 test begin: paddle.index_sample(Tensor([932832, 55],"float32"), Tensor([932832, 28],"int64"), )
[Prof] paddle.index_sample 	 paddle.index_sample(Tensor([932832, 55],"float32"), Tensor([932832, 28],"int64"), ) 	 77425056 	 1000 	 0.38974618911743164 	 0.7770094871520996 	 0.38152122497558594 	 0.26490259170532227 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:08:49.211456 test begin: paddle.index_select(Tensor([16, 11109, 286],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([16, 11109, 286],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50834864 	 1000 	 0.2023606300354004 	 0.2295060157775879 	 0.19250941276550293 	 0.20830035209655762 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:08:51.242416 test begin: paddle.index_select(Tensor([16, 12096, 263],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([16, 12096, 263],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50900048 	 1000 	 0.21416234970092773 	 0.230910062789917 	 0.20519113540649414 	 0.2171945571899414 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:08:53.310891 test begin: paddle.index_select(Tensor([16, 39201, 81],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([16, 39201, 81],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50804576 	 1000 	 0.6328871250152588 	 0.5589084625244141 	 0.6238288879394531 	 0.5431363582611084 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:08:56.935519 test begin: paddle.index_select(Tensor([205, 3060, 81],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([205, 3060, 81],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50811380 	 1000 	 0.6317863464355469 	 0.5570845603942871 	 0.615365743637085 	 0.536414384841919 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:09:00.569520 test begin: paddle.index_select(Tensor([52, 12096, 81],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([52, 12096, 81],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50948432 	 1000 	 0.6367380619049072 	 0.5584120750427246 	 0.6203587055206299 	 0.5379161834716797 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:09:04.216624 test begin: paddle.index_select(Tensor([57, 11109, 81],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([57, 11109, 81],"float32"), Tensor([80],"int64"), axis=-1, ) 	 51290333 	 1000 	 0.6380999088287354 	 0.5630724430084229 	 0.6276905536651611 	 0.5492615699768066 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:09:07.873445 test begin: paddle.index_select(Tensor([64, 3060, 260],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([64, 3060, 260],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50918480 	 1000 	 0.21402239799499512 	 0.229447603225708 	 0.2049717903137207 	 0.2128136157989502 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:09:09.938608 test begin: paddle.index_select(Tensor([64, 9801, 81],"float32"), Tensor([80],"int64"), axis=-1, )
[Prof] paddle.index_select 	 paddle.index_select(Tensor([64, 9801, 81],"float32"), Tensor([80],"int64"), axis=-1, ) 	 50808464 	 1000 	 0.6335821151733398 	 0.5608048439025879 	 0.6246094703674316 	 0.5447003841400146 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 22:09:13.548540 test begin: paddle.inner(Tensor([20, 1270081],"float64"), Tensor([1270081],"float64"), )
[Prof] paddle.inner 	 paddle.inner(Tensor([20, 1270081],"float64"), Tensor([1270081],"float64"), ) 	 26671701 	 1000 	 0.16359329223632812 	 0.16400480270385742 	 0.07439708709716797 	 0.08370852470397949 	 0.3551936149597168 	 0.36566162109375 	 0.1208951473236084 	 0.12394118309020996 	 
2025-07-24 22:09:15.362325 test begin: paddle.inner(Tensor([20, 25401601],"float64"), Tensor([25401601],"float64"), )
[Prof] paddle.inner 	 paddle.inner(Tensor([20, 25401601],"float64"), Tensor([25401601],"float64"), ) 	 533433621 	 1000 	 3.034334659576416 	 3.044405460357666 	 1.5510718822479248 	 1.5577411651611328 	 6.9139244556427 	 6.870943069458008 	 0.27361178398132324 	 0.2702302932739258 	 
2025-07-24 22:09:48.431303 test begin: paddle.inner(Tensor([508033, 50],"float64"), Tensor([50],"float64"), )
[Prof] paddle.inner 	 paddle.inner(Tensor([508033, 50],"float64"), Tensor([50],"float64"), ) 	 25401700 	 1000 	 0.15661931037902832 	 0.15626859664916992 	 0.08848881721496582 	 0.1283409595489502 	 0.37625694274902344 	 0.375424861907959 	 0.12806487083435059 	 0.12766695022583008 	 
2025-07-24 22:09:50.109724 test begin: paddle.is_complex(Tensor([100352, 507],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([100352, 507],"float32"), ) 	 50878464 	 1000 	 0.003929615020751953 	 0.0017096996307373047 	 1.7642974853515625e-05 	 1.6689300537109375e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:09:50.939829 test begin: paddle.is_complex(Tensor([507, 100352],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([507, 100352],"float32"), ) 	 50878464 	 1000 	 0.0036573410034179688 	 0.0016908645629882812 	 1.8835067749023438e-05 	 1.621246337890625e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:09:51.774915 test begin: paddle.is_complex(Tensor([6202, 8192],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([6202, 8192],"float32"), ) 	 50806784 	 1000 	 0.0036437511444091797 	 0.0017201900482177734 	 1.0013580322265625e-05 	 1.5020370483398438e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:09:52.756662 test begin: paddle.is_complex(Tensor([8192, 6202],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([8192, 6202],"float32"), ) 	 50806784 	 1000 	 0.0036373138427734375 	 0.0017027854919433594 	 5.4836273193359375e-06 	 1.4781951904296875e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:09:53.602828 test begin: paddle.is_complex(Tensor([886, 57344],"float32"), )
[Prof] paddle.is_complex 	 paddle.is_complex(Tensor([886, 57344],"float32"), ) 	 50806784 	 1000 	 0.0036182403564453125 	 0.002389669418334961 	 1.6927719116210938e-05 	 3.933906555175781e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 22:09:54.432821 test begin: paddle.is_empty(Tensor([10160641, 5],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(Tensor([10160641, 5],"float32"), ) 	 50803205 	 1000 	 0.0034897327423095703 	 0.001524209976196289 	 6.9141387939453125e-06 	 1.6927719116210938e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 22:09:55.270109 test begin: paddle.is_empty(Tensor([16934401, 3],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(Tensor([16934401, 3],"float32"), ) 	 50803203 	 1000 	 0.003555774688720703 	 0.0014934539794921875 	 8.58306884765625e-06 	 1.5497207641601562e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 22:09:56.146778 test begin: paddle.is_empty(Tensor([2, 25401601],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(Tensor([2, 25401601],"float32"), ) 	 50803202 	 1000 	 0.003528118133544922 	 0.00148773193359375 	 6.67572021484375e-06 	 1.4781951904296875e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 22:09:57.043266 test begin: paddle.is_empty(Tensor([3, 16934401],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(Tensor([3, 16934401],"float32"), ) 	 50803203 	 1000 	 0.0035407543182373047 	 0.0014982223510742188 	 1.8835067749023438e-05 	 1.4781951904296875e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 22:09:57.885993 test begin: paddle.is_empty(x=Tensor([4, 32, 396901],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(x=Tensor([4, 32, 396901],"float32"), ) 	 50803328 	 1000 	 0.003697633743286133 	 0.0014848709106445312 	 7.62939453125e-06 	 1.4781951904296875e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 22:09:58.743725 test begin: paddle.is_empty(x=Tensor([4, 396901, 32],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(x=Tensor([4, 396901, 32],"float32"), ) 	 50803328 	 1000 	 0.0036821365356445312 	 0.00147247314453125 	 7.152557373046875e-06 	 1.7404556274414062e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 22:09:59.570605 test begin: paddle.is_empty(x=Tensor([49613, 32, 32],"float32"), )
[Prof] paddle.is_empty 	 paddle.is_empty(x=Tensor([49613, 32, 32],"float32"), ) 	 50803712 	 1000 	 0.003710031509399414 	 0.001481771469116211 	 1.8835067749023438e-05 	 1.5020370483398438e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 22:10:00.428208 test begin: paddle.isclose(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), rtol=1e-05, atol=1e-08, )
[Prof] paddle.isclose 	 paddle.isclose(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), rtol=1e-05, atol=1e-08, ) 	 50803220 	 1000 	 0.36459875106811523 	 3.0830323696136475 	 0.3518490791320801 	 0.24172520637512207 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:05.097870 test begin: paddle.isclose(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), rtol=1e-05, atol=1e-08, )
[Prof] paddle.isclose 	 paddle.isclose(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), rtol=1e-05, atol=1e-08, ) 	 50803220 	 1000 	 0.3645322322845459 	 3.0828652381896973 	 0.35173988342285156 	 0.24169611930847168 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:09.665067 test begin: paddle.isclose(x=Tensor([1270081, 4, 5],"float64"), y=Tensor([1270081, 4, 5],"float64"), )
[Prof] paddle.isclose 	 paddle.isclose(x=Tensor([1270081, 4, 5],"float64"), y=Tensor([1270081, 4, 5],"float64"), ) 	 50803240 	 1000 	 0.36449694633483887 	 3.083278179168701 	 0.3518040180206299 	 0.24179601669311523 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:14.231474 test begin: paddle.isclose(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), )
[Prof] paddle.isclose 	 paddle.isclose(x=Tensor([25401601],"float64"), y=Tensor([25401601],"float64"), ) 	 50803202 	 1000 	 0.3646845817565918 	 3.083125114440918 	 0.35213708877563477 	 0.24172425270080566 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:18.784282 test begin: paddle.isclose(x=Tensor([3, 1693441, 5],"float64"), y=Tensor([3, 1693441, 5],"float64"), )
[Prof] paddle.isclose 	 paddle.isclose(x=Tensor([3, 1693441, 5],"float64"), y=Tensor([3, 1693441, 5],"float64"), ) 	 50803230 	 1000 	 0.3645291328430176 	 3.0830888748168945 	 0.3518369197845459 	 0.241835355758667 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:23.330278 test begin: paddle.isclose(x=Tensor([3, 4, 2116801],"float64"), y=Tensor([3, 4, 2116801],"float64"), )
[Prof] paddle.isclose 	 paddle.isclose(x=Tensor([3, 4, 2116801],"float64"), y=Tensor([3, 4, 2116801],"float64"), ) 	 50803224 	 1000 	 0.3631455898284912 	 3.0831427574157715 	 0.35021042823791504 	 0.24172353744506836 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:27.920509 test begin: paddle.isfinite(Tensor([1738, 94, 311],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([1738, 94, 311],"float32"), ) 	 50808692 	 1000 	 0.23350238800048828 	 0.7896764278411865 	 0.22450017929077148 	 0.20138096809387207 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:29.856462 test begin: paddle.isfinite(Tensor([28462, 17, 5, 6, 7],"float16"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([28462, 17, 5, 6, 7],"float16"), ) 	 101609340 	 1000 	 0.39533495903015137 	 0.9733588695526123 	 0.38822460174560547 	 0.24830961227416992 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:33.232756 test begin: paddle.isfinite(Tensor([4, 280, 376, 25, 5],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 280, 376, 25, 5],"float32"), ) 	 52640000 	 1000 	 0.24272680282592773 	 0.8150656223297119 	 0.23562264442443848 	 0.20784878730773926 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:37.053173 test begin: paddle.isfinite(Tensor([4, 280, 376, 41, 3],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 280, 376, 41, 3],"float32"), ) 	 51797760 	 1000 	 0.23903465270996094 	 0.8147938251495361 	 0.23192834854125977 	 0.20503616333007812 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:39.918076 test begin: paddle.isfinite(Tensor([4, 280, 605, 25, 3],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 280, 605, 25, 3],"float32"), ) 	 50820000 	 1000 	 0.23577427864074707 	 0.7887165546417236 	 0.2286205291748047 	 0.20143365859985352 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:41.782845 test begin: paddle.isfinite(Tensor([4, 40839, 311],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 40839, 311],"float32"), ) 	 50803716 	 1000 	 0.23403048515319824 	 0.7913696765899658 	 0.2270197868347168 	 0.20087742805480957 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:43.663160 test begin: paddle.isfinite(Tensor([4, 451, 376, 25, 3],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 451, 376, 25, 3],"float32"), ) 	 50872800 	 1000 	 0.2362971305847168 	 0.7905161380767822 	 0.22919583320617676 	 0.20247411727905273 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:45.614410 test begin: paddle.isfinite(Tensor([4, 94, 135115],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([4, 94, 135115],"float32"), ) 	 50803240 	 1000 	 0.23446130752563477 	 0.7872672080993652 	 0.22719097137451172 	 0.20107531547546387 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:49.424086 test begin: paddle.isfinite(Tensor([7, 280, 376, 25, 3],"float32"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([7, 280, 376, 25, 3],"float32"), ) 	 55272000 	 1000 	 0.25497889518737793 	 0.8645853996276855 	 0.24110913276672363 	 0.2193915843963623 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:53.048898 test begin: paddle.isfinite(Tensor([8, 17, 17789, 6, 7],"float16"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([8, 17, 17789, 6, 7],"float16"), ) 	 101610768 	 1000 	 0.39510416984558105 	 0.9705185890197754 	 0.38142943382263184 	 0.24792861938476562 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:56.389114 test begin: paddle.isfinite(Tensor([8, 17, 5, 21346, 7],"float16"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([8, 17, 5, 21346, 7],"float16"), ) 	 101606960 	 1000 	 0.3954885005950928 	 0.9709944725036621 	 0.3805525302886963 	 0.2480478286743164 	 None 	 None 	 None 	 None 	 
2025-07-24 22:10:59.649434 test begin: paddle.isfinite(Tensor([8, 17, 5, 6, 24904],"float16"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([8, 17, 5, 6, 24904],"float16"), ) 	 101608320 	 1000 	 0.3965003490447998 	 0.9890594482421875 	 0.3828260898590088 	 0.24842190742492676 	 None 	 None 	 None 	 None 	 
2025-07-24 22:11:03.054463 test begin: paddle.isfinite(Tensor([8, 60481, 5, 6, 7],"float16"), )
[Prof] paddle.isfinite 	 paddle.isfinite(Tensor([8, 60481, 5, 6, 7],"float16"), ) 	 101608080 	 1000 	 0.3909568786621094 	 0.9715437889099121 	 0.3837742805480957 	 0.2482163906097412 	 None 	 None 	 None 	 None 	 
2025-07-24 22:11:07.419419 test begin: paddle.isin(Tensor([396901, 64],"float64"), Tensor([4, 256],"float64"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([396901, 64],"float64"), Tensor([4, 256],"float64"), False, False, ) 	 25402688 	 1000 	 2.717088460922241 	 21.32922101020813 	 0.002479076385498047 	 0.0008721351623535156 	 None 	 None 	 None 	 None 	 
2025-07-24 22:11:32.085914 test begin: paddle.isin(Tensor([793801, 64],"float32"), Tensor([4, 256],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([793801, 64],"float32"), Tensor([4, 256],"float32"), False, False, ) 	 50804288 	 1000 	 4.446208953857422 	 25.324496269226074 	 0.0042192935943603516 	 0.002039194107055664 	 None 	 None 	 None 	 None 	 
2025-07-24 22:12:04.585217 test begin: paddle.isin(Tensor([793801, 64],"float32"), Tensor([4, 256],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([793801, 64],"float32"), Tensor([4, 256],"float32"), False, True, ) 	 50804288 	 1000 	 4.527674913406372 	 25.319345474243164 	 0.004282236099243164 	 0.0020291805267333984 	 None 	 None 	 None 	 None 	 
2025-07-24 22:12:39.197053 test begin: paddle.isin(Tensor([793801, 64],"float32"), Tensor([793801, 256],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([793801, 64],"float32"), Tensor([793801, 256],"float32"), False, False, ) 	 254016320 	 1000 	 96.53215789794922 	 45.209402561187744 	 0.05245661735534668 	 0.0025899410247802734 	 None 	 None 	 None 	 None 	 
2025-07-24 22:15:05.496054 test begin: paddle.isin(Tensor([793801, 64],"float32"), Tensor([793801, 256],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([793801, 64],"float32"), Tensor([793801, 256],"float32"), False, True, ) 	 254016320 	 1000 	 99.13736653327942 	 45.27331280708313 	 0.05263519287109375 	 0.002590179443359375 	 None 	 None 	 None 	 None 	 
2025-07-24 22:17:37.403934 test begin: paddle.isin(Tensor([8, 3175201],"float64"), Tensor([4, 256],"float64"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 3175201],"float64"), Tensor([4, 256],"float64"), False, False, ) 	 25402632 	 1000 	 2.7355315685272217 	 21.333356380462646 	 0.002468109130859375 	 0.0008795261383056641 	 None 	 None 	 None 	 None 	 
2025-07-24 22:18:02.151620 test begin: paddle.isin(Tensor([8, 3175201],"float64"), Tensor([4, 3175201],"float64"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 3175201],"float64"), Tensor([4, 3175201],"float64"), False, False, ) 	 38102412 	 1000 	 29.519578456878662 	 27.259925842285156 	 0.017345905303955078 	 0.0009756088256835938 	 None 	 None 	 None 	 None 	 
2025-07-24 22:19:00.004707 test begin: paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 256],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 256],"float32"), False, False, ) 	 50804232 	 1000 	 4.44476318359375 	 25.316317319869995 	 0.00420689582824707 	 0.002010822296142578 	 None 	 None 	 None 	 None 	 
2025-07-24 22:19:31.252598 test begin: paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 256],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 256],"float32"), False, True, ) 	 50804232 	 1000 	 4.526463747024536 	 26.400716304779053 	 0.0042819976806640625 	 0.002028942108154297 	 None 	 None 	 None 	 None 	 
2025-07-24 22:20:04.288266 test begin: paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 6350401],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 6350401],"float32"), False, False, ) 	 76204812 	 1000 	 52.74315857887268 	 30.016801834106445 	 0.03565788269042969 	 0.002201080322265625 	 None 	 None 	 None 	 None 	 
2025-07-24 22:21:28.468526 test begin: paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 6350401],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 6350401],"float32"), Tensor([4, 6350401],"float32"), False, True, ) 	 76204812 	 1000 	 54.96254920959473 	 30.00447416305542 	 0.03574848175048828 	 0.0021979808807373047 	 None 	 None 	 None 	 None 	 
2025-07-24 22:22:57.034862 test begin: paddle.isin(Tensor([8, 64],"float32"), Tensor([198451, 256],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float32"), Tensor([198451, 256],"float32"), False, False, ) 	 50803968 	 1000 	 16.621143341064453 	 8.308279037475586 	 5.340576171875e-05 	 0.000286102294921875 	 None 	 None 	 None 	 None 	 
2025-07-24 22:23:22.921880 test begin: paddle.isin(Tensor([8, 64],"float32"), Tensor([198451, 256],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float32"), Tensor([198451, 256],"float32"), False, True, ) 	 50803968 	 1000 	 20.808603048324585 	 8.354149103164673 	 5.340576171875e-05 	 0.0002727508544921875 	 None 	 None 	 None 	 None 	 
2025-07-24 22:23:53.021018 test begin: paddle.isin(Tensor([8, 64],"float32"), Tensor([4, 12700801],"float32"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float32"), Tensor([4, 12700801],"float32"), False, False, ) 	 50803716 	 1000 	 21.260351181030273 	 8.301421165466309 	 5.14984130859375e-05 	 0.00028514862060546875 	 None 	 None 	 None 	 None 	 
2025-07-24 22:24:23.655873 test begin: paddle.isin(Tensor([8, 64],"float32"), Tensor([4, 12700801],"float32"), False, True, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float32"), Tensor([4, 12700801],"float32"), False, True, ) 	 50803716 	 1000 	 20.30141043663025 	 8.306336402893066 	 5.269050598144531e-05 	 0.00028204917907714844 	 None 	 None 	 None 	 None 	 
2025-07-24 22:24:53.199601 test begin: paddle.isin(Tensor([8, 64],"float64"), Tensor([4, 6350401],"float64"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float64"), Tensor([4, 6350401],"float64"), False, False, ) 	 25402116 	 1000 	 15.831857204437256 	 12.030784606933594 	 5.269050598144531e-05 	 0.00024580955505371094 	 None 	 None 	 None 	 None 	 
2025-07-24 22:25:21.697971 test begin: paddle.isin(Tensor([8, 64],"float64"), Tensor([99226, 256],"float64"), False, False, )
[Prof] paddle.isin 	 paddle.isin(Tensor([8, 64],"float64"), Tensor([99226, 256],"float64"), False, False, ) 	 25402368 	 1000 	 10.819637537002563 	 12.001962423324585 	 5.340576171875e-05 	 0.00024771690368652344 	 None 	 None 	 None 	 None 	 
2025-07-24 22:25:45.138627 test begin: paddle.isinf(Tensor([14, 226801, 16],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([14, 226801, 16],"float32"), ) 	 50803424 	 1000 	 0.23413848876953125 	 0.4856750965118408 	 0.22612237930297852 	 0.2480008602142334 	 None 	 None 	 None 	 None 	 
2025-07-24 22:25:46.769003 test begin: paddle.isinf(Tensor([14, 36655, 99],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([14, 36655, 99],"float32"), ) 	 50803830 	 1000 	 0.23272323608398438 	 0.48708486557006836 	 0.22555112838745117 	 0.24806642532348633 	 None 	 None 	 None 	 None 	 
2025-07-24 22:25:48.308927 test begin: paddle.isinf(Tensor([14, 64, 56701],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([14, 64, 56701],"float32"), ) 	 50804096 	 1000 	 0.2335984706878662 	 0.485471248626709 	 0.22624516487121582 	 0.24807357788085938 	 None 	 None 	 None 	 None 	 
2025-07-24 22:25:49.834219 test begin: paddle.isinf(Tensor([14, 7, 518401],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([14, 7, 518401],"float32"), ) 	 50803298 	 1000 	 0.23345351219177246 	 0.5014338493347168 	 0.22628307342529297 	 0.2492821216583252 	 None 	 None 	 None 	 None 	 
2025-07-24 22:25:51.455687 test begin: paddle.isinf(Tensor([28462, 17, 5, 6, 7],"float16"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([28462, 17, 5, 6, 7],"float16"), ) 	 101609340 	 1000 	 0.3891739845275879 	 0.5220320224761963 	 0.3820526599884033 	 0.2659151554107666 	 None 	 None 	 None 	 None 	 
2025-07-24 22:25:54.239468 test begin: paddle.isinf(Tensor([49613, 64, 16],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([49613, 64, 16],"float32"), ) 	 50803712 	 1000 	 0.23394370079040527 	 0.4903535842895508 	 0.22668671607971191 	 0.24808192253112793 	 None 	 None 	 None 	 None 	 
2025-07-24 22:25:55.976222 test begin: paddle.isinf(Tensor([73310, 7, 99],"float32"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([73310, 7, 99],"float32"), ) 	 50803830 	 1000 	 0.23274588584899902 	 0.48548269271850586 	 0.22558879852294922 	 0.2480766773223877 	 None 	 None 	 None 	 None 	 
2025-07-24 22:25:57.553145 test begin: paddle.isinf(Tensor([8, 17, 17789, 6, 7],"float16"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([8, 17, 17789, 6, 7],"float16"), ) 	 101610768 	 1000 	 0.3889150619506836 	 0.5253915786743164 	 0.3817410469055176 	 0.26592493057250977 	 None 	 None 	 None 	 None 	 
2025-07-24 22:26:00.487410 test begin: paddle.isinf(Tensor([8, 17, 5, 21346, 7],"float16"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([8, 17, 5, 21346, 7],"float16"), ) 	 101606960 	 1000 	 0.3899226188659668 	 0.5203151702880859 	 0.38278889656066895 	 0.2658555507659912 	 None 	 None 	 None 	 None 	 
2025-07-24 22:26:03.283495 test begin: paddle.isinf(Tensor([8, 17, 5, 6, 24904],"float16"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([8, 17, 5, 6, 24904],"float16"), ) 	 101608320 	 1000 	 0.39047861099243164 	 0.5203566551208496 	 0.3833448886871338 	 0.26589107513427734 	 None 	 None 	 None 	 None 	 
2025-07-24 22:26:06.149002 test begin: paddle.isinf(Tensor([8, 60481, 5, 6, 7],"float16"), )
[Prof] paddle.isinf 	 paddle.isinf(Tensor([8, 60481, 5, 6, 7],"float16"), ) 	 101608080 	 1000 	 0.38539576530456543 	 0.5203297138214111 	 0.37822675704956055 	 0.26587724685668945 	 None 	 None 	 None 	 None 	 
2025-07-24 22:26:11.239840 test begin: paddle.isnan(Tensor([10445, 4864],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([10445, 4864],"float32"), ) 	 50804480 	 1000 	 0.2354903221130371 	 1.1744804382324219 	 0.22837305068969727 	 0.17580056190490723 	 None 	 None 	 None 	 None 	 
2025-07-24 22:26:14.561191 test begin: paddle.isnan(Tensor([16, 64, 320, 320],"float16"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([16, 64, 320, 320],"float16"), ) 	 104857600 	 1000 	 0.4066195487976074 	 0.23392462730407715 	 0.3993704319000244 	 0.22118806838989258 	 None 	 None 	 None 	 None 	 
2025-07-24 22:26:17.162364 test begin: paddle.isnan(Tensor([4, 125, 320, 320],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 125, 320, 320],"float32"), ) 	 51200000 	 1000 	 0.23620080947875977 	 0.1875293254852295 	 0.22219276428222656 	 0.16988730430603027 	 None 	 None 	 None 	 None 	 
2025-07-24 22:26:18.421153 test begin: paddle.isnan(Tensor([4, 249, 320, 320],"float16"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 249, 320, 320],"float16"), ) 	 101990400 	 1000 	 0.3938305377960205 	 0.23620009422302246 	 0.3865926265716553 	 0.21501374244689941 	 None 	 None 	 None 	 None 	 
2025-07-24 22:26:20.973343 test begin: paddle.isnan(Tensor([4, 64, 1241, 320],"float16"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 64, 1241, 320],"float16"), ) 	 101662720 	 1000 	 0.3917555809020996 	 0.22516870498657227 	 0.3845088481903076 	 0.21404790878295898 	 None 	 None 	 None 	 None 	 
2025-07-24 22:26:23.507958 test begin: paddle.isnan(Tensor([4, 64, 320, 1241],"float16"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 64, 320, 1241],"float16"), ) 	 101662720 	 1000 	 0.3917202949523926 	 0.2282576560974121 	 0.38452696800231934 	 0.2154536247253418 	 None 	 None 	 None 	 None 	 
2025-07-24 22:26:26.026814 test begin: paddle.isnan(Tensor([4, 64, 320, 621],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 64, 320, 621],"float32"), ) 	 50872320 	 1000 	 0.23435592651367188 	 0.18633198738098145 	 0.22723865509033203 	 0.17524123191833496 	 None 	 None 	 None 	 None 	 
2025-07-24 22:26:27.260854 test begin: paddle.isnan(Tensor([4, 64, 621, 320],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4, 64, 621, 320],"float32"), ) 	 50872320 	 1000 	 0.235581636428833 	 0.20708537101745605 	 0.22843408584594727 	 0.17647862434387207 	 None 	 None 	 None 	 None 	 
2025-07-24 22:26:28.533931 test begin: paddle.isnan(Tensor([4864, 10445],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([4864, 10445],"float32"), ) 	 50804480 	 1000 	 0.23403239250183105 	 0.186079740524292 	 0.224287748336792 	 0.17505145072937012 	 None 	 None 	 None 	 None 	 
2025-07-24 22:26:29.762901 test begin: paddle.isnan(Tensor([8, 64, 320, 320],"float32"), )
[Prof] paddle.isnan 	 paddle.isnan(Tensor([8, 64, 320, 320],"float32"), ) 	 52428800 	 1000 	 0.24168729782104492 	 0.19179797172546387 	 0.23451566696166992 	 0.18095970153808594 	 None 	 None 	 None 	 None 	 
2025-07-24 22:26:31.083268 test begin: paddle.isneginf(Tensor([11, 17, 271675],"int32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbf72694580>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 22:36:36.267416 test begin: paddle.isneginf(Tensor([11, 17, 543350],"int16"), )
W0724 22:36:40.260444 33977 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8690677010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 22:46:43.401729 test begin: paddle.isneginf(Tensor([11, 4618473],"float32"), )
W0724 22:46:44.524613 63964 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2f97186e30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 22:56:50.110677 test begin: paddle.isneginf(Tensor([11, 461848, 10],"int32"), )
W0724 22:56:52.668001 103102 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb77f6fb010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 23:06:54.968678 test begin: paddle.isneginf(Tensor([11, 923695, 10],"int16"), )
W0724 23:06:56.304147 142685 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2174107070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 23:17:02.808186 test begin: paddle.isneginf(Tensor([2988424, 17],"float32"), )
W0724 23:17:03.814152 19362 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc7ac223070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 23:27:07.736368 test begin: paddle.isneginf(Tensor([298843, 17, 10],"int32"), )
W0724 23:27:08.598013 59561 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7eb4d96e30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 23:37:14.167317 test begin: paddle.isneginf(Tensor([597685, 17, 10],"int16"), )
W0724 23:37:15.312358 99131 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f19e2b06e30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 23:47:24.071395 test begin: paddle.isposinf(Tensor([11, 17, 271675],"int32"), )
W0724 23:47:24.947677 140853 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8998a77010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 23:57:31.392664 test begin: paddle.isposinf(Tensor([11, 17, 543350],"int16"), )
W0724 23:57:32.527642 26795 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f28da27f010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 00:07:43.584616 test begin: paddle.isposinf(Tensor([11, 4618473],"float32"), )
W0725 00:07:44.683904 75159 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f170a95b070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 00:17:49.688521 test begin: paddle.isposinf(Tensor([11, 461848, 10],"int32"), )
W0725 00:17:50.945048 124432 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f17add1ae30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 00:27:55.969732 test begin: paddle.isposinf(Tensor([11, 923695, 10],"int16"), )
W0725 00:27:57.183666  9388 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdcab6bf070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 00:38:06.747015 test begin: paddle.isposinf(Tensor([2988424, 17],"float32"), )
W0725 00:38:07.700295 59004 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f91e6d9b010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 00:48:12.170895 test begin: paddle.isposinf(Tensor([298843, 17, 10],"int32"), )
W0725 00:48:13.011523 107708 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f224fef7010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 00:58:17.307822 test begin: paddle.isposinf(Tensor([597685, 17, 10],"int16"), )
W0725 00:58:18.481562 157211 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe23c10b070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 01:08:28.306547 test begin: paddle.isreal(Tensor([1587601, 32],"bool"), )
W0725 01:08:29.239452 43979 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.isreal 	 paddle.isreal(Tensor([1587601, 32],"bool"), ) 	 50803232 	 1000 	 0.041823625564575195 	 0.037534236907958984 	 0.02509760856628418 	 0.023773193359375 	 None 	 None 	 None 	 None 	 
2025-07-25 01:08:29.444870 test begin: paddle.isreal(Tensor([3175201, 32],"bfloat16"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([3175201, 32],"bfloat16"), ) 	 101606432 	 1000 	 0.08129000663757324 	 0.06868886947631836 	 0.06425714492797852 	 0.05672812461853027 	 None 	 None 	 None 	 None 	 
2025-07-25 01:08:31.257698 test begin: paddle.isreal(Tensor([3175201, 32],"float16"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([3175201, 32],"float16"), ) 	 101606432 	 1000 	 0.08339691162109375 	 0.07014632225036621 	 0.06385588645935059 	 0.05817270278930664 	 None 	 None 	 None 	 None 	 
2025-07-25 01:08:33.605974 test begin: paddle.isreal(Tensor([64, 1587601],"bfloat16"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([64, 1587601],"bfloat16"), ) 	 101606464 	 1000 	 0.755908727645874 	 0.0752706527709961 	 0.06402897834777832 	 0.05693674087524414 	 None 	 None 	 None 	 None 	 
2025-07-25 01:08:39.050611 test begin: paddle.isreal(Tensor([64, 1587601],"float16"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([64, 1587601],"float16"), ) 	 101606464 	 1000 	 0.08123588562011719 	 0.06874251365661621 	 0.0640707015991211 	 0.05700182914733887 	 None 	 None 	 None 	 None 	 
2025-07-25 01:08:41.150205 test begin: paddle.isreal(Tensor([64, 793801],"bool"), )
[Prof] paddle.isreal 	 paddle.isreal(Tensor([64, 793801],"bool"), ) 	 50803264 	 1000 	 0.04158592224121094 	 0.038256168365478516 	 0.025383710861206055 	 0.024474620819091797 	 None 	 None 	 None 	 None 	 
2025-07-25 01:08:41.946086 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([423361, 5, 4, 3, 2],"float32"), )
W0725 01:14:05.129685 44842 backward.cc:466] While running Node (KronGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   KronGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::kron_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::KronGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 18.925712GB memory on GPU 0, 38.909180GB memory has been allocated and available memory is only 496.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-25 01:14:05.141780 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 423361, 4, 3, 2],"float32"), )
W0725 01:19:26.376253 70669 backward.cc:466] While running Node (KronGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   KronGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::kron_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::KronGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 18.925712GB memory on GPU 0, 38.909180GB memory has been allocated and available memory is only 496.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-25 01:19:26.389347 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 338689, 3, 2],"float32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_kron(_object*, _object*, _object*)
1   kron_ad_func(paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::kron(paddle::Tensor const&, paddle::Tensor const&)
3   void phi::KronKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 18.925723GB memory on GPU 0, 38.907227GB memory has been allocated and available memory is only 498.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-25 01:19:38.735953 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 4, 254017, 2],"float32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_kron(_object*, _object*, _object*)
1   kron_ad_func(paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::kron(paddle::Tensor const&, paddle::Tensor const&)
3   void phi::KronKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 18.925741GB memory on GPU 0, 39.096680GB memory has been allocated and available memory is only 304.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-25 01:19:49.777429 test begin: paddle.kron(Tensor([10, 10],"float32"), Tensor([5, 5, 4, 3, 169345],"float32"), )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_kron(_object*, _object*, _object*)
1   kron_ad_func(paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::kron(paddle::Tensor const&, paddle::Tensor const&)
3   void phi::KronKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
5   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
6   paddle::memory::allocation::Allocator::Allocate(unsigned long)
7   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 18.925779GB memory on GPU 0, 39.286133GB memory has been allocated and available memory is only 110.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-25 01:20:00.768288 test begin: paddle.kthvalue(Tensor([30, 200, 8468],"float32"), k=1, axis=1, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.817139MB memory on GPU 0, 39.286133GB memory has been allocated and available memory is only 110.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-25 01:20:11.778460 test begin: paddle.kthvalue(Tensor([30, 200, 8468],"float32"), k=1, axis=1, keepdim=True, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.817139MB memory on GPU 0, 39.286133GB memory has been allocated and available memory is only 110.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-25 01:20:23.379924 test begin: paddle.kthvalue(Tensor([30, 200, 8468],"float32"), k=2, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.817139MB memory on GPU 0, 39.286133GB memory has been allocated and available memory is only 110.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-25 01:20:34.372666 test begin: paddle.kthvalue(Tensor([30, 42337, 40],"float32"), k=1, axis=1, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.803467MB memory on GPU 0, 39.286133GB memory has been allocated and available memory is only 110.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-25 01:20:46.823553 test begin: paddle.kthvalue(Tensor([30, 42337, 40],"float32"), k=1, axis=1, keepdim=True, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.803467MB memory on GPU 0, 39.286133GB memory has been allocated and available memory is only 110.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-25 01:20:57.840205 test begin: paddle.kthvalue(Tensor([30, 42337, 40],"float32"), k=2, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.803467MB memory on GPU 0, 39.286133GB memory has been allocated and available memory is only 110.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-25 01:21:09.716192 test begin: paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=1, axis=1, )
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   phi::MemoryUtils::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
7   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 193.817139MB memory on GPU 0, 39.286133GB memory has been allocated and available memory is only 110.312500MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-25 01:21:20.692712 test begin: paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=1, axis=1, keepdim=True, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=1, axis=1, keepdim=True, ) 	 50808000 	 1000 	 3.981140613555908 	 4.144417762756348 	 1.0147669315338135 	 4.114122629165649 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-25 01:21:31.012755 test begin: paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=2, )
[Prof] paddle.kthvalue 	 paddle.kthvalue(Tensor([6351, 200, 40],"float32"), k=2, ) 	 50808000 	 1000 	 5.221442461013794 	 5.1427226066589355 	 5.207534074783325 	 5.123230695724487 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-25 01:21:44.205119 test begin: paddle.lcm(Tensor([1],"int64"), Tensor([25401601],"int64"), )
[Prof] paddle.lcm 	 paddle.lcm(Tensor([1],"int64"), Tensor([25401601],"int64"), ) 	 25401602 	 1000 	 78.52504706382751 	 5.621218919754028 	 0.002239227294921875 	 0.0009102821350097656 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-25 01:23:09.491753 test begin: paddle.lcm(Tensor([25401601],"int64"), Tensor([1],"int64"), )
[Prof] paddle.lcm 	 paddle.lcm(Tensor([25401601],"int64"), Tensor([1],"int64"), ) 	 25401602 	 1000 	 87.93804907798767 	 5.758676052093506 	 0.00225830078125 	 0.0009112358093261719 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-25 01:24:45.472739 test begin: paddle.lcm(Tensor([25401601],"int64"), Tensor([25401601],"int64"), )
[Prof] paddle.lcm 	 paddle.lcm(Tensor([25401601],"int64"), Tensor([25401601],"int64"), ) 	 50803202 	 1000 	 106.878333568573 	 7.229816913604736 	 0.002388477325439453 	 0.0008988380432128906 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-25 01:26:44.409321 test begin: paddle.lcm(Tensor([50803201],"int32"), Tensor([1],"int32"), )
[Prof] paddle.lcm 	 paddle.lcm(Tensor([50803201],"int32"), Tensor([1],"int32"), ) 	 50803202 	 1000 	 101.08871126174927 	 8.077198505401611 	 0.0022940635681152344 	 0.0013055801391601562 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-25 01:28:39.205275 test begin: paddle.ldexp(Tensor([25401601],"float64"), Tensor([25401601],"int32"), )
[Prof] paddle.ldexp 	 paddle.ldexp(Tensor([25401601],"float64"), Tensor([25401601],"int32"), ) 	 50803202 	 1000 	 1.2906467914581299 	 1.1056642532348633 	 0.32768845558166504 	 0.369457483291626 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-25 01:30:08.447202 test begin: paddle.ldexp(Tensor([25401601],"int64"), Tensor([25401601],"int32"), )
[Prof] paddle.ldexp 	 paddle.ldexp(Tensor([25401601],"int64"), Tensor([25401601],"int32"), ) 	 50803202 	 1000 	 0.8252780437469482 	 0.6417438983917236 	 0.16871929168701172 	 0.21873259544372559 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-25 01:30:43.094446 test begin: paddle.ldexp(Tensor([50803201],"float64"), Tensor([50803201],"int32"), )
[Prof] paddle.ldexp 	 paddle.ldexp(Tensor([50803201],"float64"), Tensor([50803201],"int32"), ) 	 101606402 	 1000 	 2.556918144226074 	 2.167370319366455 	 0.6524910926818848 	 0.7356774806976318 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-25 01:33:41.820310 test begin: paddle.ldexp(Tensor([50803201],"int32"), Tensor([50803201],"int32"), )
[Prof] paddle.ldexp 	 paddle.ldexp(Tensor([50803201],"int32"), Tensor([50803201],"int32"), ) 	 101606402 	 1000 	 1.477320671081543 	 1.135202407836914 	 0.3028833866119385 	 0.3849489688873291 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-25 01:34:50.385330 test begin: paddle.ldexp(Tensor([50803201],"int64"), Tensor([50803201],"int32"), )
[Prof] paddle.ldexp 	 paddle.ldexp(Tensor([50803201],"int64"), Tensor([50803201],"int32"), ) 	 101606402 	 1000 	 1.6358129978179932 	 1.2748193740844727 	 0.3330221176147461 	 0.4346597194671631 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-25 01:35:59.789914 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 28, 604801],"float32"), 0.36, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 28, 604801],"float32"), 0.36, ) 	 50803285 	 1000 	 0.29940319061279297 	 0.32207441329956055 	 0.1528463363647461 	 0.2918844223022461 	 0.6350786685943604 	 0.750558614730835 	 0.2167801856994629 	 0.19123458862304688 	 
2025-07-25 01:36:03.535759 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 604801, 28],"float32"), 0.36, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([3, 604801, 28],"float32"), 0.36, ) 	 50803285 	 1000 	 0.29935240745544434 	 0.3113436698913574 	 0.152848482131958 	 0.2887909412384033 	 0.6352787017822266 	 0.7505924701690674 	 0.2170090675354004 	 0.1912086009979248 	 
2025-07-25 01:36:07.583104 test begin: paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([64801, 28, 28],"float32"), 0.36, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 1, 1],"float32"), Tensor([64801, 28, 28],"float32"), 0.36, ) 	 50803985 	 1000 	 0.2994420528411865 	 0.3042891025543213 	 0.15287399291992188 	 0.2906014919281006 	 0.6339523792266846 	 0.7504651546478271 	 0.21566414833068848 	 0.19241070747375488 	 
2025-07-25 01:36:11.262473 test begin: paddle.lerp(Tensor([1, 1814401, 28],"float32"), Tensor([3, 1814401, 28],"float32"), 1.0, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 1814401, 28],"float32"), Tensor([3, 1814401, 28],"float32"), 1.0, ) 	 203212912 	 1000 	 1.3450582027435303 	 1.338515043258667 	 0.686657190322876 	 1.3150365352630615 	 2.220827341079712 	 2.3577475547790527 	 1.136113166809082 	 0.804762601852417 	 
2025-07-25 01:36:24.828573 test begin: paddle.lerp(Tensor([1, 28, 1814401],"float32"), Tensor([3, 28, 1814401],"float32"), 1.0, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 28, 1814401],"float32"), Tensor([3, 28, 1814401],"float32"), 1.0, ) 	 203212912 	 1000 	 1.3446478843688965 	 1.3319988250732422 	 0.6878619194030762 	 1.31349515914917 	 2.220972776412964 	 2.355381488800049 	 1.136216402053833 	 0.8022360801696777 	 
2025-07-25 01:36:40.138404 test begin: paddle.lerp(Tensor([1, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.0, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([1, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.0, ) 	 50804768 	 1000 	 0.30213165283203125 	 0.30841565132141113 	 0.15279388427734375 	 0.2911384105682373 	 0.793633222579956 	 0.7817950248718262 	 0.2709383964538574 	 0.19953179359436035 	 
2025-07-25 01:36:44.065834 test begin: paddle.lerp(Tensor([3, 28, 604801],"float32"), Tensor([3, 28, 604801],"float32"), 1.2, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([3, 28, 604801],"float32"), Tensor([3, 28, 604801],"float32"), 1.2, ) 	 101606568 	 1000 	 0.4548525810241699 	 0.4470489025115967 	 0.23294520378112793 	 0.4346628189086914 	 0.47408461570739746 	 0.5956175327301025 	 0.4143385887145996 	 0.30423688888549805 	 
2025-07-25 01:36:50.318104 test begin: paddle.lerp(Tensor([3, 604801, 28],"float32"), Tensor([3, 604801, 28],"float32"), 1.2, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([3, 604801, 28],"float32"), Tensor([3, 604801, 28],"float32"), 1.2, ) 	 101606568 	 1000 	 0.4593393802642822 	 0.462874174118042 	 0.23142671585083008 	 0.4340782165527344 	 0.47521257400512695 	 0.5954821109771729 	 0.4154956340789795 	 0.3042292594909668 	 
2025-07-25 01:36:55.587104 test begin: paddle.lerp(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.0, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.0, ) 	 101607968 	 1000 	 0.45345520973205566 	 0.44702863693237305 	 0.23156189918518066 	 0.43447136878967285 	 0.47719383239746094 	 0.5955281257629395 	 0.41588544845581055 	 0.3042566776275635 	 
2025-07-25 01:37:00.254858 test begin: paddle.lerp(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.2, )
[Prof] paddle.lerp 	 paddle.lerp(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), 1.2, ) 	 101607968 	 1000 	 0.4531419277191162 	 0.4504997730255127 	 0.23148036003112793 	 0.4353959560394287 	 0.4772145748138428 	 0.5956094264984131 	 0.4170572757720947 	 0.3042902946472168 	 
2025-07-25 01:37:04.759087 test begin: paddle.less(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 50803600 	 1000 	 0.1914069652557373 	 0.2438218593597412 	 0.18095111846923828 	 0.2318732738494873 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:06.030356 test begin: paddle.less(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), ) 	 50803600 	 1000 	 0.1887493133544922 	 0.24771857261657715 	 0.17845559120178223 	 0.2337028980255127 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:07.428581 test begin: paddle.less(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 101606800 	 1000 	 0.32731056213378906 	 0.32791781425476074 	 0.31764817237854004 	 0.316892147064209 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:09.718894 test begin: paddle.less(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), ) 	 101606420 	 1000 	 0.3272979259490967 	 0.3292961120605469 	 0.3174881935119629 	 0.3157668113708496 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:12.133274 test begin: paddle.less(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([10, 5080321],"float32"), Tensor([10, 5080321],"float32"), ) 	 101606420 	 1000 	 0.32746219635009766 	 0.32801079750061035 	 0.3180046081542969 	 0.31690311431884766 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:14.483269 test begin: paddle.less(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), ) 	 101606440 	 1000 	 0.32936978340148926 	 0.3314363956451416 	 0.3191077709197998 	 0.3179965019226074 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:17.196356 test begin: paddle.less(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([49613, 1024],"float32"), Tensor([49613, 1024],"float32"), ) 	 101607424 	 1000 	 0.3273468017578125 	 0.3278772830963135 	 0.31780242919921875 	 0.31673264503479004 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:19.520070 test begin: paddle.less(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.less 	 paddle.less(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 1000 	 0.32862091064453125 	 0.3280339241027832 	 0.3189883232116699 	 0.3135237693786621 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:22.181421 test begin: paddle.less_equal(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 50803600 	 1000 	 0.19074058532714844 	 0.24382328987121582 	 0.17341327667236328 	 0.22517681121826172 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:23.454754 test begin: paddle.less_equal(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), ) 	 50803600 	 1000 	 0.18889999389648438 	 0.24452710151672363 	 0.17845964431762695 	 0.23263859748840332 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:24.721631 test begin: paddle.less_equal(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 101606800 	 1000 	 0.32865405082702637 	 0.32789087295532227 	 0.3195223808288574 	 0.31668663024902344 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:27.077422 test begin: paddle.less_equal(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), ) 	 101606420 	 1000 	 0.3272995948791504 	 0.32793164253234863 	 0.31828761100769043 	 0.3166537284851074 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:29.369666 test begin: paddle.less_equal(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float32"), )
W0725 01:37:32.903438 19630 dygraph_functions.cc:90806] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([16934401, 3, 2],"float16"), Tensor([16934401, 3, 2],"float32"), ) 	 203212812 	 1000 	 1.1315109729766846 	 0.7361376285552979 	 0.5777580738067627 	 0.7100472450256348 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:35.071511 test begin: paddle.less_equal(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), ) 	 101606440 	 1000 	 0.7763116359710693 	 0.3361539840698242 	 0.31847310066223145 	 0.3064594268798828 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:39.308442 test begin: paddle.less_equal(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([4, 12700801, 2],"float16"), Tensor([4, 12700801, 2],"float32"), ) 	 203212816 	 1000 	 1.1306264400482178 	 0.725677490234375 	 0.5777168273925781 	 0.7100062370300293 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:44.797049 test begin: paddle.less_equal(Tensor([4, 3, 4233601],"float16"), Tensor([4, 3, 4233601],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([4, 3, 4233601],"float16"), Tensor([4, 3, 4233601],"float32"), ) 	 101606424 	 1000 	 0.569934606552124 	 0.3634927272796631 	 0.2912256717681885 	 0.35201501846313477 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:47.597834 test begin: paddle.less_equal(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([4, 3, 8467201],"float16"), Tensor([4, 3, 8467201],"float32"), ) 	 203212824 	 1000 	 1.1317226886749268 	 0.7192819118499756 	 0.5778791904449463 	 0.7077727317810059 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:54.846621 test begin: paddle.less_equal(Tensor([4, 6350401, 2],"float16"), Tensor([4, 6350401, 2],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([4, 6350401, 2],"float16"), Tensor([4, 6350401, 2],"float32"), ) 	 101606416 	 1000 	 0.5761275291442871 	 0.3861873149871826 	 0.29123997688293457 	 0.3557779788970947 	 None 	 None 	 None 	 None 	 
2025-07-25 01:37:58.512708 test begin: paddle.less_equal(Tensor([8467201, 3, 2],"float16"), Tensor([8467201, 3, 2],"float32"), )
[Prof] paddle.less_equal 	 paddle.less_equal(Tensor([8467201, 3, 2],"float16"), Tensor([8467201, 3, 2],"float32"), ) 	 101606412 	 1000 	 0.569894552230835 	 0.36359429359436035 	 0.29123830795288086 	 0.35105276107788086 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:01.295510 test begin: paddle.less_than(Tensor([1, 128, 198451],"int64"), Tensor([1, 128, 198451],"int64"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 128, 198451],"int64"), Tensor([1, 128, 198451],"int64"), ) 	 50803456 	 1000 	 0.3096029758453369 	 0.31746459007263184 	 0.3003273010253906 	 0.3032989501953125 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:02.932477 test begin: paddle.less_than(Tensor([1, 128, 256],"float32"), Tensor([1551, 128, 256],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 128, 256],"float32"), Tensor([1551, 128, 256],"float32"), ) 	 50855936 	 1000 	 0.19042134284973145 	 0.24442338943481445 	 0.1803302764892578 	 0.2323286533355713 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:04.196174 test begin: paddle.less_than(Tensor([1, 128, 256],"int64"), Tensor([776, 128, 256],"int64"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 128, 256],"int64"), Tensor([776, 128, 256],"int64"), ) 	 25460736 	 1000 	 0.20733976364135742 	 0.1818375587463379 	 0.19004464149475098 	 0.1629624366760254 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:05.049627 test begin: paddle.less_than(Tensor([1, 128, 396901],"float32"), Tensor([1, 128, 396901],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 128, 396901],"float32"), Tensor([1, 128, 396901],"float32"), ) 	 101606656 	 1000 	 0.32718443870544434 	 0.3316221237182617 	 0.3106250762939453 	 0.3128511905670166 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:07.548877 test begin: paddle.less_than(Tensor([1, 198451, 256],"float32"), Tensor([1, 198451, 256],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 198451, 256],"float32"), Tensor([1, 198451, 256],"float32"), ) 	 101606912 	 1000 	 0.33017945289611816 	 0.3279764652252197 	 0.319122314453125 	 0.31671738624572754 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:09.943709 test begin: paddle.less_than(Tensor([1, 99226, 256],"int64"), Tensor([1, 99226, 256],"int64"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1, 99226, 256],"int64"), Tensor([1, 99226, 256],"int64"), ) 	 50803712 	 1000 	 0.3097975254058838 	 0.3132669925689697 	 0.30014514923095703 	 0.2958359718322754 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:11.413638 test begin: paddle.less_than(Tensor([1551, 128, 256],"float32"), Tensor([1, 128, 256],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1551, 128, 256],"float32"), Tensor([1, 128, 256],"float32"), ) 	 50855936 	 1000 	 0.19119668006896973 	 0.24457597732543945 	 0.1796588897705078 	 0.2325587272644043 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:12.690945 test begin: paddle.less_than(Tensor([1551, 128, 256],"float32"), Tensor([1551, 128, 256],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([1551, 128, 256],"float32"), Tensor([1551, 128, 256],"float32"), ) 	 101646336 	 1000 	 0.3274993896484375 	 0.3280007839202881 	 0.3179941177368164 	 0.3168148994445801 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:15.161845 test begin: paddle.less_than(Tensor([3101, 1, 128, 128],"float32"), Tensor([3101, 1, 128, 128],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([3101, 1, 128, 128],"float32"), Tensor([3101, 1, 128, 128],"float32"), ) 	 101613568 	 1000 	 0.32869625091552734 	 0.32791781425476074 	 0.3192100524902344 	 0.3166496753692627 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:17.456733 test begin: paddle.less_than(Tensor([776, 128, 256],"int64"), Tensor([1, 128, 256],"int64"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([776, 128, 256],"int64"), Tensor([1, 128, 256],"int64"), ) 	 25460736 	 1000 	 0.1762998104095459 	 0.18169808387756348 	 0.1657242774963379 	 0.1697840690612793 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:18.236973 test begin: paddle.less_than(Tensor([776, 128, 256],"int64"), Tensor([776, 128, 256],"int64"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([776, 128, 256],"int64"), Tensor([776, 128, 256],"int64"), ) 	 50855936 	 1000 	 0.3099033832550049 	 0.3135809898376465 	 0.30069828033447266 	 0.3024473190307617 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:19.716110 test begin: paddle.less_than(Tensor([8, 1, 128, 128],"float32"), Tensor([8, 388, 128, 128],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([8, 1, 128, 128],"float32"), Tensor([8, 388, 128, 128],"float32"), ) 	 50987008 	 1000 	 0.19288921356201172 	 0.2615785598754883 	 0.1826341152191162 	 0.24863481521606445 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:21.020666 test begin: paddle.less_than(Tensor([8, 1, 128, 49613],"float32"), Tensor([8, 1, 128, 49613],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([8, 1, 128, 49613],"float32"), Tensor([8, 1, 128, 49613],"float32"), ) 	 101607424 	 1000 	 0.32735228538513184 	 0.33394527435302734 	 0.3178260326385498 	 0.3180990219116211 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:23.483095 test begin: paddle.less_than(Tensor([8, 1, 49613, 128],"float32"), Tensor([8, 1, 49613, 128],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([8, 1, 49613, 128],"float32"), Tensor([8, 1, 49613, 128],"float32"), ) 	 101607424 	 1000 	 0.3286135196685791 	 0.34868836402893066 	 0.3190431594848633 	 0.31772708892822266 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:25.855932 test begin: paddle.less_than(Tensor([8, 388, 128, 128],"float32"), Tensor([8, 1, 128, 128],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([8, 388, 128, 128],"float32"), Tensor([8, 1, 128, 128],"float32"), ) 	 50987008 	 1000 	 0.1908257007598877 	 0.2605781555175781 	 0.1802971363067627 	 0.24731898307800293 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:27.149836 test begin: paddle.less_than(Tensor([8, 388, 128, 128],"float32"), Tensor([8, 388, 128, 128],"float32"), )
[Prof] paddle.less_than 	 paddle.less_than(Tensor([8, 388, 128, 128],"float32"), Tensor([8, 388, 128, 128],"float32"), ) 	 101711872 	 1000 	 0.3277015686035156 	 0.32821059226989746 	 0.3182988166809082 	 0.31709766387939453 	 None 	 None 	 None 	 None 	 
2025-07-25 01:38:29.517425 test begin: paddle.lgamma(Tensor([10, 10, 10, 25402],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([10, 10, 10, 25402],"float64"), ) 	 25402000 	 1000 	 0.7159593105316162 	 0.6906030178070068 	 0.7056231498718262 	 0.6803228855133057 	 1.3836307525634766 	 1.5891053676605225 	 1.3332455158233643 	 0.8132789134979248 	 
2025-07-25 01:38:35.079306 test begin: paddle.lgamma(Tensor([10, 10, 127009, 2],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([10, 10, 127009, 2],"float64"), ) 	 25401800 	 1000 	 0.9374728202819824 	 0.7028663158416748 	 0.7042238712310791 	 0.6812028884887695 	 1.3836185932159424 	 1.5917770862579346 	 1.3324179649353027 	 0.8133022785186768 	 
2025-07-25 01:38:41.483980 test begin: paddle.lgamma(Tensor([10, 127009, 10, 2],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([10, 127009, 10, 2],"float64"), ) 	 25401800 	 1000 	 0.713019847869873 	 0.6975514888763428 	 0.7043349742889404 	 0.6785013675689697 	 1.3860838413238525 	 1.586893081665039 	 1.3257410526275635 	 0.8101480007171631 	 
2025-07-25 01:38:47.007205 test begin: paddle.lgamma(Tensor([100, 254017],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([100, 254017],"float64"), ) 	 25401700 	 1000 	 0.7128441333770752 	 0.7002320289611816 	 0.7041268348693848 	 0.6811826229095459 	 1.3848941326141357 	 1.5865535736083984 	 1.3340251445770264 	 0.8107130527496338 	 
2025-07-25 01:38:52.547641 test begin: paddle.lgamma(Tensor([127009, 10, 10, 2],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([127009, 10, 10, 2],"float64"), ) 	 25401800 	 1000 	 0.7174417972564697 	 0.6902196407318115 	 0.7066717147827148 	 0.67972731590271 	 1.3854010105133057 	 1.5883958339691162 	 1.3344025611877441 	 0.8128907680511475 	 
2025-07-25 01:38:58.116627 test begin: paddle.lgamma(Tensor([1948, 26080],"float32"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([1948, 26080],"float32"), ) 	 50803840 	 1000 	 0.8447740077972412 	 0.3948404788970947 	 0.3900151252746582 	 0.3695342540740967 	 0.9639451503753662 	 1.5118074417114258 	 0.9119727611541748 	 0.7738358974456787 	 
2025-07-25 01:39:04.753150 test begin: paddle.lgamma(Tensor([254017, 100],"float64"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([254017, 100],"float64"), ) 	 25401700 	 1000 	 0.7159402370452881 	 0.6912367343902588 	 0.7047770023345947 	 0.6797137260437012 	 1.382323980331421 	 1.5882651805877686 	 1.329348087310791 	 0.8115143775939941 	 
2025-07-25 01:39:10.387458 test begin: paddle.lgamma(Tensor([50803201, 1],"float32"), )
[Prof] paddle.lgamma 	 paddle.lgamma(Tensor([50803201, 1],"float32"), ) 	 50803201 	 1000 	 0.39896631240844727 	 0.381028413772583 	 0.39009594917297363 	 0.37082695960998535 	 0.9633417129516602 	 1.5117664337158203 	 0.9086341857910156 	 0.7724058628082275 	 
2025-07-25 01:39:15.345487 test begin: paddle.linalg.cond(x=Tensor([4, 396901, 4, 4],"float64"), p=-1, )
[Prof] paddle.linalg.cond 	 paddle.linalg.cond(x=Tensor([4, 396901, 4, 4],"float64"), p=-1, ) 	 25401664 	 1000 	 79.02226710319519 	 3.496443033218384 	 0.0012044906616210938 	 0.05357789993286133 	 None 	 None 	 None 	 None 	 
[Error] one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.DoubleTensor [4, 396901, 4, 4]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
2025-07-25 01:40:59.673609 test begin: paddle.linalg.cond(x=Tensor([4, 396901, 4, 4],"float64"), p=-math.inf, )
[Prof] paddle.linalg.cond 	 paddle.linalg.cond(x=Tensor([4, 396901, 4, 4],"float64"), p=-math.inf, ) 	 25401664 	 1000 	 79.00130271911621 	 3.490844488143921 	 0.0011546611785888672 	 0.05336117744445801 	 None 	 None 	 None 	 None 	 
[Error] one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.DoubleTensor [4, 396901, 4, 4]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
2025-07-25 01:42:43.566524 test begin: paddle.linalg.cond(x=Tensor([793801, 2, 4, 4],"float64"), p=-1, )
[Prof] paddle.linalg.cond 	 paddle.linalg.cond(x=Tensor([793801, 2, 4, 4],"float64"), p=-1, ) 	 25401632 	 1000 	 79.55369114875793 	 3.495375394821167 	 0.0011987686157226562 	 0.05357170104980469 	 None 	 None 	 None 	 None 	 
[Error] one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.DoubleTensor [793801, 2, 4, 4]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
2025-07-25 01:44:28.892789 test begin: paddle.linalg.cond(x=Tensor([793801, 2, 4, 4],"float64"), p=-math.inf, )
[Prof] paddle.linalg.cond 	 paddle.linalg.cond(x=Tensor([793801, 2, 4, 4],"float64"), p=-math.inf, ) 	 25401632 	 1000 	 81.24692368507385 	 3.485639810562134 	 0.001135110855102539 	 0.053415536880493164 	 None 	 None 	 None 	 None 	 
[Error] one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.DoubleTensor [793801, 2, 4, 4]], which is output 0 of LinalgInvExBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
2025-07-25 01:46:14.808635 test begin: paddle.linalg.corrcoef(Tensor([10160641, 5],"float32"), rowvar=False, )
[Error] CUDA out of memory. Tried to allocate 384593.85 GiB. GPU 0 has a total capacity of 39.39 GiB of which 37.11 GiB is free. Process 149008 has 2.28 GiB memory in use. Of the allocated memory 628.89 MiB is allocated by PyTorch, and 15.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-07-25 01:46:28.123282 test begin: paddle.linalg.corrcoef(Tensor([4, 12700801],"float32"), )
W0725 01:46:29.180956 63561 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.linalg.corrcoef 	 paddle.linalg.corrcoef(Tensor([4, 12700801],"float32"), ) 	 50803204 	 1000 	 2.88250732421875 	 2.351255416870117 	 0.16369271278381348 	 0.002008199691772461 	 4.528531074523926 	 3.4474971294403076 	 0.1038823127746582 	 0.06365633010864258 	 
2025-07-25 01:46:48.877707 test begin: paddle.linalg.corrcoef(Tensor([4, 6350401],"float64"), )
[Prof] paddle.linalg.corrcoef 	 paddle.linalg.corrcoef(Tensor([4, 6350401],"float64"), ) 	 25401604 	 1000 	 1.9713528156280518 	 1.2718183994293213 	 0.11187291145324707 	 0.0009698867797851562 	 4.955060005187988 	 3.3543288707733154 	 0.15339112281799316 	 0.07984399795532227 	 
2025-07-25 01:47:01.081355 test begin: paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=None, aweights=Tensor([50803201],"int32"), )
W0725 01:47:01.656654 65876 backward.cc:462] While running Node (CastGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-25 01:47:01.657222 test begin: paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int64"), aweights=Tensor([25401601],"float64"), )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int64"), aweights=Tensor([25401601],"float64"), ) 	 25401811 	 1000 	 0.9802649021148682 	 0.5454227924346924 	 2.1219253540039062e-05 	 0.00016164779663085938 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-25 01:47:03.804062 test begin: paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([25401601],"int64"), aweights=Tensor([10],"float64"), )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([25401601],"int64"), aweights=Tensor([10],"float64"), ) 	 25401811 	 1000 	 0.830256462097168 	 0.42740416526794434 	 1.6450881958007812e-05 	 6.771087646484375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-25 01:47:05.543485 test begin: paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([50803201],"int32"), aweights=None, )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 10],"float64"), rowvar=True, ddof=True, fweights=Tensor([50803201],"int32"), aweights=None, ) 	 50803401 	 1000 	 0.6111681461334229 	 0.3159329891204834 	 1.6927719116210938e-05 	 0.00010919570922851562 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-25 01:47:06.852822 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=None, aweights=Tensor([10],"int32"), )
W0725 01:47:09.965494 66275 backward.cc:462] While running Node (CastGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-25 01:47:09.986049 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=None, aweights=Tensor([1270081],"int32"), )
W0725 01:47:13.106101 66640 backward.cc:462] While running Node (CastGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-25 01:47:13.120894 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int32"), aweights=None, )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int32"), aweights=None, ) 	 25401630 	 1000 	 2.2700960636138916 	 1.6857054233551025 	 0.0016765594482421875 	 0.0009140968322753906 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-25 01:47:22.039340 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int64"), aweights=Tensor([10],"float64"), )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([10],"int64"), aweights=Tensor([10],"float64"), ) 	 25401640 	 1000 	 2.4233930110931396 	 1.7814691066741943 	 0.0016605854034423828 	 0.0009214878082275391 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-25 01:47:31.343734 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([1270081],"int32"), aweights=None, )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([1270081],"int32"), aweights=None, ) 	 26671701 	 1000 	 2.2689664363861084 	 1.6941478252410889 	 0.001638650894165039 	 0.0009105205535888672 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-25 01:47:41.344623 test begin: paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([1270081],"int64"), aweights=Tensor([1270081],"float64"), )
[Prof] paddle.linalg.cov 	 paddle.linalg.cov(Tensor([20, 1270081],"float64"), rowvar=True, ddof=True, fweights=Tensor([1270081],"int64"), aweights=Tensor([1270081],"float64"), ) 	 27941782 	 1000 	 2.519815683364868 	 1.8405659198760986 	 0.0015916824340820312 	 0.0009140968322753906 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-25 01:47:50.691910 test begin: paddle.linalg.cov(Tensor([20, 25401601],"float64"), rowvar=True, ddof=True, fweights=Tensor([25401601],"int64"), aweights=Tensor([25401601],"float64"), )
[Error] CUDA out of memory. Tried to allocate 3.79 GiB. GPU 0 has a total capacity of 39.39 GiB of which 3.26 GiB is free. Process 9065 has 36.13 GiB memory in use. Of the allocated memory 4.37 GiB is allocated by PyTorch, and 7.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-07-25 01:50:14.487061 test begin: paddle.linalg.det(Tensor([3, 677377, 5, 5],"float32"), )
W0725 01:50:16.738137 80945 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe03fd6b010>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753380015 (unix time) try "date -d @1753380015" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13af8) received by PID 80632 (TID 0x7fe03b30e640) from PID 80632 ***]

2025-07-25 02:00:21.187787 test begin: paddle.linalg.det(Tensor([677377, 3, 5, 5],"float32"), )
W0725 02:00:23.530392 130595 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1323d5f010>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753380621 (unix time) try "date -d @1753380621" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1fd13) received by PID 130323 (TID 0x7f131f30e640) from PID 130323 ***]

2025-07-25 02:10:27.917859 test begin: paddle.linalg.inv(x=Tensor([5, 317521, 4, 4],"float64"), )
W0725 02:10:29.319972 13148 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.linalg.inv 	 paddle.linalg.inv(x=Tensor([5, 317521, 4, 4],"float64"), ) 	 25401680 	 1000 	 78.9054388999939 	 2.8696014881134033 	 0.000141143798828125 	 9.608268737792969e-05 	 19.84413480758667 	 19.567911624908447 	 0.4050734043121338 	 0.3924069404602051 	 
2025-07-25 02:12:32.017995 test begin: paddle.linalg.inv(x=Tensor([529201, 3, 4, 4],"float64"), )
[Prof] paddle.linalg.inv 	 paddle.linalg.inv(x=Tensor([529201, 3, 4, 4],"float64"), ) 	 25401648 	 1000 	 81.40125274658203 	 2.8655524253845215 	 0.00011491775512695312 	 8.225440979003906e-05 	 20.78391718864441 	 19.592927932739258 	 0.40491604804992676 	 0.3917272090911865 	 
2025-07-25 02:14:39.122904 test begin: paddle.linalg.lu(Tensor([203213, 5, 5, 5],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9398d6af20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 02:24:46.332766 test begin: paddle.linalg.lu(Tensor([3, 338689, 5, 5],"float64"), )
W0725 02:24:47.102186 78245 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff183e6f010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 02:34:53.198883 test begin: paddle.linalg.lu(Tensor([3, 5, 338689, 5],"float64"), )
W0725 02:34:54.013453 124254 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0725 02:35:08.718478 124254 backward.cc:466] While running Node (LuGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   LuGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::lu_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, bool, paddle::Tensor*)
4   void phi::LUGradKernel<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
5   double* phi::DeviceContext::Alloc<double>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 12.519403TB memory on GPU 0, 1.797852GB memory has been allocated and available memory is only 37.596008GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-25 02:35:08.731899 test begin: paddle.linalg.lu(Tensor([3, 5, 5, 338689],"float64"), )
=========================================================================================
   WARNING batched routines are designed for small sizes. It might be better to use the
   Native/Hybrid classical routines if you want good performance.
=========================================================================================
/usr/local/lib/python3.10/dist-packages/torch/functional.py:2162: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
LU, pivots = torch.lu(A, compute_pivots)
should be replaced with
LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
and
LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
should be replaced with
LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2055.)
  return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))
[Prof] paddle.linalg.lu 	 paddle.linalg.lu(Tensor([3, 5, 5, 338689],"float64"), ) 	 25401675 	 1000 	 4.551833152770996 	 2.8180620670318604 	 0.0005385875701904297 	 0.00021004676818847656 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-25 02:35:29.725682 test begin: paddle.linalg.lu(Tensor([3, 5, 5, 677377],"float32"), )
W0725 02:35:39.745698 127151 backward.cc:462] While running Node (LuGradNode) raises an EnforceNotMet exception
[Error] (External) CUBLAS error(15). 
  [Hint: 'CUBLAS_STATUS_NOT_SUPPORTED'.  The functionality requested is not supported ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:212)

2025-07-25 02:35:39.829319 test begin: paddle.linalg.lu(Tensor([3, 5, 5, 677377],"float32"), pivot=True, get_infos=True, )
W0725 02:35:48.419968 127748 backward.cc:462] While running Node (LuGradNode) raises an EnforceNotMet exception
[Error] (External) CUBLAS error(15). 
  [Hint: 'CUBLAS_STATUS_NOT_SUPPORTED'.  The functionality requested is not supported ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:212)

2025-07-25 02:35:48.451224 test begin: paddle.linalg.lu(Tensor([3, 5, 677377, 5],"float32"), )
W0725 02:36:04.478379 128315 backward.cc:466] While running Node (LuGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   LuGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::lu_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, bool, paddle::Tensor*)
4   void phi::LUGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 25.038731TB memory on GPU 0, 1.897400GB memory has been allocated and available memory is only 37.496460GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-25 02:36:04.592842 test begin: paddle.linalg.lu(Tensor([3, 5, 677377, 5],"float32"), pivot=True, get_infos=True, )
W0725 02:36:20.487351 129619 backward.cc:466] While running Node (LuGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[Error] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   LuGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::lu_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, bool, paddle::Tensor*)
4   void phi::LUGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 25.038731TB memory on GPU 0, 2.465820GB memory has been allocated and available memory is only 36.928040GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

2025-07-25 02:36:20.527952 test begin: paddle.linalg.lu(Tensor([3, 677377, 5, 5],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f083abd5990>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 02:46:27.565969 test begin: paddle.linalg.lu(Tensor([3, 677377, 5, 5],"float32"), pivot=True, get_infos=True, )
W0725 02:46:29.486549 13456 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f46eb396ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 02:56:34.418536 test begin: paddle.linalg.lu(Tensor([406426, 5, 5, 5],"float32"), )
W0725 02:56:39.618664 60133 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4259387010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 03:06:42.607646 test begin: paddle.linalg.lu_unpack(Tensor([2032129, 5, 5, 5],"float32"), Tensor([2032129, 5, 5],"int32"), )
W0725 03:06:48.646788 106009 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0419d77130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 03:16:48.893506 test begin: paddle.linalg.lu_unpack(Tensor([2032129, 5, 5, 5],"float64"), Tensor([2032129, 5, 5],"int32"), )
W0725 03:16:56.134989 151889 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f39d26770d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 03:26:54.393426 test begin: paddle.linalg.lu_unpack(Tensor([203213, 5, 5, 5],"float64"), Tensor([203213, 5, 5],"int32"), )
W0725 03:26:55.451011 35064 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f333ffef0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 03:36:59.230815 test begin: paddle.linalg.lu_unpack(Tensor([3, 5, 5, 338689],"float64"), Tensor([3, 5, 5],"int32"), )
W0725 03:36:59.866799 80518 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.linalg.lu_unpack 	 paddle.linalg.lu_unpack(Tensor([3, 5, 5, 338689],"float64"), Tensor([3, 5, 5],"int32"), ) 	 25401750 	 1000 	 0.9277505874633789 	 0.36327695846557617 	 4.863739013671875e-05 	 0.04640913009643555 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-25 03:37:03.878497 test begin: paddle.linalg.lu_unpack(Tensor([3, 5, 5, 677377],"float32"), Tensor([3, 5, 5],"int32"), )
[Prof] paddle.linalg.lu_unpack 	 paddle.linalg.lu_unpack(Tensor([3, 5, 5, 677377],"float32"), Tensor([3, 5, 5],"int32"), ) 	 50803350 	 1000 	 1.2493741512298584 	 0.4118466377258301 	 5.626678466796875e-05 	 0.05187797546386719 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-25 03:37:08.821973 test begin: paddle.linalg.lu_unpack(Tensor([406426, 5, 5, 5],"float32"), Tensor([406426, 5, 5],"int32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe7dda428c0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 03:47:13.882985 test begin: paddle.linalg.lu_unpack(Tensor([677377, 5, 5, 3],"float32"), Tensor([677377, 5, 3],"int32"), )
W0725 03:47:15.206115 126967 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5d2c21f0d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 03:57:18.557126 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 4, 1058401],"float64"), p=-2, axis=list[1,2,], keepdim=False, )
W0725 03:57:19.492362  9082 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7b00276d10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 04:07:25.271272 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 4, 1058401],"float64"), p=-2, axis=list[1,2,], keepdim=True, )
W0725 04:07:26.191618 56051 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8047ef6cb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 04:17:34.134895 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 4233601],"float64"), p="fro", axis=list[0,1,], keepdim=False, )
W0725 04:17:34.812336 101910 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.linalg.matrix_norm 	 paddle.linalg.matrix_norm(x=Tensor([2, 3, 4233601],"float64"), p="fro", axis=list[0,1,], keepdim=False, ) 	 25401606 	 1000 	 0.2339799404144287 	 0.1920638084411621 	 0.1193850040435791 	 0.156996488571167 	 1.296163558959961 	 1.2562994956970215 	 0.4416239261627197 	 0.3207263946533203 	 
2025-07-25 04:17:41.776695 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 846721, 5],"float64"), p=-2, axis=list[1,2,], keepdim=False, )
[Prof] paddle.linalg.matrix_norm 	 paddle.linalg.matrix_norm(x=Tensor([2, 3, 846721, 5],"float64"), p=-2, axis=list[1,2,], keepdim=False, ) 	 25401630 	 1000 	 17.423081159591675 	 17.71016025543213 	 0.00011968612670898438 	 0.00024819374084472656 	 1.304128885269165 	 1.7029199600219727 	 0.0703592300415039 	 0.2495110034942627 	 
2025-07-25 04:18:20.968397 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3, 846721, 5],"float64"), p=-2, axis=list[1,2,], keepdim=True, )
[Prof] paddle.linalg.matrix_norm 	 paddle.linalg.matrix_norm(x=Tensor([2, 3, 846721, 5],"float64"), p=-2, axis=list[1,2,], keepdim=True, ) 	 25401630 	 1000 	 17.272393703460693 	 17.608119249343872 	 7.987022399902344e-05 	 0.00025963783264160156 	 1.30772066116333 	 1.7040879726409912 	 0.06720161437988281 	 0.24816632270812988 	 
2025-07-25 04:18:59.965443 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 3175201, 4],"float64"), p="fro", axis=list[0,1,], keepdim=False, )
[Prof] paddle.linalg.matrix_norm 	 paddle.linalg.matrix_norm(x=Tensor([2, 3175201, 4],"float64"), p="fro", axis=list[0,1,], keepdim=False, ) 	 25401608 	 1000 	 5.240311861038208 	 0.15845274925231934 	 1.7855658531188965 	 0.08096957206726074 	 1.0809476375579834 	 0.909886360168457 	 0.36826539039611816 	 0.23314499855041504 	 
2025-07-25 04:19:08.298688 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 635041, 4, 5],"float64"), p=-2, axis=list[1,2,], keepdim=False, )
[Prof] paddle.linalg.matrix_norm 	 paddle.linalg.matrix_norm(x=Tensor([2, 635041, 4, 5],"float64"), p=-2, axis=list[1,2,], keepdim=False, ) 	 25401640 	 1000 	 17.944528102874756 	 16.483999013900757 	 0.0002448558807373047 	 0.0002510547637939453 	 2.5688319206237793 	 1.7661805152893066 	 0.13850665092468262 	 0.2569756507873535 	 
2025-07-25 04:19:47.675401 test begin: paddle.linalg.matrix_norm(x=Tensor([2, 635041, 4, 5],"float64"), p=-2, axis=list[1,2,], keepdim=True, )
[Prof] paddle.linalg.matrix_norm 	 paddle.linalg.matrix_norm(x=Tensor([2, 635041, 4, 5],"float64"), p=-2, axis=list[1,2,], keepdim=True, ) 	 25401640 	 1000 	 17.7690110206604 	 16.35306143760681 	 0.000244140625 	 0.00023889541625976562 	 2.569276809692383 	 1.7620453834533691 	 0.13149595260620117 	 0.25701284408569336 	 
2025-07-25 04:20:26.761950 test begin: paddle.linalg.matrix_norm(x=Tensor([2116801, 3, 4],"float64"), p="fro", axis=list[0,1,], keepdim=False, )
[Prof] paddle.linalg.matrix_norm 	 paddle.linalg.matrix_norm(x=Tensor([2116801, 3, 4],"float64"), p="fro", axis=list[0,1,], keepdim=False, ) 	 25401612 	 1000 	 5.239672660827637 	 0.15859675407409668 	 1.7857844829559326 	 0.0809779167175293 	 1.08372163772583 	 0.908942461013794 	 0.36814141273498535 	 0.23200178146362305 	 
2025-07-25 04:20:34.720217 test begin: paddle.linalg.matrix_norm(x=Tensor([423361, 3, 4, 5],"float64"), p=-2, axis=list[1,2,], keepdim=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f99ab036a40>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(4), driver shutting down. 
  [Hint: 'cudaErrorCudartUnloading'. This indicates that a CUDA Runtime API call cannot be executed because it is being called during process shut down, at a pointin time after CUDA driver has been unloaded.] (at ../paddle/phi/backends/gpu/cuda/cuda_info.cc:266)

2025-07-25 04:30:45.920442 test begin: paddle.linalg.matrix_norm(x=Tensor([423361, 3, 4, 5],"float64"), p=-2, axis=list[1,2,], keepdim=True, )
W0725 04:30:46.588021 161642 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdb86502ad0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 04:40:52.857492 test begin: paddle.linalg.matrix_power(x=Tensor([2068, 2, 3, 2, 1, 32, 32],"float64"), n=-10, )
W0725 04:40:53.538769 44866 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([2068, 2, 3, 2, 1, 32, 32],"float64"), n=-10, ) 	 25411584 	 1000 	 7.194316387176514 	 6.29427433013916 	 0.0032126903533935547 	 0.0015239715576171875 	 20.559903144836426 	 6.76706337928772 	 0.016400575637817383 	 0.460376501083374 	 
2025-07-25 04:41:39.506734 test begin: paddle.linalg.matrix_power(x=Tensor([2068, 2, 3, 2, 1, 32, 32],"float64"), n=-2, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([2068, 2, 3, 2, 1, 32, 32],"float64"), n=-2, ) 	 25411584 	 1000 	 4.736991882324219 	 5.033277273178101 	 0.0003178119659423828 	 0.0003597736358642578 	 6.180427312850952 	 2.617849349975586 	 0.002213001251220703 	 0.4453775882720947 	 
2025-07-25 04:41:59.340220 test begin: paddle.linalg.matrix_power(x=Tensor([3, 1379, 3, 2, 1, 32, 32],"float64"), n=-10, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 1379, 3, 2, 1, 32, 32],"float64"), n=-10, ) 	 25417728 	 1000 	 7.190089464187622 	 6.27793550491333 	 0.0032160282135009766 	 0.0015304088592529297 	 20.56244397163391 	 6.763704538345337 	 0.016402244567871094 	 0.46195387840270996 	 
2025-07-25 04:42:41.901639 test begin: paddle.linalg.matrix_power(x=Tensor([3, 1379, 3, 2, 1, 32, 32],"float64"), n=-2, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 1379, 3, 2, 1, 32, 32],"float64"), n=-2, ) 	 25417728 	 1000 	 4.717550992965698 	 5.026426315307617 	 0.00031280517578125 	 0.0003635883331298828 	 6.171891450881958 	 2.6216208934783936 	 0.002197742462158203 	 0.44553184509277344 	 
2025-07-25 04:43:01.573410 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 2005, 6, 1, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 2005, 6, 1, 11, 4, 4],"float64"), n=3, ) 	 25407360 	 1000 	 17.47282648086548 	 17.139684200286865 	 0.3438725471496582 	 0.3505899906158447 	 46.58973693847656 	 42.6829354763031 	 0.36464715003967285 	 0.4262988567352295 	 
2025-07-25 04:45:06.700384 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 1719, 1, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 1719, 1, 11, 4, 4],"float64"), n=3, ) 	 25413696 	 1000 	 17.483615159988403 	 17.124454498291016 	 0.344118595123291 	 0.3504445552825928 	 46.521647930145264 	 42.791879415512085 	 0.3633739948272705 	 0.42786526679992676 	 
2025-07-25 04:47:11.897658 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 6, 1, 3151, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 6, 1, 3151, 4, 4],"float64"), n=3, ) 	 25409664 	 1000 	 17.487563848495483 	 17.12867021560669 	 0.34522342681884766 	 0.3503432273864746 	 46.63493251800537 	 42.76707983016968 	 0.3633272647857666 	 0.4263639450073242 	 
2025-07-25 04:49:17.219900 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 6, 287, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2, 7, 6, 287, 11, 4, 4],"float64"), n=3, ) 	 25458048 	 1000 	 17.528330087661743 	 17.15979027748108 	 0.3445866107940674 	 0.35251927375793457 	 46.67912244796753 	 42.79464769363403 	 0.3640472888946533 	 0.42702579498291016 	 
2025-07-25 04:51:23.837621 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2068, 2, 1, 32, 32],"float64"), n=-10, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2068, 2, 1, 32, 32],"float64"), n=-10, ) 	 25411584 	 1000 	 7.191807270050049 	 6.24859881401062 	 0.003223419189453125 	 0.001539468765258789 	 20.56780433654785 	 6.7709479331970215 	 0.01638317108154297 	 0.4603896141052246 	 
2025-07-25 04:52:05.793848 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 2068, 2, 1, 32, 32],"float64"), n=-2, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 2068, 2, 1, 32, 32],"float64"), n=-2, ) 	 25411584 	 1000 	 4.718376398086548 	 5.037838459014893 	 0.0003230571746826172 	 0.00036454200744628906 	 6.170901298522949 	 2.619579315185547 	 0.002212047576904297 	 0.44541263580322266 	 
2025-07-25 04:52:25.891641 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 1379, 1, 32, 32],"float64"), n=-10, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 1379, 1, 32, 32],"float64"), n=-10, ) 	 25417728 	 1000 	 7.194915056228638 	 6.2566633224487305 	 0.0032172203063964844 	 0.001535177230834961 	 20.57299256324768 	 6.770120143890381 	 0.016396284103393555 	 0.4605832099914551 	 
2025-07-25 04:53:08.061771 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 1379, 1, 32, 32],"float64"), n=-2, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 1379, 1, 32, 32],"float64"), n=-2, ) 	 25417728 	 1000 	 4.72330904006958 	 5.026645660400391 	 0.0003364086151123047 	 0.00036072731018066406 	 6.20114278793335 	 2.6216094493865967 	 0.0022172927856445312 	 0.44557881355285645 	 
2025-07-25 04:53:28.209159 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 690, 32, 32],"float64"), n=-10, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 690, 32, 32],"float64"), n=-10, ) 	 25436160 	 1000 	 7.191829442977905 	 6.262771368026733 	 0.003198862075805664 	 0.0015408992767333984 	 20.61338186264038 	 6.770833253860474 	 0.01642584800720215 	 0.46210718154907227 	 
2025-07-25 04:54:11.040237 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 690, 32, 32],"float64"), n=-2, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 3, 2, 690, 32, 32],"float64"), n=-2, ) 	 25436160 	 1000 	 4.764045000076294 	 5.028735637664795 	 0.00032806396484375 	 0.0003635883331298828 	 6.207846641540527 	 2.618837356567383 	 0.0022153854370117188 	 0.44562649726867676 	 
2025-07-25 04:54:30.986997 test begin: paddle.linalg.matrix_power(x=Tensor([3, 2, 573, 7, 6, 1, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 2, 573, 7, 6, 1, 11, 4, 4],"float64"), n=3, ) 	 25413696 	 1000 	 17.480350017547607 	 17.133894681930542 	 0.34526538848876953 	 0.3509402275085449 	 46.45375943183899 	 42.880425691604614 	 0.3629577159881592 	 0.4268016815185547 	 
2025-07-25 04:56:39.263235 test begin: paddle.linalg.matrix_power(x=Tensor([3, 573, 2, 7, 6, 1, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([3, 573, 2, 7, 6, 1, 11, 4, 4],"float64"), n=3, ) 	 25413696 	 1000 	 17.469798803329468 	 17.12522006034851 	 0.3434927463531494 	 0.3504011631011963 	 46.53674578666687 	 42.735456228256226 	 0.3647148609161377 	 0.42670321464538574 	 
2025-07-25 04:58:44.541680 test begin: paddle.linalg.matrix_power(x=Tensor([860, 2, 2, 7, 6, 1, 11, 4, 4],"float64"), n=3, )
[Prof] paddle.linalg.matrix_power 	 paddle.linalg.matrix_power(x=Tensor([860, 2, 2, 7, 6, 1, 11, 4, 4],"float64"), n=3, ) 	 25428480 	 1000 	 17.503563404083252 	 17.153360843658447 	 0.34989333152770996 	 0.3508617877960205 	 46.66316199302673 	 42.73035955429077 	 0.36823511123657227 	 0.42865800857543945 	 
2025-07-25 05:00:49.860359 test begin: paddle.linalg.matrix_transpose(Tensor([2, 3, 8467201],"float32"), )
[Prof] paddle.linalg.matrix_transpose 	 paddle.linalg.matrix_transpose(Tensor([2, 3, 8467201],"float32"), ) 	 50803206 	 1000 	 0.004385471343994141 	 0.0037658214569091797 	 2.3603439331054688e-05 	 2.193450927734375e-05 	 0.04008817672729492 	 0.05138707160949707 	 3.62396240234375e-05 	 5.125999450683594e-05 	 combined
2025-07-25 05:00:51.595617 test begin: paddle.linalg.matrix_transpose(Tensor([2, 6350401, 4],"float32"), )
[Prof] paddle.linalg.matrix_transpose 	 paddle.linalg.matrix_transpose(Tensor([2, 6350401, 4],"float32"), ) 	 50803208 	 1000 	 0.00440216064453125 	 0.0037276744842529297 	 1.8358230590820312e-05 	 1.8596649169921875e-05 	 0.039449453353881836 	 0.05399441719055176 	 3.910064697265625e-05 	 8.487701416015625e-05 	 combined
2025-07-25 05:00:53.408976 test begin: paddle.linalg.matrix_transpose(Tensor([4233601, 3, 4],"float32"), )
[Prof] paddle.linalg.matrix_transpose 	 paddle.linalg.matrix_transpose(Tensor([4233601, 3, 4],"float32"), ) 	 50803212 	 1000 	 0.0045452117919921875 	 0.0037508010864257812 	 2.3126602172851562e-05 	 1.6927719116210938e-05 	 0.039494991302490234 	 0.05110955238342285 	 3.910064697265625e-05 	 4.6253204345703125e-05 	 combined
2025-07-25 05:00:55.229307 test begin: paddle.linalg.multi_dot(list[Tensor([25401601],"float64"),Tensor([25401601, 31],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([25401601],"float64"),Tensor([25401601, 31],"float64"),], ) 	 812851232 	 1000 	 6.227412223815918 	 6.221165180206299 	 3.181989908218384 	 3.17879581451416 	 12.582690954208374 	 12.592735290527344 	 0.5001044273376465 	 0.49887943267822266 	 
2025-07-25 05:01:51.969633 test begin: paddle.linalg.multi_dot(list[Tensor([4, 4],"float64"),Tensor([4, 6350401],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([4, 4],"float64"),Tensor([4, 6350401],"float64"),], ) 	 25401620 	 1000 	 0.7769267559051514 	 0.7752726078033447 	 0.1130056381225586 	 0.11314558982849121 	 2.0297696590423584 	 2.0490195751190186 	 0.229949951171875 	 0.2378249168395996 	 
2025-07-25 05:01:58.792146 test begin: paddle.linalg.multi_dot(list[Tensor([4233601, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 5],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([4233601, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 5],"float64"),], ) 	 25401656 	 1000 	 1.3728852272033691 	 1.214750051498413 	 0.15129470825195312 	 0.15328073501586914 	 3.090925455093384 	 2.0223140716552734 	 0.1511368751525879 	 0.1874094009399414 	 
2025-07-25 05:02:09.108972 test begin: paddle.linalg.multi_dot(list[Tensor([4],"float64"),Tensor([4, 6350401],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([4],"float64"),Tensor([4, 6350401],"float64"),], ) 	 25401608 	 1000 	 0.20758914947509766 	 0.1940135955810547 	 0.18288731575012207 	 0.1719367504119873 	 0.5713975429534912 	 0.5662999153137207 	 0.06481814384460449 	 0.06430888175964355 	 
2025-07-25 05:02:11.415095 test begin: paddle.linalg.multi_dot(list[Tensor([6350401, 4],"float64"),Tensor([4, 31],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([6350401, 4],"float64"),Tensor([4, 31],"float64"),], ) 	 25401728 	 1000 	 1.7362358570098877 	 1.7836084365844727 	 0.25324344635009766 	 0.2603940963745117 	 3.424415111541748 	 3.7581212520599365 	 0.38750672340393066 	 0.42555665969848633 	 
2025-07-25 05:02:26.822123 test begin: paddle.linalg.multi_dot(list[Tensor([8, 3175201],"float64"),Tensor([3175201, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 5],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 3175201],"float64"),Tensor([3175201, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 5],"float64"),], ) 	 34927243 	 1000 	 0.40361976623535156 	 0.4074099063873291 	 0.10351800918579102 	 0.10389995574951172 	 1.959284782409668 	 1.6133899688720703 	 0.11103153228759766 	 0.13711929321289062 	 
2025-07-25 05:02:32.167941 test begin: paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 6350401],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 6350401],"float64"),], ) 	 25401682 	 1000 	 1.3616442680358887 	 1.4230897426605225 	 0.15394282341003418 	 0.159515380859375 	 3.463693141937256 	 2.12992262840271 	 0.1472938060760498 	 0.16692304611206055 	 
2025-07-25 05:02:43.039369 test begin: paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 5080321],"float64"),Tensor([5080321, 5],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 5080321],"float64"),Tensor([5080321, 5],"float64"),], ) 	 40642634 	 1000 	 0.6494226455688477 	 0.6338951587677002 	 0.16668081283569336 	 0.16162729263305664 	 3.053341865539551 	 2.505291700363159 	 0.15585756301879883 	 0.18241214752197266 	 
2025-07-25 05:02:50.808264 test begin: paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 8467201],"float64"),Tensor([8467201, 5],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 3],"float64"),Tensor([3, 8467201],"float64"),Tensor([8467201, 5],"float64"),], ) 	 67737674 	 1000 	 1.0450270175933838 	 1.0297729969024658 	 0.2663755416870117 	 0.2628440856933594 	 5.038839340209961 	 4.16035008430481 	 0.18404102325439453 	 0.19424986839294434 	 
2025-07-25 05:03:03.600064 test begin: paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 4233601],"float64"),Tensor([4233601, 4],"float64"),Tensor([4, 5],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 4233601],"float64"),Tensor([4233601, 4],"float64"),Tensor([4, 5],"float64"),], ) 	 42336078 	 1000 	 0.5279896259307861 	 0.5313773155212402 	 0.1340782642364502 	 0.13515424728393555 	 2.578148603439331 	 2.138178825378418 	 0.1316540241241455 	 0.15584993362426758 	 
2025-07-25 05:03:11.365256 test begin: paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 6350401],"float64"),Tensor([6350401, 4],"float64"),Tensor([4, 5],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 6],"float64"),Tensor([6, 6350401],"float64"),Tensor([6350401, 4],"float64"),Tensor([4, 5],"float64"),], ) 	 63504078 	 1000 	 0.772688627243042 	 0.7810566425323486 	 0.1979994773864746 	 0.20053863525390625 	 3.8396003246307373 	 3.193216323852539 	 0.16390347480773926 	 0.1818246841430664 	 
2025-07-25 05:03:22.837996 test begin: paddle.linalg.multi_dot(list[Tensor([8, 8467201],"float64"),Tensor([8467201, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 5],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([8, 8467201],"float64"),Tensor([8467201, 3],"float64"),Tensor([3, 4],"float64"),Tensor([4, 5],"float64"),], ) 	 93139243 	 1000 	 1.0174610614776611 	 1.0270545482635498 	 0.2593808174133301 	 0.2619202136993408 	 5.102620363235474 	 4.2561116218566895 	 0.1856536865234375 	 0.19716429710388184 	 
2025-07-25 05:03:39.172186 test begin: paddle.linalg.multi_dot(list[Tensor([819407],"float64"),Tensor([819407, 31],"float64"),], )
[Prof] paddle.linalg.multi_dot 	 paddle.linalg.multi_dot(list[Tensor([819407],"float64"),Tensor([819407, 31],"float64"),], ) 	 26221024 	 1000 	 0.16437268257141113 	 0.16551685333251953 	 0.08395123481750488 	 0.0838778018951416 	 0.44610142707824707 	 0.3382833003997803 	 0.22788214683532715 	 0.17275428771972656 	 
2025-07-25 05:03:41.166102 test begin: paddle.linalg.norm(Tensor([12700801, 1, 4],"float32"), p=1.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([12700801, 1, 4],"float32"), p=1.0, axis=-1, ) 	 50803204 	 1000 	 0.40776801109313965 	 0.4898641109466553 	 0.3848998546600342 	 0.4695708751678467 	 1.9827821254730225 	 0.6408426761627197 	 1.908574104309082 	 0.3266332149505615 	 
2025-07-25 05:03:45.898109 test begin: paddle.linalg.norm(Tensor([25402, 50, 20],"float64"), p=2.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([25402, 50, 20],"float64"), p=2.0, axis=-1, ) 	 25402000 	 1000 	 0.32001328468322754 	 0.2681894302368164 	 0.16312789916992188 	 0.24883627891540527 	 1.461838960647583 	 0.9375295639038086 	 1.409134864807129 	 0.23901081085205078 	 
2025-07-25 05:03:49.982423 test begin: paddle.linalg.norm(Tensor([50, 25402, 20],"float64"), p=2.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([50, 25402, 20],"float64"), p=2.0, axis=-1, ) 	 25402000 	 1000 	 0.33274269104003906 	 0.26671719551086426 	 0.1631309986114502 	 0.24746417999267578 	 1.4615819454193115 	 0.9363582134246826 	 1.4087755680084229 	 0.23899149894714355 	 
2025-07-25 05:03:53.542754 test begin: paddle.linalg.norm(Tensor([50, 50, 10161],"float64"), p=2.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([50, 50, 10161],"float64"), p=2.0, axis=-1, ) 	 25402500 	 1000 	 0.1555483341217041 	 0.1515195369720459 	 0.07924985885620117 	 0.13248562812805176 	 1.4754395484924316 	 0.910423755645752 	 1.422262191772461 	 0.2326955795288086 	 
2025-07-25 05:03:56.819206 test begin: paddle.linalg.norm(Tensor([50803201],"float32"), p=2, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([50803201],"float32"), p=2, ) 	 50803201 	 1000 	 0.15293598175048828 	 0.15236568450927734 	 0.05198860168457031 	 0.07781481742858887 	 0.999434232711792 	 0.9115173816680908 	 0.9466257095336914 	 0.23403358459472656 	 
2025-07-25 05:04:00.114695 test begin: paddle.linalg.norm(Tensor([8550, 1, 5942],"float32"), p=1.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([8550, 1, 5942],"float32"), p=1.0, axis=-1, ) 	 50804100 	 1000 	 0.15028023719787598 	 0.15722155570983887 	 0.12961411476135254 	 0.1316673755645752 	 1.9185969829559326 	 0.6069900989532471 	 1.8649415969848633 	 0.30948948860168457 	 
2025-07-25 05:04:03.811568 test begin: paddle.linalg.norm(Tensor([8550, 1486, 4],"float32"), p=1.0, axis=-1, )
[Prof] paddle.linalg.norm 	 paddle.linalg.norm(Tensor([8550, 1486, 4],"float32"), p=1.0, axis=-1, ) 	 50821200 	 1000 	 0.4053957462310791 	 0.4896411895751953 	 0.3857541084289551 	 0.4700639247894287 	 1.9823544025421143 	 0.6408822536468506 	 1.8954834938049316 	 0.326718807220459 	 
2025-07-25 05:04:08.397723 test begin: paddle.linalg.pinv(Tensor([211681, 6, 5, 4],"float64"), rcond=1e-15, hermitian=False, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f829d886fe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 05:14:13.497830 test begin: paddle.linalg.pinv(Tensor([3, 423361, 5, 4],"float64"), rcond=1e-15, hermitian=False, )
W0725 05:14:14.563901 32981 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcdf72c70d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 05:24:18.426252 test begin: paddle.linalg.pinv(Tensor([424, 200, 300],"float64"), rcond=1e-15, hermitian=False, )
W0725 05:24:19.181294 78228 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1e237a2fb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 05:34:25.552180 test begin: paddle.linalg.pinv(x=Tensor([158761, 4, 40],"float64"), )
W0725 05:34:26.598295 124752 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb7fbffb010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 05:44:30.115896 test begin: paddle.linalg.qr(Tensor([10585, 3, 100, 8],"float64"), )
W0725 05:44:30.805573  6577 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0b0629b010>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753394070 (unix time) try "date -d @1753394070" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x18ba) received by PID 6330 (TID 0x7f0afd7fb640) from PID 6330 ***]

2025-07-25 05:54:36.947499 test begin: paddle.linalg.qr(Tensor([14113, 3, 100, 6],"float64"), )
W0725 05:54:39.955433 53403 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f38d8757010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 06:04:43.649988 test begin: paddle.linalg.qr(Tensor([2, 10585, 100, 12],"float64"), )
W0725 06:04:44.536725 99142 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe2f2677010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 06:14:52.336069 test begin: paddle.linalg.qr(Tensor([2, 15877, 100, 8],"float64"), )
W0725 06:14:53.061074 144964 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4d9c093010>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753395892 (unix time) try "date -d @1753395892" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23472) received by PID 144498 (TID 0x7f4d937f8640) from PID 144498 ***]

2025-07-25 06:24:59.361757 test begin: paddle.linalg.qr(Tensor([2, 21169, 100, 6],"float64"), )
W0725 06:25:00.136899 28303 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe7a1703070>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753396499 (unix time) try "date -d @1753396499" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6cc3) received by PID 27843 (TID 0x7fe7983f9640) from PID 27843 ***]

2025-07-25 06:35:05.954755 test begin: paddle.linalg.qr(Tensor([2, 3, 100, 42337],"float64"), )
W0725 06:35:06.779645 73774 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f391aa32e30>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<_object*>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753397111 (unix time) try "date -d @1753397111" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x11eeb) received by PID 73451 (TID 0x7f3916222640) from PID 73451 ***]

2025-07-25 06:45:17.765043 test begin: paddle.linalg.qr(Tensor([2, 3, 352801, 12],"float64"), )
W0725 06:45:18.597895 120509 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 3, 352801, 12],"float64"), ) 	 25401672 	 1000 	 12.984030723571777 	 11.609719038009644 	 0.0008978843688964844 	 0.14215350151062012 	 None 	 None 	 None 	 None 	 combined
2025-07-25 06:45:43.560996 test begin: paddle.linalg.qr(Tensor([2, 3, 529201, 8],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 3, 529201, 8],"float64"), ) 	 25401648 	 1000 	 12.603478193283081 	 11.353837251663208 	 0.0008347034454345703 	 0.14136171340942383 	 None 	 None 	 None 	 None 	 combined
2025-07-25 06:46:08.140574 test begin: paddle.linalg.qr(Tensor([2, 3, 705601, 6],"float64"), )
[Prof] paddle.linalg.qr 	 paddle.linalg.qr(Tensor([2, 3, 705601, 6],"float64"), ) 	 25401636 	 1000 	 13.646348237991333 	 12.21477222442627 	 0.0009024143218994141 	 0.15173912048339844 	 None 	 None 	 None 	 None 	 combined
2025-07-25 06:46:34.604721 test begin: paddle.linalg.qr(Tensor([7057, 3, 100, 12],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f76739befe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 06:56:43.267960 test begin: paddle.linalg.slogdet(Tensor([3, 677377, 5, 5],"float32"), )
W0725 06:56:45.553871  8655 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f84745bb070>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753398403 (unix time) try "date -d @1753398403" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2130) received by PID 8496 (TID 0x7f846b9f8640) from PID 8496 ***]

2025-07-25 07:06:52.235684 test begin: paddle.linalg.slogdet(Tensor([677377, 3, 5, 5],"float32"), )
W0725 07:06:54.644402 56523 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f71d4737010>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753399012 (unix time) try "date -d @1753399012" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xdab6) received by PID 55990 (TID 0x7f71cb9f8640) from PID 55990 ***]

2025-07-25 07:16:58.950153 test begin: paddle.linalg.solve(x=Tensor([129601, 14, 14],"float64"), y=Tensor([129601, 14, 2],"float64"), )
W0725 07:16:59.832463 110785 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.linalg.solve 	 paddle.linalg.solve(x=Tensor([129601, 14, 14],"float64"), y=Tensor([129601, 14, 2],"float64"), ) 	 29030624 	 1000 	 3.708606719970703 	 2.3628616333007812 	 0.001238107681274414 	 7.295608520507812e-05 	 5.987734794616699 	 2.772338628768921 	 0.0019299983978271484 	 0.25739359855651855 	 
2025-07-25 07:17:15.463714 test begin: paddle.linalg.solve(x=Tensor([14, 14],"float64"), y=Tensor([14, 1814401],"float64"), )
[Prof] paddle.linalg.solve 	 paddle.linalg.solve(x=Tensor([14, 14],"float64"), y=Tensor([14, 1814401],"float64"), ) 	 25401810 	 1000 	 5.02555775642395 	 3.792227268218994 	 0.003911733627319336 	 0.0002574920654296875 	 6.0257568359375 	 4.040473222732544 	 0.004523515701293945 	 0.4588913917541504 	 
2025-07-25 07:17:37.367271 test begin: paddle.linalg.solve(x=Tensor([4, 14, 14],"float64"), y=Tensor([4, 14, 453601],"float64"), )
[Prof] paddle.linalg.solve 	 paddle.linalg.solve(x=Tensor([4, 14, 14],"float64"), y=Tensor([4, 14, 453601],"float64"), ) 	 25402440 	 1000 	 4.347771406173706 	 2.8954007625579834 	 0.0031049251556396484 	 0.00011491775512695312 	 11.077796697616577 	 10.275339365005493 	 0.009570837020874023 	 0.5857384204864502 	 
2025-07-25 07:18:08.887051 test begin: paddle.linalg.solve(x=Tensor([907201, 14, 14],"float64"), y=Tensor([907201, 14, 2],"float64"), )
[Prof] paddle.linalg.solve 	 paddle.linalg.solve(x=Tensor([907201, 14, 14],"float64"), y=Tensor([907201, 14, 2],"float64"), ) 	 203213024 	 1000 	 27.929080963134766 	 15.474863529205322 	 0.008179187774658203 	 0.00023698806762695312 	 43.91775608062744 	 18.951049327850342 	 0.018673181533813477 	 0.41361451148986816 	 
2025-07-25 07:20:01.781821 test begin: paddle.linalg.svdvals(Tensor([10, 3, 846721],"float64"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff4270429e0>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753399802 (unix time) try "date -d @1753399802" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1aeba) received by PID 110266 (TID 0x7ff4225e9640) from PID 110266 ***]

2025-07-25 07:30:08.930987 test begin: paddle.linalg.svdvals(Tensor([10, 423361, 6],"float64"), )
W0725 07:30:09.823455 14840 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9bd7ca6e30>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753400409 (unix time) try "date -d @1753400409" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3897) received by PID 14487 (TID 0x7f9bd3499640) from PID 14487 ***]

2025-07-25 07:40:15.978340 test begin: paddle.linalg.svdvals(Tensor([10, 5080321],"float32"), )
W0725 07:40:16.956509 60331 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbea68e3070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 07:50:20.799011 test begin: paddle.linalg.svdvals(Tensor([1411201, 3, 6],"float64"), )
W0725 07:50:21.564963 105257 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4f1e2ff010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 08:00:25.569749 test begin: paddle.linalg.svdvals(Tensor([40, 635041],"float64"), )
W0725 08:00:28.194682 150238 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe0a5e0b070>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753402225 (unix time) try "date -d @1753402225" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x249c2) received by PID 149954 (TID 0x7fe09cdfa640) from PID 149954 ***]

2025-07-25 08:10:32.068588 test begin: paddle.linalg.svdvals(Tensor([4233601, 12],"float32"), )
W0725 08:10:33.166119 32518 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f241c35b070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 08:20:37.170491 test begin: paddle.linalg.svdvals(Tensor([635041, 40],"float64"), )
W0725 08:20:40.905681 76781 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0459d6b070>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753403437 (unix time) try "date -d @1753403437" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x12a4f) received by PID 76367 (TID 0x7f0450dfa640) from PID 76367 ***]

2025-07-25 08:30:44.206871 test begin: paddle.log(Tensor([192, 40, 6625],"float32"), )
W0725 08:30:45.283377 121370 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.log 	 paddle.log(Tensor([192, 40, 6625],"float32"), ) 	 50880000 	 1000 	 0.2967970371246338 	 0.3005077838897705 	 0.2878730297088623 	 0.28622961044311523 	 0.452667236328125 	 0.4504232406616211 	 0.3980982303619385 	 0.3808751106262207 	 
2025-07-25 08:30:48.078884 test begin: paddle.log(Tensor([307, 25, 6626],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([307, 25, 6626],"float32"), ) 	 50854550 	 1000 	 0.296306848526001 	 0.3023843765258789 	 0.2868657112121582 	 0.28841090202331543 	 0.45110440254211426 	 0.4515385627746582 	 0.39763903617858887 	 0.3855857849121094 	 
2025-07-25 08:30:51.292113 test begin: paddle.log(Tensor([64, 120, 6625],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([64, 120, 6625],"float32"), ) 	 50880000 	 1000 	 0.29677867889404297 	 0.2984182834625244 	 0.2810091972351074 	 0.2802586555480957 	 0.4511682987213135 	 0.4503953456878662 	 0.3886449337005615 	 0.3845508098602295 	 
2025-07-25 08:30:54.575238 test begin: paddle.log(Tensor([64, 120, 6626],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([64, 120, 6626],"float32"), ) 	 50887680 	 1000 	 0.2971949577331543 	 0.2983119487762451 	 0.28805017471313477 	 0.28705334663391113 	 0.45152783393859863 	 0.4504055976867676 	 0.39798521995544434 	 0.3867380619049072 	 
2025-07-25 08:30:57.782458 test begin: paddle.log(Tensor([64, 25, 31753],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([64, 25, 31753],"float32"), ) 	 50804800 	 1000 	 0.2959415912628174 	 0.3016839027404785 	 0.2872343063354492 	 0.28775811195373535 	 0.45358753204345703 	 0.44980621337890625 	 0.4002091884613037 	 0.3874366283416748 	 
2025-07-25 08:31:01.015321 test begin: paddle.log(Tensor([64, 40, 19846],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([64, 40, 19846],"float32"), ) 	 50805760 	 1000 	 0.29627037048339844 	 0.2994346618652344 	 0.28717994689941406 	 0.28655290603637695 	 0.45066022872924805 	 0.4497826099395752 	 0.39751744270324707 	 0.38332700729370117 	 
2025-07-25 08:31:04.244883 test begin: paddle.log(Tensor([64, 80, 9923],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([64, 80, 9923],"float32"), ) 	 50805760 	 1000 	 0.9648916721343994 	 0.7404437065124512 	 0.2874789237976074 	 0.2867462635040283 	 0.4506833553314209 	 0.44973254203796387 	 0.39771294593811035 	 0.38285279273986816 	 
2025-07-25 08:31:11.977755 test begin: paddle.log(Tensor([96, 80, 6625],"float32"), )
[Prof] paddle.log 	 paddle.log(Tensor([96, 80, 6625],"float32"), ) 	 50880000 	 1000 	 0.743828296661377 	 0.3092975616455078 	 0.28783440589904785 	 0.2867140769958496 	 0.4510796070098877 	 0.45047664642333984 	 0.39785099029541016 	 0.3875417709350586 	 
2025-07-25 08:31:16.168301 test begin: paddle.log10(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.log10 	 paddle.log10(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 1000 	 0.29618120193481445 	 0.2977743148803711 	 0.28754496574401855 	 0.28598833084106445 	 0.45035576820373535 	 0.747180700302124 	 0.39728736877441406 	 0.38234400749206543 	 
2025-07-25 08:31:19.773217 test begin: paddle.log10(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.log10 	 paddle.log10(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 1000 	 0.29607081413269043 	 0.2976508140563965 	 0.28055262565612793 	 0.2803027629852295 	 0.45026540756225586 	 0.746006965637207 	 0.38785243034362793 	 0.381178617477417 	 
2025-07-25 08:31:23.279968 test begin: paddle.log10(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.log10 	 paddle.log10(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 1000 	 0.30063366889953613 	 0.29776549339294434 	 0.288773775100708 	 0.28447723388671875 	 0.4503967761993408 	 0.7458953857421875 	 0.396500825881958 	 0.3811686038970947 	 
2025-07-25 08:31:26.810293 test begin: paddle.log10(x=Tensor([12700801, 2],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([12700801, 2],"float64"), ) 	 25401602 	 1000 	 0.3063037395477295 	 0.30682873725891113 	 0.29771947860717773 	 0.295957088470459 	 0.44931912422180176 	 0.7458631992340088 	 0.3775339126586914 	 0.38163185119628906 	 
2025-07-25 08:31:29.669927 test begin: paddle.log10(x=Tensor([2, 12700801],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([2, 12700801],"float64"), ) 	 25401602 	 1000 	 0.3063037395477295 	 0.306865930557251 	 0.29776525497436523 	 0.29581117630004883 	 0.44789791107177734 	 0.7448415756225586 	 0.3936803340911865 	 0.38056302070617676 	 
2025-07-25 08:31:32.516390 test begin: paddle.log10(x=Tensor([2, 3, 2, 2116801],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([2, 3, 2, 2116801],"float64"), ) 	 25401612 	 1000 	 0.30634117126464844 	 0.31222987174987793 	 0.2975900173187256 	 0.29721903800964355 	 0.44818615913391113 	 0.744776725769043 	 0.39511656761169434 	 0.38045358657836914 	 
2025-07-25 08:31:39.413282 test begin: paddle.log10(x=Tensor([2, 3, 2116801, 2],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([2, 3, 2116801, 2],"float64"), ) 	 25401612 	 1000 	 0.3107419013977051 	 0.3206794261932373 	 0.2990531921386719 	 0.29708170890808105 	 0.44943881034851074 	 0.7446575164794922 	 0.3967444896697998 	 0.38042187690734863 	 
2025-07-25 08:31:42.366139 test begin: paddle.log10(x=Tensor([2, 3175201, 2, 2],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([2, 3175201, 2, 2],"float64"), ) 	 25401608 	 1000 	 0.30680108070373535 	 0.30690574645996094 	 0.29326725006103516 	 0.2940230369567871 	 0.44823741912841797 	 0.7449440956115723 	 0.3947460651397705 	 0.38054752349853516 	 
2025-07-25 08:31:45.415125 test begin: paddle.log10(x=Tensor([2116801, 3, 2, 2],"float64"), )
[Prof] paddle.log10 	 paddle.log10(x=Tensor([2116801, 3, 2, 2],"float64"), ) 	 25401612 	 1000 	 0.31265783309936523 	 0.31069040298461914 	 0.29210686683654785 	 0.28272318840026855 	 0.4482295513153076 	 0.7448489665985107 	 0.3861050605773926 	 0.38053393363952637 	 
2025-07-25 08:31:48.351241 test begin: paddle.log1p(Tensor([10, 16935, 300],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([10, 16935, 300],"float32"), ) 	 50805000 	 1000 	 0.2982766628265381 	 0.316605806350708 	 0.28830838203430176 	 0.28814101219177246 	 0.45197176933288574 	 0.7460153102874756 	 0.3985438346862793 	 0.3785724639892578 	 
2025-07-25 08:31:51.927581 test begin: paddle.log1p(Tensor([10, 200, 25402],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([10, 200, 25402],"float32"), ) 	 50804000 	 1000 	 0.2960636615753174 	 0.30336451530456543 	 0.2871131896972656 	 0.28814005851745605 	 0.4519033432006836 	 0.7471659183502197 	 0.3984971046447754 	 0.38233113288879395 	 
2025-07-25 08:31:55.464980 test begin: paddle.log1p(Tensor([1016065, 5, 5],"float64"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([1016065, 5, 5],"float64"), ) 	 25401625 	 1000 	 0.3055419921875 	 0.3362691402435303 	 0.296680212020874 	 0.3251829147338867 	 0.4479100704193115 	 0.7448616027832031 	 0.3951268196105957 	 0.38054966926574707 	 
2025-07-25 08:31:58.629117 test begin: paddle.log1p(Tensor([108, 157920, 3],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([108, 157920, 3],"float32"), ) 	 51166080 	 1000 	 0.2989068031311035 	 0.3011934757232666 	 0.2896158695220947 	 0.29027724266052246 	 0.45365095138549805 	 0.7511470317840576 	 0.4006683826446533 	 0.38373875617980957 	 
2025-07-25 08:32:02.078293 test begin: paddle.log1p(Tensor([4, 157920, 81],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([4, 157920, 81],"float32"), ) 	 51166080 	 1000 	 0.3056468963623047 	 0.3023340702056885 	 0.28842806816101074 	 0.2916259765625 	 0.4548835754394531 	 0.7512211799621582 	 0.40162229537963867 	 0.3838317394256592 	 
2025-07-25 08:32:05.594438 test begin: paddle.log1p(Tensor([4, 4233601, 3],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([4, 4233601, 3],"float32"), ) 	 50803212 	 1000 	 0.29604125022888184 	 0.3003537654876709 	 0.28722453117370605 	 0.2881581783294678 	 0.4516258239746094 	 0.7471177577972412 	 0.3975062370300293 	 0.3823511600494385 	 
2025-07-25 08:32:09.211948 test begin: paddle.log1p(Tensor([50000, 102, 5],"float64"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([50000, 102, 5],"float64"), ) 	 25500000 	 1000 	 0.30659961700439453 	 0.3390977382659912 	 0.29789233207702637 	 0.3263857364654541 	 0.44922661781311035 	 0.7476298809051514 	 0.3965921401977539 	 0.3819451332092285 	 
2025-07-25 08:32:12.328069 test begin: paddle.log1p(Tensor([50000, 5, 102],"float64"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([50000, 5, 102],"float64"), ) 	 25500000 	 1000 	 0.7555327415466309 	 0.34862470626831055 	 0.2977874279022217 	 0.3264143466949463 	 0.4492819309234619 	 0.749030351638794 	 0.3956918716430664 	 0.3820497989654541 	 
2025-07-25 08:32:18.126478 test begin: paddle.log1p(Tensor([847, 200, 300],"float32"), )
[Prof] paddle.log1p 	 paddle.log1p(Tensor([847, 200, 300],"float32"), ) 	 50820000 	 1000 	 0.29618144035339355 	 0.2991485595703125 	 0.2804296016693115 	 0.2819056510925293 	 0.45206117630004883 	 0.7461068630218506 	 0.3897852897644043 	 0.38117504119873047 	 
2025-07-25 08:32:21.675575 test begin: paddle.log2(Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([10, 20, 254017],"float32"), ) 	 50803400 	 1000 	 0.29871582984924316 	 0.2977116107940674 	 0.28812098503112793 	 0.28676772117614746 	 0.45038437843322754 	 0.7470474243164062 	 0.37183403968811035 	 0.3822972774505615 	 
2025-07-25 08:32:25.199198 test begin: paddle.log2(Tensor([10, 2540161],"float64"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([10, 2540161],"float64"), ) 	 25401610 	 1000 	 0.30628108978271484 	 0.3148043155670166 	 0.2977421283721924 	 0.29686594009399414 	 0.4481480121612549 	 0.7447564601898193 	 0.3947575092315674 	 0.3805375099182129 	 
2025-07-25 08:32:28.071700 test begin: paddle.log2(Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([10, 5080321, 1],"float32"), ) 	 50803210 	 1000 	 0.2981901168823242 	 0.29773807525634766 	 0.2799994945526123 	 0.2758331298828125 	 0.45051050186157227 	 0.7458891868591309 	 0.386929988861084 	 0.38109660148620605 	 
2025-07-25 08:32:31.536312 test begin: paddle.log2(Tensor([10, 5080321],"float32"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([10, 5080321],"float32"), ) 	 50803210 	 1000 	 0.29597043991088867 	 0.3009192943572998 	 0.28695130348205566 	 0.2867612838745117 	 0.45047879219055176 	 0.7459793090820312 	 0.39705729484558105 	 0.38110804557800293 	 
2025-07-25 08:32:34.959677 test begin: paddle.log2(Tensor([2116801, 12],"float64"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([2116801, 12],"float64"), ) 	 25401612 	 1000 	 0.7613763809204102 	 0.7642650604248047 	 0.2907533645629883 	 0.28955888748168945 	 0.4481532573699951 	 0.7460403442382812 	 0.38593292236328125 	 0.38051676750183105 	 
2025-07-25 08:32:41.288910 test begin: paddle.log2(Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([2540161, 20, 1],"float32"), ) 	 50803220 	 1000 	 0.29602551460266113 	 0.2977261543273926 	 0.2803163528442383 	 0.28046536445617676 	 0.4505288600921631 	 0.7458662986755371 	 0.38732385635375977 	 0.38109469413757324 	 
2025-07-25 08:32:44.978788 test begin: paddle.log2(Tensor([4233601, 12],"float32"), )
[Prof] paddle.log2 	 paddle.log2(Tensor([4233601, 12],"float32"), ) 	 50803212 	 1000 	 0.2976498603820801 	 0.29769134521484375 	 0.28172779083251953 	 0.28676676750183105 	 0.4503817558288574 	 0.7458932399749756 	 0.39723753929138184 	 0.38109564781188965 	 
2025-07-25 08:32:48.508868 test begin: paddle.logaddexp(Tensor([10, 16935, 300],"float32"), Tensor([10, 16935, 300],"float32"), )
[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([10, 16935, 300],"float32"), Tensor([10, 16935, 300],"float32"), ) 	 101610000 	 1000 	 2.536320686340332 	 0.45073676109313965 	 0.370098352432251 	 0.4388744831085205 	 4.673422574996948 	 2.9803128242492676 	 0.5297398567199707 	 0.3805365562438965 	 
2025-07-25 08:33:02.237232 test begin: paddle.logaddexp(Tensor([10, 16935, 300],"int32"), Tensor([10, 16935, 300],"int32"), )
W0725 08:33:06.935508 131353 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int32) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():7.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-25 08:33:06.973048 test begin: paddle.logaddexp(Tensor([10, 200, 12701],"int64"), Tensor([10, 200, 12701],"int64"), )
W0725 08:33:10.486729 131742 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int64) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():9.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-25 08:33:10.519360 test begin: paddle.logaddexp(Tensor([10, 200, 25402],"float32"), Tensor([10, 200, 25402],"float32"), )
[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([10, 200, 25402],"float32"), Tensor([10, 200, 25402],"float32"), ) 	 101608000 	 1000 	 2.537501573562622 	 0.457165002822876 	 0.37015223503112793 	 0.43834519386291504 	 4.672433376312256 	 2.9828970432281494 	 0.5296440124511719 	 0.3804934024810791 	 
2025-07-25 08:33:26.579824 test begin: paddle.logaddexp(Tensor([10, 200, 25402],"int32"), Tensor([10, 200, 25402],"int32"), )
W0725 08:33:31.273759 133085 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int32) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():7.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-25 08:33:31.305194 test begin: paddle.logaddexp(Tensor([10, 8468, 300],"int64"), Tensor([10, 8468, 300],"int64"), )
W0725 08:33:34.739902 133463 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int64) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():9.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-25 08:33:34.818409 test begin: paddle.logaddexp(Tensor([424, 200, 300],"int64"), Tensor([424, 200, 300],"int64"), )
W0725 08:33:39.984771 133814 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int64) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():9.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-25 08:33:40.065710 test begin: paddle.logaddexp(Tensor([847, 200, 300],"float32"), Tensor([847, 200, 300],"float32"), )
[Prof] paddle.logaddexp 	 paddle.logaddexp(Tensor([847, 200, 300],"float32"), Tensor([847, 200, 300],"float32"), ) 	 101640000 	 1000 	 2.536606550216675 	 0.464977502822876 	 0.3701953887939453 	 0.44020771980285645 	 4.675894737243652 	 2.9825026988983154 	 0.5299994945526123 	 0.3806729316711426 	 
2025-07-25 08:33:53.443323 test begin: paddle.logaddexp(Tensor([847, 200, 300],"int32"), Tensor([847, 200, 300],"int32"), )
W0725 08:33:58.205305 134996 backward.cc:462] While running Node (AbsGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (int32) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():7.] (at ../paddle/phi/core/dense_tensor.cc:153)

2025-07-25 08:33:58.247616 test begin: paddle.logcumsumexp(Tensor([10, 10, 508033],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe55d486e30>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753404238 (unix time) try "date -d @1753404238" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d940) received by PID 121152 (TID 0x7fe5543f9640) from PID 121152 ***]

2025-07-25 08:44:05.158868 test begin: paddle.logcumsumexp(Tensor([10, 10, 508033],"float32"), axis=-1, )
W0725 08:44:06.188509  9106 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.logcumsumexp 	 paddle.logcumsumexp(Tensor([10, 10, 508033],"float32"), axis=-1, ) 	 50803300 	 1000 	 3.3897294998168945 	 2.4763128757476807 	 3.3804380893707275 	 2.4639012813568115 	 10.945478439331055 	 10.824756383895874 	 1.0202860832214355 	 0.55326247215271 	 
2025-07-25 08:44:39.553715 test begin: paddle.logcumsumexp(Tensor([10, 10, 508033],"float32"), axis=0, )
[Prof] paddle.logcumsumexp 	 paddle.logcumsumexp(Tensor([10, 10, 508033],"float32"), axis=0, ) 	 50803300 	 1000 	 23.78707718849182 	 0.35498046875 	 8.104109048843384 	 0.3435790538787842 	 192.17920994758606 	 6.560822010040283 	 13.069860219955444 	 0.335033655166626 	 
2025-07-25 08:48:24.374498 test begin: paddle.logcumsumexp(Tensor([10, 508033, 10],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2c7598ea70>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753405104 (unix time) try "date -d @1753405104" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2215) received by PID 8725 (TID 0x7f2c6cdfa640) from PID 8725 ***]

2025-07-25 08:58:33.883990 test begin: paddle.logcumsumexp(Tensor([10, 508033, 10],"float32"), axis=-1, )
W0725 08:58:35.052659 61088 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.logcumsumexp 	 paddle.logcumsumexp(Tensor([10, 508033, 10],"float32"), axis=-1, ) 	 50803300 	 1000 	 22.857878923416138 	 103.61308932304382 	 22.846956968307495 	 103.59666061401367 	 190.29114413261414 	 213.08396911621094 	 17.696558713912964 	 10.867334365844727 	 
2025-07-25 09:07:26.839603 test begin: paddle.logcumsumexp(Tensor([10, 508033, 10],"float32"), axis=0, )
[Prof] paddle.logcumsumexp 	 paddle.logcumsumexp(Tensor([10, 508033, 10],"float32"), axis=0, ) 	 50803300 	 1000 	 23.7568199634552 	 0.35589027404785156 	 8.101233959197998 	 0.3444480895996094 	 191.94964170455933 	 6.556849479675293 	 13.05367112159729 	 0.3350963592529297 	 
2025-07-25 09:11:11.833583 test begin: paddle.logcumsumexp(Tensor([508033, 10, 10],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f819f9628c0>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753406472 (unix time) try "date -d @1753406472" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xed3c) received by PID 60732 (TID 0x7f819af18640) from PID 60732 ***]

2025-07-25 09:21:19.302700 test begin: paddle.logcumsumexp(Tensor([508033, 10, 10],"float32"), axis=-1, )
W0725 09:21:20.454965 140963 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.logcumsumexp 	 paddle.logcumsumexp(Tensor([508033, 10, 10],"float32"), axis=-1, ) 	 50803300 	 1000 	 22.854307413101196 	 103.61495065689087 	 22.843980312347412 	 103.5937888622284 	 190.2918405532837 	 213.07376742362976 	 17.693856477737427 	 10.866563081741333 	 
2025-07-25 09:30:14.240801 test begin: paddle.logcumsumexp(Tensor([508033, 10, 10],"float32"), axis=0, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc14eabeb30>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753407614 (unix time) try "date -d @1753407614" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22586) received by PID 140678 (TID 0x7fc14a2a2640) from PID 140678 ***]

2025-07-25 09:40:21.720839 test begin: paddle.logical_and(Tensor([138, 369303],"bool"), Tensor([138, 369303],"bool"), )
W0725 09:40:23.290333 45179 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.logical_and 	 paddle.logical_and(Tensor([138, 369303],"bool"), Tensor([138, 369303],"bool"), ) 	 101927628 	 1000 	 0.11852359771728516 	 0.11747574806213379 	 0.10194277763366699 	 0.08857488632202148 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:23.736797 test begin: paddle.logical_and(Tensor([146, 349866],"bool"), Tensor([146, 349866],"bool"), )
[Prof] paddle.logical_and 	 paddle.logical_and(Tensor([146, 349866],"bool"), Tensor([146, 349866],"bool"), ) 	 102160872 	 1000 	 0.1182560920715332 	 0.11613607406616211 	 0.10909223556518555 	 0.10280585289001465 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:25.525272 test begin: paddle.logical_and(Tensor([49, 1036801],"bool"), Tensor([49, 1036801],"bool"), )
[Prof] paddle.logical_and 	 paddle.logical_and(Tensor([49, 1036801],"bool"), Tensor([49, 1036801],"bool"), ) 	 101606498 	 1000 	 0.11761713027954102 	 0.11614465713500977 	 0.10824251174926758 	 0.10342836380004883 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:27.199104 test begin: paddle.logical_and(Tensor([53, 958551],"bool"), Tensor([53, 958551],"bool"), )
[Prof] paddle.logical_and 	 paddle.logical_and(Tensor([53, 958551],"bool"), Tensor([53, 958551],"bool"), ) 	 101606406 	 1000 	 0.11758852005004883 	 0.11616849899291992 	 0.10852837562561035 	 0.10349082946777344 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:28.919579 test begin: paddle.logical_and(Tensor([55, 923695],"bool"), Tensor([55, 923695],"bool"), )
[Prof] paddle.logical_and 	 paddle.logical_and(Tensor([55, 923695],"bool"), Tensor([55, 923695],"bool"), ) 	 101606450 	 1000 	 0.11758065223693848 	 0.11617279052734375 	 0.10845661163330078 	 0.10350990295410156 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:30.578889 test begin: paddle.logical_not(Tensor([215040, 237],"bool"), )
[Prof] paddle.logical_not 	 paddle.logical_not(Tensor([215040, 237],"bool"), ) 	 50964480 	 1000 	 0.08298754692077637 	 0.0795602798461914 	 0.07393407821655273 	 0.0640864372253418 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:31.480912 test begin: paddle.logical_not(Tensor([220416, 231],"bool"), )
[Prof] paddle.logical_not 	 paddle.logical_not(Tensor([220416, 231],"bool"), ) 	 50916096 	 1000 	 0.08297371864318848 	 0.08066844940185547 	 0.07448220252990723 	 0.06831717491149902 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:32.342122 test begin: paddle.logical_not(Tensor([225792, 226],"bool"), )
[Prof] paddle.logical_not 	 paddle.logical_not(Tensor([225792, 226],"bool"), ) 	 51028992 	 1000 	 0.08237910270690918 	 0.07956385612487793 	 0.07384324073791504 	 0.06737852096557617 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:33.237094 test begin: paddle.logical_not(Tensor([635041, 80],"bool"), )
[Prof] paddle.logical_not 	 paddle.logical_not(Tensor([635041, 80],"bool"), ) 	 50803280 	 1000 	 0.10001540184020996 	 0.08470749855041504 	 0.0753931999206543 	 0.0677497386932373 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:34.171094 test begin: paddle.logical_or(Tensor([50803201],"bool"), Tensor([50803201],"bool"), )
[Prof] paddle.logical_or 	 paddle.logical_or(Tensor([50803201],"bool"), Tensor([50803201],"bool"), ) 	 101606402 	 1000 	 0.898195743560791 	 0.13094735145568848 	 0.1015317440032959 	 0.09660768508911133 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:39.588673 test begin: paddle.logical_or(Tensor([640, 79381],"bool"), Tensor([640, 79381],"bool"), )
[Prof] paddle.logical_or 	 paddle.logical_or(Tensor([640, 79381],"bool"), Tensor([640, 79381],"bool"), ) 	 101607680 	 1000 	 0.12147021293640137 	 0.12926054000854492 	 0.10892009735107422 	 0.10364079475402832 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:41.301245 test begin: paddle.logical_or(Tensor([79381, 640],"bool"), Tensor([79381, 640],"bool"), )
[Prof] paddle.logical_or 	 paddle.logical_or(Tensor([79381, 640],"bool"), Tensor([79381, 640],"bool"), ) 	 101607680 	 1000 	 0.11714911460876465 	 0.11508584022521973 	 0.10080671310424805 	 0.09522414207458496 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:42.914965 test begin: paddle.logical_xor(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.logical_xor 	 paddle.logical_xor(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 50803600 	 1000 	 0.19051289558410645 	 0.24895501136779785 	 0.17282819747924805 	 0.21477293968200684 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:44.177529 test begin: paddle.logical_xor(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), )
[Prof] paddle.logical_xor 	 paddle.logical_xor(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 1],"float32"), ) 	 50803600 	 1000 	 0.188493013381958 	 0.24543213844299316 	 0.17067193984985352 	 0.2194514274597168 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:45.528583 test begin: paddle.logical_xor(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), )
[Prof] paddle.logical_xor 	 paddle.logical_xor(Tensor([10, 20, 254017],"float32"), Tensor([10, 20, 254017],"float32"), ) 	 101606800 	 1000 	 0.32739782333374023 	 0.3278334140777588 	 0.31829214096069336 	 0.31506991386413574 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:47.892774 test begin: paddle.logical_xor(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), )
[Prof] paddle.logical_xor 	 paddle.logical_xor(Tensor([10, 5080321, 1],"float32"), Tensor([10, 5080321, 1],"float32"), ) 	 101606420 	 1000 	 0.32735204696655273 	 0.33409595489501953 	 0.31824827194213867 	 0.3162691593170166 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:50.549557 test begin: paddle.logical_xor(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), )
[Prof] paddle.logical_xor 	 paddle.logical_xor(Tensor([2540161, 20, 1],"float32"), Tensor([2540161, 20, 1],"float32"), ) 	 101606440 	 1000 	 0.3274831771850586 	 0.3279099464416504 	 0.3182640075683594 	 0.3147623538970947 	 None 	 None 	 None 	 None 	 
2025-07-25 09:40:52.892758 test begin: paddle.logit(Tensor([10, 20, 254017],"float32"), 0.001, )
[Prof] paddle.logit 	 paddle.logit(Tensor([10, 20, 254017],"float32"), 0.001, ) 	 50803400 	 1000 	 0.3006579875946045 	 0.3122708797454834 	 0.288954496383667 	 0.28891468048095703 	 0.45195770263671875 	 0.45015549659729004 	 0.39686107635498047 	 0.3756840229034424 	 
2025-07-25 09:40:56.377136 test begin: paddle.logit(Tensor([10, 5080321, 1],"float32"), 0.001, )
[Prof] paddle.logit 	 paddle.logit(Tensor([10, 5080321, 1],"float32"), 0.001, ) 	 50803210 	 1000 	 0.2985379695892334 	 0.29997849464416504 	 0.28026843070983887 	 0.2876598834991455 	 0.4506032466888428 	 0.45140814781188965 	 0.39398741722106934 	 0.3892807960510254 	 
2025-07-25 09:40:59.793637 test begin: paddle.logit(Tensor([2540161, 20, 1],"float32"), 0.001, )
[Prof] paddle.logit 	 paddle.logit(Tensor([2540161, 20, 1],"float32"), 0.001, ) 	 50803220 	 1000 	 0.29854702949523926 	 0.29995203018188477 	 0.28963518142700195 	 0.28771328926086426 	 0.4505736827850342 	 0.4500160217285156 	 0.39768052101135254 	 0.3884563446044922 	 
2025-07-25 09:41:02.970521 test begin: paddle.logit(Tensor([50803201],"float32"), 1e-08, )
[Prof] paddle.logit 	 paddle.logit(Tensor([50803201],"float32"), 1e-08, ) 	 50803201 	 1000 	 0.2985060214996338 	 0.2999105453491211 	 0.28971290588378906 	 0.2876467704772949 	 0.4506404399871826 	 0.4501008987426758 	 0.3982822895050049 	 0.35799598693847656 	 
2025-07-25 09:41:07.720423 test begin: paddle.logit(x=Tensor([4, 3, 2, 1058401],"float64"), eps=0.2, )
[Prof] paddle.logit 	 paddle.logit(x=Tensor([4, 3, 2, 1058401],"float64"), eps=0.2, ) 	 25401624 	 1000 	 0.7727034091949463 	 0.30829548835754395 	 0.3165404796600342 	 0.29070544242858887 	 0.4442727565765381 	 0.44883275032043457 	 0.3922748565673828 	 0.38571667671203613 	 
2025-07-25 09:41:12.712706 test begin: paddle.logit(x=Tensor([4, 3, 423361, 5],"float64"), eps=0.2, )
[Prof] paddle.logit 	 paddle.logit(x=Tensor([4, 3, 423361, 5],"float64"), eps=0.2, ) 	 25401660 	 1000 	 0.328754186630249 	 0.32128334045410156 	 0.3081092834472656 	 0.28506016731262207 	 0.44437694549560547 	 0.4492452144622803 	 0.3819239139556885 	 0.35805368423461914 	 
2025-07-25 09:41:16.202863 test begin: paddle.logit(x=Tensor([4, 635041, 2, 5],"float64"), eps=0.2, )
[Prof] paddle.logit 	 paddle.logit(x=Tensor([4, 635041, 2, 5],"float64"), eps=0.2, ) 	 25401640 	 1000 	 0.3257122039794922 	 0.30293893814086914 	 0.3090834617614746 	 0.2842414379119873 	 0.44707822799682617 	 0.44886326789855957 	 0.38527965545654297 	 0.35252857208251953 	 
2025-07-25 09:41:18.796939 test begin: paddle.logit(x=Tensor([846721, 3, 2, 5],"float64"), eps=0.2, )
[Prof] paddle.logit 	 paddle.logit(x=Tensor([846721, 3, 2, 5],"float64"), eps=0.2, ) 	 25401630 	 1000 	 0.32565808296203613 	 0.30301356315612793 	 0.30904150009155273 	 0.2837536334991455 	 0.4443230628967285 	 0.44890594482421875 	 0.38103461265563965 	 0.36162304878234863 	 
2025-07-25 09:41:21.272386 test begin: paddle.logsumexp(Tensor([1024, 49613],"float32"), axis=1, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([1024, 49613],"float32"), axis=1, ) 	 50803712 	 1000 	 0.7148170471191406 	 0.9368960857391357 	 0.10389208793640137 	 0.10648059844970703 	 0.8091814517974854 	 0.9058847427368164 	 0.7472357749938965 	 0.30855774879455566 	 
2025-07-25 09:41:26.051335 test begin: paddle.logsumexp(Tensor([132301, 384],"float32"), axis=1, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([132301, 384],"float32"), axis=1, ) 	 50803584 	 1000 	 0.4896252155303955 	 0.9313724040985107 	 0.24941730499267578 	 0.10586214065551758 	 0.8216149806976318 	 0.910297155380249 	 0.7586343288421631 	 0.3101084232330322 	 
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 9, in <module>
    from tester import (APIConfig, APITestAccuracy, APITestCINNVSDygraph,
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/__init__.py", line 74, in __getattr__
    from .api_config import APIConfig
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/__init__.py", line 22, in __getattr__
    from .config_analyzer import APIConfig
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py", line 8, in <module>
    import paddle
  File "/usr/local/lib/python3.10/dist-packages/paddle/__init__.py", line 38, in <module>
    from .base import core  # noqa: F401
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/__init__.py", line 205, in <module>
    __bootstrap__()
  File "/usr/local/lib/python3.10/dist-packages/paddle/base/__init__.py", line 197, in __bootstrap__
    core.init_devices()
KeyboardInterrupt
2025-07-24 15:40:42.609330 test begin: paddle.logsumexp(Tensor([30, 200, 8468],"float32"), axis=-1, keepdim=False, )
W0724 15:40:43.662106 108264 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([30, 200, 8468],"float32"), axis=-1, keepdim=False, ) 	 50808000 	 1000 	 0.6340806484222412 	 0.9382107257843018 	 0.1293327808380127 	 0.10674142837524414 	 1.2061891555786133 	 0.9097363948822021 	 1.1403746604919434 	 0.3098130226135254 	 
2025-07-24 15:40:48.451705 test begin: paddle.logsumexp(Tensor([30, 200, 8468],"float32"), axis=list[0,2,], keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([30, 200, 8468],"float32"), axis=list[0,2,], keepdim=False, ) 	 50808000 	 1000 	 0.7307300567626953 	 0.9858670234680176 	 0.10671663284301758 	 0.09154605865478516 	 1.2015039920806885 	 0.9127204418182373 	 1.1470139026641846 	 0.31082582473754883 	 
2025-07-24 15:40:53.147563 test begin: paddle.logsumexp(Tensor([30, 42337, 40],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([30, 42337, 40],"float32"), axis=-1, keepdim=False, ) 	 50804400 	 1000 	 0.6652717590332031 	 1.626330852508545 	 0.33915138244628906 	 0.18485045433044434 	 1.2307498455047607 	 0.918842077255249 	 1.177379846572876 	 0.31294989585876465 	 
2025-07-24 15:40:58.551346 test begin: paddle.logsumexp(Tensor([30, 42337, 40],"float32"), axis=list[0,2,], keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([30, 42337, 40],"float32"), axis=list[0,2,], keepdim=False, ) 	 50804400 	 1000 	 0.7204806804656982 	 0.94907546043396 	 0.14694714546203613 	 0.1080172061920166 	 1.2103862762451172 	 0.9132139682769775 	 1.154054880142212 	 0.31101226806640625 	 
2025-07-24 15:41:03.213066 test begin: paddle.logsumexp(Tensor([6351, 200, 40],"float32"), axis=-1, keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([6351, 200, 40],"float32"), axis=-1, keepdim=False, ) 	 50808000 	 1000 	 0.6630926132202148 	 1.6269543170928955 	 0.33887720108032227 	 0.18496441841125488 	 1.230776309967041 	 0.9189598560333252 	 1.169994592666626 	 0.31298065185546875 	 
2025-07-24 15:41:11.159082 test begin: paddle.logsumexp(Tensor([6351, 200, 40],"float32"), axis=list[0,2,], keepdim=False, )
[Prof] paddle.logsumexp 	 paddle.logsumexp(Tensor([6351, 200, 40],"float32"), axis=list[0,2,], keepdim=False, ) 	 50808000 	 1000 	 0.7362713813781738 	 0.9887151718139648 	 0.10750722885131836 	 0.09193897247314453 	 1.202150821685791 	 0.9125030040740967 	 1.1482973098754883 	 0.3108518123626709 	 
2025-07-24 15:41:17.390351 test begin: paddle.masked_fill(Tensor([120],"float32"), Tensor([423361, 120],"bool"), Tensor([1],"float32"), )
[Error] (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/backends/gpu/gpu_context.cc:580)

2025-07-24 15:41:25.163114 test begin: paddle.masked_fill(Tensor([169345],"float32"), Tensor([300, 169345],"bool"), Tensor([1],"float32"), )
W0724 15:41:26.128068 109329 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Error] (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/backends/gpu/gpu_context.cc:580)

2025-07-24 15:41:32.613141 test begin: paddle.masked_fill(Tensor([20, 127009, 20],"int32"), Tensor([20, 127009, 20],"bool"), 0, )
W0724 15:41:34.267715 109483 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([20, 127009, 20],"int32"), Tensor([20, 127009, 20],"bool"), 0, ) 	 101607200 	 1000 	 0.3836188316345215 	 0.6595675945281982 	 0.09612822532653809 	 0.2218160629272461 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:41:41.357880 test begin: paddle.masked_fill(Tensor([20, 60, 42337],"int32"), Tensor([20, 60, 42337],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([20, 60, 42337],"int32"), Tensor([20, 60, 42337],"bool"), 0, ) 	 101608800 	 1000 	 0.3765292167663574 	 0.6511924266815186 	 0.09623193740844727 	 0.221663236618042 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:41:44.585636 test begin: paddle.masked_fill(Tensor([28225, 60, 30],"int32"), Tensor([28225, 60, 30],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([28225, 60, 30],"int32"), Tensor([28225, 60, 30],"bool"), 0, ) 	 101610000 	 1000 	 0.37886571884155273 	 0.6508767604827881 	 0.09635710716247559 	 0.2215898036956787 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:41:47.841821 test begin: paddle.masked_fill(Tensor([30, 56449, 30],"int32"), Tensor([30, 56449, 30],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([30, 56449, 30],"int32"), Tensor([30, 56449, 30],"bool"), 0, ) 	 101608200 	 1000 	 0.37665438652038574 	 0.6509122848510742 	 0.09625411033630371 	 0.22160792350769043 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:41:51.042921 test begin: paddle.masked_fill(Tensor([30, 60, 28225],"int32"), Tensor([30, 60, 28225],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([30, 60, 28225],"int32"), Tensor([30, 60, 28225],"bool"), 0, ) 	 101610000 	 1000 	 0.3767726421356201 	 0.6508760452270508 	 0.09626197814941406 	 0.22164058685302734 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:41:54.355020 test begin: paddle.masked_fill(Tensor([42337, 60, 20],"int32"), Tensor([42337, 60, 20],"bool"), 0, )
[Prof] paddle.masked_fill 	 paddle.masked_fill(Tensor([42337, 60, 20],"int32"), Tensor([42337, 60, 20],"bool"), 0, ) 	 101608800 	 1000 	 0.37669968605041504 	 0.6514754295349121 	 0.09629631042480469 	 0.22177457809448242 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:41:57.551392 test begin: paddle.masked_scatter(Tensor([120],"float32"), Tensor([300, 120],"bool"), Tensor([169345, 300],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([120],"float32"), Tensor([300, 120],"bool"), Tensor([169345, 300],"float32"), ) 	 50839620 	 1000 	 0.45142173767089844 	 0.03435397148132324 	 2.1457672119140625e-05 	 4.76837158203125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:41:59.247778 test begin: paddle.masked_scatter(Tensor([120],"float32"), Tensor([300, 120],"bool"), Tensor([300, 169345],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([120],"float32"), Tensor([300, 120],"bool"), Tensor([300, 169345],"float32"), ) 	 50839620 	 1000 	 0.46654534339904785 	 0.034996986389160156 	 1.9311904907226562e-05 	 7.176399230957031e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:42:00.801209 test begin: paddle.masked_scatter(Tensor([300, 40],"float32"), Tensor([40],"bool"), Tensor([169345, 300],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([300, 40],"float32"), Tensor([40],"bool"), Tensor([169345, 300],"float32"), ) 	 50815540 	 1000 	 0.43016719818115234 	 0.04341316223144531 	 1.9788742065429688e-05 	 6.651878356933594e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:42:02.266528 test begin: paddle.masked_scatter(Tensor([300, 40],"float32"), Tensor([40],"bool"), Tensor([300, 169345],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([300, 40],"float32"), Tensor([40],"bool"), Tensor([300, 169345],"float32"), ) 	 50815540 	 1000 	 0.4344198703765869 	 0.04344367980957031 	 2.0265579223632812e-05 	 6.794929504394531e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:42:03.758034 test begin: paddle.masked_scatter(Tensor([6, 8, 9, 18],"float32"), Tensor([6, 8, 9, 18],"bool"), Tensor([169345, 300],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([6, 8, 9, 18],"float32"), Tensor([6, 8, 9, 18],"bool"), Tensor([169345, 300],"float32"), ) 	 50819052 	 1000 	 0.43056631088256836 	 0.03369283676147461 	 2.5272369384765625e-05 	 4.124641418457031e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:42:05.236127 test begin: paddle.masked_scatter(Tensor([6, 8, 9, 18],"float32"), Tensor([6, 8, 9, 18],"bool"), Tensor([300, 169345],"float32"), )
[Prof] paddle.masked_scatter 	 paddle.masked_scatter(Tensor([6, 8, 9, 18],"float32"), Tensor([6, 8, 9, 18],"bool"), Tensor([300, 169345],"float32"), ) 	 50819052 	 1000 	 0.43381619453430176 	 0.03328108787536621 	 3.7670135498046875e-05 	 6.365776062011719e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:42:06.742688 test begin: paddle.masked_select(Tensor([16, 10164, 313],"float32"), Tensor([16, 10164, 313],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([16, 10164, 313],"float32"), Tensor([16, 10164, 313],"bool"), ) 	 101802624 	 1000 	 1.3695330619812012 	 3.137014627456665 	 0.0008556842803955078 	 0.003027200698852539 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:42:16.633278 test begin: paddle.masked_select(Tensor([16, 11109, 286],"float32"), Tensor([16, 11109, 286],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([16, 11109, 286],"float32"), Tensor([16, 11109, 286],"bool"), ) 	 101669568 	 1000 	 1.3749032020568848 	 3.120607376098633 	 0.0008625984191894531 	 0.003000497817993164 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:42:25.001268 test begin: paddle.masked_select(Tensor([16, 12096, 263],"float32"), Tensor([16, 12096, 263],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([16, 12096, 263],"float32"), Tensor([16, 12096, 263],"bool"), ) 	 101799936 	 1000 	 1.3780217170715332 	 3.1202032566070557 	 0.0008602142333984375 	 0.003021717071533203 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:42:33.293189 test begin: paddle.masked_select(Tensor([16, 46695, 68],"float32"), Tensor([16, 46695, 68],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([16, 46695, 68],"float32"), Tensor([16, 46695, 68],"bool"), ) 	 101608320 	 1000 	 1.3641228675842285 	 3.121617078781128 	 0.0008513927459716797 	 0.003016948699951172 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:42:42.859568 test begin: paddle.masked_select(Tensor([62, 12096, 68],"float32"), Tensor([62, 12096, 68],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([62, 12096, 68],"float32"), Tensor([62, 12096, 68],"bool"), ) 	 101993472 	 1000 	 1.3718304634094238 	 3.131288766860962 	 0.0008552074432373047 	 0.0030128955841064453 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:42:51.150108 test begin: paddle.masked_select(Tensor([68, 11109, 68],"float32"), Tensor([68, 11109, 68],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([68, 11109, 68],"float32"), Tensor([68, 11109, 68],"bool"), ) 	 102736032 	 1000 	 1.3818752765655518 	 3.1763994693756104 	 0.0008625984191894531 	 0.003056764602661133 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:42:59.612325 test begin: paddle.masked_select(Tensor([74, 10164, 68],"float32"), Tensor([74, 10164, 68],"bool"), )
[Prof] paddle.masked_select 	 paddle.masked_select(Tensor([74, 10164, 68],"float32"), Tensor([74, 10164, 68],"bool"), ) 	 102290496 	 1000 	 1.3825056552886963 	 3.134770393371582 	 0.0008580684661865234 	 0.003034353256225586 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:43:08.035603 test begin: paddle.matmul(Tensor([1, 32, 388, 4096],"float32"), Tensor([1, 32, 4096, 128],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([1, 32, 388, 4096],"float32"), Tensor([1, 32, 4096, 128],"float32"), ) 	 67633152 	 1000 	 1.6391513347625732 	 1.6391527652740479 	 1.6263916492462158 	 1.6167795658111572 	 1.8262367248535156 	 1.8263778686523438 	 0.9331362247467041 	 0.9331824779510498 	 
2025-07-24 15:43:16.250931 test begin: paddle.matmul(Tensor([1, 32, 4096, 4096],"float32"), Tensor([1, 32, 4096, 128],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([1, 32, 4096, 4096],"float32"), Tensor([1, 32, 4096, 128],"float32"), ) 	 553648128 	 1000 	 8.160294532775879 	 8.162745237350464 	 8.147605180740356 	 8.13924264907837 	 16.30668544769287 	 16.309280157089233 	 8.332468032836914 	 8.335853576660156 	 
2025-07-24 15:44:15.049126 test begin: paddle.matmul(Tensor([1, 32, 4096, 4096],"float32"), Tensor([1, 32, 4096, 388],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([1, 32, 4096, 4096],"float32"), Tensor([1, 32, 4096, 388],"float32"), ) 	 587726848 	 1000 	 30.98480772972107 	 30.98128080368042 	 30.970957040786743 	 30.95885133743286 	 54.982282638549805 	 54.9760947227478 	 28.09373426437378 	 28.092366933822632 	 
2025-07-24 15:47:19.694698 test begin: paddle.matmul(Tensor([1, 32, 4096, 4096],"float32"), Tensor([4, 32, 4096, 128],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([1, 32, 4096, 4096],"float32"), Tensor([4, 32, 4096, 128],"float32"), ) 	 603979776 	 1000 	 31.092097997665405 	 43.33992099761963 	 0.0001049041748046875 	 8.874840259552002 	 71.43107461929321 	 80.78798723220825 	 0.00774836540222168 	 13.742830276489258 	 
2025-07-24 15:51:18.643035 test begin: paddle.matmul(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 4, 4096, 128],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([1, 4, 4096, 4096],"float32"), Tensor([1, 4, 4096, 128],"float32"), ) 	 69206016 	 1000 	 1.639953851699829 	 1.6396667957305908 	 1.6259777545928955 	 1.6070213317871094 	 2.6654763221740723 	 2.6655797958374023 	 1.361961841583252 	 1.3619656562805176 	 
2025-07-24 15:51:28.405012 test begin: paddle.matmul(Tensor([1, 97, 4096, 4096],"float32"), Tensor([1, 97, 4096, 128],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([1, 97, 4096, 4096],"float32"), Tensor([1, 97, 4096, 128],"float32"), ) 	 1678245888 	 1000 	 23.647618293762207 	 23.800334930419922 	 23.63472580909729 	 23.773988723754883 	 48.327173471450806 	 48.32529664039612 	 24.694867372512817 	 24.693830251693726 	 
2025-07-24 15:54:27.306858 test begin: paddle.matmul(Tensor([10, 23, 499, 3600],"float32"), Tensor([10, 23, 3600, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 23, 499, 3600],"float32"), Tensor([10, 23, 3600, 64],"float32"), ) 	 466164000 	 1000 	 6.456683397293091 	 6.45701003074646 	 6.443699359893799 	 6.4337990283966064 	 9.861268997192383 	 9.860788106918335 	 5.039085388183594 	 5.037568807601929 	 
2025-07-24 15:55:09.900745 test begin: paddle.matmul(Tensor([10, 3, 499, 3600],"float32"), Tensor([10, 3, 3600, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 3, 499, 3600],"float32"), Tensor([10, 3, 3600, 64],"float32"), ) 	 60804000 	 1000 	 1.441849708557129 	 1.4418694972991943 	 1.4292070865631104 	 1.4195904731750488 	 1.390557050704956 	 1.390864372253418 	 0.7104802131652832 	 0.7106845378875732 	 
2025-07-24 15:55:16.572400 test begin: paddle.matmul(Tensor([10, 8, 177, 3600],"float32"), Tensor([10, 8, 3600, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 8, 177, 3600],"float32"), Tensor([10, 8, 3600, 64],"float32"), ) 	 69408000 	 1000 	 1.441497564315796 	 1.4416146278381348 	 1.4287419319152832 	 1.409510850906372 	 1.471470594406128 	 1.471708059310913 	 0.7518551349639893 	 0.7519192695617676 	 
2025-07-24 15:55:24.809546 test begin: paddle.matmul(Tensor([10, 8, 499, 1273],"float32"), Tensor([10, 8, 1273, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 8, 499, 1273],"float32"), Tensor([10, 8, 1273, 64],"float32"), ) 	 57335920 	 1000 	 0.7771410942077637 	 0.7771728038787842 	 0.7646782398223877 	 0.7549338340759277 	 1.258852481842041 	 1.2587251663208008 	 0.6431963443756104 	 0.6431300640106201 	 
2025-07-24 15:55:29.841508 test begin: paddle.matmul(Tensor([10, 8, 499, 3600],"float32"), Tensor([10, 8, 3600, 177],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 8, 499, 3600],"float32"), Tensor([10, 8, 3600, 177],"float32"), ) 	 194688000 	 1000 	 4.30807900428772 	 4.311878442764282 	 4.295546054840088 	 4.281702995300293 	 7.6224048137664795 	 7.62277364730835 	 3.8950037956237793 	 3.8954222202301025 	 
2025-07-24 15:55:57.114635 test begin: paddle.matmul(Tensor([10, 8, 499, 9923],"float32"), Tensor([10, 8, 9923, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([10, 8, 499, 9923],"float32"), Tensor([10, 8, 9923, 64],"float32"), ) 	 446931920 	 1000 	 5.934463977813721 	 5.934639930725098 	 5.921924829483032 	 5.902385711669922 	 9.248105764389038 	 9.248421669006348 	 4.725785970687866 	 4.725845098495483 	 
2025-07-24 15:56:36.977572 test begin: paddle.matmul(Tensor([1379, 4, 256, 256],"float32"), Tensor([1379, 4, 256, 36],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([1379, 4, 256, 256],"float32"), Tensor([1379, 4, 256, 36],"float32"), ) 	 412332032 	 1000 	 5.3016357421875 	 5.302358865737915 	 5.281035900115967 	 5.270103693008423 	 7.247471570968628 	 7.236377477645874 	 3.703495740890503 	 3.698021650314331 	 
2025-07-24 15:57:10.114362 test begin: paddle.matmul(Tensor([194, 4, 256, 256],"float32"), Tensor([194, 4, 256, 36],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([194, 4, 256, 256],"float32"), Tensor([194, 4, 256, 36],"float32"), ) 	 58007552 	 1000 	 0.788088321685791 	 0.7899231910705566 	 0.7675840854644775 	 0.7556335926055908 	 1.0714426040649414 	 1.0695679187774658 	 0.5474143028259277 	 0.5464460849761963 	 
2025-07-24 15:57:14.938436 test begin: paddle.matmul(Tensor([28, 8, 499, 3600],"float32"), Tensor([28, 8, 3600, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([28, 8, 499, 3600],"float32"), Tensor([28, 8, 3600, 64],"float32"), ) 	 454003200 	 1000 	 6.457121849060059 	 6.4668128490448 	 6.436555624008179 	 6.42306113243103 	 9.618001699447632 	 9.60403323173523 	 4.914396286010742 	 4.911738872528076 	 
2025-07-24 15:57:56.262986 test begin: paddle.matmul(Tensor([4, 32, 4096, 4096],"float32"), Tensor([4, 32, 4096, 128],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([4, 32, 4096, 4096],"float32"), Tensor([4, 32, 4096, 128],"float32"), ) 	 2214592512 	 1000 	 30.97931218147278 	 30.98663592338562 	 30.95857834815979 	 30.954198122024536 	 63.5198974609375 	 63.57624268531799 	 32.45832800865173 	 32.47850179672241 	 
2025-07-24 16:01:45.165220 test begin: paddle.matmul(Tensor([4, 8, 499, 3600],"float32"), Tensor([4, 8, 3600, 64],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([4, 8, 499, 3600],"float32"), Tensor([4, 8, 3600, 64],"float32"), ) 	 64857600 	 1000 	 1.4420924186706543 	 1.442032814025879 	 1.4297235012054443 	 1.4197852611541748 	 1.4309709072113037 	 1.4309427738189697 	 0.7311549186706543 	 0.7311296463012695 	 
2025-07-24 16:01:52.081156 test begin: paddle.matmul(Tensor([512, 11, 256, 256],"float32"), Tensor([512, 11, 256, 36],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([512, 11, 256, 256],"float32"), Tensor([512, 11, 256, 36],"float32"), ) 	 421003264 	 1000 	 5.449971675872803 	 5.45075249671936 	 5.437060117721558 	 5.428163528442383 	 7.426933765411377 	 7.427198171615601 	 3.795135974884033 	 3.7952158451080322 	 
2025-07-24 16:02:25.914573 test begin: paddle.matmul(Tensor([512, 2, 256, 256],"float32"), Tensor([512, 2, 256, 36],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([512, 2, 256, 256],"float32"), Tensor([512, 2, 256, 36],"float32"), ) 	 76546048 	 1000 	 0.9976439476013184 	 0.9978885650634766 	 0.9850108623504639 	 0.975722074508667 	 1.3632853031158447 	 1.3634538650512695 	 0.696568489074707 	 0.6965999603271484 	 
2025-07-24 16:02:32.075203 test begin: paddle.matmul(Tensor([512, 4, 256, 256],"float32"), Tensor([512, 4, 256, 97],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([512, 4, 256, 256],"float32"), Tensor([512, 4, 256, 97],"float32"), ) 	 185073664 	 1000 	 1.9973959922790527 	 1.9968721866607666 	 1.9846951961517334 	 1.9744110107421875 	 3.686608076095581 	 3.6869704723358154 	 1.883760690689087 	 1.8839948177337646 	 
2025-07-24 16:02:47.912870 test begin: paddle.matmul(Tensor([512, 4, 97, 256],"float32"), Tensor([512, 4, 256, 36],"float32"), )
[Prof] paddle.matmul 	 paddle.matmul(Tensor([512, 4, 97, 256],"float32"), Tensor([512, 4, 256, 36],"float32"), ) 	 69730304 	 1000 	 0.9944536685943604 	 0.9945778846740723 	 0.9816453456878662 	 0.9713561534881592 	 1.21964430809021 	 1.2194287776947021 	 0.623161792755127 	 0.6230084896087646 	 
2025-07-24 16:02:53.598846 test begin: paddle.matrix_transpose(Tensor([2, 12700801, 4],"float16"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([2, 12700801, 4],"float16"), ) 	 101606408 	 1000 	 0.004307270050048828 	 0.004348039627075195 	 1.1682510375976562e-05 	 7.319450378417969e-05 	 0.03975963592529297 	 0.05313920974731445 	 2.47955322265625e-05 	 4.2438507080078125e-05 	 combined
2025-07-24 16:02:57.732561 test begin: paddle.matrix_transpose(Tensor([2, 3, 16934401],"float16"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([2, 3, 16934401],"float16"), ) 	 101606406 	 1000 	 0.004271984100341797 	 0.003905057907104492 	 1.049041748046875e-05 	 1.7404556274414062e-05 	 0.03970193862915039 	 0.06390786170959473 	 2.956390380859375e-05 	 5.221366882324219e-05 	 combined
2025-07-24 16:03:02.771414 test begin: paddle.matrix_transpose(Tensor([2, 3, 4233601],"float64"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([2, 3, 4233601],"float64"), ) 	 25401606 	 1000 	 0.004957914352416992 	 0.00392913818359375 	 3.218650817871094e-05 	 1.7642974853515625e-05 	 0.04066777229309082 	 0.07088255882263184 	 2.7894973754882812e-05 	 5.340576171875e-05 	 combined
2025-07-24 16:03:03.984013 test begin: paddle.matrix_transpose(Tensor([2, 3, 8467201],"float32"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([2, 3, 8467201],"float32"), ) 	 50803206 	 1000 	 0.0043468475341796875 	 0.006163358688354492 	 1.0728836059570312e-05 	 7.605552673339844e-05 	 0.03953123092651367 	 0.05486702919006348 	 2.2649765014648438e-05 	 3.886222839355469e-05 	 combined
2025-07-24 16:03:05.795119 test begin: paddle.matrix_transpose(Tensor([2, 3175201, 4],"float64"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([2, 3175201, 4],"float64"), ) 	 25401608 	 1000 	 0.0045778751373291016 	 0.003887176513671875 	 2.7179718017578125e-05 	 1.7881393432617188e-05 	 0.03961467742919922 	 0.05183291435241699 	 3.0040740966796875e-05 	 4.0531158447265625e-05 	 combined
2025-07-24 16:03:07.023309 test begin: paddle.matrix_transpose(Tensor([2, 6350401, 4],"float32"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([2, 6350401, 4],"float32"), ) 	 50803208 	 1000 	 0.006972074508666992 	 0.003924846649169922 	 3.218650817871094e-05 	 1.71661376953125e-05 	 0.05698990821838379 	 0.05161404609680176 	 4.0531158447265625e-05 	 4.172325134277344e-05 	 combined
2025-07-24 16:03:08.871774 test begin: paddle.matrix_transpose(Tensor([2116801, 3, 4],"float64"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([2116801, 3, 4],"float64"), ) 	 25401612 	 1000 	 0.006242990493774414 	 0.003905057907104492 	 4.00543212890625e-05 	 1.811981201171875e-05 	 0.047975778579711914 	 0.05317187309265137 	 4.124641418457031e-05 	 4.887580871582031e-05 	 combined
2025-07-24 16:03:10.079330 test begin: paddle.matrix_transpose(Tensor([4233601, 3, 4],"float32"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([4233601, 3, 4],"float32"), ) 	 50803212 	 1000 	 0.004279136657714844 	 0.00391840934753418 	 7.152557373046875e-06 	 1.7404556274414062e-05 	 0.039540767669677734 	 0.058533668518066406 	 2.4318695068359375e-05 	 3.0517578125e-05 	 combined
2025-07-24 16:03:11.752972 test begin: paddle.matrix_transpose(Tensor([8467201, 3, 4],"float16"), )
[Prof] paddle.matrix_transpose 	 paddle.matrix_transpose(Tensor([8467201, 3, 4],"float16"), ) 	 101606412 	 1000 	 0.00887155532836914 	 0.007218837738037109 	 1.9788742065429688e-05 	 1.8596649169921875e-05 	 0.04697537422180176 	 0.05885672569274902 	 1.9788742065429688e-05 	 4.315376281738281e-05 	 combined
2025-07-24 16:03:15.727682 test begin: paddle.max(Tensor([416, 50, 10, 256],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([416, 50, 10, 256],"float32"), axis=1, ) 	 53248000 	 1000 	 0.19482064247131348 	 0.1603071689605713 	 0.18325090408325195 	 0.14580082893371582 	 1.1400270462036133 	 1.3915972709655762 	 0.2913057804107666 	 0.2842824459075928 	 
2025-07-24 16:03:19.638149 test begin: paddle.max(Tensor([416, 50, 7, 349],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([416, 50, 7, 349],"float32"), axis=1, ) 	 50814400 	 1000 	 0.1958451271057129 	 0.16255998611450195 	 0.18442273139953613 	 0.14762496948242188 	 1.1050159931182861 	 1.3326568603515625 	 0.2824103832244873 	 0.27222561836242676 	 
2025-07-24 16:03:23.289988 test begin: paddle.max(Tensor([416, 69, 7, 256],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([416, 69, 7, 256],"float32"), axis=1, ) 	 51437568 	 1000 	 0.19265985488891602 	 0.15472650527954102 	 0.18105268478393555 	 0.1394975185394287 	 1.0936229228973389 	 1.337778091430664 	 0.27947068214416504 	 0.2732570171356201 	 
2025-07-24 16:03:26.926212 test begin: paddle.max(Tensor([49, 1024, 1024],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.max 	 paddle.max(Tensor([49, 1024, 1024],"float32"), axis=-1, keepdim=True, ) 	 51380224 	 1000 	 0.15545296669006348 	 0.1493821144104004 	 0.1432187557220459 	 0.13553214073181152 	 1.0569682121276855 	 1.2876274585723877 	 0.27008986473083496 	 0.26299095153808594 	 
2025-07-24 16:03:30.525886 test begin: paddle.max(Tensor([512, 50, 7, 284],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([512, 50, 7, 284],"float32"), axis=1, ) 	 50892800 	 1000 	 0.2032773494720459 	 0.15542292594909668 	 0.18370962142944336 	 0.1335463523864746 	 1.1165974140167236 	 1.33461332321167 	 0.28537511825561523 	 0.27262020111083984 	 
2025-07-24 16:03:34.238677 test begin: paddle.max(Tensor([512, 50, 8, 256],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([512, 50, 8, 256],"float32"), axis=1, ) 	 52428800 	 1000 	 0.193101167678833 	 0.15704655647277832 	 0.17346572875976562 	 0.1350710391998291 	 1.1233205795288086 	 1.3720474243164062 	 0.28705430030822754 	 0.2802302837371826 	 
2025-07-24 16:03:39.771903 test begin: paddle.max(Tensor([512, 56, 7, 256],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([512, 56, 7, 256],"float32"), axis=1, ) 	 51380224 	 1000 	 0.1956806182861328 	 0.1540539264678955 	 0.18404293060302734 	 0.13910293579101562 	 1.0962505340576172 	 1.3386790752410889 	 0.2801492214202881 	 0.27341532707214355 	 
2025-07-24 16:03:43.671411 test begin: paddle.max(Tensor([568, 50, 7, 256],"float32"), axis=1, )
[Prof] paddle.max 	 paddle.max(Tensor([568, 50, 7, 256],"float32"), axis=1, ) 	 50892800 	 1000 	 0.18949460983276367 	 0.153397798538208 	 0.1779470443725586 	 0.13684701919555664 	 1.093343734741211 	 1.3310668468475342 	 0.2793734073638916 	 0.27189135551452637 	 
2025-07-24 16:03:47.330887 test begin: paddle.max(Tensor([8, 1024, 6202],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.max 	 paddle.max(Tensor([8, 1024, 6202],"float32"), axis=-1, keepdim=True, ) 	 50806784 	 1000 	 0.1517939567565918 	 0.16346359252929688 	 0.13966822624206543 	 0.15010857582092285 	 1.0482888221740723 	 1.2911944389343262 	 0.2678828239440918 	 0.263744592666626 	 
2025-07-24 16:03:50.846509 test begin: paddle.max(Tensor([8, 6202, 1024],"float32"), axis=-1, keepdim=True, )
[Prof] paddle.max 	 paddle.max(Tensor([8, 6202, 1024],"float32"), axis=-1, keepdim=True, ) 	 50806784 	 1000 	 0.15360641479492188 	 0.14780211448669434 	 0.13922595977783203 	 0.13317370414733887 	 1.045891284942627 	 1.2739477157592773 	 0.26724672317504883 	 0.26023077964782715 	 
2025-07-24 16:03:54.362315 test begin: paddle.maximum(Tensor([11585, 4386],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([11585, 4386],"float32"), Tensor([1],"float32"), ) 	 50811811 	 1000 	 0.2965884208679199 	 0.3045210838317871 	 0.2863013744354248 	 0.29291462898254395 	 0.7401354312896729 	 3.3035497665405273 	 0.251723051071167 	 0.28098011016845703 	 
2025-07-24 16:04:00.879695 test begin: paddle.maximum(Tensor([120961, 420],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([120961, 420],"float32"), Tensor([1],"float32"), ) 	 50803621 	 1000 	 0.2964940071105957 	 0.32041049003601074 	 0.28607797622680664 	 0.29279661178588867 	 0.7400116920471191 	 3.303262948989868 	 0.25171780586242676 	 0.280900239944458 	 
2025-07-24 16:04:08.571355 test begin: paddle.maximum(Tensor([121539, 418],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([121539, 418],"float32"), Tensor([1],"float32"), ) 	 50803303 	 1000 	 0.2964954376220703 	 0.3044097423553467 	 0.2862560749053955 	 0.2930488586425781 	 0.7399983406066895 	 3.2996885776519775 	 0.2517268657684326 	 0.280742883682251 	 
2025-07-24 16:04:14.899983 test begin: paddle.maximum(Tensor([14877, 3415],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([14877, 3415],"float32"), Tensor([1],"float32"), ) 	 50804956 	 1000 	 0.29663801193237305 	 0.3044314384460449 	 0.2848246097564697 	 0.2928965091705322 	 0.7402160167694092 	 3.2968621253967285 	 0.2517845630645752 	 0.28040456771850586 	 
2025-07-24 16:04:21.258252 test begin: paddle.maximum(Tensor([16121, 3152],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([16121, 3152],"float32"), Tensor([1],"float32"), ) 	 50813393 	 1000 	 0.29676103591918945 	 0.3053555488586426 	 0.2864987850189209 	 0.2927525043487549 	 0.7403957843780518 	 3.298926591873169 	 0.25184035301208496 	 0.28052711486816406 	 
2025-07-24 16:04:27.596190 test begin: paddle.maximum(Tensor([62643, 811],"float32"), Tensor([1],"float32"), )
[Prof] paddle.maximum 	 paddle.maximum(Tensor([62643, 811],"float32"), Tensor([1],"float32"), ) 	 50803474 	 1000 	 0.29650378227233887 	 0.30439305305480957 	 0.2861979007720947 	 0.2929244041442871 	 0.7403223514556885 	 3.3023219108581543 	 0.2518181800842285 	 0.2808403968811035 	 
2025-07-24 16:04:33.933000 test begin: paddle.mean(Tensor([7573, 11, 1280],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([7573, 11, 1280],"bfloat16"), axis=1, ) 	 106627840 	 1000 	 0.2100386619567871 	 0.1991560459136963 	 0.19866013526916504 	 0.173628568649292 	 0.34720468521118164 	 0.4496631622314453 	 0.287595272064209 	 0.22962331771850586 	 
2025-07-24 16:04:39.379706 test begin: paddle.mean(Tensor([7573, 8, 1678],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([7573, 8, 1678],"bfloat16"), axis=1, ) 	 101659952 	 1000 	 0.18515276908874512 	 0.18251442909240723 	 0.17357778549194336 	 0.16714239120483398 	 0.34960126876831055 	 0.4427170753479004 	 0.28431177139282227 	 0.22615408897399902 	 
2025-07-24 16:04:42.467582 test begin: paddle.mean(Tensor([7710, 11, 1280],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([7710, 11, 1280],"bfloat16"), axis=1, ) 	 108556800 	 1000 	 0.21370339393615723 	 0.1991724967956543 	 0.20136284828186035 	 0.18387269973754883 	 0.35254764556884766 	 0.4575796127319336 	 0.29279017448425293 	 0.2337636947631836 	 
2025-07-24 16:04:45.672495 test begin: paddle.mean(Tensor([7710, 8, 1648],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([7710, 8, 1648],"bfloat16"), axis=1, ) 	 101648640 	 1000 	 0.18545889854431152 	 0.1865828037261963 	 0.17404818534851074 	 0.16355371475219727 	 0.34979987144470215 	 0.44506096839904785 	 0.285414457321167 	 0.22736859321594238 	 
2025-07-24 16:04:48.728539 test begin: paddle.mean(Tensor([8162, 10, 1280],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([8162, 10, 1280],"bfloat16"), axis=1, ) 	 104473600 	 1000 	 0.20529603958129883 	 0.19178509712219238 	 0.19322800636291504 	 0.17650747299194336 	 0.34499573707580566 	 0.44745445251464844 	 0.28112125396728516 	 0.2285754680633545 	 
2025-07-24 16:04:51.789315 test begin: paddle.mean(Tensor([8162, 8, 1557],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([8162, 8, 1557],"bfloat16"), axis=1, ) 	 101665872 	 1000 	 0.18549728393554688 	 0.21957993507385254 	 0.16609835624694824 	 0.19306468963623047 	 0.35002684593200684 	 0.45999932289123535 	 0.27968502044677734 	 0.2350006103515625 	 
2025-07-24 16:04:54.925536 test begin: paddle.mean(Tensor([9923, 8, 1280],"bfloat16"), axis=1, )
[Prof] paddle.mean 	 paddle.mean(Tensor([9923, 8, 1280],"bfloat16"), axis=1, ) 	 101611520 	 1000 	 0.18230891227722168 	 0.17623019218444824 	 0.1709291934967041 	 0.16086316108703613 	 0.35085415840148926 	 0.4522590637207031 	 0.2909250259399414 	 0.2310476303100586 	 
2025-07-24 16:04:57.995683 test begin: paddle.median(Tensor([2, 25401601],"float32"), axis=1, mode="min", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f61241571f0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 16:15:23.139536 test begin: paddle.median(Tensor([25401601],"int64"), )
W0724 16:15:23.704025 124278 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdc2a477010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 16:25:27.733755 test begin: paddle.median(Tensor([50803201],"float32"), )
W0724 16:25:28.716477 128590 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4342baee30>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753346127 (unix time) try "date -d @1753346127" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1f5e6) received by PID 128486 (TID 0x7f433e3a2640) from PID 128486 ***]

2025-07-24 16:35:34.036151 test begin: paddle.median(Tensor([508033, 100],"float32"), axis=1, mode="min", )
W0724 16:35:35.093544 133052 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.median 	 paddle.median(Tensor([508033, 100],"float32"), axis=1, mode="min", ) 	 50803300 	 1000 	 264.1534321308136 	 3.6281516551971436 	 6.856653451919556 	 3.6082892417907715 	 None 	 None 	 None 	 None 	 combined
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:40:06.299653 test begin: paddle.min(Tensor([10, 10, 28225, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.min 	 paddle.min(Tensor([10, 10, 28225, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402502 	 1000 	 8.223226070404053 	 0.15728449821472168 	 0.008114814758300781 	 0.08030414581298828 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:40:21.675308 test begin: paddle.min(Tensor([10, 10, 9, 28225],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.min 	 paddle.min(Tensor([10, 10, 9, 28225],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402502 	 1000 	 0.3039853572845459 	 0.16198015213012695 	 0.00020194053649902344 	 0.14767765998840332 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:40:23.836638 test begin: paddle.min(Tensor([10, 31361, 9, 9],"float64"), Tensor([2],"int64"), )
[Prof] paddle.min 	 paddle.min(Tensor([10, 31361, 9, 9],"float64"), Tensor([2],"int64"), ) 	 25402412 	 1000 	 7.510891675949097 	 0.16234183311462402 	 0.0074634552001953125 	 0.08285641670227051 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:40:39.560770 test begin: paddle.min(Tensor([10, 31361, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.min 	 paddle.min(Tensor([10, 31361, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402412 	 1000 	 0.3365285396575928 	 0.1828911304473877 	 0.0002465248107910156 	 0.09345197677612305 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:40:41.783291 test begin: paddle.min(Tensor([10, 5, 56449, 9],"float64"), Tensor([2],"int64"), )
[Prof] paddle.min 	 paddle.min(Tensor([10, 5, 56449, 9],"float64"), Tensor([2],"int64"), ) 	 25402052 	 1000 	 0.5547835826873779 	 0.16927289962768555 	 0.0005202293395996094 	 0.08648133277893066 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:40:44.421243 test begin: paddle.min(Tensor([10, 5, 9, 56449],"float64"), Tensor([2],"int64"), )
[Prof] paddle.min 	 paddle.min(Tensor([10, 5, 9, 56449],"float64"), Tensor([2],"int64"), ) 	 25402052 	 1000 	 0.1941511631011963 	 0.15417695045471191 	 0.0001678466796875 	 0.1395576000213623 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:40:46.481409 test begin: paddle.min(Tensor([31361, 10, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.min 	 paddle.min(Tensor([31361, 10, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402412 	 1000 	 8.226681232452393 	 0.15743517875671387 	 0.00811147689819336 	 0.08045053482055664 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:41:01.813049 test begin: paddle.min(Tensor([62721, 5, 9, 9],"float64"), Tensor([2],"int64"), )
[Prof] paddle.min 	 paddle.min(Tensor([62721, 5, 9, 9],"float64"), Tensor([2],"int64"), ) 	 25402007 	 1000 	 0.8621454238891602 	 0.16677188873291016 	 0.0008397102355957031 	 0.1521284580230713 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:41:04.875397 test begin: paddle.min(Tensor([64, 1, 28, 28351],"float32"), )
[Prof] paddle.min 	 paddle.min(Tensor([64, 1, 28, 28351],"float32"), ) 	 50804992 	 1000 	 0.15183043479919434 	 0.1526477336883545 	 0.07753634452819824 	 0.07798409461975098 	 1.0439887046813965 	 1.247330904006958 	 0.2134876251220703 	 0.21232318878173828 	 
2025-07-24 16:41:10.986057 test begin: paddle.min(Tensor([64, 1, 28351, 28],"float32"), )
[Prof] paddle.min 	 paddle.min(Tensor([64, 1, 28351, 28],"float32"), ) 	 50804992 	 1000 	 0.1518101692199707 	 0.1526350975036621 	 0.07753944396972656 	 0.07798027992248535 	 1.0438508987426758 	 1.247030258178711 	 0.21348094940185547 	 0.21227169036865234 	 
2025-07-24 16:41:16.632922 test begin: paddle.min(Tensor([64, 1013, 28, 28],"float32"), )
[Prof] paddle.min 	 paddle.min(Tensor([64, 1013, 28, 28],"float32"), ) 	 50828288 	 1000 	 0.1520240306854248 	 0.15268850326538086 	 0.07763481140136719 	 0.0780034065246582 	 1.044785976409912 	 1.2475917339324951 	 0.21367454528808594 	 0.21243953704833984 	 
2025-07-24 16:41:22.217804 test begin: paddle.min(Tensor([64801, 1, 28, 28],"float32"), )
[Prof] paddle.min 	 paddle.min(Tensor([64801, 1, 28, 28],"float32"), ) 	 50803984 	 1000 	 0.15187597274780273 	 0.15255069732666016 	 0.07758283615112305 	 0.07792115211486816 	 1.0441076755523682 	 1.2467594146728516 	 0.21357226371765137 	 0.21222639083862305 	 
2025-07-24 16:41:25.648935 test begin: paddle.minimum(Tensor([13, 1, 113],"float32"), Tensor([451143, 113],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([13, 1, 113],"float32"), Tensor([451143, 113],"float32"), ) 	 50980628 	 1000 	 3.7888193130493164 	 4.095469951629639 	 3.7779617309570312 	 2.092486619949341 	 18.51654028892517 	 47.10702872276306 	 4.724762678146362 	 2.18243408203125 	 
2025-07-24 16:42:52.008444 test begin: paddle.minimum(Tensor([13, 1, 2],"float32"), Tensor([25401601, 2],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([13, 1, 2],"float32"), Tensor([25401601, 2],"float32"), ) 	 50803228 	 1000 	 3.772570848464966 	 4.080671310424805 	 3.7619271278381348 	 2.0820794105529785 	 31.963904857635498 	 47.28947639465332 	 8.157456159591675 	 2.7762694358825684 	 
2025-07-24 16:44:33.305130 test begin: paddle.minimum(Tensor([16, 1, 113],"float32"), Tensor([451143, 113],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([16, 1, 113],"float32"), Tensor([451143, 113],"float32"), ) 	 50980967 	 1000 	 6.019085884094238 	 5.034168481826782 	 4.643096685409546 	 2.5713798999786377 	 22.509159088134766 	 57.30242156982422 	 5.742981433868408 	 2.6545863151550293 	 
2025-07-24 16:46:19.862511 test begin: paddle.minimum(Tensor([16, 1, 2],"float32"), Tensor([25401601, 2],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([16, 1, 2],"float32"), Tensor([25401601, 2],"float32"), ) 	 50803234 	 1000 	 4.642280340194702 	 5.011141538619995 	 4.62636137008667 	 2.5603702068328857 	 38.135995388031006 	 58.62831234931946 	 9.73165225982666 	 2.7174665927886963 	 
2025-07-24 16:48:20.988424 test begin: paddle.minimum(Tensor([9, 1, 113],"float32"), Tensor([451143, 113],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([9, 1, 113],"float32"), Tensor([451143, 113],"float32"), ) 	 50980176 	 1000 	 2.623699188232422 	 2.8296923637390137 	 2.612860679626465 	 2.8166794776916504 	 13.05660367012024 	 32.03246235847473 	 3.3317418098449707 	 2.5159547328948975 	 
2025-07-24 16:49:20.136572 test begin: paddle.minimum(Tensor([9, 1, 2],"float32"), Tensor([25401601, 2],"float32"), )
[Prof] paddle.minimum 	 paddle.minimum(Tensor([9, 1, 2],"float32"), Tensor([25401601, 2],"float32"), ) 	 50803220 	 1000 	 2.614631175994873 	 2.817553997039795 	 2.602926731109619 	 2.8049159049987793 	 23.298614978790283 	 31.976441144943237 	 5.947137355804443 	 2.5117180347442627 	 
2025-07-24 16:50:29.396414 test begin: paddle.mm(Tensor([1838, 6, 144, 144],"float32"), Tensor([1838, 6, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([1838, 6, 144, 144],"float32"), Tensor([1838, 6, 144, 32],"float32"), ) 	 279493632 	 1000 	 6.032442808151245 	 6.03320050239563 	 6.019859790802002 	 6.009803295135498 	 9.454280138015747 	 9.455061674118042 	 4.831125020980835 	 4.831587076187134 	 
2025-07-24 16:51:07.137258 test begin: paddle.mm(Tensor([2048, 2, 144, 144],"float32"), Tensor([2048, 2, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([2048, 2, 144, 144],"float32"), Tensor([2048, 2, 144, 32],"float32"), ) 	 103809024 	 1000 	 2.252962350845337 	 2.2540483474731445 	 2.232071876525879 	 2.2302424907684326 	 3.530618667602539 	 3.530444860458374 	 1.8040642738342285 	 1.8039357662200928 	 
2025-07-24 16:51:21.434224 test begin: paddle.mm(Tensor([2048, 6, 144, 144],"float32"), Tensor([2048, 6, 144, 29],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([2048, 6, 144, 144],"float32"), Tensor([2048, 6, 144, 29],"float32"), ) 	 306118656 	 1000 	 6.736665487289429 	 6.735205173492432 	 6.722499132156372 	 6.712494611740112 	 10.28622841835022 	 10.28635311126709 	 5.256828308105469 	 5.256181716918945 	 
2025-07-24 16:52:01.525018 test begin: paddle.mm(Tensor([2048, 6, 144, 144],"float32"), Tensor([2048, 6, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([2048, 6, 144, 144],"float32"), Tensor([2048, 6, 144, 32],"float32"), ) 	 311427072 	 1000 	 6.735087156295776 	 6.735205411911011 	 6.714571952819824 	 6.702563047409058 	 10.548486471176147 	 10.550056219100952 	 5.390235185623169 	 5.390338897705078 	 
2025-07-24 16:52:42.449658 test begin: paddle.mm(Tensor([2048, 6, 29, 144],"float32"), Tensor([2048, 6, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([2048, 6, 29, 144],"float32"), Tensor([2048, 6, 144, 32],"float32"), ) 	 107937792 	 1000 	 3.368708610534668 	 3.368663787841797 	 3.3554258346557617 	 3.3458337783813477 	 3.727511405944824 	 3.727302074432373 	 1.9047105312347412 	 1.904555082321167 	 
2025-07-24 16:52:58.617337 test begin: paddle.mm(Tensor([2757, 4, 144, 144],"float32"), Tensor([2757, 4, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([2757, 4, 144, 144],"float32"), Tensor([2757, 4, 144, 32],"float32"), ) 	 279493632 	 1000 	 6.032828330993652 	 6.0329225063323975 	 6.019996166229248 	 6.010183334350586 	 9.454764366149902 	 9.45624566078186 	 4.83149790763855 	 4.832332134246826 	 
2025-07-24 16:53:36.971315 test begin: paddle.mm(Tensor([3840, 1, 144, 144],"float32"), Tensor([3840, 1, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([3840, 1, 144, 144],"float32"), Tensor([3840, 1, 144, 32],"float32"), ) 	 97320960 	 1000 	 2.1152896881103516 	 2.1147072315216064 	 2.102567434310913 	 2.0916826725006104 	 3.3133339881896973 	 3.3130273818969727 	 1.6928908824920654 	 1.6927778720855713 	 
2025-07-24 16:53:50.126384 test begin: paddle.mm(Tensor([3840, 1, 144, 144],"float32"), Tensor([3840, 4, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([3840, 1, 144, 144],"float32"), Tensor([3840, 4, 144, 32],"float32"), ) 	 150405120 	 1000 	 9.519611835479736 	 9.869914531707764 	 5.5789947509765625e-05 	 5.043582201004028 	 14.563869953155518 	 14.313205242156982 	 0.0011830329895019531 	 4.87302565574646 	 
2025-07-24 16:54:42.379554 test begin: paddle.mm(Tensor([3840, 3, 144, 144],"float32"), Tensor([3840, 3, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([3840, 3, 144, 144],"float32"), Tensor([3840, 3, 144, 32],"float32"), ) 	 291962880 	 1000 	 6.320432424545288 	 6.320554733276367 	 6.3078014850616455 	 6.2973456382751465 	 9.89753007888794 	 9.897921085357666 	 5.057588815689087 	 5.0584235191345215 	 
2025-07-24 16:55:20.599425 test begin: paddle.mm(Tensor([3840, 4, 144, 144],"float32"), Tensor([3840, 4, 144, 23],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([3840, 4, 144, 144],"float32"), Tensor([3840, 4, 144, 23],"float32"), ) 	 369377280 	 1000 	 8.403014421463013 	 8.404293060302734 	 8.390173196792603 	 8.380826234817505 	 11.905906200408936 	 11.905914545059204 	 6.083984613418579 	 6.084317684173584 	 
2025-07-24 16:56:09.213654 test begin: paddle.mm(Tensor([3840, 4, 23, 144],"float32"), Tensor([3840, 4, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([3840, 4, 23, 144],"float32"), Tensor([3840, 4, 144, 32],"float32"), ) 	 121651200 	 1000 	 4.216991186141968 	 4.214789867401123 	 4.202044725418091 	 4.181663751602173 	 4.209518194198608 	 4.209102153778076 	 2.150972366333008 	 2.1507368087768555 	 
2025-07-24 16:56:28.312727 test begin: paddle.mm(Tensor([409, 6, 144, 144],"float32"), Tensor([409, 6, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([409, 6, 144, 144],"float32"), Tensor([409, 6, 144, 32],"float32"), ) 	 62194176 	 1000 	 1.3665988445281982 	 1.3665812015533447 	 1.3540067672729492 	 1.343951940536499 	 2.1357457637786865 	 2.3487677574157715 	 1.0912868976593018 	 1.304180383682251 	 
2025-07-24 16:56:37.179935 test begin: paddle.mm(Tensor([4096, 1, 144, 144],"float32"), Tensor([4096, 1, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([4096, 1, 144, 144],"float32"), Tensor([4096, 1, 144, 32],"float32"), ) 	 103809024 	 1000 	 2.255350351333618 	 2.253542423248291 	 2.2405529022216797 	 2.2209701538085938 	 3.5305674076080322 	 3.530362844467163 	 1.8040833473205566 	 1.8038747310638428 	 
2025-07-24 16:56:51.358096 test begin: paddle.mm(Tensor([4096, 1, 144, 144],"float32"), Tensor([4096, 4, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([4096, 1, 144, 144],"float32"), Tensor([4096, 4, 144, 32],"float32"), ) 	 160432128 	 1000 	 10.14234709739685 	 10.526593923568726 	 0.00010204315185546875 	 5.378555774688721 	 15.524869680404663 	 15.262933254241943 	 0.001268148422241211 	 5.1952385902404785 	 
2025-07-24 16:57:46.793662 test begin: paddle.mm(Tensor([4096, 3, 144, 144],"float32"), Tensor([4096, 3, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([4096, 3, 144, 144],"float32"), Tensor([4096, 3, 144, 32],"float32"), ) 	 311427072 	 1000 	 6.736362457275391 	 6.736465930938721 	 6.723477840423584 	 6.713382244110107 	 10.550822019577026 	 10.549586057662964 	 5.391319751739502 	 5.390600919723511 	 
2025-07-24 16:58:27.550950 test begin: paddle.mm(Tensor([4096, 4, 144, 144],"float32"), Tensor([4096, 4, 144, 22],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([4096, 4, 144, 144],"float32"), Tensor([4096, 4, 144, 22],"float32"), ) 	 391643136 	 1000 	 8.962285995483398 	 8.96167540550232 	 8.94920563697815 	 8.938642740249634 	 12.698547601699829 	 12.68920373916626 	 6.489205837249756 	 6.484021425247192 	 
2025-07-24 16:59:18.278573 test begin: paddle.mm(Tensor([4096, 4, 22, 144],"float32"), Tensor([4096, 4, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([4096, 4, 22, 144],"float32"), Tensor([4096, 4, 144, 32],"float32"), ) 	 127401984 	 1000 	 4.488592147827148 	 4.488503932952881 	 4.475715160369873 	 4.465635061264038 	 4.4831812381744385 	 4.482841491699219 	 2.290811777114868 	 2.290611982345581 	 
2025-07-24 16:59:39.377275 test begin: paddle.mm(Tensor([613, 4, 144, 144],"float32"), Tensor([613, 4, 144, 32],"float32"), )
[Prof] paddle.mm 	 paddle.mm(Tensor([613, 4, 144, 144],"float32"), Tensor([613, 4, 144, 32],"float32"), ) 	 62143488 	 1000 	 1.3676788806915283 	 1.3664731979370117 	 1.3534834384918213 	 1.3435184955596924 	 2.1354641914367676 	 2.1351640224456787 	 1.0911152362823486 	 1.0909278392791748 	 
2025-07-24 16:59:47.632173 test begin: paddle.mod(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([10, 2540161],"int64"), Tensor([10, 2540161],"int64"), ) 	 50803220 	 1000 	 0.4482440948486328 	 0.4495360851287842 	 0.43893933296203613 	 0.43574023246765137 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:59:50.548085 test begin: paddle.mod(Tensor([10, 5080321],"int32"), Tensor([10, 5080321],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([10, 5080321],"int32"), Tensor([10, 5080321],"int32"), ) 	 101606420 	 1000 	 0.4509284496307373 	 0.4494640827178955 	 0.44135475158691406 	 0.4376344680786133 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 16:59:56.063426 test begin: paddle.mod(Tensor([1270081, 2, 4, 5],"int32"), Tensor([1270081, 2, 4, 5],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([1270081, 2, 4, 5],"int32"), Tensor([1270081, 2, 4, 5],"int32"), ) 	 101606480 	 1000 	 0.46071529388427734 	 0.4495398998260498 	 0.43402886390686035 	 0.4381752014160156 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 17:00:01.396308 test begin: paddle.mod(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([24807, 1024],"int64"), Tensor([24807, 1024],"int64"), ) 	 50804736 	 1000 	 0.4484403133392334 	 0.4469635486602783 	 0.43794798851013184 	 0.4360072612762451 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 17:00:04.318858 test begin: paddle.mod(Tensor([2540161, 20],"int32"), Tensor([2540161, 20],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([2540161, 20],"int32"), Tensor([2540161, 20],"int32"), ) 	 101606440 	 1000 	 0.4510223865509033 	 0.45238351821899414 	 0.44110703468322754 	 0.43805980682373047 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 17:00:08.185624 test begin: paddle.mod(Tensor([6, 2, 4, 1058401],"int32"), Tensor([6, 2, 4, 1058401],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([6, 2, 4, 1058401],"int32"), Tensor([6, 2, 4, 1058401],"int32"), ) 	 101606496 	 1000 	 0.45101475715637207 	 0.44951677322387695 	 0.44132161140441895 	 0.4374263286590576 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 17:00:12.030826 test begin: paddle.mod(Tensor([6, 2, 846721, 5],"int32"), Tensor([6, 2, 846721, 5],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([6, 2, 846721, 5],"int32"), Tensor([6, 2, 846721, 5],"int32"), ) 	 101606520 	 1000 	 0.4509623050689697 	 0.4494814872741699 	 0.4412837028503418 	 0.43823909759521484 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 17:00:15.866825 test begin: paddle.mod(Tensor([6, 423361, 4, 5],"int32"), Tensor([6, 423361, 4, 5],"int32"), )
[Prof] paddle.mod 	 paddle.mod(Tensor([6, 423361, 4, 5],"int32"), Tensor([6, 423361, 4, 5],"int32"), ) 	 101606640 	 1000 	 0.45096278190612793 	 0.4493982791900635 	 0.4412879943847656 	 0.4382445812225342 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 17:00:19.708159 test begin: paddle.mode(Tensor([2, 10, 1270081],"float64"), -1, )
[Prof] paddle.mode 	 paddle.mode(Tensor([2, 10, 1270081],"float64"), -1, ) 	 25401620 	 1000 	 80.85724401473999 	 14.395146608352661 	 0.000102996826171875 	 0.00029397010803222656 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 17:01:56.296824 test begin: paddle.mode(Tensor([2, 10, 1270081],"float64"), -1, keepdim=True, )
[Prof] paddle.mode 	 paddle.mode(Tensor([2, 10, 1270081],"float64"), -1, keepdim=True, ) 	 25401620 	 1000 	 76.35394287109375 	 13.88616681098938 	 9.441375732421875e-05 	 0.00026226043701171875 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 17:03:27.827660 test begin: paddle.mode(Tensor([2, 10, 1270081],"float64"), 1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f24104aa860>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called after throwing an instance of 'thrust::system::system_error'
  what():  CUDA free failed: cudaErrorCudartUnloading: driver shutting down
2025-07-24 17:13:35.358046 test begin: paddle.mode(Tensor([2, 1270081, 10],"float64"), -1, )
W0724 17:13:39.417285 149378 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f9d14e6f040>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 17:23:41.982000 test begin: paddle.mode(Tensor([2, 1270081, 10],"float64"), -1, keepdim=True, )
W0724 17:23:42.648423 153832 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f6a5204ef50>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(4), driver shutting down. 
  [Hint: 'cudaErrorCudartUnloading'. This indicates that a CUDA Runtime API call cannot be executed because it is being called during process shut down, at a pointin time after CUDA driver has been unloaded.] (at ../paddle/phi/backends/gpu/cuda/cuda_info.cc:266)

2025-07-24 17:33:48.631603 test begin: paddle.mode(Tensor([2, 1270081, 10],"float64"), 1, )
W0724 17:33:49.289063 158147 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.mode 	 paddle.mode(Tensor([2, 1270081, 10],"float64"), 1, ) 	 25401620 	 1000 	 71.86800456047058 	 14.837563753128052 	 9.846687316894531e-05 	 0.00025773048400878906 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 17:35:22.967799 test begin: paddle.moveaxis(Tensor([2, 3, 120961, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([2, 3, 120961, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 25401810 	 1000 	 0.008891105651855469 	 0.0061037540435791016 	 1.1682510375976562e-05 	 1.8596649169921875e-05 	 0.04063606262207031 	 0.07183456420898438 	 3.314018249511719e-05 	 3.790855407714844e-05 	 
2025-07-24 17:35:24.122511 test begin: paddle.moveaxis(Tensor([2, 3, 4, 151201, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([2, 3, 4, 151201, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 25401768 	 1000 	 0.008350610733032227 	 0.006024599075317383 	 8.58306884765625e-06 	 1.8596649169921875e-05 	 0.044461727142333984 	 0.05341315269470215 	 2.6226043701171875e-05 	 4.458427429199219e-05 	 
2025-07-24 17:35:25.269494 test begin: paddle.moveaxis(Tensor([2, 3, 4, 5, 211681],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([2, 3, 4, 5, 211681],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 25401720 	 1000 	 0.013480424880981445 	 0.01019430160522461 	 3.0279159545898438e-05 	 2.4080276489257812e-05 	 0.04729771614074707 	 0.0639963150024414 	 1.4781951904296875e-05 	 6.67572021484375e-05 	 
2025-07-24 17:35:26.430768 test begin: paddle.moveaxis(Tensor([2, 90721, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([2, 90721, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 25401880 	 1000 	 0.013416767120361328 	 0.010155439376831055 	 1.7881393432617188e-05 	 2.1457672119140625e-05 	 0.04770207405090332 	 0.05977201461791992 	 2.4557113647460938e-05 	 3.409385681152344e-05 	 
2025-07-24 17:35:27.608152 test begin: paddle.moveaxis(Tensor([60481, 3, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
[Prof] paddle.moveaxis 	 paddle.moveaxis(Tensor([60481, 3, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], ) 	 25402020 	 1000 	 0.008265495300292969 	 0.006104707717895508 	 8.344650268554688e-06 	 1.7642974853515625e-05 	 0.04024481773376465 	 0.053965091705322266 	 1.7881393432617188e-05 	 4.029273986816406e-05 	 
2025-07-24 17:35:28.783770 test begin: paddle.moveaxis(x=Tensor([120961, 2, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([120961, 2, 3, 5, 7],"float64"), source=0, destination=2, ) 	 25401810 	 1000 	 0.007143497467041016 	 0.005004167556762695 	 1.4066696166992188e-05 	 5.1021575927734375e-05 	 0.0406339168548584 	 0.055216073989868164 	 2.5272369384765625e-05 	 5.3882598876953125e-05 	 
2025-07-24 17:35:29.929686 test begin: paddle.moveaxis(x=Tensor([120961, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([120961, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 25401810 	 1000 	 0.00791168212890625 	 0.00592350959777832 	 1.3589859008789062e-05 	 2.2411346435546875e-05 	 0.041391849517822266 	 0.05362868309020996 	 3.3855438232421875e-05 	 3.7670135498046875e-05 	 
2025-07-24 17:35:31.091208 test begin: paddle.moveaxis(x=Tensor([4, 2, 3, 151201, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([4, 2, 3, 151201, 7],"float64"), source=0, destination=2, ) 	 25401768 	 1000 	 0.007222414016723633 	 0.004832029342651367 	 1.0251998901367188e-05 	 1.9788742065429688e-05 	 0.04018521308898926 	 0.07032489776611328 	 2.8371810913085938e-05 	 0.0001201629638671875 	 
2025-07-24 17:35:32.190942 test begin: paddle.moveaxis(x=Tensor([4, 2, 3, 151201, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([4, 2, 3, 151201, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 25401768 	 1000 	 0.012955427169799805 	 0.01577019691467285 	 1.5974044799804688e-05 	 6.341934204101562e-05 	 0.04738974571228027 	 0.060811519622802734 	 2.8133392333984375e-05 	 3.4332275390625e-05 	 
2025-07-24 17:35:33.405895 test begin: paddle.moveaxis(x=Tensor([4, 2, 3, 5, 211681],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([4, 2, 3, 5, 211681],"float64"), source=0, destination=2, ) 	 25401720 	 1000 	 0.01240229606628418 	 0.008313655853271484 	 1.1205673217773438e-05 	 1.8835067749023438e-05 	 0.05000662803649902 	 0.06027936935424805 	 3.6716461181640625e-05 	 3.981590270996094e-05 	 
2025-07-24 17:35:34.581241 test begin: paddle.moveaxis(x=Tensor([4, 2, 3, 5, 211681],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([4, 2, 3, 5, 211681],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 25401720 	 1000 	 0.007799863815307617 	 0.0060079097747802734 	 7.152557373046875e-06 	 2.2649765014648438e-05 	 0.041634559631347656 	 0.05377316474914551 	 2.2649765014648438e-05 	 3.457069396972656e-05 	 
2025-07-24 17:35:37.011039 test begin: paddle.moveaxis(x=Tensor([4, 2, 90721, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([4, 2, 90721, 5, 7],"float64"), source=0, destination=2, ) 	 25401880 	 1000 	 0.007325410842895508 	 0.004704713821411133 	 6.9141387939453125e-06 	 1.7881393432617188e-05 	 0.03981781005859375 	 0.05350828170776367 	 1.5020370483398438e-05 	 4.696846008300781e-05 	 
2025-07-24 17:35:39.337215 test begin: paddle.moveaxis(x=Tensor([4, 2, 90721, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([4, 2, 90721, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 25401880 	 1000 	 0.012988567352294922 	 0.010212421417236328 	 1.049041748046875e-05 	 2.4557113647460938e-05 	 0.05077934265136719 	 0.06031298637390137 	 3.170967102050781e-05 	 3.528594970703125e-05 	 
2025-07-24 17:35:40.612626 test begin: paddle.moveaxis(x=Tensor([4, 60481, 3, 5, 7],"float64"), source=0, destination=2, )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([4, 60481, 3, 5, 7],"float64"), source=0, destination=2, ) 	 25402020 	 1000 	 0.012395620346069336 	 0.008350133895874023 	 1.0013580322265625e-05 	 2.002716064453125e-05 	 0.047196388244628906 	 0.059891700744628906 	 2.1457672119140625e-05 	 4.076957702636719e-05 	 
2025-07-24 17:35:43.375840 test begin: paddle.moveaxis(x=Tensor([4, 60481, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
[Prof] paddle.moveaxis 	 paddle.moveaxis(x=Tensor([4, 60481, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), ) 	 25402020 	 1000 	 0.0077495574951171875 	 0.010107278823852539 	 7.3909759521484375e-06 	 2.0503997802734375e-05 	 0.04362630844116211 	 0.05644702911376953 	 3.62396240234375e-05 	 4.172325134277344e-05 	 
2025-07-24 17:35:45.892460 test begin: paddle.multigammaln(Tensor([10, 2540161],"float64"), 2, )
[Prof] paddle.multigammaln 	 paddle.multigammaln(Tensor([10, 2540161],"float64"), 2, ) 	 25401610 	 1000 	 2.945631504058838 	 2.8016085624694824 	 0.5014851093292236 	 0.5720703601837158 	 4.019109725952148 	 3.6669061183929443 	 1.0277020931243896 	 0.7490952014923096 	 
2025-07-24 17:36:00.521976 test begin: paddle.multigammaln(Tensor([10, 5080321],"float32"), 2, )
[Prof] paddle.multigammaln 	 paddle.multigammaln(Tensor([10, 5080321],"float32"), 2, ) 	 50803210 	 1000 	 2.397993564605713 	 2.5812692642211914 	 0.40860819816589355 	 0.5278892517089844 	 3.4535062313079834 	 3.984086036682129 	 0.8832569122314453 	 0.8138856887817383 	 
2025-07-24 17:36:14.628775 test begin: paddle.multigammaln(Tensor([1270081, 20],"float64"), 2, )
[Prof] paddle.multigammaln 	 paddle.multigammaln(Tensor([1270081, 20],"float64"), 2, ) 	 25401620 	 1000 	 2.9453415870666504 	 2.7999086380004883 	 0.5015435218811035 	 0.5719723701477051 	 4.014666795730591 	 3.6667425632476807 	 1.0269136428833008 	 0.7489814758300781 	 
2025-07-24 17:36:29.174635 test begin: paddle.multigammaln(Tensor([2540161, 20],"float32"), 2, )
[Prof] paddle.multigammaln 	 paddle.multigammaln(Tensor([2540161, 20],"float32"), 2, ) 	 50803220 	 1000 	 2.3988196849823 	 2.5829501152038574 	 0.408597469329834 	 0.5278778076171875 	 3.4531939029693604 	 3.9838995933532715 	 0.883246660232544 	 0.8137466907501221 	 
2025-07-24 17:36:43.770487 test begin: paddle.multiplex(inputs=list[Tensor([12700801, 4],"float32"),Tensor([12700801, 4],"float32"),], index=Tensor([12700801, 1],"int32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbd37408e50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 17:46:48.660843 test begin: paddle.multiplex(inputs=list[Tensor([12700801, 4],"float32"),Tensor([12700801, 4],"float32"),], index=Tensor([6, 1],"int32"), )
W0724 17:46:50.391198 10114 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.multiplex 	 paddle.multiplex(inputs=list[Tensor([12700801, 4],"float32"),Tensor([12700801, 4],"float32"),], index=Tensor([6, 1],"int32"), ) 	 101606414 	 1000 	 0.06393647193908691 	 0.26743245124816895 	 6.461143493652344e-05 	 7.772445678710938e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 17:46:51.411815 test begin: paddle.multiplex(inputs=list[Tensor([7, 7257601],"float32"),Tensor([7, 7257601],"float32"),], index=Tensor([6, 1],"int32"), )
[Prof] paddle.multiplex 	 paddle.multiplex(inputs=list[Tensor([7, 7257601],"float32"),Tensor([7, 7257601],"float32"),], index=Tensor([6, 1],"int32"), ) 	 101606420 	 1000 	 0.33106446266174316 	 0.46285438537597656 	 0.00028896331787109375 	 0.0002803802490234375 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 17:46:55.242140 test begin: paddle.multiply(Tensor([298, 872, 14, 14],"float32"), Tensor([298, 872, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([298, 872, 14, 14],"float32"), Tensor([298, 872, 1, 1],"float32"), ) 	 51191632 	 1000 	 0.2977898120880127 	 0.3084394931793213 	 0.28741884231567383 	 0.2945702075958252 	 0.8796608448028564 	 0.9126129150390625 	 0.45000195503234863 	 0.310727596282959 	 
2025-07-24 17:46:59.407258 test begin: paddle.multiply(Tensor([512, 507, 14, 14],"float32"), Tensor([512, 507, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([512, 507, 14, 14],"float32"), Tensor([512, 507, 1, 1],"float32"), ) 	 51138048 	 1000 	 0.296994686126709 	 0.3067800998687744 	 0.28694748878479004 	 0.29428601264953613 	 0.8783102035522461 	 0.9114344120025635 	 0.4493601322174072 	 0.3103041648864746 	 
2025-07-24 17:47:03.456511 test begin: paddle.multiply(Tensor([512, 872, 14, 9],"float32"), Tensor([512, 872, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([512, 872, 14, 9],"float32"), Tensor([512, 872, 1, 1],"float32"), ) 	 56700928 	 1000 	 0.3291451930999756 	 0.3431692123413086 	 0.31827831268310547 	 0.3283071517944336 	 0.8959155082702637 	 1.0284345149993896 	 0.45773983001708984 	 0.35016942024230957 	 
2025-07-24 17:47:07.866698 test begin: paddle.multiply(Tensor([512, 872, 14, 9],"float32"), Tensor([512, 872, 1, 9],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([512, 872, 14, 9],"float32"), Tensor([512, 872, 1, 9],"float32"), ) 	 60272640 	 1000 	 0.3385167121887207 	 0.3662285804748535 	 0.3282489776611328 	 0.3431663513183594 	 0.8712084293365479 	 1.0406510829925537 	 0.44510674476623535 	 0.35430288314819336 	 
2025-07-24 17:47:12.514537 test begin: paddle.multiply(Tensor([512, 872, 9, 14],"float32"), Tensor([512, 872, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([512, 872, 9, 14],"float32"), Tensor([512, 872, 1, 1],"float32"), ) 	 56700928 	 1000 	 0.3286590576171875 	 0.3415961265563965 	 0.3183634281158447 	 0.32336974143981934 	 0.8967275619506836 	 1.0285987854003906 	 0.45750975608825684 	 0.3502345085144043 	 
2025-07-24 17:47:16.885345 test begin: paddle.multiply(Tensor([512, 872, 9, 14],"float32"), Tensor([512, 872, 9, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(Tensor([512, 872, 9, 14],"float32"), Tensor([512, 872, 9, 1],"float32"), ) 	 60272640 	 1000 	 0.33858418464660645 	 0.3560144901275635 	 0.3282458782196045 	 0.33976125717163086 	 0.9319775104522705 	 1.1862034797668457 	 0.4761645793914795 	 0.40395212173461914 	 
2025-07-24 17:47:21.584074 test begin: paddle.multiply(x=Tensor([128, 127, 56, 56],"float32"), y=Tensor([128, 127, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 127, 56, 56],"float32"), y=Tensor([128, 127, 1, 1],"float32"), ) 	 50995072 	 1000 	 0.297177791595459 	 0.3089618682861328 	 0.28669238090515137 	 0.2965118885040283 	 0.7417595386505127 	 0.9062435626983643 	 0.3789634704589844 	 0.3081338405609131 	 
2025-07-24 17:47:25.449777 test begin: paddle.multiply(x=Tensor([128, 224, 32, 56],"float32"), y=Tensor([128, 224, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 224, 32, 56],"float32"), y=Tensor([128, 224, 1, 1],"float32"), ) 	 51408896 	 1000 	 0.2995028495788574 	 0.3132650852203369 	 0.2866785526275635 	 0.30106377601623535 	 0.746875524520874 	 0.9153852462768555 	 0.38156628608703613 	 0.31267833709716797 	 
2025-07-24 17:47:29.366564 test begin: paddle.multiply(x=Tensor([128, 224, 32, 56],"float32"), y=Tensor([128, 224, 32, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 224, 32, 56],"float32"), y=Tensor([128, 224, 32, 1],"float32"), ) 	 52297728 	 1000 	 0.30199503898620605 	 0.31217503547668457 	 0.29143309593200684 	 0.3001120090484619 	 0.8604748249053955 	 1.0880885124206543 	 0.4396553039550781 	 0.3705456256866455 	 
2025-07-24 17:47:35.667243 test begin: paddle.multiply(x=Tensor([128, 224, 56, 32],"float32"), y=Tensor([128, 224, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 224, 56, 32],"float32"), y=Tensor([128, 224, 1, 1],"float32"), ) 	 51408896 	 1000 	 0.9711563587188721 	 0.3213796615600586 	 0.28069186210632324 	 0.2946314811706543 	 0.747002124786377 	 0.914046049118042 	 0.3816230297088623 	 0.31124424934387207 	 
2025-07-24 17:47:42.588592 test begin: paddle.multiply(x=Tensor([128, 224, 56, 32],"float32"), y=Tensor([128, 224, 1, 32],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 224, 56, 32],"float32"), y=Tensor([128, 224, 1, 32],"float32"), ) 	 52297728 	 1000 	 0.3064439296722412 	 0.3153054714202881 	 0.29262661933898926 	 0.30313920974731445 	 0.7857584953308105 	 0.9185283184051514 	 0.4014458656311035 	 0.3127419948577881 	 
2025-07-24 17:47:46.596826 test begin: paddle.multiply(x=Tensor([128, 256, 28, 56],"float32"), y=Tensor([128, 256, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 256, 28, 56],"float32"), y=Tensor([128, 256, 1, 1],"float32"), ) 	 51412992 	 1000 	 0.2994527816772461 	 0.3123509883880615 	 0.28881406784057617 	 0.30035996437072754 	 0.7496943473815918 	 0.9140567779541016 	 0.383026123046875 	 0.31124091148376465 	 
2025-07-24 17:47:50.520787 test begin: paddle.multiply(x=Tensor([128, 256, 28, 56],"float32"), y=Tensor([128, 256, 28, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 256, 28, 56],"float32"), y=Tensor([128, 256, 28, 1],"float32"), ) 	 52297728 	 1000 	 0.3020048141479492 	 0.3120734691619873 	 0.2915534973144531 	 0.29950642585754395 	 0.8604717254638672 	 1.0880050659179688 	 0.43963074684143066 	 0.3705415725708008 	 
2025-07-24 17:47:54.755230 test begin: paddle.multiply(x=Tensor([128, 256, 56, 28],"float32"), y=Tensor([128, 256, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 256, 56, 28],"float32"), y=Tensor([128, 256, 1, 1],"float32"), ) 	 51412992 	 1000 	 0.29944419860839844 	 0.3123009204864502 	 0.2888774871826172 	 0.3001444339752197 	 0.7509236335754395 	 0.913987398147583 	 0.3829996585845947 	 0.31125926971435547 	 
2025-07-24 17:47:58.672939 test begin: paddle.multiply(x=Tensor([128, 256, 56, 28],"float32"), y=Tensor([128, 256, 1, 28],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([128, 256, 56, 28],"float32"), y=Tensor([128, 256, 1, 28],"float32"), ) 	 52297728 	 1000 	 0.3017706871032715 	 0.31614041328430176 	 0.29117441177368164 	 0.3039259910583496 	 1.0097739696502686 	 0.923133134841919 	 0.5166168212890625 	 0.31429266929626465 	 
2025-07-24 17:48:02.876511 test begin: paddle.multiply(x=Tensor([64, 256, 56, 56],"float32"), y=Tensor([64, 256, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([64, 256, 56, 56],"float32"), y=Tensor([64, 256, 1, 1],"float32"), ) 	 51396608 	 1000 	 0.2993340492248535 	 0.31139183044433594 	 0.2887728214263916 	 0.2993185520172119 	 0.749298095703125 	 0.9133923053741455 	 0.38352084159851074 	 0.3105454444885254 	 
2025-07-24 17:48:06.791961 test begin: paddle.multiply(x=Tensor([73, 224, 56, 56],"float32"), y=Tensor([73, 224, 1, 1],"float32"), )
[Prof] paddle.multiply 	 paddle.multiply(x=Tensor([73, 224, 56, 56],"float32"), y=Tensor([73, 224, 1, 1],"float32"), ) 	 51296224 	 1000 	 0.29887866973876953 	 0.3108251094818115 	 0.28072690963745117 	 0.2924535274505615 	 0.7463140487670898 	 0.9116673469543457 	 0.3812825679779053 	 0.3099539279937744 	 
2025-07-24 17:48:10.761558 test begin: paddle.mv(Tensor([1411201, 36],"float32"), Tensor([36],"float32"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([1411201, 36],"float32"), Tensor([36],"float32"), ) 	 50803272 	 1000 	 0.19679594039916992 	 0.1934814453125 	 0.18580317497253418 	 0.177689790725708 	 0.4918084144592285 	 0.41938161849975586 	 0.16736578941345215 	 0.1427311897277832 	 
2025-07-24 17:48:13.032024 test begin: paddle.mv(Tensor([254017, 100],"float64"), Tensor([100],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([254017, 100],"float64"), Tensor([100],"float64"), ) 	 25401800 	 1000 	 0.15772604942321777 	 0.15872550010681152 	 0.1466681957244873 	 0.14310956001281738 	 0.3384730815887451 	 0.33007216453552246 	 0.11517167091369629 	 0.11234807968139648 	 
2025-07-24 17:48:14.585482 test begin: paddle.mv(Tensor([3, 16934401],"float32"), Tensor([16934401],"float32"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([3, 16934401],"float32"), Tensor([16934401],"float32"), ) 	 67737604 	 1000 	 0.23533177375793457 	 0.23384356498718262 	 0.1202232837677002 	 0.11946487426757812 	 0.6275520324707031 	 0.6056709289550781 	 0.320598840713501 	 0.3094182014465332 	 
2025-07-24 17:48:17.366416 test begin: paddle.mv(Tensor([3, 50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([3, 50803201],"float32"), Tensor([50803201],"float32"), ) 	 203212804 	 1000 	 0.6746068000793457 	 0.6740820407867432 	 0.34467530250549316 	 0.34378981590270996 	 1.8522095680236816 	 1.7996923923492432 	 0.9463913440704346 	 0.9195730686187744 	 
2025-07-24 17:48:25.754421 test begin: paddle.mv(Tensor([5, 25401601],"float64"), Tensor([25401601],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([5, 25401601],"float64"), Tensor([25401601],"float64"), ) 	 152409606 	 1000 	 0.9553186893463135 	 0.9515969753265381 	 0.488131046295166 	 0.48541784286499023 	 2.4013447761535645 	 2.4089558124542236 	 1.2269999980926514 	 1.230865240097046 	 
2025-07-24 17:48:37.038074 test begin: paddle.mv(Tensor([5, 5080321],"float64"), Tensor([5080321],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([5, 5080321],"float64"), Tensor([5080321],"float64"), ) 	 30481926 	 1000 	 0.2307720184326172 	 0.22495532035827637 	 0.11802077293395996 	 0.11493086814880371 	 0.4909365177154541 	 0.4943578243255615 	 0.25069737434387207 	 0.2518026828765869 	 
2025-07-24 17:48:40.083293 test begin: paddle.mv(Tensor([64, 396901],"float64"), Tensor([396901],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([64, 396901],"float64"), Tensor([396901],"float64"), ) 	 25798565 	 1000 	 0.15636968612670898 	 0.15501713752746582 	 0.0797121524810791 	 0.07921743392944336 	 0.3267543315887451 	 0.33147120475769043 	 0.16691875457763672 	 0.16927647590637207 	 
2025-07-24 17:48:41.599547 test begin: paddle.mv(Tensor([793801, 32],"float64"), Tensor([32],"float64"), )
[Prof] paddle.mv 	 paddle.mv(Tensor([793801, 32],"float64"), Tensor([32],"float64"), ) 	 25401664 	 1000 	 0.1752336025238037 	 0.16260600090026855 	 0.16138386726379395 	 0.14378690719604492 	 0.33449888229370117 	 0.3379848003387451 	 0.11385536193847656 	 0.11501646041870117 	 
2025-07-24 17:48:44.728477 test begin: paddle.nan_to_num(Tensor([148, 114422, 3],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([148, 114422, 3],"float32"), neginf=-1.1920928955078125e-07, ) 	 50803368 	 1000 	 3.0088858604431152 	 0.29816627502441406 	 0.279569149017334 	 0.27755236625671387 	 1.2535090446472168 	 1.15920090675354 	 0.4269826412200928 	 0.23694467544555664 	 
2025-07-24 17:48:53.794871 test begin: paddle.nan_to_num(Tensor([148, 5, 68653],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([148, 5, 68653],"float32"), neginf=-1.1920928955078125e-07, ) 	 50803220 	 1000 	 3.0092990398406982 	 0.29810118675231934 	 0.27957606315612793 	 0.27132439613342285 	 1.2534914016723633 	 1.1601483821868896 	 0.4270138740539551 	 0.23816728591918945 	 
2025-07-24 17:49:01.317579 test begin: paddle.nan_to_num(Tensor([1948, 26080],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([1948, 26080],"float32"), neginf=-1.1920928955078125e-07, ) 	 50803840 	 1000 	 3.006596326828003 	 0.29798197746276855 	 0.2795383930206299 	 0.2850315570831299 	 1.2571165561676025 	 1.1580941677093506 	 0.42825961112976074 	 0.23671436309814453 	 
2025-07-24 17:49:08.793415 test begin: paddle.nan_to_num(Tensor([25401601, 1],"float64"), neginf=-2.220446049250313e-16, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([25401601, 1],"float64"), neginf=-2.220446049250313e-16, ) 	 25401601 	 1000 	 2.885169506072998 	 0.307985782623291 	 0.26807546615600586 	 0.2786567211151123 	 0.9848005771636963 	 1.031296730041504 	 0.33544206619262695 	 0.2104637622833252 	 
2025-07-24 17:49:15.067899 test begin: paddle.nan_to_num(Tensor([25401601, 2],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([25401601, 2],"float32"), neginf=-1.1920928955078125e-07, ) 	 50803202 	 1000 	 3.0079612731933594 	 0.29795193672180176 	 0.2796337604522705 	 0.27811694145202637 	 1.253422498703003 	 1.1586928367614746 	 0.42696571350097656 	 0.23680472373962402 	 
2025-07-24 17:49:22.482771 test begin: paddle.nan_to_num(Tensor([3386881, 5, 3],"float32"), neginf=-1.1920928955078125e-07, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([3386881, 5, 3],"float32"), neginf=-1.1920928955078125e-07, ) 	 50803215 	 1000 	 3.0078084468841553 	 0.2980201244354248 	 0.27959227561950684 	 0.2782449722290039 	 1.2549030780792236 	 1.1587400436401367 	 0.42696475982666016 	 0.23682785034179688 	 
2025-07-24 17:49:29.877583 test begin: paddle.nan_to_num(Tensor([400, 63505],"float64"), neginf=-2.220446049250313e-16, )
[Prof] paddle.nan_to_num 	 paddle.nan_to_num(Tensor([400, 63505],"float64"), neginf=-2.220446049250313e-16, ) 	 25402000 	 1000 	 2.8853015899658203 	 0.2991952896118164 	 0.26809239387512207 	 0.27956414222717285 	 0.984201192855835 	 1.0311317443847656 	 0.33526039123535156 	 0.21069908142089844 	 
2025-07-24 17:49:38.961731 test begin: paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), -1, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), -1, False, ) 	 50803240 	 1000 	 2.484701633453369 	 1.6906135082244873 	 0.2535395622253418 	 0.28770947456359863 	 0.7105488777160645 	 0.7390356063842773 	 0.24210214614868164 	 0.1888427734375 	 
2025-07-24 17:49:45.680271 test begin: paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), 2, True, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), 2, True, ) 	 50803240 	 1000 	 2.820518732070923 	 1.3825113773345947 	 0.287888765335083 	 0.23448491096496582 	 0.7388508319854736 	 0.785752534866333 	 0.25180482864379883 	 0.20075035095214844 	 
2025-07-24 17:49:53.916408 test begin: paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), None, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 1270081, 4, 5],"float32"), None, False, ) 	 50803240 	 1000 	 1.9806725978851318 	 1.1002049446105957 	 0.1683354377746582 	 0.140625 	 0.5627486705780029 	 0.5934112071990967 	 0.1913127899169922 	 0.15168142318725586 	 
2025-07-24 17:49:59.046870 test begin: paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), -1, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), -1, False, ) 	 50803230 	 1000 	 2.4852259159088135 	 1.6892640590667725 	 0.2535536289215088 	 0.28777313232421875 	 0.7054703235626221 	 0.739011287689209 	 0.24046754837036133 	 0.18883347511291504 	 
2025-07-24 17:50:05.777576 test begin: paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), 2, True, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), 2, True, ) 	 50803230 	 1000 	 14.4063880443573 	 1.1334288120269775 	 1.2242112159729004 	 0.14473605155944824 	 0.5591964721679688 	 0.6299052238464355 	 0.19063282012939453 	 0.1610252857208252 	 
2025-07-24 17:50:23.399716 test begin: paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), None, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 1693441, 5],"float32"), None, False, ) 	 50803230 	 1000 	 1.9808502197265625 	 1.1003141403198242 	 0.16834521293640137 	 0.1405951976776123 	 0.5554614067077637 	 0.595491886138916 	 0.18935465812683105 	 0.1517953872680664 	 
2025-07-24 17:50:28.497503 test begin: paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), -1, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), -1, False, ) 	 50803224 	 1000 	 2.0422165393829346 	 1.1014504432678223 	 0.17362427711486816 	 0.14060521125793457 	 0.5563957691192627 	 0.611180305480957 	 0.1896657943725586 	 0.15623021125793457 	 
2025-07-24 17:50:33.672747 test begin: paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), 2, True, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), 2, True, ) 	 50803224 	 1000 	 2.3298842906951904 	 1.3834161758422852 	 0.23794817924499512 	 0.2342383861541748 	 0.7532665729522705 	 0.7906684875488281 	 0.2566854953765869 	 0.20207524299621582 	 
2025-07-24 17:50:41.504204 test begin: paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), None, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([2, 3, 4, 2116801],"float32"), None, False, ) 	 50803224 	 1000 	 1.9807016849517822 	 1.1003084182739258 	 0.16831088066101074 	 0.14060616493225098 	 0.5569858551025391 	 0.5939013957977295 	 0.18938469886779785 	 0.1517930030822754 	 
2025-07-24 17:50:46.585092 test begin: paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), -1, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), -1, False, ) 	 50803260 	 1000 	 2.4847404956817627 	 1.6891183853149414 	 0.2535429000854492 	 0.28772425651550293 	 0.7104854583740234 	 0.739048957824707 	 0.24218297004699707 	 0.18883895874023438 	 
2025-07-24 17:50:54.216766 test begin: paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), 2, True, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), 2, True, ) 	 50803260 	 1000 	 2.828101396560669 	 1.377159595489502 	 0.28791284561157227 	 0.2345132827758789 	 0.7387423515319824 	 0.7857375144958496 	 0.25174570083618164 	 0.2008063793182373 	 
2025-07-24 17:51:02.833507 test begin: paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), None, False, )
[Prof] paddle.nanmean 	 paddle.nanmean(Tensor([846721, 3, 4, 5],"float32"), None, False, ) 	 50803260 	 1000 	 1.9832420349121094 	 1.1002891063690186 	 0.16834211349487305 	 0.14057397842407227 	 0.5612661838531494 	 0.5933823585510254 	 0.1913130283355713 	 0.15169072151184082 	 
2025-07-24 17:51:09.713089 test begin: paddle.nanmedian(Tensor([2, 25401601],"float32"), axis=1, mode="min", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0f19d168f0>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception
2025-07-24 18:01:57.279650 test begin: paddle.nanmedian(Tensor([50803201],"float32"), keepdim=False, )
W0724 18:01:58.285296 32277 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc5ffe6ee00>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753351917 (unix time) try "date -d @1753351917" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x7d6c) received by PID 32108 (TID 0x7fc5fb40e640) from PID 32108 ***]

2025-07-24 18:12:02.979106 test begin: paddle.nanmedian(Tensor([50803201],"float32"), keepdim=False, mode="min", )
W0724 18:12:04.037602 45363 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f99a26fec20>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753352523 (unix time) try "date -d @1753352523" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xb087) received by PID 45191 (TID 0x7f999deef640) from PID 45191 ***]

2025-07-24 18:22:09.807537 test begin: paddle.nanquantile(Tensor([4, 1058401, 6],"float64"), q=0, axis=1, )
W0724 18:22:10.671813 59024 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 18:22:10.717203 59024 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([4, 1058401, 6],"float64"), q=0, axis=1, ) 	 25401624 	 1000 	 43.17734742164612 	 13.200982332229614 	 1.189512014389038 	 0.0270388126373291 	 6.504973649978638 	 2.9400928020477295 	 0.39420127868652344 	 0.2732100486755371 	 
2025-07-24 18:23:17.697523 test begin: paddle.nanquantile(Tensor([4, 1058401, 6],"float64"), q=0.35, axis=2, keepdim=True, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe27927e860>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753353258 (unix time) try "date -d @1753353258" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xe618) received by PID 58904 (TID 0x7fe2703f9640) from PID 58904 ***]

2025-07-24 18:34:24.681228 test begin: paddle.nanquantile(Tensor([4, 7, 907201],"float64"), q=0, axis=1, )
W0724 18:34:25.368515 76100 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 18:34:25.399582 76100 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f7481d9ee30>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753353864 (unix time) try "date -d @1753353864" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x12893) received by PID 75923 (TID 0x7f7478dfa640) from PID 75923 ***]

2025-07-24 18:44:30.641904 test begin: paddle.nanquantile(Tensor([4, 7, 907201],"float64"), q=0.35, axis=2, keepdim=True, )
W0724 18:44:31.367708 89981 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 18:44:31.405519 89981 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nanquantile 	 paddle.nanquantile(Tensor([4, 7, 907201],"float64"), q=0.35, axis=2, keepdim=True, ) 	 25401628 	 1000 	 29.75109052658081 	 9.317152738571167 	 0.8922328948974609 	 0.20251798629760742 	 5.772874116897583 	 2.1885826587677 	 0.42079877853393555 	 0.2030484676361084 	 
2025-07-24 18:45:19.754420 test begin: paddle.nanquantile(Tensor([604801, 7, 6],"float64"), q=0, axis=1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2cc3966ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 18:56:10.532506 test begin: paddle.nanquantile(Tensor([604801, 7, 6],"float64"), q=0.35, axis=2, keepdim=True, )
W0724 18:56:11.322839 114117 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 18:56:11.346290 114117 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f36b3ffae00>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 19:06:15.333110 test begin: paddle.nansum(Tensor([2, 1270081, 4, 5],"float32"), axis=None, keepdim=False, name=None, )
W0724 19:06:16.293368 137507 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 1270081, 4, 5],"float32"), axis=None, keepdim=False, name=None, ) 	 50803240 	 1000 	 1.0090577602386475 	 0.15375256538391113 	 0.20571160316467285 	 0.07916402816772461 	 0.5522794723510742 	 0.5898678302764893 	 0.2821230888366699 	 0.20087075233459473 	 
2025-07-24 19:06:19.116753 test begin: paddle.nansum(Tensor([2, 1270081, 4, 5],"float32"), axis=None, keepdim=True, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 1270081, 4, 5],"float32"), axis=None, keepdim=True, name=None, ) 	 50803240 	 1000 	 1.007478952407837 	 0.15247845649719238 	 0.20568585395812988 	 0.07789850234985352 	 0.5535075664520264 	 0.5896701812744141 	 0.28348660469055176 	 0.2008683681488037 	 
2025-07-24 19:06:22.282593 test begin: paddle.nansum(Tensor([2, 3, 1693441, 5],"float32"), axis=None, keepdim=False, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 3, 1693441, 5],"float32"), axis=None, keepdim=False, name=None, ) 	 50803230 	 1000 	 1.007481575012207 	 0.15246009826660156 	 0.20564532279968262 	 0.07788276672363281 	 0.5520479679107666 	 0.5897402763366699 	 0.28200697898864746 	 0.20087599754333496 	 
2025-07-24 19:06:25.446380 test begin: paddle.nansum(Tensor([2, 3, 1693441, 5],"float32"), axis=None, keepdim=True, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 3, 1693441, 5],"float32"), axis=None, keepdim=True, name=None, ) 	 50803230 	 1000 	 1.0091063976287842 	 0.1524817943572998 	 0.2072451114654541 	 0.07789492607116699 	 0.5520467758178711 	 0.5898411273956299 	 0.28205418586730957 	 0.20096778869628906 	 
2025-07-24 19:06:28.695149 test begin: paddle.nansum(Tensor([2, 3, 4, 2116801],"float32"), axis=None, keepdim=False, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 3, 4, 2116801],"float32"), axis=None, keepdim=False, name=None, ) 	 50803224 	 1000 	 1.0088348388671875 	 0.1524193286895752 	 0.2069714069366455 	 0.07785654067993164 	 0.5520672798156738 	 0.5911033153533936 	 0.2820618152618408 	 0.20082306861877441 	 
2025-07-24 19:06:31.905363 test begin: paddle.nansum(Tensor([2, 3, 4, 2116801],"float32"), axis=None, keepdim=True, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([2, 3, 4, 2116801],"float32"), axis=None, keepdim=True, name=None, ) 	 50803224 	 1000 	 1.0076568126678467 	 0.15246009826660156 	 0.20570158958435059 	 0.0778505802154541 	 0.5522587299346924 	 0.5911140441894531 	 0.28225111961364746 	 0.20083308219909668 	 
2025-07-24 19:06:35.105718 test begin: paddle.nansum(Tensor([846721, 3, 4, 5],"float32"), axis=None, keepdim=False, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([846721, 3, 4, 5],"float32"), axis=None, keepdim=False, name=None, ) 	 50803260 	 1000 	 1.0087080001831055 	 0.15244126319885254 	 0.20580101013183594 	 0.0778512954711914 	 0.5526607036590576 	 0.5911664962768555 	 0.28214073181152344 	 0.2023019790649414 	 
2025-07-24 19:06:39.833605 test begin: paddle.nansum(Tensor([846721, 3, 4, 5],"float32"), axis=None, keepdim=True, name=None, )
[Prof] paddle.nansum 	 paddle.nansum(Tensor([846721, 3, 4, 5],"float32"), axis=None, keepdim=True, name=None, ) 	 50803260 	 1000 	 1.007575273513794 	 0.152435302734375 	 0.2057173252105713 	 0.07788252830505371 	 0.5521559715270996 	 0.5896620750427246 	 0.28212976455688477 	 0.200850248336792 	 
2025-07-24 19:06:42.986518 test begin: paddle.nansum(x=Tensor([105841, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([105841, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401840 	 1000 	 1.065042495727539 	 0.18956637382507324 	 0.2718524932861328 	 0.17363238334655762 	 0.5314865112304688 	 0.44153690338134766 	 0.2722792625427246 	 0.15034127235412598 	 
2025-07-24 19:06:45.928549 test begin: paddle.nansum(x=Tensor([3, 2, 105841, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 2, 105841, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401840 	 1000 	 1.0632567405700684 	 0.19831275939941406 	 0.2718813419342041 	 0.1739788055419922 	 0.5299654006958008 	 0.44274139404296875 	 0.27072787284851074 	 0.15032386779785156 	 
2025-07-24 19:06:48.850987 test begin: paddle.nansum(x=Tensor([3, 2, 3, 141121, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 2, 3, 141121, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401780 	 1000 	 5.678940534591675 	 0.17250561714172363 	 1.1606807708740234 	 0.08812260627746582 	 0.46562933921813965 	 0.41720056533813477 	 0.237870454788208 	 0.14203882217407227 	 
2025-07-24 19:06:56.152935 test begin: paddle.nansum(x=Tensor([3, 2, 3, 4, 176401, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 2, 3, 4, 176401, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401744 	 1000 	 0.9759521484375 	 0.19015860557556152 	 0.24918651580810547 	 0.17325639724731445 	 0.5115237236022949 	 0.444744348526001 	 0.2612898349761963 	 0.1510179042816162 	 
2025-07-24 19:06:58.984049 test begin: paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 1, 70561],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 1, 70561],"float64"), axis=3, keepdim=True, ) 	 25401960 	 1000 	 0.9751138687133789 	 0.19043445587158203 	 0.24918103218078613 	 0.17467975616455078 	 0.5116758346557617 	 0.44536423683166504 	 0.2613711357116699 	 0.15117168426513672 	 
2025-07-24 19:07:01.866078 test begin: paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 35281, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 35281, 2],"float64"), axis=3, keepdim=True, ) 	 25402320 	 1000 	 0.976813554763794 	 0.19018793106079102 	 0.2506251335144043 	 0.1673281192779541 	 0.5116698741912842 	 0.44526243209838867 	 0.26136064529418945 	 0.1516132354736328 	 
2025-07-24 19:07:04.738376 test begin: paddle.nansum(x=Tensor([3, 70561, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
[Prof] paddle.nansum 	 paddle.nansum(x=Tensor([3, 70561, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, ) 	 25401960 	 1000 	 1.0647361278533936 	 0.18958473205566406 	 0.2719259262084961 	 0.17043590545654297 	 0.52937912940979 	 0.4430701732635498 	 0.2703990936279297 	 0.15167832374572754 	 
2025-07-24 19:07:07.685352 test begin: paddle.neg(Tensor([3175201, 8],"float64"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([3175201, 8],"float64"), ) 	 25401608 	 1000 	 0.29831743240356445 	 0.30050110816955566 	 0.2889890670776367 	 0.28789258003234863 	 0.2977004051208496 	 0.2984960079193115 	 0.24782156944274902 	 0.23849701881408691 	 
2025-07-24 19:07:12.783015 test begin: paddle.neg(Tensor([32, 1587601],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([32, 1587601],"float32"), ) 	 50803232 	 1000 	 0.29596686363220215 	 0.2978792190551758 	 0.28499794006347656 	 0.2873725891113281 	 0.29616641998291016 	 0.2977759838104248 	 0.2469315528869629 	 0.23741579055786133 	 
2025-07-24 19:07:15.651711 test begin: paddle.neg(Tensor([32, 793801],"float64"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([32, 793801],"float64"), ) 	 25401632 	 1000 	 0.298337459564209 	 0.2983860969543457 	 0.2891242504119873 	 0.28769373893737793 	 0.29762816429138184 	 0.29968905448913574 	 0.24828052520751953 	 0.241363525390625 	 
2025-07-24 19:07:17.880312 test begin: paddle.neg(Tensor([6350401, 8],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([6350401, 8],"float32"), ) 	 50803208 	 1000 	 0.2959110736846924 	 0.2978684902191162 	 0.2866251468658447 	 0.2872915267944336 	 0.29613804817199707 	 0.29784083366394043 	 0.24663949012756348 	 0.23537373542785645 	 
2025-07-24 19:07:20.734523 test begin: paddle.neg(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 1000 	 0.29615187644958496 	 0.2980618476867676 	 0.28687453269958496 	 0.2872445583343506 	 0.2960944175720215 	 0.2979261875152588 	 0.2466726303100586 	 0.23798656463623047 	 
2025-07-24 19:07:23.742852 test begin: paddle.neg(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 1000 	 0.29607701301574707 	 0.3008882999420166 	 0.2867929935455322 	 0.2872965335845947 	 0.2961697578430176 	 0.29785633087158203 	 0.2465684413909912 	 0.23720836639404297 	 
2025-07-24 19:07:26.712359 test begin: paddle.neg(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.neg 	 paddle.neg(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 1000 	 0.29607176780700684 	 0.29784226417541504 	 0.2868227958679199 	 0.28734779357910156 	 0.29615283012390137 	 0.2978024482727051 	 0.24706459045410156 	 0.23713397979736328 	 
2025-07-24 19:07:29.650517 test begin: paddle.negative(Tensor([1693441, 3, 4, 5],"float16"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([1693441, 3, 4, 5],"float16"), ) 	 101606460 	 1000 	 0.29880738258361816 	 0.2963376045227051 	 0.2871572971343994 	 0.28562259674072266 	 0.2980473041534424 	 0.29622769355773926 	 0.2485501766204834 	 0.23514080047607422 	 
2025-07-24 19:07:34.689629 test begin: paddle.negative(Tensor([2, 1270081, 4, 5],"float32"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 1270081, 4, 5],"float32"), ) 	 50803240 	 1000 	 0.7448842525482178 	 0.3089261054992676 	 0.28106117248535156 	 0.2866177558898926 	 0.2963285446166992 	 0.2991213798522949 	 0.2461717128753662 	 0.23992300033569336 	 
2025-07-24 19:07:39.890855 test begin: paddle.negative(Tensor([2, 2540161, 4, 5],"float16"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 2540161, 4, 5],"float16"), ) 	 101606440 	 1000 	 0.29879307746887207 	 0.2963545322418213 	 0.28714561462402344 	 0.2850189208984375 	 0.29954051971435547 	 0.29751086235046387 	 0.24988651275634766 	 0.2195909023284912 	 
2025-07-24 19:07:45.044355 test begin: paddle.negative(Tensor([2, 3, 1693441, 5],"float32"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 1693441, 5],"float32"), ) 	 50803230 	 1000 	 0.29593324661254883 	 0.2980518341064453 	 0.2753005027770996 	 0.2794060707092285 	 0.2962515354156494 	 0.2978236675262451 	 0.23683977127075195 	 0.23667550086975098 	 
2025-07-24 19:07:47.813399 test begin: paddle.negative(Tensor([2, 3, 3386881, 5],"float16"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 3386881, 5],"float16"), ) 	 101606430 	 1000 	 0.2987983226776123 	 0.2963285446166992 	 0.2786433696746826 	 0.2855069637298584 	 0.29811668395996094 	 0.296205997467041 	 0.24692296981811523 	 0.2351698875427246 	 
2025-07-24 19:07:53.012957 test begin: paddle.negative(Tensor([2, 3, 4, 1058401],"float64"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 4, 1058401],"float64"), ) 	 25401624 	 1000 	 0.2982668876647949 	 0.2984030246734619 	 0.28679728507995605 	 0.2871675491333008 	 0.297640323638916 	 0.29846858978271484 	 0.24318337440490723 	 0.23909783363342285 	 
2025-07-24 19:07:55.262495 test begin: paddle.negative(Tensor([2, 3, 4, 2116801],"float32"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 4, 2116801],"float32"), ) 	 50803224 	 1000 	 0.29592108726501465 	 0.2978379726409912 	 0.2844827175140381 	 0.286940336227417 	 0.29615020751953125 	 0.29785704612731934 	 0.2472069263458252 	 0.23404765129089355 	 
2025-07-24 19:07:58.161266 test begin: paddle.negative(Tensor([2, 3, 4, 4233601],"float16"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 4, 4233601],"float16"), ) 	 101606424 	 1000 	 0.29877352714538574 	 0.29761362075805664 	 0.28711605072021484 	 0.2867717742919922 	 0.298109769821167 	 0.2962064743041992 	 0.2465369701385498 	 0.23645687103271484 	 
2025-07-24 19:08:03.270146 test begin: paddle.negative(Tensor([2, 3, 846721, 5],"float64"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 3, 846721, 5],"float64"), ) 	 25401630 	 1000 	 0.29828763008117676 	 0.30039072036743164 	 0.286853551864624 	 0.2877466678619385 	 0.2976067066192627 	 0.29840898513793945 	 0.24857091903686523 	 0.23883676528930664 	 
2025-07-24 19:08:05.554759 test begin: paddle.negative(Tensor([2, 635041, 4, 5],"float64"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([2, 635041, 4, 5],"float64"), ) 	 25401640 	 1000 	 0.2982914447784424 	 0.30016350746154785 	 0.2867419719696045 	 0.2889235019683838 	 0.2975788116455078 	 0.2984013557434082 	 0.23124337196350098 	 0.23827362060546875 	 
2025-07-24 19:08:07.835002 test begin: paddle.negative(Tensor([423361, 3, 4, 5],"float64"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([423361, 3, 4, 5],"float64"), ) 	 25401660 	 1000 	 0.29828524589538574 	 0.29836273193359375 	 0.2868778705596924 	 0.28777027130126953 	 0.29757237434387207 	 0.29834628105163574 	 0.24866032600402832 	 0.22670507431030273 	 
2025-07-24 19:08:10.090483 test begin: paddle.negative(Tensor([846721, 3, 4, 5],"float32"), )
[Prof] paddle.negative 	 paddle.negative(Tensor([846721, 3, 4, 5],"float32"), ) 	 50803260 	 1000 	 0.30077695846557617 	 0.29789280891418457 	 0.28549718856811523 	 0.2870185375213623 	 0.2961399555206299 	 0.2979283332824707 	 0.24649930000305176 	 0.2208237648010254 	 
2025-07-24 19:08:12.928774 test begin: paddle.nextafter(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([2, 1270081, 4, 5],"float32"), Tensor([2, 1270081, 4, 5],"float32"), ) 	 101606480 	 1000 	 0.4543743133544922 	 0.4654226303100586 	 0.44204092025756836 	 0.4376487731933594 	 None 	 None 	 None 	 None 	 
2025-07-24 19:08:17.472687 test begin: paddle.nextafter(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([2, 3, 1693441, 5],"float32"), Tensor([2, 3, 1693441, 5],"float32"), ) 	 101606460 	 1000 	 0.4510378837585449 	 0.44897985458374023 	 0.4423403739929199 	 0.4379439353942871 	 None 	 None 	 None 	 None 	 
2025-07-24 19:08:20.011933 test begin: paddle.nextafter(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([2, 3, 4, 2116801],"float32"), Tensor([2, 3, 4, 2116801],"float32"), ) 	 101606448 	 1000 	 0.45116615295410156 	 0.4504554271697998 	 0.43955206871032715 	 0.43942832946777344 	 None 	 None 	 None 	 None 	 
2025-07-24 19:08:22.641930 test begin: paddle.nextafter(Tensor([4, 3, 2116801],"float32"), Tensor([4, 3, 2116801],"float64"), )
W0724 19:08:23.506385 142710 dygraph_functions.cc:57914] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3, 2116801],"float32"), Tensor([4, 3, 2116801],"float64"), ) 	 50803224 	 1000 	 0.6845250129699707 	 0.3876051902770996 	 0.3497617244720459 	 0.37610960006713867 	 None 	 None 	 None 	 None 	 
2025-07-24 19:08:24.680602 test begin: paddle.nextafter(Tensor([4, 3, 2116801],"float64"), Tensor([4, 3, 2116801],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3, 2116801],"float64"), Tensor([4, 3, 2116801],"float32"), ) 	 50803224 	 1000 	 0.6845288276672363 	 0.3838822841644287 	 0.34977030754089355 	 0.37237071990966797 	 None 	 None 	 None 	 None 	 
2025-07-24 19:08:26.709326 test begin: paddle.nextafter(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float64"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float64"), ) 	 101606424 	 1000 	 1.3632328510284424 	 0.7667100429534912 	 0.6966965198516846 	 0.755133867263794 	 None 	 None 	 None 	 None 	 
2025-07-24 19:08:30.750858 test begin: paddle.nextafter(Tensor([4, 3, 4233601],"float64"), Tensor([4, 3, 4233601],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3, 4233601],"float64"), Tensor([4, 3, 4233601],"float32"), ) 	 101606424 	 1000 	 1.3640127182006836 	 0.7787861824035645 	 0.696948766708374 	 0.7465240955352783 	 None 	 None 	 None 	 None 	 
2025-07-24 19:08:34.789945 test begin: paddle.nextafter(Tensor([4, 3175201, 2],"float32"), Tensor([4, 3175201, 2],"float64"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3175201, 2],"float32"), Tensor([4, 3175201, 2],"float64"), ) 	 50803216 	 1000 	 1.8421225547790527 	 0.39636659622192383 	 0.34990501403808594 	 0.37604427337646484 	 None 	 None 	 None 	 None 	 
2025-07-24 19:08:40.605241 test begin: paddle.nextafter(Tensor([4, 3175201, 2],"float64"), Tensor([4, 3175201, 2],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 3175201, 2],"float64"), Tensor([4, 3175201, 2],"float32"), ) 	 50803216 	 1000 	 0.6842758655548096 	 0.38380932807922363 	 0.34954094886779785 	 0.3645009994506836 	 None 	 None 	 None 	 None 	 
2025-07-24 19:08:42.651050 test begin: paddle.nextafter(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float64"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float64"), ) 	 101606416 	 1000 	 1.3651185035705566 	 0.7667295932769775 	 0.6966474056243896 	 0.7553596496582031 	 None 	 None 	 None 	 None 	 
2025-07-24 19:08:46.888134 test begin: paddle.nextafter(Tensor([4, 6350401, 2],"float64"), Tensor([4, 6350401, 2],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4, 6350401, 2],"float64"), Tensor([4, 6350401, 2],"float32"), ) 	 101606416 	 1000 	 1.3682167530059814 	 0.7587065696716309 	 0.6969337463378906 	 0.7471742630004883 	 None 	 None 	 None 	 None 	 
2025-07-24 19:08:51.013963 test begin: paddle.nextafter(Tensor([4233601, 3, 2],"float32"), Tensor([4233601, 3, 2],"float64"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4233601, 3, 2],"float32"), Tensor([4233601, 3, 2],"float64"), ) 	 50803212 	 1000 	 0.6847336292266846 	 0.38756418228149414 	 0.34979963302612305 	 0.3760669231414795 	 None 	 None 	 None 	 None 	 
2025-07-24 19:08:53.060210 test begin: paddle.nextafter(Tensor([4233601, 3, 2],"float64"), Tensor([4233601, 3, 2],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([4233601, 3, 2],"float64"), Tensor([4233601, 3, 2],"float32"), ) 	 50803212 	 1000 	 0.6900866031646729 	 0.3838462829589844 	 0.3508322238922119 	 0.37233805656433105 	 None 	 None 	 None 	 None 	 
2025-07-24 19:08:55.142212 test begin: paddle.nextafter(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float64"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float64"), ) 	 101606412 	 1000 	 1.364548921585083 	 0.7676858901977539 	 0.6977939605712891 	 0.7562046051025391 	 None 	 None 	 None 	 None 	 
2025-07-24 19:08:59.219589 test begin: paddle.nextafter(Tensor([8467201, 3, 2],"float64"), Tensor([8467201, 3, 2],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([8467201, 3, 2],"float64"), Tensor([8467201, 3, 2],"float32"), ) 	 101606412 	 1000 	 1.3650636672973633 	 0.7587969303131104 	 0.6983811855316162 	 0.7474308013916016 	 None 	 None 	 None 	 None 	 
2025-07-24 19:09:03.343899 test begin: paddle.nextafter(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), )
[Prof] paddle.nextafter 	 paddle.nextafter(Tensor([846721, 3, 4, 5],"float32"), Tensor([846721, 3, 4, 5],"float32"), ) 	 101606520 	 1000 	 0.45232534408569336 	 0.4490230083465576 	 0.44261622428894043 	 0.4337480068206787 	 None 	 None 	 None 	 None 	 
2025-07-24 19:09:05.891454 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([124, 1536, 267],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([124, 1536, 267],"float32"), 1, None, ) 	 50853888 	 1000 	 0.3012847900390625 	 0.17545819282531738 	 0.2599213123321533 	 0.1547412872314453 	 0.835930585861206 	 0.17151761054992676 	 0.4269900321960449 	 0.09095263481140137 	 
2025-07-24 19:09:08.342080 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([124, 8362, 49],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([124, 8362, 49],"float32"), 1, None, ) 	 50807512 	 1000 	 0.3547353744506836 	 0.37689948081970215 	 0.3060750961303711 	 0.35857486724853516 	 0.8618073463439941 	 0.1740400791168213 	 0.44028759002685547 	 0.0926816463470459 	 
2025-07-24 19:09:10.947304 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([128, 1536, 259],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([128, 1536, 259],"float32"), 1, None, ) 	 50921472 	 1000 	 0.28449130058288574 	 0.15478944778442383 	 0.2435457706451416 	 0.13555502891540527 	 0.8389265537261963 	 0.1703951358795166 	 0.4293396472930908 	 0.08817672729492188 	 
2025-07-24 19:09:13.251478 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([128, 8101, 49],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([128, 8101, 49],"float32"), 1, None, ) 	 50809472 	 1000 	 0.35617494583129883 	 0.3769190311431885 	 0.3134779930114746 	 0.35889363288879395 	 0.8615908622741699 	 0.17560291290283203 	 0.4400675296783447 	 0.09569692611694336 	 
2025-07-24 19:09:15.889772 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([345, 1024, 144],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([345, 1024, 144],"float32"), 1, None, ) 	 50872320 	 1000 	 0.4581594467163086 	 0.5805573463439941 	 0.4166727066040039 	 0.20105266571044922 	 0.8456757068634033 	 0.17061877250671387 	 0.43134188652038574 	 0.08957982063293457 	 
2025-07-24 19:09:19.674053 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([64, 1024, 776],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([64, 1024, 776],"float32"), 1, None, ) 	 50855936 	 1000 	 0.8281075954437256 	 0.1543869972229004 	 0.14788484573364258 	 0.13612937927246094 	 0.8216092586517334 	 0.171797513961792 	 0.4203920364379883 	 0.08580446243286133 	 
2025-07-24 19:09:23.040067 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([64, 5513, 144],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([64, 5513, 144],"float32"), 1, None, ) 	 50807808 	 1000 	 0.4574892520904541 	 0.21923136711120605 	 0.41610002517700195 	 0.20111370086669922 	 0.8433146476745605 	 0.17035365104675293 	 0.43085289001464844 	 0.08628034591674805 	 
2025-07-24 19:09:25.591150 test begin: paddle.nn.functional.adaptive_avg_pool1d(Tensor([676, 1536, 49],"float32"), 1, None, )
[Prof] paddle.nn.functional.adaptive_avg_pool1d 	 paddle.nn.functional.adaptive_avg_pool1d(Tensor([676, 1536, 49],"float32"), 1, None, ) 	 50878464 	 1000 	 0.3553354740142822 	 0.37882304191589355 	 0.31334495544433594 	 0.3551909923553467 	 0.8638279438018799 	 0.17449402809143066 	 0.44059109687805176 	 0.08988285064697266 	 
2025-07-24 19:09:28.231277 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 2048, 2, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 2048, 2, 7],"float32"), output_size=1, ) 	 58404864 	 1000 	 0.3002948760986328 	 0.364241361618042 	 0.2703230381011963 	 0.3449375629425049 	 1.0236587524414062 	 0.2238297462463379 	 0.5229594707489014 	 0.1562197208404541 	 
2025-07-24 19:09:31.206334 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 2048, 7, 2],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 2048, 7, 2],"float32"), output_size=1, ) 	 58404864 	 1000 	 0.3013308048248291 	 0.3642449378967285 	 0.27048540115356445 	 0.3453505039215088 	 1.022277593612671 	 0.2238445281982422 	 0.5222868919372559 	 0.15703797340393066 	 
2025-07-24 19:09:34.114292 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 509, 7, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 509, 7, 7],"float32"), output_size=1, ) 	 50804817 	 1000 	 0.35417699813842773 	 0.3769841194152832 	 0.32490038871765137 	 0.3580808639526367 	 0.8622269630432129 	 0.17417287826538086 	 0.4406740665435791 	 0.1019287109375 	 
2025-07-24 19:09:39.180629 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 2048, 2, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 2048, 2, 7],"float32"), output_size=1, ) 	 58634240 	 1000 	 0.3036210536956787 	 0.3655707836151123 	 0.27179741859436035 	 0.3463718891143799 	 1.0280542373657227 	 0.2245316505432129 	 0.5253076553344727 	 0.1426243782043457 	 
2025-07-24 19:09:42.142794 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 2048, 7, 2],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 2048, 7, 2],"float32"), output_size=1, ) 	 58634240 	 1000 	 0.30205798149108887 	 0.3694343566894531 	 0.27187609672546387 	 0.34778833389282227 	 1.0293583869934082 	 0.2245500087738037 	 0.5265789031982422 	 0.15675592422485352 	 
2025-07-24 19:09:45.132292 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 507, 7, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 507, 7, 7],"float32"), output_size=1, ) 	 50803935 	 1000 	 0.3543698787689209 	 0.37685537338256836 	 0.32128214836120605 	 0.3563821315765381 	 0.8615283966064453 	 0.17413783073425293 	 0.44014525413513184 	 0.10711407661437988 	 
2025-07-24 19:09:47.732796 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 2048, 2, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 2048, 2, 7],"float32"), output_size=1, ) 	 58720256 	 1000 	 0.30447983741760254 	 0.3735640048980713 	 0.2689359188079834 	 0.34690356254577637 	 1.0299811363220215 	 0.22490787506103516 	 0.5256054401397705 	 0.15902161598205566 	 
2025-07-24 19:09:50.721692 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 2048, 7, 2],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 2048, 7, 2],"float32"), output_size=1, ) 	 58720256 	 1000 	 0.30147290229797363 	 0.3675556182861328 	 0.2717580795288086 	 0.3483891487121582 	 1.0288143157958984 	 0.2249007225036621 	 0.5255982875823975 	 0.12165451049804688 	 
2025-07-24 19:09:53.645356 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 507, 7, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 507, 7, 7],"float32"), output_size=1, ) 	 50878464 	 1000 	 0.3546104431152344 	 0.37749385833740234 	 0.32497596740722656 	 0.357666015625 	 0.8620278835296631 	 0.17437171936035156 	 0.44040870666503906 	 0.10077333450317383 	 
2025-07-24 19:09:56.264510 test begin: paddle.nn.functional.adaptive_avg_pool2d(Tensor([507, 2048, 7, 7],"float32"), output_size=1, )
[Prof] paddle.nn.functional.adaptive_avg_pool2d 	 paddle.nn.functional.adaptive_avg_pool2d(Tensor([507, 2048, 7, 7],"float32"), output_size=1, ) 	 50878464 	 1000 	 0.3547642230987549 	 0.3789339065551758 	 0.3254265785217285 	 0.3583855628967285 	 0.8636283874511719 	 0.1742997169494629 	 0.441847562789917 	 0.10318613052368164 	 
2025-07-24 19:09:58.862239 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 45361, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 45361, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50804320 	 1000 	 0.8954932689666748 	 0.1489253044128418 	 0.8714056015014648 	 0.1270143985748291 	 2.2273647785186768 	 0.17433738708496094 	 1.1387038230895996 	 0.10608363151550293 	 
2025-07-24 19:10:03.169004 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 50401, 16, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 50401, 16, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50804208 	 1000 	 0.5052714347839355 	 0.15232110023498535 	 0.48124122619628906 	 0.12183737754821777 	 2.2597005367279053 	 0.17274808883666992 	 1.154632568359375 	 0.10613155364990234 	 
2025-07-24 19:10:07.153851 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 50401, 16, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 50401, 16, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50804208 	 1000 	 0.5459144115447998 	 0.151108980178833 	 0.5218219757080078 	 0.12169122695922852 	 2.2590863704681396 	 0.17281460762023926 	 1.1542928218841553 	 0.10394644737243652 	 
2025-07-24 19:10:11.286340 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 1051, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 1051, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50851584 	 1000 	 15.988309860229492 	 0.1507115364074707 	 15.964359521865845 	 0.12062430381774902 	 2.2595250606536865 	 0.16561412811279297 	 1.1538841724395752 	 0.08487963676452637 	 
2025-07-24 19:10:30.750065 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 1051, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 1051, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50851584 	 1000 	 19.337326288223267 	 0.15204429626464844 	 19.31283473968506 	 0.12963342666625977 	 2.2598986625671387 	 0.1665668487548828 	 1.1554510593414307 	 0.09969210624694824 	 
2025-07-24 19:10:53.579645 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 414, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 414, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50872320 	 1000 	 39.341283559799194 	 0.1493980884552002 	 39.31684112548828 	 0.12770605087280273 	 2.2186131477355957 	 0.1651897430419922 	 1.132948875427246 	 0.09728837013244629 	 
2025-07-24 19:11:39.196417 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 460, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 460, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50872320 	 1000 	 39.20573687553406 	 0.1493847370147705 	 39.1817090511322 	 0.12749481201171875 	 2.220032215118408 	 0.1655576229095459 	 1.135735273361206 	 0.0883169174194336 	 
2025-07-24 19:12:21.850668 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 591, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 591, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50835456 	 1000 	 24.424238204956055 	 0.15035128593444824 	 24.400065183639526 	 0.12736058235168457 	 2.2158565521240234 	 0.16666078567504883 	 1.132253885269165 	 0.09079694747924805 	 
2025-07-24 19:12:49.703639 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 591],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 591],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50835456 	 1000 	 23.20177435874939 	 0.15028977394104004 	 23.177461862564087 	 0.12860965728759766 	 2.1569721698760986 	 0.16533589363098145 	 1.1020748615264893 	 0.09569668769836426 	 
2025-07-24 19:13:16.355761 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 9, 460],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 9, 460],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50872320 	 1000 	 39.08952784538269 	 0.14934062957763672 	 39.06466746330261 	 0.12756919860839844 	 2.1595828533172607 	 0.1656358242034912 	 1.1041224002838135 	 0.09122943878173828 	 
2025-07-24 19:13:58.866561 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 946, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 946, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 50856960 	 1000 	 17.372021198272705 	 0.15004682540893555 	 17.348082542419434 	 0.1284644603729248 	 2.2196922302246094 	 0.16555452346801758 	 1.134852409362793 	 0.09810853004455566 	 
2025-07-24 19:14:19.703458 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([60, 768, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([60, 768, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 51609600 	 1000 	 0.8955409526824951 	 0.15106582641601562 	 0.8707976341247559 	 0.12504959106445312 	 2.2623066902160645 	 0.1756289005279541 	 1.1566085815429688 	 0.10872960090637207 	 
2025-07-24 19:14:24.020980 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([66, 768, 16, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([66, 768, 16, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 51093504 	 1000 	 0.5070900917053223 	 0.15216612815856934 	 0.4831821918487549 	 0.13041281700134277 	 2.273808479309082 	 0.17391252517700195 	 1.1625111103057861 	 0.10706520080566406 	 
2025-07-24 19:14:27.948687 test begin: paddle.nn.functional.adaptive_avg_pool3d(Tensor([66, 768, 16, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
[Prof] paddle.nn.functional.adaptive_avg_pool3d 	 paddle.nn.functional.adaptive_avg_pool3d(Tensor([66, 768, 16, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, ) 	 51093504 	 1000 	 0.549067497253418 	 0.15215206146240234 	 0.5252723693847656 	 0.13028383255004883 	 2.2720274925231934 	 0.17526984214782715 	 1.1609182357788086 	 0.1084754467010498 	 
2025-07-24 19:14:31.935064 test begin: paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([128, 16],"float32"), Tensor([128],"int64"), Tensor([16, 3175201],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, )
W0724 19:14:32.740301 156372 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.adaptive_log_softmax_with_loss 	 paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([128, 16],"float32"), Tensor([128],"int64"), Tensor([16, 3175201],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, ) 	 50805392 	 1000 	 12.674423694610596 	 8.129666566848755 	 0.009419679641723633 	 0.00659942626953125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:15:13.390222 test begin: paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([25401601, 16],"float32"), Tensor([25401601],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, )
[Prof] paddle.nn.functional.adaptive_log_softmax_with_loss 	 paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([25401601, 16],"float32"), Tensor([25401601],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, ) 	 431827345 	 1000 	 49.88307309150696 	 21.003541946411133 	 0.0072672367095947266 	 0.006204366683959961 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:17:10.011119 test begin: paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([3175201, 16],"float32"), Tensor([3175201],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, )
[Prof] paddle.nn.functional.adaptive_log_softmax_with_loss 	 paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([3175201, 16],"float32"), Tensor([3175201],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, ) 	 53978545 	 1000 	 6.734308481216431 	 3.155402898788452 	 0.0007970333099365234 	 0.0006284713745117188 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:17:25.761890 test begin: paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([8, 8],"float32"), Tensor([8],"int64"), Tensor([8, 6350401],"float32"), list[list[Tensor([8, 4],"float32"),Tensor([4, 2],"float32"),],], list[2,4,], None, )
[Prof] paddle.nn.functional.adaptive_log_softmax_with_loss 	 paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([8, 8],"float32"), Tensor([8],"int64"), Tensor([8, 6350401],"float32"), list[list[Tensor([8, 4],"float32"),Tensor([4, 2],"float32"),],], list[2,4,], None, ) 	 50803280 	 1000 	 7.961557865142822 	 4.688847541809082 	 0.006692171096801758 	 0.003983497619628906 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 19:17:58.690242 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 4233601],"float64"), 8, False, None, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 4233601],"float64"), 8, False, None, ) 	 25401606 	 1000 	 62.30644416809082 	 62.75932312011719 	 62.282904624938965 	 62.73630237579346 	 0.44915103912353516 	 0.13801050186157227 	 0.2293839454650879 	 0.04863786697387695 	 
2025-07-24 19:20:05.047548 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 8467201],"float32"), 16, False, None, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 8467201],"float32"), 16, False, None, ) 	 50803206 	 1000 	 48.7095513343811 	 42.18034768104553 	 48.6862850189209 	 42.15394401550293 	 0.759852409362793 	 0.1386878490447998 	 0.3881566524505615 	 0.04848837852478027 	 
2025-07-24 19:21:39.122722 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 8467201],"float32"), output_size=16, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 8467201],"float32"), output_size=16, ) 	 50803206 	 1000 	 48.7111496925354 	 42.18026924133301 	 48.687731981277466 	 42.15782022476196 	 0.759868860244751 	 0.13881921768188477 	 0.3881819248199463 	 0.05083179473876953 	 
2025-07-24 19:23:11.861801 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 396901, 32],"float64"), 8, False, None, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 396901, 32],"float64"), 8, False, None, ) 	 25401664 	 1000 	 0.21024346351623535 	 0.8578352928161621 	 0.18442034721374512 	 0.8343715667724609 	 0.5247297286987305 	 0.7770764827728271 	 0.26808619499206543 	 0.3970050811767578 	 
2025-07-24 19:23:14.889054 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 793801, 32],"float32"), 16, False, None, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 793801, 32],"float32"), 16, False, None, ) 	 50803264 	 1000 	 0.41277194023132324 	 1.6982307434082031 	 0.3863539695739746 	 1.675069808959961 	 0.8837196826934814 	 1.4114336967468262 	 0.4515647888183594 	 0.7218496799468994 	 
2025-07-24 19:23:20.612189 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 793801, 32],"float32"), output_size=16, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 793801, 32],"float32"), output_size=16, ) 	 50803264 	 1000 	 0.4086418151855469 	 1.7036445140838623 	 0.38457226753234863 	 1.6650476455688477 	 0.88350510597229 	 1.4114625453948975 	 0.45139646530151367 	 0.7217962741851807 	 
2025-07-24 19:23:27.573468 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([264601, 3, 32],"float64"), 8, False, None, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([264601, 3, 32],"float64"), 8, False, None, ) 	 25401696 	 1000 	 0.21036338806152344 	 3.410116195678711 	 0.1852250099182129 	 3.385098934173584 	 0.5230903625488281 	 3.326876640319824 	 0.267214298248291 	 1.699925184249878 	 
2025-07-24 19:23:36.980375 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([529201, 3, 32],"float32"), 16, False, None, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([529201, 3, 32],"float32"), 16, False, None, ) 	 50803296 	 1000 	 0.4231693744659424 	 6.800857305526733 	 0.3845832347869873 	 6.777810335159302 	 0.8835783004760742 	 6.515441656112671 	 0.4513106346130371 	 3.329216718673706 	 
2025-07-24 19:23:54.148897 test begin: paddle.nn.functional.adaptive_max_pool1d(Tensor([529201, 3, 32],"float32"), output_size=16, )
[Prof] paddle.nn.functional.adaptive_max_pool1d 	 paddle.nn.functional.adaptive_max_pool1d(Tensor([529201, 3, 32],"float32"), output_size=16, ) 	 50803296 	 1000 	 0.4110429286956787 	 6.803528547286987 	 0.3844177722930908 	 6.770700454711914 	 0.8834714889526367 	 6.512910604476929 	 0.4513828754425049 	 3.3279197216033936 	 
2025-07-24 19:24:09.978257 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 1209601, 7],"float32"), output_size=5, return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool2d 	 paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 1209601, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803242 	 1000 	 327.4026110172272 	 136.46816873550415 	 327.3895182609558 	 136.45015454292297 	 0.7784152030944824 	 0.13739633560180664 	 0.3976719379425049 	 0.06780147552490234 	 
2025-07-24 19:31:56.356701 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 1209601, 7],"float32"), output_size=list[2,5,], return_mask=False, name=None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f597615b520>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 19:46:51.173528 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 1209601, 7],"float32"), output_size=list[3,3,], return_mask=False, name=None, )
W0724 19:46:52.184862 65354 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc888793130>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 19:57:19.942133 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 1209601],"float32"), output_size=5, return_mask=False, name=None, )
W0724 19:57:21.021819 90669 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.adaptive_max_pool2d 	 paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 1209601],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803242 	 1000 	 114.19638991355896 	 56.47533202171326 	 114.18104887008667 	 56.452937602996826 	 0.8012945652008057 	 0.13779401779174805 	 0.4086277484893799 	 0.06390666961669922 	 
2025-07-24 20:00:13.431900 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 1209601],"float32"), output_size=list[2,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool2d 	 paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 1209601],"float32"), output_size=list[2,5,], return_mask=False, name=None, ) 	 50803242 	 1000 	 177.07659268379211 	 79.74437880516052 	 177.06341814994812 	 79.72430968284607 	 0.7702226638793945 	 0.1374378204345703 	 0.3934817314147949 	 0.051703453063964844 	 
2025-07-24 20:04:32.338579 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 1209601],"float32"), output_size=list[3,3,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool2d 	 paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 1209601],"float32"), output_size=list[3,3,], return_mask=False, name=None, ) 	 50803242 	 1000 	 222.3764886856079 	 97.55537223815918 	 222.3634009361267 	 97.52677845954895 	 0.7804911136627197 	 0.1373291015625 	 0.3987736701965332 	 0.05076098442077637 	 
2025-07-24 20:09:54.405592 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool2d 	 paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803298 	 1000 	 0.5827949047088623 	 4.698643445968628 	 0.5681922435760498 	 4.654128551483154 	 1.089543342590332 	 1.1751811504364014 	 0.5566554069519043 	 0.6003813743591309 	 
2025-07-24 20:10:04.359795 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=list[2,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool2d 	 paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=list[2,5,], return_mask=False, name=None, ) 	 50803298 	 1000 	 0.43742966651916504 	 2.6291961669921875 	 0.42389655113220215 	 2.608273983001709 	 1.0136785507202148 	 0.9681260585784912 	 0.5178828239440918 	 0.4945962429046631 	 
2025-07-24 20:10:10.404900 test begin: paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=list[3,3,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool2d 	 paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 518401, 7, 7],"float32"), output_size=list[3,3,], return_mask=False, name=None, ) 	 50803298 	 1000 	 0.3870961666107178 	 3.277308225631714 	 0.373668909072876 	 3.256392478942871 	 1.0356204509735107 	 0.9968223571777344 	 0.5291187763214111 	 0.5092849731445312 	 
2025-07-24 20:10:17.153252 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803690 	 1000 	 1.9676339626312256 	 10.276225805282593 	 1.9512834548950195 	 0.6573317050933838 	 1.2616653442382812 	 1.486072301864624 	 0.6447412967681885 	 0.0892176628112793 	 
2025-07-24 20:10:33.479990 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, ) 	 50803690 	 1000 	 1.5628068447113037 	 6.212671756744385 	 1.5483441352844238 	 0.8087582588195801 	 0.47473883628845215 	 0.5952696800231934 	 0.24224591255187988 	 0.07576704025268555 	 
2025-07-24 20:10:45.031661 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 103681, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, ) 	 50803690 	 1000 	 1.913759708404541 	 6.858129262924194 	 1.9002537727355957 	 0.7000534534454346 	 0.5773565769195557 	 0.8089416027069092 	 0.29495930671691895 	 0.07508206367492676 	 
2025-07-24 20:10:56.156145 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 172801, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 172801, 7, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803494 	 1000 	 30.401611804962158 	 39.87715291976929 	 30.388190984725952 	 39.858824729919434 	 0.14962029457092285 	 0.1376023292541504 	 0.07643795013427734 	 0.0702202320098877 	 
2025-07-24 20:12:07.657494 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 172801, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 172801, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, ) 	 50803494 	 1000 	 102.96085119247437 	 135.80235695838928 	 102.94746780395508 	 135.78138637542725 	 0.1495656967163086 	 0.13750100135803223 	 0.07640862464904785 	 0.0679006576538086 	 
2025-07-24 20:16:07.811364 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 172801, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 172801, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, ) 	 50803494 	 1000 	 72.09721040725708 	 91.11043763160706 	 72.07635045051575 	 91.09005999565125 	 0.14945173263549805 	 0.13753771781921387 	 0.07627701759338379 	 0.07020115852355957 	 
2025-07-24 20:18:52.332509 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 241921, 7],"float32"), output_size=5, return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 241921, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803410 	 1000 	 23.859034538269043 	 29.993053913116455 	 23.84542465209961 	 29.975154399871826 	 0.14945316314697266 	 0.13760042190551758 	 0.07625246047973633 	 0.06725835800170898 	 
2025-07-24 20:19:47.435960 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 241921, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 241921, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, ) 	 50803410 	 1000 	 106.3320848941803 	 136.8132402896881 	 106.31855797767639 	 136.79282236099243 	 0.14954018592834473 	 0.13761043548583984 	 0.07642531394958496 	 0.07023286819458008 	 
2025-07-24 20:23:51.988526 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 241921, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 241921, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, ) 	 50803410 	 1000 	 108.78060817718506 	 133.6398594379425 	 108.76707196235657 	 133.61730003356934 	 0.14931678771972656 	 0.13752245903015137 	 0.07624626159667969 	 0.07021737098693848 	 
2025-07-24 20:27:55.861464 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 241921],"float32"), output_size=5, return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 241921],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803410 	 1000 	 10.184823989868164 	 11.886854648590088 	 10.171361684799194 	 11.86772608757019 	 0.14948725700378418 	 0.13761663436889648 	 0.07631778717041016 	 0.07024359703063965 	 
2025-07-24 20:28:19.076081 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 241921],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 241921],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, ) 	 50803410 	 1000 	 31.48261332511902 	 36.02993035316467 	 31.4686176776886 	 35.991729497909546 	 0.14959383010864258 	 0.13762927055358887 	 0.07639193534851074 	 0.06842994689941406 	 
2025-07-24 20:29:28.029379 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 241921],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 241921],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, ) 	 50803410 	 1000 	 48.43051719665527 	 58.5688955783844 	 48.41679668426514 	 58.54837608337402 	 0.14916682243347168 	 0.1375739574432373 	 0.07613587379455566 	 0.07022953033447266 	 
2025-07-24 20:31:16.328300 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, ) 	 50803935 	 1000 	 1.9628729820251465 	 10.278148651123047 	 1.9480500221252441 	 0.6561493873596191 	 1.2657673358917236 	 1.4858381748199463 	 0.6460528373718262 	 0.08921480178833008 	 
2025-07-24 20:31:32.638160 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, ) 	 50803935 	 1000 	 1.5773732662200928 	 5.533665657043457 	 1.551541805267334 	 0.8073015213012695 	 0.47545599937438965 	 0.5932142734527588 	 0.24290919303894043 	 0.07571554183959961 	 
2025-07-24 20:31:43.182645 test begin: paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
[Prof] paddle.nn.functional.adaptive_max_pool3d 	 paddle.nn.functional.adaptive_max_pool3d(Tensor([69121, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, ) 	 50803935 	 1000 	 1.9164001941680908 	 6.860220432281494 	 1.9028894901275635 	 0.7000763416290283 	 0.578636884689331 	 0.8091354370117188 	 0.29560112953186035 	 0.07510924339294434 	 
2025-07-24 20:31:54.804011 test begin: paddle.nn.functional.avg_pool1d(Tensor([13, 1, 3907939],"float32"), 25, 1, 0, True, False, None, )
W0724 20:31:55.598044  7301 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.avg_pool1d 	 paddle.nn.functional.avg_pool1d(Tensor([13, 1, 3907939],"float32"), 25, 1, 0, True, False, None, ) 	 50803207 	 1000 	 36.69749355316162 	 1.5778803825378418 	 36.664799213409424 	 1.5621795654296875 	 45.7446985244751 	 3.7256782054901123 	 45.66298151016235 	 3.6365339756011963 	 
2025-07-24 20:33:24.555408 test begin: paddle.nn.functional.avg_pool1d(Tensor([13, 32567, 120],"float32"), 25, 1, 0, True, False, None, )
[Prof] paddle.nn.functional.avg_pool1d 	 paddle.nn.functional.avg_pool1d(Tensor([13, 32567, 120],"float32"), 25, 1, 0, True, False, None, ) 	 50804520 	 1000 	 7.4863362312316895 	 1.3794739246368408 	 7.453899621963501 	 1.3555431365966797 	 11.393676996231079 	 3.8432905673980713 	 11.316627025604248 	 3.749009609222412 	 
2025-07-24 20:33:50.272856 test begin: paddle.nn.functional.avg_pool1d(Tensor([16, 1, 3175201],"float32"), 25, 1, 0, True, False, None, )
[Prof] paddle.nn.functional.avg_pool1d 	 paddle.nn.functional.avg_pool1d(Tensor([16, 1, 3175201],"float32"), 25, 1, 0, True, False, None, ) 	 50803216 	 1000 	 36.69847249984741 	 1.5820505619049072 	 36.664135694503784 	 1.5638468265533447 	 45.74093985557556 	 3.726240873336792 	 45.66437125205994 	 3.6380670070648193 	 
2025-07-24 20:35:19.870588 test begin: paddle.nn.functional.avg_pool1d(Tensor([16, 2, 1587601],"float32"), 25, 1, 0, True, False, None, )
[Prof] paddle.nn.functional.avg_pool1d 	 paddle.nn.functional.avg_pool1d(Tensor([16, 2, 1587601],"float32"), 25, 1, 0, True, False, None, ) 	 50803232 	 1000 	 18.431323051452637 	 1.5812501907348633 	 18.3975248336792 	 1.5623457431793213 	 22.900388479232788 	 3.7251408100128174 	 22.82075047492981 	 3.6370835304260254 	 
2025-07-24 20:36:09.688819 test begin: paddle.nn.functional.avg_pool1d(Tensor([16, 26461, 120],"float32"), 25, 1, 0, True, False, None, )
[Prof] paddle.nn.functional.avg_pool1d 	 paddle.nn.functional.avg_pool1d(Tensor([16, 26461, 120],"float32"), 25, 1, 0, True, False, None, ) 	 50805120 	 1000 	 7.484425067901611 	 1.3812177181243896 	 7.451732397079468 	 1.3654005527496338 	 11.399024724960327 	 3.8432934284210205 	 11.322163105010986 	 3.7281181812286377 	 
2025-07-24 20:36:37.099771 test begin: paddle.nn.functional.avg_pool2d(Tensor([128, 127, 56, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([128, 127, 56, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 50978816 	 1000 	 0.6439635753631592 	 0.42204856872558594 	 0.1746523380279541 	 0.4047229290008545 	 0.35302114486694336 	 1.568269968032837 	 0.29277610778808594 	 1.4996001720428467 	 
2025-07-24 20:36:41.868184 test begin: paddle.nn.functional.avg_pool2d(Tensor([128, 256, 28, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([128, 256, 28, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51380224 	 1000 	 0.1996774673461914 	 0.41918492317199707 	 0.17908835411071777 	 0.4073524475097656 	 0.3331608772277832 	 1.5787599086761475 	 0.27373528480529785 	 1.5080921649932861 	 
2025-07-24 20:36:45.487637 test begin: paddle.nn.functional.avg_pool2d(Tensor([128, 256, 56, 28],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([128, 256, 56, 28],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51380224 	 1000 	 0.20026946067810059 	 0.41951775550842285 	 0.18034076690673828 	 0.40752077102661133 	 0.33452558517456055 	 1.576488733291626 	 0.27416110038757324 	 1.5097310543060303 	 
2025-07-24 20:36:49.110477 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 128, 256, 97],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([16, 128, 256, 97],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 50855936 	 1000 	 0.19292449951171875 	 0.409243106842041 	 0.17042255401611328 	 0.39751124382019043 	 0.33042168617248535 	 1.5603141784667969 	 0.27080440521240234 	 1.4945120811462402 	 
2025-07-24 20:36:52.692084 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 128, 97, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([16, 128, 97, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 50855936 	 1000 	 0.19088149070739746 	 0.40743279457092285 	 0.16828179359436035 	 0.3930819034576416 	 0.3289487361907959 	 1.5473220348358154 	 0.2679765224456787 	 1.4820983409881592 	 
2025-07-24 20:36:56.266093 test begin: paddle.nn.functional.avg_pool2d(Tensor([16, 49, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([16, 49, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51380224 	 1000 	 0.19638586044311523 	 0.4160573482513428 	 0.17446374893188477 	 0.40125489234924316 	 0.35465216636657715 	 1.564570665359497 	 0.2949237823486328 	 1.4973957538604736 	 
2025-07-24 20:36:59.876880 test begin: paddle.nn.functional.avg_pool2d(Tensor([4, 256, 240, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([4, 256, 240, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 58982400 	 1000 	 0.2237536907196045 	 0.48003411293029785 	 0.19582009315490723 	 0.4616246223449707 	 0.38196587562561035 	 1.806281566619873 	 0.31198859214782715 	 1.7393906116485596 	 
2025-07-24 20:37:04.040828 test begin: paddle.nn.functional.avg_pool2d(Tensor([64, 256, 56, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([64, 256, 56, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51380224 	 1000 	 0.1958925724029541 	 0.4188385009765625 	 0.1763763427734375 	 0.40699124336242676 	 0.353147029876709 	 1.5811033248901367 	 0.2909576892852783 	 1.5129950046539307 	 
2025-07-24 20:37:07.704451 test begin: paddle.nn.functional.avg_pool2d(Tensor([7, 128, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([7, 128, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 58720256 	 1000 	 0.22130131721496582 	 0.47136759757995605 	 0.19903349876403809 	 0.4589536190032959 	 0.38054442405700684 	 1.786505937576294 	 0.31992101669311523 	 1.689650058746338 	 
2025-07-24 20:37:12.702812 test begin: paddle.nn.functional.avg_pool2d(Tensor([8, 111, 240, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([8, 111, 240, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51148800 	 1000 	 0.6435930728912354 	 0.43436503410339355 	 0.17475295066833496 	 0.40466761589050293 	 0.33639001846313477 	 1.566098928451538 	 0.27686238288879395 	 1.4866571426391602 	 
2025-07-24 20:37:17.200080 test begin: paddle.nn.functional.avg_pool2d(Tensor([8, 256, 104, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([8, 256, 104, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51118080 	 1000 	 0.19444060325622559 	 0.4155721664428711 	 0.174910306930542 	 0.40337061882019043 	 0.3332960605621338 	 1.5663044452667236 	 0.27289628982543945 	 1.4918556213378906 	 
2025-07-24 20:37:20.758294 test begin: paddle.nn.functional.avg_pool2d(Tensor([8, 256, 240, 104],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.avg_pool2d 	 paddle.nn.functional.avg_pool2d(Tensor([8, 256, 240, 104],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, ) 	 51118080 	 1000 	 0.19355392456054688 	 0.4161336421966553 	 0.16611123085021973 	 0.3975687026977539 	 0.3337414264678955 	 1.5702300071716309 	 0.2647364139556885 	 1.4795572757720947 	 
2025-07-24 20:37:24.462575 test begin: paddle.nn.functional.avg_pool3d(Tensor([2, 776, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(Tensor([2, 776, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", ) 	 50855936 	 1000 	 0.17688536643981934 	 0.4258568286895752 	 0.1582932472229004 	 0.4007375240325928 	 0.42435240745544434 	 0.49779200553894043 	 0.36338233947753906 	 0.25429248809814453 	 
2025-07-24 20:37:26.938595 test begin: paddle.nn.functional.avg_pool3d(Tensor([517, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(Tensor([517, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", ) 	 50823168 	 1000 	 0.17847752571105957 	 0.4219932556152344 	 0.15955281257629395 	 0.4077732563018799 	 0.558448314666748 	 0.49869823455810547 	 0.4951198101043701 	 0.25402212142944336 	 
2025-07-24 20:37:29.564432 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([127, 2048, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([127, 2048, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 	 50978816 	 1000 	 0.23993539810180664 	 2.2434194087982178 	 0.21587610244750977 	 0.5729258060455322 	 3.1404542922973633 	 2.8877291679382324 	 3.0772101879119873 	 0.17355680465698242 	 
2025-07-24 20:37:39.444256 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([127, 256, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([127, 256, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 	 50978816 	 1000 	 0.26514339447021484 	 1.9946060180664062 	 0.2430436611175537 	 1.9807512760162354 	 2.8851072788238525 	 2.8728253841400146 	 2.8250210285186768 	 0.17382550239562988 	 
2025-07-24 20:37:48.309160 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 111, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 111, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 	 50921472 	 1000 	 0.3410167694091797 	 10.933158159255981 	 0.3188140392303467 	 10.919334173202515 	 4.430817604064941 	 4.750610589981079 	 4.37071418762207 	 1.620234727859497 	 
2025-07-24 20:38:09.693991 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 7, 111],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 7, 111],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 	 50921472 	 1000 	 0.3859570026397705 	 0.7690396308898926 	 0.3631100654602051 	 0.7552058696746826 	 3.0736498832702637 	 0.9804785251617432 	 3.0125060081481934 	 0.3342289924621582 	 
2025-07-24 20:38:16.586567 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 64, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 64, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 	 51380224 	 1000 	 0.3182706832885742 	 7.899078369140625 	 0.29616856575012207 	 0.5042774677276611 	 4.28700852394104 	 4.353865385055542 	 4.223733186721802 	 0.24708223342895508 	 
2025-07-24 20:38:34.967419 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 111, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 111, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 	 50921472 	 1000 	 0.35746240615844727 	 10.569846630096436 	 0.33533549308776855 	 10.555676221847534 	 4.422622442245483 	 4.7434141635894775 	 4.362027883529663 	 1.6163716316223145 	 
2025-07-24 20:38:56.595419 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 7, 111],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 7, 111],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 	 50921472 	 1000 	 0.41434764862060547 	 0.9847297668457031 	 0.3923523426055908 	 0.9698476791381836 	 3.062281847000122 	 0.9603638648986816 	 3.0019149780273438 	 0.32683348655700684 	 
2025-07-24 20:39:02.896974 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 507, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 507, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 	 50878464 	 1000 	 2.2151801586151123 	 50.48576283454895 	 2.193030834197998 	 3.4388225078582764 	 15.102899551391602 	 17.578962802886963 	 15.042896270751953 	 1.0574324131011963 	 
2025-07-24 20:40:29.390613 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 32401, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 32401, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", ) 	 50804768 	 1000 	 0.23961853981018066 	 2.235168218612671 	 0.2175157070159912 	 0.5711445808410645 	 3.1313183307647705 	 2.87796688079834 	 3.0713727474212646 	 0.17294692993164062 	 
2025-07-24 20:40:39.225992 test begin: paddle.nn.functional.avg_pool3d(x=Tensor([8, 4051, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.avg_pool3d 	 paddle.nn.functional.avg_pool3d(x=Tensor([8, 4051, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", ) 	 50815744 	 1000 	 0.2669520378112793 	 1.9850592613220215 	 0.2446601390838623 	 1.9644215106964111 	 2.8770620822906494 	 2.8608803749084473 	 2.8142006397247314 	 0.17196869850158691 	 
2025-07-24 20:40:48.110570 test begin: paddle.nn.functional.batch_norm(Tensor([30, 40, 50, 847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), use_global_stats=True, data_format="NHWC", )
[Prof] paddle.nn.functional.batch_norm 	 paddle.nn.functional.batch_norm(Tensor([30, 40, 50, 847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), Tensor([847],"float32"), use_global_stats=True, data_format="NHWC", ) 	 50823388 	 1000 	 0.37812256813049316 	 0.3754143714904785 	 0.35669708251953125 	 0.34548139572143555 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([30, 40, 50, 847]) and output[0] has a shape of torch.Size([30, 847, 40, 50]).
2025-07-24 20:40:55.009062 test begin: paddle.nn.functional.batch_norm(Tensor([30, 40, 706, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", )
[Prof] paddle.nn.functional.batch_norm 	 paddle.nn.functional.batch_norm(Tensor([30, 40, 706, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", ) 	 50832240 	 1000 	 0.3695344924926758 	 0.37221336364746094 	 0.35022807121276855 	 0.34282517433166504 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([30, 40, 706, 60]) and output[0] has a shape of torch.Size([30, 60, 40, 706]).
2025-07-24 20:41:01.056872 test begin: paddle.nn.functional.batch_norm(Tensor([30, 565, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", )
[Prof] paddle.nn.functional.batch_norm 	 paddle.nn.functional.batch_norm(Tensor([30, 565, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", ) 	 50850240 	 1000 	 0.37557029724121094 	 0.3818526268005371 	 0.35635972023010254 	 0.3491706848144531 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([30, 565, 50, 60]) and output[0] has a shape of torch.Size([30, 60, 565, 50]).
2025-07-24 20:41:09.670509 test begin: paddle.nn.functional.batch_norm(Tensor([424, 40, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", )
[Prof] paddle.nn.functional.batch_norm 	 paddle.nn.functional.batch_norm(Tensor([424, 40, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", ) 	 50880240 	 1000 	 0.33942151069641113 	 0.3477046489715576 	 0.32017016410827637 	 0.3152437210083008 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([424, 40, 50, 60]) and output[0] has a shape of torch.Size([424, 60, 40, 50]).
2025-07-24 20:41:15.489623 test begin: paddle.nn.functional.bilinear(Tensor([1, 3],"float32"), Tensor([1, 3],"float32"), Tensor([5644801, 3, 3],"float32"), Tensor([1, 5644801],"float32"), None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fac7154aaa0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 20:51:22.969205 test begin: paddle.nn.functional.bilinear(Tensor([25401601, 1],"float32"), Tensor([25401601, 2],"float32"), Tensor([4, 1, 2],"float32"), Tensor([1, 4],"float32"), None, )
W0724 20:51:24.373438 60090 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.bilinear 	 paddle.nn.functional.bilinear(Tensor([25401601, 1],"float32"), Tensor([25401601, 2],"float32"), Tensor([4, 1, 2],"float32"), Tensor([1, 4],"float32"), None, ) 	 76204815 	 1000 	 15.015317916870117 	 118.34120750427246 	 0.14637541770935059 	 0.07283568382263184 	 30.24374794960022 	 124.92154932022095 	 0.2509944438934326 	 0.0788111686706543 	 
2025-07-24 20:56:15.662479 test begin: paddle.nn.functional.bilinear(Tensor([3, 1],"float32"), Tensor([3, 12700801],"float32"), Tensor([4, 1, 12700801],"float32"), Tensor([1, 4],"float32"), None, )
[Prof] paddle.nn.functional.bilinear 	 paddle.nn.functional.bilinear(Tensor([3, 1],"float32"), Tensor([3, 12700801],"float32"), Tensor([4, 1, 12700801],"float32"), Tensor([1, 4],"float32"), None, ) 	 88905614 	 1000 	 3.375183582305908 	 45.9268593788147 	 0.05302166938781738 	 0.7573928833007812 	 6.646864652633667 	 50.36546611785889 	 0.09168720245361328 	 0.5579767227172852 	 
2025-07-24 20:58:03.751968 test begin: paddle.nn.functional.bilinear(Tensor([3, 1],"float32"), Tensor([3, 16934401],"float32"), Tensor([4, 1, 16934401],"float32"), Tensor([1, 4],"float32"), None, )
[Prof] paddle.nn.functional.bilinear 	 paddle.nn.functional.bilinear(Tensor([3, 1],"float32"), Tensor([3, 16934401],"float32"), Tensor([4, 1, 16934401],"float32"), Tensor([1, 4],"float32"), None, ) 	 118540814 	 1000 	 4.548190116882324 	 59.408156633377075 	 0.0573880672454834 	 0.7862048149108887 	 8.778653621673584 	 66.58774447441101 	 0.09797978401184082 	 0.606670618057251 	 
2025-07-24 21:00:25.247881 test begin: paddle.nn.functional.bilinear(Tensor([5, 5],"float32"), Tensor([5, 10161],"float32"), Tensor([1000, 5, 10161],"float32"), Tensor([1, 1000],"float32"), None, )
[Prof] paddle.nn.functional.bilinear 	 paddle.nn.functional.bilinear(Tensor([5, 5],"float32"), Tensor([5, 10161],"float32"), Tensor([1000, 5, 10161],"float32"), Tensor([1, 1000],"float32"), None, ) 	 50856830 	 1000 	 30.067948818206787 	 42.171430349349976 	 0.007636547088623047 	 0.00027370452880859375 	 38.26180648803711 	 104.36460542678833 	 0.0064470767974853516 	 0.00024628639221191406 	 
2025-07-24 21:04:01.255534 test begin: paddle.nn.functional.bilinear(Tensor([5, 5],"float32"), Tensor([5, 4],"float32"), Tensor([2540161, 5, 4],"float32"), Tensor([1, 2540161],"float32"), None, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f431e9027d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 21:14:08.546844 test begin: paddle.nn.functional.bilinear(Tensor([50803201, 1],"float32"), Tensor([50803201, 2],"float32"), Tensor([4, 1, 2],"float32"), Tensor([1, 4],"float32"), None, )
W0724 21:14:10.986737 120475 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.bilinear 	 paddle.nn.functional.bilinear(Tensor([50803201, 1],"float32"), Tensor([50803201, 2],"float32"), Tensor([4, 1, 2],"float32"), Tensor([1, 4],"float32"), None, ) 	 152409615 	 1000 	 29.980194091796875 	 236.66155791282654 	 0.15494608879089355 	 0.07419347763061523 	 60.27458429336548 	 249.6769733428955 	 0.2790868282318115 	 0.09016728401184082 	 
2025-07-24 21:23:52.473829 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([16, 10164, 313],"float32"), Tensor([16, 10164, 313],"float32"), weight=Tensor([16, 10164, 313],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([16, 10164, 313],"float32"), Tensor([16, 10164, 313],"float32"), weight=Tensor([16, 10164, 313],"float32"), reduction="sum", ) 	 152703936 	 1000 	 1.0547924041748047 	 1.0607941150665283 	 0.2690088748931885 	 0.21533489227294922 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 21:23:59.873561 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([16, 11109, 286],"float32"), Tensor([16, 11109, 286],"float32"), weight=Tensor([16, 11109, 286],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([16, 11109, 286],"float32"), Tensor([16, 11109, 286],"float32"), weight=Tensor([16, 11109, 286],"float32"), reduction="sum", ) 	 152504352 	 1000 	 1.0468709468841553 	 1.0551438331604004 	 0.2670552730560303 	 0.2164292335510254 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 21:24:06.331063 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([16, 12096, 263],"float32"), Tensor([16, 12096, 263],"float32"), weight=Tensor([16, 12096, 263],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([16, 12096, 263],"float32"), Tensor([16, 12096, 263],"float32"), weight=Tensor([16, 12096, 263],"float32"), reduction="sum", ) 	 152699904 	 1000 	 1.0484769344329834 	 1.0549578666687012 	 0.2675168514251709 	 0.2152564525604248 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 21:24:12.769073 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([16, 39691, 80],"float32"), Tensor([16, 39691, 80],"float32"), weight=Tensor([16, 39691, 80],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([16, 39691, 80],"float32"), Tensor([16, 39691, 80],"float32"), weight=Tensor([16, 39691, 80],"float32"), reduction="sum", ) 	 152413440 	 1000 	 1.0477464199066162 	 1.0544805526733398 	 0.2670290470123291 	 0.21623897552490234 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 21:24:19.142569 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([53, 12096, 80],"float32"), Tensor([53, 12096, 80],"float32"), weight=Tensor([53, 12096, 80],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([53, 12096, 80],"float32"), Tensor([53, 12096, 80],"float32"), weight=Tensor([53, 12096, 80],"float32"), reduction="sum", ) 	 153861120 	 1000 	 1.056311845779419 	 1.0754165649414062 	 0.26955127716064453 	 0.21691679954528809 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 21:24:25.525538 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([58, 11109, 80],"float32"), Tensor([58, 11109, 80],"float32"), weight=Tensor([58, 11109, 80],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([58, 11109, 80],"float32"), Tensor([58, 11109, 80],"float32"), weight=Tensor([58, 11109, 80],"float32"), reduction="sum", ) 	 154637280 	 1000 	 1.0649709701538086 	 1.072052001953125 	 0.2709622383117676 	 0.21799921989440918 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 21:24:31.985923 test begin: paddle.nn.functional.binary_cross_entropy(Tensor([63, 10164, 80],"float32"), Tensor([63, 10164, 80],"float32"), weight=Tensor([63, 10164, 80],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.binary_cross_entropy 	 paddle.nn.functional.binary_cross_entropy(Tensor([63, 10164, 80],"float32"), Tensor([63, 10164, 80],"float32"), weight=Tensor([63, 10164, 80],"float32"), reduction="sum", ) 	 153679680 	 1000 	 1.0568416118621826 	 1.083003282546997 	 0.2692599296569824 	 0.2166891098022461 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 21:24:40.934897 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([16, 300, 10585],"float32"), Tensor([16, 300, 10585],"float32"), weight=Tensor([16, 300, 10585],"float32"), reduction="none", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([16, 300, 10585],"float32"), Tensor([16, 300, 10585],"float32"), weight=Tensor([16, 300, 10585],"float32"), reduction="none", ) 	 152424000 	 1000 	 1.03859281539917 	 2.2140326499938965 	 0.3540680408477783 	 0.37729454040527344 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 21:24:49.474562 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([16, 39691, 80],"float32"), Tensor([16, 39691, 80],"float32"), weight=Tensor([16, 39691, 80],"float32"), reduction="none", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([16, 39691, 80],"float32"), Tensor([16, 39691, 80],"float32"), weight=Tensor([16, 39691, 80],"float32"), reduction="none", ) 	 152413440 	 1000 	 1.0393812656402588 	 2.2175252437591553 	 0.3540151119232178 	 0.3772873878479004 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 21:24:58.384340 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([2117, 300, 80],"float32"), Tensor([2117, 300, 80],"float32"), weight=Tensor([2117, 300, 80],"float32"), reduction="none", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([2117, 300, 80],"float32"), Tensor([2117, 300, 80],"float32"), weight=Tensor([2117, 300, 80],"float32"), reduction="none", ) 	 152424000 	 1000 	 1.059718132019043 	 2.2160532474517822 	 0.35394954681396484 	 0.377286434173584 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 21:25:07.262820 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([300, 169345],"float32"), Tensor([300, 169345],"float32"), weight=Tensor([300, 169345],"float32"), reduction="none", pos_weight=None, )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([300, 169345],"float32"), Tensor([300, 169345],"float32"), weight=Tensor([300, 169345],"float32"), reduction="none", pos_weight=None, ) 	 152410500 	 1000 	 1.038590431213379 	 2.215322256088257 	 0.3540928363800049 	 0.377272367477417 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 21:25:16.051735 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([50804, 1000],"float32"), Tensor([50804, 1000],"float32"), weight=Tensor([50804, 1000],"float32"), reduction="none", pos_weight=None, )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([50804, 1000],"float32"), Tensor([50804, 1000],"float32"), weight=Tensor([50804, 1000],"float32"), reduction="none", pos_weight=None, ) 	 152412000 	 1000 	 1.039022445678711 	 2.2153520584106445 	 0.35378003120422363 	 0.37726902961730957 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 21:25:24.462746 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([512, 28, 3544],"float32"), Tensor([512, 28, 3544],"float32"), weight=Tensor([512, 1, 3544],"float32"), reduction="mean", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([512, 28, 3544],"float32"), Tensor([512, 28, 3544],"float32"), weight=Tensor([512, 1, 3544],"float32"), reduction="mean", ) 	 103428096 	 1000 	 1.044480562210083 	 2.2427380084991455 	 0.21294450759887695 	 0.2864809036254883 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 21:25:31.234098 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([512, 3544, 28],"float32"), Tensor([512, 3544, 28],"float32"), weight=Tensor([512, 3544, 1],"float32"), reduction="mean", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([512, 3544, 28],"float32"), Tensor([512, 3544, 28],"float32"), weight=Tensor([512, 3544, 1],"float32"), reduction="mean", ) 	 103428096 	 1000 	 1.0425407886505127 	 2.236466884613037 	 0.2127056121826172 	 0.2858250141143799 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 21:25:39.814791 test begin: paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), weight=Tensor([64801, 1, 1],"float32"), reduction="mean", )
[Prof] paddle.nn.functional.binary_cross_entropy_with_logits 	 paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([64801, 28, 28],"float32"), Tensor([64801, 28, 28],"float32"), weight=Tensor([64801, 1, 1],"float32"), reduction="mean", ) 	 101672769 	 1000 	 1.0373620986938477 	 2.233043909072876 	 0.21155524253845215 	 0.28509092330932617 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 21:25:46.709930 test begin: paddle.nn.functional.celu(Tensor([1587601, 4, 4],"float64"), 0.2, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([1587601, 4, 4],"float64"), 0.2, None, ) 	 25401616 	 1000 	 0.30716514587402344 	 0.30401158332824707 	 0.2978987693786621 	 0.2878115177154541 	 0.4470951557159424 	 0.44939708709716797 	 0.39692091941833496 	 0.36682868003845215 	 
2025-07-24 21:25:49.327274 test begin: paddle.nn.functional.celu(Tensor([1587601, 4, 4],"float64"), 1.0, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([1587601, 4, 4],"float64"), 1.0, None, ) 	 25401616 	 1000 	 0.3092458248138428 	 0.3039686679840088 	 0.2999002933502197 	 0.2879602909088135 	 0.44709348678588867 	 0.44940733909606934 	 0.3955392837524414 	 0.3868072032928467 	 
2025-07-24 21:25:51.898583 test begin: paddle.nn.functional.celu(Tensor([2, 3175201, 4],"float64"), 0.2, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([2, 3175201, 4],"float64"), 0.2, None, ) 	 25401608 	 1000 	 0.30704641342163086 	 0.30393218994140625 	 0.29834794998168945 	 0.28773021697998047 	 0.4470546245574951 	 0.44942378997802734 	 0.3760249614715576 	 0.38701820373535156 	 
2025-07-24 21:25:54.518324 test begin: paddle.nn.functional.celu(Tensor([2, 3175201, 4],"float64"), 1.0, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([2, 3175201, 4],"float64"), 1.0, None, ) 	 25401608 	 1000 	 0.3089001178741455 	 0.30408740043640137 	 0.29975390434265137 	 0.28793811798095703 	 0.44705629348754883 	 0.4493863582611084 	 0.38692641258239746 	 0.37947559356689453 	 
2025-07-24 21:25:57.142513 test begin: paddle.nn.functional.celu(Tensor([2, 4, 3175201],"float64"), 0.2, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([2, 4, 3175201],"float64"), 0.2, None, ) 	 25401608 	 1000 	 0.3070492744445801 	 0.31518054008483887 	 0.2983205318450928 	 0.2891566753387451 	 0.4470062255859375 	 0.45066213607788086 	 0.3967607021331787 	 0.38398075103759766 	 
2025-07-24 21:25:59.796782 test begin: paddle.nn.functional.celu(Tensor([2, 4, 3175201],"float64"), 1.0, None, )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(Tensor([2, 4, 3175201],"float64"), 1.0, None, ) 	 25401608 	 1000 	 0.3070387840270996 	 0.3039100170135498 	 0.2983565330505371 	 0.287764310836792 	 0.44701361656188965 	 0.44939517974853516 	 0.39673805236816406 	 0.3873441219329834 	 
2025-07-24 21:26:02.390533 test begin: paddle.nn.functional.celu(x=Tensor([1587601, 4, 4],"float64"), )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(x=Tensor([1587601, 4, 4],"float64"), ) 	 25401616 	 1000 	 0.3085441589355469 	 0.31814050674438477 	 0.298137903213501 	 0.28776073455810547 	 0.44702577590942383 	 0.44945263862609863 	 0.39678502082824707 	 0.3623170852661133 	 
2025-07-24 21:26:06.746684 test begin: paddle.nn.functional.celu(x=Tensor([2, 3175201, 4],"float64"), )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(x=Tensor([2, 3175201, 4],"float64"), ) 	 25401608 	 1000 	 0.30838584899902344 	 0.3160526752471924 	 0.2982187271118164 	 0.2893638610839844 	 0.4471273422241211 	 0.44940614700317383 	 0.3970348834991455 	 0.38596653938293457 	 
2025-07-24 21:26:10.319483 test begin: paddle.nn.functional.celu(x=Tensor([2, 4, 3175201],"float64"), )
[Prof] paddle.nn.functional.celu 	 paddle.nn.functional.celu(x=Tensor([2, 4, 3175201],"float64"), ) 	 25401608 	 1000 	 0.3070240020751953 	 0.30396461486816406 	 0.29806971549987793 	 0.2879769802093506 	 0.4470555782318115 	 0.44941043853759766 	 0.39667820930480957 	 0.38735151290893555 	 
2025-07-24 21:26:12.922562 test begin: paddle.nn.functional.channel_shuffle(Tensor([176401, 4, 4, 9],"float64"), 3, "NHWC", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([176401, 4, 4, 9],"float64"), 3, "NHWC", ) 	 25401744 	 1000 	 0.31527209281921387 	 0.3008873462677002 	 0.30488157272338867 	 0.2786746025085449 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([176401, 4, 4, 9]) and output[0] has a shape of torch.Size([176401, 9, 4, 4]).
2025-07-24 21:26:15.008499 test begin: paddle.nn.functional.channel_shuffle(Tensor([176401, 4, 4, 9],"float64"), 3, "NHWC", None, )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([176401, 4, 4, 9],"float64"), 3, "NHWC", None, ) 	 25401744 	 1000 	 0.31522703170776367 	 0.29747557640075684 	 0.3050880432128906 	 0.27881860733032227 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([176401, 4, 4, 9]) and output[0] has a shape of torch.Size([176401, 9, 4, 4]).
2025-07-24 21:26:16.989286 test begin: paddle.nn.functional.channel_shuffle(Tensor([176401, 9, 4, 4],"float64"), 3, "NCHW", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([176401, 9, 4, 4],"float64"), 3, "NCHW", ) 	 25401744 	 1000 	 0.3134918212890625 	 0.3067319393157959 	 0.30331921577453613 	 0.2855517864227295 	 0.31379032135009766 	 0.3027794361114502 	 0.2645089626312256 	 0.23495149612426758 	 
2025-07-24 21:26:19.271186 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 352801, 4, 9],"float64"), 3, "NHWC", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 352801, 4, 9],"float64"), 3, "NHWC", ) 	 25401672 	 1000 	 0.3146228790283203 	 1.3037610054016113 	 0.30427074432373047 	 1.2851746082305908 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 352801, 4, 9]) and output[0] has a shape of torch.Size([2, 9, 352801, 4]).
2025-07-24 21:26:22.319481 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 352801, 4, 9],"float64"), 3, "NHWC", None, )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 352801, 4, 9],"float64"), 3, "NHWC", None, ) 	 25401672 	 1000 	 0.31461429595947266 	 1.3067359924316406 	 0.30446815490722656 	 1.2859916687011719 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 352801, 4, 9]) and output[0] has a shape of torch.Size([2, 9, 352801, 4]).
2025-07-24 21:26:25.310297 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 4, 352801, 9],"float64"), 3, "NHWC", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 4, 352801, 9],"float64"), 3, "NHWC", ) 	 25401672 	 1000 	 0.3146250247955322 	 1.3052265644073486 	 0.3043959140777588 	 1.2865512371063232 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 4, 352801, 9]) and output[0] has a shape of torch.Size([2, 9, 4, 352801]).
2025-07-24 21:26:28.350412 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 4, 352801, 9],"float64"), 3, "NHWC", None, )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 4, 352801, 9],"float64"), 3, "NHWC", None, ) 	 25401672 	 1000 	 0.31464195251464844 	 1.3038065433502197 	 0.3039085865020752 	 1.2851758003234863 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([2, 4, 352801, 9]) and output[0] has a shape of torch.Size([2, 9, 4, 352801]).
2025-07-24 21:26:31.445640 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 9, 352801, 4],"float64"), 3, "NCHW", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 9, 352801, 4],"float64"), 3, "NCHW", ) 	 25401672 	 1000 	 0.3150975704193115 	 0.3195943832397461 	 0.3036351203918457 	 0.28385448455810547 	 0.3147115707397461 	 0.3025546073913574 	 0.2652750015258789 	 0.2344529628753662 	 
2025-07-24 21:26:33.770455 test begin: paddle.nn.functional.channel_shuffle(Tensor([2, 9, 4, 352801],"float64"), 3, "NCHW", )
[Prof] paddle.nn.functional.channel_shuffle 	 paddle.nn.functional.channel_shuffle(Tensor([2, 9, 4, 352801],"float64"), 3, "NCHW", ) 	 25401672 	 1000 	 0.31528687477111816 	 0.31082653999328613 	 0.3033027648925781 	 0.2748556137084961 	 0.314868688583374 	 0.3028395175933838 	 0.2641792297363281 	 0.20666289329528809 	 
2025-07-24 21:26:39.078403 test begin: paddle.nn.functional.conv1d(Tensor([16, 125, 25500],"float32"), Tensor([1, 125, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
W0724 21:26:40.345542 152043 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([16, 125, 25500],"float32"), Tensor([1, 125, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 51000126 	 1000 	 0.3266904354095459 	 0.16307806968688965 	 0.10858392715454102 	 0.08304238319396973 	 0.8263394832611084 	 0.33690524101257324 	 0.14084124565124512 	 0.06864452362060547 	 
2025-07-24 21:26:42.430895 test begin: paddle.nn.functional.conv1d(Tensor([16, 64, 49613],"float32"), Tensor([1, 64, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([16, 64, 49613],"float32"), Tensor([1, 64, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50803777 	 1000 	 0.3369147777557373 	 0.17014741897583008 	 0.11460614204406738 	 0.08586263656616211 	 0.734180212020874 	 0.36008358001708984 	 0.12502145767211914 	 0.07339763641357422 	 
2025-07-24 21:26:45.112204 test begin: paddle.nn.functional.conv1d(Tensor([28, 32, 58081],"float32"), Tensor([1, 32, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([28, 32, 58081],"float32"), Tensor([1, 32, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 52040609 	 1000 	 0.3962421417236328 	 0.1851634979248047 	 0.13404250144958496 	 0.09459686279296875 	 0.7689034938812256 	 0.39429569244384766 	 0.13097786903381348 	 0.08038330078125 	 
2025-07-24 21:26:47.728957 test begin: paddle.nn.functional.conv1d(Tensor([28, 32, 58081],"float32"), Tensor([32, 32, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([28, 32, 58081],"float32"), Tensor([32, 32, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 52041632 	 1000 	 0.6784143447875977 	 0.995295524597168 	 0.17317676544189453 	 0.5084786415100098 	 1.2556111812591553 	 12.788375854492188 	 0.16045808792114258 	 2.608661651611328 	 
2025-07-24 21:27:05.312377 test begin: paddle.nn.functional.conv1d(Tensor([32, 32, 49613],"float32"), Tensor([1, 32, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([32, 32, 49613],"float32"), Tensor([1, 32, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50803745 	 1000 	 0.3845088481903076 	 0.17797541618347168 	 0.1308896541595459 	 0.09091591835021973 	 0.7401957511901855 	 0.3707249164581299 	 0.12713098526000977 	 0.07555508613586426 	 
2025-07-24 21:27:07.858129 test begin: paddle.nn.functional.conv1d(Tensor([32, 32, 49613],"float32"), Tensor([32, 32, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([32, 32, 49613],"float32"), Tensor([32, 32, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50804768 	 1000 	 0.6569106578826904 	 0.9540131092071533 	 0.16799402236938477 	 0.4845919609069824 	 1.2210569381713867 	 11.068831443786621 	 0.15607357025146484 	 2.2571818828582764 	 
2025-07-24 21:27:23.928474 test begin: paddle.nn.functional.conv1d(Tensor([32, 32, 58081],"float32"), Tensor([32, 32, 49613],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fda6bef2740>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753364245 (unix time) try "date -d @1753364245" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d58a) received by PID 120202 (TID 0x7fda674f8640) from PID 120202 ***]

2025-07-24 21:37:32.724357 test begin: paddle.nn.functional.conv1d(Tensor([32, 64, 25500],"float32"), Tensor([1, 64, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
W0724 21:37:33.716817 12038 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 21:37:33.727684 12038 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv1d 	 paddle.nn.functional.conv1d(Tensor([32, 64, 25500],"float32"), Tensor([1, 64, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", ) 	 52224065 	 1000 	 0.34979915618896484 	 0.1715526580810547 	 0.11707282066345215 	 0.08733463287353516 	 0.6373670101165771 	 0.3491659164428711 	 0.08144831657409668 	 0.07089662551879883 	 
2025-07-24 21:37:40.640301 test begin: paddle.nn.functional.conv1d_transpose(Tensor([1, 24807, 7],"float32"), Tensor([24807, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([1, 24807, 7],"float32"), Tensor([24807, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50978641 	 1000 	 2.2538704872131348 	 2.3606390953063965 	 0.7684669494628906 	 0.8045716285705566 	 1.3050079345703125 	 1.0698974132537842 	 0.26453661918640137 	 0.2733123302459717 	 
2025-07-24 21:37:48.517710 test begin: paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 7],"float32"), Tensor([512, 12404, 8],"float32"), bias=Tensor([12404],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 7],"float32"), Tensor([512, 12404, 8],"float32"), bias=Tensor([12404],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50822772 	 1000 	 0.26637887954711914 	 0.26802802085876465 	 0.09028196334838867 	 0.09137940406799316 	 21.206172943115234 	 9.577104330062866 	 4.330519914627075 	 2.442117214202881 	 
2025-07-24 21:38:21.842408 test begin: paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 7],"float32"), Tensor([512, 256, 388],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 7],"float32"), Tensor([512, 256, 388],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50859776 	 1000 	 0.26334428787231445 	 0.2654857635498047 	 0.08968973159790039 	 0.09051108360290527 	 21.407859086990356 	 20.109134912490845 	 4.368211030960083 	 4.10280442237854 	 
2025-07-24 21:39:04.876944 test begin: paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 99226],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 99226],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 51852544 	 1000 	 14.517149686813354 	 14.638028144836426 	 4.94975471496582 	 4.992008686065674 	 40.01811170578003 	 26.74510145187378 	 6.815835475921631 	 4.547950983047485 	 
2025-07-24 21:40:43.617857 test begin: paddle.nn.functional.conv1d_transpose(Tensor([14176, 512, 7],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([14176, 512, 7],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 51855616 	 1000 	 62.79682493209839 	 63.22161149978638 	 21.4124493598938 	 21.558384656906128 	 127.81027889251709 	 128.5707926750183 	 21.841771602630615 	 21.854897022247314 	 
2025-07-24 21:47:09.129805 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 24807, 7],"float32"), Tensor([24807, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 24807, 7],"float32"), Tensor([24807, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 51152290 	 1000 	 2.2521095275878906 	 2.33243989944458 	 0.7675065994262695 	 0.7944173812866211 	 1.3191773891448975 	 1.5999839305877686 	 0.26958537101745605 	 0.41007137298583984 	 
2025-07-24 21:47:17.503653 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 256, 28],"float32"), Tensor([256, 128, 1551],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 256, 28],"float32"), Tensor([256, 128, 1551],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50837632 	 1000 	 0.4902489185333252 	 0.49350643157958984 	 0.16663241386413574 	 0.16830658912658691 	 17.701197147369385 	 39.91594386100769 	 3.608480453491211 	 8.139505624771118 	 
2025-07-24 21:48:18.034470 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 256, 28],"float32"), Tensor([256, 24807, 8],"float32"), bias=Tensor([24807],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 256, 28],"float32"), Tensor([256, 24807, 8],"float32"), bias=Tensor([24807],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50843879 	 1000 	 0.5419831275939941 	 1.0564687252044678 	 0.18408679962158203 	 0.35642552375793457 	 43.232539892196655 	 20.336419582366943 	 7.346773147583008 	 5.1865553855896 	 
2025-07-24 21:49:25.400574 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 256, 99226],"float32"), Tensor([256, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 256, 99226],"float32"), Tensor([256, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 51065984 	 1000 	 7.89018440246582 	 7.97337007522583 	 2.691520929336548 	 2.71852707862854 	 17.119349479675293 	 13.62173867225647 	 2.9145781993865967 	 2.3166213035583496 	 
2025-07-24 21:50:14.662979 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 49613, 28],"float32"), Tensor([49613, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 49613, 28],"float32"), Tensor([49613, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 53582168 	 1000 	 4.406792163848877 	 4.668441295623779 	 1.5053443908691406 	 1.5906906127929688 	 1.831923484802246 	 1.8895292282104492 	 0.3120546340942383 	 0.48459672927856445 	 
2025-07-24 21:50:28.379008 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 49613],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 49613],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 51852544 	 1000 	 14.528692722320557 	 14.662157773971558 	 4.955209732055664 	 4.999222993850708 	 29.526435136795044 	 26.745798587799072 	 5.035171031951904 	 4.546955347061157 	 
2025-07-24 21:51:56.790035 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 7],"float32"), Tensor([512, 12404, 8],"float32"), bias=Tensor([12404],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 7],"float32"), Tensor([512, 12404, 8],"float32"), bias=Tensor([12404],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50826356 	 1000 	 0.4763951301574707 	 0.8862559795379639 	 0.16158723831176758 	 0.30121898651123047 	 21.366591215133667 	 9.59224820137024 	 4.464485168457031 	 2.4466216564178467 	 
2025-07-24 21:52:30.155414 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 7],"float32"), Tensor([512, 256, 388],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 7],"float32"), Tensor([512, 256, 388],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 50863360 	 1000 	 0.4689667224884033 	 0.47420334815979004 	 0.15980815887451172 	 0.1616811752319336 	 20.760019063949585 	 20.088006258010864 	 3.5303962230682373 	 4.097826719284058 	 
2025-07-24 21:53:12.899964 test begin: paddle.nn.functional.conv1d_transpose(Tensor([2, 907201, 28],"float32"), Tensor([907201, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([2, 907201, 28],"float32"), Tensor([907201, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 979777208 	 1000 	 80.83145356178284 	 84.77128267288208 	 27.561712980270386 	 28.9055757522583 	 35.6877920627594 	 36.03620505332947 	 6.089265823364258 	 7.375072717666626 	 
2025-07-24 21:57:27.245968 test begin: paddle.nn.functional.conv1d_transpose(Tensor([7088, 256, 28],"float32"), Tensor([256, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
[Prof] paddle.nn.functional.conv1d_transpose 	 paddle.nn.functional.conv1d_transpose(Tensor([7088, 256, 28],"float32"), Tensor([256, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", ) 	 51069056 	 1000 	 8.87408185005188 	 8.973183155059814 	 3.0268964767456055 	 3.061939239501953 	 23.83300018310547 	 22.0722599029541 	 4.069347620010376 	 3.7504138946533203 	 
2025-07-24 21:58:33.797736 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 191, 260],"float32"), Tensor([1, 1, 191, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 191, 260],"float32"), Tensor([1, 1, 191, 4],"float32"), ) 	 50852604 	 1000 	 0.37549781799316406 	 0.37759971618652344 	 0.33236074447631836 	 0.35758137702941895 	 69.89817214012146 	 4.017460823059082 	 23.76083254814148 	 1.0275530815124512 	 
2025-07-24 21:59:49.519769 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 191, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 191, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50851856 	 1000 	 1.093817949295044 	 1.1052355766296387 	 1.0516526699066162 	 1.083336353302002 	 11.756755113601685 	 13.102880239486694 	 3.9975779056549072 	 4.46674108505249 	 
2025-07-24 22:00:18.352380 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 192, 259],"float32"), Tensor([1, 1, 192, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 192, 259],"float32"), Tensor([1, 1, 192, 4],"float32"), ) 	 50922240 	 1000 	 0.9635665416717529 	 1.1839208602905273 	 0.49166345596313477 	 1.163881778717041 	 70.26989793777466 	 70.72183871269226 	 23.8920419216156 	 24.0449538230896 	 
2025-07-24 22:02:42.523636 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 192, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 192, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50921488 	 1000 	 1.0967199802398682 	 1.105090856552124 	 1.0462987422943115 	 1.0847928524017334 	 11.912856817245483 	 13.120708465576172 	 4.070918083190918 	 4.472368001937866 	 
2025-07-24 22:03:12.643841 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 193, 258],"float32"), Tensor([1, 1, 193, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 193, 258],"float32"), Tensor([1, 1, 193, 4],"float32"), ) 	 50989828 	 1000 	 0.9849882125854492 	 0.3753950595855713 	 0.5041940212249756 	 0.35489940643310547 	 70.82811117172241 	 71.27833604812622 	 24.08464217185974 	 24.232215642929077 	 
2025-07-24 22:05:37.171987 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 193, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 193, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50989072 	 1000 	 1.5504090785980225 	 1.108903169631958 	 1.057447910308838 	 1.088489294052124 	 11.962708234786987 	 13.149088144302368 	 4.0613112449646 	 4.4820075035095215 	 
2025-07-24 22:06:07.802139 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 193],"float32"), Tensor([1, 1, 4, 193],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 193],"float32"), Tensor([1, 1, 4, 193],"float32"), ) 	 50989828 	 1000 	 1.3622376918792725 	 2.8750767707824707 	 0.6959168910980225 	 2.8532912731170654 	 58.7624077796936 	 58.78619575500488 	 19.983705282211304 	 19.990018129348755 	 
2025-07-24 22:08:10.730992 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 193],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 193],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50989072 	 1000 	 1.104771375656128 	 1.1131057739257812 	 1.0628042221069336 	 1.0914490222930908 	 12.046056747436523 	 13.157899141311646 	 4.111385822296143 	 4.4834887981414795 	 
2025-07-24 22:08:39.975857 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 68161552 	 1000 	 1.4797298908233643 	 1.4857218265533447 	 1.43178129196167 	 1.4651870727539062 	 15.872438669204712 	 17.626044273376465 	 5.393450975418091 	 6.009711980819702 	 
2025-07-24 22:09:19.720786 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 192],"float32"), Tensor([1, 1, 4, 192],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 192],"float32"), Tensor([1, 1, 4, 192],"float32"), ) 	 50922240 	 1000 	 2.310807466506958 	 2.2542576789855957 	 1.1810030937194824 	 2.228761672973633 	 58.44307851791382 	 58.45940637588501 	 19.875184059143066 	 19.87487030029297 	 
2025-07-24 22:11:22.223366 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 192],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 192],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50921488 	 1000 	 1.1076970100402832 	 1.1974766254425049 	 1.0636875629425049 	 1.170952320098877 	 11.941621780395508 	 13.136228084564209 	 4.086353778839111 	 4.47761082649231 	 
2025-07-24 22:11:52.187007 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 68690960 	 1000 	 1.483891487121582 	 1.4920873641967773 	 1.4375450611114502 	 1.4579951763153076 	 16.091484785079956 	 17.76689076423645 	 5.450396537780762 	 6.054370641708374 	 
2025-07-24 22:12:33.014376 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 191],"float32"), Tensor([1, 1, 4, 191],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 191],"float32"), Tensor([1, 1, 4, 191],"float32"), ) 	 50852604 	 1000 	 2.839773416519165 	 2.8673958778381348 	 2.7972323894500732 	 2.8476550579071045 	 57.688804149627686 	 3.2685694694519043 	 19.616466760635376 	 0.8334574699401855 	 
2025-07-24 22:13:41.509351 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 191],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 191],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50851856 	 1000 	 1.1003549098968506 	 1.107530117034912 	 1.0582194328308105 	 1.086812973022461 	 12.013415336608887 	 13.12166714668274 	 4.0873942375183105 	 4.472188949584961 	 
2025-07-24 22:14:10.675184 test begin: paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 69222416 	 1000 	 1.4925580024719238 	 1.504812479019165 	 1.449037790298462 	 1.4841103553771973 	 16.1567165851593 	 17.902457237243652 	 5.525737285614014 	 6.104527711868286 	 
2025-07-24 22:14:50.123715 test begin: paddle.nn.functional.conv2d(Tensor([752, 1, 260, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([752, 1, 260, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50835216 	 1000 	 1.1000018119812012 	 1.1060729026794434 	 1.0551204681396484 	 1.0859854221343994 	 12.629587173461914 	 12.73146939277649 	 4.3048951625823975 	 4.341720342636108 	 
2025-07-24 22:15:19.452859 test begin: paddle.nn.functional.conv2d(Tensor([758, 1, 259, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([758, 1, 259, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50847414 	 1000 	 1.1026027202606201 	 1.4313197135925293 	 1.0584867000579834 	 1.411334753036499 	 12.601325511932373 	 12.69416332244873 	 4.294403076171875 	 4.325858116149902 	 
2025-07-24 22:15:49.648404 test begin: paddle.nn.functional.conv2d(Tensor([764, 1, 258, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), )
[Prof] paddle.nn.functional.conv2d 	 paddle.nn.functional.conv2d(Tensor([764, 1, 258, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), ) 	 50854912 	 1000 	 1.0999712944030762 	 1.1073288917541504 	 1.0563137531280518 	 1.0759837627410889 	 12.502177715301514 	 12.628366470336914 	 4.261040925979614 	 4.301773548126221 	 
2025-07-24 22:16:18.775332 test begin: paddle.nn.functional.conv2d_transpose(Tensor([16, 32, 320, 320],"float32"), Tensor([32, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([16, 32, 320, 320],"float32"), Tensor([32, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 52428929 	 1000 	 0.6257779598236084 	 0.6287219524383545 	 0.21330928802490234 	 0.21429657936096191 	 0.6632130146026611 	 0.6188082695007324 	 0.09680461883544922 	 0.1052250862121582 	 
2025-07-24 22:16:22.323398 test begin: paddle.nn.functional.conv2d_transpose(Tensor([16, 64, 156, 320],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([16, 64, 156, 320],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 51118337 	 1000 	 0.5173656940460205 	 0.5338201522827148 	 0.17633318901062012 	 0.17747998237609863 	 0.5629415512084961 	 0.5362060070037842 	 0.08172035217285156 	 0.09111142158508301 	 
2025-07-24 22:16:25.492181 test begin: paddle.nn.functional.conv2d_transpose(Tensor([16, 64, 320, 156],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([16, 64, 320, 156],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 51118337 	 1000 	 0.5173566341400146 	 0.520681619644165 	 0.17633605003356934 	 0.17749381065368652 	 0.5477793216705322 	 0.5358250141143799 	 0.07985901832580566 	 0.09079408645629883 	 
2025-07-24 22:16:28.525111 test begin: paddle.nn.functional.conv2d_transpose(Tensor([4, 56, 480, 480],"float32"), Tensor([56, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([4, 56, 480, 480],"float32"), Tensor([56, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 51609825 	 1000 	 0.5344936847686768 	 0.5367870330810547 	 0.182175874710083 	 0.18297195434570312 	 0.6127088069915771 	 0.605560302734375 	 0.08934164047241211 	 0.10291194915771484 	 
2025-07-24 22:16:31.742999 test begin: paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 414, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 414, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 50872577 	 1000 	 0.5149519443511963 	 0.5176215171813965 	 0.1763627529144287 	 0.17645907402038574 	 0.5823073387145996 	 0.5705740451812744 	 0.0858757495880127 	 0.09695792198181152 	 
2025-07-24 22:16:34.943778 test begin: paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 414],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 414],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 50872577 	 1000 	 0.5137135982513428 	 0.5236461162567139 	 0.1750938892364502 	 0.17639422416687012 	 0.5801098346710205 	 0.5722284317016602 	 0.08458113670349121 	 0.09701299667358398 	 
2025-07-24 22:16:40.316279 test begin: paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 58982657 	 1000 	 0.5974440574645996 	 0.5963337421417236 	 0.20236945152282715 	 0.2032768726348877 	 0.662545919418335 	 0.653106689453125 	 0.09661269187927246 	 0.1109621524810791 	 
2025-07-24 22:16:43.853108 test begin: paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 480],"float32"), Tensor([64, 1, 2, 396901],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f501668e6b0>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753367204 (unix time) try "date -d @1753367204" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2e07) received by PID 11783 (TID 0x7f4fef9b1640) from PID 11783 ***]

2025-07-24 22:26:51.124090 test begin: paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 480],"float32"), Tensor([64, 1, 396901, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
W0724 22:26:52.928604  4037 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0724 22:26:52.982264  4037 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fe8f7042ef0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-25 00:02:45.971623 test begin: paddle.nn.functional.conv2d_transpose(Tensor([8, 28, 480, 480],"float32"), Tensor([28, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
W0725 00:02:47.220875 51797 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0725 00:02:47.229785 51797 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([8, 28, 480, 480],"float32"), Tensor([28, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 51609713 	 1000 	 0.7011816501617432 	 0.7047386169433594 	 0.2389512062072754 	 0.23998522758483887 	 0.6971862316131592 	 0.6574051380157471 	 0.10145401954650879 	 0.11132478713989258 	 
2025-07-25 00:02:51.155706 test begin: paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 207, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 207, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 50872577 	 1000 	 0.5146055221557617 	 0.5201797485351562 	 0.17502903938293457 	 0.17627787590026855 	 0.5609865188598633 	 0.5541126728057861 	 0.08179426193237305 	 0.09394192695617676 	 
2025-07-25 00:02:54.251010 test begin: paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 320, 320],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 320, 320],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 52429057 	 1000 	 0.5319163799285889 	 0.5331361293792725 	 0.180617094039917 	 0.1817011833190918 	 0.6046619415283203 	 0.5620479583740234 	 0.0881805419921875 	 0.09546041488647461 	 
2025-07-25 00:02:57.397639 test begin: paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 480, 207],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
[Prof] paddle.nn.functional.conv2d_transpose 	 paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 480, 207],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", ) 	 50872577 	 1000 	 0.5249714851379395 	 0.5226476192474365 	 0.17509198188781738 	 0.17769384384155273 	 0.5582723617553711 	 0.550743579864502 	 0.0812234878540039 	 0.09358692169189453 	 
2025-07-25 00:03:00.435255 test begin: paddle.nn.functional.conv3d(Tensor([16538, 6, 8, 8, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([16538, 6, 8, 8, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", ) 	 50805060 	 1000 	 6.389624118804932 	 1.4808616638183594 	 6.2668421268463135 	 1.455801248550415 	 43.270517110824585 	 18.34436583518982 	 22.113202333450317 	 9.371406316757202 	 
2025-07-25 00:04:11.809185 test begin: paddle.nn.functional.conv3d(Tensor([16538, 6, 8, 8, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([16538, 6, 8, 8, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50805392 	 1000 	 12.036132574081421 	 12.091309309005737 	 11.937616348266602 	 12.047497749328613 	 81.14718818664551 	 22.974949598312378 	 20.771502017974854 	 5.871453762054443 	 
2025-07-25 00:06:22.378090 test begin: paddle.nn.functional.conv3d(Tensor([33076, 3, 8, 8, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([33076, 3, 8, 8, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", ) 	 50805146 	 1000 	 18.99849581718445 	 26.36798334121704 	 9.707854509353638 	 13.476329326629639 	 240.56995725631714 	 47.24555516242981 	 35.12727379798889 	 6.8953468799591064 	 
2025-07-25 00:12:00.063584 test begin: paddle.nn.functional.conv3d(Tensor([4, 3, 66151, 8, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 3, 66151, 8, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", ) 	 50804378 	 1000 	 19.25008201599121 	 19.205861568450928 	 9.838898181915283 	 9.813049077987671 	 278.4717698097229 	 277.481210231781 	 47.60707092285156 	 47.169058084487915 	 
2025-07-25 00:21:58.463780 test begin: paddle.nn.functional.conv3d(Tensor([4, 3, 8, 66151, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 3, 8, 66151, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", ) 	 50804378 	 1000 	 15.283905982971191 	 15.459892988204956 	 7.81253457069397 	 7.899490118026733 	 213.50343465805054 	 224.45525407791138 	 36.5063693523407 	 38.15995454788208 	 
2025-07-25 00:29:50.476240 test begin: paddle.nn.functional.conv3d(Tensor([4, 3, 8, 8, 66151],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 3, 8, 8, 66151],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", ) 	 50804378 	 1000 	 14.96176791191101 	 15.582326889038086 	 7.645191431045532 	 8.034855127334595 	 198.13091921806335 	 221.77171301841736 	 33.86362099647522 	 37.70071530342102 	 
2025-07-25 00:37:24.414982 test begin: paddle.nn.functional.conv3d(Tensor([4, 3, 8, 8, 8],"float32"), Tensor([627201, 3, 3, 3, 3],"float32"), Tensor([627201],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
[Error] CUDA out of memory. Tried to allocate 7.48 GiB. GPU 0 has a total capacity of 39.39 GiB of which 924.38 MiB is free. Process 76662 has 38.48 GiB memory in use. Of the allocated memory 15.15 GiB is allocated by PyTorch, and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-07-25 00:45:14.358320 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 33076, 8, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
W0725 00:45:15.415130 93586 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0725 00:45:15.419593 93586 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 33076, 8, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", ) 	 50805060 	 1000 	 8.324291467666626 	 1.9535272121429443 	 8.208465337753296 	 1.9275994300842285 	 57.38165640830994 	 22.247236728668213 	 29.325762033462524 	 11.366344213485718 	 
2025-07-25 00:46:47.753366 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 33076, 8, 8],"float32"), Tensor([12, 1, 33076, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa4dd151270>,)) (kwargs={}) timed out after 600.000000 seconds.



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool)
1   torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&)
2   torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&)
3   at::accelerator::getAccelerator(bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753376231 (unix time) try "date -d @1753376231" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 93186 (TID 0x7fa259fff640) from PID 0 ***]

2025-07-25 00:57:17.625879 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 33076, 8, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
W0725 00:57:18.640326 152262 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
W0725 00:57:18.644902 152262 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 33076, 8, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", ) 	 50805392 	 1000 	 12.214251279830933 	 12.264748811721802 	 12.115602970123291 	 12.215906143188477 	 88.87256097793579 	 87.05920386314392 	 18.21874737739563 	 22.198577165603638 	 
2025-07-25 01:00:41.741237 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 8, 33076, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
[Prof] paddle.nn.functional.conv3d 	 paddle.nn.functional.conv3d(Tensor([4, 6, 8, 33076, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", ) 	 50805060 	 1000 	 8.57939100265503 	 1.962684154510498 	 8.463905811309814 	 1.9437713623046875 	 57.44011735916138 	 12.208958387374878 	 29.35511803627014 	 6.238492727279663 	 
2025-07-25 01:02:03.909337 test begin: paddle.nn.functional.conv3d(Tensor([4, 6, 8, 33076, 8],"float32"), Tensor([12, 1, 3, 33076, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcdf26727d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 15:40:47.628412 test begin: paddle.nn.functional.label_smooth(label=Tensor([256, 6, 33712],"float32"), epsilon=0.1, )
W0724 15:40:48.715658 108420 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.label_smooth 	 paddle.nn.functional.label_smooth(label=Tensor([256, 6, 33712],"float32"), epsilon=0.1, ) 	 51781632 	 1000 	 0.3018934726715088 	 0.6162211894989014 	 0.2850322723388672 	 0.20981836318969727 	 0.30185580253601074 	 0.3034684658050537 	 0.2430408000946045 	 0.22408008575439453 	 combined
2025-07-24 15:40:51.633045 test begin: paddle.nn.functional.label_smooth(label=Tensor([38, 40, 33712],"float32"), epsilon=0.1, )
[Prof] paddle.nn.functional.label_smooth 	 paddle.nn.functional.label_smooth(label=Tensor([38, 40, 33712],"float32"), epsilon=0.1, ) 	 51242240 	 1000 	 0.3010847568511963 	 0.6148850917816162 	 0.2869374752044678 	 0.20765137672424316 	 0.29868483543395996 	 0.30036306381225586 	 0.2497258186340332 	 0.23281049728393555 	 combined
2025-07-24 15:40:54.919904 test begin: paddle.nn.functional.label_smooth(label=Tensor([48, 32, 33712],"float32"), epsilon=0.1, )
[Prof] paddle.nn.functional.label_smooth 	 paddle.nn.functional.label_smooth(label=Tensor([48, 32, 33712],"float32"), epsilon=0.1, ) 	 51781632 	 1000 	 0.30195188522338867 	 0.6162247657775879 	 0.29207301139831543 	 0.20983314514160156 	 0.30173182487487793 	 0.3034169673919678 	 0.2531919479370117 	 0.2301926612854004 	 combined
2025-07-24 15:40:58.162306 test begin: paddle.nn.functional.label_smooth(label=Tensor([76, 20, 33712],"float32"), epsilon=0.1, )
[Prof] paddle.nn.functional.label_smooth 	 paddle.nn.functional.label_smooth(label=Tensor([76, 20, 33712],"float32"), epsilon=0.1, ) 	 51242240 	 1000 	 0.29857325553894043 	 0.6099753379821777 	 0.28905606269836426 	 0.20770525932312012 	 0.2986786365509033 	 0.3003096580505371 	 0.23530364036560059 	 0.23201608657836914 	 combined
2025-07-24 15:41:01.403265 test begin: paddle.nn.functional.layer_norm(Tensor([115, 435, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
[Prof] paddle.nn.functional.layer_norm 	 paddle.nn.functional.layer_norm(Tensor([115, 435, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, ) 	 51227648 	 1000 	 0.3023505210876465 	 0.3165421485900879 	 0.2864725589752197 	 0.2936553955078125 	 0.4714055061340332 	 1.0151011943817139 	 0.24073529243469238 	 0.5186889171600342 	 
2025-07-24 15:41:06.742336 test begin: paddle.nn.functional.layer_norm(Tensor([174, 286, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
[Prof] paddle.nn.functional.layer_norm 	 paddle.nn.functional.layer_norm(Tensor([174, 286, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, ) 	 50960384 	 1000 	 1.1048481464385986 	 0.3188505172729492 	 0.28474974632263184 	 0.29166364669799805 	 0.46988677978515625 	 1.009906530380249 	 0.2400047779083252 	 0.5160136222839355 	 
2025-07-24 15:41:13.366215 test begin: paddle.nn.functional.layer_norm(Tensor([226, 220, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
[Prof] paddle.nn.functional.layer_norm 	 paddle.nn.functional.layer_norm(Tensor([226, 220, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, ) 	 50915328 	 1000 	 0.300609827041626 	 0.3147120475769043 	 0.2761363983154297 	 0.28295207023620605 	 0.46872901916503906 	 1.0092308521270752 	 0.23940467834472656 	 0.5156874656677246 	 
2025-07-24 15:41:17.228885 test begin: paddle.nn.functional.layer_norm(Tensor([7, 7088, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
[Prof] paddle.nn.functional.layer_norm 	 paddle.nn.functional.layer_norm(Tensor([7, 7088, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, ) 	 50808832 	 1000 	 0.3018829822540283 	 0.7089359760284424 	 0.27612781524658203 	 0.28205037117004395 	 0.46705126762390137 	 1.0054378509521484 	 0.2386026382446289 	 0.5136537551879883 	 
2025-07-24 15:41:21.490526 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 26, 304, 544],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 26, 304, 544],"float32"), 0.1, ) 	 51597312 	 1000 	 0.30101537704467773 	 0.30786633491516113 	 0.2849392890930176 	 0.27802467346191406 	 0.4574267864227295 	 0.45380282402038574 	 0.39665842056274414 	 0.3859221935272217 	 
2025-07-24 15:41:24.756293 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 32, 122, 1088],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 32, 122, 1088],"float32"), 0.1, ) 	 50970624 	 1000 	 0.29725027084350586 	 0.29899120330810547 	 0.2814664840698242 	 0.2746562957763672 	 0.45166611671447754 	 0.4483964443206787 	 0.39017677307128906 	 0.37814760208129883 	 
2025-07-24 15:41:28.019725 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 32, 608, 218],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 32, 608, 218],"float32"), 0.1, ) 	 50896896 	 1000 	 0.2967839241027832 	 0.299576997756958 	 0.28090667724609375 	 0.27191805839538574 	 0.4525146484375 	 0.44770169258117676 	 0.3919699192047119 	 0.37812089920043945 	 
2025-07-24 15:41:31.217453 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 64, 122, 544],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 64, 122, 544],"float32"), 0.1, ) 	 50970624 	 1000 	 0.29729127883911133 	 0.30426025390625 	 0.28818726539611816 	 0.27894091606140137 	 0.4516794681549072 	 0.4483804702758789 	 0.4010467529296875 	 0.3865845203399658 	 
2025-07-24 15:41:34.498103 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 64, 304, 218],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 64, 304, 218],"float32"), 0.1, ) 	 50896896 	 1000 	 0.9651050567626953 	 0.31107234954833984 	 0.28751397132873535 	 0.281538724899292 	 0.45116257667541504 	 0.44776296615600586 	 0.3990335464477539 	 0.3836503028869629 	 
2025-07-24 15:41:40.705870 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 7, 608, 1088],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([12, 7, 608, 1088],"float32"), 0.1, ) 	 55566336 	 1000 	 0.3241236209869385 	 0.3253912925720215 	 0.31423091888427734 	 0.30869317054748535 	 0.4923100471496582 	 0.48811817169189453 	 0.44162869453430176 	 0.42696166038513184 	 
2025-07-24 15:41:44.265659 test begin: paddle.nn.functional.leaky_relu(Tensor([13, 64, 256, 256],"float32"), 0.1, None, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([13, 64, 256, 256],"float32"), 0.1, None, ) 	 54525952 	 1000 	 0.3175532817840576 	 0.3237764835357666 	 0.30173230171203613 	 0.29505300521850586 	 0.4832429885864258 	 0.4791276454925537 	 0.4186112880706787 	 0.409923791885376 	 
2025-07-24 15:41:47.702469 test begin: paddle.nn.functional.leaky_relu(Tensor([3, 32, 608, 1088],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([3, 32, 608, 1088],"float32"), 0.1, ) 	 63504384 	 1000 	 0.369842529296875 	 0.3713064193725586 	 0.36028122901916504 	 0.3542327880859375 	 0.5617544651031494 	 0.55892014503479 	 0.5110647678375244 	 0.4923269748687744 	 
2025-07-24 15:41:51.761249 test begin: paddle.nn.functional.leaky_relu(Tensor([5, 64, 304, 544],"float32"), 0.1, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([5, 64, 304, 544],"float32"), 0.1, ) 	 52920320 	 1000 	 0.3082551956176758 	 0.31128525733947754 	 0.2991361618041992 	 0.2933082580566406 	 0.46900248527526855 	 0.46503543853759766 	 0.41849708557128906 	 0.4029347896575928 	 
2025-07-24 15:41:55.094672 test begin: paddle.nn.functional.leaky_relu(Tensor([64, 13, 256, 256],"float32"), 0.1, None, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([64, 13, 256, 256],"float32"), 0.1, None, ) 	 54525952 	 1000 	 0.3175845146179199 	 0.31940388679504395 	 0.30851030349731445 	 0.30260682106018066 	 0.483104944229126 	 0.47908759117126465 	 0.42786383628845215 	 0.4148571491241455 	 
2025-07-24 15:41:58.575582 test begin: paddle.nn.functional.leaky_relu(Tensor([64, 64, 256, 49],"float32"), 0.1, None, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([64, 64, 256, 49],"float32"), 0.1, None, ) 	 51380224 	 1000 	 0.2996530532836914 	 0.3013150691986084 	 0.29055261611938477 	 0.2846200466156006 	 0.455491304397583 	 0.45192694664001465 	 0.4029231071472168 	 0.38451099395751953 	 
2025-07-24 15:42:01.821694 test begin: paddle.nn.functional.leaky_relu(Tensor([64, 64, 49, 256],"float32"), 0.1, None, )
[Prof] paddle.nn.functional.leaky_relu 	 paddle.nn.functional.leaky_relu(Tensor([64, 64, 49, 256],"float32"), 0.1, None, ) 	 51380224 	 1000 	 0.29961180686950684 	 0.301347017288208 	 0.29045534133911133 	 0.2847862243652344 	 0.4555702209472656 	 0.4519190788269043 	 0.40521240234375 	 0.3907351493835449 	 
2025-07-24 15:42:05.044728 test begin: paddle.nn.functional.linear(x=Tensor([1, 12404],"float32"), weight=Tensor([12404, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, )
Warning: The core code of paddle.nn.functional.linear is too complex.
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([1, 12404],"float32"), weight=Tensor([12404, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, ) 	 50823284 	 1000 	 0.16039419174194336 	 0.15418386459350586 	 0.05448555946350098 	 0.1324772834777832 	 0.31569361686706543 	 0.315934419631958 	 0.08028388023376465 	 0.10743856430053711 	 
2025-07-24 15:42:07.053463 test begin: paddle.nn.functional.linear(x=Tensor([1, 25088],"float32"), weight=Tensor([25088, 2026],"float32"), bias=Tensor([2026],"float32"), name=None, )
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([1, 25088],"float32"), weight=Tensor([25088, 2026],"float32"), bias=Tensor([2026],"float32"), name=None, ) 	 50855402 	 1000 	 0.168165922164917 	 0.15609097480773926 	 0.05716109275817871 	 0.1306748390197754 	 0.3231368064880371 	 0.322190523147583 	 0.08261466026306152 	 0.09607601165771484 	 
2025-07-24 15:42:08.925043 test begin: paddle.nn.functional.linear(x=Tensor([2, 12404],"float32"), weight=Tensor([12404, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, )
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([2, 12404],"float32"), weight=Tensor([12404, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, ) 	 50835688 	 1000 	 0.21503353118896484 	 0.250948429107666 	 0.07309722900390625 	 0.0642549991607666 	 0.4306483268737793 	 0.42612361907958984 	 0.07356786727905273 	 0.0869908332824707 	 
2025-07-24 15:42:11.120088 test begin: paddle.nn.functional.linear(x=Tensor([2, 25088],"float32"), weight=Tensor([25088, 2026],"float32"), bias=Tensor([2026],"float32"), name=None, )
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([2, 25088],"float32"), weight=Tensor([25088, 2026],"float32"), bias=Tensor([2026],"float32"), name=None, ) 	 50880490 	 1000 	 0.22953271865844727 	 0.3145730495452881 	 0.0780327320098877 	 0.08027791976928711 	 0.3370687961578369 	 0.33495521545410156 	 0.08622121810913086 	 0.1139373779296875 	 
2025-07-24 15:42:16.440540 test begin: paddle.nn.functional.linear(x=Tensor([2026, 25088],"float32"), weight=Tensor([25088, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, )
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([2026, 25088],"float32"), weight=Tensor([25088, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, ) 	 153592832 	 1000 	 23.878236293792725 	 24.861904859542847 	 12.200933694839478 	 24.835425853729248 	 48.13536596298218 	 48.0660343170166 	 9.866876602172852 	 12.256140947341919 	 
2025-07-24 15:44:44.443231 test begin: paddle.nn.functional.linear(x=Tensor([4051, 12544],"float32"), weight=Tensor([12544, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, )
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([4051, 12544],"float32"), weight=Tensor([12544, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, ) 	 63661824 	 1000 	 6.266141414642334 	 6.262615203857422 	 3.202033042907715 	 6.235526084899902 	 12.161137104034424 	 12.151587009429932 	 2.4928946495056152 	 3.0986156463623047 	 
2025-07-24 15:45:22.502272 test begin: paddle.nn.functional.linear(x=Tensor([4096, 12404],"float32"), weight=Tensor([12404, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, )
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([4096, 12404],"float32"), weight=Tensor([12404, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, ) 	 63509504 	 1000 	 6.2069878578186035 	 6.20222282409668 	 3.1718060970306396 	 6.172451019287109 	 12.063551902770996 	 12.030141353607178 	 2.4728877544403076 	 3.067607879638672 	 
2025-07-24 15:46:00.866686 test begin: paddle.nn.functional.linear(x=Tensor([4096, 12544],"float32"), weight=Tensor([12544, 4051],"float32"), bias=Tensor([4051],"float32"), name=None, )
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([4096, 12544],"float32"), weight=Tensor([12544, 4051],"float32"), bias=Tensor([4051],"float32"), name=None, ) 	 102200019 	 1000 	 24.037108659744263 	 24.052385568618774 	 12.28346061706543 	 24.03037118911743 	 48.090514183044434 	 47.95116376876831 	 9.858269453048706 	 12.22765040397644 	 
2025-07-24 15:48:27.511180 test begin: paddle.nn.functional.linear(x=Tensor([4096, 49613],"float32"), weight=Tensor([49613, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, )
[Prof] paddle.nn.functional.linear 	 paddle.nn.functional.linear(x=Tensor([4096, 49613],"float32"), weight=Tensor([49613, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, ) 	 254019584 	 1000 	 24.777305603027344 	 24.773014545440674 	 12.661802291870117 	 24.74504327774048 	 47.81320357322693 	 47.748539447784424 	 9.801461935043335 	 12.175530910491943 	 
2025-07-24 15:50:58.519221 test begin: paddle.nn.functional.local_response_norm(Tensor([10585, 3, 40, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, )
W0724 15:51:00.223260 114001 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(Tensor([10585, 3, 40, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, ) 	 50808000 	 1000 	 4.258253335952759 	 5.091715574264526 	 0.7231509685516357 	 0.6509120464324951 	 8.863974332809448 	 8.642985105514526 	 1.0042831897735596 	 0.5194399356842041 	 
2025-07-24 15:51:30.765308 test begin: paddle.nn.functional.local_response_norm(Tensor([3, 10585, 40, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(Tensor([3, 10585, 40, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, ) 	 50808000 	 1000 	 3.427774429321289 	 4.76026177406311 	 0.5836136341094971 	 0.6077587604522705 	 7.167681455612183 	 7.567877769470215 	 0.812493085861206 	 0.48346757888793945 	 
2025-07-24 15:51:56.516924 test begin: paddle.nn.functional.local_response_norm(Tensor([3, 3, 141121, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(Tensor([3, 3, 141121, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, ) 	 50803560 	 1000 	 4.328758239746094 	 5.253416299819946 	 0.7366676330566406 	 0.6718177795410156 	 8.950819492340088 	 8.852013111114502 	 1.0139656066894531 	 0.5656120777130127 	 
2025-07-24 15:52:26.073275 test begin: paddle.nn.functional.local_response_norm(Tensor([3, 3, 40, 141121],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(Tensor([3, 3, 40, 141121],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, ) 	 50803560 	 1000 	 4.332639455795288 	 4.3099305629730225 	 0.7366328239440918 	 0.5502235889434814 	 8.9182288646698 	 7.9166882038116455 	 1.0103716850280762 	 0.5056746006011963 	 
2025-07-24 15:52:53.389955 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3, 40, 47041],"float32"), size=5, data_format="NCDHW", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3, 40, 47041],"float32"), size=5, data_format="NCDHW", ) 	 50804280 	 1000 	 4.328901052474976 	 4.681267499923706 	 0.7361223697662354 	 0.5982224941253662 	 9.03981065750122 	 8.565758466720581 	 1.182380199432373 	 0.5473496913909912 	 
2025-07-24 15:53:22.065579 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3, 47041, 40],"float32"), size=5, data_format="NCDHW", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3, 47041, 40],"float32"), size=5, data_format="NCDHW", ) 	 50804280 	 1000 	 4.32573390007019 	 5.766035556793213 	 0.7361977100372314 	 0.598212480545044 	 8.91538143157959 	 8.565649271011353 	 1.0579779148101807 	 0.5472450256347656 	 
2025-07-24 15:53:54.040467 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3529, 40, 40],"float32"), size=5, data_format="NCDHW", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3529, 40, 40],"float32"), size=5, data_format="NCDHW", ) 	 50817600 	 1000 	 4.3212549686431885 	 4.253036260604858 	 0.7354271411895752 	 0.543403148651123 	 8.93649697303772 	 7.833345413208008 	 1.0124616622924805 	 0.5004136562347412 	 
2025-07-24 15:54:21.749380 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 40, 40, 3529],"float32"), size=5, data_format="NDHWC", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 40, 40, 3529],"float32"), size=5, data_format="NDHWC", ) 	 50817600 	 1000 	 7.121174097061157 	 4.815937280654907 	 1.2113890647888184 	 0.6160221099853516 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 3, 40, 40, 3529]) and output[0] has a shape of torch.Size([3, 3529, 3, 40, 40]).
2025-07-24 15:54:48.995912 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 40, 47041, 3],"float32"), size=5, data_format="NDHWC", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 40, 47041, 3],"float32"), size=5, data_format="NDHWC", ) 	 50804280 	 1000 	 7.918537855148315 	 4.983827590942383 	 1.3469772338867188 	 0.6373491287231445 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 3, 40, 47041, 3]) and output[0] has a shape of torch.Size([3, 3, 3, 40, 47041]).
2025-07-24 15:55:29.190013 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 47041, 40, 3],"float32"), size=5, data_format="NDHWC", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 47041, 40, 3],"float32"), size=5, data_format="NDHWC", ) 	 50804280 	 1000 	 7.9188055992126465 	 4.983719110488892 	 1.3469734191894531 	 0.6372783184051514 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 3, 47041, 40, 3]) and output[0] has a shape of torch.Size([3, 3, 3, 47041, 40]).
2025-07-24 15:56:10.806437 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3529, 3, 40, 40],"float32"), size=5, data_format="NCDHW", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3529, 3, 40, 40],"float32"), size=5, data_format="NCDHW", ) 	 50817600 	 1000 	 3.428861618041992 	 4.244731426239014 	 0.5837123394012451 	 0.5415828227996826 	 7.132846355438232 	 7.331568717956543 	 0.8086256980895996 	 0.4683339595794678 	 
2025-07-24 15:56:36.993791 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3, 3529, 40, 40, 3],"float32"), size=5, data_format="NDHWC", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3, 3529, 40, 40, 3],"float32"), size=5, data_format="NDHWC", ) 	 50817600 	 1000 	 7.928094387054443 	 4.5533013343811035 	 1.3472650051116943 	 0.5822012424468994 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 3529, 40, 40, 3]) and output[0] has a shape of torch.Size([3, 3, 3529, 40, 40]).
2025-07-24 15:57:18.251211 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3529, 3, 3, 40, 40],"float32"), size=5, data_format="NCDHW", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3529, 3, 3, 40, 40],"float32"), size=5, data_format="NCDHW", ) 	 50817600 	 1000 	 4.250218868255615 	 4.538507461547852 	 0.7233555316925049 	 0.5786948204040527 	 8.764858961105347 	 8.54202651977539 	 0.9930222034454346 	 0.5456888675689697 	 
2025-07-24 15:57:46.764867 test begin: paddle.nn.functional.local_response_norm(x=Tensor([3529, 3, 40, 40, 3],"float32"), size=5, data_format="NDHWC", )
[Prof] paddle.nn.functional.local_response_norm 	 paddle.nn.functional.local_response_norm(x=Tensor([3529, 3, 40, 40, 3],"float32"), size=5, data_format="NDHWC", ) 	 50817600 	 1000 	 7.924009561538696 	 4.537982702255249 	 1.3478717803955078 	 0.5802626609802246 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([3529, 3, 40, 40, 3]) and output[0] has a shape of torch.Size([3529, 3, 3, 40, 40]).
2025-07-24 15:58:26.438034 test begin: paddle.nn.functional.log_loss(Tensor([50803201, 1],"float32"), Tensor([50803201, 1],"float32"), )
[Prof] paddle.nn.functional.log_loss 	 paddle.nn.functional.log_loss(Tensor([50803201, 1],"float32"), Tensor([50803201, 1],"float32"), ) 	 101606402 	 1000 	 0.4924302101135254 	 3.419684648513794 	 0.4828517436981201 	 0.34968042373657227 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 15:58:34.049223 test begin: paddle.nn.functional.log_loss(Tensor([50803201, 1],"float32"), Tensor([50803201, 1],"float32"), epsilon=1e-07, )
[Prof] paddle.nn.functional.log_loss 	 paddle.nn.functional.log_loss(Tensor([50803201, 1],"float32"), Tensor([50803201, 1],"float32"), epsilon=1e-07, ) 	 101606402 	 1000 	 0.4924154281616211 	 4.327921628952026 	 0.47566890716552734 	 0.3496851921081543 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 15:58:43.606647 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 10, 254017],"float64"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([10, 10, 254017],"float64"), None, ) 	 25401700 	 1000 	 0.4405965805053711 	 0.46009111404418945 	 0.4311561584472656 	 0.44283556938171387 	 0.45002007484436035 	 0.4510509967803955 	 0.39984893798828125 	 0.3844478130340576 	 
2025-07-24 15:58:46.491218 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 10, 508033],"float32"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([10, 10, 508033],"float32"), None, ) 	 50803300 	 1000 	 0.30074000358581543 	 0.2985663414001465 	 0.29175305366516113 	 0.2783851623535156 	 0.4503190517425537 	 0.45038557052612305 	 0.39940977096557617 	 0.377077579498291 	 
2025-07-24 15:58:49.657953 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 254017, 10],"float64"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([10, 254017, 10],"float64"), None, ) 	 25401700 	 1000 	 0.44060206413269043 	 0.4564197063446045 	 0.43149805068969727 	 0.44310593605041504 	 0.45001864433288574 	 0.450986385345459 	 0.3994121551513672 	 0.382049560546875 	 
2025-07-24 15:58:52.565360 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 508033, 10],"float32"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([10, 508033, 10],"float32"), None, ) 	 50803300 	 1000 	 0.30067014694213867 	 0.30209898948669434 	 0.29161548614501953 	 0.2849600315093994 	 0.4503467082977295 	 0.4503138065338135 	 0.3993864059448242 	 0.38231873512268066 	 
2025-07-24 15:58:55.729600 test begin: paddle.nn.functional.log_sigmoid(Tensor([254017, 10, 10],"float64"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([254017, 10, 10],"float64"), None, ) 	 25401700 	 1000 	 0.44066381454467773 	 0.45647621154785156 	 0.43172192573547363 	 0.4432997703552246 	 0.4500277042388916 	 0.45095181465148926 	 0.3993525505065918 	 0.3812274932861328 	 
2025-07-24 15:58:58.568720 test begin: paddle.nn.functional.log_sigmoid(Tensor([508033, 10, 10],"float32"), None, )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(Tensor([508033, 10, 10],"float32"), None, ) 	 50803300 	 1000 	 0.30063867568969727 	 0.30037355422973633 	 0.29170918464660645 	 0.2850198745727539 	 0.4503347873687744 	 0.45040178298950195 	 0.39945077896118164 	 0.38273191452026367 	 
2025-07-24 15:59:01.741064 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([10, 10, 508033],"float32"), )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(x=Tensor([10, 10, 508033],"float32"), ) 	 50803300 	 1000 	 0.3006603717803955 	 0.29845380783081055 	 0.29133081436157227 	 0.28475403785705566 	 0.4502246379852295 	 0.4503796100616455 	 0.3992640972137451 	 0.38287997245788574 	 
2025-07-24 15:59:04.892880 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([10, 508033, 10],"float32"), )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(x=Tensor([10, 508033, 10],"float32"), ) 	 50803300 	 1000 	 0.3006608486175537 	 0.2984888553619385 	 0.29128193855285645 	 0.2850925922393799 	 0.4500293731689453 	 0.45041871070861816 	 0.39928674697875977 	 0.3827848434448242 	 
2025-07-24 15:59:08.059701 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([508033, 10, 10],"float32"), )
[Prof] paddle.nn.functional.log_sigmoid 	 paddle.nn.functional.log_sigmoid(x=Tensor([508033, 10, 10],"float32"), ) 	 50803300 	 1000 	 0.3006734848022461 	 0.29859209060668945 	 0.29140591621398926 	 0.2801322937011719 	 0.45003724098205566 	 0.45038676261901855 	 0.394167423248291 	 0.38181066513061523 	 
2025-07-24 15:59:11.234579 test begin: paddle.nn.functional.log_softmax(Tensor([128, 396901],"float32"), axis=-1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([128, 396901],"float32"), axis=-1, ) 	 50803328 	 1000 	 0.7060463428497314 	 0.6308848857879639 	 0.6970462799072266 	 0.6054432392120361 	 1.3969597816467285 	 0.6488940715789795 	 1.3340034484863281 	 0.5755136013031006 	 
2025-07-24 15:59:16.432284 test begin: paddle.nn.functional.log_softmax(Tensor([264, 192612],"float32"), axis=-1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([264, 192612],"float32"), axis=-1, ) 	 50849568 	 1000 	 0.6189587116241455 	 0.6455178260803223 	 0.6016643047332764 	 0.623265266418457 	 0.860095739364624 	 0.6692469120025635 	 0.7974598407745361 	 0.5961947441101074 	 
2025-07-24 15:59:21.018366 test begin: paddle.nn.functional.log_softmax(Tensor([2944, 17257],"float32"), axis=1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([2944, 17257],"float32"), axis=1, ) 	 50804608 	 1000 	 0.3332698345184326 	 0.33760666847229004 	 0.32295918464660645 	 0.32300853729248047 	 0.6464724540710449 	 0.5229015350341797 	 0.5931077003479004 	 0.45445775985717773 	 
2025-07-24 15:59:24.587267 test begin: paddle.nn.functional.log_softmax(Tensor([4224, 12028],"float32"), axis=1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([4224, 12028],"float32"), axis=1, ) 	 50806272 	 1000 	 0.30513548851013184 	 0.3079540729522705 	 0.29476022720336914 	 0.2935495376586914 	 0.6303615570068359 	 0.4546804428100586 	 0.5774331092834473 	 0.39002156257629395 	 
2025-07-24 15:59:27.944662 test begin: paddle.nn.functional.log_softmax(Tensor([7664, 6629],"float32"), axis=1, )
[Prof] paddle.nn.functional.log_softmax 	 paddle.nn.functional.log_softmax(Tensor([7664, 6629],"float32"), axis=1, ) 	 50804656 	 1000 	 0.30310869216918945 	 0.29938292503356934 	 0.29271531105041504 	 0.2849147319793701 	 0.5953130722045898 	 0.5162410736083984 	 0.5417935848236084 	 0.4503300189971924 	 
2025-07-24 15:59:31.386964 test begin: paddle.nn.functional.lp_pool1d(Tensor([2, 3, 8467201],"float32"), 4.0, 3, 2, 1, False, "NCL", None, )
[Prof] paddle.nn.functional.lp_pool1d 	 paddle.nn.functional.lp_pool1d(Tensor([2, 3, 8467201],"float32"), 4.0, 3, 2, 1, False, "NCL", None, ) 	 50803206 	 1000 	 0.9310972690582275 	 1.9160284996032715 	 0.902172327041626 	 0.24429082870483398 	 1.4192001819610596 	 4.948427438735962 	 0.7251319885253906 	 0.3163759708404541 	 
2025-07-24 15:59:42.917094 test begin: paddle.nn.functional.lp_pool1d(Tensor([2, 793801, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, )
[Prof] paddle.nn.functional.lp_pool1d 	 paddle.nn.functional.lp_pool1d(Tensor([2, 793801, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, ) 	 50803264 	 1000 	 0.9312584400177002 	 1.9390842914581299 	 0.902672529220581 	 0.24725127220153809 	 1.4237573146820068 	 5.140634775161743 	 0.7273831367492676 	 0.32865214347839355 	 
2025-07-24 15:59:53.604976 test begin: paddle.nn.functional.lp_pool1d(Tensor([529201, 3, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, )
[Prof] paddle.nn.functional.lp_pool1d 	 paddle.nn.functional.lp_pool1d(Tensor([529201, 3, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, ) 	 50803296 	 1000 	 0.9313981533050537 	 1.9388234615325928 	 0.902651309967041 	 0.24717020988464355 	 1.4215645790100098 	 5.141718626022339 	 0.7263727188110352 	 0.3287661075592041 	 
2025-07-24 16:00:04.336432 test begin: paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, ) 	 50804736 	 1000 	 2.101728916168213 	 3.4944117069244385 	 2.085222005844116 	 0.445573091506958 	 2.2539963722229004 	 6.803547620773315 	 1.1517143249511719 	 0.4633634090423584 	 
2025-07-24 16:00:20.660222 test begin: paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, ) 	 50804736 	 1000 	 0.555072546005249 	 1.206080675125122 	 0.5389063358306885 	 0.1536242961883545 	 0.9978020191192627 	 3.6540024280548096 	 0.5097332000732422 	 0.24923372268676758 	 
2025-07-24 16:00:28.120022 test begin: paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([16538, 3, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, ) 	 50804736 	 1000 	 0.9396800994873047 	 0.7921650409698486 	 0.9172978401184082 	 0.10053086280822754 	 1.925386905670166 	 3.2912001609802246 	 0.9838228225708008 	 0.22451543807983398 	 
2025-07-24 16:00:37.535064 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, ) 	 50804736 	 1000 	 2.1015007495880127 	 3.5009584426879883 	 2.0764732360839844 	 0.4455296993255615 	 2.2537612915039062 	 6.803568124771118 	 1.1516141891479492 	 0.46320319175720215 	 
2025-07-24 16:00:55.159344 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, ) 	 50804736 	 1000 	 0.5551416873931885 	 1.2051458358764648 	 0.538597583770752 	 0.15354609489440918 	 0.9982562065124512 	 3.649350881576538 	 0.5101635456085205 	 0.24881958961486816 	 
2025-07-24 16:01:02.695995 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 24807, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, ) 	 50804736 	 1000 	 0.9397099018096924 	 0.7952899932861328 	 0.9233624935150146 	 0.10049676895141602 	 1.9253284931182861 	 3.297564744949341 	 0.9837994575500488 	 0.22494888305664062 	 
2025-07-24 16:01:13.159289 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 3, 264601, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 3, 264601, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, ) 	 50803392 	 1000 	 2.169039249420166 	 3.5112876892089844 	 2.1526687145233154 	 0.4477865695953369 	 2.296715259552002 	 6.940605640411377 	 1.1735410690307617 	 0.44359302520751953 	 
2025-07-24 16:01:29.776718 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 3, 264601, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 3, 264601, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, ) 	 50803392 	 1000 	 0.5556879043579102 	 1.2072875499725342 	 0.5395979881286621 	 0.1537787914276123 	 1.003997802734375 	 3.655400514602661 	 0.5129687786102295 	 0.23377132415771484 	 
2025-07-24 16:01:37.318106 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 264601],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 264601],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, ) 	 50803392 	 1000 	 2.092561721801758 	 3.5938198566436768 	 2.0762851238250732 	 0.4566199779510498 	 2.2962470054626465 	 6.928901195526123 	 1.1733341217041016 	 0.4428701400756836 	 
2025-07-24 16:01:55.307399 test begin: paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 264601],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, )
[Prof] paddle.nn.functional.lp_pool2d 	 paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 264601],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, ) 	 50803392 	 1000 	 0.5554461479187012 	 1.2263987064361572 	 0.5389673709869385 	 0.15437650680541992 	 1.0009198188781738 	 3.609567880630493 	 0.5113465785980225 	 0.23082804679870605 	 
2025-07-24 16:02:02.740631 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([1373060, 37],"float32"), Tensor([1373060],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([1373060, 37],"float32"), Tensor([1373060],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, ) 	 52176280 	 1000 	 2.9560000896453857 	 1.653463363647461 	 0.33559322357177734 	 0.11251401901245117 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:02:08.934899 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([25401601, 37],"float16"), Tensor([25401601],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([25401601, 37],"float16"), Tensor([25401601],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, ) 	 965260838 	 1000 	 52.47353506088257 	 19.43569040298462 	 5.959190845489502 	 1.3224599361419678 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:04:00.288950 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([25401601, 37],"float32"), Tensor([25401601],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([25401601, 37],"float32"), Tensor([25401601],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, ) 	 965260838 	 1000 	 54.94206118583679 	 30.164459705352783 	 6.23717737197876 	 1.8113417625427246 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:05:53.256142 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([2746119, 37],"float16"), Tensor([2746119],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([2746119, 37],"float16"), Tensor([2746119],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, ) 	 104352522 	 1000 	 5.746784448623657 	 2.1493406295776367 	 0.6525683403015137 	 0.1462843418121338 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:06:05.402479 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([5, 10160641],"float32"), Tensor([5],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([5, 10160641],"float32"), Tensor([5],"int64"), return_softmax=False, margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, group=None, reduction=None, ) 	 50803210 	 1000 	 70.0385193824768 	 6.194364547729492 	 6.51349401473999 	 0.4211158752441406 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:07:22.944626 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, return_softmax=True, reduction="mean", )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, return_softmax=True, reduction="mean", ) 	 25401610 	 1000 	 41.9746527671814 	 15.89382028579712 	 3.607391595840454 	 1.0870518684387207 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:08:24.415578 test begin: paddle.nn.functional.margin_cross_entropy(Tensor([686530, 37],"float64"), Tensor([686530],"int64"), margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, return_softmax=True, reduction="mean", )
[Prof] paddle.nn.functional.margin_cross_entropy 	 paddle.nn.functional.margin_cross_entropy(Tensor([686530, 37],"float64"), Tensor([686530],"int64"), margin1=1.0, margin2=0.5, margin3=0.0, scale=2.0, return_softmax=True, reduction="mean", ) 	 26088140 	 1000 	 2.4912140369415283 	 25.835553646087646 	 0.23080754280090332 	 1.7811322212219238 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:08:56.671388 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), 0.0, "mean", )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), 0.0, "mean", ) 	 76204830 	 1000 	 1.339674949645996 	 1.9366686344146729 	 0.2734510898590088 	 0.28275036811828613 	 1.8070390224456787 	 2.1100494861602783 	 0.46192383766174316 	 0.26990652084350586 	 
2025-07-24 16:09:05.531151 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), 0.0, "mean", None, )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), Tensor([10, 2540161],"float64"), 0.0, "mean", None, ) 	 76204830 	 1000 	 1.3401286602020264 	 1.9365801811218262 	 0.2735602855682373 	 0.2827413082122803 	 1.8067858219146729 	 2.110342264175415 	 0.46193671226501465 	 0.270007848739624 	 
2025-07-24 16:09:14.353727 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), 0.0, "mean", )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), 0.0, "mean", ) 	 76204830 	 1000 	 1.339874029159546 	 1.9366579055786133 	 0.27349281311035156 	 0.2827944755554199 	 1.8078665733337402 	 2.110126495361328 	 0.46225595474243164 	 0.269916296005249 	 
2025-07-24 16:09:23.315442 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), 0.0, "mean", None, )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), Tensor([2540161, 10],"float64"), 0.0, "mean", None, ) 	 76204830 	 1000 	 1.3401360511779785 	 1.9363970756530762 	 0.27353572845458984 	 0.28272223472595215 	 1.8067669868469238 	 2.1105856895446777 	 0.4618563652038574 	 0.26993799209594727 	 
2025-07-24 16:09:33.601245 test begin: paddle.nn.functional.margin_ranking_loss(Tensor([50803201],"float32"), Tensor([50803201],"float32"), Tensor([50803201],"float32"), 0.5, "mean", None, )
[Prof] paddle.nn.functional.margin_ranking_loss 	 paddle.nn.functional.margin_ranking_loss(Tensor([50803201],"float32"), Tensor([50803201],"float32"), Tensor([50803201],"float32"), 0.5, "mean", None, ) 	 152409603 	 1000 	 1.6482102870941162 	 1.9398376941680908 	 0.24002909660339355 	 0.2832334041595459 	 2.1837401390075684 	 2.248832941055298 	 0.5584287643432617 	 0.2876625061035156 	 
2025-07-24 16:09:45.823669 test begin: paddle.nn.functional.max_pool1d(Tensor([2, 3, 8467201],"float32"), 2, None, 0, False, False, None, )
[Prof] paddle.nn.functional.max_pool1d 	 paddle.nn.functional.max_pool1d(Tensor([2, 3, 8467201],"float32"), 2, None, 0, False, False, None, ) 	 50803206 	 1000 	 0.26883530616760254 	 0.418689489364624 	 0.23195362091064453 	 0.38907599449157715 	 0.7777838706970215 	 1.3030030727386475 	 0.39737439155578613 	 0.6657791137695312 	 
2025-07-24 16:09:49.882527 test begin: paddle.nn.functional.max_pool1d(Tensor([226801, 32, 7],"float32"), 7, )
[Prof] paddle.nn.functional.max_pool1d 	 paddle.nn.functional.max_pool1d(Tensor([226801, 32, 7],"float32"), 7, ) 	 50803424 	 1000 	 0.315035343170166 	 0.21575284004211426 	 0.28353142738342285 	 0.18927240371704102 	 18.17148447036743 	 4.402930498123169 	 18.070788621902466 	 2.2498438358306885 	 
2025-07-24 16:10:14.045935 test begin: paddle.nn.functional.max_pool1d(Tensor([91, 32, 17447],"float32"), 7, )
[Prof] paddle.nn.functional.max_pool1d 	 paddle.nn.functional.max_pool1d(Tensor([91, 32, 17447],"float32"), 7, ) 	 50805664 	 1000 	 0.3138298988342285 	 0.21761488914489746 	 0.2826712131500244 	 0.18963360786437988 	 0.7476410865783691 	 1.2752575874328613 	 0.3819711208343506 	 0.6515464782714844 	 
2025-07-24 16:10:17.543118 test begin: paddle.nn.functional.max_pool2d(Tensor([10, 128, 480, 83],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([10, 128, 480, 83],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 50995200 	 1000 	 0.20929265022277832 	 0.2755732536315918 	 0.19097375869750977 	 0.25595879554748535 	 0.6711359024047852 	 1.3533601760864258 	 0.3428633213043213 	 0.6915087699890137 	 
2025-07-24 16:10:21.104601 test begin: paddle.nn.functional.max_pool2d(Tensor([10, 128, 83, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([10, 128, 83, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 50995200 	 1000 	 0.19766736030578613 	 0.2732260227203369 	 0.17891645431518555 	 0.2531311511993408 	 0.6640701293945312 	 1.3563547134399414 	 0.3392524719238281 	 0.6930375099182129 	 
2025-07-24 16:10:24.741384 test begin: paddle.nn.functional.max_pool2d(Tensor([10, 23, 480, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([10, 23, 480, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 52992000 	 1000 	 0.21039795875549316 	 0.2881340980529785 	 0.19218087196350098 	 0.26918792724609375 	 0.697446346282959 	 1.4112012386322021 	 0.3563380241394043 	 0.7210721969604492 	 
2025-07-24 16:10:28.448946 test begin: paddle.nn.functional.max_pool2d(Tensor([1536, 24, 112, 13],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([1536, 24, 112, 13],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 53673984 	 1000 	 0.3636050224304199 	 0.3823845386505127 	 0.345456600189209 	 0.36289286613464355 	 0.5001022815704346 	 1.4794886112213135 	 0.4267427921295166 	 0.755993127822876 	 
2025-07-24 16:10:32.339405 test begin: paddle.nn.functional.max_pool2d(Tensor([1536, 24, 13, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([1536, 24, 13, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 53673984 	 1000 	 1.0732917785644531 	 0.38379836082458496 	 0.31972336769104004 	 0.34778618812561035 	 0.4906604290008545 	 1.4625039100646973 	 0.4250209331512451 	 0.7473263740539551 	 
2025-07-24 16:10:39.974582 test begin: paddle.nn.functional.max_pool2d(Tensor([1536, 3, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([1536, 3, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 57802752 	 1000 	 0.4097423553466797 	 0.3779923915863037 	 0.3868417739868164 	 0.35809326171875 	 0.9111120700836182 	 1.5808336734771729 	 0.46552228927612305 	 0.8077714443206787 	 
2025-07-24 16:10:45.653476 test begin: paddle.nn.functional.max_pool2d(Tensor([169, 24, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([169, 24, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 50878464 	 1000 	 0.3007192611694336 	 0.3333475589752197 	 0.2828981876373291 	 0.3140838146209717 	 0.806720495223999 	 1.3955774307250977 	 0.41274476051330566 	 0.7130937576293945 	 
2025-07-24 16:10:49.563634 test begin: paddle.nn.functional.max_pool2d(Tensor([2, 128, 480, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([2, 128, 480, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, ) 	 58982400 	 1000 	 0.23076176643371582 	 0.32005977630615234 	 0.21233034133911133 	 0.3007314205169678 	 0.7730357646942139 	 1.5697994232177734 	 0.394939661026001 	 0.8021082878112793 	 
2025-07-24 16:10:53.710394 test begin: paddle.nn.functional.max_pool2d(Tensor([2, 64, 704, 704],"float32"), kernel_size=3, stride=2, padding=1, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([2, 64, 704, 704],"float32"), kernel_size=3, stride=2, padding=1, ) 	 63438848 	 1000 	 0.3837294578552246 	 0.4179806709289551 	 0.36635375022888184 	 0.3917524814605713 	 1.008342981338501 	 1.7680809497833252 	 0.5152044296264648 	 0.9034185409545898 	 
2025-07-24 16:10:58.598841 test begin: paddle.nn.functional.max_pool2d(Tensor([8, 13, 704, 704],"float32"), kernel_size=3, stride=2, padding=1, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([8, 13, 704, 704],"float32"), kernel_size=3, stride=2, padding=1, ) 	 51544064 	 1000 	 0.3527348041534424 	 0.35268712043762207 	 0.335695743560791 	 0.32152223587036133 	 0.82242751121521 	 1.4383440017700195 	 0.4201991558074951 	 0.7349183559417725 	 
2025-07-24 16:11:02.678660 test begin: paddle.nn.functional.max_pool2d(Tensor([8, 64, 141, 704],"float32"), kernel_size=3, stride=2, padding=1, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([8, 64, 141, 704],"float32"), kernel_size=3, stride=2, padding=1, ) 	 50823168 	 1000 	 0.30791497230529785 	 0.33713412284851074 	 0.2907555103302002 	 0.31819677352905273 	 0.804908037185669 	 1.4179105758666992 	 0.41123270988464355 	 0.7245082855224609 	 
2025-07-24 16:11:07.340073 test begin: paddle.nn.functional.max_pool2d(Tensor([8, 64, 704, 141],"float32"), kernel_size=3, stride=2, padding=1, )
[Prof] paddle.nn.functional.max_pool2d 	 paddle.nn.functional.max_pool2d(Tensor([8, 64, 704, 141],"float32"), kernel_size=3, stride=2, padding=1, ) 	 50823168 	 1000 	 0.7538750171661377 	 0.33829355239868164 	 0.2837791442871094 	 0.3115653991699219 	 0.806180477142334 	 1.4000968933105469 	 0.41188621520996094 	 0.7153739929199219 	 
2025-07-24 16:11:12.725076 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 16934401],"float32"), Tensor([1, 3, 16934401],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/api_config/config_analyzer.py:1878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  self.paddle_tensor = paddle.to_tensor(
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 16934401],"float32"), Tensor([1, 3, 16934401],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 101606406 	 1000 	 0.061959028244018555 	 0.05506420135498047 	 2.2649765014648438e-05 	 4.4345855712890625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:11:12.978616 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 16934401],"float32"), Tensor([1, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 16934401],"float32"), Tensor([1, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 76204806 	 1000 	 0.06273865699768066 	 0.07076764106750488 	 2.1219253540039062e-05 	 3.9577484130859375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:11:13.214461 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 16934401],"float32"), Tensor([1, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 16934401],"float32"), Tensor([1, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 50803227 	 1000 	 0.06526422500610352 	 0.07240462303161621 	 2.5987625122070312e-05 	 6.914138793945312e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:11:13.451969 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 16934401],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 16934401],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, ) 	 101606406 	 1000 	 2.209228754043579 	 2.3306050300598145 	 1.1282973289489746 	 1.1906390190124512 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:11:24.185382 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 16934401],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 16934401],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 101606406 	 1000 	 2.209167242050171 	 2.3306150436401367 	 1.1282081604003906 	 1.1908133029937744 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:11:34.814033 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float32"), Tensor([1, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float32"), Tensor([1, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 50803206 	 1000 	 0.041912078857421875 	 0.0410914421081543 	 3.62396240234375e-05 	 5.221366882324219e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:11:34.979373 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, ) 	 76204806 	 1000 	 1.1240057945251465 	 1.2004711627960205 	 0.570744514465332 	 0.5995960235595703 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:11:47.694619 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 76204806 	 1000 	 1.121734380722046 	 1.174203634262085 	 0.573462963104248 	 0.5998859405517578 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:11:53.048355 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 8467201],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 8467201],"int32"), kernel_size=2, stride=2, ) 	 50803206 	 1000 	 1.1202640533447266 	 1.174187183380127 	 0.5719537734985352 	 0.5999138355255127 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:11:58.334758 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 8467201],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 8467201],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 50803206 	 1000 	 1.1199061870574951 	 1.1742360591888428 	 0.5717549324035645 	 0.5999271869659424 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:03.682435 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, ) 	 25401627 	 1000 	 1.1247975826263428 	 1.174086332321167 	 0.5718302726745605 	 0.5998101234436035 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:09.206754 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8467201],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 25401627 	 1000 	 1.1198766231536865 	 1.174346685409546 	 0.5715911388397217 	 0.6000056266784668 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:14.594203 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float32"), Tensor([1, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float32"), Tensor([1, 3, 8467201],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 25401627 	 1000 	 0.0435338020324707 	 0.04831242561340332 	 2.2172927856445312e-05 	 3.9577484130859375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:14.765987 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float32"), Tensor([1, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float32"), Tensor([1, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 25401632 	 1000 	 0.04184150695800781 	 0.04108762741088867 	 2.0503997802734375e-05 	 3.9577484130859375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:14.938905 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 25401648 	 1000 	 0.04120492935180664 	 0.04113149642944336 	 2.1219253540039062e-05 	 4.5299530029296875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:15.102374 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, ) 	 50803227 	 1000 	 0.04913735389709473 	 0.04556632041931152 	 2.6702880859375e-05 	 5.2928924560546875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:15.274506 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([1, 3, 16934401],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 50803227 	 1000 	 0.04073524475097656 	 0.03899383544921875 	 2.5987625122070312e-05 	 5.221366882324219e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:15.438298 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, ) 	 50803232 	 1000 	 0.03548407554626465 	 0.040996551513671875 	 1.6927719116210938e-05 	 4.6253204345703125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:15.599154 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 50803232 	 1000 	 0.037018775939941406 	 0.042682647705078125 	 1.4066696166992188e-05 	 5.125999450683594e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:15.759662 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, ) 	 50803248 	 1000 	 0.039705514907836914 	 0.04002666473388672 	 1.4543533325195312e-05 	 4.410743713378906e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:15.920162 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 50803248 	 1000 	 0.0386805534362793 	 0.04282379150390625 	 3.0279159545898438e-05 	 3.7670135498046875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:16.092634 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float32"), Tensor([1, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float32"), Tensor([1, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 50803216 	 1000 	 0.04865741729736328 	 0.04109334945678711 	 1.9311904907226562e-05 	 4.673004150390625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:16.264225 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, ) 	 25401632 	 1000 	 1.1198670864105225 	 1.1741814613342285 	 0.5716838836669922 	 0.5999040603637695 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:21.660935 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 25401632 	 1000 	 1.1218407154083252 	 1.1744580268859863 	 0.5716452598571777 	 0.6000747680664062 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:27.007924 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 3175201, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 3175201, 8],"int32"), kernel_size=2, stride=2, ) 	 50803216 	 1000 	 1.120018482208252 	 1.1742753982543945 	 0.5718855857849121 	 0.5999209880828857 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:32.356967 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 3175201, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 3175201, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 50803216 	 1000 	 1.1199026107788086 	 1.176243782043457 	 0.5717282295227051 	 0.5998406410217285 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:39.571444 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, ) 	 76204816 	 1000 	 1.1199674606323242 	 1.1742010116577148 	 0.5717864036560059 	 0.5998837947845459 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:44.929944 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 3175201, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 76204816 	 1000 	 1.1199901103973389 	 1.1836163997650146 	 0.5718739032745361 	 0.5999231338500977 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:52.652726 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 6350401, 8],"float32"), Tensor([1, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 6350401, 8],"float32"), Tensor([1, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 50803232 	 1000 	 0.061522722244262695 	 0.05418801307678223 	 1.9311904907226562e-05 	 7.224082946777344e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:52.869230 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 6350401, 8],"float32"), Tensor([1, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 6350401, 8],"float32"), Tensor([1, 3175201, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 76204816 	 1000 	 0.061768293380737305 	 0.05377531051635742 	 2.4318695068359375e-05 	 5.435943603515625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:53.079354 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 6350401, 8],"float32"), Tensor([1, 6350401, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 6350401, 8],"float32"), Tensor([1, 6350401, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 101606416 	 1000 	 0.07591509819030762 	 0.07228684425354004 	 3.5762786865234375e-05 	 5.888938903808594e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:12:53.322343 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 6350401, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 6350401, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, ) 	 101606416 	 1000 	 2.208864212036133 	 2.3305630683898926 	 1.128075361251831 	 1.190687656402588 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:13:03.919421 test begin: paddle.nn.functional.max_unpool1d(Tensor([1, 6350401, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1, 6350401, 8],"float64"), Tensor([1, 6350401, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 101606416 	 1000 	 2.208432912826538 	 2.3307995796203613 	 1.1280202865600586 	 1.1909408569335938 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:13:14.622049 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 50803248 	 1000 	 0.06295919418334961 	 0.0652320384979248 	 2.9325485229492188e-05 	 8.034706115722656e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:13:14.852874 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, ) 	 25401648 	 1000 	 1.121901512145996 	 1.1743769645690918 	 0.5722801685333252 	 0.5999956130981445 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:13:20.442645 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 25401648 	 1000 	 1.1257712841033936 	 1.1742744445800781 	 0.5720317363739014 	 0.5998785495758057 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:13:25.776085 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([1058401, 3, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([1058401, 3, 8],"int32"), kernel_size=2, stride=2, ) 	 50803248 	 1000 	 1.120696783065796 	 1.1743345260620117 	 0.572258472442627 	 0.5999579429626465 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:13:31.083325 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([1058401, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([1058401, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 50803248 	 1000 	 1.1205012798309326 	 1.1748709678649902 	 0.5719366073608398 	 0.599869966506958 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:13:37.945099 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, ) 	 76204848 	 1000 	 1.126312017440796 	 1.1809358596801758 	 0.5706396102905273 	 0.6000385284423828 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:13:44.307465 test begin: paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([1058401, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 76204848 	 1000 	 1.1206610202789307 	 1.1744866371154785 	 0.5720853805541992 	 0.5999932289123535 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:13:49.574680 test begin: paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([1, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([1, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 50803248 	 1000 	 0.04387664794921875 	 0.04176497459411621 	 2.9325485229492188e-05 	 4.673004150390625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:13:49.742863 test begin: paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([1058401, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 76204848 	 1000 	 0.04138493537902832 	 0.04138755798339844 	 1.2636184692382812e-05 	 6.437301635742188e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:13:49.914573 test begin: paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([2116801, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float32"), Tensor([2116801, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], ) 	 101606448 	 1000 	 0.04138636589050293 	 0.04127645492553711 	 2.5033950805664062e-05 	 3.1948089599609375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:13:50.076267 test begin: paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, ) 	 101606448 	 1000 	 2.3075504302978516 	 2.332674264907837 	 1.2268083095550537 	 1.1907179355621338 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:01.205319 test begin: paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
[Prof] paddle.nn.functional.max_unpool1d 	 paddle.nn.functional.max_unpool1d(Tensor([2116801, 3, 8],"float64"), Tensor([2116801, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, ) 	 101606448 	 1000 	 2.2082271575927734 	 2.330481767654419 	 1.1277117729187012 	 1.1905815601348877 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:11.883534 test begin: paddle.nn.functional.max_unpool2d(Tensor([1894, 8, 86, 39],"float32"), Tensor([1894, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([1894, 8, 86, 39],"float32"), Tensor([1894, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 101639616 	 1000 	 0.05910825729370117 	 0.08561944961547852 	 0.0299680233001709 	 0.04358863830566406 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:12.316223 test begin: paddle.nn.functional.max_unpool2d(Tensor([1894, 8, 86, 39],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([1894, 8, 86, 39],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 52537056 	 1000 	 0.05874490737915039 	 0.08567452430725098 	 0.02997565269470215 	 0.043604135513305664 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:12.731104 test begin: paddle.nn.functional.max_unpool2d(Tensor([3887, 16, 43, 19],"float32"), Tensor([3887, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([3887, 16, 43, 19],"float32"), Tensor([3887, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 101621728 	 1000 	 0.026495695114135742 	 0.03161001205444336 	 2.6226043701171875e-05 	 4.744529724121094e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:12.946260 test begin: paddle.nn.functional.max_unpool2d(Tensor([3887, 16, 43, 19],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([3887, 16, 43, 19],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 51647472 	 1000 	 0.0214688777923584 	 0.030890226364135742 	 2.8848648071289062e-05 	 4.696846008300781e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:13.138307 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 16, 2612, 19],"float32"), Tensor([64, 16, 2612, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 16, 2612, 19],"float32"), Tensor([64, 16, 2612, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 101638144 	 1000 	 0.022075414657592773 	 0.03311967849731445 	 2.384185791015625e-05 	 4.744529724121094e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:13.334831 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 16, 2612, 19],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 16, 2612, 19],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 51655680 	 1000 	 0.02153325080871582 	 0.03062272071838379 	 2.47955322265625e-05 	 6.389617919921875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:13.525621 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 1154],"float32"), Tensor([64, 16, 43, 1154],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 1154],"float32"), Tensor([64, 16, 43, 1154],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 101625856 	 1000 	 0.02156686782836914 	 0.037885427474975586 	 2.3603439331054688e-05 	 4.0531158447265625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:13.774366 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 1154],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 1154],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 51649536 	 1000 	 0.02904224395751953 	 0.04949951171875 	 3.1948089599609375e-05 	 4.601478576660156e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:14.011172 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([3887, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([3887, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 51647472 	 1000 	 0.021484375 	 0.03350424766540527 	 2.6941299438476562e-05 	 4.172325134277344e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:14.209232 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([64, 16, 2612, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([64, 16, 2612, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 51655680 	 1000 	 0.02202892303466797 	 0.033190011978149414 	 2.4080276489257812e-05 	 4.7206878662109375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:14.409621 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([64, 16, 43, 1154],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([64, 16, 43, 1154],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 51649536 	 1000 	 0.021951675415039062 	 0.03319501876831055 	 2.5510787963867188e-05 	 5.0067901611328125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:14.603352 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([64, 972, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([64, 972, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 51660544 	 1000 	 0.02256178855895996 	 0.03165769577026367 	 2.9325485229492188e-05 	 4.315376281738281e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:14.800262 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 237, 86, 39],"float32"), Tensor([64, 237, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 237, 86, 39],"float32"), Tensor([64, 237, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 101746944 	 1000 	 0.05871915817260742 	 0.08563661575317383 	 0.02997756004333496 	 0.04361438751220703 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:15.222121 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 237, 86, 39],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 237, 86, 39],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 52590720 	 1000 	 0.05876326560974121 	 0.08561015129089355 	 0.03000354766845703 	 0.04361438751220703 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:15.640861 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 1182],"float32"), Tensor([64, 32, 21, 1182],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 1182],"float32"), Tensor([64, 32, 21, 1182],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 101670912 	 1000 	 0.02188420295715332 	 0.03125905990600586 	 1.71661376953125e-05 	 5.7697296142578125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:15.797456 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 1182],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 1182],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 51222528 	 1000 	 0.02179884910583496 	 0.03663921356201172 	 1.6927719116210938e-05 	 4.291534423828125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:15.955154 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([64, 32, 21, 1182],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([64, 32, 21, 1182],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 51222528 	 1000 	 0.022380352020263672 	 0.03367733955383301 	 1.621246337890625e-05 	 5.459785461425781e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:16.120699 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([64, 32, 2757, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([64, 32, 2757, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 51204096 	 1000 	 0.021720170974731445 	 0.031247854232788086 	 1.5974044799804688e-05 	 6.103515625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:16.273843 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([64, 4201, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([64, 4201, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 51202368 	 1000 	 0.023041725158691406 	 0.03269815444946289 	 2.384185791015625e-05 	 6.29425048828125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:16.429590 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([8401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([8401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 51196320 	 1000 	 0.022031068801879883 	 0.03178906440734863 	 1.9311904907226562e-05 	 3.886222839355469e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:16.595136 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 32, 2757, 9],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 32, 2757, 9],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 51204096 	 1000 	 0.021923065185546875 	 0.03182268142700195 	 1.5974044799804688e-05 	 4.0531158447265625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:16.751762 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 32, 2757, 9],"float32"), Tensor([64, 32, 2757, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 32, 2757, 9],"float32"), Tensor([64, 32, 2757, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 101634048 	 1000 	 0.022596120834350586 	 0.032860517501831055 	 1.6927719116210938e-05 	 6.29425048828125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:16.912797 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 4201, 21, 9],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 4201, 21, 9],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 51202368 	 1000 	 0.022056102752685547 	 0.032799482345581055 	 1.6689300537109375e-05 	 4.076957702636719e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:17.070530 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 4201, 21, 9],"float32"), Tensor([64, 4201, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 4201, 21, 9],"float32"), Tensor([64, 4201, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 101630592 	 1000 	 0.02189803123474121 	 0.03243660926818848 	 1.9073486328125e-05 	 3.838539123535156e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:17.229518 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 8, 2545, 39],"float32"), Tensor([64, 8, 2545, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 8, 2545, 39],"float32"), Tensor([64, 8, 2545, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 101637120 	 1000 	 0.058759212493896484 	 0.08563423156738281 	 0.029999494552612305 	 0.04362344741821289 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:17.644510 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 8, 2545, 39],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 8, 2545, 39],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 52535808 	 1000 	 0.05874156951904297 	 0.0856471061706543 	 0.03000020980834961 	 0.04360795021057129 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:18.069099 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 1154],"float32"), Tensor([64, 8, 86, 1154],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 1154],"float32"), Tensor([64, 8, 86, 1154],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 101625856 	 1000 	 0.05873751640319824 	 0.08560562133789062 	 0.029989957809448242 	 0.04359745979309082 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:18.495954 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 1154],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 1154],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 52530176 	 1000 	 0.05874824523925781 	 0.08564972877502441 	 0.0299985408782959 	 0.043636322021484375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:18.909877 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([1894, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([1894, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 52537056 	 1000 	 0.058754682540893555 	 0.0858154296875 	 0.02998661994934082 	 0.04217886924743652 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:19.335354 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([64, 237, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([64, 237, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 52590720 	 1000 	 0.05877971649169922 	 0.0856623649597168 	 0.024642229080200195 	 0.04361271858215332 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:19.767693 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([64, 8, 2545, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([64, 8, 2545, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 52535808 	 1000 	 0.05878782272338867 	 0.0856173038482666 	 0.030017614364624023 	 0.043604373931884766 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:20.190969 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([64, 8, 86, 1154],"int32"), 2, 2, output_size=list[64,8,172,79,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([64, 8, 86, 1154],"int32"), 2, 2, output_size=list[64,8,172,79,], ) 	 52530176 	 1000 	 0.05875110626220703 	 0.08560347557067871 	 0.030005931854248047 	 0.04361248016357422 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:20.610377 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 972, 43, 19],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 972, 43, 19],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 51660544 	 1000 	 0.030980348587036133 	 0.0403134822845459 	 2.0503997802734375e-05 	 4.935264587402344e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:20.830399 test begin: paddle.nn.functional.max_unpool2d(Tensor([64, 972, 43, 19],"float32"), Tensor([64, 972, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([64, 972, 43, 19],"float32"), Tensor([64, 972, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], ) 	 101647872 	 1000 	 0.030925989151000977 	 0.04017162322998047 	 2.1219253540039062e-05 	 6.723403930664062e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:21.052248 test begin: paddle.nn.functional.max_unpool2d(Tensor([8401, 32, 21, 9],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([8401, 32, 21, 9],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 51196320 	 1000 	 0.03129458427429199 	 0.03982353210449219 	 2.3603439331054688e-05 	 4.649162292480469e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:21.233351 test begin: paddle.nn.functional.max_unpool2d(Tensor([8401, 32, 21, 9],"float32"), Tensor([8401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
[Prof] paddle.nn.functional.max_unpool2d 	 paddle.nn.functional.max_unpool2d(Tensor([8401, 32, 21, 9],"float32"), Tensor([8401, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], ) 	 101618496 	 1000 	 0.030969619750976562 	 0.043489933013916016 	 1.4781951904296875e-05 	 6.29425048828125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:21.421535 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 211681, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 211681, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50803440 	 1000 	 1.6979577541351318 	 1.7693989276885986 	 0.8675415515899658 	 0.16374659538269043 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:31.688320 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 211681, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 211681, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 50803440 	 1000 	 2.6368563175201416 	 2.598856210708618 	 1.3455524444580078 	 0.18967413902282715 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:47.882370 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 211681, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 211681, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50803440 	 1000 	 1.7231040000915527 	 1.7696256637573242 	 0.8797454833984375 	 0.16377687454223633 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:14:58.241422 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25402080 	 1000 	 2.1430957317352295 	 1.769944429397583 	 0.8664274215698242 	 0.16380667686462402 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:15:09.937286 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 25402080 	 1000 	 2.6318225860595703 	 2.599970579147339 	 1.344191312789917 	 0.18967843055725098 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:15:25.252825 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25402080 	 1000 	 1.721264362335205 	 1.7689850330352783 	 0.879274845123291 	 0.16370606422424316 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:15:37.069419 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 76205040 	 1000 	 1.699979305267334 	 1.7685062885284424 	 0.8677763938903809 	 0.16367888450622559 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:15:47.479353 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 211681, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 76205040 	 1000 	 2.6318628787994385 	 2.602670431137085 	 1.3440709114074707 	 0.18998408317565918 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:02.708800 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 282241, 5, 6],"float64"), Tensor([1, 3, 282241, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 282241, 5, 6],"float64"), Tensor([1, 3, 282241, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50803380 	 1000 	 0.029008865356445312 	 0.039490699768066406 	 1.2636184692382812e-05 	 5.1021575927734375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:02.840716 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 282241, 5, 6],"float64"), Tensor([1, 3, 282241, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 282241, 5, 6],"float64"), Tensor([1, 3, 282241, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 50803380 	 1000 	 0.02891707420349121 	 0.033620357513427734 	 1.430511474609375e-05 	 3.552436828613281e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:02.964888 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 282241, 5, 6],"float64"), Tensor([1, 3, 282241, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 282241, 5, 6],"float64"), Tensor([1, 3, 282241, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50803380 	 1000 	 0.029561281204223633 	 0.035363197326660156 	 1.4781951904296875e-05 	 3.552436828613281e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:03.092741 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 282241, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 282241, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25402050 	 1000 	 0.029057025909423828 	 0.03462100028991699 	 3.457069396972656e-05 	 4.553794860839844e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:03.220177 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 282241, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 282241, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 25402050 	 1000 	 0.02940845489501953 	 0.03349041938781738 	 2.384185791015625e-05 	 3.838539123535156e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:04.544487 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 282241, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 282241, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25402050 	 1000 	 0.029114723205566406 	 0.034253597259521484 	 2.2411346435546875e-05 	 4.57763671875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:05.223067 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 282241, 5, 6],"float64"), Tensor([1, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 282241, 5, 6],"float64"), Tensor([1, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 76204980 	 1000 	 0.03136444091796875 	 0.033414363861083984 	 2.7894973754882812e-05 	 4.8160552978515625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:05.353263 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 282241, 5, 6],"float64"), Tensor([1, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 282241, 5, 6],"float64"), Tensor([1, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 76204980 	 1000 	 0.028998613357543945 	 0.034195899963378906 	 1.7404556274414062e-05 	 4.482269287109375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:05.479017 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 352801, 6],"float64"), Tensor([1, 3, 4, 352801, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 352801, 6],"float64"), Tensor([1, 3, 4, 352801, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50803344 	 1000 	 0.028933048248291016 	 0.03437328338623047 	 1.1205673217773438e-05 	 5.1975250244140625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:05.606954 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 352801, 6],"float64"), Tensor([1, 3, 4, 352801, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 352801, 6],"float64"), Tensor([1, 3, 4, 352801, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 50803344 	 1000 	 0.03167986869812012 	 0.034680843353271484 	 1.7404556274414062e-05 	 4.029273986816406e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:06.891526 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 352801, 6],"float64"), Tensor([1, 3, 4, 352801, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 352801, 6],"float64"), Tensor([1, 3, 4, 352801, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50803344 	 1000 	 0.030370235443115234 	 0.03448772430419922 	 2.193450927734375e-05 	 6.198883056640625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:07.344528 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 352801, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 352801, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25402032 	 1000 	 0.029052019119262695 	 0.03644895553588867 	 1.0967254638671875e-05 	 4.076957702636719e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:08.298070 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 352801, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 352801, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 25402032 	 1000 	 0.029750823974609375 	 0.0336301326751709 	 1.4781951904296875e-05 	 3.5762786865234375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:08.426776 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 352801, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 352801, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25402032 	 1000 	 0.029126882553100586 	 0.03382992744445801 	 1.0967254638671875e-05 	 3.075599670410156e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:08.549626 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 352801, 6],"float64"), Tensor([1, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 352801, 6],"float64"), Tensor([1, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 76204944 	 1000 	 0.028980255126953125 	 0.034004926681518555 	 1.0728836059570312e-05 	 5.6743621826171875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:08.672909 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 352801, 6],"float64"), Tensor([1, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 352801, 6],"float64"), Tensor([1, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 76204944 	 1000 	 0.030094385147094727 	 0.034949541091918945 	 2.4080276489257812e-05 	 4.315376281738281e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:08.799211 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 423361],"float64"), Tensor([1, 3, 4, 5, 423361],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 423361],"float64"), Tensor([1, 3, 4, 5, 423361],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50803320 	 1000 	 0.029064416885375977 	 0.03459572792053223 	 1.1682510375976562e-05 	 4.0531158447265625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:08.930604 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 423361],"float64"), Tensor([1, 3, 4, 5, 423361],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 423361],"float64"), Tensor([1, 3, 4, 5, 423361],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 50803320 	 1000 	 0.03856086730957031 	 0.04437375068664551 	 1.3828277587890625e-05 	 3.790855407714844e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:09.084970 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 423361],"float64"), Tensor([1, 3, 4, 5, 423361],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 423361],"float64"), Tensor([1, 3, 4, 5, 423361],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50803320 	 1000 	 0.039154767990112305 	 0.04499959945678711 	 1.6689300537109375e-05 	 5.4836273193359375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:09.243313 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 423361],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 423361],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25402020 	 1000 	 0.0289459228515625 	 0.033539533615112305 	 1.33514404296875e-05 	 4.172325134277344e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:09.366348 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 423361],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 423361],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 25402020 	 1000 	 0.028825044631958008 	 0.03336024284362793 	 1.4543533325195312e-05 	 4.124641418457031e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:09.489168 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 423361],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 423361],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25402020 	 1000 	 0.029132366180419922 	 0.0358734130859375 	 1.4781951904296875e-05 	 3.4809112548828125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:09.614364 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 423361],"float64"), Tensor([1, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 423361],"float64"), Tensor([1, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 76204920 	 1000 	 0.029716968536376953 	 0.036597251892089844 	 1.4066696166992188e-05 	 5.0067901611328125e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:09.743159 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 423361],"float64"), Tensor([1, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 423361],"float64"), Tensor([1, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 76204920 	 1000 	 0.029185771942138672 	 0.03404712677001953 	 1.1920928955078125e-05 	 6.4849853515625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:09.874769 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 211681, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 211681, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25402080 	 1000 	 0.030397415161132812 	 0.03345227241516113 	 2.956390380859375e-05 	 3.695487976074219e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:09.999769 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 282241, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 282241, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25402050 	 1000 	 0.028864383697509766 	 0.0336151123046875 	 1.1920928955078125e-05 	 4.5299530029296875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:10.122021 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 352801, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 352801, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25402032 	 1000 	 0.028967857360839844 	 0.0370180606842041 	 1.6927719116210938e-05 	 4.863739013671875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:10.248634 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 423361],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 423361],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25402020 	 1000 	 0.033496856689453125 	 0.036538124084472656 	 1.5974044799804688e-05 	 4.458427429199219e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:10.380999 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50803620 	 1000 	 0.02908039093017578 	 0.03364253044128418 	 1.8596649169921875e-05 	 4.267692565917969e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:10.505091 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 50803620 	 1000 	 0.029261350631713867 	 0.03382134437561035 	 1.4781951904296875e-05 	 4.9114227294921875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:10.629680 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50803632 	 1000 	 0.032225847244262695 	 0.03384542465209961 	 2.2649765014648438e-05 	 4.982948303222656e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:10.758249 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 50803632 	 1000 	 0.028942584991455078 	 0.03375434875488281 	 1.2636184692382812e-05 	 5.9604644775390625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:10.894885 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50803650 	 1000 	 0.029132843017578125 	 0.035067081451416016 	 1.8596649169921875e-05 	 4.3392181396484375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:11.020940 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 50803650 	 1000 	 0.02912425994873047 	 0.03380560874938965 	 1.2636184692382812e-05 	 4.8160552978515625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:11.145898 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50803680 	 1000 	 0.0288236141204834 	 0.03493785858154297 	 1.4781951904296875e-05 	 3.933906555175781e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:11.274860 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 50803680 	 1000 	 0.029488086700439453 	 0.03435206413269043 	 1.5735626220703125e-05 	 3.933906555175781e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:11.400899 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50803920 	 1000 	 0.029007434844970703 	 0.034746408462524414 	 1.2159347534179688e-05 	 3.910064697265625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:11.525283 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 50803920 	 1000 	 0.02928757667541504 	 0.03449749946594238 	 1.3828277587890625e-05 	 4.673004150390625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:11.649533 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25402320 	 1000 	 0.029317855834960938 	 0.03340888023376465 	 1.3589859008789062e-05 	 3.0040740966796875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:11.772345 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 846721],"float64"), Tensor([1, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 846721],"float64"), Tensor([1, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 101606520 	 1000 	 0.028975963592529297 	 0.036281585693359375 	 9.775161743164062e-06 	 4.172325134277344e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:11.900980 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 846721],"float64"), Tensor([1, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 846721],"float64"), Tensor([1, 3, 4, 5, 846721],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 101606520 	 1000 	 0.02914738655090332 	 0.0334630012512207 	 1.0728836059570312e-05 	 2.9087066650390625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:12.024619 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 705601, 6],"float64"), Tensor([1, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 705601, 6],"float64"), Tensor([1, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 101606544 	 1000 	 0.02898383140563965 	 0.03463912010192871 	 1.239776611328125e-05 	 4.3392181396484375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:12.154747 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 705601, 6],"float64"), Tensor([1, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 705601, 6],"float64"), Tensor([1, 3, 4, 705601, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 101606544 	 1000 	 0.028944015502929688 	 0.0339360237121582 	 1.4543533325195312e-05 	 4.863739013671875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:12.277993 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 564481, 5, 6],"float64"), Tensor([1, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 564481, 5, 6],"float64"), Tensor([1, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 101606580 	 1000 	 0.03336668014526367 	 0.03362870216369629 	 1.52587890625e-05 	 4.482269287109375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:12.408841 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 3, 564481, 5, 6],"float64"), Tensor([1, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 3, 564481, 5, 6],"float64"), Tensor([1, 3, 564481, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 101606580 	 1000 	 0.02915811538696289 	 0.0333714485168457 	 1.1205673217773438e-05 	 3.170967102050781e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:12.531517 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 423361, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 423361, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 101606640 	 1000 	 3.3958637714385986 	 3.547971725463867 	 1.734743595123291 	 0.16413283348083496 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:16:33.063335 test begin: paddle.nn.functional.max_unpool3d(Tensor([1, 423361, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([1, 423361, 4, 5, 6],"float64"), Tensor([1, 423361, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 101606640 	 1000 	 5.26097846031189 	 5.182018041610718 	 2.686932325363159 	 0.18821239471435547 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:17:03.293356 test begin: paddle.nn.functional.max_unpool3d(Tensor([141121, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([141121, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 101607120 	 1000 	 3.3992016315460205 	 3.5694520473480225 	 1.7348284721374512 	 0.1651475429534912 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:17:25.937013 test begin: paddle.nn.functional.max_unpool3d(Tensor([141121, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([141121, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 101607120 	 1000 	 5.26881742477417 	 5.183719158172607 	 2.691382884979248 	 0.18818974494934082 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:17:57.693945 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25402320 	 1000 	 1.6960313320159912 	 1.7697279453277588 	 0.86614990234375 	 0.163771390914917 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:18:07.787008 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 25402320 	 1000 	 2.632274866104126 	 2.5990374088287354 	 1.344494104385376 	 0.18967556953430176 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:18:22.838288 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 25402320 	 1000 	 1.7213325500488281 	 1.7718651294708252 	 0.8790326118469238 	 0.16384482383728027 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:18:33.097812 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 76205520 	 1000 	 1.7051975727081299 	 1.7756524085998535 	 0.8678152561187744 	 0.1638789176940918 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:18:44.404500 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([141121, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 76205520 	 1000 	 2.6319198608398438 	 2.5989878177642822 	 1.3439733982086182 	 0.18969225883483887 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:18:59.459648 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50803920 	 1000 	 1.6988239288330078 	 1.7686975002288818 	 0.8675920963287354 	 0.16373729705810547 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:19:09.706898 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, ) 	 50803920 	 1000 	 2.6346070766448975 	 2.6057639122009277 	 1.3447027206420898 	 0.18997693061828613 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:19:27.043872 test begin: paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
[Prof] paddle.nn.functional.max_unpool3d 	 paddle.nn.functional.max_unpool3d(Tensor([70561, 3, 4, 5, 6],"float64"), Tensor([70561, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, ) 	 50803920 	 1000 	 1.7214748859405518 	 1.793790340423584 	 0.8791990280151367 	 0.16413331031799316 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:19:39.793663 test begin: paddle.nn.functional.maxout(Tensor([100, 4, 21169, 3],"float64"), 2, 1, None, )
[Error] shape '[100, 0, 2, 2, 2, 21169, 3]' is invalid for input of size 25402800
2025-07-24 16:19:41.634171 test begin: paddle.nn.functional.maxout(Tensor([100, 4, 3, 21169],"float64"), 2, 1, None, )
[Error] shape '[100, 0, 2, 2, 2, 3, 21169]' is invalid for input of size 25402800
2025-07-24 16:19:43.444767 test begin: paddle.nn.functional.maxout(Tensor([100, 4, 3, 42337],"float32"), 2, 1, None, )
[Error] shape '[100, 0, 2, 2, 2, 3, 42337]' is invalid for input of size 50804400
2025-07-24 16:19:45.937759 test begin: paddle.nn.functional.maxout(Tensor([100, 4, 42337, 3],"float32"), 2, 1, None, )
[Error] shape '[100, 0, 2, 2, 2, 42337, 3]' is invalid for input of size 50804400
2025-07-24 16:19:48.353392 test begin: paddle.nn.functional.maxout(Tensor([1411201, 4, 3, 3],"float32"), 2, 1, None, )
[Error] shape '[1411201, 0, 2, 2, 2, 3, 3]' is invalid for input of size 50803236
2025-07-24 16:19:50.774465 test begin: paddle.nn.functional.maxout(Tensor([705601, 4, 3, 3],"float64"), 2, 1, None, )
[Error] shape '[705601, 0, 2, 2, 2, 3, 3]' is invalid for input of size 25401636
2025-07-24 16:19:52.572763 test begin: paddle.nn.functional.maxout(x=Tensor([100, 4, 3, 42337],"float32"), groups=2, )
[Error] shape '[100, 0, 2, 2, 2, 3, 42337]' is invalid for input of size 50804400
2025-07-24 16:19:54.974916 test begin: paddle.nn.functional.maxout(x=Tensor([100, 4, 42337, 3],"float32"), groups=2, )
[Error] shape '[100, 0, 2, 2, 2, 42337, 3]' is invalid for input of size 50804400
2025-07-24 16:19:57.407612 test begin: paddle.nn.functional.maxout(x=Tensor([1411201, 4, 3, 3],"float32"), groups=2, )
[Error] shape '[1411201, 0, 2, 2, 2, 3, 3]' is invalid for input of size 50803236
2025-07-24 16:19:59.821086 test begin: paddle.nn.functional.mish(Tensor([12, 10585, 20, 20],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 10585, 20, 20],"float32"), ) 	 50808000 	 1000 	 0.3055586814880371 	 0.3004026412963867 	 0.29598522186279297 	 0.28466367721557617 	 0.4534292221069336 	 0.4544234275817871 	 0.402324914932251 	 0.3846578598022461 	 
2025-07-24 16:20:03.019640 test begin: paddle.nn.functional.mish(Tensor([12, 128, 40, 827],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 128, 40, 827],"float32"), ) 	 50810880 	 1000 	 0.3059241771697998 	 0.30027127265930176 	 0.2962021827697754 	 0.28484249114990234 	 0.45363545417785645 	 0.45450925827026367 	 0.4030940532684326 	 0.38457512855529785 	 
2025-07-24 16:20:06.190698 test begin: paddle.nn.functional.mish(Tensor([12, 128, 827, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 128, 827, 40],"float32"), ) 	 50810880 	 1000 	 0.3054497241973877 	 0.303424596786499 	 0.2961249351501465 	 0.28410792350769043 	 0.45377326011657715 	 0.4545583724975586 	 0.40134477615356445 	 0.3857743740081787 	 
2025-07-24 16:20:09.421399 test begin: paddle.nn.functional.mish(Tensor([12, 256, 40, 414],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 256, 40, 414],"float32"), ) 	 50872320 	 1000 	 0.3060617446899414 	 0.30054521560668945 	 0.2963581085205078 	 0.2848832607269287 	 0.453798770904541 	 0.454817533493042 	 0.40286970138549805 	 0.38231801986694336 	 
2025-07-24 16:20:12.596169 test begin: paddle.nn.functional.mish(Tensor([12, 256, 414, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 256, 414, 40],"float32"), ) 	 50872320 	 1000 	 0.3055086135864258 	 0.3014719486236572 	 0.29625415802001953 	 0.28463053703308105 	 0.45429491996765137 	 0.45496535301208496 	 0.3905925750732422 	 0.3853633403778076 	 
2025-07-24 16:20:15.767739 test begin: paddle.nn.functional.mish(Tensor([12, 2647, 40, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 2647, 40, 40],"float32"), ) 	 50822400 	 1000 	 0.30583786964416504 	 0.30034470558166504 	 0.29651570320129395 	 0.2847418785095215 	 0.453885555267334 	 0.454500675201416 	 0.401430606842041 	 0.38570237159729004 	 
2025-07-24 16:20:19.025575 test begin: paddle.nn.functional.mish(Tensor([12, 512, 20, 414],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 512, 20, 414],"float32"), ) 	 50872320 	 1000 	 0.3055853843688965 	 0.3006250858306885 	 0.29626941680908203 	 0.27971315383911133 	 0.4540688991546631 	 0.45500755310058594 	 0.39946508407592773 	 0.3855719566345215 	 
2025-07-24 16:20:22.202283 test begin: paddle.nn.functional.mish(Tensor([12, 512, 414, 20],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([12, 512, 414, 20],"float32"), ) 	 50872320 	 1000 	 0.30549192428588867 	 0.30759286880493164 	 0.2962062358856201 	 0.2847745418548584 	 0.4544193744659424 	 0.45502495765686035 	 0.40340685844421387 	 0.38272833824157715 	 
2025-07-24 16:20:28.262306 test begin: paddle.nn.functional.mish(Tensor([125, 256, 40, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([125, 256, 40, 40],"float32"), ) 	 51200000 	 1000 	 0.3101081848144531 	 0.3025858402252197 	 0.2983546257019043 	 0.2870054244995117 	 0.4572176933288574 	 0.45780181884765625 	 0.40659666061401367 	 0.38888049125671387 	 
2025-07-24 16:20:31.461656 test begin: paddle.nn.functional.mish(Tensor([249, 128, 40, 40],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([249, 128, 40, 40],"float32"), ) 	 50995200 	 1000 	 0.30613255500793457 	 0.3034474849700928 	 0.29695749282836914 	 0.28532958030700684 	 0.4553351402282715 	 0.45603132247924805 	 0.4053952693939209 	 0.38387513160705566 	 
2025-07-24 16:20:34.672134 test begin: paddle.nn.functional.mish(Tensor([249, 512, 20, 20],"float32"), )
[Prof] paddle.nn.functional.mish 	 paddle.nn.functional.mish(Tensor([249, 512, 20, 20],"float32"), ) 	 50995200 	 1000 	 1.0341951847076416 	 0.31173110008239746 	 0.29673337936401367 	 0.282822847366333 	 0.4551098346710205 	 0.45604562759399414 	 0.4050276279449463 	 0.3847987651824951 	 
2025-07-24 16:20:40.174421 test begin: paddle.nn.functional.mse_loss(Tensor([24904, 12, 170, 1],"float32"), Tensor([24904, 12, 170, 1],"float32"), "mean", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([24904, 12, 170, 1],"float32"), Tensor([24904, 12, 170, 1],"float32"), "mean", ) 	 101608320 	 1000 	 0.8939504623413086 	 0.601165771484375 	 0.22811031341552734 	 0.20369839668273926 	 1.0586552619934082 	 1.1604726314544678 	 0.36078476905822754 	 0.2964630126953125 	 
2025-07-24 16:20:45.700348 test begin: paddle.nn.functional.mse_loss(Tensor([3548, 12, 1194, 1],"float32"), Tensor([3548, 12, 1194, 1],"float32"), "mean", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([3548, 12, 1194, 1],"float32"), Tensor([3548, 12, 1194, 1],"float32"), "mean", ) 	 101671488 	 1000 	 0.8946971893310547 	 0.5991630554199219 	 0.22832107543945312 	 0.203810453414917 	 1.0591552257537842 	 1.1611647605895996 	 0.36101245880126953 	 0.29666972160339355 	 
2025-07-24 16:20:51.070990 test begin: paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 8],"float32"), "mean", )
/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py:104: UserWarning: Using a target size (torch.Size([3548, 12, 170, 8])) that is different to the input size (torch.Size([3548, 12, 170, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return func(*args, **kwargs)
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 8],"float32"), "mean", ) 	 65141280 	 1000 	 0.866668701171875 	 0.5752885341644287 	 0.2206592559814453 	 0.1957685947418213 	 1.547368049621582 	 1.618156909942627 	 0.3954787254333496 	 0.33103322982788086 	 
2025-07-24 16:20:56.795216 test begin: paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 1],"float32"), "mean", )
/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py:104: UserWarning: Using a target size (torch.Size([3548, 12, 170, 1])) that is different to the input size (torch.Size([3548, 12, 170, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return func(*args, **kwargs)
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 1],"float32"), "mean", ) 	 65141280 	 1000 	 0.8654837608337402 	 0.5758798122406006 	 0.2209322452545166 	 0.19557952880859375 	 1.1372885704040527 	 1.6180341243743896 	 0.3876152038574219 	 0.3309907913208008 	 
2025-07-24 16:21:02.101204 test begin: paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 8],"float32"), "mean", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 8],"float32"), "mean", ) 	 115806720 	 1000 	 1.0189764499664307 	 0.7126543521881104 	 0.2594327926635742 	 0.24143624305725098 	 1.2070059776306152 	 1.3197855949401855 	 0.411334753036499 	 0.33760857582092285 	 
2025-07-24 16:21:09.932666 test begin: paddle.nn.functional.mse_loss(Tensor([3548, 85, 170, 1],"float32"), Tensor([3548, 85, 170, 1],"float32"), "mean", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([3548, 85, 170, 1],"float32"), Tensor([3548, 85, 170, 1],"float32"), "mean", ) 	 102537200 	 1000 	 0.9021120071411133 	 0.6041207313537598 	 0.2302241325378418 	 0.20544815063476562 	 1.0679116249084473 	 1.1707310676574707 	 0.3639833927154541 	 0.29901838302612305 	 
2025-07-24 16:21:15.353171 test begin: paddle.nn.functional.mse_loss(Tensor([517, 4, 3, 64, 128],"float32"), Tensor([517, 4, 3, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([517, 4, 3, 64, 128],"float32"), Tensor([517, 4, 3, 64, 128],"float32"), "none", ) 	 101646336 	 1000 	 0.7488782405853271 	 0.44696688652038574 	 0.3813023567199707 	 0.4231138229370117 	 0.9251034259796143 	 1.4462103843688965 	 0.47260546684265137 	 0.3694915771484375 	 
2025-07-24 16:21:21.453662 test begin: paddle.nn.functional.mse_loss(Tensor([64, 3, 3, 64, 1379],"float32"), Tensor([64, 3, 3, 64, 1379],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 3, 3, 64, 1379],"float32"), Tensor([64, 3, 3, 64, 1379],"float32"), "none", ) 	 101670912 	 1000 	 0.7465388774871826 	 0.4471549987792969 	 0.38138628005981445 	 0.42325544357299805 	 0.9252793788909912 	 1.4465210437774658 	 0.4726738929748535 	 0.3695075511932373 	 
2025-07-24 16:21:27.489623 test begin: paddle.nn.functional.mse_loss(Tensor([64, 3, 3, 690, 128],"float32"), Tensor([64, 3, 3, 690, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 3, 3, 690, 128],"float32"), Tensor([64, 3, 3, 690, 128],"float32"), "none", ) 	 101744640 	 1000 	 0.7621743679046631 	 0.44749903678894043 	 0.38158464431762695 	 0.4227135181427002 	 0.9255585670471191 	 1.4481048583984375 	 0.4728367328643799 	 0.3698406219482422 	 
2025-07-24 16:21:37.111872 test begin: paddle.nn.functional.mse_loss(Tensor([64, 3, 33, 64, 128],"float32"), Tensor([64, 3, 33, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 3, 33, 64, 128],"float32"), Tensor([64, 3, 33, 64, 128],"float32"), "none", ) 	 103809024 	 1000 	 0.7684216499328613 	 0.4564790725708008 	 0.3892374038696289 	 0.432340145111084 	 0.9430744647979736 	 1.4764139652252197 	 0.4818708896636963 	 0.3771240711212158 	 
2025-07-24 16:21:43.878498 test begin: paddle.nn.functional.mse_loss(Tensor([64, 33, 3, 64, 128],"float32"), Tensor([64, 33, 3, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 33, 3, 64, 128],"float32"), Tensor([64, 33, 3, 64, 128],"float32"), "none", ) 	 103809024 	 1000 	 0.7618024349212646 	 0.4583611488342285 	 0.38919854164123535 	 0.43149757385253906 	 0.9431097507476807 	 1.4764854907989502 	 0.48189640045166016 	 0.37716150283813477 	 
2025-07-24 16:21:50.117463 test begin: paddle.nn.functional.mse_loss(Tensor([64, 4, 25, 64, 128],"float32"), Tensor([64, 4, 25, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 4, 25, 64, 128],"float32"), Tensor([64, 4, 25, 64, 128],"float32"), "none", ) 	 104857600 	 1000 	 0.7695422172546387 	 0.46593785285949707 	 0.3932163715362549 	 0.43678998947143555 	 0.9512302875518799 	 1.4909837245941162 	 0.4860382080078125 	 0.38084888458251953 	 
2025-07-24 16:21:56.432595 test begin: paddle.nn.functional.mse_loss(Tensor([64, 4, 3, 517, 128],"float32"), Tensor([64, 4, 3, 517, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 4, 3, 517, 128],"float32"), Tensor([64, 4, 3, 517, 128],"float32"), "none", ) 	 101646336 	 1000 	 0.7461998462677002 	 0.4470179080963135 	 0.38118529319763184 	 0.42299985885620117 	 0.9250035285949707 	 1.4461774826049805 	 0.4726078510284424 	 0.36940717697143555 	 
2025-07-24 16:22:02.528537 test begin: paddle.nn.functional.mse_loss(Tensor([64, 4, 3, 64, 1034],"float32"), Tensor([64, 4, 3, 64, 1034],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([64, 4, 3, 64, 1034],"float32"), Tensor([64, 4, 3, 64, 1034],"float32"), "none", ) 	 101646336 	 1000 	 0.746232271194458 	 0.4469742774963379 	 0.3812439441680908 	 0.4230184555053711 	 0.9250912666320801 	 1.4463040828704834 	 0.4726738929748535 	 0.3694579601287842 	 
2025-07-24 16:22:08.598329 test begin: paddle.nn.functional.mse_loss(Tensor([690, 3, 3, 64, 128],"float32"), Tensor([690, 3, 3, 64, 128],"float32"), "none", )
[Prof] paddle.nn.functional.mse_loss 	 paddle.nn.functional.mse_loss(Tensor([690, 3, 3, 64, 128],"float32"), Tensor([690, 3, 3, 64, 128],"float32"), "none", ) 	 101744640 	 1000 	 0.7468879222869873 	 0.447434663772583 	 0.3815340995788574 	 0.4228172302246094 	 0.9254179000854492 	 1.4479048252105713 	 0.4727623462677002 	 0.36998915672302246 	 
2025-07-24 16:22:14.684629 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), reduction="mean", weight=None, )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), reduction="mean", weight=None, ) 	 50803210 	 1000 	 3.3766531944274902 	 3.3254916667938232 	 0.3135819435119629 	 0.2827012538909912 	 4.627105236053467 	 4.302181959152222 	 0.3647630214691162 	 0.33884739875793457 	 
2025-07-24 16:22:31.487522 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), reduction="mean", weight=Tensor([5, 5080321],"float64"), )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), reduction="mean", weight=Tensor([5, 5080321],"float64"), ) 	 76204815 	 1000 	 3.823033094406128 	 3.7816789150238037 	 0.3251364231109619 	 0.2956063747406006 	 5.372297525405884 	 5.064671754837036 	 0.39211559295654297 	 0.34527587890625 	 
2025-07-24 16:22:53.108078 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), weight=Tensor([5, 5080321],"float64"), reduction="mean", name=None, )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), weight=Tensor([5, 5080321],"float64"), reduction="mean", name=None, ) 	 76204815 	 1000 	 3.8226664066314697 	 3.768278121948242 	 0.3250551223754883 	 0.29560327529907227 	 5.371500730514526 	 5.064579010009766 	 0.3920769691467285 	 0.3452877998352051 	 
2025-07-24 16:23:12.935224 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), reduction="mean", weight=None, )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), reduction="mean", weight=None, ) 	 50803210 	 1000 	 3.3936355113983154 	 3.4871997833251953 	 0.315126895904541 	 0.2964506149291992 	 4.713527202606201 	 4.457538604736328 	 0.3715813159942627 	 0.35097312927246094 	 
2025-07-24 16:23:30.197233 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), reduction="mean", weight=Tensor([5080321, 5],"float64"), )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), reduction="mean", weight=Tensor([5080321, 5],"float64"), ) 	 76204815 	 1000 	 3.839571475982666 	 3.9311437606811523 	 0.32637763023376465 	 0.3086373805999756 	 5.45695948600769 	 5.2125701904296875 	 0.39836931228637695 	 0.35520029067993164 	 
2025-07-24 16:23:52.686733 test begin: paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), weight=Tensor([5080321, 5],"float64"), reduction="mean", name=None, )
[Prof] paddle.nn.functional.multi_label_soft_margin_loss 	 paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), weight=Tensor([5080321, 5],"float64"), reduction="mean", name=None, ) 	 76204815 	 1000 	 3.8389785289764404 	 3.932339906692505 	 0.3263401985168457 	 0.3087007999420166 	 5.457212209701538 	 5.212447166442871 	 0.39833498001098633 	 0.35529470443725586 	 
2025-07-24 16:24:12.916740 test begin: paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, )
W0724 16:24:13.577788 127803 dygraph_functions.cc:93089] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, ) 	 38102403 	 1000 	 3.868302822113037 	 16.254098176956177 	 0.00019502639770507812 	 5.526492357254028 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:24:39.388363 test begin: paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, ) 	 38102403 	 1000 	 3.7970352172851562 	 16.17168402671814 	 0.0001354217529296875 	 16.152430534362793 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:25:04.759817 test begin: paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([12700801, 2],"float64"), Tensor([12700801],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, ) 	 38102403 	 1000 	 3.87381649017334 	 16.25609254837036 	 0.00018739700317382812 	 5.528152942657471 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:25:29.905074 test begin: paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, ) 	 76204803 	 1000 	 7.534276723861694 	 32.49384045600891 	 0.0004184246063232422 	 11.048280715942383 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:26:19.741497 test begin: paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, ) 	 76204803 	 1000 	 7.386535406112671 	 32.98519492149353 	 0.0002791881561279297 	 32.29967927932739 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:27:11.856447 test begin: paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([25401601, 2],"float64"), Tensor([25401601],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, ) 	 76204803 	 1000 	 7.53589653968811 	 32.496899127960205 	 0.0004153251647949219 	 11.048957347869873 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:28:01.705909 test begin: paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="mean", name=None, ) 	 25401610 	 1000 	 1.2866852283477783 	 6.036929607391357 	 3.0517578125e-05 	 3.084921360015869 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:28:11.926087 test begin: paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="none", name=None, ) 	 25401610 	 1000 	 1.2625420093536377 	 6.032220125198364 	 3.695487976074219e-05 	 6.013648748397827 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:28:22.096550 test begin: paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, )
[Prof] paddle.nn.functional.multi_margin_loss 	 paddle.nn.functional.multi_margin_loss(Tensor([5, 5080321],"float64"), Tensor([5],"int64"), p=1, margin=1.0, weight=None, reduction="sum", name=None, ) 	 25401610 	 1000 	 1.2824153900146484 	 6.040614366531372 	 4.00543212890625e-05 	 3.086913585662842 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:28:32.319922 test begin: paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="mean", name=None, )
[Prof] paddle.nn.functional.nll_loss 	 paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="mean", name=None, ) 	 25443143 	 1000 	 0.02758955955505371 	 0.03741145133972168 	 1.3828277587890625e-05 	 3.886222839355469e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:28:33.066718 test begin: paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="none", name=None, )
[Prof] paddle.nn.functional.nll_loss 	 paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="none", name=None, ) 	 25443143 	 1000 	 0.02833104133605957 	 0.0244443416595459 	 1.7881393432617188e-05 	 3.790855407714844e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:28:33.817811 test begin: paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="sum", name=None, )
[Prof] paddle.nn.functional.nll_loss 	 paddle.nn.functional.nll_loss(Tensor([5, 40643, 5, 5, 5],"float64"), Tensor([5, 5, 5, 5],"int64"), weight=Tensor([40643],"float64"), ignore_index=-100, reduction="sum", name=None, ) 	 25443143 	 1000 	 0.024782657623291016 	 0.03565382957458496 	 1.7642974853515625e-05 	 5.5789947509765625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:28:34.554419 test begin: paddle.nn.functional.normalize(Tensor([2009, 25288],"float32"), )
[Prof] paddle.nn.functional.normalize 	 paddle.nn.functional.normalize(Tensor([2009, 25288],"float32"), ) 	 50803592 	 1000 	 0.9938151836395264 	 0.4698953628540039 	 0.09701251983642578 	 0.15999364852905273 	 2.6823952198028564 	 3.2036964893341064 	 0.5491025447845459 	 0.23389434814453125 	 
2025-07-24 16:28:47.239259 test begin: paddle.nn.functional.normalize(Tensor([2081, 24413],"float32"), )
[Prof] paddle.nn.functional.normalize 	 paddle.nn.functional.normalize(Tensor([2081, 24413],"float32"), ) 	 50803453 	 1000 	 0.47544312477111816 	 0.47069525718688965 	 0.09720206260681152 	 0.16031193733215332 	 2.6836769580841064 	 3.204446315765381 	 0.5493385791778564 	 0.23391079902648926 	 
2025-07-24 16:28:55.716806 test begin: paddle.nn.functional.normalize(Tensor([2331, 21795],"float32"), )
[Prof] paddle.nn.functional.normalize 	 paddle.nn.functional.normalize(Tensor([2331, 21795],"float32"), ) 	 50804145 	 1000 	 0.46935009956359863 	 0.47094035148620605 	 0.09588932991027832 	 0.16037368774414062 	 2.678894281387329 	 3.2048027515411377 	 0.5483934879302979 	 0.23393702507019043 	 
2025-07-24 16:29:04.220813 test begin: paddle.nn.functional.normalize(Tensor([99226, 512],"float32"), )
[Prof] paddle.nn.functional.normalize 	 paddle.nn.functional.normalize(Tensor([99226, 512],"float32"), ) 	 50803712 	 1000 	 0.4887218475341797 	 0.4678325653076172 	 0.09994220733642578 	 0.1593167781829834 	 2.715667486190796 	 3.214629650115967 	 0.5558855533599854 	 0.2346632480621338 	 
2025-07-24 16:29:13.298535 test begin: paddle.nn.functional.npair_loss(Tensor([18, 2822401],"float32"), positive=Tensor([18, 2822401],"float32"), labels=Tensor([18],"float32"), l2_reg=0.002, )
[Prof] paddle.nn.functional.npair_loss 	 paddle.nn.functional.npair_loss(Tensor([18, 2822401],"float32"), positive=Tensor([18, 2822401],"float32"), labels=Tensor([18],"float32"), l2_reg=0.002, ) 	 101606454 	 1000 	 1.469144344329834 	 1.4172773361206055 	 0.06266498565673828 	 0.07219457626342773 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 16:29:22.141860 test begin: paddle.nn.functional.pad(Tensor([7573, 11, 1280],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([7573, 11, 1280],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, ) 	 106627840 	 1000 	 1.2362887859344482 	 0.46927976608276367 	 1.2168738842010498 	 0.1598033905029297 	 0.9866883754730225 	 0.8026368618011475 	 0.9234533309936523 	 0.2731771469116211 	 
2025-07-24 16:29:29.087165 test begin: paddle.nn.functional.pad(Tensor([7573, 8, 1678],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([7573, 8, 1678],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, ) 	 101659952 	 1000 	 1.17936372756958 	 0.4499845504760742 	 1.159904956817627 	 0.1525423526763916 	 0.94142746925354 	 0.7659015655517578 	 0.877436637878418 	 0.26068997383117676 	 
2025-07-24 16:29:37.563786 test begin: paddle.nn.functional.pad(Tensor([7710, 11, 1280],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([7710, 11, 1280],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, ) 	 108556800 	 1000 	 1.2626941204071045 	 0.4775111675262451 	 1.2463171482086182 	 0.16260242462158203 	 1.0041935443878174 	 0.8165054321289062 	 0.9397428035736084 	 0.27791357040405273 	 
2025-07-24 16:29:45.879685 test begin: paddle.nn.functional.pad(Tensor([7710, 8, 1648],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([7710, 8, 1648],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, ) 	 101648640 	 1000 	 1.1791722774505615 	 0.44760751724243164 	 1.1674656867980957 	 0.15241312980651855 	 0.9408388137817383 	 0.7653322219848633 	 0.8873312473297119 	 0.2605314254760742 	 
2025-07-24 16:29:52.503308 test begin: paddle.nn.functional.pad(Tensor([8162, 10, 1280],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([8162, 10, 1280],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, ) 	 104473600 	 1000 	 1.2130305767059326 	 0.4615778923034668 	 1.2004454135894775 	 0.15663933753967285 	 0.9666886329650879 	 0.7869646549224854 	 0.9136006832122803 	 0.26782798767089844 	 
2025-07-24 16:29:59.278427 test begin: paddle.nn.functional.pad(Tensor([8162, 8, 1557],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([8162, 8, 1557],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, ) 	 101665872 	 1000 	 1.1794891357421875 	 0.4481334686279297 	 1.1678776741027832 	 0.15258169174194336 	 0.941523551940918 	 0.7659475803375244 	 0.8878278732299805 	 0.26068925857543945 	 
2025-07-24 16:30:06.000551 test begin: paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,2,0,0,0,0,], mode="constant", value=0, ) 	 101611520 	 1000 	 1.1798861026763916 	 0.4446737766265869 	 1.1673076152801514 	 0.22710514068603516 	 0.9404356479644775 	 0.7621054649353027 	 0.8870983123779297 	 0.3893930912017822 	 
2025-07-24 16:30:12.635827 test begin: paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,3,0,0,0,0,], mode="constant", value=0, ) 	 101611520 	 1000 	 1.1808667182922363 	 0.4485819339752197 	 1.1669809818267822 	 0.2271432876586914 	 0.9404675960540771 	 0.76210618019104 	 0.8872272968292236 	 0.38939762115478516 	 
2025-07-24 16:30:22.087156 test begin: paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, )
[Prof] paddle.nn.functional.pad 	 paddle.nn.functional.pad(Tensor([9923, 8, 1280],"bfloat16"), list[0,6,0,0,0,0,], mode="constant", value=0, ) 	 101611520 	 1000 	 1.1786901950836182 	 0.4448223114013672 	 1.1669728755950928 	 0.2271866798400879 	 0.9404857158660889 	 0.7622413635253906 	 0.8869304656982422 	 0.38945531845092773 	 
2025-07-24 16:30:28.855230 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 508033],"float32"), Tensor([100, 508033],"float32"), -1, 1e-06, False, None, )
[Prof] paddle.nn.functional.pairwise_distance 	 paddle.nn.functional.pairwise_distance(Tensor([100, 508033],"float32"), Tensor([100, 508033],"float32"), -1, 1e-06, False, None, ) 	 101606600 	 1000 	 1.1419386863708496 	 1.0629615783691406 	 0.19414591789245605 	 0.27142953872680664 	 1.8954782485961914 	 2.77512264251709 	 0.968480110168457 	 0.2836582660675049 	 
2025-07-24 16:30:37.982266 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 508033],"float32"), Tensor([100, 508033],"float32"), -1, 1e-06, True, None, )
[Prof] paddle.nn.functional.pairwise_distance 	 paddle.nn.functional.pairwise_distance(Tensor([100, 508033],"float32"), Tensor([100, 508033],"float32"), -1, 1e-06, True, None, ) 	 101606600 	 1000 	 1.1594362258911133 	 1.0626702308654785 	 0.19408273696899414 	 0.27132320404052734 	 1.8947601318359375 	 2.7754600048065186 	 0.9682390689849854 	 0.2836802005767822 	 
2025-07-24 16:30:47.554576 test begin: paddle.nn.functional.pairwise_distance(Tensor([508033, 100],"float32"), Tensor([508033, 100],"float32"), -1, 1e-06, False, None, )
[Prof] paddle.nn.functional.pairwise_distance 	 paddle.nn.functional.pairwise_distance(Tensor([508033, 100],"float32"), Tensor([508033, 100],"float32"), -1, 1e-06, False, None, ) 	 101606600 	 1000 	 1.5263218879699707 	 1.6456701755523682 	 0.31214022636413574 	 0.5606949329376221 	 1.9826345443725586 	 2.7866928577423096 	 1.0130789279937744 	 0.28483057022094727 	 
2025-07-24 16:30:57.280506 test begin: paddle.nn.functional.pairwise_distance(Tensor([508033, 100],"float32"), Tensor([508033, 100],"float32"), -1, 1e-06, True, None, )
[Prof] paddle.nn.functional.pairwise_distance 	 paddle.nn.functional.pairwise_distance(Tensor([508033, 100],"float32"), Tensor([508033, 100],"float32"), -1, 1e-06, True, None, ) 	 101606600 	 1000 	 1.5262401103973389 	 1.645531177520752 	 0.3121373653411865 	 0.5606791973114014 	 1.9826228618621826 	 2.786717414855957 	 1.0130224227905273 	 0.2848398685455322 	 
2025-07-24 16:31:07.559807 test begin: paddle.nn.functional.pixel_shuffle(Tensor([13, 256, 128, 128],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([13, 256, 128, 128],"float32"), 2, "NCHW", None, ) 	 54525952 	 1000 	 0.4132959842681885 	 0.3519327640533447 	 0.3966834545135498 	 0.3337094783782959 	 0.3953683376312256 	 0.34313201904296875 	 0.3462820053100586 	 0.26818418502807617 	 
2025-07-24 16:31:12.646297 test begin: paddle.nn.functional.pixel_shuffle(Tensor([4, 256, 128, 388],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([4, 256, 128, 388],"float32"), 2, "NCHW", None, ) 	 50855936 	 1000 	 0.37656641006469727 	 0.33349180221557617 	 0.3664214611053467 	 0.31505608558654785 	 0.40233922004699707 	 0.3252716064453125 	 0.35310888290405273 	 0.24977660179138184 	 
2025-07-24 16:31:15.726387 test begin: paddle.nn.functional.pixel_shuffle(Tensor([4, 256, 388, 128],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([4, 256, 388, 128],"float32"), 2, "NCHW", None, ) 	 50855936 	 1000 	 0.38440752029418945 	 0.33440542221069336 	 0.3742811679840088 	 0.3158407211303711 	 0.4003458023071289 	 0.3223278522491455 	 0.34155702590942383 	 0.24808287620544434 	 
2025-07-24 16:31:18.817987 test begin: paddle.nn.functional.pixel_shuffle(Tensor([4, 776, 128, 128],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([4, 776, 128, 128],"float32"), 2, "NCHW", None, ) 	 50855936 	 1000 	 0.3789675235748291 	 0.32867431640625 	 0.368910551071167 	 0.30153965950012207 	 0.3681485652923584 	 0.32042360305786133 	 0.3188154697418213 	 0.24628996849060059 	 
2025-07-24 16:31:21.879852 test begin: paddle.nn.functional.pixel_shuffle(Tensor([49, 256, 64, 64],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([49, 256, 64, 64],"float32"), 2, "NCHW", None, ) 	 51380224 	 1000 	 0.8932147026062012 	 0.7955143451690674 	 0.37719082832336426 	 0.3039252758026123 	 0.3663341999053955 	 0.3276331424713135 	 0.31161928176879883 	 0.24581360816955566 	 
2025-07-24 16:31:28.460539 test begin: paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 128, 25],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 128, 25],"float32"), 2, "NCHW", None, ) 	 52428800 	 1000 	 0.38890576362609863 	 0.34238600730895996 	 0.37784886360168457 	 0.32276391983032227 	 0.38985371589660645 	 0.3446831703186035 	 0.34024882316589355 	 0.27050232887268066 	 
2025-07-24 16:31:31.650444 test begin: paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 25, 128],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 25, 128],"float32"), 2, "NCHW", None, ) 	 52428800 	 1000 	 0.3945035934448242 	 0.33888745307922363 	 0.3841898441314697 	 0.320620059967041 	 0.37929582595825195 	 0.32239842414855957 	 0.32674169540405273 	 0.2467949390411377 	 
2025-07-24 16:31:34.778514 test begin: paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 49, 64],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 49, 64],"float32"), 2, "NCHW", None, ) 	 51380224 	 1000 	 0.887479305267334 	 0.3493218421936035 	 0.36919379234313965 	 0.31020116806030273 	 0.36734962463378906 	 0.3189725875854492 	 0.3051574230194092 	 0.23841261863708496 	 
2025-07-24 16:31:40.481973 test begin: paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 64, 49],"float32"), 2, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_shuffle 	 paddle.nn.functional.pixel_shuffle(Tensor([64, 256, 64, 49],"float32"), 2, "NCHW", None, ) 	 51380224 	 1000 	 0.37920427322387695 	 0.3342592716217041 	 0.3686337471008301 	 0.3131535053253174 	 0.3773481845855713 	 0.33278703689575195 	 0.3280453681945801 	 0.2584233283996582 	 
2025-07-24 16:31:43.653376 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([176401, 1, 12, 12],"float64"), 3, "NCHW", )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([176401, 1, 12, 12],"float64"), 3, "NCHW", ) 	 25401744 	 1000 	 0.31732916831970215 	 0.302091121673584 	 0.3063235282897949 	 0.2837183475494385 	 0.3161203861236572 	 0.3022346496582031 	 0.26659655570983887 	 0.2216796875 	 
2025-07-24 16:31:45.948455 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([176401, 1, 12, 12],"float64"), 3, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([176401, 1, 12, 12],"float64"), 3, "NCHW", None, ) 	 25401744 	 1000 	 0.31728053092956543 	 0.30205321311950684 	 0.30648350715637207 	 0.2838006019592285 	 0.316143274307251 	 0.30220580101013184 	 0.26646995544433594 	 0.22755193710327148 	 
2025-07-24 16:31:48.238108 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([2, 176401, 12, 12],"float32"), 3, "NCHW", )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([2, 176401, 12, 12],"float32"), 3, "NCHW", ) 	 50803488 	 1000 	 0.36652708053588867 	 0.32720279693603516 	 0.35581064224243164 	 0.3088822364807129 	 0.3635261058807373 	 0.3229200839996338 	 0.31394410133361816 	 0.24825787544250488 	 
2025-07-24 16:31:51.293396 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([2, 88201, 12, 12],"float64"), 3, "NCHW", )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([2, 88201, 12, 12],"float64"), 3, "NCHW", ) 	 25401888 	 1000 	 0.31716346740722656 	 0.30206966400146484 	 0.30628323554992676 	 0.2836782932281494 	 0.31610703468322754 	 0.3022127151489258 	 0.2671029567718506 	 0.21998167037963867 	 
2025-07-24 16:31:53.617525 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([2, 88201, 12, 12],"float64"), 3, "NCHW", None, )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([2, 88201, 12, 12],"float64"), 3, "NCHW", None, ) 	 25401888 	 1000 	 0.3201425075531006 	 0.302074670791626 	 0.30588388442993164 	 0.2838432788848877 	 0.3160743713378906 	 0.30222487449645996 	 0.26172447204589844 	 0.22014427185058594 	 
2025-07-24 16:31:55.943885 test begin: paddle.nn.functional.pixel_unshuffle(Tensor([352801, 1, 12, 12],"float32"), 3, "NCHW", )
[Prof] paddle.nn.functional.pixel_unshuffle 	 paddle.nn.functional.pixel_unshuffle(Tensor([352801, 1, 12, 12],"float32"), 3, "NCHW", ) 	 50803344 	 1000 	 0.36591076850891113 	 0.32724452018737793 	 0.3549492359161377 	 0.3087906837463379 	 0.3634822368621826 	 0.32297396659851074 	 0.31450796127319336 	 0.24855899810791016 	 
2025-07-24 16:31:59.011682 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([16934401, 3, 2],"float32"), Tensor([16934401, 3, 2],"bfloat16"), )
W0724 16:32:02.081647 131357 multiply_fwd_func.cc:76] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([16934401, 3, 2],"float32"), Tensor([16934401, 3, 2],"bfloat16"), ) 	 203212812 	 1000 	 3.150498151779175 	 2.5929336547851562 	 0.5356993675231934 	 0.5292489528656006 	 5.693205833435059 	 4.8115222454071045 	 0.9692542552947998 	 0.7024574279785156 	 
2025-07-24 16:32:18.756975 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([16934401, 3, 2],"float32"), Tensor([16934401, 3, 2],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([16934401, 3, 2],"float32"), Tensor([16934401, 3, 2],"float16"), ) 	 203212812 	 1000 	 3.149348258972168 	 2.593902349472046 	 0.5356810092926025 	 0.5285463333129883 	 5.6932854652404785 	 4.807852745056152 	 0.9693460464477539 	 0.7019224166870117 	 
2025-07-24 16:32:42.226382 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 12700801, 2],"float32"), Tensor([4, 12700801, 2],"bfloat16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 12700801, 2],"float32"), Tensor([4, 12700801, 2],"bfloat16"), ) 	 203212816 	 1000 	 3.1506736278533936 	 2.5928211212158203 	 0.5357325077056885 	 0.5292153358459473 	 5.692913055419922 	 4.811677694320679 	 0.9692201614379883 	 0.7024972438812256 	 
2025-07-24 16:33:01.854224 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 12700801, 2],"float32"), Tensor([4, 12700801, 2],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 12700801, 2],"float32"), Tensor([4, 12700801, 2],"float16"), ) 	 203212816 	 1000 	 3.149374485015869 	 2.5907397270202637 	 0.5357334613800049 	 0.5287864208221436 	 5.693408727645874 	 4.807952880859375 	 0.9691734313964844 	 0.7020037174224854 	 
2025-07-24 16:33:21.950032 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 2116801],"float32"), Tensor([4, 3, 2116801],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 2116801],"float32"), Tensor([4, 3, 2116801],"float64"), ) 	 50803224 	 1000 	 1.668964147567749 	 1.070155143737793 	 0.2437129020690918 	 0.2181847095489502 	 2.270963191986084 	 2.1709108352661133 	 0.3315143585205078 	 0.27739691734313965 	 
2025-07-24 16:33:30.143685 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"bfloat16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"bfloat16"), ) 	 101606424 	 1000 	 1.5871305465698242 	 1.3173763751983643 	 0.26991868019104004 	 0.2682766914367676 	 2.8616814613342285 	 2.4211409091949463 	 0.4871938228607178 	 0.35344481468200684 	 
2025-07-24 16:33:42.130951 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float16"), ) 	 101606424 	 1000 	 1.5992324352264404 	 1.311631441116333 	 0.26991891860961914 	 0.26779937744140625 	 2.8618292808532715 	 2.4200856685638428 	 0.48725032806396484 	 0.3533036708831787 	 
2025-07-24 16:33:52.748257 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 4233601],"float32"), Tensor([4, 3, 4233601],"float64"), ) 	 101606424 	 1000 	 3.3090314865112305 	 2.1015818119049072 	 0.48297786712646484 	 0.42905449867248535 	 4.5117011070251465 	 4.299537897109985 	 0.6586098670959473 	 0.5494308471679688 	 
2025-07-24 16:34:08.963181 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 8467201],"float32"), Tensor([4, 3, 8467201],"bfloat16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 8467201],"float32"), Tensor([4, 3, 8467201],"bfloat16"), ) 	 203212824 	 1000 	 3.1511127948760986 	 2.5928573608398438 	 0.535679817199707 	 0.5292198657989502 	 5.693567276000977 	 4.811703443527222 	 0.9690749645233154 	 0.7024726867675781 	 
2025-07-24 16:34:28.722809 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 8467201],"float32"), Tensor([4, 3, 8467201],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3, 8467201],"float32"), Tensor([4, 3, 8467201],"float16"), ) 	 203212824 	 1000 	 3.1492323875427246 	 2.5893640518188477 	 0.5357162952423096 	 0.5285625457763672 	 5.695045471191406 	 4.808326721191406 	 0.9692831039428711 	 0.7019317150115967 	 
2025-07-24 16:34:50.161074 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 3175201, 2],"float32"), Tensor([4, 3175201, 2],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 3175201, 2],"float32"), Tensor([4, 3175201, 2],"float64"), ) 	 50803216 	 1000 	 1.6688251495361328 	 1.0684928894042969 	 0.2436368465423584 	 0.21819186210632324 	 2.271409273147583 	 2.170912027359009 	 0.3315770626068115 	 0.2774028778076172 	 
2025-07-24 16:34:58.402250 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"bfloat16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"bfloat16"), ) 	 101606416 	 1000 	 1.5889148712158203 	 1.313171625137329 	 0.27000904083251953 	 0.2681095600128174 	 2.861574411392212 	 2.4210875034332275 	 0.48715877532958984 	 0.35347795486450195 	 
2025-07-24 16:35:08.238289 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float16"), ) 	 101606416 	 1000 	 1.5882952213287354 	 1.3111960887908936 	 0.26990532875061035 	 0.2676260471343994 	 2.8619320392608643 	 2.4199845790863037 	 0.4872138500213623 	 0.3533029556274414 	 
2025-07-24 16:35:18.202821 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4, 6350401, 2],"float32"), Tensor([4, 6350401, 2],"float64"), ) 	 101606416 	 1000 	 3.3085339069366455 	 2.103086233139038 	 0.4830167293548584 	 0.4292938709259033 	 4.511816501617432 	 4.3008928298950195 	 0.6586589813232422 	 0.5494890213012695 	 
2025-07-24 16:35:34.485835 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([4233601, 3, 2],"float32"), Tensor([4233601, 3, 2],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([4233601, 3, 2],"float32"), Tensor([4233601, 3, 2],"float64"), ) 	 50803212 	 1000 	 1.6729841232299805 	 1.0696775913238525 	 0.24367260932922363 	 0.21841859817504883 	 2.2715587615966797 	 2.171210289001465 	 0.3316514492034912 	 0.27753162384033203 	 
2025-07-24 16:35:44.485453 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"bfloat16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"bfloat16"), ) 	 101606412 	 1000 	 1.5869920253753662 	 1.3227252960205078 	 0.26997852325439453 	 0.2681419849395752 	 2.8613736629486084 	 2.4213242530822754 	 0.48715949058532715 	 0.35349559783935547 	 
2025-07-24 16:35:57.312544 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float16"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float16"), ) 	 101606412 	 1000 	 1.5867087841033936 	 1.3124017715454102 	 0.2699735164642334 	 0.26787590980529785 	 2.861978769302368 	 2.4198672771453857 	 0.48719096183776855 	 0.35327816009521484 	 
2025-07-24 16:36:07.327965 test begin: paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float64"), )
[Prof] paddle.nn.functional.poisson_nll_loss 	 paddle.nn.functional.poisson_nll_loss(Tensor([8467201, 3, 2],"float32"), Tensor([8467201, 3, 2],"float64"), ) 	 101606412 	 1000 	 3.3087775707244873 	 2.101628303527832 	 0.48308873176574707 	 0.42902565002441406 	 4.511597156524658 	 4.29932165145874 	 0.6586358547210693 	 0.549487829208374 	 
2025-07-24 16:36:23.569910 test begin: paddle.nn.functional.prelu(Tensor([104, 128, 56, 69],"float32"), Tensor([128],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([104, 128, 56, 69],"float32"), Tensor([128],"float32"), data_format="NCHW", ) 	 51437696 	 1000 	 0.3055734634399414 	 0.3128180503845215 	 0.29248046875 	 0.29689550399780273 	 1.1006479263305664 	 0.7973248958587646 	 0.3743441104888916 	 0.27121710777282715 	 
2025-07-24 16:36:27.769015 test begin: paddle.nn.functional.prelu(Tensor([104, 128, 69, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([104, 128, 69, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", ) 	 51437696 	 1000 	 0.30475378036499023 	 0.3127763271331787 	 0.29233670234680176 	 0.2967853546142578 	 1.098611831665039 	 0.796597957611084 	 0.3736727237701416 	 0.27096104621887207 	 
2025-07-24 16:36:32.014044 test begin: paddle.nn.functional.prelu(Tensor([104, 156, 56, 56],"float32"), Tensor([156],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([104, 156, 56, 56],"float32"), Tensor([156],"float32"), data_format="NCHW", ) 	 50878620 	 1000 	 0.30150651931762695 	 0.3118875026702881 	 0.2891042232513428 	 0.29324984550476074 	 1.08573579788208 	 0.7937517166137695 	 0.3692920207977295 	 0.27005648612976074 	 
2025-07-24 16:36:37.442177 test begin: paddle.nn.functional.prelu(Tensor([127, 128, 56, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([127, 128, 56, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", ) 	 50978944 	 1000 	 0.9794118404388428 	 0.30996179580688477 	 0.28911375999450684 	 0.294048547744751 	 1.090320348739624 	 0.7880182266235352 	 0.37087583541870117 	 0.26807594299316406 	 
2025-07-24 16:36:42.866780 test begin: paddle.nn.functional.prelu(Tensor([128, 128, 56, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([128, 128, 56, 56],"float32"), Tensor([128],"float32"), data_format="NCHW", ) 	 51380352 	 1000 	 0.3048081398010254 	 0.3122670650482178 	 0.2915458679199219 	 0.29681944847106934 	 1.0993421077728271 	 0.7941281795501709 	 0.3739619255065918 	 0.270153284072876 	 
2025-07-24 16:36:47.095987 test begin: paddle.nn.functional.prelu(Tensor([128, 256, 28, 56],"float32"), Tensor([256],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([128, 256, 28, 56],"float32"), Tensor([256],"float32"), data_format="NCHW", ) 	 51380480 	 1000 	 0.305889368057251 	 0.3122749328613281 	 0.29196691513061523 	 0.2953836917877197 	 1.0990071296691895 	 0.7939040660858154 	 0.37386226654052734 	 0.2700941562652588 	 
2025-07-24 16:36:51.272445 test begin: paddle.nn.functional.prelu(Tensor([128, 256, 56, 28],"float32"), Tensor([256],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([128, 256, 56, 28],"float32"), Tensor([256],"float32"), data_format="NCHW", ) 	 51380480 	 1000 	 0.30445241928100586 	 0.31691861152648926 	 0.29201745986938477 	 0.29206204414367676 	 1.098912239074707 	 0.7945752143859863 	 0.37380003929138184 	 0.270251989364624 	 
2025-07-24 16:36:57.068824 test begin: paddle.nn.functional.prelu(Tensor([128, 507, 28, 28],"float32"), Tensor([507],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([128, 507, 28, 28],"float32"), Tensor([507],"float32"), data_format="NCHW", ) 	 50878971 	 1000 	 1.0515341758728027 	 0.3093562126159668 	 0.2811398506164551 	 0.2864673137664795 	 1.0779170989990234 	 0.7807421684265137 	 0.3666980266571045 	 0.39885759353637695 	 
2025-07-24 16:37:02.524323 test begin: paddle.nn.functional.prelu(Tensor([254, 256, 28, 28],"float32"), Tensor([256],"float32"), data_format="NCHW", )
[Prof] paddle.nn.functional.prelu 	 paddle.nn.functional.prelu(Tensor([254, 256, 28, 28],"float32"), Tensor([256],"float32"), data_format="NCHW", ) 	 50979072 	 1000 	 0.3020505905151367 	 0.3099203109741211 	 0.28939318656921387 	 0.2941875457763672 	 1.0910301208496094 	 0.7897601127624512 	 0.37111783027648926 	 0.2686445713043213 	 
2025-07-24 16:37:06.648660 test begin: paddle.nn.functional.relu(Tensor([10, 128, 480, 83],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([10, 128, 480, 83],"float32"), None, ) 	 50995200 	 1000 	 0.2971229553222656 	 0.29901576042175293 	 0.28817081451416016 	 0.2827918529510498 	 0.4519340991973877 	 0.44838809967041016 	 0.4011545181274414 	 0.3774886131286621 	 
2025-07-24 16:37:09.882592 test begin: paddle.nn.functional.relu(Tensor([10, 128, 83, 480],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([10, 128, 83, 480],"float32"), None, ) 	 50995200 	 1000 	 0.2971031665802002 	 0.29903626441955566 	 0.28762173652648926 	 0.2828562259674072 	 0.4519011974334717 	 0.448427677154541 	 0.40087127685546875 	 0.37891411781311035 	 
2025-07-24 16:37:13.042677 test begin: paddle.nn.functional.relu(Tensor([10, 23, 480, 480],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([10, 23, 480, 480],"float32"), None, ) 	 52992000 	 1000 	 0.3085038661956787 	 0.3105947971343994 	 0.29961633682250977 	 0.29427051544189453 	 0.46945834159851074 	 0.4657413959503174 	 0.4184277057647705 	 0.39640188217163086 	 
2025-07-24 16:37:16.372722 test begin: paddle.nn.functional.relu(Tensor([2, 128, 480, 480],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([2, 128, 480, 480],"float32"), None, ) 	 58982400 	 1000 	 0.3427762985229492 	 0.345062255859375 	 0.33365917205810547 	 0.32774829864501953 	 0.52223801612854 	 0.5179555416107178 	 0.47148656845092773 	 0.4494760036468506 	 
2025-07-24 16:37:20.054537 test begin: paddle.nn.functional.relu(Tensor([2, 256, 352, 352],"float32"), )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([2, 256, 352, 352],"float32"), ) 	 63438848 	 1000 	 0.3712153434753418 	 0.370685338973999 	 0.3596327304840088 	 0.353623628616333 	 0.5610175132751465 	 0.5566387176513672 	 0.5085365772247314 	 0.4877777099609375 	 
2025-07-24 16:37:23.984322 test begin: paddle.nn.functional.relu(Tensor([64, 64, 112, 112],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([64, 64, 112, 112],"float32"), None, ) 	 51380224 	 1000 	 0.2991914749145508 	 0.30124354362487793 	 0.29039859771728516 	 0.28531527519226074 	 0.4553189277648926 	 0.45171165466308594 	 0.40415191650390625 	 0.38424110412597656 	 
2025-07-24 16:37:27.141360 test begin: paddle.nn.functional.relu(Tensor([640, 64, 112, 12],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([640, 64, 112, 12],"float32"), None, ) 	 55050240 	 1000 	 0.3202083110809326 	 0.3222963809967041 	 0.31134653091430664 	 0.30647778511047363 	 0.48764729499816895 	 0.4835166931152344 	 0.43382763862609863 	 0.415783166885376 	 
2025-07-24 16:37:30.523016 test begin: paddle.nn.functional.relu(Tensor([640, 64, 12, 112],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([640, 64, 12, 112],"float32"), None, ) 	 55050240 	 1000 	 0.32021188735961914 	 0.32242393493652344 	 0.31113505363464355 	 0.30595970153808594 	 0.4878537654876709 	 0.48355603218078613 	 0.4247758388519287 	 0.39470815658569336 	 
2025-07-24 16:37:34.225507 test begin: paddle.nn.functional.relu(Tensor([640, 7, 112, 112],"float32"), None, )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([640, 7, 112, 112],"float32"), None, ) 	 56197120 	 1000 	 0.327617883682251 	 1.5388290882110596 	 0.3178434371948242 	 0.3121194839477539 	 0.49787306785583496 	 0.49379873275756836 	 0.4433145523071289 	 0.41542816162109375 	 
2025-07-24 16:37:39.977899 test begin: paddle.nn.functional.relu(Tensor([8, 256, 352, 71],"float32"), )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([8, 256, 352, 71],"float32"), ) 	 51183616 	 1000 	 0.2988917827606201 	 0.30017566680908203 	 0.2890133857727051 	 0.2842402458190918 	 0.4537060260772705 	 0.4499945640563965 	 0.4025394916534424 	 0.38055920600891113 	 
2025-07-24 16:37:43.316533 test begin: paddle.nn.functional.relu(Tensor([8, 256, 71, 352],"float32"), )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([8, 256, 71, 352],"float32"), ) 	 51183616 	 1000 	 0.29811525344848633 	 0.30014967918395996 	 0.28224706649780273 	 0.2768559455871582 	 0.4536769390106201 	 0.4499661922454834 	 0.3930482864379883 	 0.3726212978363037 	 
2025-07-24 16:37:46.625982 test begin: paddle.nn.functional.relu(Tensor([8, 52, 352, 352],"float32"), )
[Prof] paddle.nn.functional.relu 	 paddle.nn.functional.relu(Tensor([8, 52, 352, 352],"float32"), ) 	 51544064 	 1000 	 0.3002204895019531 	 0.3028755187988281 	 0.2862973213195801 	 0.2858695983886719 	 0.4567580223083496 	 0.4531228542327881 	 0.4062824249267578 	 0.3824465274810791 	 
2025-07-24 16:37:49.801385 test begin: paddle.nn.functional.relu6(Tensor([128, 144, 112, 25],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([128, 144, 112, 25],"float32"), ) 	 51609600 	 1000 	 0.3005702495574951 	 0.30257105827331543 	 0.29012227058410645 	 0.2838265895843506 	 0.457256555557251 	 0.45380139350891113 	 0.40712451934814453 	 0.38634300231933594 	 
2025-07-24 16:37:53.020956 test begin: paddle.nn.functional.relu6(Tensor([128, 144, 25, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([128, 144, 25, 112],"float32"), ) 	 51609600 	 1000 	 0.30060696601867676 	 0.30254507064819336 	 0.2884104251861572 	 0.28389501571655273 	 0.4573366641998291 	 0.4537217617034912 	 0.40705227851867676 	 0.38536787033081055 	 
2025-07-24 16:37:56.295442 test begin: paddle.nn.functional.relu6(Tensor([128, 192, 112, 19],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([128, 192, 112, 19],"float32"), ) 	 52297728 	 1000 	 0.3045814037322998 	 0.306473970413208 	 0.295457124710083 	 0.2883298397064209 	 0.46333837509155273 	 0.4596412181854248 	 0.4102513790130615 	 0.3917367458343506 	 
2025-07-24 16:37:59.604169 test begin: paddle.nn.functional.relu6(Tensor([128, 192, 19, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([128, 192, 19, 112],"float32"), ) 	 52297728 	 1000 	 0.7644305229187012 	 0.3169519901275635 	 0.28652215003967285 	 0.2802586555480957 	 0.46340060234069824 	 0.4596836566925049 	 0.40350794792175293 	 0.39153623580932617 	 
2025-07-24 16:38:05.132243 test begin: paddle.nn.functional.relu6(Tensor([128, 32, 112, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([128, 32, 112, 112],"float32"), ) 	 51380224 	 1000 	 0.2996406555175781 	 0.3013186454772949 	 0.2828972339630127 	 0.27456021308898926 	 0.4553706645965576 	 0.45185279846191406 	 0.39502596855163574 	 0.35564255714416504 	 
2025-07-24 16:38:08.227735 test begin: paddle.nn.functional.relu6(Tensor([22, 192, 112, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([22, 192, 112, 112],"float32"), ) 	 52985856 	 1000 	 0.30852556228637695 	 0.3106815814971924 	 0.29239583015441895 	 0.28399014472961426 	 0.46937060356140137 	 0.46568870544433594 	 0.40911221504211426 	 0.3873484134674072 	 
2025-07-24 16:38:11.525728 test begin: paddle.nn.functional.relu6(Tensor([256, 16, 112, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([256, 16, 112, 112],"float32"), ) 	 51380224 	 1000 	 0.2992074489593506 	 0.30299806594848633 	 0.2900416851043701 	 0.28272342681884766 	 0.4552924633026123 	 0.45171642303466797 	 0.404003381729126 	 0.3842146396636963 	 
2025-07-24 16:38:14.774132 test begin: paddle.nn.functional.relu6(Tensor([256, 96, 112, 19],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([256, 96, 112, 19],"float32"), ) 	 52297728 	 1000 	 0.30467724800109863 	 0.3066895008087158 	 0.29476475715637207 	 0.2880520820617676 	 0.46333837509155273 	 0.4596216678619385 	 0.4123389720916748 	 0.3903770446777344 	 
2025-07-24 16:38:18.064896 test begin: paddle.nn.functional.relu6(Tensor([256, 96, 19, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([256, 96, 19, 112],"float32"), ) 	 52297728 	 1000 	 0.3071935176849365 	 0.30652284622192383 	 0.2955191135406494 	 0.28779077529907227 	 0.46338891983032227 	 0.45960187911987305 	 0.4130995273590088 	 0.3907322883605957 	 
2025-07-24 16:38:21.329903 test begin: paddle.nn.functional.relu6(Tensor([29, 144, 112, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([29, 144, 112, 112],"float32"), ) 	 52383744 	 1000 	 0.30509328842163086 	 0.3070394992828369 	 0.2939639091491699 	 0.2885410785675049 	 0.4641861915588379 	 0.46033763885498047 	 0.4096658229827881 	 0.37253308296203613 	 
2025-07-24 16:38:24.540066 test begin: paddle.nn.functional.relu6(Tensor([43, 96, 112, 112],"float32"), )
[Prof] paddle.nn.functional.relu6 	 paddle.nn.functional.relu6(Tensor([43, 96, 112, 112],"float32"), ) 	 51781632 	 1000 	 0.30147218704223633 	 0.3035745620727539 	 0.29245686531066895 	 0.2852940559387207 	 0.4587688446044922 	 0.45528078079223633 	 0.40804505348205566 	 0.38829970359802246 	 
2025-07-24 16:38:27.727499 test begin: paddle.nn.functional.rrelu(Tensor([1, 2, 3, 4233601],"float64"), 0.05, 0.25, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([1, 2, 3, 4233601],"float64"), 0.05, 0.25, training=False, ) 	 25401606 	 1000 	 0.4815342426300049 	 0.2988135814666748 	 0.4711425304412842 	 0.2758769989013672 	 0.58463454246521 	 0.443342924118042 	 0.5346238613128662 	 0.3755660057067871 	 
2025-07-24 16:38:30.615595 test begin: paddle.nn.functional.rrelu(Tensor([1, 2, 3175201, 4],"float64"), 0.05, 0.25, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([1, 2, 3175201, 4],"float64"), 0.05, 0.25, training=False, ) 	 25401608 	 1000 	 0.48146486282348633 	 0.2988896369934082 	 0.4711792469024658 	 0.27695465087890625 	 0.5846335887908936 	 0.4432947635650635 	 0.5311222076416016 	 0.37556886672973633 	 
2025-07-24 16:38:33.567225 test begin: paddle.nn.functional.rrelu(Tensor([1, 2116801, 3, 4],"float64"), 0.05, 0.25, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([1, 2116801, 3, 4],"float64"), 0.05, 0.25, training=False, ) 	 25401612 	 1000 	 0.4815061092376709 	 0.31379246711730957 	 0.4712820053100586 	 0.275958776473999 	 0.5846302509307861 	 0.44335365295410156 	 0.5347509384155273 	 0.37644290924072266 	 
2025-07-24 16:38:39.111895 test begin: paddle.nn.functional.rrelu(Tensor([1058401, 2, 3, 4],"float64"), 0.05, 0.25, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([1058401, 2, 3, 4],"float64"), 0.05, 0.25, training=False, ) 	 25401624 	 1000 	 0.48604846000671387 	 0.29894089698791504 	 0.4699978828430176 	 0.27556610107421875 	 0.5846042633056641 	 0.443359375 	 0.5348610877990723 	 0.37456321716308594 	 
2025-07-24 16:38:42.019779 test begin: paddle.nn.functional.rrelu(Tensor([2, 1270081, 4, 5],"float32"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 1270081, 4, 5],"float32"), 0.1, 0.3, training=False, ) 	 50803240 	 1000 	 0.49118471145629883 	 0.2981150150299072 	 0.4810044765472412 	 0.27243995666503906 	 0.6032853126525879 	 0.4467015266418457 	 0.5532224178314209 	 0.37442636489868164 	 
2025-07-24 16:38:45.523256 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 1693441, 5],"float32"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 3, 1693441, 5],"float32"), 0.1, 0.3, training=False, ) 	 50803230 	 1000 	 0.4911971092224121 	 0.2981102466583252 	 0.4810018539428711 	 0.27555298805236816 	 0.6032204627990723 	 0.4467201232910156 	 0.5504534244537354 	 0.378464937210083 	 
2025-07-24 16:38:49.025113 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 1058401],"float64"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 3, 4, 1058401],"float64"), 0.1, 0.3, training=False, ) 	 25401624 	 1000 	 0.48152852058410645 	 0.29882335662841797 	 0.4711577892303467 	 0.2766413688659668 	 0.5845801830291748 	 0.4432978630065918 	 0.5343272686004639 	 0.37625694274902344 	 
2025-07-24 16:38:51.890306 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 2116801],"float32"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 3, 4, 2116801],"float32"), 0.1, 0.3, training=False, ) 	 50803224 	 1000 	 0.49121594429016113 	 0.29815220832824707 	 0.48073673248291016 	 0.27634763717651367 	 0.6032838821411133 	 0.4467484951019287 	 0.5463731288909912 	 0.3796114921569824 	 
2025-07-24 16:38:55.363765 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 846721, 5],"float64"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 3, 846721, 5],"float64"), 0.1, 0.3, training=False, ) 	 25401630 	 1000 	 0.48151397705078125 	 0.298839807510376 	 0.4712510108947754 	 0.2770087718963623 	 0.5846743583679199 	 0.44335246086120605 	 0.5222852230072021 	 0.3767859935760498 	 
2025-07-24 16:38:58.219623 test begin: paddle.nn.functional.rrelu(Tensor([2, 635041, 4, 5],"float64"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([2, 635041, 4, 5],"float64"), 0.1, 0.3, training=False, ) 	 25401640 	 1000 	 0.48111939430236816 	 0.29891276359558105 	 0.4709503650665283 	 0.27634620666503906 	 0.5845658779144287 	 0.443314790725708 	 0.5345461368560791 	 0.37076544761657715 	 
2025-07-24 16:39:01.106750 test begin: paddle.nn.functional.rrelu(Tensor([423361, 3, 4, 5],"float64"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([423361, 3, 4, 5],"float64"), 0.1, 0.3, training=False, ) 	 25401660 	 1000 	 0.4811842441558838 	 0.29886746406555176 	 0.4709599018096924 	 0.2766153812408447 	 0.584592342376709 	 0.44331860542297363 	 0.5346314907073975 	 0.3763892650604248 	 
2025-07-24 16:39:03.962181 test begin: paddle.nn.functional.rrelu(Tensor([846721, 3, 4, 5],"float32"), 0.1, 0.3, training=False, )
[Prof] paddle.nn.functional.rrelu 	 paddle.nn.functional.rrelu(Tensor([846721, 3, 4, 5],"float32"), 0.1, 0.3, training=False, ) 	 50803260 	 1000 	 1.250519037246704 	 0.30531930923461914 	 0.48084259033203125 	 0.2654557228088379 	 0.6032798290252686 	 0.4467322826385498 	 0.5534384250640869 	 0.3784186840057373 	 
2025-07-24 16:39:09.765264 test begin: paddle.nn.functional.selu(Tensor([101607, 5, 5, 10],"float64"), 1.5, 2.0, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([101607, 5, 5, 10],"float64"), 1.5, 2.0, ) 	 25401750 	 1000 	 0.2998368740081787 	 1.826625108718872 	 0.2907726764678955 	 0.31136512756347656 	 0.4477810859680176 	 2.125011682510376 	 0.39635467529296875 	 0.27165651321411133 	 
2025-07-24 16:39:15.573364 test begin: paddle.nn.functional.selu(Tensor([101607, 5, 5, 10],"float64"), 1.5, 2.0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([101607, 5, 5, 10],"float64"), 1.5, 2.0, None, ) 	 25401750 	 1000 	 0.2997932434082031 	 1.826695203781128 	 0.290785551071167 	 0.31131958961486816 	 0.44791603088378906 	 2.1249847412109375 	 0.3992033004760742 	 0.27167844772338867 	 
2025-07-24 16:39:21.371173 test begin: paddle.nn.functional.selu(Tensor([2822401, 3, 3],"float64"), 1.0507009873554805, 0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([2822401, 3, 3],"float64"), 1.0507009873554805, 0, None, ) 	 25401609 	 1000 	 0.29986047744750977 	 1.826439380645752 	 0.2908194065093994 	 0.31125473976135254 	 0.4476044178009033 	 2.1251049041748047 	 0.39868617057800293 	 0.2717154026031494 	 
2025-07-24 16:39:27.114018 test begin: paddle.nn.functional.selu(Tensor([3, 169345, 5, 10],"float64"), 1.5, 2.0, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 169345, 5, 10],"float64"), 1.5, 2.0, ) 	 25401750 	 1000 	 0.2997889518737793 	 1.8266792297363281 	 0.29083847999572754 	 0.31127047538757324 	 0.4481232166290283 	 2.1249349117279053 	 0.3978252410888672 	 0.27166748046875 	 
2025-07-24 16:39:32.855453 test begin: paddle.nn.functional.selu(Tensor([3, 169345, 5, 10],"float64"), 1.5, 2.0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 169345, 5, 10],"float64"), 1.5, 2.0, None, ) 	 25401750 	 1000 	 0.29981064796447754 	 1.826603651046753 	 0.2908046245574951 	 0.31131672859191895 	 0.44774365425109863 	 2.12497878074646 	 0.3991072177886963 	 0.2716484069824219 	 
2025-07-24 16:39:39.595973 test begin: paddle.nn.functional.selu(Tensor([3, 2822401, 3],"float64"), 1.0507009873554805, 0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 2822401, 3],"float64"), 1.0507009873554805, 0, None, ) 	 25401609 	 1000 	 0.30202484130859375 	 1.826871395111084 	 0.2839322090148926 	 0.3113369941711426 	 0.4476947784423828 	 2.1248598098754883 	 0.37230896949768066 	 0.2716853618621826 	 
2025-07-24 16:39:45.385791 test begin: paddle.nn.functional.selu(Tensor([3, 3, 2822401],"float64"), 1.0507009873554805, 0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 3, 2822401],"float64"), 1.0507009873554805, 0, None, ) 	 25401609 	 1000 	 0.30638957023620605 	 1.8265979290008545 	 0.28389596939086914 	 0.3113114833831787 	 0.44809937477111816 	 2.124943971633911 	 0.3886699676513672 	 0.2716846466064453 	 
2025-07-24 16:39:51.170911 test begin: paddle.nn.functional.selu(Tensor([3, 5, 169345, 10],"float64"), 1.5, 2.0, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 5, 169345, 10],"float64"), 1.5, 2.0, ) 	 25401750 	 1000 	 0.29984545707702637 	 1.8293039798736572 	 0.28393983840942383 	 0.31136512756347656 	 0.44764137268066406 	 2.124950408935547 	 0.38953185081481934 	 0.27166223526000977 	 
2025-07-24 16:39:56.976267 test begin: paddle.nn.functional.selu(Tensor([3, 5, 169345, 10],"float64"), 1.5, 2.0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 5, 169345, 10],"float64"), 1.5, 2.0, None, ) 	 25401750 	 1000 	 0.29982995986938477 	 1.8268136978149414 	 0.28395843505859375 	 0.31130528450012207 	 0.44765400886535645 	 2.125046491622925 	 0.38977861404418945 	 0.2716972827911377 	 
2025-07-24 16:40:02.767359 test begin: paddle.nn.functional.selu(Tensor([3, 5, 5, 338689],"float64"), 1.5, 2.0, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 5, 5, 338689],"float64"), 1.5, 2.0, ) 	 25401675 	 1000 	 0.3003652095794678 	 1.8266665935516357 	 0.2913806438446045 	 0.31125760078430176 	 0.4479858875274658 	 2.125126838684082 	 0.3992044925689697 	 0.27167749404907227 	 
2025-07-24 16:40:08.524987 test begin: paddle.nn.functional.selu(Tensor([3, 5, 5, 338689],"float64"), 1.5, 2.0, None, )
[Prof] paddle.nn.functional.selu 	 paddle.nn.functional.selu(Tensor([3, 5, 5, 338689],"float64"), 1.5, 2.0, None, ) 	 25401675 	 1000 	 0.3008568286895752 	 1.8314332962036133 	 0.29141974449157715 	 0.311276912689209 	 0.4477565288543701 	 2.125190019607544 	 0.3987722396850586 	 0.2716553211212158 	 
2025-07-24 16:40:15.925410 test begin: paddle.nn.functional.sequence_mask(Tensor([2, 2, 3, 3, 705601],"float64"), maxlen=5, dtype=type(numpy.int32), )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([2, 2, 3, 3, 705601],"float64"), maxlen=5, dtype=type(numpy.int32), ) 	 25401636 	 1000 	 0.7793776988983154 	 1.4831204414367676 	 0.7675540447235107 	 0.5056717395782471 	 None 	 None 	 None 	 None 	 combined
2025-07-24 16:40:18.720228 test begin: paddle.nn.functional.sequence_mask(Tensor([2, 2, 3, 705601, 3],"float64"), maxlen=5, dtype=type(numpy.int32), )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([2, 2, 3, 705601, 3],"float64"), maxlen=5, dtype=type(numpy.int32), ) 	 25401636 	 1000 	 0.7793457508087158 	 1.4832792282104492 	 0.7598509788513184 	 0.5056941509246826 	 None 	 None 	 None 	 None 	 combined
2025-07-24 16:40:21.564801 test begin: paddle.nn.functional.sequence_mask(Tensor([2, 2, 705601, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([2, 2, 705601, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), ) 	 25401636 	 1000 	 0.780705451965332 	 1.484804630279541 	 0.7676293849945068 	 0.5056724548339844 	 None 	 None 	 None 	 None 	 combined
2025-07-24 16:40:24.381161 test begin: paddle.nn.functional.sequence_mask(Tensor([2, 470401, 3, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([2, 470401, 3, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), ) 	 25401654 	 1000 	 0.7799582481384277 	 1.4831557273864746 	 0.7594718933105469 	 0.5056769847869873 	 None 	 None 	 None 	 None 	 combined
2025-07-24 16:40:27.244801 test begin: paddle.nn.functional.sequence_mask(Tensor([470401, 2, 3, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([470401, 2, 3, 3, 3],"float64"), maxlen=5, dtype=type(numpy.int32), ) 	 25401654 	 1000 	 0.7793059349060059 	 1.483154058456421 	 0.7675797939300537 	 0.5056750774383545 	 None 	 None 	 None 	 None 	 combined
2025-07-24 16:40:30.039912 test begin: paddle.nn.functional.sequence_mask(Tensor([50803201],"int32"), maxlen=4, dtype="float32", )
[Prof] paddle.nn.functional.sequence_mask 	 paddle.nn.functional.sequence_mask(Tensor([50803201],"int32"), maxlen=4, dtype="float32", ) 	 50803201 	 1000 	 1.2115042209625244 	 2.4268198013305664 	 1.1993560791015625 	 0.8274590969085693 	 None 	 None 	 None 	 None 	 combined
2025-07-24 16:40:34.756278 test begin: paddle.nn.functional.sigmoid(Tensor([10, 32, 400, 400],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([10, 32, 400, 400],"float32"), ) 	 51200000 	 1000 	 1.0293521881103516 	 0.30643486976623535 	 0.2881762981414795 	 0.28362059593200684 	 0.45392799377441406 	 0.4501810073852539 	 0.40108513832092285 	 0.3688335418701172 	 
2025-07-24 16:40:40.250647 test begin: paddle.nn.functional.sigmoid(Tensor([364, 304, 460],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([364, 304, 460],"float32"), ) 	 50901760 	 1000 	 0.29662251472473145 	 0.2989494800567627 	 0.28627705574035645 	 0.28171849250793457 	 0.4510350227355957 	 0.44753289222717285 	 0.39034533500671387 	 0.3753337860107422 	 
2025-07-24 16:40:43.587476 test begin: paddle.nn.functional.sigmoid(Tensor([364, 416, 336],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([364, 416, 336],"float32"), ) 	 50878464 	 1000 	 0.2955949306488037 	 0.30014586448669434 	 0.2866528034210205 	 0.28762197494506836 	 0.45085883140563965 	 0.4472362995147705 	 0.3999898433685303 	 0.3787984848022461 	 
2025-07-24 16:40:46.757151 test begin: paddle.nn.functional.sigmoid(Tensor([372, 304, 450],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([372, 304, 450],"float32"), ) 	 50889600 	 1000 	 0.2957308292388916 	 0.2989077568054199 	 0.28684449195861816 	 0.2879014015197754 	 0.45094847679138184 	 0.447385311126709 	 0.39922285079956055 	 0.3803243637084961 	 
2025-07-24 16:40:49.917260 test begin: paddle.nn.functional.sigmoid(Tensor([372, 407, 336],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([372, 407, 336],"float32"), ) 	 50871744 	 1000 	 0.2955660820007324 	 0.2987964153289795 	 0.28627514839172363 	 0.28784847259521484 	 0.4507753849029541 	 0.4472208023071289 	 0.3998525142669678 	 0.37926292419433594 	 
2025-07-24 16:40:53.048442 test begin: paddle.nn.functional.sigmoid(Tensor([498, 304, 336],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([498, 304, 336],"float32"), ) 	 50867712 	 1000 	 0.29682421684265137 	 0.29889988899230957 	 0.28670692443847656 	 0.28168582916259766 	 0.4508991241455078 	 0.44720959663391113 	 0.3985178470611572 	 0.377349853515625 	 
2025-07-24 16:40:56.265391 test begin: paddle.nn.functional.sigmoid(Tensor([8, 32, 400, 497],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([8, 32, 400, 497],"float32"), ) 	 50892800 	 1000 	 0.29569196701049805 	 0.2989072799682617 	 0.27991724014282227 	 0.28792858123779297 	 0.4510676860809326 	 0.44737958908081055 	 0.38897061347961426 	 0.3778233528137207 	 
2025-07-24 16:40:59.523870 test begin: paddle.nn.functional.sigmoid(Tensor([8, 32, 497, 400],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([8, 32, 497, 400],"float32"), ) 	 50892800 	 1000 	 0.2957134246826172 	 0.30205678939819336 	 0.28674888610839844 	 0.2878446578979492 	 0.45093464851379395 	 0.4474492073059082 	 0.40001821517944336 	 0.37932252883911133 	 
2025-07-24 16:41:02.678904 test begin: paddle.nn.functional.sigmoid(Tensor([8, 40, 400, 400],"float32"), )
[Prof] paddle.nn.functional.sigmoid 	 paddle.nn.functional.sigmoid(Tensor([8, 40, 400, 400],"float32"), ) 	 51200000 	 1000 	 0.2972404956817627 	 0.30068278312683105 	 0.28831052780151367 	 0.2899746894836426 	 0.4537489414215088 	 0.45012784004211426 	 0.40305256843566895 	 0.38262104988098145 	 
2025-07-24 16:41:07.301265 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803248 	 1000 	 6.902352333068848 	 5.641154766082764 	 0.0006930828094482422 	 0.3034343719482422 	 9.254951477050781 	 9.169852256774902 	 0.41147851943969727 	 0.37486934661865234 	 combined
2025-07-24 16:41:41.061834 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803249 	 1000 	 7.209673166275024 	 5.942083358764648 	 0.000911712646484375 	 0.30327725410461426 	 10.00706171989441 	 10.965377807617188 	 0.39339494705200195 	 0.3505065441131592 	 combined
2025-07-24 16:42:16.270297 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 4, 1058401],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", ) 	 50803249 	 1000 	 7.20521092414856 	 5.949364185333252 	 0.0009522438049316406 	 0.3032209873199463 	 10.003746509552002 	 10.965674877166748 	 0.39324164390563965 	 0.3504631519317627 	 combined
2025-07-24 16:42:54.405057 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803320 	 1000 	 6.90717887878418 	 5.640589237213135 	 0.0006761550903320312 	 0.3033623695373535 	 9.257741451263428 	 9.167480707168579 	 0.41162610054016113 	 0.37479686737060547 	 combined
2025-07-24 16:43:27.700646 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803321 	 1000 	 7.211332559585571 	 5.941803693771362 	 0.000949859619140625 	 0.30320143699645996 	 10.01283073425293 	 10.96516466140747 	 0.39362645149230957 	 0.3504190444946289 	 combined
2025-07-24 16:44:04.750301 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 3, 423361, 10],"float64"), Tensor([2, 3, 423361, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", ) 	 50803321 	 1000 	 7.201523303985596 	 5.941784381866455 	 0.0009458065032958984 	 0.3031761646270752 	 10.01589035987854 	 10.966665983200073 	 0.3937664031982422 	 0.35055017471313477 	 combined
2025-07-24 16:44:40.138516 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803360 	 1000 	 6.901390790939331 	 5.642714262008667 	 0.0006742477416992188 	 0.3034384250640869 	 9.249833583831787 	 9.16766905784607 	 0.4113173484802246 	 0.37482547760009766 	 combined
2025-07-24 16:45:13.524899 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803361 	 1000 	 7.206487655639648 	 5.942322015762329 	 0.0009446144104003906 	 0.3032517433166504 	 10.01169466972351 	 10.96650242805481 	 0.3935668468475342 	 0.3504757881164551 	 combined
2025-07-24 16:45:48.913315 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([2, 317521, 4, 10],"float64"), Tensor([2, 317521, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", ) 	 50803361 	 1000 	 7.201251745223999 	 5.941834449768066 	 0.0009777545928955078 	 0.30324292182922363 	 10.005163669586182 	 10.966743469238281 	 0.39336204528808594 	 0.35045909881591797 	 combined
2025-07-24 16:46:24.103697 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), None, alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803440 	 1000 	 6.902521133422852 	 5.6406800746917725 	 0.0006630420684814453 	 0.3033921718597412 	 9.256595849990845 	 9.168459177017212 	 0.41159558296203613 	 0.37488341331481934 	 combined
2025-07-24 16:46:56.210582 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.25, gamma=0.0, reduction="mean", ) 	 50803441 	 1000 	 7.197692632675171 	 5.941850900650024 	 0.0009446144104003906 	 0.3032195568084717 	 10.00373387336731 	 10.965716361999512 	 0.39330339431762695 	 0.35042810440063477 	 combined
2025-07-24 16:47:31.389324 test begin: paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", )
[Prof] paddle.nn.functional.sigmoid_focal_loss 	 paddle.nn.functional.sigmoid_focal_loss(Tensor([211681, 3, 4, 10],"float64"), Tensor([211681, 3, 4, 10],"float64"), Tensor([1],"float64"), alpha=0.5, gamma=0.0, reduction="mean", ) 	 50803441 	 1000 	 7.194471120834351 	 5.941967487335205 	 0.0009365081787109375 	 0.30324339866638184 	 10.007498502731323 	 10.966223239898682 	 0.3933756351470947 	 0.35041189193725586 	 combined
2025-07-24 16:48:06.843861 test begin: paddle.nn.functional.silu(Tensor([128, 128, 128, 25],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 128, 128, 25],"float32"), None, ) 	 52428800 	 1000 	 0.30504441261291504 	 0.3077199459075928 	 0.2939896583557129 	 0.292081356048584 	 0.46477746963500977 	 0.46468281745910645 	 0.4016437530517578 	 0.3968932628631592 	 
2025-07-24 16:48:10.168038 test begin: paddle.nn.functional.silu(Tensor([128, 128, 25, 128],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 128, 25, 128],"float32"), None, ) 	 52428800 	 1000 	 0.3045332431793213 	 0.30766963958740234 	 0.2955665588378906 	 0.2911491394042969 	 0.46461987495422363 	 0.46468067169189453 	 0.41055750846862793 	 0.39745140075683594 	 
2025-07-24 16:48:13.423577 test begin: paddle.nn.functional.silu(Tensor([128, 25, 128, 128],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 25, 128, 128],"float32"), None, ) 	 52428800 	 1000 	 0.30451393127441406 	 0.3096802234649658 	 0.29563450813293457 	 0.2916259765625 	 0.4646604061126709 	 0.46462559700012207 	 0.3925614356994629 	 0.39493846893310547 	 
2025-07-24 16:48:16.729217 test begin: paddle.nn.functional.silu(Tensor([128, 256, 25, 64],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 256, 25, 64],"float32"), None, ) 	 52428800 	 1000 	 0.30454468727111816 	 0.30764102935791016 	 0.29572606086730957 	 0.2914104461669922 	 0.46463465690612793 	 0.4646158218383789 	 0.41263628005981445 	 0.39720582962036133 	 
2025-07-24 16:48:19.990456 test begin: paddle.nn.functional.silu(Tensor([128, 256, 64, 25],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 256, 64, 25],"float32"), None, ) 	 52428800 	 1000 	 0.3045623302459717 	 0.30765724182128906 	 0.29574131965637207 	 0.29217076301574707 	 0.46465563774108887 	 0.46462202072143555 	 0.4111788272857666 	 0.3958580493927002 	 
2025-07-24 16:48:23.317109 test begin: paddle.nn.functional.silu(Tensor([128, 64, 128, 49],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 64, 128, 49],"float32"), None, ) 	 51380224 	 1000 	 0.29866528511047363 	 0.3016378879547119 	 0.2883918285369873 	 0.28632616996765137 	 0.4556000232696533 	 0.45539212226867676 	 0.40241312980651855 	 0.3852396011352539 	 
2025-07-24 16:48:26.534024 test begin: paddle.nn.functional.silu(Tensor([128, 64, 49, 128],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 64, 49, 128],"float32"), None, ) 	 51380224 	 1000 	 0.29877734184265137 	 0.30161333084106445 	 0.27933287620544434 	 0.27913475036621094 	 0.45555925369262695 	 0.45549702644348145 	 0.3943483829498291 	 0.37613344192504883 	 
2025-07-24 16:48:29.803066 test begin: paddle.nn.functional.silu(Tensor([128, 97, 64, 64],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([128, 97, 64, 64],"float32"), None, ) 	 50855936 	 1000 	 0.2956535816192627 	 0.29862022399902344 	 0.28690433502197266 	 0.28312087059020996 	 0.45084571838378906 	 0.4508187770843506 	 0.3990750312805176 	 0.38365626335144043 	 
2025-07-24 16:48:33.018080 test begin: paddle.nn.functional.silu(Tensor([25, 128, 128, 128],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([25, 128, 128, 128],"float32"), None, ) 	 52428800 	 1000 	 0.3045063018798828 	 0.7754685878753662 	 0.2956674098968506 	 0.283984899520874 	 0.46476244926452637 	 0.46463966369628906 	 0.41275715827941895 	 0.38585782051086426 	 
2025-07-24 16:48:39.327924 test begin: paddle.nn.functional.silu(Tensor([49, 256, 64, 64],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([49, 256, 64, 64],"float32"), None, ) 	 51380224 	 1000 	 0.2999393939971924 	 0.3016524314880371 	 0.2897682189941406 	 0.2862436771392822 	 0.45554256439208984 	 0.4554738998413086 	 0.39349365234375 	 0.38885021209716797 	 
2025-07-24 16:48:42.524756 test begin: paddle.nn.functional.silu(Tensor([49, 64, 128, 128],"float32"), None, )
[Prof] paddle.nn.functional.silu 	 paddle.nn.functional.silu(Tensor([49, 64, 128, 128],"float32"), None, ) 	 51380224 	 1000 	 0.29866909980773926 	 0.30373573303222656 	 0.28947901725769043 	 0.28590869903564453 	 0.45559096336364746 	 0.4554002285003662 	 0.40408754348754883 	 0.38715696334838867 	 
2025-07-24 16:48:45.839735 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([1016065, 50],"float32"), Tensor([1016065, 50],"float32"), reduction="none", )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([1016065, 50],"float32"), Tensor([1016065, 50],"float32"), reduction="none", ) 	 101606500 	 1000 	 0.8138749599456787 	 0.4470703601837158 	 0.4158039093017578 	 0.42131829261779785 	 1.6252238750457764 	 1.4470007419586182 	 0.4151918888092041 	 0.36963963508605957 	 
2025-07-24 16:48:52.690426 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([1017, 50000],"float32"), Tensor([1017, 50000],"float32"), reduction="mean", delta=1.0, name=None, )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([1017, 50000],"float32"), Tensor([1017, 50000],"float32"), reduction="mean", delta=1.0, name=None, ) 	 101700000 	 1000 	 0.9640064239501953 	 0.5986952781677246 	 0.24598217010498047 	 0.20370268821716309 	 1.7624194622039795 	 1.1616430282592773 	 0.36031389236450195 	 0.29672956466674805 	 
2025-07-24 16:48:58.937196 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([1914, 26543],"float32"), Tensor([1914, 26543],"float32"), reduction="none", )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([1914, 26543],"float32"), Tensor([1914, 26543],"float32"), reduction="none", ) 	 101606604 	 1000 	 0.8123452663421631 	 0.44997310638427734 	 0.4149966239929199 	 0.4212183952331543 	 1.6238183975219727 	 1.447683334350586 	 0.4148287773132324 	 0.3702397346496582 	 
2025-07-24 16:49:07.594322 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([33960, 187, 8],"float32"), Tensor([33960, 187, 8],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([33960, 187, 8],"float32"), Tensor([33960, 187, 8],"float32"), reduction="sum", ) 	 101608320 	 1000 	 0.9641258716583252 	 0.5987353324890137 	 0.2459876537322998 	 0.20373225212097168 	 1.7611064910888672 	 1.1605539321899414 	 0.36002588272094727 	 0.29645848274230957 	 
2025-07-24 16:49:13.957618 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([64, 187, 4245],"float32"), Tensor([64, 187, 4245],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([64, 187, 4245],"float32"), Tensor([64, 187, 4245],"float32"), reduction="sum", ) 	 101608320 	 1000 	 0.9636733531951904 	 0.5988037586212158 	 0.2458972930908203 	 0.20371174812316895 	 1.760216236114502 	 1.1606481075286865 	 0.35987114906311035 	 0.2964794635772705 	 
2025-07-24 16:49:20.218415 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([64, 99226, 8],"float32"), Tensor([64, 99226, 8],"float32"), reduction="sum", )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([64, 99226, 8],"float32"), Tensor([64, 99226, 8],"float32"), reduction="sum", ) 	 101607424 	 1000 	 0.9631941318511963 	 0.5987839698791504 	 0.24572396278381348 	 0.2037341594696045 	 1.7590186595916748 	 1.160438060760498 	 0.35968923568725586 	 0.2963688373565674 	 
2025-07-24 16:49:26.487472 test begin: paddle.nn.functional.smooth_l1_loss(Tensor([7, 7257601],"float32"), Tensor([7, 7257601],"float32"), reduction="mean", delta=1.0, name=None, )
[Prof] paddle.nn.functional.smooth_l1_loss 	 paddle.nn.functional.smooth_l1_loss(Tensor([7, 7257601],"float32"), Tensor([7, 7257601],"float32"), reduction="mean", delta=1.0, name=None, ) 	 101606414 	 1000 	 0.9624190330505371 	 0.5987780094146729 	 0.24553608894348145 	 0.20375657081604004 	 1.761127233505249 	 1.1607015132904053 	 0.360060453414917 	 0.2964756488800049 	 
2025-07-24 16:49:32.736664 test begin: paddle.nn.functional.softmax(Tensor([10, 2304, 2304],"float32"), axis=-1, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([10, 2304, 2304],"float32"), axis=-1, ) 	 53084160 	 1000 	 0.3164372444152832 	 1.212620735168457 	 0.3031651973724365 	 0.5075898170471191 	 0.4900527000427246 	 0.9311935901641846 	 0.41518735885620117 	 0.47576045989990234 	 
2025-07-24 16:49:39.686856 test begin: paddle.nn.functional.softmax(Tensor([3840, 1, 144, 144],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([3840, 1, 144, 144],"float32"), -1, name=None, ) 	 79626240 	 1000 	 0.46250295639038086 	 0.5030765533447266 	 0.4527602195739746 	 0.4881312847137451 	 0.7009737491607666 	 1.392798900604248 	 0.6492643356323242 	 0.7117023468017578 	 
2025-07-24 16:49:45.443354 test begin: paddle.nn.functional.softmax(Tensor([3840, 4, 144, 23],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([3840, 4, 144, 23],"float32"), -1, name=None, ) 	 50872320 	 1000 	 0.36687374114990234 	 0.50439453125 	 0.35768985748291016 	 0.4884934425354004 	 0.45099353790283203 	 0.8951821327209473 	 0.40146970748901367 	 0.4573657512664795 	 
2025-07-24 16:49:49.379288 test begin: paddle.nn.functional.softmax(Tensor([3840, 4, 23, 144],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([3840, 4, 23, 144],"float32"), -1, name=None, ) 	 50872320 	 1000 	 0.29697704315185547 	 0.32387518882751465 	 0.281038761138916 	 0.30121445655822754 	 0.4497995376586914 	 0.8926520347595215 	 0.3876662254333496 	 0.4561002254486084 	 
2025-07-24 16:49:53.093212 test begin: paddle.nn.functional.softmax(Tensor([4096, 1, 144, 144],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([4096, 1, 144, 144],"float32"), -1, name=None, ) 	 84934656 	 1000 	 0.49329352378845215 	 0.535675048828125 	 0.47713732719421387 	 0.5122478008270264 	 0.7473719120025635 	 1.484969139099121 	 0.681685209274292 	 0.7587871551513672 	 
2025-07-24 16:49:59.343599 test begin: paddle.nn.functional.softmax(Tensor([4096, 4, 144, 22],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([4096, 4, 144, 22],"float32"), -1, name=None, ) 	 51904512 	 1000 	 0.4614827632904053 	 0.5365784168243408 	 0.445375919342041 	 0.5212843418121338 	 0.4633798599243164 	 0.9145340919494629 	 0.4044675827026367 	 0.4672422409057617 	 
2025-07-24 16:50:03.486695 test begin: paddle.nn.functional.softmax(Tensor([4096, 4, 22, 144],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([4096, 4, 22, 144],"float32"), -1, name=None, ) 	 51904512 	 1000 	 0.30306291580200195 	 0.3303649425506592 	 0.28471851348876953 	 0.31542110443115234 	 0.45883870124816895 	 0.9106550216674805 	 0.4094216823577881 	 0.4653170108795166 	 
2025-07-24 16:50:07.284408 test begin: paddle.nn.functional.softmax(Tensor([60, 2304, 368],"float32"), axis=-1, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([60, 2304, 368],"float32"), axis=-1, ) 	 50872320 	 1000 	 1.5762159824371338 	 0.30691099166870117 	 0.28470730781555176 	 0.2800276279449463 	 0.4505906105041504 	 0.8930847644805908 	 0.3779616355895996 	 0.4562368392944336 	 
2025-07-24 16:50:14.235773 test begin: paddle.nn.functional.softmax(Tensor([60, 368, 2304],"float32"), axis=-1, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([60, 368, 2304],"float32"), axis=-1, ) 	 50872320 	 1000 	 0.30299949645996094 	 0.5082530975341797 	 0.2848978042602539 	 0.48600101470947266 	 0.46939516067504883 	 0.89241623878479 	 0.4012715816497803 	 0.4559617042541504 	 
2025-07-24 16:50:18.183450 test begin: paddle.nn.functional.softmax(Tensor([613, 4, 144, 144],"float32"), -1, name=None, )
[Prof] paddle.nn.functional.softmax 	 paddle.nn.functional.softmax(Tensor([613, 4, 144, 144],"float32"), -1, name=None, ) 	 50844672 	 1000 	 0.29831910133361816 	 0.3261873722076416 	 0.2877035140991211 	 0.3089892864227295 	 0.44962549209594727 	 0.8921425342559814 	 0.3966524600982666 	 0.45581555366516113 	 
2025-07-24 16:50:21.901446 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([1551, 16, 32, 64],"float32"), Tensor([1551, 16, 1, 64],"int64"), axis=2, )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:129: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:147: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([1551, 16, 32, 64],"float32"), Tensor([1551, 16, 1, 64],"int64"), axis=2, ) 	 52411392 	 1000 	 1.273766279220581 	 2.24637770652771 	 0.650749683380127 	 0.38191676139831543 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:50:26.929579 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([1551, 16, 32, 64],"float32"), Tensor([1551, 16, 32, 1],"int64"), axis=-1, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([1551, 16, 32, 64],"float32"), Tensor([1551, 16, 32, 1],"int64"), axis=-1, ) 	 51617280 	 1000 	 0.345409631729126 	 1.2768528461456299 	 0.32770633697509766 	 0.2605276107788086 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:50:29.978710 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([1551, 16, 32, 64],"float32"), Tensor([1551, 16, 32, 1],"int64"), axis=3, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([1551, 16, 32, 64],"float32"), Tensor([1551, 16, 32, 1],"int64"), axis=3, ) 	 51617280 	 1000 	 0.3453795909881592 	 1.2767815589904785 	 0.3275465965270996 	 0.26050353050231934 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:50:33.062843 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 1, 64],"int64"), axis=2, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 1, 64],"int64"), axis=2, ) 	 52394496 	 1000 	 1.2731659412384033 	 2.248196601867676 	 0.6501154899597168 	 0.38182950019836426 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:50:39.327453 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 32, 1],"int64"), axis=-1, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 32, 1],"int64"), axis=-1, ) 	 51600640 	 1000 	 0.3444221019744873 	 1.2763886451721191 	 0.32657766342163086 	 0.26038336753845215 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:50:42.351714 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 32, 1],"int64"), axis=3, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 12404, 32, 64],"float32"), Tensor([2, 12404, 32, 1],"int64"), axis=3, ) 	 51600640 	 1000 	 0.3467574119567871 	 1.276360034942627 	 0.3265690803527832 	 0.2604231834411621 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:50:45.392471 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 24807, 64],"float32"), Tensor([2, 16, 1, 64],"int64"), axis=2, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 24807, 64],"float32"), Tensor([2, 16, 1, 64],"int64"), axis=2, ) 	 50806784 	 1000 	 25.705400228500366 	 1.5070767402648926 	 13.135477781295776 	 0.25617218017578125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:51:15.082411 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 24807, 64],"float32"), Tensor([2, 16, 24807, 1],"int64"), axis=-1, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 24807, 64],"float32"), Tensor([2, 16, 24807, 1],"int64"), axis=-1, ) 	 51598560 	 1000 	 0.875713586807251 	 1.276362419128418 	 0.3266305923461914 	 0.26041388511657715 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:51:19.319034 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 24807, 64],"float32"), Tensor([2, 16, 24807, 1],"int64"), axis=3, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 24807, 64],"float32"), Tensor([2, 16, 24807, 1],"int64"), axis=3, ) 	 51598560 	 1000 	 0.34478020668029785 	 1.2763454914093018 	 0.32633161544799805 	 0.26039671897888184 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:51:22.341014 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 1, 49613],"int64"), axis=2, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 1, 49613],"int64"), axis=2, ) 	 52391328 	 1000 	 1.5380611419677734 	 2.218242645263672 	 0.7857885360717773 	 0.37714481353759766 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:51:27.543838 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 32, 1],"int64"), axis=-1, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 32, 1],"int64"), axis=-1, ) 	 50804736 	 1000 	 0.6421644687652588 	 1.0769073963165283 	 0.6239960193634033 	 0.21968603134155273 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:51:30.571931 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 32, 1],"int64"), axis=3, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 49613],"float32"), Tensor([2, 16, 32, 1],"int64"), axis=3, ) 	 50804736 	 1000 	 0.642256498336792 	 1.0795738697052002 	 0.6247329711914062 	 0.22230887413024902 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:51:33.595667 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 793801],"float32"), Tensor([2, 16, 1, 793801],"int64"), axis=2, )
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 32, 793801],"float32"), Tensor([2, 16, 1, 793801],"int64"), axis=2, ) 	 838253856 	 1000 	 23.9937162399292 	 44.57264184951782 	 12.260439395904541 	 5.047425270080566 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:53:07.214495 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 793801, 64],"float32"), Tensor([2, 16, 793801, 1],"int64"), axis=-1, )
[Error] CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacity of 39.39 GiB of which 1.35 GiB is free. Process 121189 has 38.03 GiB memory in use. Of the allocated memory 18.75 GiB is allocated by PyTorch, and 199.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-07-24 16:54:13.796984 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 16, 793801, 64],"float32"), Tensor([2, 16, 793801, 1],"int64"), axis=3, )
W0724 16:54:41.540608 140681 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:129: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:147: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Error] CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacity of 39.39 GiB of which 1.43 GiB is free. Process 23660 has 37.96 GiB memory in use. Of the allocated memory 18.74 GiB is allocated by PyTorch, and 196.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-07-24 16:55:18.297722 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 198451, 32, 64],"float32"), Tensor([2, 198451, 1, 64],"int64"), axis=2, )
W0724 16:55:34.838605 141306 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:129: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
W0724 16:55:36.157320 141306 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:147: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 198451, 32, 64],"float32"), Tensor([2, 198451, 1, 64],"int64"), axis=2, ) 	 838257024 	 1000 	 20.30323052406311 	 42.41598296165466 	 10.369489431381226 	 4.804438352584839 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 16:56:50.328871 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 396901, 32, 64],"float32"), Tensor([2, 396901, 32, 1],"int64"), axis=-1, )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:129: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:147: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Error] CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacity of 39.39 GiB of which 1.41 GiB is free. Process 84861 has 37.97 GiB memory in use. Of the allocated memory 18.74 GiB is allocated by PyTorch, and 196.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-07-24 16:57:53.049160 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([2, 396901, 32, 64],"float32"), Tensor([2, 396901, 32, 1],"int64"), axis=3, )
W0724 16:58:17.837883 142325 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:129: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:147: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Error] CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacity of 39.39 GiB of which 1.43 GiB is free. Process 67370 has 37.96 GiB memory in use. Of the allocated memory 18.74 GiB is allocated by PyTorch, and 196.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-07-24 16:58:57.625868 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([24807, 16, 32, 64],"float32"), Tensor([24807, 16, 1, 64],"int64"), axis=2, )
W0724 16:59:10.604387 142680 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:129: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
W0724 16:59:10.700773 142680 gpu_resources.cc:243] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:147: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.nn.functional.softmax_with_cross_entropy 	 paddle.nn.functional.softmax_with_cross_entropy(Tensor([24807, 16, 32, 64],"float32"), Tensor([24807, 16, 1, 64],"int64"), axis=2, ) 	 838278144 	 1000 	 20.27738904953003 	 42.41363072395325 	 10.361431121826172 	 4.804019212722778 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 17:00:24.870531 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([49613, 16, 32, 64],"float32"), Tensor([49613, 16, 32, 1],"int64"), axis=-1, )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:129: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:147: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Error] CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacity of 39.39 GiB of which 1.41 GiB is free. Process 127981 has 37.97 GiB memory in use. Of the allocated memory 18.74 GiB is allocated by PyTorch, and 196.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-07-24 17:01:26.735159 test begin: paddle.nn.functional.softmax_with_cross_entropy(Tensor([49613, 16, 32, 64],"float32"), Tensor([49613, 16, 32, 1],"int64"), axis=3, )
W0724 17:01:54.216290 143990 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:129: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:147: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.loss.softmax_with_cross_entropy" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.nn.functional.cross_entropy" instead.
    Reason: Please notice that behavior of "paddle.nn.functional.softmax_with_cross_entropy" and "paddle.nn.functional.cross_entropy" is different. [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Error] CUDA out of memory. Tried to allocate 6.06 GiB. GPU 0 has a total capacity of 39.39 GiB of which 1.43 GiB is free. Process 105426 has 37.96 GiB memory in use. Of the allocated memory 18.74 GiB is allocated by PyTorch, and 196.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-07-24 17:02:30.729975 test begin: paddle.nn.functional.softplus(Tensor([113401, 7, 64],"float32"), )
W0724 17:02:31.752552 144410 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([113401, 7, 64],"float32"), ) 	 50803648 	 1000 	 0.2996995449066162 	 0.30133557319641113 	 0.28353452682495117 	 0.2885153293609619 	 0.45453906059265137 	 0.4505884647369385 	 0.38656115531921387 	 0.3553595542907715 	 
2025-07-24 17:02:34.582177 test begin: paddle.nn.functional.softplus(Tensor([13, 10, 390794],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([13, 10, 390794],"float32"), ) 	 50803220 	 1000 	 0.7448198795318604 	 0.3051316738128662 	 0.2829916477203369 	 0.28867077827453613 	 0.450747013092041 	 0.4507594108581543 	 0.39383411407470703 	 0.3865382671356201 	 
2025-07-24 17:02:39.874466 test begin: paddle.nn.functional.softplus(Tensor([13, 1007, 3881],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([13, 1007, 3881],"float32"), ) 	 50806171 	 1000 	 0.3012716770172119 	 0.29995059967041016 	 0.28989696502685547 	 0.2885291576385498 	 0.45069336891174316 	 0.4506704807281494 	 0.39663076400756836 	 0.38915443420410156 	 
2025-07-24 17:02:43.093444 test begin: paddle.nn.functional.softplus(Tensor([13, 61062, 64],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([13, 61062, 64],"float32"), ) 	 50803584 	 1000 	 0.3035879135131836 	 0.2999100685119629 	 0.2923133373260498 	 0.28888440132141113 	 0.4506716728210449 	 0.4506359100341797 	 0.3955225944519043 	 0.388763427734375 	 
2025-07-24 17:02:46.274215 test begin: paddle.nn.functional.softplus(Tensor([14, 56701, 64],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([14, 56701, 64],"float32"), ) 	 50804096 	 1000 	 0.3023979663848877 	 0.30001282691955566 	 0.2920699119567871 	 0.28812646865844727 	 0.4506232738494873 	 0.45072412490844727 	 0.3960299491882324 	 0.38918137550354004 	 
2025-07-24 17:02:49.462836 test begin: paddle.nn.functional.softplus(Tensor([14, 7, 518401],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([14, 7, 518401],"float32"), ) 	 50803298 	 1000 	 0.30158448219299316 	 0.2998776435852051 	 0.2925748825073242 	 0.28879594802856445 	 0.45063281059265137 	 0.4507181644439697 	 0.3912804126739502 	 0.38887453079223633 	 
2025-07-24 17:02:52.640045 test begin: paddle.nn.functional.softplus(Tensor([789, 1007, 64],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([789, 1007, 64],"float32"), ) 	 50849472 	 1000 	 0.3004772663116455 	 0.30027294158935547 	 0.29110145568847656 	 0.28910374641418457 	 0.4509296417236328 	 0.451066255569458 	 0.39641618728637695 	 0.3887758255004883 	 
2025-07-24 17:02:55.830643 test begin: paddle.nn.functional.softplus(Tensor([79381, 10, 64],"float32"), )
[Prof] paddle.nn.functional.softplus 	 paddle.nn.functional.softplus(Tensor([79381, 10, 64],"float32"), ) 	 50803840 	 1000 	 0.30148983001708984 	 0.2999088764190674 	 0.2924365997314453 	 0.2888453006744385 	 0.4506711959838867 	 0.45064663887023926 	 0.39639949798583984 	 0.3888249397277832 	 
2025-07-24 17:02:59.026076 test begin: paddle.nn.functional.softshrink(Tensor([2822401, 3, 3],"float64"), 0, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([2822401, 3, 3],"float64"), 0, None, ) 	 25401609 	 1000 	 0.2983231544494629 	 0.2989933490753174 	 0.28952455520629883 	 0.2870042324066162 	 0.44843602180480957 	 0.4459671974182129 	 0.3943154811859131 	 0.38489603996276855 	 
2025-07-24 17:03:01.576313 test begin: paddle.nn.functional.softshrink(Tensor([2822401, 3, 3],"float64"), 5, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([2822401, 3, 3],"float64"), 5, None, ) 	 25401609 	 1000 	 0.29828572273254395 	 0.2988097667694092 	 0.28955650329589844 	 0.2870004177093506 	 0.4485163688659668 	 0.4459233283996582 	 0.39417195320129395 	 0.35370707511901855 	 
2025-07-24 17:03:04.191353 test begin: paddle.nn.functional.softshrink(Tensor([3, 2822401, 3],"float64"), 0, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([3, 2822401, 3],"float64"), 0, None, ) 	 25401609 	 1000 	 0.2983226776123047 	 0.30028462409973145 	 0.2894413471221924 	 0.2871568202972412 	 0.4479711055755615 	 0.4459700584411621 	 0.3939526081085205 	 0.38454174995422363 	 
2025-07-24 17:03:06.733627 test begin: paddle.nn.functional.softshrink(Tensor([3, 2822401, 3],"float64"), 5, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([3, 2822401, 3],"float64"), 5, None, ) 	 25401609 	 1000 	 0.2982940673828125 	 0.9709696769714355 	 0.2896425724029541 	 0.2870166301727295 	 0.4480593204498291 	 0.4458956718444824 	 0.394329309463501 	 0.38442134857177734 	 
2025-07-24 17:03:12.460745 test begin: paddle.nn.functional.softshrink(Tensor([3, 3, 2822401],"float64"), 0, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([3, 3, 2822401],"float64"), 0, None, ) 	 25401609 	 1000 	 0.2983081340789795 	 0.2988770008087158 	 0.28253173828125 	 0.2806587219238281 	 0.44822120666503906 	 0.4459671974182129 	 0.3854954242706299 	 0.3775012493133545 	 
2025-07-24 17:03:15.014568 test begin: paddle.nn.functional.softshrink(Tensor([3, 3, 2822401],"float64"), 5, None, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([3, 3, 2822401],"float64"), 5, None, ) 	 25401609 	 1000 	 0.2982814311981201 	 0.29868292808532715 	 0.28957653045654297 	 0.2871696949005127 	 0.44841837882995605 	 0.44592785835266113 	 0.3892397880554199 	 0.37847352027893066 	 
2025-07-24 17:03:17.611841 test begin: paddle.nn.functional.softshrink(Tensor([32, 15, 207, 8, 32, 2],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([32, 15, 207, 8, 32, 2],"float32"), threshold=0.01, ) 	 50872320 	 1000 	 0.29716968536376953 	 0.2988460063934326 	 0.28748488426208496 	 0.2862880229949951 	 0.45073580741882324 	 0.44728994369506836 	 0.39657163619995117 	 0.38114309310913086 	 
2025-07-24 17:03:20.764802 test begin: paddle.nn.functional.softshrink(Tensor([32, 15, 8, 207, 32, 2],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([32, 15, 8, 207, 32, 2],"float32"), threshold=0.01, ) 	 50872320 	 1000 	 0.2964200973510742 	 0.2988274097442627 	 0.2873406410217285 	 0.28618645668029785 	 0.45102858543395996 	 0.44731879234313965 	 0.37360072135925293 	 0.38393521308898926 	 
2025-07-24 17:03:23.972147 test begin: paddle.nn.functional.softshrink(Tensor([32, 15, 8, 8, 32, 52],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([32, 15, 8, 8, 32, 52],"float32"), threshold=0.01, ) 	 51118080 	 1000 	 0.2983057498931885 	 0.30037879943847656 	 0.28876209259033203 	 0.28824448585510254 	 0.45298194885253906 	 0.4495117664337158 	 0.39873218536376953 	 0.386415958404541 	 
2025-07-24 17:03:27.160961 test begin: paddle.nn.functional.softshrink(Tensor([32, 15, 8, 8, 827, 2],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([32, 15, 8, 8, 827, 2],"float32"), threshold=0.01, ) 	 50810880 	 1000 	 0.2960693836212158 	 0.29846644401550293 	 0.28715038299560547 	 0.28627729415893555 	 0.4504048824310303 	 0.446796178817749 	 0.3964040279388428 	 0.38319921493530273 	 
2025-07-24 17:03:30.378149 test begin: paddle.nn.functional.softshrink(Tensor([32, 388, 8, 8, 32, 2],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([32, 388, 8, 8, 32, 2],"float32"), threshold=0.01, ) 	 50855936 	 1000 	 0.296222448348999 	 0.2987813949584961 	 0.2870450019836426 	 0.2866218090057373 	 0.4507410526275635 	 0.44717907905578613 	 0.39551305770874023 	 0.3818807601928711 	 
2025-07-24 17:03:33.518045 test begin: paddle.nn.functional.softshrink(Tensor([827, 15, 8, 8, 32, 2],"float32"), threshold=0.01, )
[Prof] paddle.nn.functional.softshrink 	 paddle.nn.functional.softshrink(Tensor([827, 15, 8, 8, 32, 2],"float32"), threshold=0.01, ) 	 50810880 	 1000 	 0.29702091217041016 	 0.3096630573272705 	 0.2869527339935303 	 0.2795538902282715 	 0.4505343437194824 	 0.4467887878417969 	 0.39600324630737305 	 0.3720731735229492 	 
2025-07-24 17:03:39.119463 test begin: paddle.nn.functional.softsign(Tensor([12404, 4096],"float32"), )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([12404, 4096],"float32"), ) 	 50806784 	 1000 	 0.2956852912902832 	 1.0432777404785156 	 0.2800416946411133 	 0.3553488254547119 	 0.4504213333129883 	 3.2794220447540283 	 0.3876335620880127 	 0.41881680488586426 	 
2025-07-24 17:03:45.998890 test begin: paddle.nn.functional.softsign(Tensor([2822401, 3, 3],"float64"), None, )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([2822401, 3, 3],"float64"), None, ) 	 25401609 	 1000 	 0.3006618022918701 	 1.0430247783660889 	 0.2840712070465088 	 0.3552825450897217 	 0.4478340148925781 	 3.2697174549102783 	 0.38539981842041016 	 0.41763901710510254 	 
2025-07-24 17:03:52.133763 test begin: paddle.nn.functional.softsign(Tensor([3, 2822401, 3],"float64"), None, )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([3, 2822401, 3],"float64"), None, ) 	 25401609 	 1000 	 0.2993500232696533 	 1.0476725101470947 	 0.2841172218322754 	 0.35532450675964355 	 0.4479074478149414 	 3.2697675228118896 	 0.38574647903442383 	 0.41765332221984863 	 
2025-07-24 17:03:58.319911 test begin: paddle.nn.functional.softsign(Tensor([3, 3, 2822401],"float64"), None, )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([3, 3, 2822401],"float64"), None, ) 	 25401609 	 1000 	 0.2993733882904053 	 1.0430638790130615 	 0.28404855728149414 	 0.35526108741760254 	 0.4481637477874756 	 3.2696890830993652 	 0.38550257682800293 	 0.41760826110839844 	 
2025-07-24 17:04:04.452816 test begin: paddle.nn.functional.softsign(Tensor([300, 169345],"float32"), )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([300, 169345],"float32"), ) 	 50803500 	 1000 	 0.29519104957580566 	 1.0431296825408936 	 0.2796146869659424 	 0.3553135395050049 	 0.4503312110900879 	 3.279059648513794 	 0.38700008392333984 	 0.41880154609680176 	 
2025-07-24 17:04:11.159105 test begin: paddle.nn.functional.softsign(Tensor([32, 1587601],"float32"), )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([32, 1587601],"float32"), ) 	 50803232 	 1000 	 0.2952597141265869 	 1.0456821918487549 	 0.2797393798828125 	 0.35535669326782227 	 0.45059823989868164 	 3.279360294342041 	 0.39551496505737305 	 0.4188845157623291 	 
2025-07-24 17:04:19.835952 test begin: paddle.nn.functional.softsign(Tensor([396901, 128],"float32"), )
[Prof] paddle.nn.functional.softsign 	 paddle.nn.functional.softsign(Tensor([396901, 128],"float32"), ) 	 50803328 	 1000 	 0.29514169692993164 	 1.043200969696045 	 0.2796463966369629 	 0.35538816452026367 	 0.45034217834472656 	 3.27921724319458 	 0.3870279788970947 	 0.4188401699066162 	 
2025-07-24 17:04:26.546840 test begin: paddle.nn.functional.square_error_cost(Tensor([10161, 100, 100],"float16"), Tensor([10161, 100, 100],"float32"), )
W0724 17:04:29.846112 145375 dygraph_functions.cc:93089] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([10161, 100, 100],"float16"), Tensor([10161, 100, 100],"float32"), ) 	 203220000 	 1000 	 1.9673845767974854 	 1.4057857990264893 	 0.6689581871032715 	 0.7166202068328857 	 2.291788101196289 	 3.1375627517700195 	 0.7805008888244629 	 0.534475564956665 	 combined
2025-07-24 17:04:43.560759 test begin: paddle.nn.functional.square_error_cost(Tensor([3, 2, 1, 2],"float64"), label=Tensor([3, 2, 2116801, 2],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([3, 2, 1, 2],"float64"), label=Tensor([3, 2, 2116801, 2],"float64"), ) 	 25401624 	 1000 	 0.6041557788848877 	 0.5989377498626709 	 0.3058903217315674 	 0.305919885635376 	 4.141871929168701 	 1.5168232917785645 	 1.0592410564422607 	 0.22136545181274414 	 combined
2025-07-24 17:04:51.589278 test begin: paddle.nn.functional.square_error_cost(Tensor([3, 2, 1, 4233601],"float64"), label=Tensor([3, 2, 1, 4233601],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([3, 2, 1, 4233601],"float64"), label=Tensor([3, 2, 1, 4233601],"float64"), ) 	 50803212 	 1000 	 0.7434041500091553 	 0.7414274215698242 	 0.37975049018859863 	 0.37877345085144043 	 0.9264233112335205 	 1.3523473739624023 	 0.4733293056488037 	 0.27657413482666016 	 combined
2025-07-24 17:04:56.975631 test begin: paddle.nn.functional.square_error_cost(Tensor([3, 2, 2116801, 2],"float64"), label=Tensor([3, 2, 1, 2],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([3, 2, 2116801, 2],"float64"), label=Tensor([3, 2, 1, 2],"float64"), ) 	 25401624 	 1000 	 0.5983576774597168 	 0.5987966060638428 	 0.30499792098999023 	 0.30586910247802734 	 3.8378922939300537 	 1.5169506072998047 	 1.3082263469696045 	 0.22144818305969238 	 combined
2025-07-24 17:05:04.679118 test begin: paddle.nn.functional.square_error_cost(Tensor([3, 2, 2116801, 2],"float64"), label=Tensor([3, 2, 2116801, 2],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([3, 2, 2116801, 2],"float64"), label=Tensor([3, 2, 2116801, 2],"float64"), ) 	 50803224 	 1000 	 0.7437281608581543 	 0.7413434982299805 	 0.37993288040161133 	 0.37871432304382324 	 0.9262540340423584 	 1.3525183200836182 	 0.47321534156799316 	 0.27665114402770996 	 combined
2025-07-24 17:05:10.106200 test begin: paddle.nn.functional.square_error_cost(Tensor([3, 4233601, 1, 2],"float64"), label=Tensor([3, 4233601, 1, 2],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([3, 4233601, 1, 2],"float64"), label=Tensor([3, 4233601, 1, 2],"float64"), ) 	 50803212 	 1000 	 0.7439901828765869 	 0.7413804531097412 	 0.38007426261901855 	 0.3786940574645996 	 0.9263505935668945 	 1.3524930477142334 	 0.47326135635375977 	 0.2766561508178711 	 combined
2025-07-24 17:05:15.551911 test begin: paddle.nn.functional.square_error_cost(Tensor([5081, 100, 100],"float16"), Tensor([5081, 100, 100],"float32"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([5081, 100, 100],"float16"), Tensor([5081, 100, 100],"float32"), ) 	 101620000 	 1000 	 1.8776509761810303 	 0.7065467834472656 	 0.33619236946105957 	 0.36094069480895996 	 1.150463342666626 	 1.5800418853759766 	 0.3918430805206299 	 0.2691068649291992 	 combined
2025-07-24 17:05:24.987341 test begin: paddle.nn.functional.square_error_cost(Tensor([5081, 100, 100],"float32"), Tensor([5081, 100, 100],"float32"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([5081, 100, 100],"float32"), Tensor([5081, 100, 100],"float32"), ) 	 101620000 	 1000 	 0.7457869052886963 	 0.7431924343109131 	 0.3810305595397949 	 0.379697322845459 	 0.9243159294128418 	 1.3536131381988525 	 0.47228336334228516 	 0.27684545516967773 	 combined
2025-07-24 17:05:31.265921 test begin: paddle.nn.functional.square_error_cost(Tensor([6350401, 2, 1, 2],"float64"), label=Tensor([6350401, 2, 1, 2],"float64"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([6350401, 2, 1, 2],"float64"), label=Tensor([6350401, 2, 1, 2],"float64"), ) 	 50803208 	 1000 	 0.7439053058624268 	 0.7413222789764404 	 0.380018949508667 	 0.37871766090393066 	 0.9264328479766846 	 1.3526453971862793 	 0.473315954208374 	 0.27666687965393066 	 combined
2025-07-24 17:05:37.582190 test begin: paddle.nn.functional.square_error_cost(Tensor([8, 100, 63505],"float32"), Tensor([8, 100, 63505],"float32"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([8, 100, 63505],"float32"), Tensor([8, 100, 63505],"float32"), ) 	 101608000 	 1000 	 0.7534630298614502 	 0.74318528175354 	 0.38103795051574707 	 0.3796520233154297 	 0.9243488311767578 	 1.3534905910491943 	 0.47231292724609375 	 0.2768404483795166 	 combined
2025-07-24 17:05:44.299724 test begin: paddle.nn.functional.square_error_cost(Tensor([8, 63505, 100],"float32"), Tensor([8, 63505, 100],"float32"), )
[Prof] paddle.nn.functional.square_error_cost 	 paddle.nn.functional.square_error_cost(Tensor([8, 63505, 100],"float32"), Tensor([8, 63505, 100],"float32"), ) 	 101608000 	 1000 	 0.750586986541748 	 0.7431201934814453 	 0.3810591697692871 	 0.37961888313293457 	 0.924144983291626 	 1.3534746170043945 	 0.4721226692199707 	 0.2768669128417969 	 combined
2025-07-24 17:05:50.603149 test begin: paddle.nn.functional.swish(Tensor([128, 32, 112, 112],"float32"), )
2025-07-24 17:05:54.870751 test begin: paddle.nn.functional.swish(Tensor([128, 96, 112, 37],"float32"), )
2025-07-24 17:05:59.110144 test begin: paddle.nn.functional.swish(Tensor([128, 96, 37, 112],"float32"), )
2025-07-24 17:06:03.332272 test begin: paddle.nn.functional.swish(Tensor([16, 22, 384, 384],"float32"), )
2025-07-24 17:06:07.584697 test begin: paddle.nn.functional.swish(Tensor([16, 24, 368, 368],"float32"), )
2025-07-24 17:06:11.920504 test begin: paddle.nn.functional.swish(Tensor([16, 64, 130, 384],"float32"), )
2025-07-24 17:06:16.220580 test begin: paddle.nn.functional.swish(Tensor([16, 64, 135, 368],"float32"), )
2025-07-24 17:06:20.422485 test begin: paddle.nn.functional.swish(Tensor([16, 64, 368, 135],"float32"), )
2025-07-24 17:06:26.177236 test begin: paddle.nn.functional.swish(Tensor([16, 64, 384, 130],"float32"), )
2025-07-24 17:06:30.421145 test begin: paddle.nn.functional.swish(Tensor([43, 96, 112, 112],"float32"), )
2025-07-24 17:06:34.714373 test begin: paddle.nn.functional.swish(Tensor([6, 64, 368, 368],"float32"), )
2025-07-24 17:06:40.810362 test begin: paddle.nn.functional.swish(Tensor([6, 64, 384, 384],"float32"), )
2025-07-24 17:06:45.519474 test begin: paddle.nn.functional.tanh(Tensor([1016065, 50],"float32"), None, )
[Prof] paddle.nn.functional.tanh 	 paddle.nn.functional.tanh(Tensor([1016065, 50],"float32"), None, ) 	 50803250 	 1000 	 0.2956719398498535 	 0.30206847190856934 	 0.2798018455505371 	 0.2866525650024414 	 0.4502675533294678 	 0.44655394554138184 	 0.38715028762817383 	 0.37858128547668457 	 
2025-07-24 17:06:48.682913 test begin: paddle.nn.functional.tanh(Tensor([147015, 346],"float32"), None, )
[Prof] paddle.nn.functional.tanh 	 paddle.nn.functional.tanh(Tensor([147015, 346],"float32"), None, ) 	 50867190 	 1000 	 0.29597043991088867 	 0.2984650135040283 	 0.2875678539276123 	 0.28756189346313477 	 0.4508230686187744 	 0.44715261459350586 	 0.39673542976379395 	 0.38535475730895996 	 
2025-07-24 17:06:51.862296 test begin: paddle.nn.functional.tanh(Tensor([282600, 180],"float32"), None, )
[Prof] paddle.nn.functional.tanh 	 paddle.nn.functional.tanh(Tensor([282600, 180],"float32"), None, ) 	 50868000 	 1000 	 0.2960524559020996 	 0.30278682708740234 	 0.28758764266967773 	 0.2854475975036621 	 0.45088863372802734 	 0.44719743728637695 	 0.39202070236206055 	 0.38548946380615234 	 
2025-07-24 17:06:55.036918 test begin: paddle.nn.functional.tanh(Tensor([564481, 90],"float32"), None, )
[Prof] paddle.nn.functional.tanh 	 paddle.nn.functional.tanh(Tensor([564481, 90],"float32"), None, ) 	 50803290 	 1000 	 0.29546022415161133 	 0.29811787605285645 	 0.28699612617492676 	 0.2860746383666992 	 0.4503135681152344 	 0.4465603828430176 	 0.3959038257598877 	 0.38611793518066406 	 
2025-07-24 17:06:58.170763 test begin: paddle.nn.functional.tanh(Tensor([93401, 544],"float32"), None, )
[Prof] paddle.nn.functional.tanh 	 paddle.nn.functional.tanh(Tensor([93401, 544],"float32"), None, ) 	 50810144 	 1000 	 0.2979590892791748 	 0.2981226444244385 	 0.28714895248413086 	 0.28734278678894043 	 0.45032382011413574 	 0.4466221332550049 	 0.39614415168762207 	 0.3852269649505615 	 
2025-07-24 17:07:01.338488 test begin: paddle.nn.functional.tanhshrink(Tensor([2822401, 3, 3],"float64"), None, )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(Tensor([2822401, 3, 3],"float64"), None, ) 	 25401609 	 1000 	 0.2995173931121826 	 0.743053674697876 	 0.29109978675842285 	 0.37958407402038574 	 0.44950437545776367 	 1.1850011348724365 	 0.39617133140563965 	 0.4037907123565674 	 
2025-07-24 17:07:05.083435 test begin: paddle.nn.functional.tanhshrink(Tensor([3, 2822401, 3],"float64"), None, )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(Tensor([3, 2822401, 3],"float64"), None, ) 	 25401609 	 1000 	 0.2995171546936035 	 0.7431938648223877 	 0.291229248046875 	 0.37963175773620605 	 0.44820547103881836 	 1.1848726272583008 	 0.39504051208496094 	 0.40374326705932617 	 
2025-07-24 17:07:08.854420 test begin: paddle.nn.functional.tanhshrink(Tensor([3, 3, 2822401],"float64"), None, )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(Tensor([3, 3, 2822401],"float64"), None, ) 	 25401609 	 1000 	 0.29948949813842773 	 0.7429819107055664 	 0.29109835624694824 	 0.3796091079711914 	 0.4480302333831787 	 1.1849329471588135 	 0.39459967613220215 	 0.40372443199157715 	 
2025-07-24 17:07:12.606288 test begin: paddle.nn.functional.tanhshrink(Tensor([50803201],"float32"), None, )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(Tensor([50803201],"float32"), None, ) 	 50803201 	 1000 	 0.2956676483154297 	 0.7454485893249512 	 0.2870368957519531 	 0.37969446182250977 	 0.4502241611480713 	 1.1891038417816162 	 0.39607810974121094 	 0.4051787853240967 	 
2025-07-24 17:07:16.964965 test begin: paddle.nn.functional.tanhshrink(x=Tensor([2822401, 3, 3],"float64"), )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(x=Tensor([2822401, 3, 3],"float64"), ) 	 25401609 	 1000 	 0.299501895904541 	 0.7430505752563477 	 0.290816068649292 	 0.3795511722564697 	 0.44771790504455566 	 1.1850488185882568 	 0.39477992057800293 	 0.40380239486694336 	 
2025-07-24 17:07:20.703212 test begin: paddle.nn.functional.tanhshrink(x=Tensor([3, 2822401, 3],"float64"), )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(x=Tensor([3, 2822401, 3],"float64"), ) 	 25401609 	 1000 	 0.2994868755340576 	 0.7430896759033203 	 0.29076623916625977 	 0.379575252532959 	 0.4476604461669922 	 1.1849498748779297 	 0.39442014694213867 	 0.40373659133911133 	 
2025-07-24 17:07:24.456605 test begin: paddle.nn.functional.tanhshrink(x=Tensor([3, 3, 2822401],"float64"), )
[Prof] paddle.nn.functional.tanhshrink 	 paddle.nn.functional.tanhshrink(x=Tensor([3, 3, 2822401],"float64"), ) 	 25401609 	 1000 	 0.2994866371154785 	 0.7431533336639404 	 0.29079508781433105 	 0.37964892387390137 	 0.4483773708343506 	 1.1849803924560547 	 0.39522242546081543 	 0.4037353992462158 	 
2025-07-24 17:07:28.900052 test begin: paddle.nn.functional.temporal_shift(Tensor([128, 127, 56, 56],"float32"), 8, 0.125, )
[Error] too many values to unpack (expected 4)
2025-07-24 17:07:32.780045 test begin: paddle.nn.functional.temporal_shift(Tensor([128, 256, 28, 56],"float32"), 8, 0.125, )
[Error] too many values to unpack (expected 4)
2025-07-24 17:07:37.240278 test begin: paddle.nn.functional.temporal_shift(Tensor([128, 256, 56, 28],"float32"), 8, 0.125, )
[Error] too many values to unpack (expected 4)
2025-07-24 17:07:41.279560 test begin: paddle.nn.functional.temporal_shift(Tensor([240, 256, 15, 56],"float32"), 8, 0.125, data_format="NCHW", )
[Error] too many values to unpack (expected 4)
2025-07-24 17:07:43.918389 test begin: paddle.nn.functional.temporal_shift(Tensor([240, 256, 56, 15],"float32"), 8, 0.125, data_format="NCHW", )
[Error] too many values to unpack (expected 4)
2025-07-24 17:07:46.603159 test begin: paddle.nn.functional.temporal_shift(Tensor([240, 271, 28, 28],"float32"), 8, 0.125, data_format="NCHW", )
[Error] too many values to unpack (expected 4)
2025-07-24 17:07:49.248150 test begin: paddle.nn.functional.temporal_shift(Tensor([240, 512, 15, 28],"float32"), 8, 0.125, data_format="NCHW", )
[Error] too many values to unpack (expected 4)
2025-07-24 17:07:51.887342 test begin: paddle.nn.functional.temporal_shift(Tensor([240, 512, 28, 15],"float32"), 8, 0.125, data_format="NCHW", )
[Error] too many values to unpack (expected 4)
2025-07-24 17:07:54.568085 test begin: paddle.nn.functional.temporal_shift(Tensor([240, 68, 56, 56],"float32"), 8, 0.125, data_format="NCHW", )
[Error] too many values to unpack (expected 4)
2025-07-24 17:07:57.179061 test begin: paddle.nn.functional.temporal_shift(Tensor([64, 256, 56, 56],"float32"), 8, 0.125, )
[Error] too many values to unpack (expected 4)
2025-07-24 17:07:59.839541 test begin: paddle.nn.functional.temporal_shift(Tensor([64, 256, 56, 56],"float32"), 8, 0.125, data_format="NCHW", )
[Error] too many values to unpack (expected 4)
2025-07-24 17:08:02.502348 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 28225, 3, 3],"float64"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 28225, 3, 3],"float64"), 1.0, 0.0, None, ) 	 25402500 	 1000 	 0.2981269359588623 	 0.29845190048217773 	 0.28922462463378906 	 0.2793698310852051 	 0.44857287406921387 	 0.4459555149078369 	 0.3953843116760254 	 0.3849813938140869 	 
2025-07-24 17:08:05.040843 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4, 21169, 3],"float64"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 4, 21169, 3],"float64"), 1.0, 0.0, None, ) 	 25402800 	 1000 	 0.2982363700866699 	 0.29822278022766113 	 0.2892749309539795 	 0.2793292999267578 	 0.4482381343841553 	 0.44591641426086426 	 0.3943181037902832 	 0.3841845989227295 	 
2025-07-24 17:08:07.625024 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3, 21169],"float64"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3, 21169],"float64"), 1.0, 0.0, None, ) 	 25402800 	 1000 	 0.29825353622436523 	 0.29831576347351074 	 0.28928422927856445 	 0.279430627822876 	 0.44851112365722656 	 0.4459240436553955 	 0.3929324150085449 	 0.38263916969299316 	 
2025-07-24 17:08:10.162380 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3, 42337],"float32"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3, 42337],"float32"), 1.0, 0.0, None, ) 	 50804400 	 1000 	 0.2978217601776123 	 0.29802751541137695 	 0.28698301315307617 	 0.2792677879333496 	 0.45024752616882324 	 0.4466969966888428 	 0.396756649017334 	 0.3815336227416992 	 
2025-07-24 17:08:13.313600 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4, 42337, 3],"float32"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 4, 42337, 3],"float32"), 1.0, 0.0, None, ) 	 50804400 	 1000 	 0.29781603813171387 	 0.29802489280700684 	 0.28697896003723145 	 0.2792816162109375 	 0.4502267837524414 	 0.4467024803161621 	 0.3964977264404297 	 0.383648157119751 	 
2025-07-24 17:08:16.416910 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 56449, 3, 3],"float32"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([100, 56449, 3, 3],"float32"), 1.0, 0.0, None, ) 	 50804100 	 1000 	 0.29956555366516113 	 0.29804301261901855 	 0.28695058822631836 	 0.2793428897857666 	 0.45025634765625 	 0.44668102264404297 	 0.3966057300567627 	 0.38136744499206543 	 
2025-07-24 17:08:19.558279 test begin: paddle.nn.functional.thresholded_relu(Tensor([1411201, 4, 3, 3],"float32"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([1411201, 4, 3, 3],"float32"), 1.0, 0.0, None, ) 	 50803236 	 1000 	 0.2959411144256592 	 0.3048117160797119 	 0.2870006561279297 	 0.27900171279907227 	 0.450289249420166 	 0.44666004180908203 	 0.39638757705688477 	 0.3851940631866455 	 
2025-07-24 17:08:22.675968 test begin: paddle.nn.functional.thresholded_relu(Tensor([705601, 4, 3, 3],"float64"), 1.0, 0.0, None, )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(Tensor([705601, 4, 3, 3],"float64"), 1.0, 0.0, None, ) 	 25401636 	 1000 	 0.298203706741333 	 0.2982940673828125 	 0.28922414779663086 	 0.278151273727417 	 0.44854044914245605 	 0.44587206840515137 	 0.3943023681640625 	 0.3834977149963379 	 
2025-07-24 17:08:25.203248 test begin: paddle.nn.functional.thresholded_relu(x=Tensor([100, 4, 3, 42337],"float32"), )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(x=Tensor([100, 4, 3, 42337],"float32"), ) 	 50804400 	 1000 	 0.2959895133972168 	 0.29801201820373535 	 0.28673434257507324 	 0.27923583984375 	 0.4502131938934326 	 0.44670629501342773 	 0.3969416618347168 	 0.38139796257019043 	 
2025-07-24 17:08:28.384611 test begin: paddle.nn.functional.thresholded_relu(x=Tensor([100, 4, 42337, 3],"float32"), )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(x=Tensor([100, 4, 42337, 3],"float32"), ) 	 50804400 	 1000 	 0.29594922065734863 	 0.29807019233703613 	 0.2866384983062744 	 0.2794361114501953 	 0.4502708911895752 	 0.44670748710632324 	 0.3965165615081787 	 0.38382697105407715 	 
2025-07-24 17:08:31.533213 test begin: paddle.nn.functional.thresholded_relu(x=Tensor([100, 56449, 3, 3],"float32"), )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(x=Tensor([100, 56449, 3, 3],"float32"), ) 	 50804100 	 1000 	 0.9684317111968994 	 0.30691099166870117 	 0.285963773727417 	 0.2713449001312256 	 0.4503462314605713 	 0.4467885494232178 	 0.3966066837310791 	 0.36949920654296875 	 
2025-07-24 17:08:38.433106 test begin: paddle.nn.functional.thresholded_relu(x=Tensor([1411201, 4, 3, 3],"float32"), )
[Prof] paddle.nn.functional.thresholded_relu 	 paddle.nn.functional.thresholded_relu(x=Tensor([1411201, 4, 3, 3],"float32"), ) 	 50803236 	 1000 	 0.29591989517211914 	 0.30191707611083984 	 0.2865118980407715 	 0.27924513816833496 	 0.45037198066711426 	 0.44677281379699707 	 0.3968193531036377 	 0.3833801746368408 	 
2025-07-24 17:08:41.749386 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="mean", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="mean", name=None, ) 	 76204815 	 1000 	 2.357171058654785 	 1.824599027633667 	 1.7881393432617188e-05 	 0.15477371215820312 	 4.073001146316528 	 2.8678247928619385 	 0.465545654296875 	 0.1834545135498047 	 
2025-07-24 17:08:55.077818 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="none", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="none", name=None, ) 	 76204815 	 1000 	 2.312896728515625 	 1.8165736198425293 	 2.288818359375e-05 	 0.168532133102417 	 4.069302558898926 	 2.8643605709075928 	 0.5208802223205566 	 0.1954953670501709 	 
2025-07-24 17:09:07.788920 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="sum", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), Tensor([5, 5080321],"float64"), margin=0.3, swap=False, reduction="sum", name=None, ) 	 76204815 	 1000 	 2.349576711654663 	 1.821079969406128 	 2.8371810913085938e-05 	 0.15471959114074707 	 4.07213568687439 	 2.8643088340759277 	 0.46413207054138184 	 0.19547057151794434 	 
2025-07-24 17:09:20.624066 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="mean", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="mean", name=None, ) 	 76204815 	 1000 	 2.7869787216186523 	 2.234154462814331 	 0.0001537799835205078 	 0.20731616020202637 	 4.419904947280884 	 3.256643056869507 	 0.503594160079956 	 0.2084028720855713 	 
2025-07-24 17:09:35.050279 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="none", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="none", name=None, ) 	 76204815 	 1000 	 2.760204315185547 	 2.1976327896118164 	 0.00015306472778320312 	 0.2490856647491455 	 4.387189626693726 	 3.223539352416992 	 0.5614273548126221 	 0.21996617317199707 	 
2025-07-24 17:09:51.880777 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="sum", name=None, )
[Prof] paddle.nn.functional.triplet_margin_with_distance_loss 	 paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), Tensor([5080321, 5],"float64"), margin=0.3, swap=False, reduction="sum", name=None, ) 	 76204815 	 1000 	 2.77486252784729 	 2.234726667404175 	 0.00018477439880371094 	 0.2073988914489746 	 4.419561862945557 	 3.200413703918457 	 0.5030553340911865 	 0.21833229064941406 	 
2025-07-24 17:10:06.128525 test begin: paddle.nn.functional.unfold(Tensor([10, 1241, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 1241, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), ) 	 50831360 	 1000 	 1.7500734329223633 	 2.643826484680176 	 0.17862939834594727 	 0.24554657936096191 	 3.739114284515381 	 7.177037715911865 	 0.34766411781311035 	 7.109833240509033 	 
2025-07-24 17:10:29.601461 test begin: paddle.nn.functional.unfold(Tensor([10, 1241, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 1241, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, ) 	 50831360 	 1000 	 1.748450517654419 	 2.643648386001587 	 0.1786501407623291 	 0.24551606178283691 	 3.7389118671417236 	 7.1769938468933105 	 0.34758973121643066 	 7.116684913635254 	 
2025-07-24 17:10:53.047738 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 26461, 64],"float32"), 3, 1, 1, tuple(1,1,), )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 3, 26461, 64],"float32"), 3, 1, 1, tuple(1,1,), ) 	 50805120 	 1000 	 1.8291757106781006 	 2.654271125793457 	 0.18692493438720703 	 0.22579622268676758 	 3.778562068939209 	 7.224339723587036 	 0.3512251377105713 	 7.163452863693237 	 
2025-07-24 17:11:17.631372 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 26461, 64],"float32"), 3, 1, tuple(1,1,), 1, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 3, 26461, 64],"float32"), 3, 1, tuple(1,1,), 1, ) 	 50805120 	 1000 	 1.8292665481567383 	 2.6511998176574707 	 0.18695831298828125 	 0.22577238082885742 	 3.7786645889282227 	 7.224419355392456 	 0.3513202667236328 	 7.164082765579224 	 
2025-07-24 17:11:41.278062 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 64, 26461],"float32"), 3, 1, 1, tuple(1,1,), )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 3, 64, 26461],"float32"), 3, 1, 1, tuple(1,1,), ) 	 50805120 	 1000 	 2.158806085586548 	 2.9877450466156006 	 0.22059369087219238 	 0.254457950592041 	 3.7133100032806396 	 7.097854137420654 	 0.3450765609741211 	 7.034974098205566 	 
2025-07-24 17:12:05.306964 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 64, 26461],"float32"), 3, 1, tuple(1,1,), 1, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([10, 3, 64, 26461],"float32"), 3, 1, tuple(1,1,), 1, ) 	 50805120 	 1000 	 2.1588640213012695 	 2.988306760787964 	 0.22064971923828125 	 0.25446414947509766 	 3.712130069732666 	 7.098116636276245 	 0.3450925350189209 	 7.028258800506592 	 
2025-07-24 17:12:30.833980 test begin: paddle.nn.functional.unfold(Tensor([338, 3, 224, 224],"float32"), 16, 16, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([338, 3, 224, 224],"float32"), 16, 16, ) 	 50878464 	 1000 	 24.245718955993652 	 24.078444957733154 	 0.07331514358520508 	 0.07280278205871582 	 3.306401014328003 	 2.110459804534912 	 0.009956836700439453 	 2.049748182296753 	 
2025-07-24 17:13:26.653622 test begin: paddle.nn.functional.unfold(Tensor([4135, 3, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([4135, 3, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), ) 	 50810880 	 1000 	 36.68523955345154 	 30.904577016830444 	 0.00905156135559082 	 0.007550477981567383 	 80.89054584503174 	 7.175067901611328 	 0.019925594329833984 	 7.1102213859558105 	 
2025-07-24 17:16:11.863931 test begin: paddle.nn.functional.unfold(Tensor([4135, 3, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([4135, 3, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, ) 	 50810880 	 1000 	 36.68606495857239 	 30.929775714874268 	 0.009053707122802734 	 0.007541656494140625 	 80.9338948726654 	 7.176867961883545 	 0.019942522048950195 	 7.112962961196899 	 
2025-07-24 17:18:57.877650 test begin: paddle.nn.functional.unfold(Tensor([64, 16, 224, 224],"float32"), 16, 16, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([64, 16, 224, 224],"float32"), 16, 16, ) 	 51380224 	 1000 	 15.376912832260132 	 7.91424560546875 	 0.24552488327026367 	 0.126417875289917 	 1.8728249073028564 	 2.131565809249878 	 0.029354095458984375 	 2.0710134506225586 	 
2025-07-24 17:19:26.974267 test begin: paddle.nn.functional.unfold(Tensor([64, 3, 1182, 224],"float32"), 16, 16, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([64, 3, 1182, 224],"float32"), 16, 16, ) 	 50835456 	 1000 	 15.345154047012329 	 7.864363670349121 	 0.24506592750549316 	 0.12562084197998047 	 1.8629858493804932 	 2.4089488983154297 	 0.029211759567260742 	 1.2309093475341797 	 
2025-07-24 17:19:56.204581 test begin: paddle.nn.functional.unfold(Tensor([64, 3, 224, 1182],"float32"), 16, 16, )
[Prof] paddle.nn.functional.unfold 	 paddle.nn.functional.unfold(Tensor([64, 3, 224, 1182],"float32"), 16, 16, ) 	 50835456 	 1000 	 15.765810251235962 	 7.965205669403076 	 0.25179004669189453 	 0.1271970272064209 	 1.8651928901672363 	 2.4112167358398438 	 0.029245615005493164 	 1.2320544719696045 	 
2025-07-24 17:20:25.957006 test begin: paddle.nn.functional.zeropad2d(Tensor([169, 3, 224, 224],"float64"), list[2,2,2,2,], )
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:129: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.common.zeropad2d" is deprecated since 3.0.0, and will be removed in future versions. Please use "paddle.nn.ZeroPad2D" instead.
    Reason: Please use class ZeroPad2D [0m
  paddle_output = self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/tester/paddle_torch_gpu_performance.py:147: VisibleDeprecationWarning: [93m
Warning:
API "paddle.nn.functional.common.zeropad2d" is deprecated since 3.0.0, and will be removed in future versions. Please use "paddle.nn.ZeroPad2D" instead.
    Reason: Please use class ZeroPad2D [0m
  self.paddle_api(*tuple(self.paddle_args), **self.paddle_kwargs)
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([169, 3, 224, 224],"float64"), list[2,2,2,2,], ) 	 25439232 	 1000 	 0.5469644069671631 	 1.5759129524230957 	 0.2888925075531006 	 0.2340092658996582 	 0.45932793617248535 	 0.3031904697418213 	 0.23461651802062988 	 0.23007941246032715 	 combined
2025-07-24 17:20:31.435637 test begin: paddle.nn.functional.zeropad2d(Tensor([169, 3, 224, 224],"int64"), Tensor([4],"int32"), )
W0724 17:20:32.517755 152284 backward.cc:462] While running Node (Pad3dGradNode) raises an EnforceNotMet exception
[Error] (NotFound) The kernel with key (GPU, Undefined(AnyLayout), int64) of kernel `pad3d_grad` is not registered and fail to fallback to CPU one. Selected wrong DataType `int64`. Paddle support following DataTypes: float64, complex128, float16, float32, complex64, bfloat16.
  [Hint: Expected kernel_iter != iter->second.end(), but received kernel_iter == iter->second.end().] (at ../paddle/phi/core/kernel_factory.cc:380)

2025-07-24 17:20:32.539372 test begin: paddle.nn.functional.zeropad2d(Tensor([338, 3, 224, 224],"float32"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([338, 3, 224, 224],"float32"), list[2,2,2,2,], ) 	 50878464 	 1000 	 0.6300935745239258 	 0.91719651222229 	 0.5960166454315186 	 0.23908162117004395 	 0.7482633590698242 	 0.31164050102233887 	 0.38227200508117676 	 0.23463129997253418 	 combined
2025-07-24 17:20:39.203220 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 127, 224, 224],"float64"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 127, 224, 224],"float64"), list[2,2,2,2,], ) 	 25489408 	 1000 	 0.32548952102661133 	 0.45994067192077637 	 0.28333091735839844 	 0.23442435264587402 	 0.46025776863098145 	 0.3038313388824463 	 0.23505616188049316 	 0.23184990882873535 	 combined
2025-07-24 17:20:41.869876 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 127, 224, 224],"int64"), Tensor([4],"int32"), )
W0724 17:20:42.987465 152316 backward.cc:462] While running Node (Pad3dGradNode) raises an EnforceNotMet exception
[Error] (NotFound) The kernel with key (GPU, Undefined(AnyLayout), int64) of kernel `pad3d_grad` is not registered and fail to fallback to CPU one. Selected wrong DataType `int64`. Paddle support following DataTypes: float64, complex128, float16, float32, complex64, bfloat16.
  [Hint: Expected kernel_iter != iter->second.end(), but received kernel_iter == iter->second.end().] (at ../paddle/phi/core/kernel_factory.cc:380)

2025-07-24 17:20:43.016159 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 254, 224, 224],"float32"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 254, 224, 224],"float32"), list[2,2,2,2,], ) 	 50978816 	 1000 	 0.6313564777374268 	 0.469188928604126 	 0.5944452285766602 	 0.2395622730255127 	 0.7495377063751221 	 0.312286376953125 	 0.38299059867858887 	 0.24122095108032227 	 combined
2025-07-24 17:20:46.950568 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 3, 18901, 224],"float32"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 3, 18901, 224],"float32"), list[2,2,2,2,], ) 	 50805888 	 1000 	 0.6191058158874512 	 0.4650075435638428 	 0.5848405361175537 	 0.237504243850708 	 0.7465560436248779 	 0.31104254722595215 	 0.3814725875854492 	 0.24029755592346191 	 combined
2025-07-24 17:20:50.838318 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 18901],"float32"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 18901],"float32"), list[2,2,2,2,], ) 	 50805888 	 1000 	 0.6177482604980469 	 0.4487473964691162 	 0.5834605693817139 	 0.22721242904663086 	 0.7469995021820068 	 0.308457612991333 	 0.38160133361816406 	 0.2368636131286621 	 combined
2025-07-24 17:20:54.683525 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 9451],"float64"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 9451],"float64"), list[2,2,2,2,], ) 	 25404288 	 1000 	 0.3197648525238037 	 0.4401428699493408 	 0.28550243377685547 	 0.2247622013092041 	 0.45594263076782227 	 0.30000996589660645 	 0.23284602165222168 	 0.2297194004058838 	 combined
2025-07-24 17:20:57.287649 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 9451],"int64"), Tensor([4],"int32"), )
W0724 17:20:58.354151 152375 backward.cc:462] While running Node (Pad3dGradNode) raises an EnforceNotMet exception
[Error] (NotFound) The kernel with key (GPU, Undefined(AnyLayout), int64) of kernel `pad3d_grad` is not registered and fail to fallback to CPU one. Selected wrong DataType `int64`. Paddle support following DataTypes: float64, complex128, float16, float32, complex64, bfloat16.
  [Hint: Expected kernel_iter != iter->second.end(), but received kernel_iter == iter->second.end().] (at ../paddle/phi/core/kernel_factory.cc:380)

2025-07-24 17:20:58.376284 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 3, 9451, 224],"float64"), list[2,2,2,2,], )
[Prof] paddle.nn.functional.zeropad2d 	 paddle.nn.functional.zeropad2d(Tensor([4, 3, 9451, 224],"float64"), list[2,2,2,2,], ) 	 25404288 	 1000 	 0.31966400146484375 	 0.4550297260284424 	 0.28037571907043457 	 0.23238587379455566 	 0.4578683376312256 	 0.30267930030822754 	 0.23392009735107422 	 0.2227342128753662 	 combined
2025-07-24 17:21:01.026690 test begin: paddle.nn.functional.zeropad2d(Tensor([4, 3, 9451, 224],"int64"), Tensor([4],"int32"), )
W0724 17:21:02.109128 152392 backward.cc:462] While running Node (Pad3dGradNode) raises an EnforceNotMet exception
[Error] (NotFound) The kernel with key (GPU, Undefined(AnyLayout), int64) of kernel `pad3d_grad` is not registered and fail to fallback to CPU one. Selected wrong DataType `int64`. Paddle support following DataTypes: float64, complex128, float16, float32, complex64, bfloat16.
  [Hint: Expected kernel_iter != iter->second.end(), but received kernel_iter == iter->second.end().] (at ../paddle/phi/core/kernel_factory.cc:380)

2025-07-24 17:21:02.131930 test begin: paddle.nonzero(Tensor([510, 128, 28, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([510, 128, 28, 28],"float32"), ) 	 51179520 	 1000 	 7.411193132400513 	 2.3264055252075195 	 0.0054471492767333984 	 0.002149820327758789 	 None 	 None 	 None 	 None 	 
2025-07-24 17:21:12.763594 test begin: paddle.nonzero(Tensor([510, 80, 28, 45],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([510, 80, 28, 45],"float32"), ) 	 51408000 	 1000 	 7.431581974029541 	 2.338301658630371 	 0.005480051040649414 	 0.002168416976928711 	 None 	 None 	 None 	 None 	 
2025-07-24 17:21:23.428546 test begin: paddle.nonzero(Tensor([510, 80, 45, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([510, 80, 45, 28],"float32"), ) 	 51408000 	 1000 	 7.428948163986206 	 2.3388397693634033 	 0.00545954704284668 	 0.002146482467651367 	 None 	 None 	 None 	 None 	 
2025-07-24 17:21:35.264334 test begin: paddle.nonzero(Tensor([511, 127, 28, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([511, 127, 28, 28],"float32"), ) 	 50879248 	 1000 	 7.408668279647827 	 2.3330323696136475 	 0.005407810211181641 	 0.002127408981323242 	 None 	 None 	 None 	 None 	 
2025-07-24 17:21:47.413717 test begin: paddle.nonzero(Tensor([511, 80, 28, 45],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([511, 80, 28, 45],"float32"), ) 	 51508800 	 1000 	 7.446161508560181 	 2.345780372619629 	 0.005471944808959961 	 0.0021562576293945312 	 None 	 None 	 None 	 None 	 
2025-07-24 17:21:58.108354 test begin: paddle.nonzero(Tensor([511, 80, 45, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([511, 80, 45, 28],"float32"), ) 	 51508800 	 1000 	 7.444295644760132 	 2.349353790283203 	 0.005499601364135742 	 0.002138376235961914 	 None 	 None 	 None 	 None 	 
2025-07-24 17:22:08.819381 test begin: paddle.nonzero(Tensor([512, 127, 28, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([512, 127, 28, 28],"float32"), ) 	 50978816 	 1000 	 7.414389133453369 	 2.324617385864258 	 0.005469560623168945 	 0.002107381820678711 	 None 	 None 	 None 	 None 	 
2025-07-24 17:22:19.448087 test begin: paddle.nonzero(Tensor([512, 80, 28, 45],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([512, 80, 28, 45],"float32"), ) 	 51609600 	 1000 	 7.472134828567505 	 2.3473947048187256 	 0.005491018295288086 	 0.0021593570709228516 	 None 	 None 	 None 	 None 	 
2025-07-24 17:22:30.179637 test begin: paddle.nonzero(Tensor([512, 80, 45, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([512, 80, 45, 28],"float32"), ) 	 51609600 	 1000 	 7.456558465957642 	 2.35591459274292 	 0.005502462387084961 	 0.002122640609741211 	 None 	 None 	 None 	 None 	 
2025-07-24 17:22:41.576041 test begin: paddle.nonzero(Tensor([811, 80, 28, 28],"float32"), )
[Prof] paddle.nonzero 	 paddle.nonzero(Tensor([811, 80, 28, 28],"float32"), ) 	 50865920 	 1000 	 7.360062122344971 	 2.3159737586975098 	 0.005405426025390625 	 0.002119779586791992 	 None 	 None 	 None 	 None 	 
2025-07-24 17:22:53.923287 test begin: paddle.not_equal(Tensor([13, 15266, 16, 4, 1],"int64"), Tensor([13, 15266, 16, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 15266, 16, 4, 1],"int64"), Tensor([13, 15266, 16, 1, 8],"int64"), ) 	 38103936 	 1000 	 0.552506685256958 	 0.5155327320098877 	 0.5413730144500732 	 0.5031332969665527 	 None 	 None 	 None 	 None 	 
2025-07-24 17:22:55.625631 test begin: paddle.not_equal(Tensor([13, 2, 122124, 4, 1],"int64"), Tensor([13, 2, 122124, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 122124, 4, 1],"int64"), Tensor([13, 2, 122124, 1, 8],"int64"), ) 	 38102688 	 1000 	 0.5572183132171631 	 0.5156760215759277 	 0.5475640296936035 	 0.5034465789794922 	 None 	 None 	 None 	 None 	 
2025-07-24 17:22:57.324356 test begin: paddle.not_equal(Tensor([13, 2, 16, 4, 15266],"int64"), Tensor([13, 2, 16, 1, 15266],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 16, 4, 15266],"int64"), Tensor([13, 2, 16, 1, 15266],"int64"), ) 	 31753280 	 1000 	 0.21326756477355957 	 0.21950364112854004 	 0.2036280632019043 	 0.20716094970703125 	 None 	 None 	 None 	 None 	 
2025-07-24 17:22:58.277107 test begin: paddle.not_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 61062],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 61062],"int64"), ) 	 25403456 	 1000 	 0.5634400844573975 	 0.47238945960998535 	 0.5539212226867676 	 0.45956969261169434 	 None 	 None 	 None 	 None 	 
2025-07-24 17:22:59.728849 test begin: paddle.not_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), ) 	 25405120 	 1000 	 1.0338339805603027 	 0.9493803977966309 	 1.0241689682006836 	 0.93682861328125 	 None 	 None 	 None 	 None 	 
2025-07-24 17:23:02.128990 test begin: paddle.not_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 61062, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 16, 61062, 1],"int64"), Tensor([13, 2, 16, 61062, 8],"int64"), ) 	 228616128 	 1000 	 1.7866365909576416 	 1.5708317756652832 	 1.7696349620819092 	 1.552541732788086 	 None 	 None 	 None 	 None 	 
2025-07-24 17:23:09.185542 test begin: paddle.not_equal(Tensor([13, 2, 244247, 4, 1],"int64"), Tensor([13, 2, 244247, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 2, 244247, 4, 1],"int64"), Tensor([13, 2, 244247, 1, 8],"int64"), ) 	 76205064 	 1000 	 1.1074118614196777 	 1.0247931480407715 	 1.0978498458862305 	 1.0129022598266602 	 None 	 None 	 None 	 None 	 
2025-07-24 17:23:12.677583 test begin: paddle.not_equal(Tensor([13, 30531, 16, 4, 1],"int64"), Tensor([13, 30531, 16, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([13, 30531, 16, 4, 1],"int64"), Tensor([13, 30531, 16, 1, 8],"int64"), ) 	 76205376 	 1000 	 1.0985076427459717 	 1.0257833003997803 	 1.0886316299438477 	 1.0121009349822998 	 None 	 None 	 None 	 None 	 
2025-07-24 17:23:16.086241 test begin: paddle.not_equal(Tensor([198451, 2, 16, 4, 1],"int64"), Tensor([198451, 2, 16, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([198451, 2, 16, 4, 1],"int64"), Tensor([198451, 2, 16, 1, 8],"int64"), ) 	 76205184 	 1000 	 1.0980679988861084 	 1.0247557163238525 	 1.088393211364746 	 1.0128281116485596 	 None 	 None 	 None 	 None 	 
2025-07-24 17:23:19.529908 test begin: paddle.not_equal(Tensor([25401601],"int64"), Tensor([25401601],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([25401601],"int64"), Tensor([25401601],"int64"), ) 	 50803202 	 1000 	 0.31040358543395996 	 0.31328630447387695 	 0.3015170097351074 	 0.3022165298461914 	 None 	 None 	 None 	 None 	 
2025-07-24 17:23:21.000294 test begin: paddle.not_equal(Tensor([99226, 2, 16, 4, 1],"int64"), Tensor([99226, 2, 16, 1, 8],"int64"), )
[Prof] paddle.not_equal 	 paddle.not_equal(Tensor([99226, 2, 16, 4, 1],"int64"), Tensor([99226, 2, 16, 1, 8],"int64"), ) 	 38102784 	 1000 	 0.5554749965667725 	 0.5155088901519775 	 0.5460715293884277 	 0.5033509731292725 	 None 	 None 	 None 	 None 	 
2025-07-24 17:23:22.738357 test begin: paddle.numel(Tensor([50803201],"float32"), )
[Prof] paddle.numel 	 paddle.numel(Tensor([50803201],"float32"), ) 	 50803201 	 1000 	 0.00810694694519043 	 0.028144359588623047 	 1.2159347534179688e-05 	 3.266334533691406e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 17:23:23.580062 test begin: paddle.ones_like(Tensor([144, 392, 901],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([144, 392, 901],"float32"), ) 	 50859648 	 1000 	 0.1349015235900879 	 0.13441991806030273 	 0.12398886680603027 	 0.12286114692687988 	 None 	 None 	 None 	 None 	 
2025-07-24 17:23:24.661751 test begin: paddle.ones_like(Tensor([144, 901, 392],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([144, 901, 392],"float32"), ) 	 50859648 	 1000 	 0.13418936729431152 	 0.1343529224395752 	 0.12387275695800781 	 0.12294840812683105 	 None 	 None 	 None 	 None 	 
2025-07-24 17:23:25.774193 test begin: paddle.ones_like(Tensor([160, 392, 811],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([160, 392, 811],"float32"), ) 	 50865920 	 1000 	 0.1349804401397705 	 0.13442134857177734 	 0.11996245384216309 	 0.12291073799133301 	 None 	 None 	 None 	 None 	 
2025-07-24 17:23:26.864167 test begin: paddle.ones_like(Tensor([160, 811, 392],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([160, 811, 392],"float32"), ) 	 50865920 	 1000 	 0.13417601585388184 	 0.13438105583190918 	 0.1240990161895752 	 0.12298846244812012 	 None 	 None 	 None 	 None 	 
2025-07-24 17:23:27.975367 test begin: paddle.ones_like(Tensor([176, 392, 737],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([176, 392, 737],"float32"), ) 	 50847104 	 1000 	 0.13428664207458496 	 0.13606786727905273 	 0.12029409408569336 	 0.12302279472351074 	 None 	 None 	 None 	 None 	 
2025-07-24 17:23:29.083048 test begin: paddle.ones_like(Tensor([176, 737, 392],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([176, 737, 392],"float32"), ) 	 50847104 	 1000 	 0.13587379455566406 	 0.13425135612487793 	 0.1239473819732666 	 0.12086176872253418 	 None 	 None 	 None 	 None 	 
2025-07-24 17:23:30.272942 test begin: paddle.ones_like(Tensor([331, 392, 392],"float32"), )
[Prof] paddle.ones_like 	 paddle.ones_like(Tensor([331, 392, 392],"float32"), ) 	 50862784 	 1000 	 0.1341848373413086 	 0.13437914848327637 	 0.12348723411560059 	 0.12291932106018066 	 None 	 None 	 None 	 None 	 
2025-07-24 17:23:31.381101 test begin: paddle.outer(Tensor([50803201],"float32"), Tensor([2],"float32"), )
[Prof] paddle.outer 	 paddle.outer(Tensor([50803201],"float32"), Tensor([2],"float32"), ) 	 50803203 	 1000 	 5.290916919708252 	 0.5199882984161377 	 0.11035656929016113 	 0.500741720199585 	 3.713935613632202 	 2.6131412982940674 	 1.262727975845337 	 0.5333914756774902 	 
2025-07-24 17:23:46.469841 test begin: paddle.outer(Tensor([50803201],"float32"), Tensor([32],"float32"), )
[Prof] paddle.outer 	 paddle.outer(Tensor([50803201],"float32"), Tensor([32],"float32"), ) 	 50803233 	 1000 	 5.837599992752075 	 5.5469584465026855 	 0.11253571510314941 	 1.4169068336486816 	 13.174771785736084 	 40.71588134765625 	 4.4840428829193115 	 2.0776333808898926 	 
2025-07-24 17:25:20.239547 test begin: paddle.outer(Tensor([50803201],"float32"), Tensor([4],"float32"), )
[Prof] paddle.outer 	 paddle.outer(Tensor([50803201],"float32"), Tensor([4],"float32"), ) 	 50803205 	 1000 	 5.299168348312378 	 0.9489164352416992 	 0.11051654815673828 	 0.9304113388061523 	 4.347259521484375 	 5.065433502197266 	 1.4788670539855957 	 1.033839225769043 	 
2025-07-24 17:25:41.596678 test begin: paddle.pdist(Tensor([10, 5080321],"float32"), 0, )
[Prof] paddle.pdist 	 paddle.pdist(Tensor([10, 5080321],"float32"), 0, ) 	 50803210 	 1000 	 5.9489922523498535 	 9.02092719078064 	 3.123283386230469e-05 	 9.003405094146729 	 5.646219253540039 	 0.13423514366149902 	 0.0054814815521240234 	 0.05294299125671387 	 
2025-07-24 17:26:03.995176 test begin: paddle.pdist(Tensor([10, 5080321],"float32"), 1.0, )
[Prof] paddle.pdist 	 paddle.pdist(Tensor([10, 5080321],"float32"), 1.0, ) 	 50803210 	 1000 	 5.9264726638793945 	 8.379586219787598 	 3.933906555175781e-05 	 8.363265752792358 	 22.809283018112183 	 13.586509704589844 	 0.022669315338134766 	 6.942298650741577 	 
2025-07-24 17:26:55.658096 test begin: paddle.pdist(Tensor([50, 508033],"float64"), 2.0, )
[Prof] paddle.pdist 	 paddle.pdist(Tensor([50, 508033],"float64"), 2.0, ) 	 25401650 	 1000 	 22.958498001098633 	 2.1213479042053223 	 4.172325134277344e-05 	 2.107738733291626 	 87.94485020637512 	 39.29975867271423 	 0.08775115013122559 	 4.44643235206604 	 
2025-07-24 17:29:28.736669 test begin: paddle.polar(Tensor([1, 793801, 64],"float32"), Tensor([1, 793801, 64],"float32"), )
[Prof] paddle.polar 	 paddle.polar(Tensor([1, 793801, 64],"float32"), Tensor([1, 793801, 64],"float32"), ) 	 101606528 	 1000 	 2.078110694885254 	 2.0880520343780518 	 0.424945592880249 	 0.42411136627197266 	 4.6016247272491455 	 5.035436391830444 	 0.6716737747192383 	 0.46769094467163086 	 combined
2025-07-24 17:29:46.672973 test begin: paddle.polar(Tensor([1, 8192, 6202],"float32"), Tensor([1, 8192, 6202],"float32"), )
[Prof] paddle.polar 	 paddle.polar(Tensor([1, 8192, 6202],"float32"), Tensor([1, 8192, 6202],"float32"), ) 	 101613568 	 1000 	 2.078068256378174 	 2.074162006378174 	 0.4249699115753174 	 0.42412686347961426 	 4.598222494125366 	 5.0363993644714355 	 0.6711547374725342 	 0.46778345108032227 	 combined
2025-07-24 17:30:03.286865 test begin: paddle.polar(Tensor([1, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), )
[Prof] paddle.polar 	 paddle.polar(Tensor([1, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), ) 	 51380224 	 1000 	 1.7932336330413818 	 1.8029260635375977 	 0.3666856288909912 	 0.3686549663543701 	 3.4993557929992676 	 4.650473356246948 	 0.3969137668609619 	 0.3654491901397705 	 combined
2025-07-24 17:30:16.990888 test begin: paddle.polar(Tensor([97, 8192, 64],"float32"), Tensor([1, 8192, 64],"float32"), )
[Prof] paddle.polar 	 paddle.polar(Tensor([97, 8192, 64],"float32"), Tensor([1, 8192, 64],"float32"), ) 	 51380224 	 1000 	 1.2128055095672607 	 1.2140264511108398 	 0.24757146835327148 	 0.2485494613647461 	 2.6063036918640137 	 2.893610954284668 	 0.29512882232666016 	 0.22733116149902344 	 combined
2025-07-24 17:30:28.213518 test begin: paddle.polar(Tensor([97, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), )
[Prof] paddle.polar 	 paddle.polar(Tensor([97, 8192, 64],"float32"), Tensor([97, 8192, 64],"float32"), ) 	 101711872 	 1000 	 2.0795819759368896 	 2.1371049880981445 	 0.42526960372924805 	 0.42458248138427734 	 4.600371837615967 	 5.041178226470947 	 0.6718323230743408 	 0.46822690963745117 	 combined
2025-07-24 17:30:46.860934 test begin: paddle.polygamma(Tensor([10, 20, 254017],"float32"), 1, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([10, 20, 254017],"float32"), 1, ) 	 50803400 	 1000 	 5.5767128467559814 	 0.6279163360595703 	 5.567705154418945 	 0.61676025390625 	 9.079288005828857 	 10.271998882293701 	 9.0264253616333 	 5.249418258666992 	 
2025-07-24 17:31:14.226692 test begin: paddle.polygamma(Tensor([10, 5080321, 1],"float32"), 1, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([10, 5080321, 1],"float32"), 1, ) 	 50803210 	 1000 	 5.578213930130005 	 0.6279246807098389 	 5.561694860458374 	 0.6171467304229736 	 9.080873012542725 	 10.267462491989136 	 9.026699304580688 	 5.246864557266235 	 
2025-07-24 17:31:41.596160 test begin: paddle.polygamma(Tensor([2, 12700801],"float64"), 1, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([2, 12700801],"float64"), 1, ) 	 25401602 	 1000 	 9.261831283569336 	 0.6410298347473145 	 9.253215789794922 	 0.6305520534515381 	 11.439425945281982 	 21.5205819606781 	 11.385836839675903 	 10.997421264648438 	 
2025-07-24 17:32:25.790279 test begin: paddle.polygamma(Tensor([2, 2, 6350401],"float64"), 2, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([2, 2, 6350401],"float64"), 2, ) 	 25401604 	 1000 	 12.04343032836914 	 21.08132314682007 	 11.372629880905151 	 21.069725275039673 	 9.128458499908447 	 9.311645746231079 	 9.075629234313965 	 4.755423307418823 	 
2025-07-24 17:33:19.220566 test begin: paddle.polygamma(Tensor([2, 2116801, 6],"float64"), 2, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([2, 2116801, 6],"float64"), 2, ) 	 25401612 	 1000 	 11.51688003540039 	 21.073986291885376 	 11.508294105529785 	 21.06321430206299 	 9.128437519073486 	 9.291551113128662 	 9.07577109336853 	 4.745140075683594 	 
2025-07-24 17:34:12.138382 test begin: paddle.polygamma(Tensor([2116801, 2, 6],"float64"), 2, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([2116801, 2, 6],"float64"), 2, ) 	 25401612 	 1000 	 11.38010573387146 	 21.27907633781433 	 11.371584415435791 	 21.05460286140442 	 9.128394603729248 	 9.311386823654175 	 9.035043001174927 	 4.757562637329102 	 
2025-07-24 17:35:05.211148 test begin: paddle.polygamma(Tensor([2540161, 20, 1],"float32"), 1, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([2540161, 20, 1],"float32"), 1, ) 	 50803220 	 1000 	 5.577852964401245 	 0.6277661323547363 	 5.569062232971191 	 0.6172764301300049 	 9.082364559173584 	 10.274259567260742 	 9.02305555343628 	 5.249926805496216 	 
2025-07-24 17:35:32.482643 test begin: paddle.polygamma(Tensor([4233601, 6],"float64"), 1, )
[Prof] paddle.polygamma 	 paddle.polygamma(Tensor([4233601, 6],"float64"), 1, ) 	 25401606 	 1000 	 9.26209044456482 	 0.6411466598510742 	 9.253616571426392 	 0.6245968341827393 	 11.43929147720337 	 21.51678204536438 	 11.385597467422485 	 10.9951651096344 	 
2025-07-24 17:36:16.962429 test begin: paddle.positive(Tensor([10, 5080321],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([10, 5080321],"float32"), ) 	 50803210 	 1000 	 0.0018651485443115234 	 0.00020194053649902344 	 9.059906005859375e-06 	 1.3828277587890625e-05 	 0.029362916946411133 	 0.04671645164489746 	 1.7642974853515625e-05 	 3.838539123535156e-05 	 combined
2025-07-24 17:36:18.728466 test begin: paddle.positive(Tensor([1693441, 3, 4, 5],"float16"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([1693441, 3, 4, 5],"float16"), ) 	 101606460 	 1000 	 0.0018799304962158203 	 0.000202178955078125 	 5.7220458984375e-06 	 1.430511474609375e-05 	 0.02966022491455078 	 0.047654151916503906 	 2.0265579223632812e-05 	 3.62396240234375e-05 	 combined
2025-07-24 17:36:22.564073 test begin: paddle.positive(Tensor([2, 1270081, 4, 5],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([2, 1270081, 4, 5],"float32"), ) 	 50803240 	 1000 	 0.001867532730102539 	 0.00020170211791992188 	 5.245208740234375e-06 	 1.3828277587890625e-05 	 0.03036785125732422 	 0.04763054847717285 	 2.9325485229492188e-05 	 3.838539123535156e-05 	 combined
2025-07-24 17:36:24.264494 test begin: paddle.positive(Tensor([2, 2540161, 4, 5],"float16"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([2, 2540161, 4, 5],"float16"), ) 	 101606440 	 1000 	 0.0018620491027832031 	 0.00020170211791992188 	 6.4373016357421875e-06 	 1.239776611328125e-05 	 0.029535770416259766 	 0.04732489585876465 	 1.7404556274414062e-05 	 3.2901763916015625e-05 	 combined
2025-07-24 17:36:28.098016 test begin: paddle.positive(Tensor([2, 3, 1693441, 5],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([2, 3, 1693441, 5],"float32"), ) 	 50803230 	 1000 	 0.0018448829650878906 	 0.00020456314086914062 	 5.0067901611328125e-06 	 1.33514404296875e-05 	 0.02941298484802246 	 0.047696590423583984 	 2.7418136596679688e-05 	 4.458427429199219e-05 	 combined
2025-07-24 17:36:29.825320 test begin: paddle.positive(Tensor([2, 3, 3386881, 5],"float16"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([2, 3, 3386881, 5],"float16"), ) 	 101606430 	 1000 	 0.0018320083618164062 	 0.0002040863037109375 	 5.9604644775390625e-06 	 1.430511474609375e-05 	 0.029401063919067383 	 0.04742431640625 	 1.5974044799804688e-05 	 3.123283386230469e-05 	 combined
2025-07-24 17:36:33.595569 test begin: paddle.positive(Tensor([2, 3, 4, 2116801],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([2, 3, 4, 2116801],"float32"), ) 	 50803224 	 1000 	 0.0019173622131347656 	 0.00020265579223632812 	 5.245208740234375e-06 	 1.3828277587890625e-05 	 0.033089637756347656 	 0.053363800048828125 	 3.600120544433594e-05 	 4.363059997558594e-05 	 combined
2025-07-24 17:36:37.093462 test begin: paddle.positive(Tensor([2, 3, 4, 4233601],"float16"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([2, 3, 4, 4233601],"float16"), ) 	 101606424 	 1000 	 0.0018954277038574219 	 0.00021314620971679688 	 7.152557373046875e-06 	 1.5497207641601562e-05 	 0.029174089431762695 	 0.0479738712310791 	 1.3828277587890625e-05 	 4.76837158203125e-05 	 combined
2025-07-24 17:36:40.892325 test begin: paddle.positive(Tensor([49613, 1024],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([49613, 1024],"float32"), ) 	 50803712 	 1000 	 0.001894235610961914 	 0.0002028942108154297 	 5.9604644775390625e-06 	 1.33514404296875e-05 	 0.029084205627441406 	 0.04579329490661621 	 1.621246337890625e-05 	 2.9802322387695312e-05 	 combined
2025-07-24 17:36:42.578171 test begin: paddle.positive(Tensor([846721, 3, 4, 5],"float32"), )
[Prof] paddle.positive 	 paddle.positive(Tensor([846721, 3, 4, 5],"float32"), ) 	 50803260 	 1000 	 0.001842498779296875 	 0.0002009868621826172 	 6.67572021484375e-06 	 1.2636184692382812e-05 	 0.029335975646972656 	 0.04728269577026367 	 1.3828277587890625e-05 	 5.054473876953125e-05 	 combined
2025-07-24 17:36:44.289242 test begin: paddle.pow(Tensor([1024, 1024, 25],"float64"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([1024, 1024, 25],"float64"), 2, ) 	 26214400 	 1000 	 0.5917086601257324 	 0.9841711521148682 	 0.5820643901824951 	 0.29186439514160156 	 0.6237602233886719 	 1.0842714309692383 	 0.5701620578765869 	 0.36940765380859375 	 
2025-07-24 17:36:50.225214 test begin: paddle.pow(Tensor([1024, 1024, 49],"float32"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([1024, 1024, 49],"float32"), 2, ) 	 51380224 	 1000 	 0.37398838996887207 	 0.30109405517578125 	 0.36426424980163574 	 0.28835630416870117 	 0.4577186107635498 	 1.0640895366668701 	 0.40343737602233887 	 0.36255574226379395 	 
2025-07-24 17:36:54.151142 test begin: paddle.pow(Tensor([1024, 3101, 8],"float64"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([1024, 3101, 8],"float64"), 2, ) 	 25403392 	 1000 	 0.5732378959655762 	 0.29827022552490234 	 0.5643653869628906 	 0.28568530082702637 	 0.6046946048736572 	 1.0527698993682861 	 0.5508749485015869 	 0.35966992378234863 	 
2025-07-24 17:36:57.739255 test begin: paddle.pow(Tensor([1024, 6202, 8],"float32"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([1024, 6202, 8],"float32"), 2, ) 	 50806784 	 1000 	 0.3692798614501953 	 0.2978401184082031 	 0.360140323638916 	 0.2849454879760742 	 0.45267772674560547 	 1.0524318218231201 	 0.39745473861694336 	 0.3585219383239746 	 
2025-07-24 17:37:01.619951 test begin: paddle.pow(Tensor([22, 81, 94, 311],"float32"), 2.0, )
[Prof] paddle.pow 	 paddle.pow(Tensor([22, 81, 94, 311],"float32"), 2.0, ) 	 52094988 	 1000 	 0.3786640167236328 	 0.3053925037384033 	 0.3697335720062256 	 0.2920222282409668 	 0.4636857509613037 	 1.0817735195159912 	 0.4083521366119385 	 0.27657508850097656 	 
2025-07-24 17:37:05.571308 test begin: paddle.pow(Tensor([3101, 1024, 8],"float64"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([3101, 1024, 8],"float64"), 2, ) 	 25403392 	 1000 	 0.5732510089874268 	 0.29819560050964355 	 0.564117431640625 	 0.2855696678161621 	 0.6047217845916748 	 1.051222324371338 	 0.5512025356292725 	 0.35811758041381836 	 
2025-07-24 17:37:09.183712 test begin: paddle.pow(Tensor([4, 435, 94, 311],"float32"), 2.0, )
[Prof] paddle.pow 	 paddle.pow(Tensor([4, 435, 94, 311],"float32"), 2.0, ) 	 50867160 	 1000 	 0.3710944652557373 	 0.2981748580932617 	 0.3605475425720215 	 0.2848505973815918 	 0.4532792568206787 	 1.0566604137420654 	 0.3970153331756592 	 0.27013492584228516 	 
2025-07-24 17:37:13.107418 test begin: paddle.pow(Tensor([4, 81, 505, 311],"float32"), 2.0, )
[Prof] paddle.pow 	 paddle.pow(Tensor([4, 81, 505, 311],"float32"), 2.0, ) 	 50885820 	 1000 	 0.37035226821899414 	 0.2982759475708008 	 0.36109256744384766 	 0.28488588333129883 	 0.4532759189605713 	 1.0569508075714111 	 0.39904093742370605 	 0.27022671699523926 	 
2025-07-24 17:37:16.937360 test begin: paddle.pow(Tensor([4, 81, 94, 1669],"float32"), 2.0, )
[Prof] paddle.pow 	 paddle.pow(Tensor([4, 81, 94, 1669],"float32"), 2.0, ) 	 50831064 	 1000 	 0.369382381439209 	 0.29790782928466797 	 0.36037325859069824 	 0.2850048542022705 	 0.45281314849853516 	 1.0559804439544678 	 0.39863085746765137 	 0.270003080368042 	 
2025-07-24 17:37:20.763447 test begin: paddle.pow(Tensor([6202, 1024, 8],"float32"), 2, )
[Prof] paddle.pow 	 paddle.pow(Tensor([6202, 1024, 8],"float32"), 2, ) 	 50806784 	 1000 	 0.36928439140319824 	 0.2977480888366699 	 0.3601565361022949 	 0.2818338871002197 	 0.4526395797729492 	 1.0538852214813232 	 0.39775538444519043 	 0.35997533798217773 	 
2025-07-24 17:37:24.591596 test begin: paddle.prod(Tensor([10, 10, 28225, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
Warning: The core code of paddle.prod is too complex.
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 10, 28225, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402502 	 1000 	 0.3111999034881592 	 0.035245418548583984 	 0.0002143383026123047 	 4.267692565917969e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([10]) and output[0] has a shape of torch.Size([1, 10, 1, 1]).
2025-07-24 17:37:26.352849 test begin: paddle.prod(Tensor([10, 10, 9, 28225],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 10, 9, 28225],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402502 	 1000 	 0.30849480628967285 	 0.03507065773010254 	 0.00021219253540039062 	 2.6464462280273438e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([10]) and output[0] has a shape of torch.Size([1, 10, 1, 1]).
2025-07-24 17:37:28.108142 test begin: paddle.prod(Tensor([10, 31361, 9, 9],"float64"), Tensor([2],"int64"), )
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 31361, 9, 9],"float64"), Tensor([2],"int64"), ) 	 25402412 	 1000 	 0.2543027400970459 	 0.024259090423583984 	 0.00022482872009277344 	 3.743171691894531e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([10, 31361]) and output[0] has a shape of torch.Size([10, 31361, 1, 1]).
2025-07-24 17:37:29.762707 test begin: paddle.prod(Tensor([10, 31361, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 31361, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402412 	 1000 	 0.3264782428741455 	 0.035299062728881836 	 0.00023031234741210938 	 3.910064697265625e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([9]) and output[0] has a shape of torch.Size([1, 1, 9, 1]).
2025-07-24 17:37:31.514462 test begin: paddle.prod(Tensor([10, 5, 56449, 9],"float64"), Tensor([2],"int64"), )
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 5, 56449, 9],"float64"), Tensor([2],"int64"), ) 	 25402052 	 1000 	 4.877849817276001 	 0.025049924850463867 	 0.0048367977142333984 	 4.3392181396484375e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([10, 9]) and output[0] has a shape of torch.Size([10, 1, 1, 9]).
2025-07-24 17:37:39.033900 test begin: paddle.prod(Tensor([10, 5, 9, 56449],"float64"), Tensor([2],"int64"), )
[Prof] paddle.prod 	 paddle.prod(Tensor([10, 5, 9, 56449],"float64"), Tensor([2],"int64"), ) 	 25402052 	 1000 	 0.20839405059814453 	 0.03606009483337402 	 0.0001659393310546875 	 3.838539123535156e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([5, 9]) and output[0] has a shape of torch.Size([1, 5, 9, 1]).
2025-07-24 17:37:40.649831 test begin: paddle.prod(Tensor([16, 3175201],"float32"), -1, )
[Prof] paddle.prod 	 paddle.prod(Tensor([16, 3175201],"float32"), -1, ) 	 50803216 	 1000 	 0.17157769203186035 	 0.15260982513427734 	 0.08765769004821777 	 0.07793617248535156 	 0.791947603225708 	 1.5807688236236572 	 0.7270064353942871 	 0.000640869140625 	 
2025-07-24 17:37:44.182865 test begin: paddle.prod(Tensor([31361, 10, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], )
[Prof] paddle.prod 	 paddle.prod(Tensor([31361, 10, 9, 9],"float64"), list[0,Tensor([1],"int64"),Tensor([1],"int64"),], ) 	 25402412 	 1000 	 0.34083032608032227 	 0.05599713325500488 	 0.00023031234741210938 	 5.054473876953125e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([9]) and output[0] has a shape of torch.Size([1, 1, 9, 1]).
2025-07-24 15:40:54.403549 test begin: paddle.prod(Tensor([49613, 1024],"float32"), -1, )
Warning: The core code of paddle.prod is too complex.
W0724 15:40:55.370451 108603 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.prod 	 paddle.prod(Tensor([49613, 1024],"float32"), -1, ) 	 50803712 	 1000 	 0.14632368087768555 	 0.14728546142578125 	 0.12154674530029297 	 0.1261274814605713 	 0.7927553653717041 	 1.6009070873260498 	 0.7287311553955078 	 0.0005936622619628906 	 
2025-07-24 15:40:58.599468 test begin: paddle.prod(Tensor([62721, 5, 9, 9],"float64"), Tensor([2],"int64"), )
[Prof] paddle.prod 	 paddle.prod(Tensor([62721, 5, 9, 9],"float64"), Tensor([2],"int64"), ) 	 25402007 	 1000 	 0.8079679012298584 	 0.026085615158081055 	 0.0007753372192382812 	 4.887580871582031e-05 	 None 	 None 	 None 	 None 	 
[Error] Mismatch in shape: grad_output[0] has a shape of torch.Size([9, 9]) and output[0] has a shape of torch.Size([1, 1, 9, 9]).
2025-07-24 15:41:00.832551 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 1016065, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 1016065, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, ) 	 25402750 	 1000 	 38.952361822128296 	 51.5247540473938 	 0.03833508491516113 	 17.57160210609436 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:43:31.256303 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([2032129, 5, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([2032129, 5, 5],"float32"), 1, "mul", True, False, ) 	 50804350 	 1000 	 0.139082670211792 	 0.025821685791015625 	 2.3603439331054688e-05 	 3.337860107421875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:43:31.522171 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 2032129, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 2032129, 5],"float32"), 1, "mul", True, False, ) 	 50804350 	 1000 	 0.1398637294769287 	 0.025825023651123047 	 1.6450881958007812e-05 	 3.337860107421875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:43:31.781473 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 2032129],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 2032129],"float32"), 1, "mul", True, False, ) 	 50804350 	 1000 	 0.14354538917541504 	 0.025790691375732422 	 1.1920928955078125e-05 	 3.814697265625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:43:32.038817 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 2032129, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 2032129, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, ) 	 50804350 	 1000 	 51.06450080871582 	 45.521681785583496 	 0.051070213317871094 	 15.521680116653442 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:47:05.924726 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([2032129, 5, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([2032129, 5, 5],"int32"), 1, "mul", True, False, ) 	 50804350 	 1000 	 0.1311936378479004 	 0.026680707931518555 	 2.2649765014648438e-05 	 4.458427429199219e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:47:06.177382 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 2032129, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 2032129, 5],"int32"), 1, "mul", True, False, ) 	 50804350 	 1000 	 0.13027358055114746 	 0.025622844696044922 	 1.5735626220703125e-05 	 3.147125244140625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:47:06.419176 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 2032129],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 2032129],"int32"), 1, "mul", True, False, ) 	 50804350 	 1000 	 0.12792205810546875 	 0.02594923973083496 	 1.2636184692382812e-05 	 3.719329833984375e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:47:06.659069 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 1016065, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 1016065, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, ) 	 25402750 	 1000 	 29.211753845214844 	 25.092684507369995 	 0.029132843017578125 	 8.563500881195068 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:49:00.141035 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([1016065, 5, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([1016065, 5, 5],"int64"), 1, "mul", True, False, ) 	 25402750 	 1000 	 0.14014363288879395 	 0.02728271484375 	 2.5033950805664062e-05 	 7.963180541992188e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:49:00.403781 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 1016065, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 1016065, 5],"int64"), 1, "mul", True, False, ) 	 25402750 	 1000 	 0.12940263748168945 	 0.025828838348388672 	 1.9550323486328125e-05 	 4.220008850097656e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:49:00.646447 test begin: paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 1016065],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 1016065],"int64"), 1, "mul", True, False, ) 	 25402750 	 1000 	 0.1279926300048828 	 0.02571845054626465 	 1.33514404296875e-05 	 5.030632019042969e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:49:00.890638 test begin: paddle.put_along_axis(Tensor([10, 10, 254017],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 254017],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, ) 	 25401950 	 1000 	 0.3746194839477539 	 0.6302318572998047 	 0.000308990478515625 	 0.12854480743408203 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:49:03.139196 test begin: paddle.put_along_axis(Tensor([10, 10, 508033],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 508033],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, ) 	 50803550 	 1000 	 0.3722712993621826 	 0.6306841373443604 	 0.0003037452697753906 	 0.1286299228668213 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:49:06.290633 test begin: paddle.put_along_axis(Tensor([10, 10, 508033],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 10, 508033],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, ) 	 50803550 	 1000 	 0.37141990661621094 	 0.6308643817901611 	 0.00030231475830078125 	 0.1286628246307373 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:49:08.996176 test begin: paddle.put_along_axis(Tensor([10, 254017, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 254017, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, ) 	 25401950 	 1000 	 0.3782196044921875 	 0.6299889087677002 	 0.0003075599670410156 	 0.12849187850952148 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:49:11.347472 test begin: paddle.put_along_axis(Tensor([10, 508033, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 508033, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, ) 	 50803550 	 1000 	 0.371549129486084 	 0.6311936378479004 	 0.0003044605255126953 	 0.12850332260131836 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:49:14.702169 test begin: paddle.put_along_axis(Tensor([10, 508033, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([10, 508033, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, ) 	 50803550 	 1000 	 0.36933350563049316 	 0.6301150321960449 	 0.0003032684326171875 	 0.1285088062286377 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:49:17.565634 test begin: paddle.put_along_axis(Tensor([254017, 10, 10],"int64"), Tensor([254017, 5, 5],"int64"), Tensor([254017, 5, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([254017, 10, 10],"int64"), Tensor([254017, 5, 5],"int64"), Tensor([254017, 5, 5],"int64"), 1, "mul", True, False, ) 	 38102550 	 1000 	 0.8277573585510254 	 1.0306875705718994 	 0.0006196498870849609 	 0.21066665649414062 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:49:33.919155 test begin: paddle.put_along_axis(Tensor([254017, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([254017, 10, 10],"int64"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"int64"), 1, "mul", True, False, ) 	 25401950 	 1000 	 0.3785533905029297 	 0.6460311412811279 	 0.0003075599670410156 	 0.12856698036193848 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:49:39.610829 test begin: paddle.put_along_axis(Tensor([508033, 10, 10],"float32"), Tensor([254017, 5, 5],"int64"), Tensor([508033, 5, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([508033, 10, 10],"float32"), Tensor([254017, 5, 5],"int64"), Tensor([508033, 5, 5],"float32"), 1, "mul", True, False, ) 	 69854550 	 1000 	 0.7835123538970947 	 0.9567320346832275 	 0.0005688667297363281 	 0.195448637008667 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:50:01.981862 test begin: paddle.put_along_axis(Tensor([508033, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([508033, 10, 10],"float32"), Tensor([5, 5, 5],"int64"), Tensor([5, 5, 5],"float32"), 1, "mul", True, False, ) 	 50803550 	 1000 	 0.376507043838501 	 0.6303143501281738 	 0.0002994537353515625 	 0.12856531143188477 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 15:50:05.177000 test begin: paddle.put_along_axis(Tensor([508033, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([508033, 10, 10],"int32"), Tensor([5, 5, 5],"int32"), Tensor([5, 5, 5],"int32"), 1, "mul", True, False, ) 	 50803550 	 1000 	 0.36894774436950684 	 0.6304962635040283 	 0.0003037452697753906 	 0.12859010696411133 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:50:07.852174 test begin: paddle.put_along_axis(Tensor([508033, 10, 10],"int32"), Tensor([508033, 5, 5],"int32"), Tensor([508033, 5, 5],"int32"), 1, "mul", True, False, )
[Prof] paddle.put_along_axis 	 paddle.put_along_axis(Tensor([508033, 10, 10],"int32"), Tensor([508033, 5, 5],"int32"), Tensor([508033, 5, 5],"int32"), 1, "mul", True, False, ) 	 76204950 	 1000 	 1.0473697185516357 	 1.2630102634429932 	 0.0008351802825927734 	 0.257523775100708 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:50:39.846444 test begin: paddle.rad2deg(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 1000 	 0.29721713066101074 	 0.2984354496002197 	 0.2814791202545166 	 0.2836425304412842 	 0.29606056213378906 	 0.29774045944213867 	 0.24667716026306152 	 0.22189664840698242 	 
2025-07-24 15:50:42.759269 test begin: paddle.rad2deg(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 1000 	 0.2961122989654541 	 0.3004496097564697 	 0.2816791534423828 	 0.28389430046081543 	 0.2961764335632324 	 0.29776453971862793 	 0.2467203140258789 	 0.2269284725189209 	 
2025-07-24 15:50:45.621296 test begin: paddle.rad2deg(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 1000 	 0.29674625396728516 	 0.29776859283447266 	 0.28169846534729004 	 0.2837693691253662 	 0.29618287086486816 	 0.29775333404541016 	 0.24696779251098633 	 0.22499608993530273 	 
2025-07-24 15:50:48.531527 test begin: paddle.rad2deg(x=Tensor([1587601, 4, 4],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([1587601, 4, 4],"float64"), ) 	 25401616 	 1000 	 0.2981414794921875 	 0.29842162132263184 	 0.28354406356811523 	 0.2838928699493408 	 0.2978518009185791 	 0.29837870597839355 	 0.24873948097229004 	 0.232757568359375 	 
2025-07-24 15:50:50.799445 test begin: paddle.rad2deg(x=Tensor([396901, 4, 4, 4],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 1000 	 0.29811644554138184 	 0.2983834743499756 	 0.28352808952331543 	 0.28446316719055176 	 0.2978482246398926 	 0.29842233657836914 	 0.24874663352966309 	 0.2366011142730713 	 
2025-07-24 15:50:53.061355 test begin: paddle.rad2deg(x=Tensor([4, 1587601, 4],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([4, 1587601, 4],"float64"), ) 	 25401616 	 1000 	 0.298112154006958 	 0.2983219623565674 	 0.28350186347961426 	 0.2842893600463867 	 0.2978324890136719 	 0.2984645366668701 	 0.24651455879211426 	 0.23637175559997559 	 
2025-07-24 15:50:55.371859 test begin: paddle.rad2deg(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 1000 	 0.29812145233154297 	 0.30359506607055664 	 0.2835042476654053 	 0.2843925952911377 	 0.29791259765625 	 0.29842591285705566 	 0.2471311092376709 	 0.23588848114013672 	 
2025-07-24 15:51:01.257367 test begin: paddle.rad2deg(x=Tensor([4, 4, 1587601],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([4, 4, 1587601],"float64"), ) 	 25401616 	 1000 	 0.300262451171875 	 0.298382043838501 	 0.28255701065063477 	 0.28436732292175293 	 0.2978231906890869 	 0.2984335422515869 	 0.24881553649902344 	 0.23637819290161133 	 
2025-07-24 15:51:03.513173 test begin: paddle.rad2deg(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 1000 	 0.2981293201446533 	 0.2984609603881836 	 0.2835712432861328 	 0.2843818664550781 	 0.29784488677978516 	 0.29848790168762207 	 0.2485659122467041 	 0.23538517951965332 	 
2025-07-24 15:51:07.836235 test begin: paddle.rad2deg(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.rad2deg 	 paddle.rad2deg(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 1000 	 1.2618684768676758 	 0.29838085174560547 	 0.283405065536499 	 0.28443479537963867 	 0.2978689670562744 	 0.2984309196472168 	 0.24818682670593262 	 0.2323625087738037 	 
2025-07-24 15:51:11.906148 test begin: paddle.rank(Tensor([10160641, 5],"float32"), )
[Prof] paddle.rank 	 paddle.rank(Tensor([10160641, 5],"float32"), ) 	 50803205 	 1000 	 0.039754390716552734 	 0.027691364288330078 	 1.9550323486328125e-05 	 3.528594970703125e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:51:12.828083 test begin: paddle.rank(Tensor([3, 16934401],"float32"), )
[Prof] paddle.rank 	 paddle.rank(Tensor([3, 16934401],"float32"), ) 	 50803203 	 1000 	 0.038613080978393555 	 0.027858734130859375 	 2.0503997802734375e-05 	 3.4809112548828125e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:51:13.768277 test begin: paddle.rank(input=Tensor([12700801, 2],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([12700801, 2],"float64"), ) 	 25401602 	 1000 	 0.03883790969848633 	 0.02793264389038086 	 2.4557113647460938e-05 	 3.600120544433594e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:51:14.399609 test begin: paddle.rank(input=Tensor([2, 12700801],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([2, 12700801],"float64"), ) 	 25401602 	 1000 	 0.03910350799560547 	 0.02778315544128418 	 1.8596649169921875e-05 	 4.482269287109375e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:51:15.023724 test begin: paddle.rank(input=Tensor([3, 2, 2, 2116801],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([3, 2, 2, 2116801],"float64"), ) 	 25401612 	 1000 	 0.03872966766357422 	 0.02789926528930664 	 2.4557113647460938e-05 	 4.1484832763671875e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:51:15.644503 test begin: paddle.rank(input=Tensor([3, 2, 2116801, 2],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([3, 2, 2116801, 2],"float64"), ) 	 25401612 	 1000 	 0.04184269905090332 	 0.03596830368041992 	 1.9550323486328125e-05 	 4.553794860839844e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:51:16.256681 test begin: paddle.rank(input=Tensor([3, 2116801, 2, 2],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([3, 2116801, 2, 2],"float64"), ) 	 25401612 	 1000 	 0.038961172103881836 	 0.027692079544067383 	 1.7642974853515625e-05 	 3.600120544433594e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:51:16.884252 test begin: paddle.rank(input=Tensor([3175201, 2, 2, 2],"float64"), )
[Prof] paddle.rank 	 paddle.rank(input=Tensor([3175201, 2, 2, 2],"float64"), ) 	 25401608 	 1000 	 0.03886055946350098 	 0.027663707733154297 	 2.193450927734375e-05 	 3.695487976074219e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 15:51:17.505491 test begin: paddle.reciprocal(Tensor([125, 1, 640, 640],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([125, 1, 640, 640],"float32"), ) 	 51200000 	 1000 	 0.29827165603637695 	 0.30072021484375 	 0.2896111011505127 	 0.2900571823120117 	 0.4538295269012451 	 1.048504114151001 	 0.4018571376800537 	 0.35716962814331055 	 
2025-07-24 15:51:21.391294 test begin: paddle.reciprocal(Tensor([16, 1, 4962, 640],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([16, 1, 4962, 640],"float32"), ) 	 50810880 	 1000 	 0.2960054874420166 	 0.29821252822875977 	 0.28751254081726074 	 0.28806233406066895 	 0.4503910541534424 	 1.0406887531280518 	 0.3985555171966553 	 0.3545377254486084 	 
2025-07-24 15:51:25.156702 test begin: paddle.reciprocal(Tensor([16, 1, 640, 4962],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([16, 1, 640, 4962],"float32"), ) 	 50810880 	 1000 	 0.29709386825561523 	 0.2981700897216797 	 0.2873716354370117 	 0.2880399227142334 	 0.45044636726379395 	 1.0406122207641602 	 0.3986849784851074 	 0.35453200340270996 	 
2025-07-24 15:51:28.921391 test begin: paddle.reciprocal(Tensor([16, 8, 640, 640],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([16, 8, 640, 640],"float32"), ) 	 52428800 	 1000 	 0.30587029457092285 	 0.30762314796447754 	 0.28943490982055664 	 0.2902817726135254 	 0.46460771560668945 	 1.0733411312103271 	 0.39724135398864746 	 0.3656492233276367 	 
2025-07-24 15:51:32.813376 test begin: paddle.reciprocal(Tensor([4, 1, 13231, 960],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([4, 1, 13231, 960],"float32"), ) 	 50807040 	 1000 	 0.2958540916442871 	 1.2007758617401123 	 0.28730344772338867 	 0.2877070903778076 	 0.4503905773162842 	 1.0407111644744873 	 0.3988463878631592 	 0.3544468879699707 	 
2025-07-24 15:51:40.047666 test begin: paddle.reciprocal(Tensor([4, 1, 960, 13231],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([4, 1, 960, 13231],"float32"), ) 	 50807040 	 1000 	 0.2968769073486328 	 0.2982301712036133 	 0.2853124141693115 	 0.28771162033081055 	 0.45028257369995117 	 1.0406699180603027 	 0.3987410068511963 	 0.35455799102783203 	 
2025-07-24 15:51:43.813131 test begin: paddle.reciprocal(Tensor([4, 14, 960, 960],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([4, 14, 960, 960],"float32"), ) 	 51609600 	 1000 	 0.30055952072143555 	 0.3027687072753906 	 0.28490614891052246 	 0.28574657440185547 	 0.45731282234191895 	 1.0566887855529785 	 0.396526575088501 	 0.3599812984466553 	 
2025-07-24 15:51:47.704560 test begin: paddle.reciprocal(Tensor([56, 1, 960, 960],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([56, 1, 960, 960],"float32"), ) 	 51609600 	 1000 	 0.3004615306854248 	 0.3077857494354248 	 0.2918567657470703 	 0.2919161319732666 	 0.4574103355407715 	 1.056654691696167 	 0.4042837619781494 	 0.3599703311920166 	 
2025-07-24 15:51:51.569836 test begin: paddle.reciprocal(Tensor([8, 1, 6616, 960],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([8, 1, 6616, 960],"float32"), ) 	 50810880 	 1000 	 0.295989990234375 	 0.3018913269042969 	 0.2875096797943115 	 0.2814979553222656 	 0.45044422149658203 	 1.040649175643921 	 0.38942432403564453 	 0.35454678535461426 	 
2025-07-24 15:51:55.406531 test begin: paddle.reciprocal(Tensor([8, 1, 960, 6616],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([8, 1, 960, 6616],"float32"), ) 	 50810880 	 1000 	 0.2959251403808594 	 0.29819703102111816 	 0.28049540519714355 	 0.281599760055542 	 0.4504852294921875 	 1.040731430053711 	 0.38669610023498535 	 0.35458946228027344 	 
2025-07-24 15:51:59.197989 test begin: paddle.reciprocal(Tensor([8, 7, 960, 960],"float32"), )
[Prof] paddle.reciprocal 	 paddle.reciprocal(Tensor([8, 7, 960, 960],"float32"), ) 	 51609600 	 1000 	 0.3005194664001465 	 1.4445312023162842 	 0.2850809097290039 	 0.2843208312988281 	 0.45734095573425293 	 1.0567333698272705 	 0.3966176509857178 	 0.35997581481933594 	 
2025-07-24 15:52:06.250861 test begin: paddle.reduce_as(Tensor([30, 1270081, 40],"float32"), Tensor([1270081, 40],"float32"), )
[Prof] paddle.reduce_as 	 paddle.reduce_as(Tensor([30, 1270081, 40],"float32"), Tensor([1270081, 40],"float32"), ) 	 1574900440 	 1000 	 5.2036542892456055 	 5.344102144241333 	 5.184798240661621 	 1.3654117584228516 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 15:52:53.297882 test begin: paddle.reduce_as(Tensor([30, 200, 254017],"float32"), Tensor([200, 254017],"float32"), )
[Prof] paddle.reduce_as 	 paddle.reduce_as(Tensor([30, 200, 254017],"float32"), Tensor([200, 254017],"float32"), ) 	 1574905400 	 1000 	 5.212074041366577 	 5.379957914352417 	 5.19277286529541 	 1.3988535404205322 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 15:53:40.817698 test begin: paddle.reduce_as(Tensor([30, 200, 8468],"float32"), Tensor([200, 8468],"float32"), )
[Prof] paddle.reduce_as 	 paddle.reduce_as(Tensor([30, 200, 8468],"float32"), Tensor([200, 8468],"float32"), ) 	 52501600 	 1000 	 0.1778559684753418 	 0.15557265281677246 	 0.1670081615447998 	 0.12148785591125488 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 15:53:42.320521 test begin: paddle.reduce_as(Tensor([30, 42337, 40],"float32"), Tensor([42337, 40],"float32"), )
[Prof] paddle.reduce_as 	 paddle.reduce_as(Tensor([30, 42337, 40],"float32"), Tensor([42337, 40],"float32"), ) 	 52497880 	 1000 	 0.1786975860595703 	 0.15623974800109863 	 0.16734004020690918 	 0.12150955200195312 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 15:53:43.750589 test begin: paddle.reduce_as(Tensor([6351, 200, 40],"float32"), Tensor([200, 40],"float32"), )
[Prof] paddle.reduce_as 	 paddle.reduce_as(Tensor([6351, 200, 40],"float32"), Tensor([200, 40],"float32"), ) 	 50816000 	 1000 	 0.2597615718841553 	 0.15568995475769043 	 0.1327521800994873 	 0.07953643798828125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 15:53:45.221010 test begin: paddle.remainder(Tensor([1, 2, 1270081, 4, 5],"float32"), Tensor([1, 2, 1270081, 4, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 1270081, 4, 5],"float32"), Tensor([1, 2, 1270081, 4, 5],"float32"), ) 	 101606480 	 1000 	 0.44991636276245117 	 0.44938158988952637 	 0.4402954578399658 	 0.4377913475036621 	 None 	 None 	 None 	 None 	 
2025-07-24 15:53:47.872928 test begin: paddle.remainder(Tensor([1, 2, 1270081, 4, 5],"int32"), Tensor([1, 2, 1270081, 4, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 1270081, 4, 5],"int32"), Tensor([1, 2, 1270081, 4, 5],"int32"), ) 	 101606480 	 1000 	 0.4500918388366699 	 0.4516432285308838 	 0.4400935173034668 	 0.4377148151397705 	 None 	 None 	 None 	 None 	 
2025-07-24 15:53:49.926070 test begin: paddle.remainder(Tensor([1, 2, 3, 1693441, 5],"float32"), Tensor([1, 2, 3, 1693441, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 1693441, 5],"float32"), Tensor([1, 2, 3, 1693441, 5],"float32"), ) 	 101606460 	 1000 	 0.45001673698425293 	 0.4493436813354492 	 0.4406924247741699 	 0.4382286071777344 	 None 	 None 	 None 	 None 	 
2025-07-24 15:53:52.539104 test begin: paddle.remainder(Tensor([1, 2, 3, 1693441, 5],"int32"), Tensor([1, 2, 3, 1693441, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 1693441, 5],"int32"), Tensor([1, 2, 3, 1693441, 5],"int32"), ) 	 101606460 	 1000 	 0.4500575065612793 	 0.45023131370544434 	 0.44045138359069824 	 0.4317183494567871 	 None 	 None 	 None 	 None 	 
2025-07-24 15:53:54.636747 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 1058401],"float64"), Tensor([1, 2, 3, 4, 1058401],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 1058401],"float64"), Tensor([1, 2, 3, 4, 1058401],"float64"), ) 	 50803248 	 1000 	 0.4470691680908203 	 0.4566826820373535 	 0.437943696975708 	 0.4456186294555664 	 None 	 None 	 None 	 None 	 
2025-07-24 15:53:56.598262 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 2116801],"float32"), Tensor([1, 2, 3, 4, 2116801],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 2116801],"float32"), Tensor([1, 2, 3, 4, 2116801],"float32"), ) 	 101606448 	 1000 	 0.44991445541381836 	 0.44939732551574707 	 0.43326449394226074 	 0.4319641590118408 	 None 	 None 	 None 	 None 	 
2025-07-24 15:53:59.298417 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 2116801],"int32"), Tensor([1, 2, 3, 4, 2116801],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 2116801],"int32"), Tensor([1, 2, 3, 4, 2116801],"int32"), ) 	 101606448 	 1000 	 0.45004796981811523 	 0.44976210594177246 	 0.4331338405609131 	 0.4323115348815918 	 None 	 None 	 None 	 None 	 
2025-07-24 15:54:01.434747 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 5],"float32"), Tensor([423361, 2, 3, 4, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 5],"float32"), Tensor([423361, 2, 3, 4, 5],"float32"), ) 	 50803440 	 1000 	 0.29741978645324707 	 0.339491605758667 	 0.28710198402404785 	 0.31884098052978516 	 None 	 None 	 None 	 None 	 
2025-07-24 15:54:02.894987 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 5],"float64"), Tensor([211681, 2, 3, 4, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 5],"float64"), Tensor([211681, 2, 3, 4, 5],"float64"), ) 	 25401840 	 1000 	 0.4373764991760254 	 0.37322020530700684 	 0.4273073673248291 	 0.3587636947631836 	 None 	 None 	 None 	 None 	 
2025-07-24 15:54:04.235974 test begin: paddle.remainder(Tensor([1, 2, 3, 4, 5],"int32"), Tensor([423361, 2, 3, 4, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 4, 5],"int32"), Tensor([423361, 2, 3, 4, 5],"int32"), ) 	 50803440 	 1000 	 0.3000218868255615 	 0.3432581424713135 	 0.2893819808959961 	 0.3294548988342285 	 None 	 None 	 None 	 None 	 
2025-07-24 15:54:05.480924 test begin: paddle.remainder(Tensor([1, 2, 3, 846721, 5],"float64"), Tensor([1, 2, 3, 846721, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 3, 846721, 5],"float64"), Tensor([1, 2, 3, 846721, 5],"float64"), ) 	 50803260 	 1000 	 0.4470856189727783 	 0.45676302909851074 	 0.4341459274291992 	 0.4457235336303711 	 None 	 None 	 None 	 None 	 
2025-07-24 15:54:07.453954 test begin: paddle.remainder(Tensor([1, 2, 635041, 4, 5],"float64"), Tensor([1, 2, 635041, 4, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 2, 635041, 4, 5],"float64"), Tensor([1, 2, 635041, 4, 5],"float64"), ) 	 50803280 	 1000 	 0.4470357894897461 	 0.45778703689575195 	 0.43776655197143555 	 0.44524645805358887 	 None 	 None 	 None 	 None 	 
2025-07-24 15:54:09.465969 test begin: paddle.remainder(Tensor([1, 423361, 3, 4, 5],"float64"), Tensor([1, 423361, 3, 4, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 423361, 3, 4, 5],"float64"), Tensor([1, 423361, 3, 4, 5],"float64"), ) 	 50803320 	 1000 	 0.4470076560974121 	 0.45662593841552734 	 0.4377412796020508 	 0.4454505443572998 	 None 	 None 	 None 	 None 	 
2025-07-24 15:54:11.472556 test begin: paddle.remainder(Tensor([1, 846721, 3, 4, 5],"float32"), Tensor([1, 846721, 3, 4, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 846721, 3, 4, 5],"float32"), Tensor([1, 846721, 3, 4, 5],"float32"), ) 	 101606520 	 1000 	 0.4498765468597412 	 0.4550294876098633 	 0.44061899185180664 	 0.4381105899810791 	 None 	 None 	 None 	 None 	 
2025-07-24 15:54:14.482416 test begin: paddle.remainder(Tensor([1, 846721, 3, 4, 5],"int32"), Tensor([1, 846721, 3, 4, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([1, 846721, 3, 4, 5],"int32"), Tensor([1, 846721, 3, 4, 5],"int32"), ) 	 101606520 	 1000 	 0.44995856285095215 	 0.4504702091217041 	 0.4402916431427002 	 0.43465352058410645 	 None 	 None 	 None 	 None 	 
2025-07-24 15:54:17.600608 test begin: paddle.remainder(Tensor([211681, 2, 3, 4, 5],"float64"), Tensor([1, 2, 3, 4, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([211681, 2, 3, 4, 5],"float64"), Tensor([1, 2, 3, 4, 5],"float64"), ) 	 25401840 	 1000 	 0.4259610176086426 	 0.38672757148742676 	 0.40775322914123535 	 0.36786365509033203 	 None 	 None 	 None 	 None 	 
2025-07-24 15:54:18.928840 test begin: paddle.remainder(Tensor([211681, 2, 3, 4, 5],"float64"), Tensor([211681, 2, 3, 4, 5],"float64"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([211681, 2, 3, 4, 5],"float64"), Tensor([211681, 2, 3, 4, 5],"float64"), ) 	 50803440 	 1000 	 0.44730687141418457 	 0.45671558380126953 	 0.4381237030029297 	 0.4454185962677002 	 None 	 None 	 None 	 None 	 
2025-07-24 15:54:20.905024 test begin: paddle.remainder(Tensor([423361, 2, 3, 4, 5],"float32"), Tensor([1, 2, 3, 4, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([423361, 2, 3, 4, 5],"float32"), Tensor([1, 2, 3, 4, 5],"float32"), ) 	 50803440 	 1000 	 0.29657793045043945 	 0.33180689811706543 	 0.2863600254058838 	 0.3198719024658203 	 None 	 None 	 None 	 None 	 
2025-07-24 15:54:22.358003 test begin: paddle.remainder(Tensor([423361, 2, 3, 4, 5],"float32"), Tensor([423361, 2, 3, 4, 5],"float32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([423361, 2, 3, 4, 5],"float32"), Tensor([423361, 2, 3, 4, 5],"float32"), ) 	 101606640 	 1000 	 0.4499022960662842 	 0.44927549362182617 	 0.4402456283569336 	 0.43814921379089355 	 None 	 None 	 None 	 None 	 
2025-07-24 15:54:24.935355 test begin: paddle.remainder(Tensor([423361, 2, 3, 4, 5],"int32"), Tensor([1, 2, 3, 4, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([423361, 2, 3, 4, 5],"int32"), Tensor([1, 2, 3, 4, 5],"int32"), ) 	 50803440 	 1000 	 0.29793334007263184 	 0.33814048767089844 	 0.287259578704834 	 0.32628440856933594 	 None 	 None 	 None 	 None 	 
2025-07-24 15:54:26.175246 test begin: paddle.remainder(Tensor([423361, 2, 3, 4, 5],"int32"), Tensor([423361, 2, 3, 4, 5],"int32"), )
[Prof] paddle.remainder 	 paddle.remainder(Tensor([423361, 2, 3, 4, 5],"int32"), Tensor([423361, 2, 3, 4, 5],"int32"), ) 	 101606640 	 1000 	 0.449932336807251 	 0.44962382316589355 	 0.4403035640716553 	 0.43871569633483887 	 None 	 None 	 None 	 None 	 
2025-07-24 15:54:28.241303 test begin: paddle.renorm(Tensor([10, 20, 254017],"float32"), 1.0, -1, 2.05, )
[Prof] paddle.renorm 	 paddle.renorm(Tensor([10, 20, 254017],"float32"), 1.0, -1, 2.05, ) 	 50803400 	 1000 	 2.8533196449279785 	 0.48992156982421875 	 0.7280890941619873 	 0.16393351554870605 	 5.56095552444458 	 2.9179811477661133 	 1.420893907546997 	 0.22910284996032715 	 
2025-07-24 15:54:43.105932 test begin: paddle.repeat_interleave(Tensor([1, 1500, 33869],"float32"), 5, axis=0, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([1, 1500, 33869],"float32"), 5, axis=0, ) 	 50803500 	 1000 	 1.8599729537963867 	 1.5085115432739258 	 0.950007438659668 	 1.477081298828125 	 2.414264678955078 	 0.8744657039642334 	 0.8230297565460205 	 0.7895956039428711 	 
2025-07-24 15:54:54.826315 test begin: paddle.repeat_interleave(Tensor([1, 39691, 1280],"float32"), 5, axis=0, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([1, 39691, 1280],"float32"), 5, axis=0, ) 	 50804480 	 1000 	 1.865692138671875 	 1.4927823543548584 	 0.9529609680175781 	 1.4613566398620605 	 2.4129414558410645 	 0.8627538681030273 	 0.8225698471069336 	 0.7734920978546143 	 
2025-07-24 15:55:06.412443 test begin: paddle.repeat_interleave(Tensor([14, 1, 384, 9451],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([14, 1, 384, 9451],"float32"), repeats=3, axis=1, ) 	 50808576 	 1000 	 1.103316307067871 	 0.8532493114471436 	 0.5636217594146729 	 0.8250648975372314 	 1.2298061847686768 	 0.586695671081543 	 0.41915392875671387 	 0.5022964477539062 	 
2025-07-24 15:55:13.500162 test begin: paddle.repeat_interleave(Tensor([14, 1, 9451, 384],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([14, 1, 9451, 384],"float32"), repeats=3, axis=1, ) 	 50808576 	 1000 	 1.103870153427124 	 0.8551700115203857 	 0.5636398792266846 	 0.8226861953735352 	 1.2290565967559814 	 0.5867660045623779 	 0.4188830852508545 	 0.4769895076751709 	 
2025-07-24 15:55:21.898592 test begin: paddle.repeat_interleave(Tensor([14, 25, 384, 384],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([14, 25, 384, 384],"float32"), repeats=3, axis=1, ) 	 51609600 	 1000 	 1.0464766025543213 	 0.7306373119354248 	 0.5345642566680908 	 0.6987228393554688 	 1.2043774127960205 	 0.602766752243042 	 0.4102783203125 	 0.5110540390014648 	 
2025-07-24 15:55:28.862521 test begin: paddle.repeat_interleave(Tensor([27, 1500, 1280],"float32"), 5, axis=0, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([27, 1500, 1280],"float32"), 5, axis=0, ) 	 51840000 	 1000 	 1.6944377422332764 	 1.1919679641723633 	 0.8656656742095947 	 1.1056132316589355 	 1.8765478134155273 	 0.8807454109191895 	 0.6395785808563232 	 0.7997407913208008 	 
2025-07-24 15:55:40.212936 test begin: paddle.repeat_interleave(Tensor([345, 1, 384, 384],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([345, 1, 384, 384],"float32"), repeats=3, axis=1, ) 	 50872320 	 1000 	 1.031214714050293 	 0.720381498336792 	 0.5267727375030518 	 0.6974239349365234 	 1.1969022750854492 	 0.5940611362457275 	 0.40776634216308594 	 0.5113890171051025 	 
2025-07-24 15:55:47.082314 test begin: paddle.repeat_interleave(Tensor([5, 1, 13231, 768],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([5, 1, 13231, 768],"float32"), repeats=3, axis=1, ) 	 50807040 	 1000 	 1.1226327419281006 	 0.9039976596832275 	 0.5732667446136475 	 0.8811025619506836 	 1.5039994716644287 	 0.5869202613830566 	 0.5126950740814209 	 0.5017423629760742 	 
2025-07-24 15:55:54.568673 test begin: paddle.repeat_interleave(Tensor([5, 1, 768, 13231],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([5, 1, 768, 13231],"float32"), repeats=3, axis=1, ) 	 50807040 	 1000 	 1.1226658821105957 	 0.9037272930145264 	 0.5732567310333252 	 0.8812437057495117 	 1.5046193599700928 	 0.5869059562683105 	 0.5128955841064453 	 0.5054700374603271 	 
2025-07-24 15:56:02.086489 test begin: paddle.repeat_interleave(Tensor([5, 18, 768, 768],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([5, 18, 768, 768],"float32"), repeats=3, axis=1, ) 	 53084160 	 1000 	 1.0561425685882568 	 0.7032997608184814 	 0.5394623279571533 	 0.6808938980102539 	 1.2542600631713867 	 0.6252174377441406 	 0.42743778228759766 	 0.5431158542633057 	 
2025-07-24 15:56:09.208325 test begin: paddle.repeat_interleave(Tensor([87, 1, 768, 768],"float32"), repeats=3, axis=1, )
[Prof] paddle.repeat_interleave 	 paddle.repeat_interleave(Tensor([87, 1, 768, 768],"float32"), repeats=3, axis=1, ) 	 51314688 	 1000 	 1.0207831859588623 	 0.6804697513580322 	 0.521430253982544 	 0.6579594612121582 	 1.2135984897613525 	 0.604745626449585 	 0.41360902786254883 	 0.5232999324798584 	 
2025-07-24 15:56:16.085042 test begin: paddle.reshape(Tensor([14176, 7168],"bfloat16"), list[-1,7168,], )
[Prof] paddle.reshape 	 paddle.reshape(Tensor([14176, 7168],"bfloat16"), list[-1,7168,], ) 	 101613568 	 1000 	 0.00542140007019043 	 0.004253387451171875 	 1.2874603271484375e-05 	 2.09808349609375e-05 	 0.04395174980163574 	 0.4536113739013672 	 2.193450927734375e-05 	 0.3805668354034424 	 
2025-07-24 15:56:19.946508 test begin: paddle.reverse(Tensor([12, 132301, 16],"float64"), axis=list[0,], )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([12, 132301, 16],"float64"), axis=list[0,], ) 	 25401792 	 1000 	 0.5195579528808594 	 0.3053872585296631 	 0.5104539394378662 	 0.29050683975219727 	 0.5190985202789307 	 0.30377769470214844 	 0.4701807498931885 	 0.2366957664489746 	 
2025-07-24 15:56:23.814150 test begin: paddle.reverse(Tensor([12, 264601, 8],"float64"), axis=0, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([12, 264601, 8],"float64"), axis=0, ) 	 25401696 	 1000 	 0.5199379920959473 	 0.30401158332824707 	 0.5109724998474121 	 0.29070234298706055 	 0.5195322036743164 	 0.30377912521362305 	 0.47129273414611816 	 0.24267983436584473 	 
2025-07-24 15:56:27.390909 test begin: paddle.reverse(Tensor([12, 4, 529201],"float64"), axis=0, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([12, 4, 529201],"float64"), axis=0, ) 	 25401648 	 1000 	 0.5189707279205322 	 0.30383849143981934 	 0.5099046230316162 	 0.2906930446624756 	 0.5203373432159424 	 0.3037717342376709 	 0.46804046630859375 	 0.24269914627075195 	 
2025-07-24 15:56:30.090700 test begin: paddle.reverse(Tensor([12, 4, 529201],"float64"), axis=list[0,], )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([12, 4, 529201],"float64"), axis=list[0,], ) 	 25401648 	 1000 	 0.51889967918396 	 0.3038594722747803 	 0.5099043846130371 	 0.2906649112701416 	 0.5202691555023193 	 0.30377960205078125 	 0.4715440273284912 	 0.24208545684814453 	 
2025-07-24 15:56:32.816531 test begin: paddle.reverse(Tensor([396901, 4, 16],"float64"), axis=list[0,], )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([396901, 4, 16],"float64"), axis=list[0,], ) 	 25401664 	 1000 	 0.5135345458984375 	 0.30803346633911133 	 0.5046284198760986 	 0.28911590576171875 	 0.5150101184844971 	 0.3025650978088379 	 0.46655917167663574 	 0.24114990234375 	 
2025-07-24 15:56:36.957572 test begin: paddle.reverse(Tensor([4, 12, 529201],"float64"), axis=1, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([4, 12, 529201],"float64"), axis=1, ) 	 25401648 	 1000 	 0.5186600685119629 	 0.3119516372680664 	 0.5095751285552979 	 0.29172396659851074 	 0.5198380947113037 	 0.30559539794921875 	 0.4713459014892578 	 0.22362232208251953 	 
2025-07-24 15:56:39.648228 test begin: paddle.reverse(Tensor([4, 198451, 32],"float64"), axis=1, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([4, 198451, 32],"float64"), axis=1, ) 	 25401728 	 1000 	 0.5147442817687988 	 0.303206205368042 	 0.5058395862579346 	 0.29003477096557617 	 0.5139174461364746 	 0.3032248020172119 	 0.46544575691223145 	 0.24203181266784668 	 
2025-07-24 15:56:42.331693 test begin: paddle.reverse(Tensor([66151, 12, 32],"float64"), axis=1, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([66151, 12, 32],"float64"), axis=1, ) 	 25401984 	 1000 	 0.5136525630950928 	 0.3032650947570801 	 0.5041897296905518 	 0.29019880294799805 	 0.5140700340270996 	 0.30330562591552734 	 0.465651273727417 	 0.24185824394226074 	 
2025-07-24 15:56:45.050267 test begin: paddle.reverse(Tensor([793801, 4, 8],"float64"), axis=0, )
[Prof] paddle.reverse 	 paddle.reverse(Tensor([793801, 4, 8],"float64"), axis=0, ) 	 25401632 	 1000 	 0.5145797729492188 	 0.3028538227081299 	 0.5055680274963379 	 0.2895545959472656 	 0.5153322219848633 	 0.3027493953704834 	 0.46514368057250977 	 0.2411332130432129 	 
2025-07-24 15:56:47.799445 test begin: paddle.roll(Tensor([128, 37, 56, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 37, 56, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 	 50921472 	 1000 	 0.5458076000213623 	 0.7863895893096924 	 0.5336093902587891 	 0.40193819999694824 	 0.5457797050476074 	 0.7836694717407227 	 0.4944274425506592 	 0.40040087699890137 	 
2025-07-24 15:56:52.191266 test begin: paddle.roll(Tensor([128, 37, 56, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 37, 56, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 	 50921472 	 1000 	 0.5458378791809082 	 0.783698320388794 	 0.5343151092529297 	 0.40044331550598145 	 0.5460503101348877 	 0.7858724594116211 	 0.4948725700378418 	 0.4015312194824219 	 
2025-07-24 15:56:56.583336 test begin: paddle.roll(Tensor([128, 56, 37, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 56, 37, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 	 50921472 	 1000 	 0.5456993579864502 	 0.7885923385620117 	 0.5341334342956543 	 0.4029428958892822 	 0.5455644130706787 	 0.7866339683532715 	 0.4937102794647217 	 0.4019291400909424 	 
2025-07-24 15:57:00.940552 test begin: paddle.roll(Tensor([128, 56, 37, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 56, 37, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 	 50921472 	 1000 	 0.5454142093658447 	 0.7866091728210449 	 0.5339269638061523 	 0.4019477367401123 	 0.5454967021942139 	 0.7886536121368408 	 0.49419260025024414 	 0.40289950370788574 	 
2025-07-24 15:57:05.309076 test begin: paddle.roll(Tensor([128, 56, 56, 127],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 56, 56, 127],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 	 50978816 	 1000 	 0.5465354919433594 	 0.7883455753326416 	 0.535069465637207 	 0.4028310775756836 	 0.54665207862854 	 0.7860136032104492 	 0.4931964874267578 	 0.4015934467315674 	 
2025-07-24 15:57:09.635012 test begin: paddle.roll(Tensor([128, 56, 56, 127],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([128, 56, 56, 127],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 	 50978816 	 1000 	 0.546318769454956 	 0.7857608795166016 	 0.5347352027893066 	 0.40151333808898926 	 0.5465054512023926 	 0.7882251739501953 	 0.4936709403991699 	 0.4027097225189209 	 
2025-07-24 15:57:14.019512 test begin: paddle.roll(Tensor([44, 96, 96, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([44, 96, 96, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 	 51904512 	 1000 	 0.5562546253204346 	 0.7972402572631836 	 0.5365386009216309 	 0.4073672294616699 	 0.5563788414001465 	 0.7947027683258057 	 0.4939424991607666 	 0.40604662895202637 	 
2025-07-24 15:57:18.470738 test begin: paddle.roll(Tensor([64, 65, 96, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([64, 65, 96, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 	 51118080 	 1000 	 0.5479469299316406 	 0.7853398323059082 	 0.5362503528594971 	 0.40129590034484863 	 0.5480139255523682 	 0.7829310894012451 	 0.49611496925354004 	 0.4000396728515625 	 
2025-07-24 15:57:22.870280 test begin: paddle.roll(Tensor([64, 96, 65, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([64, 96, 65, 128],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 	 51118080 	 1000 	 0.5480005741119385 	 0.7865278720855713 	 0.5363366603851318 	 0.40193676948547363 	 0.547980785369873 	 0.7839457988739014 	 0.49637818336486816 	 0.4005308151245117 	 
2025-07-24 15:57:29.002928 test begin: paddle.roll(Tensor([64, 96, 96, 87],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([64, 96, 96, 87],"float32"), shifts=tuple(-6,-6,), axis=tuple(1,2,), ) 	 51314688 	 1000 	 0.550262451171875 	 0.7941324710845947 	 0.5386509895324707 	 0.404498815536499 	 0.550044059753418 	 0.7900028228759766 	 0.4982178211212158 	 0.4043550491333008 	 
2025-07-24 15:57:33.433659 test begin: paddle.roll(Tensor([85, 56, 56, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([85, 56, 56, 192],"float32"), shifts=tuple(-3,-3,), axis=tuple(1,2,), ) 	 51179520 	 1000 	 0.5487534999847412 	 0.7964177131652832 	 0.537348747253418 	 0.40346431732177734 	 0.5489895343780518 	 0.7870137691497803 	 0.49781322479248047 	 0.402101993560791 	 
2025-07-24 15:57:39.465118 test begin: paddle.roll(Tensor([85, 56, 56, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), )
[Prof] paddle.roll 	 paddle.roll(Tensor([85, 56, 56, 192],"float32"), shifts=tuple(3,3,), axis=tuple(1,2,), ) 	 51179520 	 1000 	 0.548792839050293 	 0.7868752479553223 	 0.5370690822601318 	 0.4020822048187256 	 0.5489199161529541 	 0.7890574932098389 	 0.4976041316986084 	 0.4031403064727783 	 
2025-07-24 15:57:43.923444 test begin: paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), ) 	 25401664 	 1000 	 0.5270063877105713 	 0.3034060001373291 	 0.49135589599609375 	 0.2808351516723633 	 0.8385584354400635 	 0.3037548065185547 	 0.42841553688049316 	 0.22812104225158691 	 
2025-07-24 15:57:47.008412 test begin: paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 1000 	 0.8324723243713379 	 0.30340123176574707 	 0.4253547191619873 	 0.27913498878479004 	 0.5275721549987793 	 0.3033461570739746 	 0.4602320194244385 	 0.23088884353637695 	 
2025-07-24 15:57:50.115115 test begin: paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([396901, 4, 4, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 1000 	 0.8278264999389648 	 0.3033597469329834 	 0.42284202575683594 	 0.28704094886779785 	 0.527562141418457 	 0.30298876762390137 	 0.47263050079345703 	 0.23793244361877441 	 
2025-07-24 15:57:53.175669 test begin: paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), ) 	 25401664 	 1000 	 0.529456377029419 	 0.3041355609893799 	 0.5054154396057129 	 0.28878068923950195 	 0.8414497375488281 	 0.3025517463684082 	 0.42995643615722656 	 0.23828983306884766 	 
2025-07-24 15:57:56.277875 test begin: paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 1000 	 0.9649233818054199 	 0.3041262626647949 	 0.49306464195251465 	 0.2879295349121094 	 0.5319275856018066 	 0.30426692962646484 	 0.4765806198120117 	 0.237654447555542 	 
2025-07-24 15:57:59.447111 test begin: paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 396901, 4, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 1000 	 0.8276500701904297 	 0.303358793258667 	 0.4228055477142334 	 0.2872028350830078 	 0.5273897647857666 	 0.30300402641296387 	 0.4684274196624756 	 0.23887419700622559 	 
2025-07-24 15:58:02.495293 test begin: paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), ) 	 25401664 	 1000 	 0.530919075012207 	 0.3065609931945801 	 0.5073721408843994 	 0.28887057304382324 	 0.8409266471862793 	 0.30375218391418457 	 0.4296684265136719 	 0.23974156379699707 	 
2025-07-24 15:58:05.598131 test begin: paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 1000 	 0.8443949222564697 	 0.30422377586364746 	 0.4314894676208496 	 0.28803563117980957 	 0.5275604724884033 	 0.3033461570739746 	 0.4675412178039551 	 0.2373979091644287 	 
2025-07-24 15:58:08.656490 test begin: paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 396901, 4],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 1000 	 0.8320558071136475 	 0.30420732498168945 	 0.42519521713256836 	 0.2880432605743408 	 0.5314013957977295 	 0.3056201934814453 	 0.4755725860595703 	 0.2408463954925537 	 
2025-07-24 15:58:11.702786 test begin: paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), ) 	 25401664 	 1000 	 0.5310392379760742 	 0.3042564392089844 	 0.5079154968261719 	 0.2879006862640381 	 0.8410170078277588 	 0.30376386642456055 	 0.42978334426879883 	 0.239060640335083 	 
2025-07-24 15:58:14.799445 test begin: paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=list[1,2,], )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=list[1,2,], ) 	 25401664 	 1000 	 0.8416397571563721 	 0.3042562007904053 	 0.43010449409484863 	 0.2877993583679199 	 0.5316972732543945 	 0.30620431900024414 	 0.4770982265472412 	 0.24080753326416016 	 
2025-07-24 15:58:17.873729 test begin: paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=tuple(2,3,), )
[Prof] paddle.rot90 	 paddle.rot90(x=Tensor([4, 4, 4, 396901],"float64"), k=-1, axes=tuple(2,3,), ) 	 25401664 	 1000 	 0.8230915069580078 	 0.30617642402648926 	 0.4204230308532715 	 0.28734564781188965 	 0.5271544456481934 	 0.30295419692993164 	 0.46851539611816406 	 0.23793530464172363 	 
2025-07-24 15:58:20.913910 test begin: paddle.round(Tensor([128, 396901],"float32"), )
[Prof] paddle.round 	 paddle.round(Tensor([128, 396901],"float32"), ) 	 50803328 	 1000 	 0.295668363571167 	 0.2977888584136963 	 0.28715014457702637 	 0.2873826026916504 	 0.13413071632385254 	 0.1340632438659668 	 0.08588337898254395 	 0.0732276439666748 	 
2025-07-24 15:58:23.478697 test begin: paddle.round(Tensor([16, 1587601],"float64"), )
[Prof] paddle.round 	 paddle.round(Tensor([16, 1587601],"float64"), ) 	 25401616 	 1000 	 0.30486416816711426 	 0.2983286380767822 	 0.29648590087890625 	 0.28795671463012695 	 0.13394427299499512 	 0.13444900512695312 	 0.08491730690002441 	 0.07352828979492188 	 
2025-07-24 15:58:25.432341 test begin: paddle.round(Tensor([396901, 128],"float32"), )
[Prof] paddle.round 	 paddle.round(Tensor([396901, 128],"float32"), ) 	 50803328 	 1000 	 0.29567408561706543 	 0.29783177375793457 	 0.2872180938720703 	 0.28725314140319824 	 0.13408160209655762 	 0.13407206535339355 	 0.08599686622619629 	 0.05704021453857422 	 
2025-07-24 15:58:27.962316 test begin: paddle.round(Tensor([99226, 256],"float64"), )
[Prof] paddle.round 	 paddle.round(Tensor([99226, 256],"float64"), ) 	 25401856 	 1000 	 0.3042452335357666 	 0.29839205741882324 	 0.2958190441131592 	 0.28787684440612793 	 0.1339278221130371 	 0.13448500633239746 	 0.08407092094421387 	 0.0712890625 	 
2025-07-24 15:58:29.931415 test begin: paddle.round(x=Tensor([3, 3, 5644801],"float32"), )
[Prof] paddle.round 	 paddle.round(x=Tensor([3, 3, 5644801],"float32"), ) 	 50803209 	 1000 	 0.2954866886138916 	 0.3053781986236572 	 0.28670573234558105 	 0.28716588020324707 	 0.13409972190856934 	 0.13404178619384766 	 0.08560347557067871 	 0.07216644287109375 	 
2025-07-24 15:58:34.010668 test begin: paddle.round(x=Tensor([3, 5644801, 3],"float32"), )
[Prof] paddle.round 	 paddle.round(x=Tensor([3, 5644801, 3],"float32"), ) 	 50803209 	 1000 	 0.2954978942871094 	 0.3063983917236328 	 0.28669023513793945 	 0.28075075149536133 	 0.13407158851623535 	 0.1343686580657959 	 0.08371877670288086 	 0.06615686416625977 	 
2025-07-24 15:58:37.146522 test begin: paddle.round(x=Tensor([5644801, 3, 3],"float32"), )
[Prof] paddle.round 	 paddle.round(x=Tensor([5644801, 3, 3],"float32"), ) 	 50803209 	 1000 	 0.29546475410461426 	 0.9810028076171875 	 0.2865757942199707 	 0.2872583866119385 	 0.1341104507446289 	 0.13416838645935059 	 0.08594679832458496 	 0.07402205467224121 	 
2025-07-24 15:58:40.568389 test begin: paddle.row_stack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], ) 	 76204872 	 1000 	 0.9544446468353271 	 0.9220783710479736 	 0.16255736351013184 	 0.9074461460113525 	 0.9376280307769775 	 0.0707554817199707 	 0.15966415405273438 	 7.677078247070312e-05 	 
2025-07-24 15:58:46.752298 test begin: paddle.row_stack(list[Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2, 1058401],"float64"),], ) 	 25401624 	 1000 	 0.31578922271728516 	 0.3130519390106201 	 0.16134405136108398 	 0.15984845161437988 	 0.31543397903442383 	 0.052496910095214844 	 0.16112947463989258 	 4.363059997558594e-05 	 
2025-07-24 15:58:48.824083 test begin: paddle.row_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401880 	 1000 	 0.3208622932434082 	 0.32335495948791504 	 0.08212494850158691 	 0.29178857803344727 	 0.3159148693084717 	 0.09544920921325684 	 0.08083820343017578 	 7.295608520507812e-05 	 
2025-07-24 15:58:50.974817 test begin: paddle.row_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401880 	 1000 	 0.3252999782562256 	 0.32286787033081055 	 0.08294415473937988 	 0.3056497573852539 	 0.3154621124267578 	 0.08193588256835938 	 0.08038592338562012 	 4.57763671875e-05 	 
2025-07-24 15:58:53.104508 test begin: paddle.row_stack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),Tensor([3, 4, 2116801],"float64"),], ) 	 76204836 	 1000 	 0.9577929973602295 	 0.9147439002990723 	 0.1631615161895752 	 0.9001634120941162 	 0.9540951251983643 	 0.08128046989440918 	 0.16250038146972656 	 6.0558319091796875e-05 	 
2025-07-24 15:58:59.346487 test begin: paddle.row_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], ) 	 25401656 	 1000 	 0.3215470314025879 	 0.32187366485595703 	 0.08227872848510742 	 0.30785226821899414 	 0.32134580612182617 	 0.08002257347106934 	 0.08223462104797363 	 6.437301635742188e-05 	 
2025-07-24 15:59:01.472414 test begin: paddle.row_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401656 	 1000 	 0.3277761936187744 	 0.3188438415527344 	 0.08359789848327637 	 0.30437684059143066 	 0.3263251781463623 	 0.06628775596618652 	 0.08318662643432617 	 3.62396240234375e-05 	 
2025-07-24 15:59:03.615109 test begin: paddle.row_stack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 76204980 	 1000 	 0.9458029270172119 	 0.9184572696685791 	 0.16108489036560059 	 0.9035711288452148 	 0.9454302787780762 	 0.06884264945983887 	 0.16100859642028809 	 5.9604644775390625e-05 	 
2025-07-24 15:59:09.810438 test begin: paddle.row_stack(list[Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401660 	 1000 	 0.3145620822906494 	 0.3130488395690918 	 0.16072750091552734 	 0.15988969802856445 	 0.3129241466522217 	 0.06850862503051758 	 0.15984058380126953 	 0.00015282630920410156 	 
2025-07-24 15:59:11.930888 test begin: paddle.row_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),Tensor([3, 4233601, 2],"float64"),], ) 	 76204818 	 1000 	 0.9559462070465088 	 0.9221506118774414 	 0.1628575325012207 	 0.9052331447601318 	 0.9448747634887695 	 0.06651163101196289 	 0.160933256149292 	 3.8623809814453125e-05 	 
2025-07-24 15:59:18.099804 test begin: paddle.row_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 76204890 	 1000 	 0.9555835723876953 	 0.9356145858764648 	 0.162766695022583 	 0.913970947265625 	 0.9431829452514648 	 0.08440589904785156 	 0.1606309413909912 	 5.316734313964844e-05 	 
2025-07-24 15:59:24.299298 test begin: paddle.row_stack(list[Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401630 	 1000 	 0.3157997131347656 	 0.3149902820587158 	 0.16132879257202148 	 0.15986347198486328 	 0.3154768943786621 	 0.06107187271118164 	 0.16113972663879395 	 4.38690185546875e-05 	 
2025-07-24 15:59:26.452882 test begin: paddle.row_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),Tensor([3, 4, 2],"float64"),], ) 	 25401656 	 1000 	 0.32050037384033203 	 0.3080742359161377 	 0.08171272277832031 	 0.29407429695129395 	 0.3196732997894287 	 0.06700730323791504 	 0.08147859573364258 	 4.3392181396484375e-05 	 
2025-07-24 15:59:28.549983 test begin: paddle.row_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),Tensor([3175201, 4, 2],"float64"),], ) 	 76204824 	 1000 	 0.9551651477813721 	 0.915529727935791 	 0.16269278526306152 	 0.9010546207427979 	 0.9417781829833984 	 0.06995487213134766 	 0.16037535667419434 	 7.43865966796875e-05 	 
2025-07-24 15:59:34.695864 test begin: paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401880 	 1000 	 0.31969785690307617 	 0.32373547554016113 	 0.08152461051940918 	 0.2965669631958008 	 0.3129279613494873 	 0.08819007873535156 	 0.07975578308105469 	 4.649162292480469e-05 	 
2025-07-24 15:59:40.092690 test begin: paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 76204920 	 1000 	 0.9397578239440918 	 0.9379737377166748 	 0.16010308265686035 	 0.9059629440307617 	 0.9404776096343994 	 0.0691075325012207 	 0.16015172004699707 	 5.650520324707031e-05 	 
2025-07-24 15:59:47.129788 test begin: paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.row_stack 	 paddle.row_stack(list[Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401640 	 1000 	 0.3145425319671631 	 0.3130223751068115 	 0.16070199012756348 	 0.15985608100891113 	 0.31291627883911133 	 0.05362987518310547 	 0.15984177589416504 	 6.079673767089844e-05 	 
2025-07-24 15:59:49.251587 test begin: paddle.rsqrt(Tensor([10000, 1694, 3],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([10000, 1694, 3],"float32"), ) 	 50820000 	 1000 	 0.2958052158355713 	 0.3064415454864502 	 0.28614187240600586 	 0.28716468811035156 	 0.44944286346435547 	 1.0407042503356934 	 0.3961975574493408 	 0.35460638999938965 	 
2025-07-24 15:59:53.177173 test begin: paddle.rsqrt(Tensor([10000, 2, 1271],"float64"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([10000, 2, 1271],"float64"), ) 	 25420000 	 1000 	 0.2986299991607666 	 0.29915452003479004 	 0.29032182693481445 	 0.28833603858947754 	 0.44846463203430176 	 1.0400662422180176 	 0.3964352607727051 	 0.3543272018432617 	 
2025-07-24 15:59:56.395717 test begin: paddle.rsqrt(Tensor([10000, 2, 2541],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([10000, 2, 2541],"float32"), ) 	 50820000 	 1000 	 0.2958202362060547 	 0.2978692054748535 	 0.2870810031890869 	 0.2872016429901123 	 0.4493076801300049 	 1.0405480861663818 	 0.3966546058654785 	 0.3544929027557373 	 
2025-07-24 16:00:00.228829 test begin: paddle.rsqrt(Tensor([10000, 847, 3],"float64"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([10000, 847, 3],"float64"), ) 	 25410000 	 1000 	 0.29861974716186523 	 0.2989792823791504 	 0.28939270973205566 	 0.28818321228027344 	 0.4483146667480469 	 1.0395433902740479 	 0.3968799114227295 	 0.3541584014892578 	 
2025-07-24 16:00:03.427511 test begin: paddle.rsqrt(Tensor([13, 1007, 3881],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([13, 1007, 3881],"float32"), ) 	 50806171 	 1000 	 0.2955660820007324 	 0.29936861991882324 	 0.2867162227630615 	 0.28718018531799316 	 0.44932007789611816 	 1.0403413772583008 	 0.3971872329711914 	 0.35445618629455566 	 
2025-07-24 16:00:07.312264 test begin: paddle.rsqrt(Tensor([13, 3907939, 1],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([13, 3907939, 1],"float32"), ) 	 50803207 	 1000 	 0.29564404487609863 	 0.2978861331939697 	 0.28000950813293457 	 0.28087306022644043 	 0.4493443965911865 	 1.0401592254638672 	 0.38503384590148926 	 0.35436415672302246 	 
2025-07-24 16:00:11.164263 test begin: paddle.rsqrt(Tensor([4233601, 2, 3],"float64"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([4233601, 2, 3],"float64"), ) 	 25401606 	 1000 	 0.2984437942504883 	 0.29886889457702637 	 0.29001736640930176 	 0.2880990505218506 	 0.4480299949645996 	 1.0393383502960205 	 0.3964221477508545 	 0.35405635833740234 	 
2025-07-24 16:00:14.400363 test begin: paddle.rsqrt(Tensor([50451, 1007, 1],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([50451, 1007, 1],"float32"), ) 	 50804157 	 1000 	 0.2955660820007324 	 0.302203893661499 	 0.2850615978240967 	 0.28578996658325195 	 0.4491922855377197 	 1.040273904800415 	 0.3936607837677002 	 0.3544182777404785 	 
2025-07-24 16:00:18.271501 test begin: paddle.rsqrt(Tensor([8467201, 2, 3],"float32"), )
[Prof] paddle.rsqrt 	 paddle.rsqrt(Tensor([8467201, 2, 3],"float32"), ) 	 50803206 	 1000 	 0.29562926292419434 	 0.2978546619415283 	 0.2869126796722412 	 0.2869985103607178 	 0.44921422004699707 	 1.0403461456298828 	 0.3974339962005615 	 0.35440969467163086 	 
2025-07-24 16:00:22.086995 test begin: paddle.scale(Tensor([2, 256, 256, 388],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([2, 256, 256, 388],"float32"), scale=1.1111111111111112, ) 	 50855936 	 1000 	 0.2958230972290039 	 0.5956947803497314 	 0.28624987602233887 	 0.3043792247772217 	 0.29569244384765625 	 0.2979469299316406 	 0.24646782875061035 	 0.2319355010986328 	 combined
2025-07-24 16:00:25.242735 test begin: paddle.scale(Tensor([2, 256, 388, 256],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([2, 256, 388, 256],"float32"), scale=1.1111111111111112, ) 	 50855936 	 1000 	 0.2960054874420166 	 0.5957984924316406 	 0.2789747714996338 	 0.30443716049194336 	 0.29575324058532715 	 0.2979240417480469 	 0.2343297004699707 	 0.22397541999816895 	 combined
2025-07-24 16:00:28.403714 test begin: paddle.scale(Tensor([2, 388, 256, 256],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([2, 388, 256, 256],"float32"), scale=1.1111111111111112, ) 	 50855936 	 1000 	 0.2958688735961914 	 0.5957810878753662 	 0.28635621070861816 	 0.3044600486755371 	 0.2957496643066406 	 0.29793238639831543 	 0.2382655143737793 	 0.23122262954711914 	 combined
2025-07-24 16:00:31.577613 test begin: paddle.scale(Tensor([4, 194, 256, 256],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 194, 256, 256],"float32"), scale=1.1111111111111112, ) 	 50855936 	 1000 	 0.295853853225708 	 0.599346399307251 	 0.2789733409881592 	 0.3044569492340088 	 0.29572081565856934 	 0.2979891300201416 	 0.2370619773864746 	 0.22201251983642578 	 combined
2025-07-24 16:00:34.840565 test begin: paddle.scale(Tensor([4, 256, 194, 256],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 256, 194, 256],"float32"), scale=1.1111111111111112, ) 	 50855936 	 1000 	 0.2958858013153076 	 0.5957076549530029 	 0.2843635082244873 	 0.30437707901000977 	 0.29575681686401367 	 0.2980170249938965 	 0.2461686134338379 	 0.22357702255249023 	 combined
2025-07-24 16:00:39.837975 test begin: paddle.scale(Tensor([4, 256, 256, 194],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 256, 256, 194],"float32"), scale=1.1111111111111112, ) 	 50855936 	 1000 	 0.29584431648254395 	 0.6011261940002441 	 0.2861902713775635 	 0.3043668270111084 	 0.29573869705200195 	 0.29798197746276855 	 0.24632024765014648 	 0.22965645790100098 	 combined
2025-07-24 16:00:43.300063 test begin: paddle.scale(Tensor([4, 256, 256, 256],"float32"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 256, 256, 256],"float32"), scale=1.1111111111111112, ) 	 67108864 	 1000 	 0.38907837867736816 	 1.0189270973205566 	 0.37771153450012207 	 0.4001891613006592 	 0.38889002799987793 	 0.39147114753723145 	 0.3393828868865967 	 0.32558155059814453 	 combined
2025-07-24 16:00:52.004865 test begin: paddle.scale(Tensor([4, 256, 256, 388],"float16"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 256, 256, 388],"float16"), scale=1.1111111111111112, ) 	 101711872 	 1000 	 0.29874110221862793 	 0.5925567150115967 	 0.28176307678222656 	 0.30275607109069824 	 0.2985117435455322 	 0.2963273525238037 	 0.239426851272583 	 0.22372698783874512 	 combined
2025-07-24 16:00:57.343059 test begin: paddle.scale(Tensor([4, 256, 388, 256],"float16"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 256, 388, 256],"float16"), scale=1.1111111111111112, ) 	 101711872 	 1000 	 0.29875755310058594 	 0.5925812721252441 	 0.2817206382751465 	 0.3027801513671875 	 0.2985043525695801 	 0.2963600158691406 	 0.2392439842224121 	 0.23000144958496094 	 combined
2025-07-24 16:01:02.692841 test begin: paddle.scale(Tensor([4, 388, 256, 256],"float16"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([4, 388, 256, 256],"float16"), scale=1.1111111111111112, ) 	 101711872 	 1000 	 0.29871344566345215 	 0.6050968170166016 	 0.2816953659057617 	 0.3027796745300293 	 0.29853320121765137 	 0.2978782653808594 	 0.2378087043762207 	 0.22378277778625488 	 combined
2025-07-24 16:01:10.724993 test begin: paddle.scale(Tensor([7, 256, 256, 256],"float16"), scale=1.1111111111111112, )
[Prof] paddle.scale 	 paddle.scale(Tensor([7, 256, 256, 256],"float16"), scale=1.1111111111111112, ) 	 117440512 	 1000 	 0.3441202640533447 	 0.6828944683074951 	 0.3345465660095215 	 0.34894800186157227 	 0.3441753387451172 	 0.3414740562438965 	 0.2944943904876709 	 0.27573275566101074 	 combined
2025-07-24 16:01:16.776706 test begin: paddle.scatter(Tensor([262144, 194],"float32"), Tensor([19780],"int32"), Tensor([19780, 194],"float32"), overwrite=True, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f408c702470>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 16:11:21.664602 test begin: paddle.scatter(Tensor([262144, 194],"float32"), Tensor([20524],"int32"), Tensor([20524, 194],"float32"), overwrite=True, )
W0724 16:11:22.706130 122453 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5967042c50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 16:21:26.354566 test begin: paddle.scatter(Tensor([262144, 194],"float32"), Tensor([21955],"int32"), Tensor([21955, 194],"float32"), overwrite=True, )
W0724 16:21:27.446925 126767 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd59e8dac50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 16:31:31.742408 test begin: paddle.scatter(Tensor([262144, 2314],"float32"), Tensor([21955],"int32"), Tensor([21955, 2314],"float32"), overwrite=True, )
W0724 16:31:41.984603 131264 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f8a2914ac50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 16:41:37.168169 test begin: paddle.scatter(Tensor([262144, 2476],"float32"), Tensor([20524],"int32"), Tensor([20524, 2476],"float32"), overwrite=True, )
W0724 16:41:47.865918 135597 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f66e4e36cb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 16:51:43.368893 test begin: paddle.scatter(Tensor([262144, 2569],"float32"), Tensor([19780],"int32"), Tensor([19780, 2569],"float32"), overwrite=True, )
W0724 16:51:54.393949 139623 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd90e816c50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 17:01:48.215143 test begin: paddle.scatter(Tensor([262144, 64],"float32"), Tensor([19780],"int32"), Tensor([793801, 64],"float32"), overwrite=True, )
W0724 17:01:49.470757 144157 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc90c17ec50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 17:11:56.079815 test begin: paddle.scatter(Tensor([262144, 64],"float32"), Tensor([20524],"int32"), Tensor([793801, 64],"float32"), overwrite=True, )
W0724 17:11:57.271024 148549 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fd85de72c50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 17:22:00.899528 test begin: paddle.scatter(Tensor([262144, 64],"float32"), Tensor([21955],"int32"), Tensor([793801, 64],"float32"), overwrite=True, )
W0724 17:22:02.160019 153019 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcb2c9eaa70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 17:32:05.645848 test begin: paddle.scatter(Tensor([793801, 64],"float32"), Tensor([19780],"int32"), Tensor([19780, 64],"float32"), overwrite=True, )
W0724 17:32:06.623852 157373 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7ff6ba572a70>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 17:42:10.759618 test begin: paddle.scatter(Tensor([793801, 64],"float32"), Tensor([20524],"int32"), Tensor([20524, 64],"float32"), overwrite=True, )
W0724 17:42:13.419431  3837 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1656cbac50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 17:52:15.434012 test begin: paddle.scatter(Tensor([793801, 64],"float32"), Tensor([21955],"int32"), Tensor([21955, 64],"float32"), overwrite=True, )
W0724 17:52:16.436463 17991 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f2c004cec50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 18:02:20.308703 test begin: paddle.scatter_nd(Tensor([564481, 2],"int64"), Tensor([564481, 9, 10],"float32"), list[3,5,9,10,], )
W0724 18:02:21.237427 32807 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fcf33eeefe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 18:12:32.269906 test begin: paddle.scatter_nd_add(Tensor([1, 14176, 7168],"bfloat16"), Tensor([5859, 2],"int64"), Tensor([5859, 7168],"bfloat16"), )
W0724 18:12:34.666858 46005 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4219a52c80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 18:22:44.360105 test begin: paddle.scatter_nd_add(Tensor([1, 14176, 7168],"bfloat16"), Tensor([5876, 2],"int64"), Tensor([5876, 7168],"bfloat16"), )
W0724 18:22:46.788110 59724 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdf6dc32c80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 18:32:49.282486 test begin: paddle.scatter_nd_add(Tensor([1, 14176, 7168],"bfloat16"), Tensor([5953, 2],"int64"), Tensor([5953, 7168],"bfloat16"), )
W0724 18:32:51.619175 73812 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4cdf1fac80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 18:42:53.910605 test begin: paddle.scatter_nd_add(Tensor([1, 8192, 12404],"bfloat16"), Tensor([5876, 2],"int64"), Tensor([5876, 12404],"bfloat16"), )
W0724 18:42:56.714035 87508 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f56bceb2c20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 18:52:58.811080 test begin: paddle.scatter_nd_add(Tensor([1, 8192, 17069],"bfloat16"), Tensor([5953, 2],"int64"), Tensor([5953, 17069],"bfloat16"), )
W0724 18:53:02.628280 106145 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fbad9ba2a40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 19:03:03.685317 test begin: paddle.scatter_nd_add(Tensor([2, 8192, 7168],"bfloat16"), Tensor([5859, 2],"int64"), Tensor([5859, 7168],"bfloat16"), )
W0724 19:03:06.217203 130683 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb57fb0ea40>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 19:13:08.394920 test begin: paddle.scatter_nd_add(Tensor([2, 8192, 7168],"bfloat16"), Tensor([5876, 2],"int64"), Tensor([5876, 7168],"bfloat16"), )
W0724 19:13:10.942662 153112 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdf00206c20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 19:23:12.949070 test begin: paddle.scatter_nd_add(Tensor([2, 8192, 7168],"bfloat16"), Tensor([5953, 2],"int64"), Tensor([5953, 7168],"bfloat16"), )
W0724 19:23:15.786367 11837 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f41f4a76c20>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 19:33:17.570999 test begin: paddle.searchsorted(Tensor([1024],"float32"), Tensor([50803201],"float32"), )
W0724 19:33:18.504079 35251 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([1024],"float32"), Tensor([50803201],"float32"), ) 	 50804225 	 1000 	 1.343940258026123 	 1.0125102996826172 	 1.3357813358306885 	 1.0005171298980713 	 None 	 None 	 None 	 None 	 
2025-07-24 19:33:21.072003 test begin: paddle.searchsorted(Tensor([1024],"float64"), Tensor([25401601],"float64"), )
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([1024],"float64"), Tensor([25401601],"float64"), ) 	 25402625 	 1000 	 0.6318604946136475 	 0.4740316867828369 	 0.6240737438201904 	 0.4629392623901367 	 None 	 None 	 None 	 None 	 
2025-07-24 19:33:22.728769 test begin: paddle.searchsorted(Tensor([1024],"int32"), Tensor([50803201],"int32"), )
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([1024],"int32"), Tensor([50803201],"int32"), ) 	 50804225 	 1000 	 1.3506381511688232 	 1.0404138565063477 	 1.3402633666992188 	 1.0291101932525635 	 None 	 None 	 None 	 None 	 
2025-07-24 19:33:25.729863 test begin: paddle.searchsorted(Tensor([25401601],"float64"), Tensor([25401601],"float64"), )
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([25401601],"float64"), Tensor([25401601],"float64"), ) 	 50803202 	 1000 	 1.398637056350708 	 1.080045461654663 	 1.390796184539795 	 1.0688393115997314 	 None 	 None 	 None 	 None 	 
2025-07-24 19:33:29.341461 test begin: paddle.searchsorted(Tensor([25401601],"float64"), Tensor([512],"float64"), )
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([25401601],"float64"), Tensor([512],"float64"), ) 	 25402113 	 1000 	 0.00822305679321289 	 0.010693073272705078 	 0.0005199909210205078 	 3.314018249511719e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 19:33:29.890324 test begin: paddle.searchsorted(Tensor([50803201],"float32"), Tensor([50803201],"float32"), )
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([50803201],"float32"), Tensor([50803201],"float32"), ) 	 101606402 	 1000 	 2.968243360519409 	 2.3188626766204834 	 2.9604499340057373 	 2.307704448699951 	 None 	 None 	 None 	 None 	 
2025-07-24 19:33:37.025418 test begin: paddle.searchsorted(Tensor([50803201],"float32"), Tensor([512],"float32"), )
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([50803201],"float32"), Tensor([512],"float32"), ) 	 50803713 	 1000 	 0.00886392593383789 	 0.011431694030761719 	 0.0008840560913085938 	 3.528594970703125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 19:33:39.035429 test begin: paddle.searchsorted(Tensor([50803201],"int32"), Tensor([50803201],"int32"), )
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([50803201],"int32"), Tensor([50803201],"int32"), ) 	 101606402 	 1000 	 2.9672162532806396 	 2.3217151165008545 	 2.958763360977173 	 2.3095650672912598 	 None 	 None 	 None 	 None 	 
2025-07-24 19:33:45.528601 test begin: paddle.searchsorted(Tensor([50803201],"int32"), Tensor([512],"int32"), )
[Prof] paddle.searchsorted 	 paddle.searchsorted(Tensor([50803201],"int32"), Tensor([512],"int32"), ) 	 50803713 	 1000 	 0.008410215377807617 	 0.01085972785949707 	 0.0007467269897460938 	 4.291534423828125e-05 	 None 	 None 	 None 	 None 	 
2025-07-24 19:33:46.116483 test begin: paddle.select_scatter(Tensor([12700801, 3, 4],"float32"), Tensor([12700801, 4],"float32"), 1, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([12700801, 3, 4],"float32"), Tensor([12700801, 4],"float32"), 1, 1, ) 	 203212816 	 1000 	 0.7293448448181152 	 1.6536335945129395 	 0.7086071968078613 	 0.5619573593139648 	 3.2410943508148193 	 1.7948150634765625 	 0.4136478900909424 	 0.4583401679992676 	 
2025-07-24 19:34:00.526799 test begin: paddle.select_scatter(Tensor([1693441, 3, 4, 5],"float64"), Tensor([1693441, 3, 5],"float64"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([1693441, 3, 4, 5],"float64"), Tensor([1693441, 3, 5],"float64"), 2, 1, ) 	 127008075 	 1000 	 0.7446095943450928 	 1.9857313632965088 	 0.7236928939819336 	 0.6753349304199219 	 3.4985756874084473 	 2.133023738861084 	 0.44622206687927246 	 0.5442495346069336 	 
2025-07-24 19:34:14.362374 test begin: paddle.select_scatter(Tensor([2, 211681, 4, 5, 6],"int32"), Tensor([2, 211681, 5, 6],"int32"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 211681, 4, 5, 6],"int32"), Tensor([2, 211681, 5, 6],"int32"), 2, 1, ) 	 63504300 	 1000 	 0.17891740798950195 	 0.500420093536377 	 0.1582810878753662 	 0.16727757453918457 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 19:34:17.724329 test begin: paddle.select_scatter(Tensor([2, 2540161, 4, 5],"float64"), Tensor([2, 2540161, 5],"float64"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 2540161, 4, 5],"float64"), Tensor([2, 2540161, 5],"float64"), 2, 1, ) 	 127008050 	 1000 	 0.7443420886993408 	 1.9863338470458984 	 0.7234025001525879 	 0.6754536628723145 	 3.498155355453491 	 2.1326546669006348 	 0.447462797164917 	 0.5456421375274658 	 
2025-07-24 19:34:31.071170 test begin: paddle.select_scatter(Tensor([2, 3, 25401601],"float32"), Tensor([2, 25401601],"float32"), 1, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 3, 25401601],"float32"), Tensor([2, 25401601],"float32"), 1, 1, ) 	 203212808 	 1000 	 0.36437344551086426 	 1.2300770282745361 	 0.34024572372436523 	 0.41762828826904297 	 2.67965030670166 	 1.3640682697296143 	 0.3431069850921631 	 0.3477311134338379 	 
2025-07-24 19:34:43.082527 test begin: paddle.select_scatter(Tensor([2, 3, 282241, 5, 6],"int32"), Tensor([2, 3, 5, 6],"int32"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 3, 282241, 5, 6],"int32"), Tensor([2, 3, 5, 6],"int32"), 2, 1, ) 	 50803560 	 1000 	 0.02074122428894043 	 0.31679439544677734 	 1.239776611328125e-05 	 0.10760760307312012 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 19:34:45.022391 test begin: paddle.select_scatter(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 1058401],"float64"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 3, 4, 1058401],"float64"), Tensor([2, 3, 1058401],"float64"), 2, 1, ) 	 31752030 	 1000 	 0.07990717887878418 	 0.3926048278808594 	 0.05930280685424805 	 0.13351893424987793 	 0.7187960147857666 	 0.4310441017150879 	 0.09149384498596191 	 0.10998225212097168 	 
2025-07-24 19:34:47.921631 test begin: paddle.select_scatter(Tensor([2, 3, 4, 1411201, 6],"int32"), Tensor([2, 3, 1411201, 6],"int32"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 3, 4, 1411201, 6],"int32"), Tensor([2, 3, 1411201, 6],"int32"), 2, 1, ) 	 254016180 	 1000 	 0.5412201881408691 	 1.53438138961792 	 0.520392656326294 	 0.5214016437530518 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 19:34:58.956585 test begin: paddle.select_scatter(Tensor([2, 3, 4, 352801, 6],"int32"), Tensor([2, 3, 352801, 6],"int32"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 3, 4, 352801, 6],"int32"), Tensor([2, 3, 352801, 6],"int32"), 2, 1, ) 	 63504180 	 1000 	 0.13852787017822266 	 0.39238977432250977 	 0.11572480201721191 	 0.13339710235595703 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 19:35:01.797841 test begin: paddle.select_scatter(Tensor([2, 3, 4, 4233601],"float64"), Tensor([2, 3, 4233601],"float64"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 3, 4, 4233601],"float64"), Tensor([2, 3, 4233601],"float64"), 2, 1, ) 	 127008030 	 1000 	 0.3061823844909668 	 1.5314931869506836 	 0.27558135986328125 	 0.5205225944519043 	 2.752748966217041 	 1.667163610458374 	 0.3509495258331299 	 0.42498230934143066 	 
2025-07-24 19:35:13.972492 test begin: paddle.select_scatter(Tensor([2, 3, 4, 5, 1693441],"int32"), Tensor([2, 3, 5, 1693441],"int32"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 3, 4, 5, 1693441],"int32"), Tensor([2, 3, 5, 1693441],"int32"), 2, 1, ) 	 254016150 	 1000 	 0.5396261215209961 	 1.5344882011413574 	 0.5187299251556396 	 0.5214083194732666 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 19:35:25.065310 test begin: paddle.select_scatter(Tensor([2, 3, 4, 5, 423361],"int32"), Tensor([2, 3, 5, 423361],"int32"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 3, 4, 5, 423361],"int32"), Tensor([2, 3, 5, 423361],"int32"), 2, 1, ) 	 63504150 	 1000 	 0.13855719566345215 	 0.39241719245910645 	 0.11755156517028809 	 0.13342499732971191 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 19:35:27.927707 test begin: paddle.select_scatter(Tensor([2, 3, 8467201],"float32"), Tensor([2, 8467201],"float32"), 1, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 3, 8467201],"float32"), Tensor([2, 8467201],"float32"), 1, 1, ) 	 67737608 	 1000 	 0.12474870681762695 	 0.41727495193481445 	 0.10400938987731934 	 0.14189743995666504 	 0.9053406715393066 	 0.46540021896362305 	 0.1153421401977539 	 0.11889052391052246 	 
2025-07-24 19:35:31.810849 test begin: paddle.select_scatter(Tensor([2, 3, 846721, 5],"float64"), Tensor([2, 3, 5],"float64"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 3, 846721, 5],"float64"), Tensor([2, 3, 5],"float64"), 2, 1, ) 	 25401660 	 1000 	 0.020739316940307617 	 0.31607580184936523 	 1.7404556274414062e-05 	 0.10738754272460938 	 0.3311586380004883 	 0.31803321838378906 	 0.04186749458312988 	 0.0810546875 	 
2025-07-24 19:35:33.956383 test begin: paddle.select_scatter(Tensor([2, 6350401, 4],"float32"), Tensor([2, 4],"float32"), 1, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 6350401, 4],"float32"), Tensor([2, 4],"float32"), 1, 1, ) 	 50803216 	 1000 	 0.020807266235351562 	 0.31858015060424805 	 1.4543533325195312e-05 	 0.10746574401855469 	 0.3291006088256836 	 0.31810832023620605 	 0.04177403450012207 	 0.08105921745300293 	 
2025-07-24 19:35:38.867529 test begin: paddle.select_scatter(Tensor([2, 635041, 4, 5],"float64"), Tensor([2, 635041, 5],"float64"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 635041, 4, 5],"float64"), Tensor([2, 635041, 5],"float64"), 2, 1, ) 	 31752050 	 1000 	 0.18646478652954102 	 0.49817824363708496 	 0.16557550430297852 	 0.1695241928100586 	 0.899254560470581 	 0.5469210147857666 	 0.11463260650634766 	 0.13962268829345703 	 
2025-07-24 19:35:42.250093 test begin: paddle.select_scatter(Tensor([2, 846721, 4, 5, 6],"int32"), Tensor([2, 846721, 5, 6],"int32"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([2, 846721, 4, 5, 6],"int32"), Tensor([2, 846721, 5, 6],"int32"), 2, 1, ) 	 254016300 	 1000 	 0.7039010524749756 	 1.9812448024749756 	 0.6830964088439941 	 0.673975944519043 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 19:35:54.325804 test begin: paddle.select_scatter(Tensor([4233601, 3, 4],"float32"), Tensor([4233601, 4],"float32"), 1, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([4233601, 3, 4],"float32"), Tensor([4233601, 4],"float32"), 1, 1, ) 	 67737616 	 1000 	 0.2443099021911621 	 0.5627734661102295 	 0.22362327575683594 	 0.19032526016235352 	 1.1072747707366943 	 0.6154296398162842 	 0.14121389389038086 	 0.15715789794921875 	 
2025-07-24 19:35:58.955288 test begin: paddle.select_scatter(Tensor([423361, 3, 4, 5],"float64"), Tensor([423361, 3, 5],"float64"), 2, 1, )
[Prof] paddle.select_scatter 	 paddle.select_scatter(Tensor([423361, 3, 4, 5],"float64"), Tensor([423361, 3, 5],"float64"), 2, 1, ) 	 31752075 	 1000 	 0.18305206298828125 	 0.49790382385253906 	 0.16181015968322754 	 0.1694169044494629 	 0.8876876831054688 	 0.5481259822845459 	 0.11444354057312012 	 0.13959884643554688 	 
2025-07-24 19:36:02.393538 test begin: paddle.sgn(Tensor([12, 1058401, 2],"float64"), )
[Prof] paddle.sgn 	 paddle.sgn(Tensor([12, 1058401, 2],"float64"), ) 	 25401624 	 1000 	 0.3101973533630371 	 0.2988264560699463 	 0.29232215881347656 	 0.28758788108825684 	 0.29790210723876953 	 0.08095455169677734 	 0.24436497688293457 	 0.00016450881958007812 	 
2025-07-24 19:36:04.470270 test begin: paddle.sgn(Tensor([12, 20, 105841],"float64"), )
[Prof] paddle.sgn 	 paddle.sgn(Tensor([12, 20, 105841],"float64"), ) 	 25401840 	 1000 	 0.3117387294769287 	 0.29831528663635254 	 0.29260969161987305 	 0.28749775886535645 	 0.2979085445404053 	 0.052458763122558594 	 0.24448609352111816 	 4.482269287109375e-05 	 
2025-07-24 19:36:06.648655 test begin: paddle.sgn(Tensor([12, 20, 211681],"float32"), )
[Prof] paddle.sgn 	 paddle.sgn(Tensor([12, 20, 211681],"float32"), ) 	 50803440 	 1000 	 0.34212708473205566 	 0.3007535934448242 	 0.31513381004333496 	 0.28025007247924805 	 0.2962350845336914 	 0.07553744316101074 	 0.23778128623962402 	 0.00011801719665527344 	 
2025-07-24 19:36:09.319046 test begin: paddle.sgn(Tensor([12, 2116801, 2],"float32"), )
[Prof] paddle.sgn 	 paddle.sgn(Tensor([12, 2116801, 2],"float32"), ) 	 50803224 	 1000 	 0.34241628646850586 	 0.2977750301361084 	 0.325725793838501 	 0.28473544120788574 	 0.2961416244506836 	 0.051877498626708984 	 0.24179792404174805 	 3.790855407714844e-05 	 
2025-07-24 19:36:12.022281 test begin: paddle.sgn(Tensor([1270081, 20, 2],"float32"), )
[Prof] paddle.sgn 	 paddle.sgn(Tensor([1270081, 20, 2],"float32"), ) 	 50803240 	 1000 	 0.34227585792541504 	 0.3003058433532715 	 0.32640647888183594 	 0.2880570888519287 	 0.29613542556762695 	 0.054434776306152344 	 0.24127888679504395 	 6.103515625e-05 	 
2025-07-24 19:36:14.694317 test begin: paddle.sgn(Tensor([635041, 20, 2],"float64"), )
[Prof] paddle.sgn 	 paddle.sgn(Tensor([635041, 20, 2],"float64"), ) 	 25401640 	 1000 	 0.30866360664367676 	 0.2983577251434326 	 0.28527021408081055 	 0.2875382900238037 	 0.29779553413391113 	 0.05222320556640625 	 0.24422454833984375 	 5.2928924560546875e-05 	 
2025-07-24 19:36:16.777181 test begin: paddle.shape(Tensor([1, 1600, 376, 280],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([1, 1600, 376, 280],"float32"), ) 	 168448000 	 1000 	 0.009465217590332031 	 0.03916287422180176 	 1.3113021850585938e-05 	 4.4345855712890625e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:19.604050 test begin: paddle.shape(Tensor([13, 128, 256, 256],"float16"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([13, 128, 256, 256],"float16"), ) 	 109051904 	 1000 	 0.0044231414794921875 	 0.033347368240356445 	 1.1444091796875e-05 	 5.650520324707031e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:21.687546 test begin: paddle.shape(Tensor([4, 121, 376, 280],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([4, 121, 376, 280],"float32"), ) 	 50955520 	 1000 	 0.004354000091552734 	 0.04042696952819824 	 7.867813110351562e-06 	 9.083747863769531e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:23.662583 test begin: paddle.shape(Tensor([4, 128, 256, 388],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([4, 128, 256, 388],"float32"), ) 	 50855936 	 1000 	 0.004472255706787109 	 0.03081512451171875 	 8.58306884765625e-06 	 5.5789947509765625e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:25.932769 test begin: paddle.shape(Tensor([4, 128, 256, 776],"float16"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([4, 128, 256, 776],"float16"), ) 	 101711872 	 1000 	 0.0044002532958984375 	 0.03022599220275879 	 1.0013580322265625e-05 	 6.318092346191406e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:27.874371 test begin: paddle.shape(Tensor([4, 128, 388, 256],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([4, 128, 388, 256],"float32"), ) 	 50855936 	 1000 	 0.004704952239990234 	 0.030527830123901367 	 1.6689300537109375e-05 	 4.601478576660156e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:28.743239 test begin: paddle.shape(Tensor([4, 128, 776, 256],"float16"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([4, 128, 776, 256],"float16"), ) 	 101711872 	 1000 	 0.00653529167175293 	 0.03144574165344238 	 3.0279159545898438e-05 	 4.3392181396484375e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:30.775565 test begin: paddle.shape(Tensor([4, 1600, 29, 280],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([4, 1600, 29, 280],"float32"), ) 	 51968000 	 1000 	 0.0043849945068359375 	 0.03527688980102539 	 1.621246337890625e-05 	 5.459785461425781e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:31.689680 test begin: paddle.shape(Tensor([4, 1600, 376, 22],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([4, 1600, 376, 22],"float32"), ) 	 52940800 	 1000 	 0.004412412643432617 	 0.0305025577545166 	 1.9311904907226562e-05 	 4.1961669921875e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:32.597004 test begin: paddle.shape(Tensor([4, 194, 256, 256],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([4, 194, 256, 256],"float32"), ) 	 50855936 	 1000 	 0.005365848541259766 	 0.032967329025268555 	 2.8371810913085938e-05 	 4.57763671875e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:33.482485 test begin: paddle.shape(Tensor([4, 388, 256, 256],"float16"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([4, 388, 256, 256],"float16"), ) 	 101711872 	 1000 	 0.005789756774902344 	 0.030849695205688477 	 4.363059997558594e-05 	 5.936622619628906e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:36.892584 test begin: paddle.shape(Tensor([7, 128, 256, 256],"float32"), )
[Prof] paddle.shape 	 paddle.shape(Tensor([7, 128, 256, 256],"float32"), ) 	 58720256 	 1000 	 0.004420042037963867 	 0.03003215789794922 	 1.7642974853515625e-05 	 4.1484832763671875e-05 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:38.807704 test begin: paddle.shard_index(input=Tensor([12700801, 2, 1],"int64"), index_num=20, nshards=4, shard_id=1, )
[Prof] paddle.shard_index 	 paddle.shard_index(input=Tensor([12700801, 2, 1],"int64"), index_num=20, nshards=4, shard_id=1, ) 	 25401602 	 1000 	 0.3121209144592285 	 2.0385537147521973 	 0.30345916748046875 	 0.0006639957427978516 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:41.913433 test begin: paddle.shard_index(input=Tensor([12700801, 2, 1],"int64"), index_num=20, nshards=4, shard_id=1, ignore_value=16, )
[Prof] paddle.shard_index 	 paddle.shard_index(input=Tensor([12700801, 2, 1],"int64"), index_num=20, nshards=4, shard_id=1, ignore_value=16, ) 	 25401602 	 1000 	 0.30940985679626465 	 2.0420279502868652 	 0.30066776275634766 	 0.0006577968597412109 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:44.956085 test begin: paddle.shard_index(input=Tensor([25401601, 1],"int64"), index_num=13, nshards=3, shard_id=0, )
[Prof] paddle.shard_index 	 paddle.shard_index(input=Tensor([25401601, 1],"int64"), index_num=13, nshards=3, shard_id=0, ) 	 25401601 	 1000 	 0.31002211570739746 	 2.2340023517608643 	 0.3015122413635254 	 0.000762939453125 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:48.010018 test begin: paddle.shard_index(input=Tensor([4, 6350401, 1],"int64"), index_num=20, nshards=4, shard_id=1, )
[Prof] paddle.shard_index 	 paddle.shard_index(input=Tensor([4, 6350401, 1],"int64"), index_num=20, nshards=4, shard_id=1, ) 	 25401604 	 1000 	 0.30943799018859863 	 2.0423357486724854 	 0.2941405773162842 	 0.0006546974182128906 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:51.041714 test begin: paddle.shard_index(input=Tensor([4, 6350401, 1],"int64"), index_num=20, nshards=4, shard_id=1, ignore_value=16, )
[Prof] paddle.shard_index 	 paddle.shard_index(input=Tensor([4, 6350401, 1],"int64"), index_num=20, nshards=4, shard_id=1, ignore_value=16, ) 	 25401604 	 1000 	 0.30942511558532715 	 2.040452241897583 	 0.3006625175476074 	 0.0006642341613769531 	 None 	 None 	 None 	 None 	 combined
2025-07-24 19:36:54.092111 test begin: paddle.sign(Tensor([12404, 32, 128],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([12404, 32, 128],"float32"), ) 	 50806784 	 1000 	 0.3444352149963379 	 0.2978224754333496 	 0.3363487720489502 	 0.286928653717041 	 0.29605722427368164 	 0.05229759216308594 	 0.23496603965759277 	 6.151199340820312e-05 	 
2025-07-24 19:36:56.738305 test begin: paddle.sign(Tensor([32, 12404, 128],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([32, 12404, 128],"float32"), ) 	 50806784 	 1000 	 0.34449315071105957 	 0.29778265953063965 	 0.3366115093231201 	 0.2870922088623047 	 0.2960541248321533 	 0.05978274345397949 	 0.24305391311645508 	 7.891654968261719e-05 	 
2025-07-24 19:36:59.457110 test begin: paddle.sign(Tensor([32, 32, 49613],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([32, 32, 49613],"float32"), ) 	 50803712 	 1000 	 0.3449513912200928 	 0.3012809753417969 	 0.33695054054260254 	 0.28969836235046387 	 0.29586195945739746 	 0.052161455154418945 	 0.24271678924560547 	 4.1484832763671875e-05 	 
2025-07-24 19:37:02.121373 test begin: paddle.sign(Tensor([64, 1, 28, 28351],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([64, 1, 28, 28351],"float32"), ) 	 50804992 	 1000 	 0.34520912170410156 	 0.29778003692626953 	 0.3371138572692871 	 0.2869999408721924 	 0.29590582847595215 	 0.05307435989379883 	 0.24281048774719238 	 6.508827209472656e-05 	 
2025-07-24 19:37:04.810357 test begin: paddle.sign(Tensor([64, 1, 28351, 28],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([64, 1, 28351, 28],"float32"), ) 	 50804992 	 1000 	 0.34529805183410645 	 0.3004763126373291 	 0.33740854263305664 	 0.2864053249359131 	 0.29592061042785645 	 0.052759408950805664 	 0.24279141426086426 	 5.602836608886719e-05 	 
2025-07-24 19:37:07.465256 test begin: paddle.sign(Tensor([64, 1013, 28, 28],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([64, 1013, 28, 28],"float32"), ) 	 50828288 	 1000 	 0.3450171947479248 	 0.2978706359863281 	 0.33711981773376465 	 0.28711438179016113 	 0.29595518112182617 	 0.05611896514892578 	 0.24262619018554688 	 7.963180541992188e-05 	 
2025-07-24 19:37:10.154730 test begin: paddle.sign(Tensor([64801, 1, 28, 28],"float32"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([64801, 1, 28, 28],"float32"), ) 	 50803984 	 1000 	 0.34624552726745605 	 0.29772472381591797 	 0.337904691696167 	 0.2863616943359375 	 0.29598093032836914 	 0.07126975059509277 	 0.24226856231689453 	 5.2928924560546875e-05 	 
2025-07-24 19:37:12.783813 test begin: paddle.sign(Tensor([66151, 1, 384],"int64"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([66151, 1, 384],"int64"), ) 	 25401984 	 1000 	 0.308199405670166 	 0.30114197731018066 	 0.3003544807434082 	 0.2876262664794922 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 19:37:14.540214 test begin: paddle.sign(Tensor([7, 1, 3628801],"int64"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([7, 1, 3628801],"int64"), ) 	 25401607 	 1000 	 0.3079094886779785 	 0.2984631061553955 	 0.29996514320373535 	 0.2875189781188965 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 19:37:16.356900 test begin: paddle.sign(Tensor([7, 9451, 384],"int64"), )
[Prof] paddle.sign 	 paddle.sign(Tensor([7, 9451, 384],"int64"), ) 	 25404288 	 1000 	 0.30837440490722656 	 0.2985835075378418 	 0.2927427291870117 	 0.28101229667663574 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 19:37:18.058373 test begin: paddle.signal.stft(Tensor([16, 3175201],"float32"), 1024, 120, 600, window=Tensor([600],"float32"), center=True, pad_mode="reflect", )
[Prof] paddle.signal.stft 	 paddle.signal.stft(Tensor([16, 3175201],"float32"), 1024, 120, 600, window=Tensor([600],"float32"), center=True, pad_mode="reflect", ) 	 50803816 	 1000 	 19.412893295288086 	 4.756889343261719 	 2.4829533100128174 	 0.9742593765258789 	 43.31963610649109 	 33.899149894714355 	 2.947392702102661 	 1.7310280799865723 	 
2025-07-24 19:39:07.715877 test begin: paddle.signal.stft(Tensor([16, 3175201],"float32"), 2048, 240, 1200, window=Tensor([1200],"float32"), center=True, pad_mode="reflect", )
[Prof] paddle.signal.stft 	 paddle.signal.stft(Tensor([16, 3175201],"float32"), 2048, 240, 1200, window=Tensor([1200],"float32"), center=True, pad_mode="reflect", ) 	 50804416 	 1000 	 19.74289584159851 	 4.975337743759155 	 2.5242984294891357 	 1.0189800262451172 	 43.15583682060242 	 32.37235975265503 	 2.936255931854248 	 1.6505014896392822 	 
2025-07-24 19:40:54.173474 test begin: paddle.signal.stft(Tensor([16, 3175201],"float32"), 512, 50, 240, window=Tensor([240],"float32"), center=True, pad_mode="reflect", )
[Error] CUDA out of memory. Tried to allocate 1.94 GiB. GPU 0 has a total capacity of 39.39 GiB of which 1012.31 MiB is free. Process 4842 has 38.40 GiB memory in use. Of the allocated memory 4.45 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-07-24 19:42:20.204565 test begin: paddle.signal.stft(Tensor([1993, 25500],"float32"), 1024, 120, 600, window=Tensor([600],"float32"), center=True, pad_mode="reflect", )
W0724 19:42:21.201885 55499 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.signal.stft 	 paddle.signal.stft(Tensor([1993, 25500],"float32"), 1024, 120, 600, window=Tensor([600],"float32"), center=True, pad_mode="reflect", ) 	 50822100 	 1000 	 17.79650115966797 	 4.786996364593506 	 2.3624041080474854 	 0.9803495407104492 	 42.46984243392944 	 31.120118141174316 	 2.8896780014038086 	 1.5873210430145264 	 
2025-07-24 19:44:03.293428 test begin: paddle.signal.stft(Tensor([1993, 25500],"float32"), 2048, 240, 1200, window=Tensor([1200],"float32"), center=True, pad_mode="reflect", )
[Prof] paddle.signal.stft 	 paddle.signal.stft(Tensor([1993, 25500],"float32"), 2048, 240, 1200, window=Tensor([1200],"float32"), center=True, pad_mode="reflect", ) 	 50822700 	 1000 	 18.166623830795288 	 5.1282572746276855 	 2.323190450668335 	 1.0426170825958252 	 42.74590444564819 	 31.500190019607544 	 2.9061145782470703 	 1.6060490608215332 	 
2025-07-24 19:45:48.537913 test begin: paddle.signal.stft(Tensor([1993, 25500],"float32"), 512, 50, 240, window=Tensor([240],"float32"), center=True, pad_mode="reflect", )
[Error] CUDA out of memory. Tried to allocate 1.94 GiB. GPU 0 has a total capacity of 39.39 GiB of which 1.15 GiB is free. Process 39756 has 38.24 GiB memory in use. Of the allocated memory 4.47 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-07-24 19:47:11.402657 test begin: paddle.signbit(Tensor([11, 17, 271675],"int32"), )
W0724 19:47:12.289566 66258 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fc245f83010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 19:57:19.016956 test begin: paddle.signbit(Tensor([11, 17, 543350],"int16"), )
W0724 19:57:20.260208 90659 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f984fcab010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 20:07:27.653845 test begin: paddle.signbit(Tensor([11, 461848, 10],"int32"), )
W0724 20:07:28.387553 114508 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fdee458b070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 20:17:33.790307 test begin: paddle.signbit(Tensor([11, 923695, 10],"int16"), )
W0724 20:17:34.966377 136665 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7efa65143010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 20:27:44.368378 test begin: paddle.signbit(Tensor([12, 20, 211681],"float32"), )
W0724 20:27:45.333925 160640 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb693933010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 20:37:50.713350 test begin: paddle.signbit(Tensor([12, 2116801, 2],"float32"), )
W0724 20:37:51.694584 23347 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f64afb9f010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 20:47:57.694143 test begin: paddle.signbit(Tensor([1270081, 20, 2],"float32"), )
W0724 20:47:58.695580 51446 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f347e7bb010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 20:58:04.556620 test begin: paddle.signbit(Tensor([298843, 17, 10],"int32"), )
W0724 20:58:05.295430 79177 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f65ad887010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 21:08:11.243572 test begin: paddle.signbit(Tensor([597685, 17, 10],"int16"), )
W0724 21:08:12.407768 105244 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fb8db127010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 21:18:19.694094 test begin: paddle.sin(Tensor([128512, 396],"float32"), )
W0724 21:18:20.712139 131191 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.sin 	 paddle.sin(Tensor([128512, 396],"float32"), ) 	 50890752 	 1000 	 0.29621267318725586 	 0.3019702434539795 	 0.28240489959716797 	 0.28699684143066406 	 0.45123910903930664 	 0.7446091175079346 	 0.3965885639190674 	 0.3804049491882324 	 
2025-07-24 21:18:23.900682 test begin: paddle.sin(Tensor([254017, 200],"float32"), )
[Prof] paddle.sin 	 paddle.sin(Tensor([254017, 200],"float32"), ) 	 50803400 	 1000 	 0.2980234622955322 	 0.30603742599487305 	 0.28795504570007324 	 0.28691887855529785 	 0.45041441917419434 	 0.7435660362243652 	 0.3968689441680908 	 0.3799426555633545 	 
2025-07-24 21:18:29.534876 test begin: paddle.sin(Tensor([50000, 1017],"float32"), )
[Prof] paddle.sin 	 paddle.sin(Tensor([50000, 1017],"float32"), ) 	 50850000 	 1000 	 0.2989015579223633 	 0.29842090606689453 	 0.2875947952270508 	 0.2877542972564697 	 0.4508547782897949 	 0.7441697120666504 	 0.39728260040283203 	 0.3802649974822998 	 
2025-07-24 21:18:33.053566 test begin: paddle.sin(Tensor([508033, 100],"float32"), )
[Prof] paddle.sin 	 paddle.sin(Tensor([508033, 100],"float32"), ) 	 50803300 	 1000 	 0.3009459972381592 	 0.3005051612854004 	 0.28655338287353516 	 0.2875101566314697 	 0.45044755935668945 	 0.7436575889587402 	 0.39679384231567383 	 0.37999606132507324 	 
2025-07-24 21:18:40.567190 test begin: paddle.sin(Tensor([68608, 741],"float32"), )
[Prof] paddle.sin 	 paddle.sin(Tensor([68608, 741],"float32"), ) 	 50838528 	 1000 	 0.2960634231567383 	 0.2985422611236572 	 0.28640007972717285 	 0.28774309158325195 	 0.4508044719696045 	 0.7440600395202637 	 0.3973531723022461 	 0.38019680976867676 	 
2025-07-24 21:18:44.076303 test begin: paddle.sinc(Tensor([16, 1587601],"float64"), )
[Prof] paddle.sinc 	 paddle.sinc(Tensor([16, 1587601],"float64"), ) 	 25401616 	 1000 	 2.973167896270752 	 0.3054494857788086 	 0.250974178314209 	 0.291057825088501 	 2.593912363052368 	 3.7702817916870117 	 0.4410562515258789 	 0.32053685188293457 	 
2025-07-24 21:18:55.127224 test begin: paddle.sinc(Tensor([396901, 64],"float64"), )
[Prof] paddle.sinc 	 paddle.sinc(Tensor([396901, 64],"float64"), ) 	 25401664 	 1000 	 2.946561336517334 	 0.3016083240509033 	 0.25091981887817383 	 0.2915036678314209 	 2.5921449661254883 	 3.769862413406372 	 0.44107747077941895 	 0.3205709457397461 	 
2025-07-24 21:19:05.843200 test begin: paddle.sinh(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.sinh 	 paddle.sinh(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 1000 	 0.2955763339996338 	 0.29816174507141113 	 0.28560876846313477 	 0.28745436668395996 	 0.4501526355743408 	 0.7450926303863525 	 0.3954150676727295 	 0.38126206398010254 	 
2025-07-24 21:19:09.450502 test begin: paddle.sinh(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.sinh 	 paddle.sinh(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 1000 	 0.2957577705383301 	 0.314312219619751 	 0.28702855110168457 	 0.28876686096191406 	 0.4502124786376953 	 0.7437663078308105 	 0.39781641960144043 	 0.37995433807373047 	 
2025-07-24 21:19:12.952580 test begin: paddle.sinh(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.sinh 	 paddle.sinh(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 1000 	 0.29565978050231934 	 0.2981581687927246 	 0.2869141101837158 	 0.28746533393859863 	 0.4502742290496826 	 0.745098352432251 	 0.37083911895751953 	 0.38135552406311035 	 
2025-07-24 21:19:16.454275 test begin: paddle.slice(Tensor([65344, 1555],"bfloat16"), axes=list[0,], starts=list[0,], ends=list[8168,], )
W0724 21:19:18.211447 133531 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

2025-07-24 21:19:18.251564 test begin: paddle.slice(Tensor([65344, 1555],"bfloat16"), axes=list[0,], starts=list[16336,], ends=list[24504,], )
W0724 21:19:19.993789 133633 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

2025-07-24 21:19:20.019219 test begin: paddle.slice(Tensor([65344, 1555],"bfloat16"), axes=list[0,], starts=list[24504,], ends=list[32672,], )
W0724 21:19:21.786108 133727 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

2025-07-24 21:19:21.812952 test begin: paddle.slice(Tensor([79381, 1280],"bfloat16"), axes=list[0,], starts=list[0,], ends=list[8168,], )
W0724 21:19:23.593713 133820 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

2025-07-24 21:19:23.616624 test begin: paddle.slice(Tensor([79381, 1280],"bfloat16"), axes=list[0,], starts=list[16336,], ends=list[24504,], )
W0724 21:19:25.372181 133845 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

2025-07-24 21:19:25.394036 test begin: paddle.slice(Tensor([79381, 1280],"bfloat16"), axes=list[0,], starts=list[24504,], ends=list[32672,], )
W0724 21:19:27.271211 133930 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (InvalidArgument) The type of data we are trying to retrieve (bfloat16) does not match the type of data (float32) currently contained in the container.
  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():16.] (at ../paddle/phi/core/dense_tensor.cc:167)

2025-07-24 21:19:27.293228 test begin: paddle.slice_scatter(Tensor([8, 1058401, 3, 9],"float32"), Tensor([8, 1058401, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 1058401, 3, 9],"float32"), Tensor([8, 1058401, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 279417864 	 1000 	 1.4701356887817383 	 2.847648859024048 	 1.454416275024414 	 0.9705929756164551 	 4.07356333732605 	 3.7892894744873047 	 0.6932754516601562 	 0.7753946781158447 	 combined
2025-07-24 21:19:49.600189 test begin: paddle.slice_scatter(Tensor([8, 117601, 3, 9],"float64"), Tensor([8, 117601, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 117601, 3, 9],"float64"), Tensor([8, 117601, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 31046664 	 1000 	 0.23414206504821777 	 0.550889253616333 	 0.21852874755859375 	 0.18749141693115234 	 0.8300967216491699 	 0.7737696170806885 	 0.1413438320159912 	 0.1580793857574463 	 combined
2025-07-24 21:19:53.163021 test begin: paddle.slice_scatter(Tensor([8, 235201, 3, 9],"float32"), Tensor([8, 235201, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 235201, 3, 9],"float32"), Tensor([8, 235201, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 62093064 	 1000 	 0.32845258712768555 	 0.6412880420684814 	 0.31273460388183594 	 0.2183210849761963 	 0.9307193756103516 	 0.8652822971343994 	 0.15846681594848633 	 0.17683005332946777 	 combined
2025-07-24 21:19:57.838456 test begin: paddle.slice_scatter(Tensor([8, 423361, 3, 5],"float32"), Tensor([8, 2, 3, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 423361, 3, 5],"float32"), Tensor([8, 2, 3, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 50803560 	 1000 	 0.015555858612060547 	 0.33104372024536133 	 1.9073486328125e-05 	 0.10761618614196777 	 0.3234825134277344 	 0.322371244430542 	 0.05496501922607422 	 0.06572675704956055 	 combined
2025-07-24 21:20:00.494488 test begin: paddle.slice_scatter(Tensor([8, 529201, 3, 9],"float64"), Tensor([8, 529201, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 529201, 3, 9],"float64"), Tensor([8, 529201, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 139709064 	 1000 	 1.0583081245422363 	 2.4565277099609375 	 1.0426864624023438 	 0.8357234001159668 	 3.6210172176361084 	 3.3922488689422607 	 0.616081714630127 	 0.6938621997833252 	 combined
2025-07-24 21:20:16.551015 test begin: paddle.slice_scatter(Tensor([8, 6, 117601, 9],"float32"), Tensor([8, 6, 117601, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 117601, 9],"float32"), Tensor([8, 6, 117601, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 62093328 	 1000 	 0.3298461437225342 	 0.6411495208740234 	 0.3141443729400635 	 0.21828818321228027 	 0.9323091506958008 	 0.8667519092559814 	 0.15869569778442383 	 0.17680811882019043 	 combined
2025-07-24 21:20:21.221936 test begin: paddle.slice_scatter(Tensor([8, 6, 211681, 5],"float32"), Tensor([8, 2, 211681, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 211681, 5],"float32"), Tensor([8, 2, 211681, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 67737920 	 1000 	 0.18332409858703613 	 0.4182291030883789 	 0.16705918312072754 	 0.14221930503845215 	 0.7730534076690674 	 0.573850154876709 	 0.13164901733398438 	 0.11690092086791992 	 combined
2025-07-24 21:20:25.206344 test begin: paddle.slice_scatter(Tensor([8, 6, 264601, 9],"float64"), Tensor([8, 6, 264601, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 264601, 9],"float64"), Tensor([8, 6, 264601, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 139709328 	 1000 	 1.0582809448242188 	 2.4657304286956787 	 1.042724609375 	 0.835869312286377 	 3.62129282951355 	 3.3908064365386963 	 0.6161143779754639 	 0.6925613880157471 	 combined
2025-07-24 21:20:43.936495 test begin: paddle.slice_scatter(Tensor([8, 6, 3, 1058401],"float32"), Tensor([8, 2, 3, 1058401],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 3, 1058401],"float32"), Tensor([8, 2, 3, 1058401],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 203212992 	 1000 	 0.540794849395752 	 1.231339931488037 	 0.5252296924591064 	 0.4183516502380371 	 2.278642177581787 	 1.6761653423309326 	 0.38808131217956543 	 0.341707706451416 	 combined
2025-07-24 21:20:55.576688 test begin: paddle.slice_scatter(Tensor([8, 6, 3, 176401],"float64"), Tensor([8, 6, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 3, 176401],"float64"), Tensor([8, 6, 3, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 25402032 	 1000 	 0.015259265899658203 	 0.3169102668762207 	 9.298324584960938e-06 	 0.10769462585449219 	 0.3256363868713379 	 0.3220984935760498 	 0.05533623695373535 	 0.06566596031188965 	 combined
2025-07-24 21:20:57.643701 test begin: paddle.slice_scatter(Tensor([8, 6, 3, 352801],"float32"), Tensor([8, 2, 3, 352801],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 3, 352801],"float32"), Tensor([8, 2, 3, 352801],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 67737792 	 1000 	 0.18326306343078613 	 0.41819238662719727 	 0.15617704391479492 	 0.14230632781982422 	 0.774390459060669 	 0.572195291519165 	 0.1318647861480713 	 0.11685657501220703 	 combined
2025-07-24 21:21:01.552936 test begin: paddle.slice_scatter(Tensor([8, 6, 3, 352801],"float32"), Tensor([8, 6, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 3, 352801],"float32"), Tensor([8, 6, 3, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 50803632 	 1000 	 0.021265029907226562 	 0.33327794075012207 	 1.7642974853515625e-05 	 0.10768628120422363 	 0.3262062072753906 	 0.3227872848510742 	 0.05543828010559082 	 0.06580233573913574 	 combined
2025-07-24 21:21:04.231171 test begin: paddle.slice_scatter(Tensor([8, 6, 529201, 9],"float32"), Tensor([8, 6, 529201, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 529201, 9],"float32"), Tensor([8, 6, 529201, 2],"float32"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 279418128 	 1000 	 1.548792839050293 	 2.8474953174591064 	 1.5330960750579834 	 0.969050407409668 	 4.07213282585144 	 3.789205312728882 	 0.6925332546234131 	 0.7755005359649658 	 combined
2025-07-24 21:21:25.349444 test begin: paddle.slice_scatter(Tensor([8, 6, 58801, 9],"float64"), Tensor([8, 6, 58801, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 58801, 9],"float64"), Tensor([8, 6, 58801, 2],"float64"), axes=list[3,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 31046928 	 1000 	 0.23412513732910156 	 0.5508289337158203 	 0.21847057342529297 	 0.18745183944702148 	 0.83133864402771 	 0.77378249168396 	 0.14151787757873535 	 0.15799999237060547 	 combined
2025-07-24 21:21:28.892379 test begin: paddle.slice_scatter(Tensor([8, 6, 635041, 5],"float32"), Tensor([8, 2, 635041, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], )
[Prof] paddle.slice_scatter 	 paddle.slice_scatter(Tensor([8, 6, 635041, 5],"float32"), Tensor([8, 2, 635041, 5],"float32"), axes=list[1,], starts=list[2,], ends=list[6,], strides=list[2,], ) 	 203213120 	 1000 	 0.5408189296722412 	 1.4652338027954102 	 0.5252366065979004 	 0.41991519927978516 	 2.279113531112671 	 1.67299222946167 	 0.38834714889526367 	 0.3416876792907715 	 combined
2025-07-24 21:21:42.439286 test begin: paddle.sqrt(Tensor([128, 396901],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([128, 396901],"float32"), ) 	 50803328 	 1000 	 0.29468750953674316 	 0.30773329734802246 	 0.28619956970214844 	 0.2882256507873535 	 0.4503295421600342 	 0.7488307952880859 	 0.39740777015686035 	 0.38181543350219727 	 
2025-07-24 21:21:47.913684 test begin: paddle.sqrt(Tensor([18, 15, 3, 256, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([18, 15, 3, 256, 256],"float32"), ) 	 53084160 	 1000 	 0.30710887908935547 	 0.33052825927734375 	 0.2987234592437744 	 0.30091381072998047 	 0.4702155590057373 	 0.7802116870880127 	 0.4165618419647217 	 0.39862728118896484 	 
2025-07-24 21:21:51.624389 test begin: paddle.sqrt(Tensor([259, 3, 256, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([259, 3, 256, 256],"float32"), ) 	 50921472 	 1000 	 0.2952399253845215 	 0.3055896759033203 	 0.28679966926574707 	 0.28833889961242676 	 0.45139479637145996 	 0.750145673751831 	 0.39800310134887695 	 0.3838934898376465 	 
2025-07-24 21:21:55.093123 test begin: paddle.sqrt(Tensor([4, 15, 13, 256, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([4, 15, 13, 256, 256],"float32"), ) 	 51118080 	 1000 	 0.29607510566711426 	 0.3019535541534424 	 0.2876627445220947 	 0.29129505157470703 	 0.45299863815307617 	 0.751676082611084 	 0.3990764617919922 	 0.3840324878692627 	 
2025-07-24 21:21:58.553443 test begin: paddle.sqrt(Tensor([4, 15, 3, 1103, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([4, 15, 3, 1103, 256],"float32"), ) 	 50826240 	 1000 	 0.29599761962890625 	 0.30085015296936035 	 0.2873649597167969 	 0.28819942474365234 	 0.45058226585388184 	 0.7474863529205322 	 0.3965919017791748 	 0.3819088935852051 	 
2025-07-24 21:22:02.053644 test begin: paddle.sqrt(Tensor([4, 15, 3, 256, 1103],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([4, 15, 3, 256, 1103],"float32"), ) 	 50826240 	 1000 	 0.29468584060668945 	 0.29898548126220703 	 0.28304171562194824 	 0.2883749008178711 	 0.45209360122680664 	 0.7474982738494873 	 0.39754509925842285 	 0.3819098472595215 	 
2025-07-24 21:22:05.487227 test begin: paddle.sqrt(Tensor([4, 65, 3, 256, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([4, 65, 3, 256, 256],"float32"), ) 	 51118080 	 1000 	 0.29752135276794434 	 0.30066490173339844 	 0.28899097442626953 	 0.28995466232299805 	 0.4529409408569336 	 0.7530534267425537 	 0.3934154510498047 	 0.38541650772094727 	 
2025-07-24 21:22:08.950571 test begin: paddle.sqrt(Tensor([544, 93431],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([544, 93431],"float32"), ) 	 50826464 	 1000 	 0.2946932315826416 	 0.3126645088195801 	 0.28618383407592773 	 0.2882802486419678 	 0.4504854679107666 	 0.7474422454833984 	 0.397050142288208 	 0.38186049461364746 	 
2025-07-24 21:22:12.377094 test begin: paddle.sqrt(Tensor([64, 13, 256, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([64, 13, 256, 256],"float32"), ) 	 54525952 	 1000 	 0.31543517112731934 	 0.3202991485595703 	 0.3069126605987549 	 0.3095731735229492 	 0.483001708984375 	 0.8022229671478271 	 0.42966675758361816 	 0.41054725646972656 	 
2025-07-24 21:22:16.009937 test begin: paddle.sqrt(Tensor([64, 3, 1034, 256],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([64, 3, 1034, 256],"float32"), ) 	 50823168 	 1000 	 0.2944791316986084 	 0.29906177520751953 	 0.28607678413391113 	 0.28836750984191895 	 0.4505033493041992 	 0.7474706172943115 	 0.39737653732299805 	 0.38190126419067383 	 
2025-07-24 21:22:19.436848 test begin: paddle.sqrt(Tensor([64, 3, 256, 1034],"float32"), )
[Prof] paddle.sqrt 	 paddle.sqrt(Tensor([64, 3, 256, 1034],"float32"), ) 	 50823168 	 1000 	 0.2944831848144531 	 0.298999547958374 	 0.2856626510620117 	 0.2883453369140625 	 0.45053768157958984 	 0.7487380504608154 	 0.3972792625427246 	 0.3818929195404053 	 
2025-07-24 21:22:22.849981 test begin: paddle.square(Tensor([104, 488493],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([104, 488493],"float32"), ) 	 50803272 	 1000 	 0.29587483406066895 	 0.29796266555786133 	 0.2874009609222412 	 0.28609776496887207 	 0.4511072635650635 	 1.0554592609405518 	 0.39851903915405273 	 0.26982831954956055 	 
2025-07-24 21:22:26.726587 test begin: paddle.square(Tensor([128, 396901],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([128, 396901],"float32"), ) 	 50803328 	 1000 	 0.29582953453063965 	 0.2992126941680908 	 0.2873530387878418 	 0.2863781452178955 	 0.4496290683746338 	 1.055438756942749 	 0.39690685272216797 	 0.26984643936157227 	 
2025-07-24 21:22:30.436626 test begin: paddle.square(Tensor([24904, 12, 170, 1],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([24904, 12, 170, 1],"float32"), ) 	 50804160 	 1000 	 0.29709792137145996 	 0.29778099060058594 	 0.28876495361328125 	 0.2860074043273926 	 0.4494960308074951 	 1.0553841590881348 	 0.3969457149505615 	 0.26982998847961426 	 
2025-07-24 21:22:34.180924 test begin: paddle.square(Tensor([3548, 12, 1194, 1],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([3548, 12, 1194, 1],"float32"), ) 	 50835744 	 1000 	 0.29587483406066895 	 0.3171215057373047 	 0.28740429878234863 	 0.28573131561279297 	 0.4498605728149414 	 1.0562853813171387 	 0.3974456787109375 	 0.2700667381286621 	 
2025-07-24 21:22:39.576212 test begin: paddle.square(Tensor([3548, 12, 170, 8],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([3548, 12, 170, 8],"float32"), ) 	 57903360 	 1000 	 0.33631205558776855 	 0.3439960479736328 	 0.3278326988220215 	 0.326401948928833 	 0.5121254920959473 	 1.200451374053955 	 0.4595799446105957 	 0.30692529678344727 	 
2025-07-24 21:22:43.893226 test begin: paddle.square(Tensor([3548, 85, 170, 1],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([3548, 85, 170, 1],"float32"), ) 	 51268600 	 1000 	 0.29834651947021484 	 0.30048441886901855 	 0.28992176055908203 	 0.28894472122192383 	 0.4534118175506592 	 1.064974069595337 	 0.4005086421966553 	 0.2722890377044678 	 
2025-07-24 21:22:47.619879 test begin: paddle.square(Tensor([544, 93431],"float32"), )
[Prof] paddle.square 	 paddle.square(Tensor([544, 93431],"float32"), ) 	 50826464 	 1000 	 0.295778751373291 	 0.29808712005615234 	 0.2873523235321045 	 0.2862534523010254 	 0.4495973587036133 	 1.0573296546936035 	 0.3968369960784912 	 0.27004551887512207 	 
2025-07-24 21:22:53.052584 test begin: paddle.squeeze(Tensor([10, 512, 1, 100, 100],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([10, 512, 1, 100, 100],"float32"), axis=list[2,], ) 	 51200000 	 1000 	 0.004855632781982422 	 0.004959583282470703 	 1.2159347534179688e-05 	 1.8835067749023438e-05 	 0.041883230209350586 	 0.05832815170288086 	 3.4332275390625e-05 	 7.605552673339844e-05 	 
2025-07-24 21:22:54.778011 test begin: paddle.squeeze(Tensor([105344, 483],"float32"), )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([105344, 483],"float32"), ) 	 50881152 	 1000 	 0.003864765167236328 	 0.0038061141967773438 	 1.2874603271484375e-05 	 2.288818359375e-05 	 0.04149746894836426 	 0.059355974197387695 	 2.9087066650390625e-05 	 7.843971252441406e-05 	 
2025-07-24 21:22:56.508147 test begin: paddle.squeeze(Tensor([396901, 128],"float32"), )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([396901, 128],"float32"), ) 	 50803328 	 1000 	 0.0038213729858398438 	 0.003744363784790039 	 1.9073486328125e-05 	 1.7881393432617188e-05 	 0.04149222373962402 	 0.05318427085876465 	 4.291534423828125e-05 	 6.604194641113281e-05 	 
2025-07-24 21:22:58.269830 test begin: paddle.squeeze(Tensor([421120, 25, 5],"float32"), axis=-1, )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([421120, 25, 5],"float32"), axis=-1, ) 	 52640000 	 1000 	 0.004554033279418945 	 0.0039365291595458984 	 1.4543533325195312e-05 	 1.7642974853515625e-05 	 0.04145312309265137 	 0.05272340774536133 	 3.337860107421875e-05 	 3.933906555175781e-05 	 
2025-07-24 21:23:00.054936 test begin: paddle.squeeze(Tensor([421120, 31, 4],"float32"), axis=-1, )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([421120, 31, 4],"float32"), axis=-1, ) 	 52218880 	 1000 	 0.004578113555908203 	 0.003920078277587891 	 6.9141387939453125e-06 	 1.811981201171875e-05 	 0.04174470901489258 	 0.053633928298950195 	 3.62396240234375e-05 	 9.250640869140625e-05 	 
2025-07-24 21:23:01.863805 test begin: paddle.squeeze(Tensor([508033, 25, 4],"float32"), axis=-1, )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([508033, 25, 4],"float32"), axis=-1, ) 	 50803300 	 1000 	 0.0046308040618896484 	 0.0038542747497558594 	 6.9141387939453125e-06 	 1.9073486328125e-05 	 0.043182373046875 	 0.05356311798095703 	 4.0531158447265625e-05 	 6.103515625e-05 	 
2025-07-24 21:23:03.602573 test begin: paddle.squeeze(Tensor([8, 512, 1, 100, 125],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([8, 512, 1, 100, 125],"float32"), axis=list[2,], ) 	 51200000 	 1000 	 0.004719734191894531 	 0.004916191101074219 	 7.867813110351562e-06 	 1.811981201171875e-05 	 0.04192614555358887 	 0.06055712699890137 	 3.409385681152344e-05 	 6.818771362304688e-05 	 
2025-07-24 21:23:05.325412 test begin: paddle.squeeze(Tensor([8, 512, 1, 125, 100],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([8, 512, 1, 125, 100],"float32"), axis=list[2,], ) 	 51200000 	 1000 	 0.00470733642578125 	 0.004858493804931641 	 1.7404556274414062e-05 	 1.7404556274414062e-05 	 0.041756629943847656 	 0.05771756172180176 	 2.47955322265625e-05 	 4.172325134277344e-05 	 
2025-07-24 21:23:07.184203 test begin: paddle.squeeze(Tensor([8, 512, 2, 100, 100],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([8, 512, 2, 100, 100],"float32"), axis=list[2,], ) 	 81920000 	 1000 	 0.004831790924072266 	 0.00504755973815918 	 7.3909759521484375e-06 	 2.4080276489257812e-05 	 0.04182314872741699 	 0.05430102348327637 	 3.9577484130859375e-05 	 5.340576171875e-05 	 
2025-07-24 21:23:09.873245 test begin: paddle.squeeze(Tensor([8, 636, 1, 100, 100],"float32"), axis=list[2,], )
[Prof] paddle.squeeze 	 paddle.squeeze(Tensor([8, 636, 1, 100, 100],"float32"), axis=list[2,], ) 	 50880000 	 1000 	 0.0046880245208740234 	 0.004891633987426758 	 1.1920928955078125e-05 	 2.1219253540039062e-05 	 0.0433659553527832 	 0.05881786346435547 	 4.3392181396484375e-05 	 5.841255187988281e-05 	 
2025-07-24 21:23:11.757060 test begin: paddle.stack(list[Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),Tensor([11, 32, 36828, 4],"float32"),], axis=-2, ) 	 259269120 	 1000 	 1.8170783519744873 	 7.130244255065918 	 1.805114507675171 	 7.113222122192383 	 2.031411647796631 	 0.0951540470123291 	 1.9568307399749756 	 7.033348083496094e-05 	 
2025-07-24 21:23:31.417451 test begin: paddle.stack(list[Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),Tensor([11, 32, 38367, 4],"float32"),], axis=-2, ) 	 270103680 	 1000 	 1.902510643005371 	 7.429175138473511 	 1.8868293762207031 	 7.410264253616333 	 2.116281747817993 	 0.09228515625 	 2.041893482208252 	 5.9604644775390625e-05 	 
2025-07-24 21:23:55.301786 test begin: paddle.stack(list[Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),], axis=0, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),Tensor([14176, 7168],"bfloat16"),], axis=0, ) 	 609681408 	 1000 	 2.9549458026885986 	 2.5954511165618896 	 2.9423105716705322 	 2.5765817165374756 	 4.571386337280273 	 2.700951099395752 	 4.467006683349609 	 1.3801491260528564 	 
2025-07-24 21:24:28.398349 test begin: paddle.stack(list[Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),], axis=0, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),Tensor([7168, 14176],"bfloat16"),], axis=0, ) 	 609681408 	 1000 	 2.959726095199585 	 2.592533826828003 	 2.937880039215088 	 2.5755112171173096 	 4.5715484619140625 	 2.7026467323303223 	 4.458011865615845 	 1.3818013668060303 	 
2025-07-24 21:25:05.086544 test begin: paddle.stack(list[Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),Tensor([8, 32, 36828, 6],"float32"),], axis=-2, ) 	 282839040 	 1000 	 1.9955589771270752 	 8.157004356384277 	 1.9825379848480225 	 8.124661684036255 	 2.216304063796997 	 0.09406137466430664 	 2.141613483428955 	 6.29425048828125e-05 	 
2025-07-24 21:25:27.535758 test begin: paddle.stack(list[Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),Tensor([8, 32, 38367, 6],"float32"),], axis=-2, ) 	 294658560 	 1000 	 2.1140267848968506 	 8.51475477218628 	 2.09033203125 	 8.495140552520752 	 2.3061208724975586 	 0.09320378303527832 	 2.231274127960205 	 5.984306335449219e-05 	 
2025-07-24 21:25:51.623545 test begin: paddle.stack(list[Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),Tensor([8, 32, 49613, 4],"float32"),], axis=-2, ) 	 254018560 	 1000 	 1.8164114952087402 	 6.981222629547119 	 1.8048110008239746 	 6.9640161991119385 	 1.9832935333251953 	 0.09265542030334473 	 1.9084970951080322 	 6.628036499023438e-05 	 
2025-07-24 21:26:10.903264 test begin: paddle.stack(list[Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),Tensor([8, 42, 38367, 4],"float32"),], axis=-2, ) 	 257826240 	 1000 	 1.8085908889770508 	 7.0886335372924805 	 1.7970027923583984 	 7.071502447128296 	 2.0137035846710205 	 0.09350728988647461 	 1.9397060871124268 	 7.081031799316406e-05 	 
2025-07-24 21:26:30.358952 test begin: paddle.stack(list[Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),], axis=-2, )
[Prof] paddle.stack 	 paddle.stack(list[Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),Tensor([8, 44, 36828, 4],"float32"),], axis=-2, ) 	 259269120 	 1000 	 1.8066596984863281 	 7.128095865249634 	 1.7950856685638428 	 7.111132383346558 	 2.02665638923645 	 0.09429001808166504 	 1.9503092765808105 	 8.535385131835938e-05 	 
2025-07-24 21:26:50.011157 test begin: paddle.stanh(x=Tensor([12700801, 2],"float64"), scale_a=6.42, scale_b=3.58, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([12700801, 2],"float64"), scale_a=6.42, scale_b=3.58, ) 	 25401602 	 1000 	 0.30743980407714844 	 0.3068108558654785 	 0.2970426082611084 	 0.2961111068725586 	 0.44641733169555664 	 0.741248607635498 	 0.3935866355895996 	 0.37866663932800293 	 
2025-07-24 21:26:52.881663 test begin: paddle.stanh(x=Tensor([2, 12700801],"float64"), scale_a=6.42, scale_b=3.58, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2, 12700801],"float64"), scale_a=6.42, scale_b=3.58, ) 	 25401602 	 1000 	 0.3073456287384033 	 0.3118140697479248 	 0.297985315322876 	 0.29640841484069824 	 0.4476757049560547 	 0.7412102222442627 	 0.3952934741973877 	 0.3786942958831787 	 
2025-07-24 21:26:55.761681 test begin: paddle.stanh(x=Tensor([2, 25401601],"float32"), scale_a=6.42, scale_b=3.58, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2, 25401601],"float32"), scale_a=6.42, scale_b=3.58, ) 	 50803202 	 1000 	 0.2961912155151367 	 0.2989380359649658 	 0.28204965591430664 	 0.28828930854797363 	 0.4519069194793701 	 0.744175910949707 	 0.38747096061706543 	 0.37952327728271484 	 
2025-07-24 21:26:59.207961 test begin: paddle.stanh(x=Tensor([2, 3, 2, 2116801],"float64"), scale_a=0.67, scale_b=1.72, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2, 3, 2, 2116801],"float64"), scale_a=0.67, scale_b=1.72, ) 	 25401612 	 1000 	 0.29963254928588867 	 0.30330419540405273 	 0.29019975662231445 	 0.2896761894226074 	 0.44748568534851074 	 0.7412276268005371 	 0.39542245864868164 	 0.3786923885345459 	 
2025-07-24 21:27:02.080317 test begin: paddle.stanh(x=Tensor([2, 3, 2116801, 2],"float64"), scale_a=0.67, scale_b=1.72, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2, 3, 2116801, 2],"float64"), scale_a=0.67, scale_b=1.72, ) 	 25401612 	 1000 	 0.30283260345458984 	 0.31065797805786133 	 0.29166078567504883 	 0.28968214988708496 	 0.4489607810974121 	 0.7412149906158447 	 0.3775365352630615 	 0.3786776065826416 	 
2025-07-24 21:27:05.000421 test begin: paddle.stanh(x=Tensor([2, 3175201, 2, 2],"float64"), scale_a=0.67, scale_b=1.72, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2, 3175201, 2, 2],"float64"), scale_a=0.67, scale_b=1.72, ) 	 25401608 	 1000 	 0.2996184825897217 	 0.300386905670166 	 0.29026269912719727 	 0.2897953987121582 	 0.44753265380859375 	 0.7425620555877686 	 0.3932015895843506 	 0.37868213653564453 	 
2025-07-24 21:27:07.938893 test begin: paddle.stanh(x=Tensor([2116801, 3, 2, 2],"float64"), scale_a=0.67, scale_b=1.72, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([2116801, 3, 2, 2],"float64"), scale_a=0.67, scale_b=1.72, ) 	 25401612 	 1000 	 0.29961204528808594 	 0.5391991138458252 	 0.29021167755126953 	 0.28967976570129395 	 0.4475114345550537 	 0.7412269115447998 	 0.3950669765472412 	 0.3787262439727783 	 
2025-07-24 21:27:14.828853 test begin: paddle.stanh(x=Tensor([25401601, 2],"float32"), scale_a=6.42, scale_b=3.58, )
[Prof] paddle.stanh 	 paddle.stanh(x=Tensor([25401601, 2],"float32"), scale_a=6.42, scale_b=3.58, ) 	 50803202 	 1000 	 0.30059099197387695 	 0.2988409996032715 	 0.28700876235961914 	 0.28827905654907227 	 0.4505600929260254 	 0.7427985668182373 	 0.3981630802154541 	 0.37949585914611816 	 
2025-07-24 21:27:18.263613 test begin: paddle.std(Tensor([1, 1270081, 4, 10],"float32"), list[1,3,], True, False, )
W0724 21:27:19.009801 153200 dygraph_functions.cc:88394] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.std 	 paddle.std(Tensor([1, 1270081, 4, 10],"float32"), list[1,3,], True, False, ) 	 50803240 	 1000 	 1.2760541439056396 	 0.2310495376586914 	 3.4809112548828125e-05 	 0.11803627014160156 	 1.433030605316162 	 0.8012821674346924 	 0.18306446075439453 	 0.09127569198608398 	 
2025-07-24 21:27:23.175477 test begin: paddle.std(Tensor([1, 3, 1693441, 10],"float32"), list[1,3,], True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([1, 3, 1693441, 10],"float32"), list[1,3,], True, False, ) 	 50803230 	 1000 	 1.5394744873046875 	 0.793942928314209 	 5.054473876953125e-05 	 0.7766969203948975 	 1.6481244564056396 	 1.073805809020996 	 0.24060845375061035 	 0.13739848136901855 	 
2025-07-24 21:27:29.149777 test begin: paddle.std(Tensor([1, 3, 4, 2116801],"float64"), 2, True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([1, 3, 4, 2116801],"float64"), 2, True, False, ) 	 25401612 	 1000 	 1.6508715152740479 	 0.21790480613708496 	 0.00012350082397460938 	 0.20028185844421387 	 2.022108316421509 	 1.4888083934783936 	 0.2952589988708496 	 0.1900184154510498 	 
2025-07-24 21:27:35.224706 test begin: paddle.std(Tensor([1, 3, 4, 4233601],"float32"), list[1,3,], True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([1, 3, 4, 4233601],"float32"), list[1,3,], True, False, ) 	 50803212 	 1000 	 1.2378356456756592 	 0.23429036140441895 	 5.602836608886719e-05 	 0.11969375610351562 	 1.4125983715057373 	 0.7948858737945557 	 0.1807243824005127 	 0.09057259559631348 	 
2025-07-24 21:27:41.288090 test begin: paddle.std(Tensor([1, 3, 846721, 10],"float64"), 2, True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([1, 3, 846721, 10],"float64"), 2, True, False, ) 	 25401630 	 1000 	 7.971379995346069 	 0.18715167045593262 	 4.744529724121094e-05 	 0.094940185546875 	 4.808433771133423 	 0.7790513038635254 	 0.6151003837585449 	 0.08860015869140625 	 
2025-07-24 21:27:55.608279 test begin: paddle.std(Tensor([1, 635041, 4, 10],"float64"), 2, True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([1, 635041, 4, 10],"float64"), 2, True, False, ) 	 25401640 	 1000 	 1.757850170135498 	 0.19864106178283691 	 0.0001220703125 	 0.18108010292053223 	 1.9423270225524902 	 1.2669155597686768 	 0.2831587791442871 	 0.16191625595092773 	 
2025-07-24 21:28:01.440850 test begin: paddle.std(Tensor([1587601, 32],"float32"), )
[Prof] paddle.std 	 paddle.std(Tensor([1587601, 32],"float32"), ) 	 50803232 	 1000 	 1.0966682434082031 	 0.1662449836730957 	 1.9788742065429688e-05 	 0.08491325378417969 	 1.341001033782959 	 0.7746703624725342 	 0.17117857933044434 	 0.08826780319213867 	 
2025-07-24 21:28:05.680686 test begin: paddle.std(Tensor([211681, 3, 4, 10],"float64"), 2, True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([211681, 3, 4, 10],"float64"), 2, True, False, ) 	 25401720 	 1000 	 1.7547757625579834 	 0.19866251945495605 	 0.00011348724365234375 	 0.1809399127960205 	 1.9429619312286377 	 1.266862154006958 	 0.2832024097442627 	 0.1619400978088379 	 
2025-07-24 21:28:11.508802 test begin: paddle.std(Tensor([32, 1587601],"float32"), )
[Prof] paddle.std 	 paddle.std(Tensor([32, 1587601],"float32"), ) 	 50803232 	 1000 	 1.0988976955413818 	 0.1662602424621582 	 2.1219253540039062e-05 	 0.08494019508361816 	 1.3381156921386719 	 0.7748606204986572 	 0.1712350845336914 	 0.08826994895935059 	 
2025-07-24 21:28:17.990702 test begin: paddle.std(Tensor([423361, 3, 4, 10],"float32"), list[1,3,], True, False, )
[Prof] paddle.std 	 paddle.std(Tensor([423361, 3, 4, 10],"float32"), list[1,3,], True, False, ) 	 50803320 	 1000 	 1.5342261791229248 	 0.8258240222930908 	 5.412101745605469e-05 	 0.8087670803070068 	 1.6344916820526123 	 1.1033594608306885 	 0.23860836029052734 	 0.1409893035888672 	 
2025-07-24 21:28:25.954742 test begin: paddle.strided_slice(x=Tensor([3, 4, 352801, 6],"float64"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
[Prof] paddle.strided_slice 	 paddle.strided_slice(x=Tensor([3, 4, 352801, 6],"float64"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], ) 	 25401672 	 1000 	 0.00584101676940918 	 0.2102649211883545 	 1.33514404296875e-05 	 7.534027099609375e-05 	 0.1470048427581787 	 0.2054457664489746 	 0.0749819278717041 	 7.200241088867188e-05 	 combined
2025-07-24 21:28:27.321098 test begin: paddle.strided_slice(x=Tensor([3, 4, 5, 423361],"float64"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
[Prof] paddle.strided_slice 	 paddle.strided_slice(x=Tensor([3, 4, 5, 423361],"float64"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], ) 	 25401660 	 1000 	 0.005700111389160156 	 0.2055344581604004 	 1.3589859008789062e-05 	 6.508827209472656e-05 	 0.14706683158874512 	 0.19697999954223633 	 0.07506656646728516 	 0.0001857280731201172 	 combined
2025-07-24 21:28:28.533521 test begin: paddle.strided_slice(x=Tensor([3, 4, 5, 846721],"float32"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
[Prof] paddle.strided_slice 	 paddle.strided_slice(x=Tensor([3, 4, 5, 846721],"float32"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], ) 	 50803260 	 1000 	 0.00577998161315918 	 0.20959973335266113 	 1.2636184692382812e-05 	 6.270408630371094e-05 	 0.14733386039733887 	 0.2011890411376953 	 0.07520508766174316 	 7.033348083496094e-05 	 combined
2025-07-24 21:28:30.184836 test begin: paddle.strided_slice(x=Tensor([3, 4, 5, 846721],"int32"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
[Prof] paddle.strided_slice 	 paddle.strided_slice(x=Tensor([3, 4, 5, 846721],"int32"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], ) 	 50803260 	 1000 	 0.005765438079833984 	 0.2104041576385498 	 1.2159347534179688e-05 	 6.651878356933594e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:28:31.292618 test begin: paddle.strided_slice(x=Tensor([3, 4, 705601, 6],"float32"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
[Prof] paddle.strided_slice 	 paddle.strided_slice(x=Tensor([3, 4, 705601, 6],"float32"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], ) 	 50803272 	 1000 	 0.0057942867279052734 	 0.2089831829071045 	 8.344650268554688e-06 	 6.29425048828125e-05 	 0.14726614952087402 	 0.19404911994934082 	 0.07520699501037598 	 7.319450378417969e-05 	 combined
2025-07-24 21:28:32.819239 test begin: paddle.strided_slice(x=Tensor([3, 4, 705601, 6],"int32"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
[Prof] paddle.strided_slice 	 paddle.strided_slice(x=Tensor([3, 4, 705601, 6],"int32"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], ) 	 50803272 	 1000 	 0.009458303451538086 	 0.2115027904510498 	 1.1920928955078125e-05 	 6.318092346191406e-05 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:28:33.919209 test begin: paddle.strided_slice(x=Tensor([423361, 4, 5, 6],"int32"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], )
[Prof] paddle.strided_slice 	 paddle.strided_slice(x=Tensor([423361, 4, 5, 6],"int32"), axes=list[1,2,3,], starts=list[-3,0,2,], ends=list[3,2,4,], strides=list[1,1,1,], ) 	 50803320 	 1000 	 0.00573420524597168 	 0.2862560749053955 	 6.9141387939453125e-06 	 0.023733854293823242 	 None 	 None 	 None 	 None 	 combined
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:28:35.316225 test begin: paddle.subtract(Tensor([24904, 12, 170, 1],"float32"), Tensor([24904, 12, 170, 1],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([24904, 12, 170, 1],"float32"), Tensor([24904, 12, 170, 1],"float32"), ) 	 101608320 	 1000 	 1.20261549949646 	 0.4544675350189209 	 0.44087862968444824 	 0.4350149631500244 	 0.47324490547180176 	 0.2992708683013916 	 0.4156351089477539 	 0.22664809226989746 	 
2025-07-24 21:28:41.354624 test begin: paddle.subtract(Tensor([3548, 12, 1194, 1],"float32"), Tensor([3548, 12, 1194, 1],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([3548, 12, 1194, 1],"float32"), Tensor([3548, 12, 1194, 1],"float32"), ) 	 101671488 	 1000 	 0.45078349113464355 	 0.46218132972717285 	 0.44116878509521484 	 0.4367709159851074 	 0.4733738899230957 	 0.297914981842041 	 0.41576075553894043 	 0.22385334968566895 	 
2025-07-24 21:28:46.030770 test begin: paddle.subtract(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 8],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 8],"float32"), ) 	 65141280 	 1000 	 0.3593287467956543 	 0.37404751777648926 	 0.3476402759552002 	 0.3587379455566406 	 0.8837037086486816 	 0.898188591003418 	 0.4513511657714844 	 0.4588658809661865 	 
2025-07-24 21:28:51.032766 test begin: paddle.subtract(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 1],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 1],"float32"), ) 	 65141280 	 1000 	 0.357940673828125 	 0.37375736236572266 	 0.34575653076171875 	 0.35868239402770996 	 0.8200273513793945 	 0.8984067440032959 	 0.2801969051361084 	 0.45905089378356934 	 
2025-07-24 21:28:55.923525 test begin: paddle.subtract(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 8],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([3548, 12, 170, 8],"float32"), Tensor([3548, 12, 170, 8],"float32"), ) 	 115806720 	 1000 	 0.5126345157623291 	 0.5084514617919922 	 0.5030105113983154 	 0.4968729019165039 	 0.5402209758758545 	 0.3385310173034668 	 0.48249101638793945 	 0.26302433013916016 	 
2025-07-24 21:29:01.331888 test begin: paddle.subtract(Tensor([3548, 85, 170, 1],"float32"), Tensor([3548, 85, 170, 1],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([3548, 85, 170, 1],"float32"), Tensor([3548, 85, 170, 1],"float32"), ) 	 102537200 	 1000 	 0.45470619201660156 	 0.45072126388549805 	 0.44509220123291016 	 0.43941736221313477 	 0.4792635440826416 	 0.3005344867706299 	 0.42077016830444336 	 0.2279517650604248 	 
2025-07-24 21:29:06.033042 test begin: paddle.subtract(Tensor([517, 4, 3, 64, 128],"float32"), Tensor([517, 4, 3, 64, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([517, 4, 3, 64, 128],"float32"), Tensor([517, 4, 3, 64, 128],"float32"), ) 	 101646336 	 1000 	 0.451979398727417 	 0.44832372665405273 	 0.4410824775695801 	 0.43487119674682617 	 0.47330546379089355 	 0.2978222370147705 	 0.4144906997680664 	 0.2247180938720703 	 
2025-07-24 21:29:10.743731 test begin: paddle.subtract(Tensor([64, 3, 3, 64, 1379],"float32"), Tensor([64, 3, 3, 64, 1379],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 3, 3, 64, 1379],"float32"), Tensor([64, 3, 3, 64, 1379],"float32"), ) 	 101670912 	 1000 	 0.450944185256958 	 0.449298620223999 	 0.433962345123291 	 0.4286031723022461 	 0.4734325408935547 	 0.29796576499938965 	 0.40615320205688477 	 0.19540190696716309 	 
2025-07-24 21:29:14.953555 test begin: paddle.subtract(Tensor([64, 3, 3, 690, 128],"float32"), Tensor([64, 3, 3, 690, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 3, 3, 690, 128],"float32"), Tensor([64, 3, 3, 690, 128],"float32"), ) 	 101744640 	 1000 	 0.45124268531799316 	 0.44733643531799316 	 0.43790721893310547 	 0.43565845489501953 	 0.4774191379547119 	 0.29813456535339355 	 0.41955065727233887 	 0.22373104095458984 	 
2025-07-24 21:29:19.644081 test begin: paddle.subtract(Tensor([64, 3, 33, 64, 128],"float32"), Tensor([64, 3, 33, 64, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 3, 33, 64, 128],"float32"), Tensor([64, 3, 33, 64, 128],"float32"), ) 	 103809024 	 1000 	 0.4637601375579834 	 0.46102094650268555 	 0.450183629989624 	 0.4459865093231201 	 0.48401975631713867 	 0.3040883541107178 	 0.4246084690093994 	 0.22998642921447754 	 
2025-07-24 21:29:26.785179 test begin: paddle.subtract(Tensor([64, 33, 3, 64, 128],"float32"), Tensor([64, 33, 3, 64, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 33, 3, 64, 128],"float32"), Tensor([64, 33, 3, 64, 128],"float32"), ) 	 103809024 	 1000 	 0.46018433570861816 	 0.4742894172668457 	 0.45044612884521484 	 0.44588565826416016 	 0.4853546619415283 	 0.30411386489868164 	 0.4248533248901367 	 0.23041057586669922 	 
2025-07-24 21:29:31.628304 test begin: paddle.subtract(Tensor([64, 4, 25, 64, 128],"float32"), Tensor([64, 4, 25, 64, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 4, 25, 64, 128],"float32"), Tensor([64, 4, 25, 64, 128],"float32"), ) 	 104857600 	 1000 	 0.46468305587768555 	 0.6920108795166016 	 0.45481061935424805 	 0.4503624439239502 	 0.488323450088501 	 0.3069920539855957 	 0.4214134216308594 	 0.23374223709106445 	 
2025-07-24 21:29:40.074898 test begin: paddle.subtract(Tensor([64, 4, 3, 517, 128],"float32"), Tensor([64, 4, 3, 517, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 4, 3, 517, 128],"float32"), Tensor([64, 4, 3, 517, 128],"float32"), ) 	 101646336 	 1000 	 0.45159339904785156 	 0.44749927520751953 	 0.44093966484069824 	 0.43527674674987793 	 0.4737567901611328 	 0.297776460647583 	 0.4051485061645508 	 0.19399309158325195 	 
2025-07-24 21:29:44.735037 test begin: paddle.subtract(Tensor([64, 4, 3, 64, 1034],"float32"), Tensor([64, 4, 3, 64, 1034],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([64, 4, 3, 64, 1034],"float32"), Tensor([64, 4, 3, 64, 1034],"float32"), ) 	 101646336 	 1000 	 0.45074892044067383 	 0.446826696395874 	 0.4409341812133789 	 0.4353673458099365 	 0.4732789993286133 	 0.2977602481842041 	 0.4154636859893799 	 0.2241683006286621 	 
2025-07-24 21:29:49.406163 test begin: paddle.subtract(Tensor([690, 3, 3, 64, 128],"float32"), Tensor([690, 3, 3, 64, 128],"float32"), )
[Prof] paddle.subtract 	 paddle.subtract(Tensor([690, 3, 3, 64, 128],"float32"), Tensor([690, 3, 3, 64, 128],"float32"), ) 	 101744640 	 1000 	 0.4511144161224365 	 0.447437047958374 	 0.44140195846557617 	 0.43544554710388184 	 0.47524094581604004 	 0.29814815521240234 	 0.4174611568450928 	 0.22395610809326172 	 
2025-07-24 21:29:54.015547 test begin: paddle.sum(Tensor([3544, 32, 896],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([3544, 32, 896],"bfloat16"), axis=1, keepdim=False, ) 	 101613568 	 1000 	 0.17210793495178223 	 0.15982675552368164 	 0.15977787971496582 	 0.14513397216796875 	 0.26853179931640625 	 0.08901786804199219 	 0.20647048950195312 	 6.103515625e-05 	 
2025-07-24 21:29:56.794223 test begin: paddle.sum(Tensor([6017, 19, 896],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6017, 19, 896],"bfloat16"), axis=1, keepdim=False, ) 	 102433408 	 1000 	 0.17607951164245605 	 0.17517828941345215 	 0.1633131504058838 	 0.1600353717803955 	 0.2711796760559082 	 0.08865857124328613 	 0.2044072151184082 	 5.7697296142578125e-05 	 
2025-07-24 21:29:59.663243 test begin: paddle.sum(Tensor([6017, 32, 528],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6017, 32, 528],"bfloat16"), axis=1, keepdim=False, ) 	 101663232 	 1000 	 0.18401718139648438 	 0.16230487823486328 	 0.17143845558166504 	 0.14746856689453125 	 0.2688281536102295 	 0.08406519889831543 	 0.20935702323913574 	 5.7697296142578125e-05 	 
2025-07-24 21:30:02.379480 test begin: paddle.sum(Tensor([6036, 19, 896],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6036, 19, 896],"bfloat16"), axis=1, keepdim=False, ) 	 102756864 	 1000 	 0.1763296127319336 	 0.17553353309631348 	 0.16384625434875488 	 0.16047263145446777 	 0.2718505859375 	 0.08433723449707031 	 0.21246719360351562 	 5.555152893066406e-05 	 
2025-07-24 21:30:05.241346 test begin: paddle.sum(Tensor([6036, 32, 527],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6036, 32, 527],"bfloat16"), axis=1, keepdim=False, ) 	 101791104 	 1000 	 0.18662738800048828 	 0.17980241775512695 	 0.17417263984680176 	 0.1650378704071045 	 0.26923465728759766 	 0.08257007598876953 	 0.2098698616027832 	 5.173683166503906e-05 	 
2025-07-24 21:30:08.057835 test begin: paddle.sum(Tensor([6078, 19, 896],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6078, 19, 896],"bfloat16"), axis=1, keepdim=False, ) 	 103471872 	 1000 	 0.1782679557800293 	 0.17731690406799316 	 0.16481637954711914 	 0.16173148155212402 	 0.27374267578125 	 0.08767104148864746 	 0.21355795860290527 	 7.653236389160156e-05 	 
2025-07-24 21:30:10.943200 test begin: paddle.sum(Tensor([6078, 32, 523],"bfloat16"), axis=1, keepdim=False, )
[Prof] paddle.sum 	 paddle.sum(Tensor([6078, 32, 523],"bfloat16"), axis=1, keepdim=False, ) 	 101721408 	 1000 	 0.1835036277770996 	 0.17581820487976074 	 0.16999292373657227 	 0.1611316204071045 	 0.27045273780822754 	 0.08456254005432129 	 0.21111130714416504 	 6.103515625e-05 	 
2025-07-24 21:30:13.730438 test begin: paddle.t(Tensor([10, 5080321],"float32"), )
[Prof] paddle.t 	 paddle.t(Tensor([10, 5080321],"float32"), ) 	 50803210 	 1000 	 0.004196643829345703 	 0.003643512725830078 	 7.867813110351562e-06 	 1.71661376953125e-05 	 0.04059934616088867 	 0.07267594337463379 	 4.7206878662109375e-05 	 6.103515625e-05 	 
2025-07-24 21:30:15.860646 test begin: paddle.t(Tensor([20, 2540161],"float32"), )
[Prof] paddle.t 	 paddle.t(Tensor([20, 2540161],"float32"), ) 	 50803220 	 1000 	 0.004217386245727539 	 0.003618478775024414 	 6.67572021484375e-06 	 1.8596649169921875e-05 	 0.06306982040405273 	 0.057163238525390625 	 3.4332275390625e-05 	 5.793571472167969e-05 	 
2025-07-24 21:30:17.959505 test begin: paddle.t(Tensor([2540161, 20],"float32"), )
[Prof] paddle.t 	 paddle.t(Tensor([2540161, 20],"float32"), ) 	 50803220 	 1000 	 0.004186868667602539 	 0.003650665283203125 	 7.62939453125e-06 	 1.7404556274414062e-05 	 0.03960299491882324 	 0.057697296142578125 	 2.1457672119140625e-05 	 0.00015091896057128906 	 
2025-07-24 21:30:20.019187 test begin: paddle.t(Tensor([49613, 512],"int64"), )
[Prof] paddle.t 	 paddle.t(Tensor([49613, 512],"int64"), ) 	 25401856 	 1000 	 0.004189491271972656 	 0.0036695003509521484 	 7.3909759521484375e-06 	 1.811981201171875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:30:21.157248 test begin: paddle.t(Tensor([5080321, 10],"float32"), )
[Prof] paddle.t 	 paddle.t(Tensor([5080321, 10],"float32"), ) 	 50803210 	 1000 	 0.00416874885559082 	 0.003751993179321289 	 6.9141387939453125e-06 	 2.0742416381835938e-05 	 0.039707183837890625 	 0.056684255599975586 	 2.47955322265625e-05 	 6.532669067382812e-05 	 
2025-07-24 21:30:23.224288 test begin: paddle.t(Tensor([512, 49613],"int64"), )
[Prof] paddle.t 	 paddle.t(Tensor([512, 49613],"int64"), ) 	 25401856 	 1000 	 0.004194021224975586 	 0.0036416053771972656 	 6.67572021484375e-06 	 1.7642974853515625e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 21:30:24.362316 test begin: paddle.take(Tensor([12700801, 4],"float32"), Tensor([2, 12700801],"int64"), mode="raise", )
[Prof] paddle.take 	 paddle.take(Tensor([12700801, 4],"float32"), Tensor([2, 12700801],"int64"), mode="raise", ) 	 76204806 	 1000 	 3.1993632316589355 	 2.915081024169922 	 0.6534206867218018 	 0.4248230457305908 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:30:40.236700 test begin: paddle.take(Tensor([12700801, 4],"float32"), Tensor([2, 3],"int64"), mode="raise", )
[Prof] paddle.take 	 paddle.take(Tensor([12700801, 4],"float32"), Tensor([2, 3],"int64"), mode="raise", ) 	 50803210 	 1000 	 0.11939454078674316 	 0.12410998344421387 	 4.076957702636719e-05 	 0.0001163482666015625 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:30:41.596333 test begin: paddle.take(Tensor([12700801, 4],"float32"), Tensor([6350401, 3],"int64"), mode="raise", )
[Prof] paddle.take 	 paddle.take(Tensor([12700801, 4],"float32"), Tensor([6350401, 3],"int64"), mode="raise", ) 	 69854407 	 1000 	 2.402104616165161 	 2.1984827518463135 	 0.49152612686157227 	 0.3205385208129883 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:30:51.737176 test begin: paddle.take(Tensor([3, 16934401],"float32"), Tensor([2, 3],"int64"), mode="raise", )
[Prof] paddle.take 	 paddle.take(Tensor([3, 16934401],"float32"), Tensor([2, 3],"int64"), mode="raise", ) 	 50803209 	 1000 	 0.08541440963745117 	 0.14687871932983398 	 2.1219253540039062e-05 	 9.417533874511719e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:30:53.114714 test begin: paddle.take(Tensor([3, 16934401],"float32"), Tensor([2, 8467201],"int64"), mode="raise", )
[Prof] paddle.take 	 paddle.take(Tensor([3, 16934401],"float32"), Tensor([2, 8467201],"int64"), mode="raise", ) 	 67737605 	 1000 	 2.14104962348938 	 1.956336498260498 	 0.43807339668273926 	 0.2856285572052002 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:31:02.320260 test begin: paddle.take(Tensor([3, 4],"float32"), Tensor([2, 12700801],"int64"), mode="raise", )
[Prof] paddle.take 	 paddle.take(Tensor([3, 4],"float32"), Tensor([2, 12700801],"int64"), mode="raise", ) 	 25401614 	 1000 	 1.5461649894714355 	 1.142284870147705 	 0.3156404495239258 	 0.16675829887390137 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:31:16.761018 test begin: paddle.take(Tensor([3, 4],"float32"), Tensor([8467201, 3],"int64"), mode="raise", )
[Prof] paddle.take 	 paddle.take(Tensor([3, 4],"float32"), Tensor([8467201, 3],"int64"), mode="raise", ) 	 25401615 	 1000 	 1.5430316925048828 	 1.1465752124786377 	 0.315565824508667 	 0.16684246063232422 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:31:33.548742 test begin: paddle.take(Tensor([3, 4],"float64"), Tensor([3175201, 8],"int64"), mode="clip", )
[Prof] paddle.take 	 paddle.take(Tensor([3, 4],"float64"), Tensor([3175201, 8],"int64"), mode="clip", ) 	 25401620 	 1000 	 0.6533780097961426 	 0.6033339500427246 	 0.3288750648498535 	 0.3081789016723633 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:31:51.458080 test begin: paddle.take(Tensor([3, 4],"float64"), Tensor([3175201, 8],"int64"), mode="wrap", )
[Prof] paddle.take 	 paddle.take(Tensor([3, 4],"float64"), Tensor([3175201, 8],"int64"), mode="wrap", ) 	 25401620 	 1000 	 3.346818208694458 	 1.2022755146026611 	 0.3108522891998291 	 0.3083016872406006 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:32:12.585635 test begin: paddle.take(Tensor([3, 4],"float64"), Tensor([5, 5080321],"int64"), mode="clip", )
[Prof] paddle.take 	 paddle.take(Tensor([3, 4],"float64"), Tensor([5, 5080321],"int64"), mode="clip", ) 	 25401617 	 1000 	 0.6438446044921875 	 0.6033408641815186 	 0.32883572578430176 	 0.3081817626953125 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:32:30.528430 test begin: paddle.take(Tensor([3, 4],"float64"), Tensor([5, 5080321],"int64"), mode="wrap", )
[Prof] paddle.take 	 paddle.take(Tensor([3, 4],"float64"), Tensor([5, 5080321],"int64"), mode="wrap", ) 	 25401617 	 1000 	 3.344148635864258 	 1.2020142078399658 	 0.31102943420410156 	 0.30671095848083496 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:32:51.793277 test begin: paddle.take(Tensor([3, 8467201],"float64"), Tensor([5, 8467201],"int64"), mode="clip", )
[Prof] paddle.take 	 paddle.take(Tensor([3, 8467201],"float64"), Tensor([5, 8467201],"int64"), mode="clip", ) 	 67737608 	 1000 	 3.9864745140075684 	 4.008864164352417 	 2.0368595123291016 	 2.0497775077819824 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:33:10.723577 test begin: paddle.take(Tensor([3, 8467201],"float64"), Tensor([5, 8467201],"int64"), mode="wrap", )
[Prof] paddle.take 	 paddle.take(Tensor([3, 8467201],"float64"), Tensor([5, 8467201],"int64"), mode="wrap", ) 	 67737608 	 1000 	 8.46105170249939 	 4.996245861053467 	 0.786149263381958 	 1.2772538661956787 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:33:35.193588 test begin: paddle.take(Tensor([3, 8467201],"float64"), Tensor([5, 8],"int64"), mode="clip", )
[Prof] paddle.take 	 paddle.take(Tensor([3, 8467201],"float64"), Tensor([5, 8],"int64"), mode="clip", ) 	 25401643 	 1000 	 0.05815005302429199 	 0.043692827224731445 	 3.170967102050781e-05 	 8.749961853027344e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:33:39.413316 test begin: paddle.take(Tensor([3, 8467201],"float64"), Tensor([5, 8],"int64"), mode="wrap", )
[Prof] paddle.take 	 paddle.take(Tensor([3, 8467201],"float64"), Tensor([5, 8],"int64"), mode="wrap", ) 	 25401643 	 1000 	 0.1506669521331787 	 0.07245802879333496 	 2.6702880859375e-05 	 0.00010132789611816406 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:33:40.511854 test begin: paddle.take(Tensor([6350401, 4],"float64"), Tensor([5, 8],"int64"), mode="clip", )
[Prof] paddle.take 	 paddle.take(Tensor([6350401, 4],"float64"), Tensor([5, 8],"int64"), mode="clip", ) 	 25401644 	 1000 	 0.05654478073120117 	 0.04315900802612305 	 2.384185791015625e-05 	 9.179115295410156e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:33:41.431409 test begin: paddle.take(Tensor([6350401, 4],"float64"), Tensor([5, 8],"int64"), mode="wrap", )
[Prof] paddle.take 	 paddle.take(Tensor([6350401, 4],"float64"), Tensor([5, 8],"int64"), mode="wrap", ) 	 25401644 	 1000 	 0.14735722541809082 	 0.07380247116088867 	 2.5510787963867188e-05 	 9.632110595703125e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:33:42.474320 test begin: paddle.take(Tensor([6350401, 4],"float64"), Tensor([6350401, 8],"int64"), mode="clip", )
[Prof] paddle.take 	 paddle.take(Tensor([6350401, 4],"float64"), Tensor([6350401, 8],"int64"), mode="clip", ) 	 76204812 	 1000 	 4.779559135437012 	 4.808224678039551 	 2.441319704055786 	 2.4553310871124268 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:34:04.973131 test begin: paddle.take(Tensor([6350401, 4],"float64"), Tensor([6350401, 8],"int64"), mode="wrap", )
[Prof] paddle.take 	 paddle.take(Tensor([6350401, 4],"float64"), Tensor([6350401, 8],"int64"), mode="wrap", ) 	 76204812 	 1000 	 10.140405416488647 	 5.997623682022095 	 0.9422237873077393 	 1.5335109233856201 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:34:34.282711 test begin: paddle.take_along_axis(Tensor([1024, 384],"float32"), Tensor([1024, 24807],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([1024, 384],"float32"), Tensor([1024, 24807],"int64"), axis=-1, ) 	 25795584 	 1000 	 1.783984899520874 	 0.24640369415283203 	 0.19780182838439941 	 0.22223114967346191 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:34:42.149102 test begin: paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 24807],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 24807],"int64"), axis=-1, ) 	 76206080 	 1000 	 1.0045578479766846 	 0.44062256813049316 	 0.3416719436645508 	 0.4142110347747803 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:34:48.342015 test begin: paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 7],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 7],"int64"), axis=-1, ) 	 50810880 	 1000 	 0.3050961494445801 	 0.018478870391845703 	 0.10373187065124512 	 6.532669067382812e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:34:50.204609 test begin: paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 8],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([1024, 49613],"float32"), Tensor([1024, 8],"int64"), axis=-1, ) 	 50811904 	 1000 	 0.3044121265411377 	 0.016785383224487305 	 0.10373735427856445 	 4.1961669921875e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:34:52.006085 test begin: paddle.take_along_axis(Tensor([1051, 63, 768],"float32"), axis=1, indices=Tensor([1051, 7, 768],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([1051, 63, 768],"float32"), axis=1, indices=Tensor([1051, 7, 768],"int64"), ) 	 56501760 	 1000 	 0.5571472644805908 	 0.3169384002685547 	 0.18823790550231934 	 0.2878122329711914 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:34:57.343728 test begin: paddle.take_along_axis(Tensor([132301, 384],"float32"), Tensor([132301, 7],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([132301, 384],"float32"), Tensor([132301, 7],"int64"), axis=-1, ) 	 51729691 	 1000 	 0.3643155097961426 	 0.05735588073730469 	 0.12411165237426758 	 0.029767513275146484 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:34:59.183603 test begin: paddle.take_along_axis(Tensor([132301, 384],"float32"), Tensor([132301, 8],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([132301, 384],"float32"), Tensor([132301, 8],"int64"), axis=-1, ) 	 51861992 	 1000 	 0.3743624687194824 	 0.06316089630126953 	 0.12651371955871582 	 0.03948163986206055 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:35:01.220983 test begin: paddle.take_along_axis(Tensor([3175201, 384],"float32"), Tensor([3175201, 8],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([3175201, 384],"float32"), Tensor([3175201, 8],"int64"), axis=-1, ) 	 1244678792 	 1000 	 8.615495443344116 	 1.3801071643829346 	 2.9348695278167725 	 0.3512704372406006 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:35:52.591444 test begin: paddle.take_along_axis(Tensor([3628801, 384],"float32"), Tensor([3628801, 7],"int64"), axis=-1, )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([3628801, 384],"float32"), Tensor([3628801, 7],"int64"), axis=-1, ) 	 1418861191 	 1000 	 9.654997110366821 	 1.4243619441986084 	 3.2920587062835693 	 0.36270833015441895 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:36:52.978839 test begin: paddle.take_along_axis(Tensor([4726, 63, 768],"float32"), axis=1, indices=Tensor([4726, 7, 768],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([4726, 63, 768],"float32"), axis=1, indices=Tensor([4726, 7, 768],"int64"), ) 	 254069760 	 1000 	 2.4297471046447754 	 1.3812105655670166 	 0.829193115234375 	 1.3615624904632568 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:37:07.335131 test begin: paddle.take_along_axis(Tensor([8, 63, 100801],"float32"), axis=1, indices=Tensor([8, 7, 100801],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([8, 63, 100801],"float32"), axis=1, indices=Tensor([8, 7, 100801],"int64"), ) 	 56448560 	 1000 	 0.5711166858673096 	 0.31007862091064453 	 0.194685697555542 	 0.2903120517730713 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:37:10.367675 test begin: paddle.take_along_axis(Tensor([8, 63, 453601],"float32"), axis=1, indices=Tensor([8, 7, 453601],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([8, 63, 453601],"float32"), axis=1, indices=Tensor([8, 7, 453601],"int64"), ) 	 254016560 	 1000 	 3.037808656692505 	 1.592947006225586 	 1.03450345993042 	 1.575089693069458 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:37:27.001083 test begin: paddle.take_along_axis(Tensor([8, 63, 768],"float32"), axis=1, indices=Tensor([8, 4135, 768],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([8, 63, 768],"float32"), axis=1, indices=Tensor([8, 4135, 768],"int64"), ) 	 25792512 	 1000 	 0.5998246669769287 	 0.28625988960266113 	 0.2042391300201416 	 0.267559289932251 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:37:30.858324 test begin: paddle.take_along_axis(Tensor([8, 8269, 768],"float32"), axis=1, indices=Tensor([8, 4135, 768],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([8, 8269, 768],"float32"), axis=1, indices=Tensor([8, 4135, 768],"int64"), ) 	 76210176 	 1000 	 1.351118803024292 	 0.9572577476501465 	 0.46042394638061523 	 0.9318585395812988 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:37:39.817773 test begin: paddle.take_along_axis(Tensor([8, 8269, 768],"float32"), axis=1, indices=Tensor([8, 7, 768],"int64"), )
[Prof] paddle.take_along_axis 	 paddle.take_along_axis(Tensor([8, 8269, 768],"float32"), axis=1, indices=Tensor([8, 7, 768],"int64"), ) 	 50847744 	 1000 	 0.3072082996368408 	 0.01690506935119629 	 0.10475873947143555 	 3.6716461181640625e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 21:37:41.533003 test begin: paddle.tan(Tensor([8, 16, 396901],"float32"), )
[Prof] paddle.tan 	 paddle.tan(Tensor([8, 16, 396901],"float32"), ) 	 50803328 	 1000 	 0.29485177993774414 	 0.3064870834350586 	 0.28625917434692383 	 0.28746867179870605 	 0.45119547843933105 	 1.0417253971099854 	 0.39806461334228516 	 0.3557286262512207 	 
2025-07-24 21:37:45.226179 test begin: paddle.tan(Tensor([8, 198451, 32],"float32"), )
[Prof] paddle.tan 	 paddle.tan(Tensor([8, 198451, 32],"float32"), ) 	 50803456 	 1000 	 0.2960209846496582 	 0.3028407096862793 	 0.2874565124511719 	 0.2875375747680664 	 0.4508242607116699 	 1.0403542518615723 	 0.3973221778869629 	 0.3544032573699951 	 
2025-07-24 21:37:48.951849 test begin: paddle.tan(Tensor([99226, 16, 32],"float32"), )
[Prof] paddle.tan 	 paddle.tan(Tensor([99226, 16, 32],"float32"), ) 	 50803712 	 1000 	 0.29479265213012695 	 0.2980923652648926 	 0.28639841079711914 	 0.287494421005249 	 0.4522528648376465 	 1.040452003479004 	 0.4001345634460449 	 0.35444116592407227 	 
2025-07-24 21:37:52.653932 test begin: paddle.tanh(Tensor([16, 125, 25500],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([16, 125, 25500],"float32"), ) 	 51000000 	 1000 	 0.2963736057281494 	 0.29913926124572754 	 0.2876443862915039 	 0.288316011428833 	 0.45099949836730957 	 0.44976067543029785 	 0.3978443145751953 	 0.38332581520080566 	 
2025-07-24 21:37:55.870494 test begin: paddle.tanh(Tensor([16, 64, 49613],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([16, 64, 49613],"float32"), ) 	 50803712 	 1000 	 0.29508423805236816 	 0.2981455326080322 	 0.2863459587097168 	 0.28769898414611816 	 0.4491438865661621 	 0.4478747844696045 	 0.39581871032714844 	 0.3818845748901367 	 
2025-07-24 21:37:59.065681 test begin: paddle.tanh(Tensor([28, 32, 241, 241],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([28, 32, 241, 241],"float32"), ) 	 52040576 	 1000 	 0.3023958206176758 	 0.3051319122314453 	 0.2937507629394531 	 0.2940971851348877 	 0.4600334167480469 	 0.45738649368286133 	 0.4064490795135498 	 0.38220715522766113 	 
2025-07-24 21:38:02.300354 test begin: paddle.tanh(Tensor([32, 64, 25500],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([32, 64, 25500],"float32"), ) 	 52224000 	 1000 	 0.30464744567871094 	 0.3210413455963135 	 0.29587411880493164 	 0.2969822883605957 	 0.46144533157348633 	 0.4589700698852539 	 0.407381534576416 	 0.3894011974334717 	 
2025-07-24 21:38:05.593827 test begin: paddle.tanh(Tensor([64, 26, 512, 1, 60],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([64, 26, 512, 1, 60],"float32"), ) 	 51118080 	 1000 	 0.29681825637817383 	 0.7535741329193115 	 0.288210391998291 	 0.28911590576171875 	 0.45165491104125977 	 0.45092225074768066 	 0.39842939376831055 	 0.3821592330932617 	 
2025-07-24 21:38:11.310707 test begin: paddle.tanh(Tensor([64, 26, 512, 2, 40],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([64, 26, 512, 2, 40],"float32"), ) 	 68157440 	 1000 	 0.39595866203308105 	 0.40112781524658203 	 0.38704419136047363 	 0.3883790969848633 	 0.6007940769195557 	 0.5975246429443359 	 0.5475485324859619 	 0.5255489349365234 	 
2025-07-24 21:38:15.464049 test begin: paddle.tanh(Tensor([64, 26, 764, 1, 40],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([64, 26, 764, 1, 40],"float32"), ) 	 50851840 	 1000 	 0.29545044898986816 	 0.29840898513793945 	 0.2867097854614258 	 0.2878072261810303 	 0.44936108589172363 	 0.4469592571258545 	 0.39606404304504395 	 0.3779613971710205 	 
2025-07-24 21:38:18.578941 test begin: paddle.tanh(Tensor([64, 39, 512, 1, 40],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([64, 39, 512, 1, 40],"float32"), ) 	 51118080 	 1000 	 0.29680895805358887 	 0.30002927780151367 	 0.28540730476379395 	 0.28901100158691406 	 0.4516158103942871 	 0.45076584815979004 	 0.39812374114990234 	 0.383319616317749 	 
2025-07-24 21:38:21.789830 test begin: paddle.tanh(Tensor([8, 110, 241, 241],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([8, 110, 241, 241],"float32"), ) 	 51111280 	 1000 	 0.2968876361846924 	 0.2998201847076416 	 0.2881886959075928 	 0.2889587879180908 	 0.4518001079559326 	 0.4492199420928955 	 0.3988511562347412 	 0.3821866512298584 	 
2025-07-24 21:38:24.961381 test begin: paddle.tanh(Tensor([8, 32, 241, 824],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([8, 32, 241, 824],"float32"), ) 	 50837504 	 1000 	 0.29671192169189453 	 0.29834461212158203 	 0.28815245628356934 	 0.2875328063964844 	 0.4493858814239502 	 0.4482076168060303 	 0.3959805965423584 	 0.38107728958129883 	 
2025-07-24 21:38:28.123481 test begin: paddle.tanh(Tensor([8, 32, 824, 241],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([8, 32, 824, 241],"float32"), ) 	 50837504 	 1000 	 0.29538989067077637 	 0.2983865737915039 	 0.2866783142089844 	 0.28783392906188965 	 0.45084667205810547 	 0.44685983657836914 	 0.39788174629211426 	 0.37830138206481934 	 
2025-07-24 21:38:31.403098 test begin: paddle.tanh(Tensor([96, 26, 512, 1, 40],"float32"), )
[Prof] paddle.tanh 	 paddle.tanh(Tensor([96, 26, 512, 1, 40],"float32"), ) 	 51118080 	 1000 	 0.29681897163391113 	 0.30173373222351074 	 0.288161039352417 	 0.2890443801879883 	 0.45177555084228516 	 0.44930100440979004 	 0.3985929489135742 	 0.38027119636535645 	 
2025-07-24 21:38:34.640228 test begin: paddle.tensor_split(Tensor([226801, 4, 4, 7],"int64"), list[2,3,], axis=3, )
W0724 21:38:35.379941 14610 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:38:35.402371 test begin: paddle.tensor_split(Tensor([226801, 4, 4, 7],"int64"), list[2,4,6,], axis=3, )
W0724 21:38:36.122134 14687 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:38:36.142772 test begin: paddle.tensor_split(Tensor([226801, 4, 4, 7],"int64"), tuple(2,6,), axis=3, )
W0724 21:38:36.859687 14692 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:38:36.880585 test begin: paddle.tensor_split(Tensor([4, 226801, 4, 7],"int64"), list[2,3,], axis=3, )
W0724 21:38:37.587899 14694 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:38:37.608740 test begin: paddle.tensor_split(Tensor([4, 226801, 4, 7],"int64"), list[2,4,6,], axis=3, )
W0724 21:38:38.330286 14696 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:38:38.354631 test begin: paddle.tensor_split(Tensor([4, 226801, 4, 7],"int64"), tuple(2,6,), axis=3, )
W0724 21:38:39.068996 14704 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:38:39.090453 test begin: paddle.tensor_split(Tensor([4, 4, 226801, 7],"int64"), list[2,3,], axis=3, )
W0724 21:38:39.823614 14705 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:38:39.846734 test begin: paddle.tensor_split(Tensor([4, 4, 226801, 7],"int64"), list[2,4,6,], axis=3, )
W0724 21:38:40.581790 14774 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:38:40.604526 test begin: paddle.tensor_split(Tensor([4, 4, 226801, 7],"int64"), tuple(2,6,), axis=3, )
W0724 21:38:41.324700 14782 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:38:41.346853 test begin: paddle.tensor_split(Tensor([4, 4, 4, 396901],"int64"), list[2,3,], axis=3, )
W0724 21:38:42.077231 14857 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:38:42.096153 test begin: paddle.tensor_split(Tensor([4, 4, 4, 396901],"int64"), list[2,4,6,], axis=3, )
W0724 21:38:42.825042 14876 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:38:42.844011 test begin: paddle.tensor_split(Tensor([4, 4, 4, 396901],"int64"), tuple(2,6,), axis=3, )
W0724 21:38:43.586177 14884 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 21:38:43.605690 test begin: paddle.tensordot(Tensor([406426, 5, 5, 5],"float32"), Tensor([406426, 5, 5, 5],"float32"), list[0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([406426, 5, 5, 5],"float32"), Tensor([406426, 5, 5, 5],"float32"), list[0,], ) 	 101606500 	 1000 	 1.0812599658966064 	 0.8465595245361328 	 0.3681821823120117 	 0.433199405670166 	 1.593064308166504 	 1.5998859405517578 	 0.8139166831970215 	 0.8167028427124023 	 
2025-07-24 21:38:50.661473 test begin: paddle.tensordot(Tensor([406426, 5, 5, 5],"float32"), Tensor([406426, 5, 5, 5],"float32"), list[3,0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([406426, 5, 5, 5],"float32"), Tensor([406426, 5, 5, 5],"float32"), list[3,0,], ) 	 101606500 	 1000 	 1.1655693054199219 	 3.5430428981781006 	 0.2975795269012451 	 0.9038128852844238 	 0.791053056716919 	 0.7853786945343018 	 0.2026381492614746 	 0.2010800838470459 	 
2025-07-24 21:38:58.625446 test begin: paddle.tensordot(Tensor([5, 406426, 5, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 406426, 5, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[0,], ) 	 50803875 	 1000 	 5.105818748474121 	 4.125248193740845 	 0.47342658042907715 	 0.4214742183685303 	 11.533353567123413 	 10.655739307403564 	 0.9077153205871582 	 0.90614914894104 	 
2025-07-24 21:39:51.549464 test begin: paddle.tensordot(Tensor([5, 406426, 5, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[3,0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 406426, 5, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[3,0,], ) 	 50803875 	 1000 	 1.42352294921875 	 0.7263240814208984 	 0.3629496097564697 	 0.18549752235412598 	 0.7874195575714111 	 0.7782588005065918 	 0.20077061653137207 	 0.1987762451171875 	 
2025-07-24 21:39:56.977083 test begin: paddle.tensordot(Tensor([5, 5, 406426, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 406426, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[0,], ) 	 50803875 	 1000 	 5.1075615882873535 	 4.1237335205078125 	 0.4747755527496338 	 0.4211559295654297 	 11.55096960067749 	 10.66025161743164 	 0.9085421562194824 	 0.907400369644165 	 
2025-07-24 21:40:50.003506 test begin: paddle.tensordot(Tensor([5, 5, 406426, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[3,0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 406426, 5],"float32"), Tensor([5, 5, 5, 5],"float32"), list[3,0,], ) 	 50803875 	 1000 	 1.4222731590270996 	 0.7278075218200684 	 0.36305809020996094 	 0.18550395965576172 	 0.7874937057495117 	 0.7781057357788086 	 0.2007918357849121 	 0.1987285614013672 	 
2025-07-24 21:40:55.432839 test begin: paddle.tensordot(Tensor([5, 5, 5, 406426],"float32"), Tensor([5, 5, 5, 406426],"float32"), list[3,0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 5, 406426],"float32"), Tensor([5, 5, 5, 406426],"float32"), list[3,0,], ) 	 101606500 	 1000 	 2.101750135421753 	 2.5196480751037598 	 0.5365698337554932 	 0.6437592506408691 	 0.7881367206573486 	 0.7849469184875488 	 0.2013254165649414 	 0.20122861862182617 	 
2025-07-24 21:41:03.338329 test begin: paddle.tensordot(Tensor([5, 5, 5, 406426],"float32"), Tensor([5, 5, 5, 5],"float32"), list[0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 5, 406426],"float32"), Tensor([5, 5, 5, 5],"float32"), list[0,], ) 	 50803875 	 1000 	 5.103352785110474 	 4.127548694610596 	 0.47347092628479004 	 0.42114853858947754 	 11.53517460823059 	 10.658405065536499 	 0.9076335430145264 	 0.9060804843902588 	 
2025-07-24 21:42:00.384812 test begin: paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 406426, 5, 5],"float32"), list[0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 406426, 5, 5],"float32"), list[0,], ) 	 50803875 	 1000 	 4.520212411880493 	 4.588754653930664 	 0.41956472396850586 	 0.466170072555542 	 12.315612077713013 	 12.235038757324219 	 0.0665130615234375 	 0.07186365127563477 	 
2025-07-24 21:42:55.550109 test begin: paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 406426, 5, 5],"float32"), list[3,0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 406426, 5, 5],"float32"), list[3,0,], ) 	 50803875 	 1000 	 0.8629465103149414 	 1.278146743774414 	 0.22039532661437988 	 0.32592010498046875 	 0.7988979816436768 	 0.7965919971466064 	 0.2036418914794922 	 0.20344257354736328 	 
2025-07-24 21:43:00.965087 test begin: paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 5, 406426, 5],"float32"), list[0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 5, 406426, 5],"float32"), list[0,], ) 	 50803875 	 1000 	 4.516392469406128 	 5.460769414901733 	 0.4197394847869873 	 0.4657871723175049 	 12.315595388412476 	 12.231608390808105 	 0.06652212142944336 	 0.07187104225158691 	 
2025-07-24 21:43:58.991393 test begin: paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 5, 406426, 5],"float32"), list[3,0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 5, 406426, 5],"float32"), list[3,0,], ) 	 50803875 	 1000 	 0.8627557754516602 	 1.2767539024353027 	 0.2203383445739746 	 0.32589006423950195 	 0.7986838817596436 	 0.7992105484008789 	 0.2036271095275879 	 0.204697847366333 	 
2025-07-24 21:44:04.412398 test begin: paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 5, 5, 406426],"float32"), list[0,], )
[Prof] paddle.tensordot 	 paddle.tensordot(Tensor([5, 5, 5, 5],"float32"), Tensor([5, 5, 5, 406426],"float32"), list[0,], ) 	 50803875 	 1000 	 4.516380548477173 	 4.576352119445801 	 0.41955089569091797 	 0.46613121032714844 	 12.311675071716309 	 12.233840942382812 	 0.06648421287536621 	 0.07185125350952148 	 
2025-07-24 21:45:01.186130 test begin: paddle.tensordot(x=Tensor([4, 105841, 3, 5, 4],"float64"), y=Tensor([105841, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 105841, 3, 5, 4],"float64"), y=Tensor([105841, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 76205520 	 1000 	 1.6803908348083496 	 1.364732027053833 	 0.4292581081390381 	 0.4655036926269531 	 3.3532886505126953 	 3.6663947105407715 	 0.2444305419921875 	 0.267549991607666 	 
2025-07-24 21:45:13.051329 test begin: paddle.tensordot(x=Tensor([4, 2, 158761, 5, 4],"float64"), y=Tensor([2, 4, 158761, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 158761, 5, 4],"float64"), y=Tensor([2, 4, 158761, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 76205280 	 1000 	 1.6713652610778809 	 1.3531615734100342 	 0.42650556564331055 	 0.46074414253234863 	 3.3337209224700928 	 3.589724063873291 	 0.2429065704345703 	 0.26968836784362793 	 
2025-07-24 21:45:24.577110 test begin: paddle.tensordot(x=Tensor([4, 2, 3, 132301, 4],"float64"), y=Tensor([2, 4, 3, 132301, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 3, 132301, 4],"float64"), y=Tensor([2, 4, 3, 132301, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 38102688 	 1000 	 0.8410210609436035 	 0.6943089962005615 	 0.21479368209838867 	 0.2354874610900879 	 1.673163652420044 	 1.713365077972412 	 0.2132885456085205 	 0.21866798400878906 	 
2025-07-24 21:45:30.305988 test begin: paddle.tensordot(x=Tensor([4, 2, 3, 264601, 4],"float64"), y=Tensor([2, 4, 3, 264601, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 3, 264601, 4],"float64"), y=Tensor([2, 4, 3, 264601, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 76205088 	 1000 	 1.6739118099212646 	 1.3603661060333252 	 0.42686939239501953 	 0.4618244171142578 	 3.330458641052246 	 3.5636048316955566 	 0.2430732250213623 	 0.26151394844055176 	 
2025-07-24 21:45:42.577209 test begin: paddle.tensordot(x=Tensor([4, 2, 3, 5, 211681],"float64"), y=Tensor([2, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 3, 5, 211681],"float64"), y=Tensor([2, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 25402680 	 1000 	 0.5303092002868652 	 0.20372605323791504 	 0.18076467514038086 	 0.10255551338195801 	 0.35195231437683105 	 0.37963008880615234 	 0.11976122856140137 	 0.12868404388427734 	 
2025-07-24 21:45:44.763588 test begin: paddle.tensordot(x=Tensor([4, 2, 3, 5, 4],"float64"), y=Tensor([2, 4, 3, 5, 211681],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 3, 5, 4],"float64"), y=Tensor([2, 4, 3, 5, 211681],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 25402200 	 1000 	 0.4897794723510742 	 0.48812031745910645 	 0.16663241386413574 	 0.24715638160705566 	 0.3804609775543213 	 0.3702738285064697 	 0.12946343421936035 	 0.12597012519836426 	 
2025-07-24 21:45:49.639233 test begin: paddle.tensordot(x=Tensor([4, 2, 79381, 5, 4],"float64"), y=Tensor([2, 4, 79381, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 2, 79381, 5, 4],"float64"), y=Tensor([2, 4, 79381, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 38102880 	 1000 	 0.8440775871276855 	 0.6917884349822998 	 0.21623563766479492 	 0.23551154136657715 	 1.6702213287353516 	 1.7134060859680176 	 0.2131190299987793 	 0.21865415573120117 	 
2025-07-24 21:45:55.612766 test begin: paddle.tensordot(x=Tensor([4, 52921, 3, 5, 4],"float64"), y=Tensor([52921, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], )
[Prof] paddle.tensordot 	 paddle.tensordot(x=Tensor([4, 52921, 3, 5, 4],"float64"), y=Tensor([52921, 4, 3, 5, 8],"float64"), axes=list[list[0,1,2,3,],list[1,0,],], ) 	 38103120 	 1000 	 0.8484766483306885 	 0.6952049732208252 	 0.2167041301727295 	 0.23665261268615723 	 1.6817796230316162 	 1.7893986701965332 	 0.21476006507873535 	 0.22944092750549316 	 
2025-07-24 21:46:01.474234 test begin: paddle.tile(Tensor([102426, 248, 1, 1, 2],"float32"), list[1,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([102426, 248, 1, 1, 2],"float32"), list[1,1,1,1,1,], ) 	 50803296 	 1000 	 0.2960848808288574 	 0.3130214214324951 	 0.2846217155456543 	 0.15985941886901855 	 0.3128969669342041 	 0.0538330078125 	 0.15986132621765137 	 4.00543212890625e-05 	 
2025-07-24 21:46:04.064886 test begin: paddle.tile(Tensor([1511, 10, 1, 58, 58],"float32"), list[1,1,4,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([1511, 10, 1, 58, 58],"float32"), list[1,1,4,1,1,], ) 	 50830040 	 1000 	 1.9697718620300293 	 0.9367170333862305 	 1.005875825881958 	 0.9147346019744873 	 1.8873178958892822 	 0.7326619625091553 	 1.8328022956848145 	 0.653160810470581 	 
2025-07-24 21:46:13.711073 test begin: paddle.tile(Tensor([16, 1, 1, 3, 16538, 64],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 1, 1, 3, 16538, 64],"float32"), list[1,11,1,1,1,1,], ) 	 50804736 	 1000 	 6.068273544311523 	 2.748100519180298 	 3.1032772064208984 	 1.4032113552093506 	 3.364619731903076 	 1.7346446514129639 	 3.310335636138916 	 0.8863532543182373 	 
2025-07-24 21:46:37.896564 test begin: paddle.tile(Tensor([16, 1, 1, 3, 64, 16538],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 1, 1, 3, 64, 16538],"float32"), list[1,11,1,1,1,1,], ) 	 50804736 	 1000 	 6.467729568481445 	 2.750394821166992 	 3.099980354309082 	 1.404646635055542 	 3.3670198917388916 	 1.7346084117889404 	 3.310251474380493 	 0.8862714767456055 	 
2025-07-24 21:47:03.165211 test begin: paddle.tile(Tensor([16, 1, 1, 776, 64, 64],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 1, 1, 776, 64, 64],"float32"), list[1,11,1,1,1,1,], ) 	 50855936 	 1000 	 6.072643280029297 	 2.780944585800171 	 3.103158950805664 	 1.4099528789520264 	 3.364920139312744 	 1.7371621131896973 	 3.310427188873291 	 0.8883407115936279 	 
2025-07-24 21:47:27.216101 test begin: paddle.tile(Tensor([16, 1, 259, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 1, 259, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], ) 	 50921472 	 1000 	 6.082051753997803 	 2.7636256217956543 	 3.1077651977539062 	 1.4108572006225586 	 3.3686163425445557 	 1.740605354309082 	 3.312772035598755 	 0.8880648612976074 	 
2025-07-24 21:47:51.176321 test begin: paddle.tile(Tensor([16, 10, 1, 5475, 58],"float32"), list[1,1,4,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 10, 1, 5475, 58],"float32"), list[1,1,4,1,1,], ) 	 50808000 	 1000 	 1.9315321445465088 	 0.8720822334289551 	 0.9878227710723877 	 0.8319661617279053 	 1.887681484222412 	 0.7478997707366943 	 1.8328428268432617 	 0.6696267127990723 	 
2025-07-24 21:48:01.156021 test begin: paddle.tile(Tensor([16, 10, 1, 58, 5475],"float32"), list[1,1,4,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 10, 1, 58, 5475],"float32"), list[1,1,4,1,1,], ) 	 50808000 	 1000 	 1.9300832748413086 	 0.8674983978271484 	 0.9864075183868408 	 0.8456392288208008 	 1.8891425132751465 	 0.7450754642486572 	 1.8349411487579346 	 0.6708977222442627 	 
2025-07-24 21:48:10.623808 test begin: paddle.tile(Tensor([16, 10, 95, 58, 58],"float32"), list[1,1,4,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 10, 95, 58, 58],"float32"), list[1,1,4,1,1,], ) 	 51132800 	 1000 	 1.9427413940429688 	 0.8731439113616943 	 0.9934592247009277 	 0.8508584499359131 	 1.9016978740692139 	 0.7384719848632812 	 1.8473429679870605 	 0.6643855571746826 	 
2025-07-24 21:48:20.270445 test begin: paddle.tile(Tensor([16, 259, 1, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 259, 1, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], ) 	 50921472 	 1000 	 6.081756591796875 	 2.7896440029144287 	 3.106177806854248 	 1.4109342098236084 	 3.469083309173584 	 1.739532470703125 	 3.412858247756958 	 0.8894927501678467 	 
2025-07-24 21:48:44.867767 test begin: paddle.tile(Tensor([16, 944, 1, 58, 58],"float32"), list[1,1,4,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([16, 944, 1, 58, 58],"float32"), list[1,1,4,1,1,], ) 	 50809856 	 1000 	 1.966247797012329 	 0.9378740787506104 	 1.0047523975372314 	 0.9157705307006836 	 1.8872039318084717 	 0.7324254512786865 	 1.8323609828948975 	 0.6581096649169922 	 
2025-07-24 21:48:54.494625 test begin: paddle.tile(Tensor([216, 117601, 1, 1, 2],"float32"), list[1,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([216, 117601, 1, 1, 2],"float32"), list[1,1,1,1,1,], ) 	 50803632 	 1000 	 0.2959320545196533 	 0.31324028968811035 	 0.28444361686706543 	 0.15993356704711914 	 0.3145582675933838 	 0.05547833442687988 	 0.16069746017456055 	 5.602836608886719e-05 	 
2025-07-24 21:48:57.117321 test begin: paddle.tile(Tensor([216, 248, 1, 1, 949],"float32"), list[1,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([216, 248, 1, 1, 949],"float32"), list[1,1,1,1,1,], ) 	 50836032 	 1000 	 0.2961606979370117 	 0.32724547386169434 	 0.28416872024536133 	 0.15993142127990723 	 0.3158304691314697 	 0.057312965393066406 	 0.1613330841064453 	 4.57763671875e-05 	 
2025-07-24 21:49:01.458766 test begin: paddle.tile(Tensor([216, 248, 1, 475, 2],"float32"), list[1,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([216, 248, 1, 475, 2],"float32"), list[1,1,1,1,1,], ) 	 50889600 	 1000 	 0.29662585258483887 	 0.327925443649292 	 0.28513598442077637 	 0.1601564884185791 	 0.3152790069580078 	 0.06505036354064941 	 0.16100358963012695 	 4.649162292480469e-05 	 
2025-07-24 21:49:04.875816 test begin: paddle.tile(Tensor([216, 248, 475, 1, 2],"float32"), list[1,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([216, 248, 475, 1, 2],"float32"), list[1,1,1,1,1,], ) 	 50889600 	 1000 	 0.29658937454223633 	 0.31360816955566406 	 0.2851588726043701 	 0.16012048721313477 	 0.3151118755340576 	 0.05410337448120117 	 0.16098451614379883 	 3.504753112792969e-05 	 
2025-07-24 21:49:07.502183 test begin: paddle.tile(Tensor([4135, 1, 1, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], )
[Prof] paddle.tile 	 paddle.tile(Tensor([4135, 1, 1, 3, 64, 64],"float32"), list[1,11,1,1,1,1,], ) 	 50810880 	 1000 	 5.006615400314331 	 2.4630515575408936 	 2.556114912033081 	 1.258039951324463 	 3.360415458679199 	 1.7473769187927246 	 3.305891752243042 	 0.8928170204162598 	 
2025-07-24 21:49:30.095435 test begin: paddle.tolist(Tensor([10160641, 5],"float32"), )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f1d4c8929e0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 21:59:43.675140 test begin: paddle.tolist(Tensor([2, 12700801],"int64"), )
W0724 21:59:44.288450 83311 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f5f5bba3070>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753366184 (unix time) try "date -d @1753366184" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14480) received by PID 83072 (TID 0x7f5f571ab640) from PID 83072 ***]

2025-07-24 22:09:52.662098 test begin: paddle.tolist(Tensor([2, 25401601],"float32"), )
W0724 22:09:53.630527 116855 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f0cea0cf010>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753366793 (unix time) try "date -d @1753366793" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c733) received by PID 116531 (TID 0x7f0ce17fb640) from PID 116531 ***]

2025-07-24 22:20:03.050815 test begin: paddle.tolist(Tensor([8467201, 3],"int64"), )
W0724 22:20:03.626503 147406 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f82cc10b010>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 22:30:12.646967 test begin: paddle.topk(Tensor([138, 369303],"float32"), k=1, axis=0, )
W0724 22:30:17.687355 13885 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.topk 	 paddle.topk(Tensor([138, 369303],"float32"), k=1, axis=0, ) 	 50963814 	 1000 	 2.412024736404419 	 8.596932411193848 	 0.6126272678375244 	 8.581035137176514 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:30:35.257464 test begin: paddle.topk(Tensor([146, 349866],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([146, 349866],"float32"), k=1, axis=0, ) 	 51080436 	 1000 	 2.3474550247192383 	 10.821001052856445 	 0.5981335639953613 	 10.805953979492188 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:30:58.917579 test begin: paddle.topk(Tensor([148, 343728],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([148, 343728],"float32"), k=1, axis=0, ) 	 50871744 	 1000 	 2.0921621322631836 	 9.31716775894165 	 0.5328772068023682 	 9.302172899246216 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:31:20.766644 test begin: paddle.topk(Tensor([49, 1036801],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([49, 1036801],"float32"), k=1, axis=0, ) 	 50803249 	 1000 	 2.2325198650360107 	 4.139489412307739 	 0.565983772277832 	 4.121175289154053 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:31:41.282448 test begin: paddle.topk(Tensor([53, 958551],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([53, 958551],"float32"), k=1, axis=0, ) 	 50803203 	 1000 	 2.222240924835205 	 4.200055122375488 	 0.5641119480133057 	 4.184277296066284 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:31:58.652058 test begin: paddle.topk(Tensor([55, 923695],"float32"), k=1, axis=0, )
[Prof] paddle.topk 	 paddle.topk(Tensor([55, 923695],"float32"), k=1, axis=0, ) 	 50803225 	 1000 	 2.2036168575286865 	 4.419393301010132 	 0.5617530345916748 	 4.404548406600952 	 None 	 None 	 None 	 None 	 
[Error] element 1 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:32:16.196819 test begin: paddle.trace(x=Tensor([2, 3, 4233601],"float64"), offset=0, axis1=-3, axis2=-2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([2, 3, 4233601],"float64"), offset=0, axis1=-3, axis2=-2, ) 	 25401606 	 1000 	 0.8005111217498779 	 0.08091592788696289 	 8.869171142578125e-05 	 0.04904484748840332 	 1.6054749488830566 	 0.2406904697418213 	 6.651878356933594e-05 	 0.12283682823181152 	 combined
2025-07-24 22:32:19.631495 test begin: paddle.trace(x=Tensor([2, 3, 4233601],"float64"), offset=1, axis1=0, axis2=2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([2, 3, 4233601],"float64"), offset=1, axis1=0, axis2=2, ) 	 25401606 	 1000 	 0.2698955535888672 	 0.020216941833496094 	 5.054473876953125e-05 	 5.1975250244140625e-05 	 1.0625650882720947 	 0.13849282264709473 	 3.981590270996094e-05 	 0.05904245376586914 	 combined
2025-07-24 22:32:21.726588 test begin: paddle.trace(x=Tensor([2, 6350401, 2],"float64"), offset=0, axis1=-3, axis2=-2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([2, 6350401, 2],"float64"), offset=0, axis1=-3, axis2=-2, ) 	 25401604 	 1000 	 0.5276217460632324 	 0.0323030948638916 	 3.5762786865234375e-05 	 0.00011587142944335938 	 1.3391623497009277 	 0.13843488693237305 	 7.224082946777344e-05 	 0.057505130767822266 	 combined
2025-07-24 22:32:24.393224 test begin: paddle.trace(x=Tensor([2, 6350401, 2],"float64"), offset=1, axis1=0, axis2=2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([2, 6350401, 2],"float64"), offset=1, axis1=0, axis2=2, ) 	 25401604 	 1000 	 0.4848930835723877 	 0.11914849281311035 	 7.033348083496094e-05 	 0.08452582359313965 	 1.3164160251617432 	 0.335756778717041 	 6.747245788574219e-05 	 0.1721959114074707 	 combined
2025-07-24 22:32:27.569115 test begin: paddle.trace(x=Tensor([3, 8467201],"float64"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([3, 8467201],"float64"), offset=0, axis1=0, axis2=1, ) 	 25401603 	 1000 	 0.2735109329223633 	 0.020090579986572266 	 3.7670135498046875e-05 	 5.4836273193359375e-05 	 0.8247919082641602 	 0.13834166526794434 	 5.2928924560546875e-05 	 0.05654716491699219 	 combined
2025-07-24 22:32:29.361152 test begin: paddle.trace(x=Tensor([4233601, 3, 2],"float64"), offset=0, axis1=-3, axis2=-2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([4233601, 3, 2],"float64"), offset=0, axis1=-3, axis2=-2, ) 	 25401606 	 1000 	 0.7069118022918701 	 0.031449317932128906 	 3.3855438232421875e-05 	 5.221366882324219e-05 	 1.9562339782714844 	 0.13849663734436035 	 6.175041198730469e-05 	 0.052340030670166016 	 combined
2025-07-24 22:32:32.768524 test begin: paddle.trace(x=Tensor([4233601, 3, 2],"float64"), offset=1, axis1=0, axis2=2, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([4233601, 3, 2],"float64"), offset=1, axis1=0, axis2=2, ) 	 25401606 	 1000 	 0.28380370140075684 	 0.03343772888183594 	 4.7206878662109375e-05 	 7.486343383789062e-05 	 8.619696855545044 	 0.13860154151916504 	 7.62939453125e-05 	 0.0365147590637207 	 combined
2025-07-24 22:32:42.423203 test begin: paddle.trace(x=Tensor([6350401, 4],"float64"), offset=0, axis1=0, axis2=1, )
[Prof] paddle.trace 	 paddle.trace(x=Tensor([6350401, 4],"float64"), offset=0, axis1=0, axis2=1, ) 	 25401604 	 1000 	 0.3409583568572998 	 0.019994497299194336 	 3.647804260253906e-05 	 5.078315734863281e-05 	 0.9831686019897461 	 0.13827133178710938 	 3.814697265625e-05 	 0.060385704040527344 	 combined
2025-07-24 22:32:44.477080 test begin: paddle.transpose(Tensor([2, 150, 512, 512],"float32"), list[0,2,3,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([2, 150, 512, 512],"float32"), list[0,2,3,1,], ) 	 78643200 	 1000 	 0.0035157203674316406 	 0.007152557373046875 	 9.775161743164062e-06 	 5.412101745605469e-05 	 0.042081594467163086 	 0.05374026298522949 	 6.723403930664062e-05 	 6.556510925292969e-05 	 
2025-07-24 22:32:47.380359 test begin: paddle.transpose(Tensor([2, 7168, 7168],"bfloat16"), list[0,2,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([2, 7168, 7168],"bfloat16"), list[0,2,1,], ) 	 102760448 	 1000 	 0.003545045852661133 	 0.004660367965698242 	 8.58306884765625e-06 	 2.1219253540039062e-05 	 0.04510235786437988 	 0.45882391929626465 	 2.5510787963867188e-05 	 0.36515283584594727 	 
2025-07-24 22:32:54.975781 test begin: paddle.transpose(Tensor([4, 150, 166, 512],"float32"), list[0,2,3,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([4, 150, 166, 512],"float32"), list[0,2,3,1,], ) 	 50995200 	 1000 	 0.003500223159790039 	 0.004823207855224609 	 7.152557373046875e-06 	 2.1696090698242188e-05 	 0.04106640815734863 	 0.053931474685668945 	 3.337860107421875e-05 	 5.316734313964844e-05 	 
2025-07-24 22:32:56.738817 test begin: paddle.transpose(Tensor([4, 150, 512, 166],"float32"), list[0,2,3,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([4, 150, 512, 166],"float32"), list[0,2,3,1,], ) 	 50995200 	 1000 	 0.0034766197204589844 	 0.004699230194091797 	 9.059906005859375e-06 	 1.8358230590820312e-05 	 0.04097485542297363 	 0.052964210510253906 	 2.4080276489257812e-05 	 5.269050598144531e-05 	 
2025-07-24 22:32:58.547292 test begin: paddle.transpose(Tensor([4, 3584, 7168],"bfloat16"), list[0,2,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([4, 3584, 7168],"bfloat16"), list[0,2,1,], ) 	 102760448 	 1000 	 0.0035669803619384766 	 0.004656076431274414 	 8.106231689453125e-06 	 2.09808349609375e-05 	 0.04491066932678223 	 0.4587829113006592 	 2.384185791015625e-05 	 0.3808112144470215 	 
2025-07-24 22:33:02.485934 test begin: paddle.transpose(Tensor([4, 49, 512, 512],"float32"), list[0,2,3,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([4, 49, 512, 512],"float32"), list[0,2,3,1,], ) 	 51380224 	 1000 	 0.0035066604614257812 	 0.006270885467529297 	 9.5367431640625e-06 	 6.389617919921875e-05 	 0.04237699508666992 	 0.053517818450927734 	 3.8623809814453125e-05 	 4.5299530029296875e-05 	 
2025-07-24 22:33:04.315695 test begin: paddle.transpose(Tensor([6, 2363, 7168],"bfloat16"), list[0,2,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([6, 2363, 7168],"bfloat16"), list[0,2,1,], ) 	 101627904 	 1000 	 0.003510713577270508 	 0.0046672821044921875 	 7.152557373046875e-06 	 2.1696090698242188e-05 	 0.04506659507751465 	 0.45383358001708984 	 3.170967102050781e-05 	 0.3764047622680664 	 
2025-07-24 22:33:08.298739 test begin: paddle.transpose(Tensor([6, 3584, 4726],"bfloat16"), list[0,2,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([6, 3584, 4726],"bfloat16"), list[0,2,1,], ) 	 101627904 	 1000 	 0.008781671524047852 	 0.0046389102935791016 	 2.8133392333984375e-05 	 2.193450927734375e-05 	 0.04506564140319824 	 0.45380520820617676 	 2.7894973754882812e-05 	 0.37360286712646484 	 
2025-07-24 22:33:12.416452 test begin: paddle.transpose(Tensor([6, 7168, 2363],"bfloat16"), list[0,2,1,], )
[Prof] paddle.transpose 	 paddle.transpose(Tensor([6, 7168, 2363],"bfloat16"), list[0,2,1,], ) 	 101627904 	 1000 	 0.003510713577270508 	 0.004671573638916016 	 7.3909759521484375e-06 	 1.9311904907226562e-05 	 0.04528450965881348 	 0.4537684917449951 	 3.933906555175781e-05 	 0.37591552734375 	 
2025-07-24 22:33:16.262707 test begin: paddle.tril(Tensor([1, 1, 2048, 24807],"bool"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 1, 2048, 24807],"bool"), ) 	 50804736 	 1000 	 0.30939292907714844 	 0.25914478302001953 	 0.2945702075958252 	 0.2478642463684082 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:33:18.604576 test begin: paddle.tril(Tensor([1, 1, 2048, 24807],"float32"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 1, 2048, 24807],"float32"), ) 	 50804736 	 1000 	 0.3111114501953125 	 0.3353238105773926 	 0.2962217330932617 	 0.3148627281188965 	 0.3109934329986572 	 0.3330373764038086 	 0.24770283699035645 	 0.2693750858306885 	 
2025-07-24 22:33:21.732164 test begin: paddle.tril(Tensor([1, 1, 24807, 2048],"bool"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 1, 24807, 2048],"bool"), ) 	 50804736 	 1000 	 0.3737633228302002 	 0.23795413970947266 	 0.3658308982849121 	 0.22201871871948242 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:33:24.140577 test begin: paddle.tril(Tensor([1, 1, 24807, 2048],"float32"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 1, 24807, 2048],"float32"), ) 	 50804736 	 1000 	 0.4287106990814209 	 0.37659740447998047 	 0.407092809677124 	 0.36553287506103516 	 0.4155690670013428 	 0.3761124610900879 	 0.354461669921875 	 0.31410861015319824 	 
2025-07-24 22:33:27.520792 test begin: paddle.tril(Tensor([1, 13, 2048, 2048],"bool"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 13, 2048, 2048],"bool"), ) 	 54525952 	 1000 	 0.3824288845062256 	 0.2509300708770752 	 0.3718404769897461 	 0.23909330368041992 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:33:30.114112 test begin: paddle.tril(Tensor([1, 13, 2048, 2048],"float32"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([1, 13, 2048, 2048],"float32"), ) 	 54525952 	 1000 	 0.41893792152404785 	 0.381683349609375 	 0.40828466415405273 	 0.3707301616668701 	 0.4163544178009033 	 0.38127946853637695 	 0.3661773204803467 	 0.3189864158630371 	 
2025-07-24 22:33:33.540386 test begin: paddle.tril(Tensor([13, 1, 2048, 2048],"bool"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([13, 1, 2048, 2048],"bool"), ) 	 54525952 	 1000 	 0.38382387161254883 	 0.7119295597076416 	 0.3759748935699463 	 0.2398831844329834 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:33:40.000674 test begin: paddle.tril(Tensor([13, 1, 2048, 2048],"float32"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([13, 1, 2048, 2048],"float32"), ) 	 54525952 	 1000 	 0.42533278465270996 	 0.3816566467285156 	 0.39894866943359375 	 0.36421847343444824 	 0.41500282287597656 	 0.38141465187072754 	 0.35416102409362793 	 0.3153839111328125 	 
2025-07-24 22:33:43.658144 test begin: paddle.tril(Tensor([2048, 24807],"bool"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([2048, 24807],"bool"), ) 	 50804736 	 1000 	 0.30789923667907715 	 0.25835752487182617 	 0.2929670810699463 	 0.24767661094665527 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:33:46.033017 test begin: paddle.tril(Tensor([24807, 2048],"bool"), )
[Prof] paddle.tril 	 paddle.tril(Tensor([24807, 2048],"bool"), ) 	 50804736 	 1000 	 0.3737607002258301 	 0.23609209060668945 	 0.3657703399658203 	 0.22531914710998535 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 22:33:48.449710 test begin: paddle.triu(Tensor([1, 1, 1024, 99226],"float16"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 1024, 99226],"float16"), diagonal=1, ) 	 101607424 	 1000 	 0.7688751220703125 	 0.5303177833557129 	 0.7601742744445801 	 0.5168943405151367 	 0.7692646980285645 	 0.5296120643615723 	 0.7128856182098389 	 0.46570539474487305 	 
2025-07-24 22:33:55.130518 test begin: paddle.triu(Tensor([1, 1, 12404, 4096],"float32"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 12404, 4096],"float32"), diagonal=1, ) 	 50806784 	 1000 	 0.3274393081665039 	 0.34146642684936523 	 0.31636524200439453 	 0.3277437686920166 	 0.32515406608581543 	 0.3391585350036621 	 0.2735311985015869 	 0.27420568466186523 	 
2025-07-24 22:34:00.678783 test begin: paddle.triu(Tensor([1, 1, 2048, 49613],"float16"), )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 2048, 49613],"float16"), ) 	 101607424 	 1000 	 0.7686684131622314 	 0.5342938899993896 	 0.7604928016662598 	 0.5169281959533691 	 0.7703850269317627 	 0.5249214172363281 	 0.719829797744751 	 0.4631664752960205 	 
2025-07-24 22:34:07.202688 test begin: paddle.triu(Tensor([1, 1, 4096, 12404],"float32"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 4096, 12404],"float32"), diagonal=1, ) 	 50806784 	 1000 	 0.4095895290374756 	 0.373124361038208 	 0.4010276794433594 	 0.362046480178833 	 0.40976881980895996 	 0.37124133110046387 	 0.34931421279907227 	 0.30913209915161133 	 
2025-07-24 22:34:10.564910 test begin: paddle.triu(Tensor([1, 1, 49613, 2048],"float16"), )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 49613, 2048],"float16"), ) 	 101607424 	 1000 	 0.589303731918335 	 0.3801076412200928 	 0.5811665058135986 	 0.35654401779174805 	 0.5905919075012207 	 0.36593127250671387 	 0.5401351451873779 	 0.3040030002593994 	 
2025-07-24 22:34:16.350857 test begin: paddle.triu(Tensor([1, 1, 99226, 1024],"float16"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 1, 99226, 1024],"float16"), diagonal=1, ) 	 101607424 	 1000 	 0.5855515003204346 	 0.36501049995422363 	 0.5770483016967773 	 0.35382628440856934 	 0.5855257511138916 	 0.364821195602417 	 0.5316908359527588 	 0.30332088470458984 	 
2025-07-24 22:34:22.042454 test begin: paddle.triu(Tensor([1, 25, 2048, 2048],"float16"), )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 25, 2048, 2048],"float16"), ) 	 104857600 	 1000 	 0.7458910942077637 	 0.4376981258392334 	 0.7368340492248535 	 0.4225025177001953 	 0.7425730228424072 	 0.43827009201049805 	 0.6888654232025146 	 0.3770771026611328 	 
2025-07-24 22:34:28.466595 test begin: paddle.triu(Tensor([1, 4, 4096, 4096],"float32"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 4, 4096, 4096],"float32"), diagonal=1, ) 	 67108864 	 1000 	 0.4955589771270752 	 0.46819329261779785 	 0.4865987300872803 	 0.45705437660217285 	 0.49512457847595215 	 0.4680013656616211 	 0.4444091320037842 	 0.40010690689086914 	 
2025-07-24 22:34:32.614174 test begin: paddle.triu(Tensor([1, 97, 1024, 1024],"float16"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([1, 97, 1024, 1024],"float16"), diagonal=1, ) 	 101711872 	 1000 	 0.7398233413696289 	 1.0925729274749756 	 0.7314019203186035 	 0.4148595333099365 	 0.741229772567749 	 0.42818260192871094 	 0.690746545791626 	 0.35922813415527344 	 
2025-07-24 22:34:40.563364 test begin: paddle.triu(Tensor([25, 1, 2048, 2048],"float16"), )
[Prof] paddle.triu 	 paddle.triu(Tensor([25, 1, 2048, 2048],"float16"), ) 	 104857600 	 1000 	 0.7423610687255859 	 0.4555947780609131 	 0.7341976165771484 	 0.42749547958374023 	 0.7440323829650879 	 0.4370083808898926 	 0.6922357082366943 	 0.36960911750793457 	 
2025-07-24 22:34:47.088159 test begin: paddle.triu(Tensor([4, 1, 4096, 4096],"float32"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([4, 1, 4096, 4096],"float32"), diagonal=1, ) 	 67108864 	 1000 	 0.4955759048461914 	 0.47538185119628906 	 0.48418641090393066 	 0.4600505828857422 	 0.49661755561828613 	 0.4680976867675781 	 0.44328737258911133 	 0.4069981575012207 	 
2025-07-24 22:34:51.239044 test begin: paddle.triu(Tensor([97, 1, 1024, 1024],"float16"), diagonal=1, )
[Prof] paddle.triu 	 paddle.triu(Tensor([97, 1, 1024, 1024],"float16"), diagonal=1, ) 	 101711872 	 1000 	 0.7398080825805664 	 0.4267764091491699 	 0.7312169075012207 	 0.41556644439697266 	 0.7411520481109619 	 0.4262208938598633 	 0.6906769275665283 	 0.3642001152038574 	 
2025-07-24 22:34:57.380435 test begin: paddle.trunc(Tensor([20, 2540161],"float32"), )
[Prof] paddle.trunc 	 paddle.trunc(Tensor([20, 2540161],"float32"), ) 	 50803220 	 1000 	 0.011354923248291016 	 0.29799675941467285 	 4.267692565917969e-05 	 0.28743767738342285 	 0.061328887939453125 	 0.1343250274658203 	 2.5510787963867188e-05 	 0.07343697547912598 	 
2025-07-24 22:34:59.958338 test begin: paddle.trunc(Tensor([2540161, 20],"float32"), )
[Prof] paddle.trunc 	 paddle.trunc(Tensor([2540161, 20],"float32"), ) 	 50803220 	 1000 	 0.008473873138427734 	 0.31575918197631836 	 2.6464462280273438e-05 	 0.2869579792022705 	 0.05082869529724121 	 0.1342618465423584 	 3.0040740966796875e-05 	 0.04993581771850586 	 
2025-07-24 22:35:03.732291 test begin: paddle.trunc(input=Tensor([117601, 6, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([117601, 6, 6, 6],"float64"), ) 	 25401816 	 1000 	 0.46097683906555176 	 0.30952000617980957 	 8.463859558105469e-05 	 0.28298044204711914 	 0.058983564376831055 	 0.13594865798950195 	 2.5272369384765625e-05 	 0.06695938110351562 	 
2025-07-24 22:35:06.921252 test begin: paddle.trunc(input=Tensor([19601, 6, 6, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([19601, 6, 6, 6, 6],"float64"), ) 	 25402896 	 1000 	 0.008536338806152344 	 0.30137157440185547 	 2.4318695068359375e-05 	 0.2877669334411621 	 0.051282644271850586 	 0.13454222679138184 	 2.8848648071289062e-05 	 0.07106328010559082 	 
2025-07-24 22:35:08.547163 test begin: paddle.trunc(input=Tensor([3, 39201, 6, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([3, 39201, 6, 6, 6],"float64"), ) 	 25402248 	 1000 	 0.009982824325561523 	 0.3070948123931885 	 2.2411346435546875e-05 	 0.2876112461090088 	 0.05059242248535156 	 0.13463616371154785 	 4.649162292480469e-05 	 0.047502994537353516 	 
2025-07-24 22:35:10.174868 test begin: paddle.trunc(input=Tensor([3, 6, 39201, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([3, 6, 39201, 6, 6],"float64"), ) 	 25402248 	 1000 	 0.008435726165771484 	 0.30327296257019043 	 2.2411346435546875e-05 	 0.2905914783477783 	 0.050894737243652344 	 0.1345658302307129 	 4.172325134277344e-05 	 0.07155942916870117 	 
2025-07-24 22:35:11.855237 test begin: paddle.trunc(input=Tensor([3, 6, 6, 39201, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([3, 6, 6, 39201, 6],"float64"), ) 	 25402248 	 1000 	 0.00843358039855957 	 0.29844141006469727 	 1.9073486328125e-05 	 0.2877202033996582 	 0.05043172836303711 	 0.13460087776184082 	 3.7670135498046875e-05 	 0.07058215141296387 	 
2025-07-24 22:35:13.516265 test begin: paddle.trunc(input=Tensor([3, 6, 6, 6, 39201],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([3, 6, 6, 6, 39201],"float64"), ) 	 25402248 	 1000 	 0.011255264282226562 	 0.29840564727783203 	 1.5735626220703125e-05 	 0.28780269622802734 	 0.05048084259033203 	 0.13463449478149414 	 4.315376281738281e-05 	 0.07075667381286621 	 
2025-07-24 22:35:15.145240 test begin: paddle.trunc(input=Tensor([6, 117601, 6, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([6, 117601, 6, 6],"float64"), ) 	 25401816 	 1000 	 0.009411334991455078 	 0.29849767684936523 	 2.6464462280273438e-05 	 0.2879772186279297 	 0.05334043502807617 	 0.13459038734436035 	 0.0005145072937011719 	 0.07046699523925781 	 
2025-07-24 22:35:16.788603 test begin: paddle.trunc(input=Tensor([6, 6, 117601, 6],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([6, 6, 117601, 6],"float64"), ) 	 25401816 	 1000 	 0.008471012115478516 	 0.3015458583831787 	 1.6689300537109375e-05 	 0.2872185707092285 	 0.051436424255371094 	 0.1346127986907959 	 1.7642974853515625e-05 	 0.06444048881530762 	 
2025-07-24 22:35:18.458494 test begin: paddle.trunc(input=Tensor([6, 6, 6, 117601],"float64"), )
[Prof] paddle.trunc 	 paddle.trunc(input=Tensor([6, 6, 6, 117601],"float64"), ) 	 25401816 	 1000 	 0.010344505310058594 	 0.2984733581542969 	 3.838539123535156e-05 	 0.28786301612854004 	 0.05342292785644531 	 0.13465142250061035 	 6.031990051269531e-05 	 0.0715935230255127 	 
2025-07-24 22:35:20.162584 test begin: paddle.unbind(Tensor([1128961, 9, 5],"float32"), axis=0, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f63bbc139d0>,)) (kwargs={}) timed out after 600.000000 seconds.

FATAL: exception not rethrown


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753368320 (unix time) try "date -d @1753368320" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3548) received by PID 13640 (TID 0x7f63b71c1640) from PID 13640 ***]

2025-07-24 22:45:30.672089 test begin: paddle.unbind(Tensor([1693441, 5, 6],"float32"), )
W0724 22:45:31.684614 60228 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f55e0e9f070>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 22:55:36.647873 test begin: paddle.unbind(Tensor([2, 3, 1058401, 8],"float32"), axis=0, )
W0724 22:55:39.833176 97966 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.unbind 	 paddle.unbind(Tensor([2, 3, 1058401, 8],"float32"), axis=0, ) 	 50803248 	 1000 	 0.007334470748901367 	 0.005843162536621094 	 2.1219253540039062e-05 	 3.647804260253906e-05 	 0.35344672203063965 	 0.30994272232055664 	 0.29656457901000977 	 0.19348883628845215 	 
2025-07-24 22:55:41.936421 test begin: paddle.unbind(Tensor([2, 3, 8, 1058401],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([2, 3, 8, 1058401],"float32"), axis=0, ) 	 50803248 	 1000 	 0.007338047027587891 	 0.005301952362060547 	 8.344650268554688e-06 	 2.1457672119140625e-05 	 0.3518404960632324 	 0.30992627143859863 	 0.2965056896209717 	 0.23245000839233398 	 
2025-07-24 22:55:44.269005 test begin: paddle.unbind(Tensor([2, 396901, 8, 8],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([2, 396901, 8, 8],"float32"), axis=0, ) 	 50803328 	 1000 	 0.007536172866821289 	 0.005311489105224609 	 1.0728836059570312e-05 	 1.9073486328125e-05 	 0.3491504192352295 	 0.30682373046875 	 0.29374122619628906 	 0.22395801544189453 	 
2025-07-24 22:55:48.103457 test begin: paddle.unbind(Tensor([264601, 3, 8, 8],"float32"), axis=0, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7fa9b5f93580>,)) (kwargs={}) timed out after 600.000000 seconds.

W0724 23:05:48.338557 98691 backward.cc:462] While running Node (UnbindGradNode) raises an EnforceNotMet exception
terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753369548 (unix time) try "date -d @1753369548" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17dfc) received by PID 97788 (TID 0x7fa9ad6fb640) from PID 97788 ***]

2025-07-24 23:05:58.746860 test begin: paddle.unbind(Tensor([3, 3386881, 5],"float32"), axis=0, )
W0724 23:05:59.799939 138748 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.unbind 	 paddle.unbind(Tensor([3, 3386881, 5],"float32"), axis=0, ) 	 50803215 	 1000 	 0.008132219314575195 	 0.007209300994873047 	 1.049041748046875e-05 	 4.1484832763671875e-05 	 0.35248804092407227 	 0.31060791015625 	 0.2933022975921631 	 0.21819210052490234 	 
2025-07-24 23:06:01.828648 test begin: paddle.unbind(Tensor([3, 9, 1881601],"float32"), axis=0, )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([3, 9, 1881601],"float32"), axis=0, ) 	 50803227 	 1000 	 0.008135795593261719 	 0.006319999694824219 	 8.106231689453125e-06 	 2.956390380859375e-05 	 0.3538956642150879 	 0.3118314743041992 	 0.28796911239624023 	 0.22452068328857422 	 
2025-07-24 23:06:04.228283 test begin: paddle.unbind(Tensor([4, 2116801, 6],"float32"), )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([4, 2116801, 6],"float32"), ) 	 50803224 	 1000 	 0.01459050178527832 	 0.007158994674682617 	 1.0251998901367188e-05 	 2.1696090698242188e-05 	 0.35126185417175293 	 0.3108208179473877 	 0.29077982902526855 	 0.21728897094726562 	 
2025-07-24 23:06:06.601759 test begin: paddle.unbind(Tensor([4, 5, 2540161],"float32"), )
[Prof] paddle.unbind 	 paddle.unbind(Tensor([4, 5, 2540161],"float32"), ) 	 50803220 	 1000 	 0.008998870849609375 	 0.00702214241027832 	 8.106231689453125e-06 	 2.0503997802734375e-05 	 0.35274267196655273 	 0.30932021141052246 	 0.2921271324157715 	 0.21077418327331543 	 
2025-07-24 23:06:08.967253 test begin: paddle.unflatten(x=Tensor([4, 1587601, 16],"float16"), axis=0, shape=tuple(2,2,), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([4, 1587601, 16],"float16"), axis=0, shape=tuple(2,2,), ) 	 101606464 	 1000 	 0.00816035270690918 	 0.008037805557250977 	 1.4781951904296875e-05 	 6.175041198730469e-05 	 0.04363822937011719 	 0.051604270935058594 	 2.6226043701171875e-05 	 2.8848648071289062e-05 	 
2025-07-24 23:06:13.082595 test begin: paddle.unflatten(x=Tensor([4, 6, 2116801],"bool"), axis=0, shape=tuple(2,2,), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([4, 6, 2116801],"bool"), axis=0, shape=tuple(2,2,), ) 	 50803224 	 1000 	 0.008002042770385742 	 0.0053157806396484375 	 1.0967254638671875e-05 	 1.8358230590820312e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 23:06:14.645249 test begin: paddle.unflatten(x=Tensor([4, 6, 2116801],"float32"), axis=0, shape=Tensor([2],"int64"), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([4, 6, 2116801],"float32"), axis=0, shape=Tensor([2],"int64"), ) 	 50803226 	 1000 	 0.09457921981811523 	 0.005379199981689453 	 3.266334533691406e-05 	 1.8358230590820312e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:06:16.523851 test begin: paddle.unflatten(x=Tensor([4, 6, 4233601],"float16"), axis=0, shape=tuple(2,2,), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([4, 6, 4233601],"float16"), axis=0, shape=tuple(2,2,), ) 	 101606424 	 1000 	 0.008114814758300781 	 0.005372762680053711 	 7.867813110351562e-06 	 1.9073486328125e-05 	 0.04381132125854492 	 0.05196976661682129 	 5.4836273193359375e-05 	 5.698204040527344e-05 	 
2025-07-24 23:06:20.590433 test begin: paddle.unflatten(x=Tensor([4, 793801, 16],"bool"), axis=0, shape=tuple(2,2,), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([4, 793801, 16],"bool"), axis=0, shape=tuple(2,2,), ) 	 50803264 	 1000 	 0.008441448211669922 	 0.005356311798095703 	 2.956390380859375e-05 	 1.8596649169921875e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 23:06:22.085784 test begin: paddle.unflatten(x=Tensor([4, 793801, 16],"float32"), axis=0, shape=Tensor([2],"int64"), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([4, 793801, 16],"float32"), axis=0, shape=Tensor([2],"int64"), ) 	 50803266 	 1000 	 0.12903928756713867 	 0.009668588638305664 	 4.482269287109375e-05 	 2.1696090698242188e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:06:24.060631 test begin: paddle.unflatten(x=Tensor([529201, 6, 16],"float32"), axis=0, shape=Tensor([2],"int64"), )
[Prof] paddle.unflatten 	 paddle.unflatten(x=Tensor([529201, 6, 16],"float32"), axis=0, shape=Tensor([2],"int64"), ) 	 50803298 	 1000 	 0.0942230224609375 	 0.00966954231262207 	 3.504753112792969e-05 	 2.0503997802734375e-05 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:06:26.012879 test begin: paddle.unfold(Tensor([5, 10160641],"float32"), 0, 5, 1, )
[Prof] paddle.unfold 	 paddle.unfold(Tensor([5, 10160641],"float32"), 0, 5, 1, ) 	 50803205 	 1000 	 0.0164339542388916 	 0.004463672637939453 	 1.9311904907226562e-05 	 2.1696090698242188e-05 	 0.462277889251709 	 1.2330470085144043 	 0.40416407585144043 	 0.41973876953125 	 
2025-07-24 23:06:29.513126 test begin: paddle.unfold(Tensor([5, 20321281],"float16"), 0, 5, 1, )
[Prof] paddle.unfold 	 paddle.unfold(Tensor([5, 20321281],"float16"), 0, 5, 1, ) 	 101606405 	 1000 	 0.0163419246673584 	 0.004353046417236328 	 8.821487426757812e-06 	 1.811981201171875e-05 	 0.9005622863769531 	 1.4013078212738037 	 0.8420348167419434 	 0.4771232604980469 	 
2025-07-24 23:06:36.926475 test begin: paddle.unfold(Tensor([5, 5080321],"float64"), 0, 5, 1, )
[Prof] paddle.unfold 	 paddle.unfold(Tensor([5, 5080321],"float64"), 0, 5, 1, ) 	 25401605 	 1000 	 0.016534090042114258 	 0.008212804794311523 	 1.0251998901367188e-05 	 4.696846008300781e-05 	 0.3083460330963135 	 1.1615536212921143 	 0.24901771545410156 	 0.3958470821380615 	 
2025-07-24 23:06:42.496602 test begin: paddle.unique(Tensor([25401601],"int64"), )
[Prof] paddle.unique 	 paddle.unique(Tensor([25401601],"int64"), ) 	 25401601 	 1000 	 6.723015546798706 	 3.2670013904571533 	 9.393692016601562e-05 	 0.00018310546875 	 None 	 None 	 None 	 None 	 
2025-07-24 23:06:53.046792 test begin: paddle.unique(Tensor([25401601],"int64"), return_index=True, return_inverse=True, return_counts=True, dtype="int32", )
[Prof] paddle.unique 	 paddle.unique(Tensor([25401601],"int64"), return_index=True, return_inverse=True, return_counts=True, dtype="int32", ) 	 25401601 	 1000 	 10.101083040237427 	 11.189725875854492 	 8.893013000488281e-05 	 0.0002067089080810547 	 None 	 None 	 None 	 None 	 
2025-07-24 23:07:14.912751 test begin: paddle.unique_consecutive(Tensor([25401601],"float64"), )
[Prof] paddle.unique_consecutive 	 paddle.unique_consecutive(Tensor([25401601],"float64"), ) 	 25401601 	 1000 	 2.069192409515381 	 0.3789029121398926 	 8.7738037109375e-05 	 0.00012540817260742188 	 None 	 None 	 None 	 None 	 
2025-07-24 23:07:17.965740 test begin: paddle.unique_consecutive(Tensor([25401601],"float64"), return_inverse=True, return_counts=True, )
[Prof] paddle.unique_consecutive 	 paddle.unique_consecutive(Tensor([25401601],"float64"), return_inverse=True, return_counts=True, ) 	 25401601 	 1000 	 6.012632608413696 	 1.013568639755249 	 9.1552734375e-05 	 0.0002028942108154297 	 None 	 None 	 None 	 None 	 
2025-07-24 23:07:25.581046 test begin: paddle.unique_consecutive(Tensor([25401601],"float64"), return_inverse=True, return_counts=True, axis=-1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7efea1d1eaa0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 23:17:32.852783 test begin: paddle.unsqueeze(Tensor([25, 1024, 1024],"int64"), 1, )
Warning: The core code of paddle.unsqueeze is too complex.
W0724 23:17:33.481081 21792 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([25, 1024, 1024],"int64"), 1, ) 	 26214400 	 1000 	 0.008770942687988281 	 0.0074350833892822266 	 1.1920928955078125e-05 	 4.291534423828125e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 23:17:34.435393 test begin: paddle.unsqueeze(Tensor([3970, 50, 256],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([3970, 50, 256],"float32"), axis=2, ) 	 50816000 	 1000 	 0.004434823989868164 	 0.0037696361541748047 	 6.4373016357421875e-06 	 2.5987625122070312e-05 	 0.042574167251586914 	 0.07195687294006348 	 2.9087066650390625e-05 	 5.459785461425781e-05 	 
2025-07-24 23:17:39.132771 test begin: paddle.unsqueeze(Tensor([4, 1024, 6202],"int64"), 1, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([4, 1024, 6202],"int64"), 1, ) 	 25403392 	 1000 	 0.004225492477416992 	 0.0038614273071289062 	 1.1682510375976562e-05 	 1.9311904907226562e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 23:17:40.061633 test begin: paddle.unsqueeze(Tensor([4, 6202, 1024],"int64"), 1, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([4, 6202, 1024],"int64"), 1, ) 	 25403392 	 1000 	 0.004273414611816406 	 0.0039021968841552734 	 8.106231689453125e-06 	 1.7881393432617188e-05 	 None 	 None 	 None 	 None 	 
[Error] element 0 of tensors does not require grad and does not have a grad_fn
2025-07-24 23:17:41.021198 test begin: paddle.unsqueeze(Tensor([416, 478, 256],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([416, 478, 256],"float32"), axis=2, ) 	 50905088 	 1000 	 0.0044062137603759766 	 0.0038938522338867188 	 6.4373016357421875e-06 	 2.4557113647460938e-05 	 0.052008628845214844 	 0.060975074768066406 	 4.9591064453125e-05 	 6.103515625e-05 	 
2025-07-24 23:17:42.848601 test begin: paddle.unsqueeze(Tensor([416, 50, 2443],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([416, 50, 2443],"float32"), axis=2, ) 	 50814400 	 1000 	 0.004400968551635742 	 0.0038251876831054688 	 1.049041748046875e-05 	 1.9550323486328125e-05 	 0.04303169250488281 	 0.05524301528930664 	 4.124641418457031e-05 	 7.939338684082031e-05 	 
2025-07-24 23:17:44.801517 test begin: paddle.unsqueeze(Tensor([512, 388, 256],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([512, 388, 256],"float32"), axis=2, ) 	 50855936 	 1000 	 0.00507664680480957 	 0.0037224292755126953 	 3.910064697265625e-05 	 1.9311904907226562e-05 	 0.043050289154052734 	 0.05357956886291504 	 2.0265579223632812e-05 	 5.626678466796875e-05 	 
2025-07-24 23:17:46.710373 test begin: paddle.unsqueeze(Tensor([512, 50, 1985],"float32"), axis=2, )
[Prof] paddle.unsqueeze 	 paddle.unsqueeze(Tensor([512, 50, 1985],"float32"), axis=2, ) 	 50816000 	 1000 	 0.004460811614990234 	 0.00376129150390625 	 6.198883056640625e-06 	 1.9788742065429688e-05 	 0.04293632507324219 	 0.05341005325317383 	 3.0994415283203125e-05 	 4.9114227294921875e-05 	 
2025-07-24 23:17:48.558702 test begin: paddle.unstack(Tensor([338689, 10, 15],"float32"), axis=-1, )
[Prof] paddle.unstack 	 paddle.unstack(Tensor([338689, 10, 15],"float32"), axis=-1, ) 	 50803350 	 1000 	 0.32314348220825195 	 0.01861858367919922 	 0.29656434059143066 	 2.574920654296875e-05 	 0.4571561813354492 	 4.464088439941406 	 0.38272619247436523 	 4.276818037033081 	 
2025-07-24 23:17:55.414520 test begin: paddle.unstack(Tensor([338689, 10, 15],"float32"), axis=-2, )
[Prof] paddle.unstack 	 paddle.unstack(Tensor([338689, 10, 15],"float32"), axis=-2, ) 	 50803350 	 1000 	 0.3977339267730713 	 0.013929605484008789 	 0.376481294631958 	 6.0558319091796875e-05 	 0.33740973472595215 	 1.2733149528503418 	 0.2705702781677246 	 1.1436903476715088 	 
2025-07-24 23:17:59.032598 test begin: paddle.unstack(Tensor([5, 10, 1016065],"float32"), axis=-1, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7efef8daa620>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-07-24 23:28:05.067197 test begin: paddle.unstack(Tensor([5, 10, 1016065],"float32"), axis=-2, )
W0724 23:28:06.159778 63115 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.unstack 	 paddle.unstack(Tensor([5, 10, 1016065],"float32"), axis=-2, ) 	 50803250 	 1000 	 0.3953287601470947 	 0.014953136444091797 	 0.36072492599487305 	 5.340576171875e-05 	 0.3291935920715332 	 0.30744099617004395 	 0.261824369430542 	 0.16766738891601562 	 
2025-07-24 23:28:08.567273 test begin: paddle.unstack(Tensor([5, 677377, 15],"float32"), axis=-1, )
[Prof] paddle.unstack 	 paddle.unstack(Tensor([5, 677377, 15],"float32"), axis=-1, ) 	 50803275 	 1000 	 0.32416343688964844 	 0.021291255950927734 	 0.2803151607513428 	 7.653236389160156e-05 	 0.4434237480163574 	 4.481134653091431 	 0.35866665840148926 	 4.309449911117554 	 
2025-07-24 23:28:15.421757 test begin: paddle.unstack(Tensor([5, 677377, 15],"float32"), axis=-2, )
Traceback (most recent call last):
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 189, in <module>
    main()
  File "/root/paddlejob/workspace/env_run/ningzs/PaddleAPITest/engine.py", line 163, in main
    case.test()
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.10/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.paddle_torch_gpu_performance.APITestPaddleTorchGPUPerformance object at 0x7f4f1ea6e6e0>,)) (kwargs={}) timed out after 600.000000 seconds.



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1753371497 (unix time) try "date -d @1753371497" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x556399e3ddf0) received by PID 62737 (TID 0x7f4f1a00c640) from PID 18446744071996431856 ***]

2025-07-24 23:38:29.824860 test begin: paddle.unstack(x=Tensor([2, 32, 793801],"float32"), axis=0, )
W0724 23:38:30.858682 104281 gpu_resources.cc:114] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.8
[Prof] paddle.unstack 	 paddle.unstack(x=Tensor([2, 32, 793801],"float32"), axis=0, ) 	 50803264 	 1000 	 0.3865969181060791 	 0.005838155746459961 	 0.37061500549316406 	 2.574920654296875e-05 	 0.34972500801086426 	 0.30928945541381836 	 0.2700765132904053 	 0.21824049949645996 	 
2025-07-24 23:38:33.209816 test begin: paddle.unstack(x=Tensor([2, 49613, 512],"float32"), axis=0, )
[Prof] paddle.unstack 	 paddle.unstack(x=Tensor([2, 49613, 512],"float32"), axis=0, ) 	 50803712 	 1000 	 0.3869450092315674 	 0.005280494689941406 	 0.3733539581298828 	 2.0503997802734375e-05 	 0.34977293014526367 	 0.30535364151000977 	 0.28374218940734863 	 0.21071362495422363 	 
2025-07-24 23:38:39.282852 test begin: paddle.unstack(x=Tensor([3101, 32, 512],"float32"), axis=0, )
[Prof] paddle.unstack 	 paddle.unstack(x=Tensor([3101, 32, 512],"float32"), axis=0, ) 	 50806784 	 1000 	 4.98244833946228 	 5.879053831100464 	 9.942054748535156e-05 	 0.00013518333435058594 	 5.476985931396484 	 21.296958923339844 	 8.034706115722656e-05 	 0.0002300739288330078 	 
2025-07-24 23:39:19.815326 test begin: paddle.var(Tensor([264601, 192, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
W0724 23:39:20.579507 107419 dygraph_functions.cc:88394] got different data type, run type promotion automatically, this may cause data type been changed.
[Prof] paddle.var 	 paddle.var(Tensor([264601, 192, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50803392 	 1000 	 1.3133594989776611 	 0.2768681049346924 	 0.1110386848449707 	 0.26073145866394043 	 1.5762975215911865 	 0.7785706520080566 	 0.26848649978637695 	 0.19844317436218262 	 
2025-07-24 23:39:24.877400 test begin: paddle.var(Tensor([384, 132301, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([384, 132301, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50803584 	 1000 	 1.0329334735870361 	 0.18699884414672852 	 0.07530331611633301 	 0.09616947174072266 	 13.745236158370972 	 0.7753622531890869 	 2.0063791275024414 	 0.1581714153289795 	 
2025-07-24 23:39:41.546526 test begin: paddle.var(Tensor([384, 192, 1, 690],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([384, 192, 1, 690],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50872320 	 1000 	 1.0235121250152588 	 0.18008947372436523 	 0.07444381713867188 	 0.09127330780029297 	 1.3764612674713135 	 0.7759654521942139 	 0.20087933540344238 	 0.15912771224975586 	 
2025-07-24 23:39:45.723248 test begin: paddle.var(Tensor([384, 192, 690, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([384, 192, 690, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50872320 	 1000 	 1.0230016708374023 	 0.17907214164733887 	 0.07457327842712402 	 0.09149408340454102 	 13.533985376358032 	 0.7715139389038086 	 1.976311206817627 	 0.15766048431396484 	 
2025-07-24 23:40:02.110466 test begin: paddle.var(Tensor([384, 96, 1, 1379],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([384, 96, 1, 1379],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50835456 	 1000 	 1.0255672931671143 	 0.180924654006958 	 0.07465171813964844 	 0.09243631362915039 	 1.3788549900054932 	 0.7732741832733154 	 0.2008984088897705 	 0.15803766250610352 	 
2025-07-24 23:40:06.297009 test begin: paddle.var(Tensor([384, 96, 1379, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([384, 96, 1379, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50835456 	 1000 	 1.0268633365631104 	 0.18085026741027832 	 0.07466959953308105 	 0.09238862991333008 	 13.933031558990479 	 0.7788243293762207 	 2.143540620803833 	 0.15974068641662598 	 
2025-07-24 23:40:24.246897 test begin: paddle.var(Tensor([529201, 96, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([529201, 96, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50803296 	 1000 	 1.2893550395965576 	 0.45418238639831543 	 0.1092684268951416 	 0.4379749298095703 	 1.5895612239837646 	 0.8256757259368896 	 0.2708122730255127 	 0.21008658409118652 	 
2025-07-24 23:40:29.345987 test begin: paddle.var(Tensor([58801, 96, 3, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([58801, 96, 3, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50804064 	 1000 	 0.978069543838501 	 0.17697811126708984 	 0.08306360244750977 	 0.1608285903930664 	 1.3590705394744873 	 0.7727227210998535 	 0.23159265518188477 	 0.19758367538452148 	 
2025-07-24 23:40:33.474770 test begin: paddle.var(Tensor([96, 58801, 3, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([96, 58801, 3, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50804064 	 1000 	 1.004408597946167 	 0.2177748680114746 	 0.07226848602294922 	 0.11124753952026367 	 1.3463146686553955 	 0.7862403392791748 	 0.1966087818145752 	 0.1606426239013672 	 
2025-07-24 23:40:39.204589 test begin: paddle.var(Tensor([96, 96, 1838, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([96, 96, 1838, 3],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50817024 	 1000 	 0.9926626682281494 	 0.21328973770141602 	 0.07214045524597168 	 0.10895085334777832 	 1.3477182388305664 	 0.78672194480896 	 0.1964113712310791 	 0.16079950332641602 	 
2025-07-24 23:40:43.372582 test begin: paddle.var(Tensor([96, 96, 3, 1838],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Prof] paddle.var 	 paddle.var(Tensor([96, 96, 3, 1838],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, ) 	 50817024 	 1000 	 0.9928169250488281 	 0.21322202682495117 	 0.0722496509552002 	 0.10892438888549805 	 1.3494064807891846 	 0.785271406173706 	 0.19635653495788574 	 0.1604609489440918 	 
2025-07-24 23:40:47.577450 test begin: paddle.vecdot(Tensor([12700801, 4],"float32"), Tensor([12700801, 4],"float32"), axis=-1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([12700801, 4],"float32"), Tensor([12700801, 4],"float32"), axis=-1, ) 	 101606408 	 1000 	 1.1525161266326904 	 0.9317624568939209 	 0.39167284965515137 	 0.4761219024658203 	 1.6157290935516357 	 0.6835143566131592 	 0.5496022701263428 	 0.34920835494995117 	 combined
2025-07-24 23:40:53.984086 test begin: paddle.vecdot(Tensor([1270081, 4, 5],"float64"), Tensor([1270081, 4, 5],"float64"), axis=1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([1270081, 4, 5],"float64"), Tensor([1270081, 4, 5],"float64"), axis=1, ) 	 50803240 	 1000 	 1.033156156539917 	 0.6338093280792236 	 0.35067081451416016 	 0.3245246410369873 	 1.2432293891906738 	 0.6729598045349121 	 0.423722505569458 	 0.34377622604370117 	 combined
2025-07-24 23:40:58.844689 test begin: paddle.vecdot(Tensor([2, 3, 4233601],"float64"), Tensor([2, 3, 4233601],"float64"), axis=-1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([2, 3, 4233601],"float64"), Tensor([2, 3, 4233601],"float64"), axis=-1, ) 	 50803212 	 1000 	 0.9967911243438721 	 0.5974798202514648 	 0.2537822723388672 	 0.20427346229553223 	 1.1797072887420654 	 0.600557804107666 	 0.4020862579345703 	 0.3068091869354248 	 combined
2025-07-24 23:41:03.416537 test begin: paddle.vecdot(Tensor([2, 3175201, 4],"float64"), Tensor([2, 3175201, 4],"float64"), axis=-1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([2, 3175201, 4],"float64"), Tensor([2, 3175201, 4],"float64"), axis=-1, ) 	 50803216 	 1000 	 0.999091386795044 	 0.7142410278320312 	 0.3395872116088867 	 0.36432766914367676 	 1.2437317371368408 	 0.6721620559692383 	 0.42381811141967773 	 0.3433876037597656 	 combined
2025-07-24 23:41:10.294547 test begin: paddle.vecdot(Tensor([2116801, 3, 4],"float64"), Tensor([2116801, 3, 4],"float64"), axis=-1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([2116801, 3, 4],"float64"), Tensor([2116801, 3, 4],"float64"), axis=-1, ) 	 50803224 	 1000 	 0.9966294765472412 	 0.7147226333618164 	 0.3395357131958008 	 0.3643345832824707 	 1.243431568145752 	 0.6721353530883789 	 0.4238109588623047 	 0.34337496757507324 	 combined
2025-07-24 23:41:15.327096 test begin: paddle.vecdot(Tensor([3, 16934401],"float32"), Tensor([3, 16934401],"float32"), axis=-1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([3, 16934401],"float32"), Tensor([3, 16934401],"float32"), axis=-1, ) 	 101606406 	 1000 	 0.9699954986572266 	 0.5987756252288818 	 0.24721884727478027 	 0.2037525177001953 	 1.5438287258148193 	 0.6094541549682617 	 0.5262041091918945 	 0.3114199638366699 	 combined
2025-07-24 23:41:20.912544 test begin: paddle.vecdot(Tensor([3, 1693441, 5],"float64"), Tensor([3, 1693441, 5],"float64"), axis=1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([3, 1693441, 5],"float64"), Tensor([3, 1693441, 5],"float64"), axis=1, ) 	 50803230 	 1000 	 7.056737422943115 	 0.5994737148284912 	 1.80715012550354 	 0.20393085479736328 	 1.1813323497772217 	 0.6011087894439697 	 0.40215611457824707 	 0.30707526206970215 	 combined
2025-07-24 23:41:31.512104 test begin: paddle.vecdot(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), axis=1, )
[Prof] paddle.vecdot 	 paddle.vecdot(Tensor([3, 4, 2116801],"float64"), Tensor([3, 4, 2116801],"float64"), axis=1, ) 	 50803224 	 1000 	 0.9335851669311523 	 1.6444790363311768 	 0.3184933662414551 	 0.32252955436706543 	 1.3404247760772705 	 0.8990600109100342 	 0.4573085308074951 	 0.4593334197998047 	 combined
2025-07-24 23:41:39.391177 test begin: paddle.view(Tensor([10, 10, 10, 50804],"float32"), list[-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([10, 10, 10, 50804],"float32"), list[-1,], ) 	 50804000 	 1000 	 0.013796567916870117 	 0.003936290740966797 	 1.6689300537109375e-05 	 1.7642974853515625e-05 	 0.041425228118896484 	 0.05057239532470703 	 3.3855438232421875e-05 	 5.221366882324219e-05 	 
2025-07-24 23:41:41.551005 test begin: paddle.view(Tensor([10, 10, 10, 50804],"float32"), list[10,100,-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([10, 10, 10, 50804],"float32"), list[10,100,-1,], ) 	 50804000 	 1000 	 0.013736486434936523 	 0.00396275520324707 	 9.059906005859375e-06 	 2.09808349609375e-05 	 0.041405677795410156 	 0.05254626274108887 	 2.6226043701171875e-05 	 5.5789947509765625e-05 	 
2025-07-24 23:41:43.327084 test begin: paddle.view(Tensor([10, 10, 25402, 20],"float32"), list[-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([10, 10, 25402, 20],"float32"), list[-1,], ) 	 50804000 	 1000 	 0.01348567008972168 	 0.003859281539916992 	 2.86102294921875e-05 	 2.002716064453125e-05 	 0.04108405113220215 	 0.050189971923828125 	 3.790855407714844e-05 	 4.124641418457031e-05 	 
2025-07-24 23:41:45.090824 test begin: paddle.view(Tensor([10, 10, 25402, 20],"float32"), list[10,100,-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([10, 10, 25402, 20],"float32"), list[10,100,-1,], ) 	 50804000 	 1000 	 0.013620853424072266 	 0.0039103031158447266 	 1.2159347534179688e-05 	 1.9311904907226562e-05 	 0.041388511657714844 	 0.05098438262939453 	 2.6941299438476562e-05 	 4.0531158447265625e-05 	 
2025-07-24 23:41:46.817541 test begin: paddle.view(Tensor([10, 25402, 10, 20],"float32"), list[-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([10, 25402, 10, 20],"float32"), list[-1,], ) 	 50804000 	 1000 	 0.013584375381469727 	 0.003951311111450195 	 8.821487426757812e-06 	 1.8835067749023438e-05 	 0.04164886474609375 	 0.04982113838195801 	 2.4318695068359375e-05 	 4.1961669921875e-05 	 
2025-07-24 23:41:48.559016 test begin: paddle.view(Tensor([10, 25402, 10, 20],"float32"), list[10,100,-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([10, 25402, 10, 20],"float32"), list[10,100,-1,], ) 	 50804000 	 1000 	 0.013717174530029297 	 0.003954887390136719 	 2.2411346435546875e-05 	 2.09808349609375e-05 	 0.0411686897277832 	 0.05297684669494629 	 1.8596649169921875e-05 	 7.796287536621094e-05 	 
2025-07-24 23:41:50.414203 test begin: paddle.view(Tensor([25402, 10, 10, 20],"float32"), list[-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([25402, 10, 10, 20],"float32"), list[-1,], ) 	 50804000 	 1000 	 0.013673543930053711 	 0.003802061080932617 	 2.0265579223632812e-05 	 1.6927719116210938e-05 	 0.04260373115539551 	 0.051007986068725586 	 2.47955322265625e-05 	 7.128715515136719e-05 	 
2025-07-24 23:41:52.162295 test begin: paddle.view(Tensor([25402, 10, 10, 20],"float32"), list[10,100,-1,], )
[Prof] paddle.view 	 paddle.view(Tensor([25402, 10, 10, 20],"float32"), list[10,100,-1,], ) 	 50804000 	 1000 	 0.013547420501708984 	 0.00402379035949707 	 8.821487426757812e-06 	 1.8835067749023438e-05 	 0.04176783561706543 	 0.053401947021484375 	 3.0517578125e-05 	 5.316734313964844e-05 	 
2025-07-24 23:41:54.102890 test begin: paddle.view_as(Tensor([10, 10, 10, 50804],"float32"), Tensor([10, 100, 50804],"float32"), )
[Prof] paddle.view_as 	 paddle.view_as(Tensor([10, 10, 10, 50804],"float32"), Tensor([10, 100, 50804],"float32"), ) 	 101608000 	 1000 	 0.014204978942871094 	 0.004609346389770508 	 1.0251998901367188e-05 	 4.315376281738281e-05 	 None 	 None 	 None 	 None 	 combined
[Error] One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
2025-07-24 23:41:56.616806 test begin: paddle.vsplit(Tensor([2116801, 4, 3],"int64"), list[-1,1,3,], )
W0724 23:41:57.863073 117445 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 23:41:57.925389 test begin: paddle.vsplit(Tensor([2116801, 4, 3],"int64"), list[-1,], )
W0724 23:41:58.643739 117590 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 23:41:58.664055 test begin: paddle.vsplit(Tensor([2116801, 4, 3],"int64"), list[2,4,], )
W0724 23:41:59.394889 117731 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 23:41:59.459409 test begin: paddle.vsplit(Tensor([6, 1411201, 3],"int64"), list[-1,1,3,], )
W0724 23:42:00.428057 117749 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 23:42:00.454779 test begin: paddle.vsplit(Tensor([6, 1411201, 3],"int64"), list[-1,], )
W0724 23:42:01.175240 117751 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 23:42:01.197895 test begin: paddle.vsplit(Tensor([6, 1411201, 3],"int64"), list[2,4,], )
W0724 23:42:01.956807 117828 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 23:42:01.979336 test begin: paddle.vsplit(Tensor([6, 4, 1058401],"int64"), list[-1,1,3,], )
W0724 23:42:02.964814 117907 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 23:42:03.003574 test begin: paddle.vsplit(Tensor([6, 4, 1058401],"int64"), list[-1,], )
W0724 23:42:03.742007 118052 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 23:42:03.763979 test begin: paddle.vsplit(Tensor([6, 4, 1058401],"int64"), list[2,4,], )
W0724 23:42:04.521832 118064 backward.cc:462] While running Node (SliceGradNode) raises an EnforceNotMet exception
[Error] (Unimplemented) Gradient accumulation of data type (int64_t) on place (Place(gpu:0)) is not supported in imperative mode (at ../paddle/fluid/imperative/gradient_accumulator.cc:242)

2025-07-24 23:42:04.549582 test begin: paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], ) 	 76204872 	 1000 	 0.9510228633880615 	 0.9229609966278076 	 0.16172170639038086 	 0.9086735248565674 	 0.9439189434051514 	 0.07012295722961426 	 0.16024518013000488 	 5.936622619628906e-05 	 
2025-07-24 23:42:10.734063 test begin: paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),Tensor([3, 4, 2, 1058401],"float64"),], name=None, ) 	 76204872 	 1000 	 0.9497218132019043 	 0.9255771636962891 	 0.16148710250854492 	 0.9075281620025635 	 0.9410390853881836 	 0.06817936897277832 	 0.16023039817810059 	 5.054473876953125e-05 	 
2025-07-24 23:42:16.867701 test begin: paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 1058401],"float64"),], ) 	 25401624 	 1000 	 0.3138134479522705 	 0.3131685256958008 	 0.15991640090942383 	 0.1599254608154297 	 0.3131906986236572 	 0.05382704734802246 	 0.1599750518798828 	 5.817413330078125e-05 	 
2025-07-24 23:42:18.923962 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401880 	 1000 	 0.31940484046936035 	 0.3233017921447754 	 0.08172440528869629 	 0.30855417251586914 	 0.32285475730895996 	 0.0701446533203125 	 0.08228421211242676 	 5.602836608886719e-05 	 
2025-07-24 23:42:21.037303 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], name=None, ) 	 25401880 	 1000 	 0.32248759269714355 	 0.32309985160827637 	 0.08172726631164551 	 0.30913472175598145 	 0.32158684730529785 	 0.07233786582946777 	 0.082275390625 	 3.838539123535156e-05 	 
2025-07-24 23:42:23.131251 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401880 	 1000 	 0.32560253143310547 	 0.32452869415283203 	 0.08300614356994629 	 0.3078024387359619 	 0.3280487060546875 	 0.084014892578125 	 0.0836174488067627 	 0.0001049041748046875 	 
2025-07-24 23:42:25.289281 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], name=None, ) 	 25401880 	 1000 	 0.32561802864074707 	 0.3203282356262207 	 0.08299708366394043 	 0.3062760829925537 	 0.3295407295227051 	 0.0693972110748291 	 0.08363080024719238 	 5.8650970458984375e-05 	 
2025-07-24 23:42:27.443915 test begin: paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], ) 	 76204980 	 1000 	 0.9674618244171143 	 0.9193904399871826 	 0.16246461868286133 	 0.9046006202697754 	 0.9496982097625732 	 0.06878209114074707 	 0.16230416297912598 	 4.839897155761719e-05 	 
2025-07-24 23:42:39.508793 test begin: paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),Tensor([3, 4, 423361, 5],"float64"),], name=None, ) 	 76204980 	 1000 	 0.9541397094726562 	 0.9208128452301025 	 0.16232895851135254 	 0.9063804149627686 	 0.9465553760528564 	 0.06847405433654785 	 0.16095423698425293 	 4.649162292480469e-05 	 
2025-07-24 23:42:45.661895 test begin: paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 4, 423361, 5],"float64"),], ) 	 25401660 	 1000 	 0.3139948844909668 	 0.31307291984558105 	 0.16004252433776855 	 0.15982604026794434 	 0.3131122589111328 	 0.054068565368652344 	 0.15993213653564453 	 5.054473876953125e-05 	 
2025-07-24 23:42:47.779724 test begin: paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], ) 	 76204890 	 1000 	 0.9589934349060059 	 0.932755708694458 	 0.16382932662963867 	 0.9183731079101562 	 0.9393455982208252 	 0.06795907020568848 	 0.15997695922851562 	 4.100799560546875e-05 	 
2025-07-24 23:42:53.890261 test begin: paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),Tensor([3, 846721, 2, 5],"float64"),], name=None, ) 	 76204890 	 1000 	 0.9523706436157227 	 0.939340353012085 	 0.16217565536499023 	 0.9107286930084229 	 0.9406685829162598 	 0.08249235153198242 	 0.159956693649292 	 0.00010418891906738281 	 
2025-07-24 23:43:00.031600 test begin: paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([3, 846721, 2, 5],"float64"),], ) 	 25401630 	 1000 	 0.3138735294342041 	 0.3256814479827881 	 0.15992188453674316 	 0.16021728515625 	 0.31330204010009766 	 0.07624053955078125 	 0.16000962257385254 	 7.343292236328125e-05 	 
2025-07-24 23:43:02.348917 test begin: paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], ) 	 25401880 	 1000 	 0.31928467750549316 	 0.31084537506103516 	 0.08121895790100098 	 0.29695653915405273 	 0.31868934631347656 	 0.07021450996398926 	 0.08122491836547852 	 5.602836608886719e-05 	 
2025-07-24 23:43:04.476002 test begin: paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),Tensor([3, 4, 2, 5],"float64"),], name=None, ) 	 25401880 	 1000 	 0.3187885284423828 	 0.3123435974121094 	 0.0812218189239502 	 0.29832887649536133 	 0.31871700286865234 	 0.07231307029724121 	 0.08121800422668457 	 6.556510925292969e-05 	 
2025-07-24 23:43:06.577472 test begin: paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], ) 	 76204920 	 1000 	 0.9505584239959717 	 0.9273264408111572 	 0.16148948669433594 	 0.9099130630493164 	 0.9418668746948242 	 0.0732274055480957 	 0.1601569652557373 	 5.7220458984375e-05 	 
2025-07-24 23:43:13.218605 test begin: paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], name=None, )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),Tensor([635041, 4, 2, 5],"float64"),], name=None, ) 	 76204920 	 1000 	 0.9482517242431641 	 0.9230470657348633 	 0.1614823341369629 	 0.9086072444915771 	 0.9417977333068848 	 0.06785869598388672 	 0.16014766693115234 	 5.412101745605469e-05 	 
2025-07-24 23:43:19.388784 test begin: paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),], )
[Prof] paddle.vstack 	 paddle.vstack(list[Tensor([635041, 4, 2, 5],"float64"),], ) 	 25401640 	 1000 	 0.3139033317565918 	 0.31502676010131836 	 0.15993595123291016 	 0.16130542755126953 	 0.31324100494384766 	 0.0557093620300293 	 0.160003662109375 	 5.245208740234375e-05 	 
2025-07-24 23:43:21.446894 test begin: paddle.where(Tensor([1, 400, 127009],"bool"), Tensor([1, 400, 127009],"float32"), Tensor([1, 400, 127009],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([1, 400, 127009],"bool"), Tensor([1, 400, 127009],"float32"), Tensor([1, 400, 127009],"float32"), ) 	 152410800 	 1000 	 0.4850449562072754 	 0.4859640598297119 	 0.4579195976257324 	 0.4722442626953125 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:43:26.089285 test begin: paddle.where(Tensor([1, 400, 65856],"bool"), Tensor([1, 400, 65856],"float32"), Tensor([2, 400, 65856],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([1, 400, 65856],"bool"), Tensor([1, 400, 65856],"float32"), Tensor([2, 400, 65856],"float32"), ) 	 105369600 	 1000 	 0.9350626468658447 	 0.5176572799682617 	 0.3187692165374756 	 0.49260663986206055 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:43:31.287276 test begin: paddle.where(Tensor([1, 400, 65856],"bool"), Tensor([2, 400, 65856],"float32"), Tensor([1, 400, 65856],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([1, 400, 65856],"bool"), Tensor([2, 400, 65856],"float32"), Tensor([1, 400, 65856],"float32"), ) 	 105369600 	 1000 	 0.9368321895599365 	 1.5297019481658936 	 0.318650484085083 	 0.4941697120666504 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:43:40.227381 test begin: paddle.where(Tensor([1, 772, 65856],"bool"), Tensor([1, 772, 65856],"float32"), Tensor([1, 772, 65856],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([1, 772, 65856],"bool"), Tensor([1, 772, 65856],"float32"), Tensor([1, 772, 65856],"float32"), ) 	 152522496 	 1000 	 0.4976537227630615 	 0.48436450958251953 	 0.4683523178100586 	 0.4707934856414795 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:43:44.981035 test begin: paddle.where(Tensor([2, 400, 65856],"bool"), Tensor([1, 400, 65856],"float32"), Tensor([1, 400, 65856],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([2, 400, 65856],"bool"), Tensor([1, 400, 65856],"float32"), Tensor([1, 400, 65856],"float32"), ) 	 105369600 	 1000 	 1.1171960830688477 	 0.5164244174957275 	 0.38164615631103516 	 0.5008845329284668 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:43:50.244886 test begin: paddle.where(Tensor([2, 400, 65856],"bool"), Tensor([2, 400, 65856],"float32"), Tensor([2, 400, 65856],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([2, 400, 65856],"bool"), Tensor([2, 400, 65856],"float32"), Tensor([2, 400, 65856],"float32"), ) 	 158054400 	 1000 	 0.5028049945831299 	 0.5002696514129639 	 0.4759697914123535 	 0.4862709045410156 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:43:55.298976 test begin: paddle.where(Tensor([4, 125, 320, 320],"bool"), Tensor([4, 125, 320, 320],"float32"), Tensor([4, 125, 320, 320],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 125, 320, 320],"bool"), Tensor([4, 125, 320, 320],"float32"), Tensor([4, 125, 320, 320],"float32"), ) 	 153600000 	 1000 	 0.48874855041503906 	 0.48636627197265625 	 0.470259428024292 	 0.4728047847747803 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:44:00.062098 test begin: paddle.where(Tensor([4, 280, 376, 25, 5],"bool"), Tensor([4, 280, 376, 25, 5],"float32"), Tensor([4, 280, 376, 25, 5],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 280, 376, 25, 5],"bool"), Tensor([4, 280, 376, 25, 5],"float32"), Tensor([4, 280, 376, 25, 5],"float32"), ) 	 157920000 	 1000 	 0.5028257369995117 	 0.49991488456726074 	 0.48435068130493164 	 0.4857141971588135 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:44:05.020354 test begin: paddle.where(Tensor([4, 280, 376, 41, 3],"bool"), Tensor([4, 280, 376, 41, 3],"float32"), Tensor([4, 280, 376, 41, 3],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 280, 376, 41, 3],"bool"), Tensor([4, 280, 376, 41, 3],"float32"), Tensor([4, 280, 376, 41, 3],"float32"), ) 	 155393280 	 1000 	 0.49582934379577637 	 0.4929015636444092 	 0.47722887992858887 	 0.4792497158050537 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:44:09.824660 test begin: paddle.where(Tensor([4, 280, 605, 25, 3],"bool"), Tensor([4, 280, 605, 25, 3],"float32"), Tensor([4, 280, 605, 25, 3],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 280, 605, 25, 3],"bool"), Tensor([4, 280, 605, 25, 3],"float32"), Tensor([4, 280, 605, 25, 3],"float32"), ) 	 152460000 	 1000 	 0.4874579906463623 	 0.48592638969421387 	 0.4690382480621338 	 0.470416784286499 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:44:14.526266 test begin: paddle.where(Tensor([4, 451, 376, 25, 3],"bool"), Tensor([4, 451, 376, 25, 3],"float32"), Tensor([4, 451, 376, 25, 3],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 451, 376, 25, 3],"bool"), Tensor([4, 451, 376, 25, 3],"float32"), Tensor([4, 451, 376, 25, 3],"float32"), ) 	 152618400 	 1000 	 0.4895060062408447 	 0.48363828659057617 	 0.4687342643737793 	 0.46920013427734375 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:44:19.275812 test begin: paddle.where(Tensor([4, 64, 320, 621],"bool"), Tensor([4, 64, 320, 621],"float32"), Tensor([4, 64, 320, 621],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 64, 320, 621],"bool"), Tensor([4, 64, 320, 621],"float32"), Tensor([4, 64, 320, 621],"float32"), ) 	 152616960 	 1000 	 0.4856534004211426 	 0.48711109161376953 	 0.4671311378479004 	 0.4701721668243408 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:44:24.077918 test begin: paddle.where(Tensor([4, 64, 621, 320],"bool"), Tensor([4, 64, 621, 320],"float32"), Tensor([4, 64, 621, 320],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([4, 64, 621, 320],"bool"), Tensor([4, 64, 621, 320],"float32"), Tensor([4, 64, 621, 320],"float32"), ) 	 152616960 	 1000 	 0.48612332344055176 	 0.48478102684020996 	 0.467637300491333 	 0.47042226791381836 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:44:28.872899 test begin: paddle.where(Tensor([7, 280, 376, 25, 3],"bool"), Tensor([7, 280, 376, 25, 3],"float32"), Tensor([7, 280, 376, 25, 3],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([7, 280, 376, 25, 3],"bool"), Tensor([7, 280, 376, 25, 3],"float32"), Tensor([7, 280, 376, 25, 3],"float32"), ) 	 165816000 	 1000 	 0.527195930480957 	 0.5251250267028809 	 0.5087063312530518 	 0.5108580589294434 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:44:34.032246 test begin: paddle.where(Tensor([8, 64, 320, 320],"bool"), Tensor([8, 64, 320, 320],"float32"), Tensor([8, 64, 320, 320],"float32"), )
[Prof] paddle.where 	 paddle.where(Tensor([8, 64, 320, 320],"bool"), Tensor([8, 64, 320, 320],"float32"), Tensor([8, 64, 320, 320],"float32"), ) 	 157286400 	 1000 	 1.5018587112426758 	 0.4995307922363281 	 0.48272228240966797 	 0.4839794635772705 	 None 	 None 	 None 	 None 	 
[Error] One of the differentiated Tensors does not require grad
2025-07-24 23:44:41.672715 test begin: paddle.zeros_like(Tensor([16, 64, 320, 320],"float16"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([16, 64, 320, 320],"float16"), ) 	 104857600 	 1000 	 0.1412031650543213 	 0.1384427547454834 	 0.12759065628051758 	 0.1265404224395752 	 None 	 None 	 None 	 None 	 
2025-07-24 23:44:45.144733 test begin: paddle.zeros_like(Tensor([4, 1051, 12096],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 1051, 12096],"float32"), ) 	 50851584 	 1000 	 0.3584907054901123 	 0.144545316696167 	 0.12365221977233887 	 0.12268471717834473 	 None 	 None 	 None 	 None 	 
2025-07-24 23:44:47.536504 test begin: paddle.zeros_like(Tensor([4, 125, 320, 320],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 125, 320, 320],"float32"), ) 	 51200000 	 1000 	 0.13753581047058105 	 0.14020133018493652 	 0.12592864036560059 	 0.12369179725646973 	 None 	 None 	 None 	 None 	 
2025-07-24 23:44:48.651162 test begin: paddle.zeros_like(Tensor([4, 249, 320, 320],"float16"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 249, 320, 320],"float16"), ) 	 101990400 	 1000 	 0.1345977783203125 	 0.13475489616394043 	 0.12409472465515137 	 0.1224510669708252 	 None 	 None 	 None 	 None 	 
2025-07-24 23:44:50.788294 test begin: paddle.zeros_like(Tensor([4, 525, 24193],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 525, 24193],"float32"), ) 	 50805300 	 1000 	 0.13829898834228516 	 0.13426637649536133 	 0.12761950492858887 	 0.1228187084197998 	 None 	 None 	 None 	 None 	 
2025-07-24 23:44:51.912929 test begin: paddle.zeros_like(Tensor([4, 64, 1241, 320],"float16"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 64, 1241, 320],"float16"), ) 	 101662720 	 1000 	 0.13410425186157227 	 0.13775134086608887 	 0.12366080284118652 	 0.12010478973388672 	 None 	 None 	 None 	 None 	 
2025-07-24 23:44:54.094141 test begin: paddle.zeros_like(Tensor([4, 64, 320, 1241],"float16"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 64, 320, 1241],"float16"), ) 	 101662720 	 1000 	 0.13512206077575684 	 0.13422489166259766 	 0.12342357635498047 	 0.1221921443939209 	 None 	 None 	 None 	 None 	 
2025-07-24 23:44:56.350925 test begin: paddle.zeros_like(Tensor([4, 64, 320, 621],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 64, 320, 621],"float32"), ) 	 50872320 	 1000 	 0.13802838325500488 	 0.13837981224060059 	 0.12514591217041016 	 0.12364888191223145 	 None 	 None 	 None 	 None 	 
2025-07-24 23:44:57.467526 test begin: paddle.zeros_like(Tensor([4, 64, 621, 320],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([4, 64, 621, 320],"float32"), ) 	 50872320 	 1000 	 0.1343076229095459 	 0.13440942764282227 	 0.12374186515808105 	 0.12249231338500977 	 None 	 None 	 None 	 None 	 
2025-07-24 23:44:58.549368 test begin: paddle.zeros_like(Tensor([8, 64, 320, 320],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([8, 64, 320, 320],"float32"), ) 	 52428800 	 1000 	 0.13836359977722168 	 0.13841652870178223 	 0.12772631645202637 	 0.12684035301208496 	 None 	 None 	 None 	 None 	 
2025-07-24 23:44:59.661087 test begin: paddle.zeros_like(Tensor([9, 525, 12096],"float32"), )
[Prof] paddle.zeros_like 	 paddle.zeros_like(Tensor([9, 525, 12096],"float32"), ) 	 57153600 	 1000 	 0.15126371383666992 	 0.15057063102722168 	 0.14013147354125977 	 0.13884234428405762 	 None 	 None 	 None 	 None 	 
